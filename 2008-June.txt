From nouiz at mail.berlios.de  Mon Jun  2 16:54:36 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 2 Jun 2008 16:54:36 +0200
Subject: [Plearn-commits] r9081 - trunk/plearn_learners/regressors
Message-ID: <200806021454.m52EsaL3013476@sheep.berlios.de>

Author: nouiz
Date: 2008-06-02 16:54:36 +0200 (Mon, 02 Jun 2008)
New Revision: 9081

Modified:
   trunk/plearn_learners/regressors/RegressionTree.cc
Log:
always check if we need to print stuff, 


Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2008-05-31 21:22:57 UTC (rev 9080)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2008-06-02 14:54:36 UTC (rev 9081)
@@ -227,6 +227,9 @@
         }
         if (report_progress) pb->update(stage);
     }
+    pb = NULL;
+    verbose("split_cols: "+tostring(split_cols),2);
+    verbose("split_values: "+tostring(split_values),2);
     if (compute_train_stats < 1) return;
     if (report_progress)
     {
@@ -250,8 +253,6 @@
         if (report_progress) pb->update(train_sample_index);
     }
     train_stats->finalize();
-    verbose("split_cols: "+tostring(split_cols),2);
-    verbose("split_values: "+tostring(split_values),2);
 }
 
 void RegressionTree::verbose(string the_msg, int the_level)



From larocheh at mail.berlios.de  Mon Jun  2 20:40:23 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Mon, 2 Jun 2008 20:40:23 +0200
Subject: [Plearn-commits] r9082 - trunk/plearn_learners/online
Message-ID: <200806021840.m52IeNp2030373@sheep.berlios.de>

Author: larocheh
Date: 2008-06-02 20:40:23 +0200 (Mon, 02 Jun 2008)
New Revision: 9082

Modified:
   trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc
   trunk/plearn_learners/online/RBMMatrixTransposeConnection.h
Log:
Added an option to learn the scale of the transposed matrix.


Modified: trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc	2008-06-02 14:54:36 UTC (rev 9081)
+++ trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc	2008-06-02 18:40:23 UTC (rev 9082)
@@ -53,7 +53,9 @@
     real the_learning_rate,
     bool call_build_) :
     inherited(the_learning_rate, call_build_),
-    rbm_matrix_connection(the_rbm_matrix_connection)
+    rbm_matrix_connection(the_rbm_matrix_connection),
+    learn_scale( false ),
+    scale( 1.0 )
 {
     if (call_build_)
         build_();
@@ -66,6 +68,17 @@
                   OptionBase::buildoption,
                   "RBMMatrixConnection from which the weights are taken");
 
+    declareOption(ol, "learn_scale", 
+                  &RBMMatrixTransposeConnection::learn_scale,
+                  OptionBase::buildoption,
+                  "Indication that the scale of the weight matrix should be "
+                  "learned.\n");
+
+    declareOption(ol, "scale", 
+                  &RBMMatrixTransposeConnection::scale,
+                  OptionBase::learntoption,
+                  "Learned scale for weight matrix.\n");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 
@@ -144,6 +157,9 @@
 
 void RBMMatrixTransposeConnection::update()
 {
+    if( learn_scale )
+        PLERROR("In RBMMatrixTransposeConnection::update(): not implemented "
+                "for learned scale");
     // updates parameters
     //weights -= learning_rate * (weights_pos_stats/pos_count
     //                              - weights_neg_stats/neg_count)
@@ -199,6 +215,10 @@
                                            const Vec& neg_down_values, // v_1
                                            const Vec& neg_up_values )  // h_1
 {
+    if( learn_scale )
+        PLERROR("In RBMMatrixTransposeConnection::update(): not implemented "
+                "for learned scale");
+
     PLASSERT_MSG( rbm_matrix_connection, "RBMMatrixTransposeConnection must be given an rbm_matrix_connection.\n");
     // weights -= learning_rate * ( h_0 v_0' - h_1 v_1' );
     // or:
@@ -290,6 +310,8 @@
                      weights.subMatRows(start,length),
                      input_vec );
     }
+    if( learn_scale)
+        activations *= scale;
 }
 
 void RBMMatrixTransposeConnection::computeProducts(int start, int length,
@@ -325,6 +347,9 @@
                     inputs_mat,
                     weights.subMatRows(start,length) );
     }
+
+    if( learn_scale)
+        activations *= scale;
 }
 
 //! this version allows to obtain the input gradient as well
@@ -357,6 +382,19 @@
     
     // weights -= learning_rate * output_gradient * input'
     externalProductScaleAcc( weights, input, output_gradient, -learning_rate );
+    if( learn_scale )
+    {
+        real* in = input.data();
+        real* out_g;
+        real* wj;
+        for( int j=0; j<weights.width(); j++)
+        {
+            out_g = output_gradient.data();
+            wj = weights[j];
+            for( int i=0; i<weights.length(); i++ )
+                scale -= learning_rate * out_g[i] * wj[i] * in[j];
+        }
+    }
 }
 
 void RBMMatrixTransposeConnection::bpropUpdate(const Mat& inputs, const Mat& outputs,
@@ -388,6 +426,23 @@
     // weights -= learning_rate/n * output_gradients' * inputs
     transposeProductScaleAcc(weights, inputs, output_gradients,
                              -learning_rate / inputs.length(), real(1));
+
+    if( learn_scale )
+    {
+        for( int t=0; t<inputs.length(); t++)
+        {
+            real* in = inputs[t];
+            real* out_g;
+            real* wj;
+            for( int j=0; j<weights.width(); j++)
+            {
+                out_g = output_gradients[t];
+                wj = weights[j];
+                for( int i=0; i<weights.length(); i++ )
+                    scale -= learning_rate * out_g[i] * wj[i] * in[j];
+            }
+        }
+    }
 }
 
 
@@ -406,6 +461,8 @@
     if( !(rbm_matrix_connection->random_gen) )
         rbm_matrix_connection->random_gen = random_gen;
     rbm_matrix_connection->forget();
+    if( learn_scale )
+        scale = 1;
 }
 
 

Modified: trunk/plearn_learners/online/RBMMatrixTransposeConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMMatrixTransposeConnection.h	2008-06-02 14:54:36 UTC (rev 9081)
+++ trunk/plearn_learners/online/RBMMatrixTransposeConnection.h	2008-06-02 18:40:23 UTC (rev 9082)
@@ -65,8 +65,14 @@
     //! RBMMatrixConnection from which the weights are taken
     PP<RBMMatrixConnection> rbm_matrix_connection;
 
+    //! Indication that the scale of the weight matrix should be learned
+    bool learn_scale;
+
     //#####  Not Options  #####################################################
 
+    //! Learned scale for weight matrix
+    real scale;
+
     //! Accumulates positive contribution to the weights' gradient
     Mat weights_pos_stats;
 



From plearner at mail.berlios.de  Mon Jun  2 23:23:33 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Mon, 2 Jun 2008 23:23:33 +0200
Subject: [Plearn-commits] r9083 - in trunk/python_modules/plearn: plotting
	utilities
Message-ID: <200806022123.m52LNXkO016454@sheep.berlios.de>

Author: plearner
Date: 2008-06-02 23:23:30 +0200 (Mon, 02 Jun 2008)
New Revision: 9083

Added:
   trunk/python_modules/plearn/utilities/autoscript.py
Modified:
   trunk/python_modules/plearn/plotting/netplot.py
Log:
Added autoscript to allow making command-line scripts in one line from a python function or callable.
(no need to write any argument parser or script help!)



Modified: trunk/python_modules/plearn/plotting/netplot.py
===================================================================
--- trunk/python_modules/plearn/plotting/netplot.py	2008-06-02 18:40:23 UTC (rev 9082)
+++ trunk/python_modules/plearn/plotting/netplot.py	2008-06-02 21:23:30 UTC (rev 9083)
@@ -1,3 +1,37 @@
+# netplot.py
+# Copyright (C) 2007-2008 Pascal Vincent
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+
+# Author: Pascal Vincent
+
+
 from pylab import *
 from math import *
 from numpy import *

Added: trunk/python_modules/plearn/utilities/autoscript.py
===================================================================
--- trunk/python_modules/plearn/utilities/autoscript.py	2008-06-02 18:40:23 UTC (rev 9082)
+++ trunk/python_modules/plearn/utilities/autoscript.py	2008-06-02 21:23:30 UTC (rev 9083)
@@ -0,0 +1,238 @@
+# autoscript.py
+# Copyright (C) 2008 Pascal Vincent
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+
+# Author: Pascal Vincent
+
+
+import sys, inspect, types
+
+class AutoscriptError(Exception):
+    def __init__(self, message):
+        self.message = message
+    def __str__(self):
+        return repr(self.message)
+
+
+def parse_command_line(argv):
+    """argv is expected to be a list as the one in sys.argv.
+    The call returns (args, kwargs)
+    args is the list of initial non-keyword arguments passed.
+    kwargs is a dictionary of keyword arguments.
+    """
+    args = []
+    kwargs = {}
+    for arg in argv[1:]:
+        if arg.find('=')>=0:
+            key, val = arg.split('=',1)
+            kwargs[key.strip()] = val.strip()
+        else:
+            if kwargs:
+                raise AutoscriptError('You cannot specify positional arguments after you specified keyword arguments: use key=value syntax for argument '+str(arg))
+            args.append(arg)
+    return args, kwargs
+
+def prefix_lines(prefix, text):
+    return "\n".join([prefix+line for line in text.split('\n')])
+    
+def autocomplete(args, all_argnames):
+    """This function autocompletes the names in args from an official list of all_argnames
+    if it can be done unambiguously.
+    args can be either a single string or a list of strings or a map whose keys are string.
+    In all these cases an object of the same kind is returned with the strings autocompleted.
+    """       
+    if type(args) is str:
+        prefix = args
+        matches = [ arg for arg in all_argnames if arg.startswith(prefix) ]
+        if len(matches)==0:
+            raise AutoscriptError("No matching argument name completion for "+prefix+"... in"+str(all_argnames))  
+        elif len(matches)>1:
+            raise AutoscriptError("Ambiguity in completion of "+prefix+"... Possible names are: "+str(matches))
+        return matches[0]
+    
+    elif type(args) is list:
+        return [autocomplete(arg, all_argnames) for arg in args]
+
+    elif type(args) is dict:
+        compldict = {}
+        for key, val in args.items():
+            compldict[autocomplete(key, all_argnames)] = val
+        return compldict
+
+    
+def check_args(args, kwargs, all_argnames, default_values):
+    """Verify if the arguments provided in args list and kwargs dictionary
+    are compatible with calling a function defined with specified all_argnames and default_values.
+    Raise explicit AutoscriptError if not.
+    Upon success, returns a full_kwargs, corresponding to kwargs completed with the name:value of
+    given positional args.
+    Ex: check_args(['a','b','c'],[35], [1,2], {'c':3})
+
+    """
+    # check if all non default arguments have been provided
+    pos_of_first_default = len(all_argnames)-len(default_values)
+    for argname in all_argnames[len(args):pos_of_first_default]:
+        if argname not in kwargs:
+            raise AutoscriptError("Missing mandatory argument: "+argname+". You must specify it.")
+
+    # check if all specified kwargs are valid argnames
+    invalid_names = [ argname for argname in kwargs.keys() if argname not in all_argnames ]
+    if len(invalid_names)>0:
+        raise AutoscriptError("Unknown arguments: "+str(invalid_names))
+
+    # build a full_kwars
+    full_kwargs = kwargs.copy()
+    for i in range(len(args)):
+        value = args[i]
+        try:
+            name = all_argnames[i]
+        except IndexError:
+            raise AutoscriptError("Too many arguments provided.")
+        if name in full_kwargs:
+            raise AutoscriptError("Argument "+name+" is duplicated: provided both as positional argument at position "+str(i+1)+" and as a keyword argument")
+        full_kwargs[name] = value
+    return full_kwargs
+
+
+def print_call_arguments(kwargs, all_argnames, default_values):
+    n = len(default_values)
+    defaults = dict(zip(all_argnames[-n:], default_values))
+
+    for argname in all_argnames:
+        if argname in kwargs:
+            print "# "+argname+" = "+repr(kwargs[argname])
+        else:
+            print "# "+argname+" = "+repr(defaults[argname])+ "   (default value)"            
+
+def eval_str_argument_values(kwargs, all_argnames, default_values):
+    n = len(default_values)
+    defaults = dict(zip(all_argnames[-n:], default_values))
+    for key, val in kwargs.items():        
+        if type(val) is str and val!='':
+            if ((key not in defaults) or (type(defaults[key]) is not str) or val[0] in ("'",'"')):
+                try:
+                    kwargs[key] = eval(val)
+                except:
+                    pass
+
+def mystr(s):
+    if s=="" or s==" ":
+        return repr(s)
+    else:
+        return str(s)
+
+def quote_if_needed(s):
+    if ' ' in s:
+        s = '"'+s+'"'
+    return s
+
+def usage_text(scriptname, all_argnames, default_values):
+    defvalpos = len(all_argnames)-len(default_values)
+    txt = "Usage: "+scriptname+"  "+\
+          "  ".join(all_argnames[0:defvalpos])+"  "+\
+          "  ".join(quote_if_needed(name+'='+mystr(value)) for name,value in zip(all_argnames[defvalpos:],default_values))
+    return txt
+
+def autoscript(callme, autocomplete_names=False, helptext="", argv=sys.argv):
+    """Call autoscript as a one-liner to turn a callable (function, class or object) into a command-line script.
+    Typical usage goes as follows:
+    if __name__ == '__main__':
+       from plearn.utilities.autoscript import autoscript
+       autoscript(myFuncionOrClassOrObjectToCall)
+
+    If you choose to set autocomplete_names to True, then the 
+    user can abreviate optionnames as long as there is no ambiguity.
+
+    If the script is invoked with no arguments,
+    a detailed help will be printed constituted of:
+    - *** Help on scriptname ***
+    - The optionally provided helptext
+    - A usage line listing all arguments and their default values
+    - The docstring associated to the callable.
+    """
+    
+    if not callable(callme):
+        raise ValueError("First argument to autoscript must be callable: either a function or a class with an __init__ method or a callable instance""")
+
+    if type(callme) is types.FunctionType:
+        all_argnames, varargs, varkw, default_values = inspect.getargspec(callme)
+
+    elif type(callme) is types.ClassType:
+        all_argnames, varargs, varkw, default_values = inspect.getargspec(callme.__init__)
+        all_argnames = all_argnames[1:] # skip self
+
+    elif type(callme) is types.InstanceType:
+        all_argnames, varargs, varkw, default_values = inspect.getargspec(callme.__call__)
+        all_argnames = all_argnames[1:] # skip self
+
+    if default_values is None:
+        default_values = []
+
+    scriptname = argv[0]
+    if len(argv)<=1:
+        print
+        print "#"*80
+        print "# Help on "+scriptname
+        print prefix_lines("# ",helptext)
+        print "# "+usage_text(scriptname, all_argnames, default_values)
+        if callme.__doc__ is not None:
+            print prefix_lines("# ",callme.__doc__)
+        print "#"*80
+        print
+        sys.exit()
+        
+    # Parse arguments
+    try:
+        args, kwargs = parse_command_line(argv)
+
+        if(autocomplete_names):
+            kwargs = autocomplete(kwargs, all_argnames)
+
+        kwargs = check_args(args, kwargs, all_argnames, default_values)
+        eval_str_argument_values(kwargs, all_argnames, default_values)
+        print
+        print "#"*80
+        print "# Calling "+scriptname+" with following arguments: "
+        print_call_arguments(kwargs, all_argnames, default_values)
+        print "#"*80
+        print
+    except AutoscriptError, inst:
+        print
+        print "#"*80
+        print "# Error when invoking "+scriptname
+        print prefix_lines("# ",inst.message)
+        print "# "+usage_text(scriptname, all_argnames, default_values)
+        print "# Invoke "+scriptname+" with no arguments to display detailed help."
+        print "#"*80
+        print
+        sys.exit()
+
+    # Call the callable
+    callme(**kwargs)



From plearner at mail.berlios.de  Mon Jun  2 23:26:20 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Mon, 2 Jun 2008 23:26:20 +0200
Subject: [Plearn-commits] r9084 - in trunk/scripts: . EXPERIMENTAL
Message-ID: <200806022126.m52LQKeP016818@sheep.berlios.de>

Author: plearner
Date: 2008-06-02 23:26:19 +0200 (Mon, 02 Jun 2008)
New Revision: 9084

Modified:
   trunk/scripts/EXPERIMENTAL/dcaexperiment.py
   trunk/scripts/EXPERIMENTAL/linearfilters.py
   trunk/scripts/show_rows_as_images.py
Log:
Changed scripts to use the autoscript facility


Modified: trunk/scripts/EXPERIMENTAL/dcaexperiment.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/dcaexperiment.py	2008-06-02 21:23:30 UTC (rev 9083)
+++ trunk/scripts/EXPERIMENTAL/dcaexperiment.py	2008-06-02 21:26:19 UTC (rev 9084)
@@ -42,7 +42,8 @@
                  offdiag_nonlinearity="square", 
                  offdiag_premul = 1.0,
                  lr=0.01, nsteps=1, optimizer_nsteps=1,
-                 epsilon=1e-4, nu=0,
+                 force_zero_mean = False,
+                 epsilon=1e-6, nu=0,
                  img_height=None,
                  img_width=None):
 
@@ -51,8 +52,12 @@
         #some calculations for plotting
         if img_height is None:
             img_size = len(X[0])
-            img_width = math.sqrt(img_size)
-            img_height = img_size/img_width
+            if img_size==28*20: # hack for frey faces to automatically "guess" correct dimentsions
+                img_height = 28
+                img_width = 20
+            else:
+                img_width = math.sqrt(img_size)
+                img_height = img_size/img_width
         self.img_height = img_height
         self.img_width = img_width
 
@@ -83,6 +88,7 @@
             optimizer = pl.GradientOptimizer(start_learning_rate = lr,
                                              decrease_constant = 0,
                                              nstages = optimizer_nsteps),
+            force_zero_mean = force_zero_mean,
             epsilon = epsilon,
             nu = nu
             )
@@ -378,7 +384,7 @@
 if __name__ == "__main__":
 
     try:
-        dataset, learner_seed, ncomponents, constrain_norm_type, cov_transformation_type, diag_weight, diag_nonlinearity, diag_premul, offdiag_weight, offdiag_nonlinearity, offdiag_premul = sys.argv[1:]        
+        dataset, learner_seed, ncomponents, constrain_norm_type, cov_transformation_type, diag_weight, diag_nonlinearity, diag_premul, offdiag_weight, offdiag_nonlinearity, offdiag_premul, force_zero_mean = sys.argv[1:]        
 
         learner_seed = int(learner_seed)
         ncomponents = int(ncomponents)
@@ -387,23 +393,27 @@
         diag_premul = float(diag_premul)
         offdiag_weight = float(offdiag_weight)
         offdiag_premul = float(offdiag_premul)
-        
+        force_zero_mean = int(force_zero_mean)
 
 
 
     except:
-        print "Usage: "+sys.argv[0]+" dataset learner_seed ncomponents constrain_norm_type cov_transformation_type diag_weight diag_nonlinearity diag_premul offdiag_weight offdiag_nonlinearity offdiag_premul"
+        print "Usage: "+sys.argv[0]+" dataset learner_seed ncomponents constrain_norm_type cov_transformation_type diag_weight diag_nonlinearity diag_premul offdiag_weight offdiag_nonlinearity offdiag_premul force_zero_mean"
         print "  dataset can be either a .pmat or data_seed:ngaussians"
         print """  constrain_norm_type controls how to constrain the norms of rows of W:
-        -1:constrained source;
-        -2:explicit normalization;
+        -1: L1 constrained source;
+        -2: L2 constrained source;
+        -3:explicit normalization;
         >0:ordinary weight decay"""
         print """  cov_transformation_type controls the kind of transformation to apply to covariance matrix
         cov: no transformation (keep covariance)
         corr: transform into correlations, but keeping variances on the diagonal.
-        squaredist: do a 'squared distance kernel' kind of transformation."""
-        print "Ex: "+sys.argv[0]+" 123:1    123 2    -1 cov     -1 square 1       1 square 1"
-        print "Ex: "+sys.argv[0]+" 121:-2    123 4    -1 squaredist     0 exp 1       1 exp -1.6"
+        squaredist: do a 'squared distance kernel' kind of transformation.
+        sincov: uses sin of gangle instead of cos of angle
+        """
+        print "Ex: "+sys.argv[0]+" 123:1    123 2    -2 cov     -1 square 1       1 square 1   0"
+        print "Ex: "+sys.argv[0]+" 121:-2    123 4    -2 squaredist     0 exp 1       1 exp -1.6   0"
+        print "Ex: "+sys.argv[0]+" /data/icml07data/mnist_basic/plearn/mnist_basic2_train.pmat    125 400    -2 squaredist     0 exp -1       1 exp -1   0"
         raise
     # sys.exit()
 
@@ -422,6 +432,7 @@
                   offdiag_weight = offdiag_weight,
                   offdiag_nonlinearity = offdiag_nonlinearity, 
                   offdiag_premul = offdiag_premul,
+                  force_zero_mean = force_zero_mean,
                   lr=0.01, nsteps=1, optimizer_nsteps=10)
     
 

Modified: trunk/scripts/EXPERIMENTAL/linearfilters.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/linearfilters.py	2008-06-02 21:23:30 UTC (rev 9083)
+++ trunk/scripts/EXPERIMENTAL/linearfilters.py	2008-06-02 21:26:19 UTC (rev 9084)
@@ -9,7 +9,12 @@
 #server_command = 'plearn_exp server'
 #serv = launch_plearn_server(command = server_command)
 
-def computeAndShowFilters(datapmatfile, img_height, img_width, filtertype, lambd, nu):
+def computeAndShowFilters(datapmatfile, img_height, img_width, filtertype='PCA', lambd=1e-6, nu=0):
+    """
+    Input is considered to be the first img_height x img_width columns of datapmatfile.
+    Filtertype can be 'PCA' or 'denoising'        
+    Covariance matrix will get lambd*I added to its diagonal, and its off-diagonal terms multiplied by (1-nu).
+    """
     data = load_pmat_as_array(datapmatfile)
     inputs = data[:,0:img_height*img_width]
     C = cov(inputs, rowvar=0, bias=1)
@@ -54,23 +59,6 @@
 ### main program ###
 
 if __name__ == "__main__":
-
-    try:
-        datapmatfile, img_height, img_width, filtertype, lambd, nu = sys.argv[1:]
-        img_height = int(img_height)
-        img_width = int(img_width)
-        lambd = float(lambd)
-        nu = float(nu)
-    except:
-        print "Usage: "+sys.argv[0]+" <datafile.pmat> <img_height> <img_width> <filtertype> <lambda> <nu>"
-        print """
-        Input is considered to be the first img_height x img_width columns.
-        Covariance matrix will get lambda*I added to its diagonal, and its off-diagonal terms multiplied by (1-nu).
-        Filtertype can be 'PCA' or 'denoising'        
-        """
-        raise
-    # sys.exit()
-
-    computeAndShowFilters(datapmatfile, img_height, img_width, filtertype, lambd, nu)
-
-
+    from plearn.utilities.autoscript import autoscript
+    autoscript(computeAndShowFilters, True)
+    

Modified: trunk/scripts/show_rows_as_images.py
===================================================================
--- trunk/scripts/show_rows_as_images.py	2008-06-02 21:23:30 UTC (rev 9083)
+++ trunk/scripts/show_rows_as_images.py	2008-06-02 21:26:19 UTC (rev 9084)
@@ -1,33 +1,55 @@
 #!/usr/bin/env python
 
-import sys
+# show_rows_as_images.py
+# Copyright (C) 2008 Pascal Vincent
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+
+# Author: Pascal Vincent
+
+
 from plearn.vmat.PMat import PMat
 from plearn.plotting.netplot import showRowsAsImages
 
+
+def showPMatRowsAsImages(pmatfile, imgheight, imgwidth, nrows=5, ncols=7, figtitle=""):
+    """Will open a .pmat file and consider the beginning of each row a imgheight x imgwidth imagette.
+    These images will be interactively displayed in a nrows x ncols grid of imagettes."""
+    data = PMat(pmatfile)
+    if figtitle=="":
+        figtitle = pmatfile 
+    showRowsAsImages(data, img_height=imgheight, img_width=imgwidth, nrows=nrows, ncols=ncols, figtitle=figtitle)
+
 ####################
 ### main program ###
 
-if __name__ == "__main__":
+if __name__ == '__main__':
+    from plearn.utilities.autoscript import autoscript
+    autoscript(showPMatRowsAsImages)
 
-    try:
-        datapmatfile, imgheight, imgwidth, nrows, ncols = sys.argv[1:]
-        imgheight = int(imgheight)
-        imgwidth = int(imgwidth)
-        nrows = int(nrows)
-        ncols = int(ncols)
-    except:
-        print "Usage: "+sys.argv[0]+" <datafile.pmat> <imgheight> <imgwidth> <nrows> <ncols>"
-        print """
-        Will load a pmat in memory and consider the beginning of each row a imgheight x imgwidth imagette.
-        These will be interactively displayed in a nrows x ncols grid of imagettes.
-        """
-        print "Ex: "+sys.argv[0]+" /home/fringant2/lisa/data/faces/olivetti/faces.pmat 64 64  5 7"
-        print "Ex: "+sys.argv[0]+" /home/fringant2/lisa/data/icml07data/mnist_basic/plearn/mnist_basic2_train.pmat 28 28 5 7"
-        raise
-    # sys.exit()
-
-    data = PMat(datapmatfile)
-    showRowsAsImages(data, img_height=imgheight, img_width=imgwidth, nrows=nrows, ncols=ncols, figtitle=datapmatfile)
-
-
-



From plearner at mail.berlios.de  Mon Jun  2 23:29:40 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Mon, 2 Jun 2008 23:29:40 +0200
Subject: [Plearn-commits] r9085 - trunk/python_modules/plearn/utilities
Message-ID: <200806022129.m52LTepA017067@sheep.berlios.de>

Author: plearner
Date: 2008-06-02 23:29:40 +0200 (Mon, 02 Jun 2008)
New Revision: 9085

Modified:
   trunk/python_modules/plearn/utilities/autoscript.py
Log:
Changed text message.


Modified: trunk/python_modules/plearn/utilities/autoscript.py
===================================================================
--- trunk/python_modules/plearn/utilities/autoscript.py	2008-06-02 21:26:19 UTC (rev 9084)
+++ trunk/python_modules/plearn/utilities/autoscript.py	2008-06-02 21:29:40 UTC (rev 9085)
@@ -100,7 +100,7 @@
     pos_of_first_default = len(all_argnames)-len(default_values)
     for argname in all_argnames[len(args):pos_of_first_default]:
         if argname not in kwargs:
-            raise AutoscriptError("Missing mandatory argument: "+argname+". You must specify it.")
+            raise AutoscriptError("Missing mandatory argument: "+argname)
 
     # check if all specified kwargs are valid argnames
     invalid_names = [ argname for argname in kwargs.keys() if argname not in all_argnames ]



From plearner at mail.berlios.de  Mon Jun  2 23:33:04 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Mon, 2 Jun 2008 23:33:04 +0200
Subject: [Plearn-commits] r9086 - trunk/python_modules/plearn/utilities
Message-ID: <200806022133.m52LX4xt017911@sheep.berlios.de>

Author: plearner
Date: 2008-06-02 23:33:04 +0200 (Mon, 02 Jun 2008)
New Revision: 9086

Modified:
   trunk/python_modules/plearn/utilities/autoscript.py
Log:
Another cosmetic fix


Modified: trunk/python_modules/plearn/utilities/autoscript.py
===================================================================
--- trunk/python_modules/plearn/utilities/autoscript.py	2008-06-02 21:29:40 UTC (rev 9085)
+++ trunk/python_modules/plearn/utilities/autoscript.py	2008-06-02 21:33:04 UTC (rev 9086)
@@ -127,9 +127,9 @@
 
     for argname in all_argnames:
         if argname in kwargs:
-            print "# "+argname+" = "+repr(kwargs[argname])
+            print "# "+argname+"="+repr(kwargs[argname])
         else:
-            print "# "+argname+" = "+repr(defaults[argname])+ "   (default value)"            
+            print "# "+argname+"="+repr(defaults[argname])+ "   (default value)"            
 
 def eval_str_argument_values(kwargs, all_argnames, default_values):
     n = len(default_values)



From plearner at mail.berlios.de  Tue Jun  3 19:04:03 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Tue, 3 Jun 2008 19:04:03 +0200
Subject: [Plearn-commits] r9087 - trunk/python_modules/plearn/utilities
Message-ID: <200806031704.m53H436o028986@sheep.berlios.de>

Author: plearner
Date: 2008-06-03 19:04:03 +0200 (Tue, 03 Jun 2008)
New Revision: 9087

Modified:
   trunk/python_modules/plearn/utilities/autoscript.py
Log:
Now properly fetching all relevant docstrings.


Modified: trunk/python_modules/plearn/utilities/autoscript.py
===================================================================
--- trunk/python_modules/plearn/utilities/autoscript.py	2008-06-02 21:33:04 UTC (rev 9086)
+++ trunk/python_modules/plearn/utilities/autoscript.py	2008-06-03 17:04:03 UTC (rev 9087)
@@ -72,7 +72,7 @@
         prefix = args
         matches = [ arg for arg in all_argnames if arg.startswith(prefix) ]
         if len(matches)==0:
-            raise AutoscriptError("No matching argument name completion for "+prefix+"... in"+str(all_argnames))  
+            raise AutoscriptError("No matching argument name completion for "+prefix+"... in "+str(all_argnames))  
         elif len(matches)>1:
             raise AutoscriptError("Ambiguity in completion of "+prefix+"... Possible names are: "+str(matches))
         return matches[0]
@@ -181,16 +181,28 @@
     if not callable(callme):
         raise ValueError("First argument to autoscript must be callable: either a function or a class with an __init__ method or a callable instance""")
 
+    doctext=""
+
     if type(callme) is types.FunctionType:
         all_argnames, varargs, varkw, default_values = inspect.getargspec(callme)
-
+        if callme.__doc__ is not None:
+            doctext = callme.__doc__
+        
     elif type(callme) is types.ClassType:
         all_argnames, varargs, varkw, default_values = inspect.getargspec(callme.__init__)
         all_argnames = all_argnames[1:] # skip self
-
+        if callme.__doc__ is not None:
+            doctext = callme.__doc__
+        if callme.__init__.__doc__ is not None:
+            doctext = doctext+"\n"+callme.__init__.__doc__
+            
     elif type(callme) is types.InstanceType:
         all_argnames, varargs, varkw, default_values = inspect.getargspec(callme.__call__)
         all_argnames = all_argnames[1:] # skip self
+        #if callme.__doc__ is not None:
+        #    doctext = callme.__doc__
+        if callme.__call__.__doc__ is not None:
+            doctext = doctext+"\n"+callme.__call__.__doc__
 
     if default_values is None:
         default_values = []
@@ -202,8 +214,7 @@
         print "# Help on "+scriptname
         print prefix_lines("# ",helptext)
         print "# "+usage_text(scriptname, all_argnames, default_values)
-        if callme.__doc__ is not None:
-            print prefix_lines("# ",callme.__doc__)
+        print prefix_lines("# ",doctext)
         print "#"*80
         print
         sys.exit()
@@ -235,4 +246,4 @@
         sys.exit()
 
     # Call the callable
-    callme(**kwargs)
+    return callme(**kwargs)



From plearner at mail.berlios.de  Tue Jun  3 22:55:15 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Tue, 3 Jun 2008 22:55:15 +0200
Subject: [Plearn-commits] r9088 - trunk/plearn/base
Message-ID: <200806032055.m53KtFIk005759@sheep.berlios.de>

Author: plearner
Date: 2008-06-03 22:55:15 +0200 (Tue, 03 Jun 2008)
New Revision: 9088

Added:
   trunk/plearn/base/Callback.h
Log:
Virtual base class for callbacks


Added: trunk/plearn/base/Callback.h
===================================================================
--- trunk/plearn/base/Callback.h	2008-06-03 17:04:03 UTC (rev 9087)
+++ trunk/plearn/base/Callback.h	2008-06-03 20:55:15 UTC (rev 9088)
@@ -0,0 +1,70 @@
+// -*- C++ -*-
+
+// Callback.h
+//
+// Copyright (C) 2008 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file Callback.h */
+
+
+#ifndef Callback_INC
+#define Callback_INC
+
+// Put includes here
+
+namespace PLearn {
+
+//! This is a virtual base class that contains a single 
+//! abstract method callback()
+class Callback
+{
+    virtual void callback() = 0;
+};
+
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From plearner at mail.berlios.de  Tue Jun  3 22:56:56 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Tue, 3 Jun 2008 22:56:56 +0200
Subject: [Plearn-commits] r9089 - trunk/plearn/math
Message-ID: <200806032056.m53Kuu71005845@sheep.berlios.de>

Author: plearner
Date: 2008-06-03 22:56:55 +0200 (Tue, 03 Jun 2008)
New Revision: 9089

Modified:
   trunk/plearn/math/TMat_maths_impl.h
Log:
Added fillDiagonal funcitons


Modified: trunk/plearn/math/TMat_maths_impl.h
===================================================================
--- trunk/plearn/math/TMat_maths_impl.h	2008-06-03 20:55:15 UTC (rev 9088)
+++ trunk/plearn/math/TMat_maths_impl.h	2008-06-03 20:56:55 UTC (rev 9089)
@@ -4765,8 +4765,27 @@
     for (int i=0;i<le;i++,d+=mat.mod()+1,l++) *d += *l;
 }
 
+//! Fill diagonal with the specified value
+template<class T>
+void fillDiagonal(const TMat<T>& mat, T val)
+{
+    int l=mat.length();
+    for (int i=0;i<l;i++)
+        mat(i,i) = val;
+}
 
+//! Fill diagonal with the specified vector
 template<class T>
+void fillDiagonal(const TMat<T>& mat, const TVec<T>& v)
+{
+    int l=mat.length();
+    for (int i=0;i<l;i++)
+        mat(i,i) = v[i];
+}
+
+
+//! Copy diagonal of mat in d (which must have correct size)
+template<class T>
 void diag(const TMat<T>& mat, const TVec<T>& d)
 {
     T* d_ = d.data();



From plearner at mail.berlios.de  Tue Jun  3 22:58:06 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Tue, 3 Jun 2008 22:58:06 +0200
Subject: [Plearn-commits] r9090 - trunk/plearn/var/EXPERIMENTAL
Message-ID: <200806032058.m53Kw6Ho005987@sheep.berlios.de>

Author: plearner
Date: 2008-06-03 22:58:06 +0200 (Tue, 03 Jun 2008)
New Revision: 9090

Added:
   trunk/plearn/var/EXPERIMENTAL/ConstrainVariable.cc
   trunk/plearn/var/EXPERIMENTAL/ConstrainVariable.h
Modified:
   trunk/plearn/var/EXPERIMENTAL/ConstrainedSourceVariable.cc
   trunk/plearn/var/EXPERIMENTAL/ConstrainedSourceVariable.h
Log:
Experimental variables that ensure constraints.


Added: trunk/plearn/var/EXPERIMENTAL/ConstrainVariable.cc
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/ConstrainVariable.cc	2008-06-03 20:56:55 UTC (rev 9089)
+++ trunk/plearn/var/EXPERIMENTAL/ConstrainVariable.cc	2008-06-03 20:58:06 UTC (rev 9090)
@@ -0,0 +1,208 @@
+// -*- C++ -*-
+
+// ConstrainVariable.cc
+//
+// Copyright (C) 2008 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file ConstrainVariable.cc */
+
+#include "ConstrainVariable.h"
+// #include "Var_operators.h"
+//#include "Var_utils.h"
+
+namespace PLearn {
+using namespace std;
+
+
+/** ConstrainVariable **/
+
+PLEARN_IMPLEMENT_OBJECT(
+        ConstrainVariable,
+        "Correctly enforces the box constraint defined by min_value and max_value on each element.",
+        "Also makes sure the sum of each row does not exceed max_rowsum.\n"
+        "If a row sum exceeds max_rowsum, then all its elements are scaled by max_rowsum/current_row_sum.\n"
+        "BEWARE: the gradient backprop depends on the sign of the gradient received.\n"
+        "it is intentioally *not* symmetric w.r.t. that sign when stuck a box constraint.\n"
+        "It is designed to work correctly if the sign of the gradient received indicates the direction \n"
+        "in which the downstream variables would like this variable's output to move. \n"
+        "This means for ex. that if you are doing gradient descent, then the cost should have been\n"
+        "given a negative value (ex: -1) in its 'gradient'"
+);
+
+////////////////////
+// ConstrainVariable //
+////////////////////
+
+ConstrainVariable::ConstrainVariable():
+    max_rowsum(FLT_MAX)
+{}
+
+ConstrainVariable::ConstrainVariable(Variable* input, bool call_build_):
+    inherited(input, min(input->length(), input->width()), 1, call_build_),
+    max_rowsum(FLT_MAX)
+{
+    if (call_build_)
+        build_();
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void Variable::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "max_rowsum", &ConstrainVariable::max_rowsum, OptionBase::buildoption, 
+                  "maximum value the sum of elements in a row is allowed to reach\n");
+
+    inherited::declareOptions(ol);
+}
+
+///////////
+// build //
+///////////
+void ConstrainVariable::build() {
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void ConstrainVariable::build_() {
+    // Nothing to do here.
+}
+
+///////////////////
+// recomputeSize //
+///////////////////
+void ConstrainVariable::recomputeSize(int& l, int& w) const
+{
+    if (input) 
+    {
+        l = input->length();
+        w = input->width();
+    } 
+    else
+        l = w = 0;
+}
+
+
+void ConstrainVariable::fprop()
+{
+    Mat U = input->matValue;
+    int l = U.length();
+    int w = U.width();
+    for(int i=0; i<l; i++)
+    {
+        real* ui = U[i];
+        real* vi = matValue[i];
+        real tot = 0;
+        for(int j=0; j<w; j++)
+        {
+            real x = box_constrain(ui[j]);
+            tot += x;
+            vi[j] = x;
+        }
+        if(tot>max_rowsum)
+        {
+            real coef = max_rowsum/tot;
+            for(int j=0; j<w; j++)
+                vi[j] *= coef;
+        }
+    }
+}
+
+void ConstrainVariable::bprop()
+{
+    Mat U = input->matValue;
+    Mat Ug = input->matGradient;
+    int l = U.length();
+    int w = U.width();
+    for(int i=0; i<l; i++)
+    {
+        real* ui = U[i];
+        real* ugi = Ug[i];
+        real* vgi = matGradient[i];
+
+        real tot = 0;
+        for(int j=0; j<w; j++)
+            tot += box_constrain(ui[j]);
+
+        real coef = 1;
+        if(tot>max_rowsum)
+            coef = max_rowsum/tot;
+
+        for(int j=0; j<w; j++)
+        {
+            real v = ui[j];
+            real g = vgi[j];                
+            if((g<0 && v>min_value) || (g>0 && v<max_value))
+                ugi[j] += g*coef;
+        }
+    }
+
+}
+
+
+void ConstrainVariable::bbprop()
+{
+    PLERROR("bbprop not implemented for this variable");
+}
+
+
+void ConstrainVariable::symbolicBprop()
+{
+    PLERROR("symbolic bprop not yet implemented for this variable");
+}
+
+
+void ConstrainVariable::rfprop()
+{
+    PLERROR("rfprop no yet implemented for this vairable");
+}
+
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/var/EXPERIMENTAL/ConstrainVariable.h
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/ConstrainVariable.h	2008-06-03 20:56:55 UTC (rev 9089)
+++ trunk/plearn/var/EXPERIMENTAL/ConstrainVariable.h	2008-06-03 20:58:06 UTC (rev 9090)
@@ -0,0 +1,101 @@
+// -*- C++ -*-
+
+// ConstrainVariable.h
+//
+// Copyright (C) 2008 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file ConstrainVariable.h */
+
+#ifndef ConstrainVariable_INC
+#define ConstrainVariable_INC
+
+#include <plearn/var/UnaryVariable.h>
+
+namespace PLearn {
+using namespace std;
+
+
+class ConstrainVariable: public UnaryVariable
+{
+    typedef UnaryVariable inherited;
+
+public:
+    double max_rowsum;
+
+    //! Default constructor.
+    ConstrainVariable();
+
+    ConstrainVariable(Variable* input, bool call_build_ = true);
+
+    PLEARN_DECLARE_OBJECT(ConstrainVariable);
+
+    virtual void build();
+
+    virtual void recomputeSize(int& l, int& w) const;
+    virtual void fprop();
+    virtual void bprop();
+    //!  here don't approximate, do d2C/dx^2 = 4 x^2 d2C/dy^2 + 2 dC/dy 
+    virtual void bbprop();
+    virtual void symbolicBprop();
+    virtual void rfprop();
+
+    inline real box_constrain(real x) const
+    { return x<min_value ?min_value :(x>max_value ?max_value :x); }
+
+private:
+
+    void build_();
+
+};
+
+DECLARE_OBJECT_PTR(ConstrainVariable);
+
+inline Var diag(Var v)
+{ return new ConstrainVariable(v); }
+
+} // end of namespace PLearn
+
+#endif 
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Modified: trunk/plearn/var/EXPERIMENTAL/ConstrainedSourceVariable.cc
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/ConstrainedSourceVariable.cc	2008-06-03 20:56:55 UTC (rev 9089)
+++ trunk/plearn/var/EXPERIMENTAL/ConstrainedSourceVariable.cc	2008-06-03 20:58:06 UTC (rev 9090)
@@ -49,17 +49,21 @@
     ConstrainedSourceVariable,
     "SourceVariable that after each update, modifies values as needed to satisfy simple constraints",
     "The currently supported constraint is rows having norm 1.\n"
-    "i.e. after each update rows are divided by their norm.\n");
+    "i.e. after each update rows are divided by their norm (L2 or L1).\n");
 
 
 void ConstrainedSourceVariable::satisfyConstraints()
 {
     switch(constraint_mode)
     {
-    case 0:
+    case 2:
         for(int i=0; i<matValue.length(); i++)
             normalize(matValue(i), 2);
         break;
+    case 1:
+        for(int i=0; i<matValue.length(); i++)
+            normalize(matValue(i), 1);
+        break;
     default:
         PLERROR("Invalid constraint_mode %d",constraint_mode);
     }
@@ -71,7 +75,8 @@
     declareOption(
         ol, "constraint_mode", &ConstrainedSourceVariable::constraint_mode, OptionBase::buildoption,
         "The constraint_mode: \n"
-        "0: divide each row by its L2 norm after each update");
+        "2: divide each row by its L2 norm after each update\n"
+        "1: divide each row by its L1 norm after each update");
     inherited::declareOptions(ol);
 }
 

Modified: trunk/plearn/var/EXPERIMENTAL/ConstrainedSourceVariable.h
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/ConstrainedSourceVariable.h	2008-06-03 20:56:55 UTC (rev 9089)
+++ trunk/plearn/var/EXPERIMENTAL/ConstrainedSourceVariable.h	2008-06-03 20:58:06 UTC (rev 9090)
@@ -64,20 +64,20 @@
     //!  Default constructor for persistence
     ConstrainedSourceVariable()
         :SourceVariable(), 
-         constraint_mode(0)
+         constraint_mode(2)
     {}
 
-    ConstrainedSourceVariable(int thelength, int thewidth, int the_constraint_mode=0, bool call_build_ = true)
+    ConstrainedSourceVariable(int thelength, int thewidth, int the_constraint_mode=2, bool call_build_ = true)
         :SourceVariable(thelength, thewidth, call_build_), 
          constraint_mode(the_constraint_mode)
     {}
 
-    ConstrainedSourceVariable(const Vec& v, bool vertical=true, int the_constraint_mode=0, bool call_build_ = true)
+    ConstrainedSourceVariable(const Vec& v, bool vertical=true, int the_constraint_mode=2, bool call_build_ = true)
         :SourceVariable(v, vertical, call_build_), 
          constraint_mode(the_constraint_mode)
     {}
 
-    ConstrainedSourceVariable(const Mat& m, int the_constraint_mode=0, bool call_build_ = true)
+    ConstrainedSourceVariable(const Mat& m, int the_constraint_mode=2, bool call_build_ = true)
         :SourceVariable(m, call_build_), 
          constraint_mode(the_constraint_mode)
     {}



From plearner at mail.berlios.de  Tue Jun  3 23:01:12 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Tue, 3 Jun 2008 23:01:12 +0200
Subject: [Plearn-commits] r9091 - trunk/python_modules/plearn/utilities
Message-ID: <200806032101.m53L1Cnd007117@sheep.berlios.de>

Author: plearner
Date: 2008-06-03 23:01:11 +0200 (Tue, 03 Jun 2008)
New Revision: 9091

Modified:
   trunk/python_modules/plearn/utilities/autoscript.py
Log:
Irrelevant cosmetic change.


Modified: trunk/python_modules/plearn/utilities/autoscript.py
===================================================================
--- trunk/python_modules/plearn/utilities/autoscript.py	2008-06-03 20:58:06 UTC (rev 9090)
+++ trunk/python_modules/plearn/utilities/autoscript.py	2008-06-03 21:01:11 UTC (rev 9091)
@@ -123,20 +123,20 @@
 
 def print_call_arguments(kwargs, all_argnames, default_values):
     n = len(default_values)
-    defaults = dict(zip(all_argnames[-n:], default_values))
+    defaults_dict = dict(zip(all_argnames[-n:], default_values))
 
     for argname in all_argnames:
         if argname in kwargs:
             print "# "+argname+"="+repr(kwargs[argname])
         else:
-            print "# "+argname+"="+repr(defaults[argname])+ "   (default value)"            
+            print "# "+argname+"="+repr(defaults_dict[argname])+ "   (default value)"            
 
 def eval_str_argument_values(kwargs, all_argnames, default_values):
     n = len(default_values)
-    defaults = dict(zip(all_argnames[-n:], default_values))
+    defaults_dict = dict(zip(all_argnames[-n:], default_values))
     for key, val in kwargs.items():        
         if type(val) is str and val!='':
-            if ((key not in defaults) or (type(defaults[key]) is not str) or val[0] in ("'",'"')):
+            if ((key not in defaults_dict) or (type(defaults_dict[key]) is not str) or val[0] in ("'",'"')):
                 try:
                     kwargs[key] = eval(val)
                 except:



From plearner at mail.berlios.de  Tue Jun  3 23:02:00 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Tue, 3 Jun 2008 23:02:00 +0200
Subject: [Plearn-commits] r9092 - in trunk/scripts: . EXPERIMENTAL
Message-ID: <200806032102.m53L208f007386@sheep.berlios.de>

Author: plearner
Date: 2008-06-03 23:02:00 +0200 (Tue, 03 Jun 2008)
New Revision: 9092

Modified:
   trunk/scripts/EXPERIMENTAL/dcaexperiment.py
   trunk/scripts/show_rows_as_images.py
Log:
Now use autoscript


Modified: trunk/scripts/EXPERIMENTAL/dcaexperiment.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/dcaexperiment.py	2008-06-03 21:01:11 UTC (rev 9091)
+++ trunk/scripts/EXPERIMENTAL/dcaexperiment.py	2008-06-03 21:02:00 UTC (rev 9092)
@@ -30,9 +30,10 @@
 class DCAExperiment:
 
     def __init__(self,
-                 X,
+                 data_set="123:1",
                  seed=1827,
                  ncomponents=2,
+                 nonlinearity="none",
                  constrain_norm_type=-1,
                  cov_transformation_type="cov",
                  diag_weight = -1.0,
@@ -46,7 +47,26 @@
                  epsilon=1e-6, nu=0,
                  img_height=None,
                  img_width=None):
+        """
+        dataset can be either a .pmat or data_seed:ngaussians
 
+        nonlinearity can be: none square abs sqrt sqrtabs exp tanh sigmoid
+        
+        constrain_norm_type controls how to constrain the norms of rows of W:
+          -1: L1 constrained source;
+          -2: L2 constrained source;
+          -3:explicit normalization;
+          >=0:ordinary weight decay
+          
+        cov_transformation_type controls the kind of transformation to apply to covariance matrix
+          cov: no transformation (keep covariance)
+          corr: transform into correlations, but keeping variances on the diagonal.
+          squaredist: do a 'squared distance kernel' kind of transformation.
+          sincov: uses sin of gangle instead of cos of angle
+        """
+        
+        X = getDataSet(data_set)
+
         self.nsteps = nsteps
 
         #some calculations for plotting
@@ -77,6 +97,7 @@
         dcaspec = pl.DiverseComponentAnalysis(
             seed = seed,
             ncomponents = ncomponents,
+            nonlinearity=nonlinearity,
             constrain_norm_type=constrain_norm_type,
             cov_transformation_type=cov_transformation_type,
             diag_weight = diag_weight,
@@ -113,8 +134,9 @@
         self.filterfig = 3
         self.Wfig = 4
         self.Cytfig = 5
-        self.Cxfig = 6
+        self.Cxfig = 8
         self.Cythist = 7
+        self.trdatafig = 6
 
         self.draw()
         
@@ -130,10 +152,12 @@
         connect('key_press_event', self.keyPressed)
         #figure(self.Wfig)
         figure(self.Cytfig)
+        figure(self.trdatafig)
         #figure(self.Cxfig)
         #figure(self.Cythist)
 
         # start interactive loop
+        print "Starting interactive loop"
         show()
 
 
@@ -205,6 +229,21 @@
         self.dca.changeOptions({"optimizer.learning_rate":lr})
         self.draw()
 
+    def showRepresentation(self):
+        print "Plotting representation"
+        figure(self.trdatafig)
+        clf()
+        trdata = self.dca.getVarValue('trdata')
+        print "Dimensions of trdata: ",trdata.shape
+        trdata = trdata[0:50,:]
+        print "Looking only at first ",trdata.shape
+        # print trdata[3]
+        # ctrdata = self.dca.getVarValue('ctrdata')        
+        imshow(trdata, interpolation="nearest", cmap = cm.gray)
+        colorbar()
+        title("Representation")
+        draw()
+        
     def showFilters(self):
         W = self.dca.getVarValue('W')
 
@@ -228,7 +267,7 @@
         clf()
         subplot(1,3,1)
         title('Cyt')
-        imshow(Cyt, interpolation="nearest", cmap = cm.jet)
+        imshow(Cyt, interpolation="nearest", cmap = cm.gray)
         colorbar()
 
         subplot(1,3,2)
@@ -296,6 +335,8 @@
             self.changeSteps()
         elif char == 'w':
             self.showFilters()
+        elif char == 'r':
+            self.showRepresentation()            
         elif char == 's':
             self.save()
         elif char == '':
@@ -310,7 +351,8 @@
             *   ....           ...
             *   '9'     : does 90*nsteps training steps
             *   'o'     : to change optimizaiton nsteps and lr
-            *   'f'     : interactive show filters
+            *   'w'     : show weights (filters)
+            *   'r'     : show representation (transformed data) 
             *   's'     : save learner to file
             * Close window to stop.
             *******************************************************
@@ -339,13 +381,6 @@
     #X = dot(X,W)+b.T
     return X
 
-def loadInputs(datapmatfile, inputsize=None):
-    data = load_pmat_as_array(datapmatfile)
-    if inputsize is None:
-        inputsize = len(data[0])-1
-    X = data[:,0:inputsize]
-    return X
-
 def getDataSet(dataset):
     """dataset is a string that specifies either a .pmat or is of the form data_seed:ngaussians"""
     data_seed = None
@@ -358,10 +393,6 @@
 
     if data_seed is None:
         X = PMat(dataset)
-        print "** flushing"
-        X.flush()
-        print "** flushing DONE"
-        # X = loadInputs(dataset)
     else:
         random.seed(data_seed)
         if ngaussians==0:
@@ -381,73 +412,72 @@
 ####################
 ### main program ###
 
-if __name__ == "__main__":
+# if __name__ == "__main__":
 
-    try:
-        dataset, learner_seed, ncomponents, constrain_norm_type, cov_transformation_type, diag_weight, diag_nonlinearity, diag_premul, offdiag_weight, offdiag_nonlinearity, offdiag_premul, force_zero_mean = sys.argv[1:]        
+#     try:
+#         dataset, learner_seed, ncomponents, constrain_norm_type, cov_transformation_type, diag_weight, diag_nonlinearity, diag_premul, offdiag_weight, offdiag_nonlinearity, offdiag_premul, force_zero_mean = sys.argv[1:]        
 
-        learner_seed = int(learner_seed)
-        ncomponents = int(ncomponents)
-        constrain_norm_type = float(constrain_norm_type)
-        diag_weight = float(diag_weight)
-        diag_premul = float(diag_premul)
-        offdiag_weight = float(offdiag_weight)
-        offdiag_premul = float(offdiag_premul)
-        force_zero_mean = int(force_zero_mean)
+#         learner_seed = int(learner_seed)
+#         ncomponents = int(ncomponents)
+#         constrain_norm_type = float(constrain_norm_type)
+#         diag_weight = float(diag_weight)
+#         diag_premul = float(diag_premul)
+#         offdiag_weight = float(offdiag_weight)
+#         offdiag_premul = float(offdiag_premul)
+#         force_zero_mean = int(force_zero_mean)
 
 
 
-    except:
-        print "Usage: "+sys.argv[0]+" dataset learner_seed ncomponents constrain_norm_type cov_transformation_type diag_weight diag_nonlinearity diag_premul offdiag_weight offdiag_nonlinearity offdiag_premul force_zero_mean"
-        print "  dataset can be either a .pmat or data_seed:ngaussians"
-        print """  constrain_norm_type controls how to constrain the norms of rows of W:
-        -1: L1 constrained source;
-        -2: L2 constrained source;
-        -3:explicit normalization;
-        >0:ordinary weight decay"""
-        print """  cov_transformation_type controls the kind of transformation to apply to covariance matrix
-        cov: no transformation (keep covariance)
-        corr: transform into correlations, but keeping variances on the diagonal.
-        squaredist: do a 'squared distance kernel' kind of transformation.
-        sincov: uses sin of gangle instead of cos of angle
-        """
-        print "Ex: "+sys.argv[0]+" 123:1    123 2    -2 cov     -1 square 1       1 square 1   0"
-        print "Ex: "+sys.argv[0]+" 121:-2    123 4    -2 squaredist     0 exp 1       1 exp -1.6   0"
-        print "Ex: "+sys.argv[0]+" /data/icml07data/mnist_basic/plearn/mnist_basic2_train.pmat    125 400    -2 squaredist     0 exp -1       1 exp -1   0"
-        raise
-    # sys.exit()
-
-    print "Getting data"
-    X = getDataSet(dataset)
-    print "Data OK."
-
-    DCAExperiment(X,
-                  seed=learner_seed, 
-                  ncomponents=ncomponents,
-                  constrain_norm_type=constrain_norm_type,
-                  cov_transformation_type=cov_transformation_type,
-                  diag_weight = diag_weight,
-                  diag_nonlinearity = diag_nonlinearity, 
-                  diag_premul = diag_premul,
-                  offdiag_weight = offdiag_weight,
-                  offdiag_nonlinearity = offdiag_nonlinearity, 
-                  offdiag_premul = offdiag_premul,
-                  force_zero_mean = force_zero_mean,
-                  lr=0.01, nsteps=1, optimizer_nsteps=10)
-    
-
-#     try:
-#         datapmatfile, inputsize, filtertype, param = sys.argv[1:]
-#         inputsize = int(inputsize)
-#         param = float(param)
 #     except:
-#         print "Usage: "+sys.argv[0]+" <datafile.pmat> <inputsize> <filtertype> <param>"
-#         print """
-#         For filtertype 'PCA', param is the noise added to the diagonal of the covariance matrix
-#         For filtertype 'denoising', param is the destruction proportion.
+#         print "Usage: "+sys.argv[0]+" dataset learner_seed ncomponents constrain_norm_type cov_transformation_type diag_weight diag_nonlinearity diag_premul offdiag_weight offdiag_nonlinearity offdiag_premul force_zero_mean"
+#         print "  dataset can be either a .pmat or data_seed:ngaussians"
+#         print """  constrain_norm_type controls how to constrain the norms of rows of W:
+#         -1: L1 constrained source;
+#         -2: L2 constrained source;
+#         -3:explicit normalization;
+#         >0:ordinary weight decay"""
+#         print """  cov_transformation_type controls the kind of transformation to apply to covariance matrix
+#         cov: no transformation (keep covariance)
+#         corr: transform into correlations, but keeping variances on the diagonal.
+#         squaredist: do a 'squared distance kernel' kind of transformation.
+#         sincov: uses sin of gangle instead of cos of angle
 #         """
+#         print "Ex: "+sys.argv[0]+" 123:1    123 2    -2 cov     -1 square 1       1 square 1   0"
+#         print "Ex: "+sys.argv[0]+" 121:-2    123 4    -2 squaredist     0 exp 1       1 exp -1.6   0"
+#         print "Ex: "+sys.argv[0]+" /data/icml07data/mnist_basic/plearn/mnist_basic2_train.pmat    125 400    -2 squaredist     0 exp -1       1 exp -1   0"
 #         raise
 #     # sys.exit()
 
+#     print "Getting data"
+#     X = getDataSet(dataset)
+#     print "Data OK."
 
+#     DCAExperiment(X,
+#                   seed=learner_seed, 
+#                   ncomponents=ncomponents,
+#                   constrain_norm_type=constrain_norm_type,
+#                   cov_transformation_type=cov_transformation_type,
+#                   diag_weight = diag_weight,
+#                   diag_nonlinearity = diag_nonlinearity, 
+#                   diag_premul = diag_premul,
+#                   offdiag_weight = offdiag_weight,
+#                   offdiag_nonlinearity = offdiag_nonlinearity, 
+#                   offdiag_premul = offdiag_premul,
+#                   force_zero_mean = force_zero_mean,
+#                   lr=0.01, nsteps=1, optimizer_nsteps=10)
+    
 
+if __name__ == "__main__":
+
+    from plearn.utilities.autoscript import autoscript
+
+    helptext = """
+    OLDEXAMPLE: dcaexperiment.py  123:1    123 2    -2 cov     -1 square 1       1 square 1   0"
+    OLDEXAMPLE: dcaexperiment.py 121:-2    123 4    -2 squaredist     0 exp 1       1 exp -1.6   0"
+    OLDEXAMPLE: dcaexperiment.py /data/icml07data/mnist_basic/plearn/mnist_basic2_train.pmat    125 400    -2 squaredist     0 exp -1       1 exp -1   0
+    """
+    autoscript(DCAExperiment,True,helptext=helptext)
+
+
+
+

Modified: trunk/scripts/show_rows_as_images.py
===================================================================
--- trunk/scripts/show_rows_as_images.py	2008-06-03 21:01:11 UTC (rev 9091)
+++ trunk/scripts/show_rows_as_images.py	2008-06-03 21:02:00 UTC (rev 9092)
@@ -51,5 +51,5 @@
 
 if __name__ == '__main__':
     from plearn.utilities.autoscript import autoscript
-    autoscript(showPMatRowsAsImages)
+    autoscript(showPMatRowsAsImages, True)
 



From plearner at mail.berlios.de  Tue Jun  3 23:05:25 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Tue, 3 Jun 2008 23:05:25 +0200
Subject: [Plearn-commits] r9093 - trunk/plearn/var
Message-ID: <200806032105.m53L5PCe007747@sheep.berlios.de>

Author: plearner
Date: 2008-06-03 23:05:24 +0200 (Tue, 03 Jun 2008)
New Revision: 9093

Modified:
   trunk/plearn/var/Func.cc
   trunk/plearn/var/Func.h
Log:
Added optional argument to verifygradient that allows to chose which component of the output receives a non-zero gradient
(for non-scalar output)


Modified: trunk/plearn/var/Func.cc
===================================================================
--- trunk/plearn/var/Func.cc	2008-06-03 21:02:00 UTC (rev 9092)
+++ trunk/plearn/var/Func.cc	2008-06-03 21:05:24 UTC (rev 9093)
@@ -583,18 +583,18 @@
 ////////////////////
 // verifyGradient //
 ////////////////////
-void Function::verifyGradient(const Vec& input, real step)
+void Function::verifyGradient(const Vec& input, real step, int which_component)
 {
     if(outputsize!=1)
         PLWARNING("In Function::verifyGradient(...) Will verify gradient only for the first output");
     Vec output(outputsize);
     Vec output_gradient(outputsize);
-    output_gradient[0]=1.0;
+    output_gradient[which_component]=1.0;
     Vec gradient(inputsize);
     fbprop(input, output, gradient,output_gradient);
     perr << "** Verifying gradient computation **" << endl;
     perr << "Input:                " << input << endl;
-    perr << "Output:               " << output[0] << endl;
+    perr << "Output["<<which_component<<"]:            " << output[which_component] << endl;
     perr << "Computed  gradient:   " << gradient << endl;
     //displayFunction(this,true);
     // Now computing the gradient by finite difference
@@ -606,10 +606,10 @@
         real in = input[i];
         newinput[i] = in+step;
         fprop(newinput,output);
-        real out1 = output[0];
+        real out1 = output[which_component];
         newinput[i] = in-step;
         fprop(newinput,output);
-        real out2 = output[0];
+        real out2 = output[which_component];
         finitediffgradient[i] = (out1-out2)/doublestep;
         newinput[i] = input[i] = in;
     }
@@ -655,18 +655,18 @@
                                                 : acos(cos_angle) ) << endl;
 }
 
-void Function::verifyGradient(real minval, real maxval, real step)
+void Function::verifyGradient(real minval, real maxval, real step, int which_component)
 {
     Vec input(inputsize);
     fill_random_uniform(input,minval, maxval);
-    verifyGradient(input, step);
+    verifyGradient(input, step, which_component);
 }
 
-void Function::verifyGradient(real step)
+void Function::verifyGradient(real step, int which_component)
 {
     Vec input(inputsize);
     inputs >> input;
-    verifyGradient(input, step);
+    verifyGradient(input, step, which_component);
 }
 
 ////////////////////////////

Modified: trunk/plearn/var/Func.h
===================================================================
--- trunk/plearn/var/Func.h	2008-06-03 21:02:00 UTC (rev 9092)
+++ trunk/plearn/var/Func.h	2008-06-03 21:05:24 UTC (rev 9093)
@@ -184,15 +184,15 @@
     VarArray operator()(const VarArray& new_inputs) const;
 
     //!  take the values given in the in Vec
-    void verifyGradient(const Vec& in, real step=0.01);
+    void verifyGradient(const Vec& in, real step=0.01, int which_component=0);
 
     void verifyHessian(const Vec& in, real step=0.01);
 
     //!  take the values randomly between minval and maxval
-    void verifyGradient(real minval, real maxval, real step=0.01);
+    void verifyGradient(real minval, real maxval, real step=0.01,  int which_component=0);
 
     //!  take the current values of the inputs variables
-    void verifyGradient(real step=0.01);  
+    void verifyGradient(real step=0.01, int which_component=0);  
 
 /*!     Checks that the gradient computed by a bprop on the function 
   and the gradient computed by a fprop on the symbolic derivative



From plearner at mail.berlios.de  Tue Jun  3 23:08:23 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Tue, 3 Jun 2008 23:08:23 +0200
Subject: [Plearn-commits] r9094 - trunk/plearn/var
Message-ID: <200806032108.m53L8N1n007973@sheep.berlios.de>

Author: plearner
Date: 2008-06-03 23:08:22 +0200 (Tue, 03 Jun 2008)
New Revision: 9094

Modified:
   trunk/plearn/var/UnaryVariable.cc
   trunk/plearn/var/UnaryVariable.h
Log:
Added proper setParents method to UnaryVariable


Modified: trunk/plearn/var/UnaryVariable.cc
===================================================================
--- trunk/plearn/var/UnaryVariable.cc	2008-06-03 21:05:24 UTC (rev 9093)
+++ trunk/plearn/var/UnaryVariable.cc	2008-06-03 21:08:22 UTC (rev 9094)
@@ -80,6 +80,24 @@
     // Nothing to do here.
 }
 
+
+////////////////
+// setParents //
+////////////////
+void UnaryVariable::setParents(const VarArray& parents)
+{
+    if(parents.length() != 1)
+        PLERROR("In UnaryVariable::setParents  VarArray length must be 1;"
+                " you are trying to set %d parents for this BinaryVariable...", parents.length());
+
+    input= parents[0];
+  
+    // int dummy_l, dummy_w;
+    //recomputeSize(dummy_l, dummy_w);
+    sizeprop();
+}
+
+
 void UnaryVariable::declareOptions(OptionList& ol)
 {
     declareOption(ol, "input", &UnaryVariable::input, OptionBase::buildoption, 

Modified: trunk/plearn/var/UnaryVariable.h
===================================================================
--- trunk/plearn/var/UnaryVariable.h	2008-06-03 21:05:24 UTC (rev 9093)
+++ trunk/plearn/var/UnaryVariable.h	2008-06-03 21:08:22 UTC (rev 9094)
@@ -73,6 +73,8 @@
     //! Set this Variable's input (simply call build after setting the new
     //! input).
     void setInput(Var the_input);
+
+    virtual void setParents(const VarArray& parents);
   
     virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
     virtual bool markPath();



From plearner at mail.berlios.de  Tue Jun  3 23:10:28 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Tue, 3 Jun 2008 23:10:28 +0200
Subject: [Plearn-commits] r9095 - trunk/plearn/var
Message-ID: <200806032110.m53LASif008157@sheep.berlios.de>

Author: plearner
Date: 2008-06-03 23:10:27 +0200 (Tue, 03 Jun 2008)
New Revision: 9095

Modified:
   trunk/plearn/var/SourceVariable.cc
   trunk/plearn/var/SourceVariable.h
Log:
Added addtiional constructor to SourceVariable


Modified: trunk/plearn/var/SourceVariable.cc
===================================================================
--- trunk/plearn/var/SourceVariable.cc	2008-06-03 21:08:22 UTC (rev 9094)
+++ trunk/plearn/var/SourceVariable.cc	2008-06-03 21:10:27 UTC (rev 9095)
@@ -78,6 +78,21 @@
         build_();
 }
 
+SourceVariable::SourceVariable(int thelength, int thewidth, string random_type_, 
+                               real random_a_, real random_b_, bool clear_first_row_,
+                               bool call_build_): 
+    inherited(thelength, thewidth, call_build_),
+    build_length(thelength),
+    build_width(thewidth),
+    random_type(random_type_),
+    random_a(random_a_),
+    random_b(random_b_),
+    random_clear_first_row(clear_first_row_)
+{
+    if (call_build_)
+        build_();
+}
+
 SourceVariable::SourceVariable(const Vec& v, bool vertical, bool call_build_):
     inherited(vertical ?v.toMat(v.length(),1) :v.toMat(1,v.length()),
               call_build_),

Modified: trunk/plearn/var/SourceVariable.h
===================================================================
--- trunk/plearn/var/SourceVariable.h	2008-06-03 21:08:22 UTC (rev 9094)
+++ trunk/plearn/var/SourceVariable.h	2008-06-03 21:10:27 UTC (rev 9095)
@@ -79,6 +79,9 @@
     SourceVariable(int thelength, int thewidth, bool call_build_ = true);
     SourceVariable(const Vec& v, bool vertical=true, bool call_build_ = true);
     SourceVariable(const Mat& m, bool call_build_ = true);
+    SourceVariable(int thelength, int thewidth, string random_type_, 
+                   real random_a_=0, real random_b_=1, bool clear_first_row_=false,
+                   bool call_build_ = true);
 
     virtual void setParents(const VarArray& parents)
     { PLERROR("In Variable::setParents  trying to set parents of a SourceVariable..."); }



From plearner at mail.berlios.de  Tue Jun  3 23:48:14 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Tue, 3 Jun 2008 23:48:14 +0200
Subject: [Plearn-commits] r9096 - in trunk/plearn/opt/test/.pytest:
	PL_ConjGradientRosenbrock100/expected_results
	PL_ConjGradientRosenbrock2/expected_results
Message-ID: <200806032148.m53LmEe2012857@sheep.berlios.de>

Author: plearner
Date: 2008-06-03 23:48:14 +0200 (Tue, 03 Jun 2008)
New Revision: 9096

Modified:
   trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock100/expected_results/ConjRosenbrock.psave
   trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock100/expected_results/RUN.log
   trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock2/expected_results/ConjRosenbrock.psave
   trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock2/expected_results/RUN.log
Log:
Fixed pytest expected results following output format change of verifygradient


Modified: trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock100/expected_results/ConjRosenbrock.psave
===================================================================
--- trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock100/expected_results/ConjRosenbrock.psave	2008-06-03 21:10:27 UTC (rev 9095)
+++ trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock100/expected_results/ConjRosenbrock.psave	2008-06-03 21:48:14 UTC (rev 9096)
@@ -11,7 +11,8 @@
 slope_ratio = 10 ;
 minibatch_n_samples = 0 ;
 minibatch_n_line_searches = 3 ;
-nstages = 12  )
+nstages = 12 ;
+early_stop = 0  )
 ;
 D = 100 ;
 save = 1 ;

Modified: trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock100/expected_results/RUN.log
===================================================================
--- trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock100/expected_results/RUN.log	2008-06-03 21:10:27 UTC (rev 9095)
+++ trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock100/expected_results/RUN.log	2008-06-03 21:48:14 UTC (rev 9096)
@@ -1,7 +1,7 @@
 rosenbrock(0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ) = 99 
 ** Verifying gradient computation **
 Input:                0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
-Output:               99
+Output[0]:            99
 Computed  gradient:   -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 0 
 Estimated gradient:   -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 !
 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -2.00000000205591277 -!
 2.00000000205591277 -2.00000000205591277 -2.00000000205591277 !
 -2.00000
000205591277 -2.00000000205591277 -2.00000000205591277 0 
 -------------------
@@ -12,7 +12,7 @@
 angle : 0
 ** Verifying gradient computation **
 Input:                1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 
-Output:               0
+Output[0]:            0
 Computed  gradient:   0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
 Estimated gradient:   3.99968835891135422e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.9995!
 7733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733!
 679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 !
 3.999577
33679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 3.99957733679748888e-10 -1.11022243112306703e-14 
 -------------------

Modified: trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock2/expected_results/ConjRosenbrock.psave
===================================================================
--- trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock2/expected_results/ConjRosenbrock.psave	2008-06-03 21:10:27 UTC (rev 9095)
+++ trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock2/expected_results/ConjRosenbrock.psave	2008-06-03 21:48:14 UTC (rev 9096)
@@ -11,7 +11,8 @@
 slope_ratio = 10 ;
 minibatch_n_samples = 0 ;
 minibatch_n_line_searches = 3 ;
-nstages = 12  )
+nstages = 12 ;
+early_stop = 0  )
 ;
 D = 2 ;
 save = 1 ;

Modified: trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock2/expected_results/RUN.log
===================================================================
--- trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock2/expected_results/RUN.log	2008-06-03 21:10:27 UTC (rev 9095)
+++ trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock2/expected_results/RUN.log	2008-06-03 21:48:14 UTC (rev 9096)
@@ -1,7 +1,7 @@
 rosenbrock(0 0 ) = 1 
 ** Verifying gradient computation **
 Input:                0 0 
-Output:               1
+Output[0]:            1
 Computed  gradient:   -2 0 
 Estimated gradient:   -2.00000000000200018 0 
 -------------------
@@ -12,7 +12,7 @@
 angle : 0
 ** Verifying gradient computation **
 Input:                1 1 
-Output:               0
+Output[0]:            0
 Computed  gradient:   0 0 
 Estimated gradient:   3.99968835891135422e-10 -1.11022243112306703e-14 
 -------------------



From plearner at mail.berlios.de  Tue Jun  3 23:49:55 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Tue, 3 Jun 2008 23:49:55 +0200
Subject: [Plearn-commits] r9097 - trunk/commands/PLearnCommands
Message-ID: <200806032149.m53Lnt2S012950@sheep.berlios.de>

Author: plearner
Date: 2008-06-03 23:49:54 +0200 (Tue, 03 Jun 2008)
New Revision: 9097

Modified:
   trunk/commands/PLearnCommands/LearnerCommand.cc
   trunk/commands/PLearnCommands/LearnerCommand.h
Log:
Added process_dataset subcommand to learner command


Modified: trunk/commands/PLearnCommands/LearnerCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/LearnerCommand.cc	2008-06-03 21:48:14 UTC (rev 9096)
+++ trunk/commands/PLearnCommands/LearnerCommand.cc	2008-06-03 21:49:54 UTC (rev 9097)
@@ -74,6 +74,11 @@
                   "\n"
                   "learner compute_outputs <trained_learner.psave> <test_inputs.vmat> <outputs.pmat> (or 'learner co' as a shortcut)\n"
                   "\n"
+                  "learner process_dataset <trained_learner.psave> <dataset.vmat> <processed_dataset.pmat>\n"
+                  "  - process a full dataset (possibly containing input,target,weight,extra,parts). \n"
+                  "    writes processed dataset as a pmat. This calls method processDataset whose default version \n"
+                  "    uses computeOutput to process the input part, and simply passes on the other parts unchanged.\n"
+                  "    Typical usage: preprocessing data with PCA for ex. \n\n"
                   // "learner compute_costs <trained_learner.psave> <testset.vmat> <outputs.pmat> <costs.pmat>\n" 
                   "learner compute_outputs_on_1D_grid <trained_learner.psave> <gridoutputs.pmat> <xmin> <xmax> <nx> (shortcut: learner cg1)\n"
                   "  -  Computes output of learner on nx equally spaced points in range [xmin, xmax] and writes the list of (x,output)\n"
@@ -158,6 +163,18 @@
     learner->use(testinputs,testoutputs);
 }
 
+/////////////////////
+// process_dataset //
+/////////////////////
+void LearnerCommand::process_dataset(const string& trained_learner_file, const string& dataset_spec, const string& processed_dataset_pmat)
+{
+    PP<PLearner> learner =
+        (PLearner*) smartLoadObject(trained_learner_file);
+    VMat dataset = getDataSet(dataset_spec);
+    VMat processed = learner->processDataSet(dataset);
+    processed->savePMAT(processed_dataset_pmat);
+}
+
 ////////////////////////////////
 // compute_outputs_on_2D_grid //
 ////////////////////////////////
@@ -362,6 +379,13 @@
         else
             PLERROR("LearnerCommand::run you must provide 'plearn learner compute_outputs learner_spec_file trainset_spec save_learner_file'");
     }
+    else if (command=="process_dataset")
+    {
+        if (args.size()==4)
+            process_dataset(args[1],args[2],args[3]);
+        else
+            PLERROR("LearnerCommand::run you must provide: plearn learner process_dataset <trained_learner.psave> <dataset.vmat> <processed_dataset.pmat>");
+    }
     else if (command=="compute_outputs_on_1D_grid" || command=="cg1")
     {
         if(args.size()!=6)

Modified: trunk/commands/PLearnCommands/LearnerCommand.h
===================================================================
--- trunk/commands/PLearnCommands/LearnerCommand.h	2008-06-03 21:48:14 UTC (rev 9096)
+++ trunk/commands/PLearnCommands/LearnerCommand.h	2008-06-03 21:49:54 UTC (rev 9097)
@@ -69,6 +69,8 @@
     static void test(const string& trained_learner_file, const string& testset_spec, const string& stats_file, const string& outputs_file, const string& costs_file);
     static void compute_outputs(const string& trained_learner_file, const string& test_inputs_spec, const string& outputs_file);
 
+    static void process_dataset(const string& trained_learner_file, const string& dataset_spec, const string& processed_dataset_pmat);
+
     static void compute_outputs_on_1D_grid(const string& trained_learner_file, const string& grid_outputs_file, 
                                            real xmin, real xmax, int nx);
 



From plearner at mail.berlios.de  Tue Jun  3 23:51:02 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Tue, 3 Jun 2008 23:51:02 +0200
Subject: [Plearn-commits] r9098 - trunk/commands/PLearnCommands
Message-ID: <200806032151.m53Lp2Ao013405@sheep.berlios.de>

Author: plearner
Date: 2008-06-03 23:51:02 +0200 (Tue, 03 Jun 2008)
New Revision: 9098

Added:
   trunk/commands/PLearnCommands/ExtractOptionCommand.cc
   trunk/commands/PLearnCommands/ExtractOptionCommand.h
Log:
Added command to extrat an option from a saved .psave object.
In particular it can extract matrix or vector and write it to a .pmat


Added: trunk/commands/PLearnCommands/ExtractOptionCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/ExtractOptionCommand.cc	2008-06-03 21:49:54 UTC (rev 9097)
+++ trunk/commands/PLearnCommands/ExtractOptionCommand.cc	2008-06-03 21:51:02 UTC (rev 9098)
@@ -0,0 +1,132 @@
+// -*- C++ -*-
+
+// ExtractOptionCommand.cc
+//
+// Copyright (C) 2008 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file ExtractOptionCommand.cc */
+
+
+#include "ExtractOptionCommand.h"
+#include <plearn/io/openFile.h>
+#include <plearn/io/openString.h>
+#include <plearn/vmat/FileVMatrix.h>
+#include <plearn/math/TMat_maths.h>
+
+namespace PLearn {
+using namespace std;
+
+//! This allows to register the 'ExtractOptionCommand' command in the command registry
+PLearnCommandRegistry ExtractOptionCommand::reg_(new ExtractOptionCommand);
+
+ExtractOptionCommand::ExtractOptionCommand()
+    : PLearnCommand(
+        "extract_option",
+        "Extracts an option from a saved plearn object (.psave) and saves it to its own file.",
+        "   extract_option <objfile.psave> <optionname> <optionfile.psave>\n"
+        "OR extract_option <objfile.psave> <optionname> <optionfile.pmat> <transpose> \n"
+        "The first form will output the option serialized in plearn_ascii format\n"
+        "The second form is available only for Mat or Vec options, and will save it as a .pmat file.\n"
+        "For Mat, if transpose is 0 then the matrix won't be transposed. If it's 1 it will be transposed.\n"
+        "For Vec, if transpose is 0 then it will be saved as a row matrix. If it's 1, i will be saved as a column matrix.\n"
+        )
+{}
+
+//! The actual implementation of the 'ExtractOptionCommand' command
+void ExtractOptionCommand::run(const vector<string>& args)
+{
+    if(args.size()==3)
+    {
+        string objfile = args[0];
+        string optionname = args[1];
+        string optionfile = args[2];
+        PP<Object> obj = loadObject(objfile);
+        PStream out = openFile(optionfile, PStream::plearn_ascii, "w");
+        obj->writeOptionVal(out, optionname);
+        out = 0;
+        perr << "Option " << optionname << " has been written to file " << optionfile << endl;
+    }
+    else if(args.size()==4)
+    {
+        string objfile = args[0];
+        string optionname = args[1];
+        string optionfile = args[2];
+        int do_transpose = toint(args[3]);
+        PP<Object> obj = loadObject(objfile);
+        string optionval = obj->getOption(optionname);
+        bool ismat = false;
+        Mat m;
+        try
+        {
+            PStream in = openString(optionval, PStream::plearn_ascii);
+            in >> m;
+            perr << "Extracted a " << m.length() << " x " << m.width() << " matrix" << endl;
+            ismat = true;
+        }
+        catch(const PLearnError& e)
+        { ismat = false; }
+
+        if(!ismat)
+        {
+            Vec v;
+            PStream in = openString(optionval, PStream::plearn_ascii);
+            in >> v;
+            perr << "Extracted a vector of length " << v.length() << endl;
+            m = v.toMat(1,v.length());
+        }
+
+        if(do_transpose==1)
+            m = transpose(m);
+        FileVMatrix vmat(optionfile, m.length(), m.width());
+        vmat.putMat(0, 0, m);
+        vmat.flush();
+        perr << "Option " << optionname << " has been written to file " << optionfile << endl;
+    }
+    else
+        PLERROR("Wrong number of argumens, please consult help");    
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/commands/PLearnCommands/ExtractOptionCommand.h
===================================================================
--- trunk/commands/PLearnCommands/ExtractOptionCommand.h	2008-06-03 21:49:54 UTC (rev 9097)
+++ trunk/commands/PLearnCommands/ExtractOptionCommand.h	2008-06-03 21:51:02 UTC (rev 9098)
@@ -0,0 +1,86 @@
+// -*- C++ -*-
+
+// ExtractOptionCommand.h
+//
+// Copyright (C) 2008 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file ExtractOptionCommand.h */
+
+
+#ifndef ExtractOptionCommand_INC
+#define ExtractOptionCommand_INC
+
+#include <commands/PLearnCommands/PLearnCommand.h>
+#include <commands/PLearnCommands/PLearnCommandRegistry.h>
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class ExtractOptionCommand : public PLearnCommand
+{
+    typedef PLearnCommand inherited;
+
+public:
+    ExtractOptionCommand();
+    virtual void run(const std::vector<std::string>& args);
+
+protected:
+    static PLearnCommandRegistry reg_;
+};
+
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From plearner at mail.berlios.de  Tue Jun  3 23:58:54 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Tue, 3 Jun 2008 23:58:54 +0200
Subject: [Plearn-commits] r9099 - trunk/plearn/var
Message-ID: <200806032158.m53Lwswu014789@sheep.berlios.de>

Author: plearner
Date: 2008-06-03 23:58:54 +0200 (Tue, 03 Jun 2008)
New Revision: 9099

Modified:
   trunk/plearn/var/NegateElementsVariable.h
   trunk/plearn/var/TimesConstantVariable.h
   trunk/plearn/var/Var_operators.cc
   trunk/plearn/var/Var_operators.h
Log:
Moved operator definitions as inline in their respective vriable's class.
(cosmetic fix, shouldn't change anything)


Modified: trunk/plearn/var/NegateElementsVariable.h
===================================================================
--- trunk/plearn/var/NegateElementsVariable.h	2008-06-03 21:51:02 UTC (rev 9098)
+++ trunk/plearn/var/NegateElementsVariable.h	2008-06-03 21:58:54 UTC (rev 9099)
@@ -85,6 +85,10 @@
     return new NegateElementsVariable(v);
 }
 
+inline Var operator-(Var v)
+{ return new NegateElementsVariable(v); }
+
+
 } // end of namespace PLearn
 
 #endif 

Modified: trunk/plearn/var/TimesConstantVariable.h
===================================================================
--- trunk/plearn/var/TimesConstantVariable.h	2008-06-03 21:51:02 UTC (rev 9098)
+++ trunk/plearn/var/TimesConstantVariable.h	2008-06-03 21:58:54 UTC (rev 9099)
@@ -86,6 +86,21 @@
 
 DECLARE_OBJECT_PTR(TimesConstantVariable);
 
+inline Var operator*(Var v, real cte)
+{ 
+    if(cte==1)
+        return v;
+    else
+        return new TimesConstantVariable(v,cte); 
+}
+
+inline Var operator*(real cte, Var v)
+{ return v*cte; }
+
+inline Var operator/(Var v, real cte)
+{ return v*(1.0/cte); }
+
+
 } // end of namespace PLearn
 
 #endif 

Modified: trunk/plearn/var/Var_operators.cc
===================================================================
--- trunk/plearn/var/Var_operators.cc	2008-06-03 21:51:02 UTC (rev 9098)
+++ trunk/plearn/var/Var_operators.cc	2008-06-03 21:58:54 UTC (rev 9099)
@@ -55,7 +55,6 @@
 #include "MinusColumnVariable.h"
 #include "MinusVariable.h"
 
-#include "TimesConstantVariable.h"
 #include "TimesScalarVariable.h"
 #include "TimesColumnVariable.h"
 #include "TimesRowVariable.h"
@@ -125,9 +124,6 @@
         return new MinusVariable(v1,v2);
 }
 
-Var operator-(Var v)
-{ return new NegateElementsVariable(v); }
-
 void operator-=(Var& v1, const Var& v2)
 {
     if (!v2.isNull())
@@ -143,14 +139,6 @@
 { return new PlusConstantVariable(new NegateElementsVariable(v),cte); }
 
 
-
-Var operator*(Var v, real cte)
-{ return new TimesConstantVariable(v,cte); }
-
-Var operator*(real cte, Var v)
-{ return new TimesConstantVariable(v,cte); }
-
-
 //!  element-wise multiplications
 Var operator*(Var v1, Var v2)
 { 
@@ -170,9 +158,6 @@
         return new TimesVariable(v1,v2); 
 }
 
-Var operator/(Var v, real cte)
-{ return new TimesConstantVariable(v, 1.0/cte); }
-
 Var operator/(real cte, Var v)
 {
     if(fast_exact_is_equal(cte, 1.0))

Modified: trunk/plearn/var/Var_operators.h
===================================================================
--- trunk/plearn/var/Var_operators.h	2008-06-03 21:51:02 UTC (rev 9098)
+++ trunk/plearn/var/Var_operators.h	2008-06-03 21:58:54 UTC (rev 9099)
@@ -45,6 +45,8 @@
 #define Var_operators_INC
 
 #include "Var.h"
+#include "NegateElementsVariable.h"
+#include "TimesConstantVariable.h"
 
 namespace PLearn {
 using namespace std;
@@ -54,23 +56,29 @@
 Var operator+(Var v1, Var v2);
 void operator+=(Var& v1, const Var& v2);
 
+// Unary operator- is obtained through 
+// inclusion of NegateElementsVariable.h
 
-Var operator-(Var v);
+// Binary operator-
 Var operator-(Var v, real cte);
 Var operator-(real cte, Var v);
 Var operator-(Var v1, Var v2);
 void operator-=(Var& v1, const Var& v2);
 
-// elementwise multiplications
-Var operator*(Var v, real cte);
-Var operator*(real cte, Var v);
+// Elementwise multiplications with real constant 
+// are obtained from including TimesConstantVariable.h
+  
+// Var * Var elementwise multiplication:
 Var operator*(Var v1, Var v2);
 
 // elementwise divisions
-Var operator/(Var v, real cte);
+// elementwise division by a real constant 
+// is obtained from including TimesConstantVariable.h  
+// Other elementwise divisions:
 Var operator/(real cte, Var v);
 Var operator/(Var v1, Var v2);
 
+
 Var operator==(Var v1, Var v2);
 Var operator!=(Var v1, Var v2);
 Var isdifferent(Var v1, Var v2);



From plearner at mail.berlios.de  Wed Jun  4 00:02:41 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Wed, 4 Jun 2008 00:02:41 +0200
Subject: [Plearn-commits] r9100 - trunk/plearn/var
Message-ID: <200806032202.m53M2f7E015069@sheep.berlios.de>

Author: plearner
Date: 2008-06-04 00:02:41 +0200 (Wed, 04 Jun 2008)
New Revision: 9100

Modified:
   trunk/plearn/var/BinaryVariable.cc
Log:
Changed call to recomputeSize for call to sizeprop
(hopefully this won't break anything...)



Modified: trunk/plearn/var/BinaryVariable.cc
===================================================================
--- trunk/plearn/var/BinaryVariable.cc	2008-06-03 21:58:54 UTC (rev 9099)
+++ trunk/plearn/var/BinaryVariable.cc	2008-06-03 22:02:41 UTC (rev 9100)
@@ -108,9 +108,9 @@
 
     input1= parents[0];
     input2= parents[1];
-  
-    int dummy_l, dummy_w;
-    recomputeSize(dummy_l, dummy_w);
+    // int dummy_l, dummy_w;
+    //recomputeSize(dummy_l, dummy_w);
+    sizeprop();
 }
 
 #ifdef __INTEL_COMPILER



From plearner at mail.berlios.de  Wed Jun  4 00:18:12 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Wed, 4 Jun 2008 00:18:12 +0200
Subject: [Plearn-commits] r9101 - trunk/plearn/var
Message-ID: <200806032218.m53MICuP016238@sheep.berlios.de>

Author: plearner
Date: 2008-06-04 00:18:12 +0200 (Wed, 04 Jun 2008)
New Revision: 9101

Modified:
   trunk/plearn/var/Variable.cc
   trunk/plearn/var/Variable.h
Log:
Removed old version of newwrite so that default newwrite gets called, and variables get correctly serialized with their options.
(Note: hope this won't change the output of too many tests...)



Modified: trunk/plearn/var/Variable.cc
===================================================================
--- trunk/plearn/var/Variable.cc	2008-06-03 22:02:41 UTC (rev 9100)
+++ trunk/plearn/var/Variable.cc	2008-06-03 22:18:12 UTC (rev 9101)
@@ -423,6 +423,7 @@
     return oldm;
 }
 
+/*
 void Variable::newwrite(PStream& out) const
 { 
     switch(out.outmode)
@@ -444,6 +445,7 @@
         inherited::newwrite(out);
     }
 }
+*/
 
 PLEARN_IMPLEMENT_ABSTRACT_OBJECT(Variable,
                                  "The base Variable class",

Modified: trunk/plearn/var/Variable.h
===================================================================
--- trunk/plearn/var/Variable.h	2008-06-03 22:02:41 UTC (rev 9100)
+++ trunk/plearn/var/Variable.h	2008-06-03 22:18:12 UTC (rev 9101)
@@ -234,7 +234,7 @@
     virtual void copyValueInto(Vec v) { v << value; }
     virtual void copyGradientInto(Vec g) { g << gradient; }
 
-    virtual void newwrite(PStream& out) const;
+    // virtual void newwrite(PStream& out) const;
 
     //!  returns the name of this variable. If its name has not been set, 
     //!  it will be assigned a name of V_varnum



From plearner at mail.berlios.de  Wed Jun  4 00:31:02 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Wed, 4 Jun 2008 00:31:02 +0200
Subject: [Plearn-commits] r9102 - trunk/commands/PLearnCommands
Message-ID: <200806032231.m53MV2On018181@sheep.berlios.de>

Author: plearner
Date: 2008-06-04 00:30:54 +0200 (Wed, 04 Jun 2008)
New Revision: 9102

Added:
   trunk/commands/PLearnCommands/VerifyGradientCommand.cc
   trunk/commands/PLearnCommands/VerifyGradientCommand.h
Log:
Added VerfyGradientCommand to quickly verify gradient computation of a new variable class.


Added: trunk/commands/PLearnCommands/VerifyGradientCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/VerifyGradientCommand.cc	2008-06-03 22:18:12 UTC (rev 9101)
+++ trunk/commands/PLearnCommands/VerifyGradientCommand.cc	2008-06-03 22:30:54 UTC (rev 9102)
@@ -0,0 +1,121 @@
+// -*- C++ -*-
+
+// VerifyGradientCommand.cc
+//
+// Copyright (C) 2008 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file VerifyGradientCommand.cc */
+
+
+#include "VerifyGradientCommand.h"
+#include <plearn/var/SourceVariable.h>
+#include <plearn/var/Func.h>
+#include <plearn/math/PRandom.h>
+
+namespace PLearn {
+using namespace std;
+
+//! This allows to register the 'VerifyGradientCommand' command in the command registry
+PLearnCommandRegistry VerifyGradientCommand::reg_(new VerifyGradientCommand);
+
+VerifyGradientCommand::VerifyGradientCommand()
+    : PLearnCommand(
+        "verify_gradient",
+        "Verifies the gradient computation in a Variable through finite differences.",
+        "verify_gradient 'MyVariable()' stepsize which_component input1_length input1_width input1_min input1_max input2_length input2_width input2_min input2_max \n"
+        "The variable constructor can take option=value arguments if needed.\n"
+        "stepsize is the size of the step taken for finite difference\n"
+        "which_component specifies which element of MyVariable we feed gradients in (if MyVariable is not a scalar for ex.)\n"
+        "if which_component is -1, then all elements will be fed gradient (same as considering gradient to the sum of all elements)\n"
+        "input1_... and input2_... are required if the variable is a unary resp. binary variable.\n"
+        "SourceVariables input1 and input2 of the specified size will be constructed \n"
+        "and randomly initialized with a uniform between corresponding min and max.\n"
+        )
+{}
+
+//! The actual implementation of the 'VerifyGradientCommand' command
+void VerifyGradientCommand::run(const vector<string>& args)
+{
+    if(args.size()<2)
+        PLERROR("Not enough arguments provided");
+    string varspec = args[0];
+    Object* obj = newObject(varspec);
+    Variable* varp = dynamic_cast<Variable*>(obj);
+    if(!varp)
+        PLERROR("The object you specified does not seem to be a known Variable (is it spelled correctly, did your executable link with it?)");
+    Var var = varp;
+
+    double stepsize = todouble(args[1]);
+    int which_component = toint(args[2]);
+
+    unsigned int nparents = (args.size()-3)/4;
+    if(3+nparents*4 != args.size())
+        PLERROR("You must specify 4 numbers (length, width, min, max) for each of the parents variable");
+
+    VarArray parents;
+    for(unsigned int k=0; k<nparents; k++)
+    {
+        int l = toint(args[3+k*4]);
+        int w = toint(args[3+k*4+1]);
+        double mi = todouble(args[3+k*4+2]);
+        double ma = todouble(args[3+k*4+3]);
+        perr << "Params: " << l << " " << w << " " << mi << " " << ma << endl;
+        SourceVariable* sourcevar = new SourceVariable(l, w, "uniform", mi, ma);
+        sourcevar->randomInitialize(PRandom::common());
+        Var inputk = sourcevar;
+        parents.append(inputk);
+    }
+    var->setParents(parents);
+    var->build();
+
+    pout << "parents:" << *parents[0] << endl;
+    Func f(parents, var);
+    pout << "func inputs:" << *(f->inputs[0]) << endl;
+
+    f->verifyGradient(stepsize, which_component);
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/commands/PLearnCommands/VerifyGradientCommand.h
===================================================================
--- trunk/commands/PLearnCommands/VerifyGradientCommand.h	2008-06-03 22:18:12 UTC (rev 9101)
+++ trunk/commands/PLearnCommands/VerifyGradientCommand.h	2008-06-03 22:30:54 UTC (rev 9102)
@@ -0,0 +1,86 @@
+// -*- C++ -*-
+
+// VerifyGradientCommand.h
+//
+// Copyright (C) 2008 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file VerifyGradientCommand.h */
+
+
+#ifndef VerifyGradientCommand_INC
+#define VerifyGradientCommand_INC
+
+#include <commands/PLearnCommands/PLearnCommand.h>
+#include <commands/PLearnCommands/PLearnCommandRegistry.h>
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class VerifyGradientCommand : public PLearnCommand
+{
+    typedef PLearnCommand inherited;
+
+public:
+    VerifyGradientCommand();
+    virtual void run(const std::vector<std::string>& args);
+
+protected:
+    static PLearnCommandRegistry reg_;
+};
+
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From plearner at mail.berlios.de  Wed Jun  4 00:35:15 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Wed, 4 Jun 2008 00:35:15 +0200
Subject: [Plearn-commits] r9103 - in trunk: commands/EXPERIMENTAL
	plearn_learners/unsupervised/EXPERIMENTAL
Message-ID: <200806032235.m53MZFPn025133@sheep.berlios.de>

Author: plearner
Date: 2008-06-04 00:35:14 +0200 (Wed, 04 Jun 2008)
New Revision: 9103

Added:
   trunk/plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.cc
   trunk/plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.h
Modified:
   trunk/commands/EXPERIMENTAL/plearn_exp.cc
Log:
New experimental unsupervised learner DiverseComponentAnalysis


Modified: trunk/commands/EXPERIMENTAL/plearn_exp.cc
===================================================================
--- trunk/commands/EXPERIMENTAL/plearn_exp.cc	2008-06-03 22:30:54 UTC (rev 9102)
+++ trunk/commands/EXPERIMENTAL/plearn_exp.cc	2008-06-03 22:35:14 UTC (rev 9103)
@@ -65,7 +65,7 @@
 #include <commands/PLearnCommands/HelpCommand.h>
 // #include <commands/PLearnCommands/JulianDateCommand.h>
 // #include <commands/PLearnCommands/KolmogorovSmirnovCommand.h>
-// #include <commands/PLearnCommands/LearnerCommand.h>
+#include <commands/PLearnCommands/LearnerCommand.h>
 // #include <commands/PLearnCommands/PairwiseDiffsCommand.h>
 #include <commands/PLearnCommands/ReadAndWriteCommand.h>
 #include <commands/PLearnCommands/RunCommand.h>
@@ -77,8 +77,9 @@
 // #include <commands/PLearnCommands/HTMLHelpCommand.h>
 
 // //#include <commands/PLearnCommands/TxtmatCommand.h>
+#include <commands/PLearnCommands/VerifyGradientCommand.h>
+#include <commands/PLearnCommands/ExtractOptionCommand.h>
 
-
 // /**************
 //  * Dictionary *
 //  **************/
@@ -325,7 +326,7 @@
 // #include <plearn_learners/unsupervised/Isomap.h>
 // #include <plearn_learners/unsupervised/KernelPCA.h>
 // #include <plearn_learners/unsupervised/LLE.h>
-// #include <plearn_learners/unsupervised/PCA.h>
+#include <plearn_learners/unsupervised/PCA.h>
 // #include <plearn_learners/unsupervised/SpectralClustering.h>
 
 // Kernels
@@ -394,6 +395,10 @@
 #include <plearn_learners/classifiers/KNNClassifier.h>
 #include <plearn_learners/classifiers/EXPERIMENTAL/LocalGaussianClassifier.h>
 
+// Stuff used for DiverseComponentAnalysis
+#include <plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.h>
+
+
 using namespace PLearn;
 
 int main(int argc, char** argv)

Added: trunk/plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.cc
===================================================================
--- trunk/plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.cc	2008-06-03 22:30:54 UTC (rev 9102)
+++ trunk/plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.cc	2008-06-03 22:35:14 UTC (rev 9103)
@@ -0,0 +1,596 @@
+// -*- C++ -*-
+
+// DiverseComponentAnalysis.cc
+//
+// Copyright (C) 2008 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file DiverseComponentAnalysis.cc */
+
+
+#include "DiverseComponentAnalysis.h"
+#include <plearn/vmat/VMat_basic_stats.h>
+#include<plearn/math/TMat_maths.h>
+#include<plearn/var/ProductVariable.h>
+#include<plearn/var/ProductTransposeVariable.h>
+#include<plearn/var/TransposeProductVariable.h>
+#include<plearn/var/SquareVariable.h>
+#include<plearn/var/AbsVariable.h>
+#include<plearn/var/SquareRootVariable.h>
+#include<plearn/var/ExpVariable.h>
+#include<plearn/var/TimesVariable.h>
+#include<plearn/var/SumVariable.h>
+#include<plearn/var/SigmoidVariable.h>
+#include<plearn/var/TanhVariable.h>
+#include<plearn/var/NegateElementsVariable.h>
+#include<plearn/var/TimesConstantVariable.h>
+#include<plearn/var/SumSquareVariable.h>
+#include<plearn/var/RowSumSquareVariable.h>
+#include<plearn/var/EXPERIMENTAL/ConstrainedSourceVariable.h>
+#include<plearn/var/EXPERIMENTAL/Cov2CorrVariable.h>
+#include<plearn/var/EXPERIMENTAL/DiagVariable.h>
+#include<plearn/var/EXPERIMENTAL/NonDiagVariable.h>
+#include<plearn/var/TransposeVariable.h>
+#include<plearn/var/ColumnSumVariable.h>
+#include<plearn/var/Var_operators.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    DiverseComponentAnalysis,
+    "Diverse Component Analysis",
+    "This is an experimental class that finds linear\n"
+    "projection directions that should yield\n"
+    "'diverse' components, based on some diversity loss");
+
+DiverseComponentAnalysis::DiverseComponentAnalysis()
+    :ncomponents(2),
+     nonlinearity("none"),
+     cov_transformation_type("cov"),
+     diag_premul(1.0),
+     offdiag_premul(1.0),
+     diag_nonlinearity("square"),
+     offdiag_nonlinearity("square"),
+     diag_weight(-1.0),
+     offdiag_weight(1.0),
+     force_zero_mean(false),
+     epsilon(1e-8),
+     nu(0),
+     constrain_norm_type(-2),
+     normalize(false)
+/* ### Initialize all fields to their default value here */
+{
+    // ### If this learner needs to generate random numbers, uncomment the
+    // ### line below to enable the use of the inherited PRandom object.
+    random_gen = new PRandom();
+}
+
+void DiverseComponentAnalysis::declareOptions(OptionList& ol)
+{
+    // ### Declare all of this object's options here.
+    // ### For the "flags" of each option, you should typically specify
+    // ### one of OptionBase::buildoption, OptionBase::learntoption or
+    // ### OptionBase::tuningoption. If you don't provide one of these three,
+    // ### this option will be ignored when loading values from a script.
+    // ### You can also combine flags, for example with OptionBase::nosave:
+    // ### (OptionBase::buildoption | OptionBase::nosave)
+
+    // ### ex:
+    // declareOption(ol, "myoption", &DiverseComponentAnalysis::myoption,
+    //               OptionBase::buildoption,
+    //               "Help text describing this option");
+    // ...
+
+    declareOption(
+        ol, "nonlinearity", &DiverseComponentAnalysis::nonlinearity, OptionBase::buildoption,
+        "The nonlinearity to apply after linear transformation of the inpuits t ield the representation.");
+
+    declareOption(
+        ol, "force_zero_mean", &DiverseComponentAnalysis::force_zero_mean, OptionBase::buildoption,
+        "If true then input mean won't be computes but forced to 0 (and a corresponding different covariance matrix will be computed)");
+
+    declareOption(
+        ol, "epsilon", &DiverseComponentAnalysis::epsilon, OptionBase::buildoption,
+        "regularization value to add to diagonal of computed input covariance matrix.");
+
+    declareOption(
+        ol, "nu", &DiverseComponentAnalysis::nu, OptionBase::buildoption,
+        "regularization parameter simulating destruction noise: \n"
+        "off-diagonal elements of covariance matrix Cx will be multiplied by 1-nu.");
+
+    declareOption(
+        ol, "constrain_norm_type", &DiverseComponentAnalysis::constrain_norm_type, OptionBase::buildoption,
+        "How to constrain the norms of rows of W: \n"
+        "  -1: L1 norm constrained source \n"
+        "  -2: L2 norm constrained source \n"
+        "  -3:explicit L2 normalization \n"
+        "  >0:add specified value times exp(sumsquare(W)) to the cost\n");
+
+    declareOption(
+        ol, "ncomponents", &DiverseComponentAnalysis::ncomponents, OptionBase::buildoption,
+        "The number components to keep (that's also the outputsize).");
+
+    declareOption(
+        ol, "cov_transformation_type", &DiverseComponentAnalysis::cov_transformation_type, OptionBase::buildoption,
+        "Controls the kind of transformation to apply to covariance matrix\n"
+        "cov: no transformation (keep covariance)\n"
+        "corr: transform into correlations, but keeping variances on the diagonal.\n"
+        "squaredist: do a 'squared distance kernel' Dij <- Cii+Cjj-2Cij kind of transformation\n"
+        "sincov: instead of ||u|| ||v|| cos(angle(u,v)) we transform it to ||u|| ||v|| |sin(angle(u,v))|\n"
+        "        this is computed as sqrt((1-<u.v>^2) * <u,u>^2 * <v,v>^2) where <u,v> is given by the covariance matrix\n");
+
+    declareOption(
+        ol, "diag_premul", &DiverseComponentAnalysis::diag_premul, OptionBase::buildoption,
+        "diagonal elements of Cy will be pre-multiplied by diag_premul (before applying non-linearity)");
+
+    declareOption(
+        ol, "offdiag_premul", &DiverseComponentAnalysis::offdiag_premul, OptionBase::buildoption,
+        "Non-diagonal elements of Cy will be pre-multiplied by diag_premul (before applying non-linearity)");
+
+    declareOption(
+        ol, "diag_nonlinearity", &DiverseComponentAnalysis::diag_nonlinearity, OptionBase::buildoption,
+        "The kind of nonlinearity to apply to the diagonal elements of Cy\n"
+        "after it's been through cov_transformation_type\n"
+        "Currently supported: none square abs sqrt sqrtabs exp tanh sigmoid");
+
+    declareOption(
+        ol, "offdiag_nonlinearity", &DiverseComponentAnalysis::offdiag_nonlinearity, OptionBase::buildoption,
+        "The kind of nonlinearity to apply to the non-diagonal elements of Cy \n"
+        "after it's been through cov_transformation_type\n"
+        "Currently supported: none square abs sqrt sqrtabs exp tanh sigmoid");
+
+    declareOption(
+        ol, "diag_weight", &DiverseComponentAnalysis::diag_weight, OptionBase::buildoption,
+        "what weight to give to the sum of transformed diagonal elements in the cost");
+
+    declareOption(
+        ol, "offdiag_weight", &DiverseComponentAnalysis::offdiag_weight, OptionBase::buildoption,
+        "what weight to give to the sum of transformed non-diagonal elements in the cost");
+
+    declareOption(
+        ol, "optimizer", &DiverseComponentAnalysis::optimizer, OptionBase::buildoption,
+        "The gradient-based optimizer to use");
+
+    declareOption(
+        ol, "normalize", &DiverseComponentAnalysis::normalize, OptionBase::buildoption,
+        "If true computed outputs will be scaled so they have unit variance.\n"
+        "(see explanation about inv_stddev_of_projections)");
+
+
+    // learnt options
+    declareOption(
+        ol, "mu", &DiverseComponentAnalysis::mu, OptionBase::learntoption,
+        "The (weighted) mean of the samples");
+
+    declareOption(
+        ol, "Cx", &DiverseComponentAnalysis::Cx, OptionBase::learntoption,
+        "The (weighted) covariance of the samples");
+
+    declareOption(
+        ol, "W", &DiverseComponentAnalysis::W, OptionBase::learntoption,
+        "A ncomponents x inputsize matrix containing the learnt projection directions");
+
+    declareOption(
+        ol, "bias", &DiverseComponentAnalysis::bias, OptionBase::learntoption,
+        "A 1 x ncomponents matrix containing the learnt bias (for the nonlinear case only)");
+
+    declareOption(
+        ol, "inv_stddev_of_projections", &DiverseComponentAnalysis::inv_stddev_of_projections, OptionBase::learntoption,
+        "As its name implies, this is one over the standard deviation of projected data.\n"
+        "when normalize=true computeOutput will multiply the projection by this,\n"
+        " elementwise, so that the output should have unit variance" );
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void DiverseComponentAnalysis::declareMethods(RemoteMethodMap& rmm)
+{
+    rmm.inherited(inherited::_getRemoteMethodMap_());
+
+    declareMethod(rmm,
+                  "getVarValue",
+                  &DiverseComponentAnalysis::getVarValue,
+                  (BodyDoc("Returns the matValue of the variable with the given name"),
+                   ArgDoc("varname", "name of the variable searched for"),
+                   RetDoc("Returns the value of the var as a Mat")));
+
+    declareMethod(rmm,
+                  "getVarGradient",
+                  &DiverseComponentAnalysis::getVarGradient,
+                  (BodyDoc("Returns the matGradient of the variable with the given name"),
+                   ArgDoc("varname", "name of the variable searched for"),
+                   RetDoc("Returns the gradient of the var as a Mat")));
+
+    declareMethod(rmm,
+                  "listVarNames",
+                  &DiverseComponentAnalysis::listVarNames,
+                  (BodyDoc("Returns a list of the names of all vars"),
+                   RetDoc("Returns a list of the names of all vars")));
+
+}
+
+Var DiverseComponentAnalysis::nonlinear_transform(Var in, string nonlinearity)
+{
+    Var res; // result
+    if(nonlinearity=="none" || nonlinearity=="linear")
+        res = in;
+    else if(nonlinearity=="square")
+        res = square(in);
+    else if(nonlinearity=="abs")
+        res = abs(in);
+    else if(nonlinearity=="sqrt")
+        res = squareroot(in);
+    else if(nonlinearity=="sqrtabs")
+        res = squareroot(abs(in));
+    else if(nonlinearity=="exp")
+        res = exp(in);
+    else if(nonlinearity=="tanh")
+        res = tanh(in);
+    else if(nonlinearity=="sigmoid")
+        res = sigmoid(in);
+    else
+        PLERROR("Unknown nonlinearity %s",nonlinearity.c_str());
+    return res;
+}
+
+void DiverseComponentAnalysis::build_()
+{
+    perr << "Entering DiverseComponentAnalysis::build_()" << endl;
+    bool rebuild_all = inputsize_>0 && (W.isNull() || (W->matValue.width()!=inputsize_));
+    bool rebuild_some = inputsize_>0 && Cyt.isNull();
+    bool linear = (nonlinearity=="none" || nonlinearity=="linear");
+    if(rebuild_some || rebuild_all)
+    {
+        perr << "Building with inputsize_ = " << inputsize_ << endl;
+
+        Var nW;
+        
+        if(constrain_norm_type==-1) // use constrainted source to constrain L1 norms to 1
+        {
+            perr << "using constrainted source to constrain L1 norms to 1" << endl;
+            if(rebuild_all)
+                W = new ConstrainedSourceVariable(ncomponents,inputsize_,1);
+            nW = W;
+        }
+        else if(constrain_norm_type==-2) // use constrainted source to constrain L2 norms to 1
+        {
+            perr << "using constrainted source to constrain L2 norms to 1" << endl;
+            if(rebuild_all)
+                W = new ConstrainedSourceVariable(ncomponents,inputsize_,2);
+            nW = W;
+        }
+        else if(constrain_norm_type==-3) // compute L2 normalization explicitly
+        {
+            perr << "Normalizing explicitly" << endl;
+            if(rebuild_all)
+                W = Var(ncomponents,inputsize_);
+            nW = W/squareroot(rowSumSquare(W));
+        }
+        else  // using ordinary weight decay: nW is not hard-constrained to be normalized
+        {
+            perr << "Using ordinary weight decay " << constrain_norm_type << endl;
+            if(rebuild_all)
+                W = Var(ncomponents,inputsize_);
+            nW = W;
+        }
+
+        if(linear) 
+        {
+            if(rebuild_all)
+                Cx = Var(inputsize_,inputsize_);
+            Cx->setName("Cx");
+            Cy = product(nW, productTranspose(Cx, nW));
+        }
+        else // nonlinear trasform
+        {
+            int l = train_set->length();
+            perr << "Building with nonlinear transform and l="<<l <<" examples of inputsize=" << inputsize_ << endl;
+
+            inputdata = Var(l,inputsize_);
+            if(rebuild_all)
+                bias = Var(1, ncomponents);
+            trdata = productTranspose(inputdata,nW)+bias;
+            perr << "USING MAIN REPRESENTATION NONLINEARITY: " << nonlinearity << endl;
+            trdata = nonlinear_transform(trdata,nonlinearity);
+            if(force_zero_mean)
+                ctrdata = trdata;
+            else
+                ctrdata = trdata-(1.0/l)*columnSum(trdata);
+            ctrdata->setName("ctrdata");            
+            trdata->setName("trdata");            
+            Cy = (1.0/l)*transposeProduct(ctrdata,ctrdata);
+        }
+        perr << "Built Cy of size " << Cy->length() << "x" << Cy->width() << endl;
+
+        if(cov_transformation_type=="cov")
+            Cyt = Cy;
+        else if(cov_transformation_type=="corr")
+            Cyt = cov2corr(Cy,2);
+        else if(cov_transformation_type=="squaredist")
+        {
+            Var dCy = diag(Cy);
+            Cyt = Cy*(-2.0)+dCy+transpose(dCy);
+        }
+        else if(cov_transformation_type=="sincov")
+        {
+            Var dCy = diag(Cy);
+            Cyt = squareroot(((1+1e-6)-square(cov2corr(Cy)))*dCy*transpose(dCy));            
+            // Cyt = ((1.0-square(cov2corr(Cy)))*dCy*transpose(dCy));            
+        }
+        else 
+            PLERROR("Invalid cov_transformation_type");
+
+        if(diag_weight!=0)
+            L += diag_weight*sum(nonlinear_transform(diag(Cyt*diag_premul),diag_nonlinearity));
+        if(offdiag_weight!=0)
+            L += offdiag_weight*sum(nonlinear_transform(nondiag(Cyt*offdiag_premul),offdiag_nonlinearity));
+            
+        if(constrain_norm_type>0)
+            L += L+constrain_norm_type*exp(sumsquare(W));
+
+        if(!optimizer)
+            PLERROR("You must specify the optimizer field (ex: GradientOptimizer)");
+        if(linear)
+            optimizer->setToOptimize(W, L);
+        else
+            optimizer->setToOptimize(W&bias, L);
+        
+        perr << "Built optimizer" << endl;
+        nW->setName("W");
+        Cy->setName("Cy");
+        Cyt->setName("Cyt");
+        L->setName("L");
+
+        allvars = Cx & trdata& ctrdata& nW & Cy & Cyt & L;
+    }
+    perr << "Exiting DiverseComponentAnalysis::build_()" << endl;
+}
+
+
+TVec<string> DiverseComponentAnalysis::listVarNames() const
+{
+    int n = allvars.length();
+    TVec<string> names;
+    for(int i=0; i<n; i++)
+        if(allvars[i].isNotNull())
+            names.append(allvars[i]->getName());
+    return names;
+}
+
+Mat DiverseComponentAnalysis::getVarValue(string varname) const
+{
+    for(int i=0; i<allvars.length(); i++)
+    {
+        Var v = allvars[i];        
+        if(v.isNotNull() && v->getName()==varname)
+            return v->matValue;
+    }
+    PLERROR("No Var with name %s", varname.c_str());
+    return Mat();
+}
+
+Mat DiverseComponentAnalysis::getVarGradient(string varname) const
+{
+    for(int i=0; i<allvars.length(); i++)
+    {
+        Var v = allvars[i];
+        if(v.isNotNull() && v->getName()==varname)
+            return v->matGradient;
+    }
+    PLERROR("No Var with name %s", varname.c_str());
+    return Mat();
+}
+
+void DiverseComponentAnalysis::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void DiverseComponentAnalysis::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+    deepCopyField(mu, copies);
+    deepCopyField(Cx, copies);
+    deepCopyField(W, copies);
+}
+
+
+int DiverseComponentAnalysis::outputsize() const
+{
+    return ncomponents;
+}
+
+void DiverseComponentAnalysis::forget()
+{
+    //! (Re-)initialize the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!)
+    /*!
+      A typical forget() method should do the following:
+      - call inherited::forget() to initialize its random number generator
+        with the 'seed' option
+      - initialize the learner's parameters, using this random generator
+      - stage = 0
+    */
+
+    // this will reset stage=0 and reset the random_gen to the initial seed_
+    inherited::forget();
+
+    perr << "Called DCS::forget() with inputsize_ = " << inputsize_ << endl;
+    if(inputsize_>0)
+    {
+        random_gen->fill_random_normal(W->value, 0., 1.);
+        perr << "Squared norm of first row of W after fill_random_normal: " << pownorm(W->matValue(0)) << endl;
+        int normval = (constrain_norm_type==-1 ?1 :2);
+        for(int i=0; i<ncomponents; i++)
+            PLearn::normalize(W->matValue(i), normval);
+        perr << "Squared norm of first row of W after L" << normval << " normalization: " << pownorm(W->matValue(0)) << endl;
+    }
+}
+
+void DiverseComponentAnalysis::train()
+{
+    // The role of the train method is to bring the learner up to
+    // stage==nstages, updating train_stats with training costs measured
+    // on-line in the process.
+
+    /* TYPICAL CODE:
+
+    static Vec input;  // static so we don't reallocate memory each time...
+    static Vec target; // (but be careful that static means shared!)
+    input.resize(inputsize());    // the train_set's inputsize()
+    target.resize(targetsize());  // the train_set's targetsize()
+    real weight;
+
+    // This generic PLearner method does a number of standard stuff useful for
+    // (almost) any learner, and return 'false' if no training should take
+    // place. See PLearner.h for more details.
+    */
+
+    if (!initTrain())
+        return;
+
+    while(stage<nstages)
+    {
+        // clear statistics of previous epoch
+        train_stats->forget();
+
+        if(stage==0) // do stage 1
+        {
+            bool linear = (nonlinearity=="none" || nonlinearity=="linear");
+            if(!linear)
+            {
+                perr << "Nonlinear training to stage 1" << endl;
+                Mat X = inputdata->matValue;
+                int l = train_set->length();
+                Vec target;
+                real weight;
+                for(int i=0; i<l; i++)
+                {
+                    Vec Xi = X(i);
+                    train_set->getExample(i,Xi,target,weight);
+                }
+                mu.resize(inputsize_);
+                columnMean(X, mu);
+                perr << "Nonlinear training to stage 1. DONE." << endl;
+            }
+            else // linear case
+            {
+                if(force_zero_mean)
+                {
+                    mu.resize(inputsize());
+                    mu.fill(0);
+                    computeInputCovar(train_set, mu, Cx->matValue, epsilon);
+                }
+                else
+                    computeInputMeanAndCovar(train_set, mu, Cx->matValue, epsilon);
+
+                if(nu!=0)
+                {
+                    Mat C = Cx->matValue;
+                    int l = C.length();
+                    for(int i=0; i<l; i++)
+                        for(int j=0; j<l; j++)
+                            if(i!=j)
+                                C(i,j) *= (1-nu);
+                }
+            }
+        }
+        else
+        {
+            optimizer->optimizeN(*train_stats);
+            Mat C = Cy->matValue;
+            int l = C.length();            
+            inv_stddev_of_projections.resize(l);
+            for(int i=0; i<l; i++)
+                inv_stddev_of_projections = 1.0/sqrt(C(i,i));
+        }
+
+        //... train for 1 stage, and update train_stats,
+        // using train_set->getExample(input, target, weight)
+        // and train_stats->update(train_costs)
+
+        ++stage;
+        train_stats->finalize(); // finalize statistics for this epoch
+    }
+}
+
+
+void DiverseComponentAnalysis::computeOutput(const Vec& input, Vec& output) const
+{
+    static Vec x;
+    x.resize(input.length());
+    x << input;
+
+    // Center and project on directions
+    x -= mu;
+    output.resize(ncomponents);
+    product(output, W->matValue, x);
+    if(normalize)
+        output *= inv_stddev_of_projections;
+}
+
+void DiverseComponentAnalysis::computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                           const Vec& target, Vec& costs) const
+{
+    costs.resize(0);
+}
+
+TVec<string> DiverseComponentAnalysis::getTestCostNames() const
+{
+    return TVec<string>();
+}
+
+TVec<string> DiverseComponentAnalysis::getTrainCostNames() const
+{
+    return TVec<string>(1,"L");
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.h
===================================================================
--- trunk/plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.h	2008-06-03 22:30:54 UTC (rev 9102)
+++ trunk/plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.h	2008-06-03 22:35:14 UTC (rev 9103)
@@ -0,0 +1,256 @@
+// -*- C++ -*-
+
+// DiverseComponentAnalysis.h
+//
+// Copyright (C) 2008 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file DiverseComponentAnalysis.h */
+
+
+#ifndef DiverseComponentAnalysis_INC
+#define DiverseComponentAnalysis_INC
+
+#include <plearn_learners/generic/PLearner.h>
+#include <plearn/var/Var.h>
+#include <plearn/opt/Optimizer.h>
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class DiverseComponentAnalysis : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+
+    //! The number of components to keep (that's also the outputsize)    
+    int ncomponents;
+    string nonlinearity;
+
+    string cov_transformation_type;
+
+    double diag_premul;
+    double offdiag_premul;
+
+    string diag_nonlinearity;
+    string offdiag_nonlinearity;
+
+    double diag_weight;
+    double offdiag_weight;
+
+    bool force_zero_mean;
+    real epsilon; // regularization value to add to diagonal of computed covariance matrix
+    real nu; // regularization: off-diagonal elements of input covariance will be multiplied by 1-nu
+    real constrain_norm_type; // how to constrain the norms of rows of W: -1:constrained source; -2:explicit normalization; >0:ordinary weight decay
+
+    bool normalize;
+
+    PP<Optimizer> optimizer;
+
+    // *******************
+    // * learned options *
+    // *******************
+
+    //! The (weighted) mean of the samples 
+    Vec mu;
+
+    //! The (weighted) covariance matrix of the samples
+    Var Cx;
+
+    //! A ncomponents x inputsize matrix containing the projection directions
+    Var W; 
+
+    //! The covariance of transformed data Cy = cov(W x) = W Cx W^T
+    Var Cy;
+
+    //! The "transformed" covariance according to cov_transformation_type
+    Var Cyt;
+
+    //! The loss variable: sum(asCy)
+    Var L;
+
+    Vec inv_stddev_of_projections;
+
+    // For the nonlinear case
+    Var inputdata; // whole input data matrix
+    Var bias;
+    Var trdata;  // transformed data
+    Var ctrdata; // centered transformed data
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    DiverseComponentAnalysis();
+
+    //! Returns the value of the var with the specified name (searched in VarArray allvars)
+    Mat getVarValue(string varname) const;
+
+    //! Returns the gradient of the var with the specified name (searched in VarArray allvars)
+    Mat getVarGradient(string varname) const;
+
+    //! Returns the names of all vars (in VarArray allvars)
+    TVec<string> listVarNames() const;
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Returns the size of this learner's output, (which typically
+    //! may depend on its inputsize(), targetsize() and set options).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual int outputsize() const;
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training costs
+    //! measured on-line in the process.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void train();
+
+    //! Computes the output from the input.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeOutput(const Vec& input, Vec& output) const;
+
+    //! Computes the costs from already computed output.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                         const Vec& target, Vec& costs) const;
+
+    //! Returns the names of the costs computed by computeCostsFromOutpus (and
+    //! thus the test method).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec<std::string> getTestCostNames() const;
+
+    //! Returns the names of the objective costs that the train method computes
+    //! and for which it updates the VecStatsCollector train_stats.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec<std::string> getTrainCostNames() const;
+
+
+    // *** SUBCLASS WRITING: ***
+    // While in general not necessary, in case of particular needs
+    // (efficiency concerns for ex) you may also want to overload
+    // some of the following methods:
+    // virtual void computeOutputAndCosts(const Vec& input, const Vec& target,
+    //                                    Vec& output, Vec& costs) const;
+    // virtual void computeCostsOnly(const Vec& input, const Vec& target,
+    //                               Vec& costs) const;
+    // virtual void test(VMat testset, PP<VecStatsCollector> test_stats,
+    //                   VMat testoutputs=0, VMat testcosts=0) const;
+    // virtual int nTestCosts() const;
+    // virtual int nTrainCosts() const;
+    // virtual void resetInternalState();
+    // virtual bool isStatefulLearner() const;
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(DiverseComponentAnalysis);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList& ol);
+    static void declareMethods(RemoteMethodMap& rmm);
+    static Var nonlinear_transform(Var in, string nonlinearity);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+    VarArray allvars;
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(DiverseComponentAnalysis);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From lamblin at mail.berlios.de  Wed Jun  4 19:20:09 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 4 Jun 2008 19:20:09 +0200
Subject: [Plearn-commits] r9104 - trunk/plearn_learners/online
Message-ID: <200806041720.m54HK9ar011529@sheep.berlios.de>

Author: lamblin
Date: 2008-06-04 19:20:08 +0200 (Wed, 04 Jun 2008)
New Revision: 9104

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
Log:
Whitespaces and indentation changes


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2008-06-03 22:35:14 UTC (rev 9103)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2008-06-04 17:20:08 UTC (rev 9104)
@@ -1277,7 +1277,7 @@
             lr = grad_learning_rate / (1. + grad_decrease_ct * stage );
         else
             lr = grad_learning_rate;
-        
+
         layers[n_layers-1]->setLearningRate( lr );
         connections[n_layers-2]->setLearningRate( lr );
 
@@ -1362,16 +1362,16 @@
                 lr = grad_learning_rate / (1. + grad_decrease_ct * stage );
             else
                 lr = grad_learning_rate;
-            
+
             connections[ i ]->setLearningRate( lr );
             layers[ i+1 ]->setLearningRate( lr );
-            
 
+
             layers[i+1]->bpropUpdate( layers[i+1]->activation,
                                       layers[i+1]->expectation,
                                       activation_gradients[i+1],
                                       expectation_gradients[i+1] );
-            
+
             connections[i]->bpropUpdate( layers[i]->expectation,
                                          layers[i+1]->activation,
                                          expectation_gradients[i],
@@ -1389,11 +1389,11 @@
                 lr = cd_learning_rate / (1. + cd_decrease_ct * stage );
             else
                 lr = cd_learning_rate;
-            
+
             layers[i]->setLearningRate( lr );
             layers[i+1]->setLearningRate( lr );
             connections[i]->setLearningRate( lr );
-            
+
             if( i > 0 )
             {
                 save_layer_activation.resize(layers[i]->size);

Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2008-06-03 22:35:14 UTC (rev 9103)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2008-06-04 17:20:08 UTC (rev 9104)
@@ -78,13 +78,13 @@
 
 void StackedAutoassociatorsNet::declareOptions(OptionList& ol)
 {
-    declareOption(ol, "greedy_learning_rate", 
+    declareOption(ol, "greedy_learning_rate",
                   &StackedAutoassociatorsNet::greedy_learning_rate,
                   OptionBase::buildoption,
                   "The learning rate used during the autoassociator "
                   "gradient descent training");
 
-    declareOption(ol, "greedy_decrease_ct", 
+    declareOption(ol, "greedy_decrease_ct",
                   &StackedAutoassociatorsNet::greedy_decrease_ct,
                   OptionBase::buildoption,
                   "The decrease constant of the learning rate used during "
@@ -93,19 +93,19 @@
                   "its training,\n"
                   "the learning rate is reset to it's initial value.\n");
 
-    declareOption(ol, "fine_tuning_learning_rate", 
+    declareOption(ol, "fine_tuning_learning_rate",
                   &StackedAutoassociatorsNet::fine_tuning_learning_rate,
                   OptionBase::buildoption,
                   "The learning rate used during the fine tuning gradient descent");
 
-    declareOption(ol, "fine_tuning_decrease_ct", 
+    declareOption(ol, "fine_tuning_decrease_ct",
                   &StackedAutoassociatorsNet::fine_tuning_decrease_ct,
                   OptionBase::buildoption,
                   "The decrease constant of the learning rate used during "
                   "fine tuning\n"
                   "gradient descent.\n");
 
-    declareOption(ol, "l1_neuron_decay", 
+    declareOption(ol, "l1_neuron_decay",
                   &StackedAutoassociatorsNet::l1_neuron_decay,
                   OptionBase::buildoption,
                   " L1 penalty weight on the hidden layers, to encourage "
@@ -113,14 +113,14 @@
                   "the greedy unsupervised phases.\n"
                   );
 
-    declareOption(ol, "l1_neuron_decay_center", 
+    declareOption(ol, "l1_neuron_decay_center",
                   &StackedAutoassociatorsNet::l1_neuron_decay_center,
                   OptionBase::buildoption,
                   "Value around which the L1 penalty should be centered, i.e.\n"
                   "   L1(h) = | h - l1_neuron_decay_center |\n"
                   "where h are the values of the neurons.\n");
 
-    declareOption(ol, "training_schedule", 
+    declareOption(ol, "training_schedule",
                   &StackedAutoassociatorsNet::training_schedule,
                   OptionBase::buildoption,
                   "Number of examples to use during each phase of greedy pre-training.\n"
@@ -138,20 +138,20 @@
                   OptionBase::buildoption,
                   "The weights of the connections between the layers");
 
-    declareOption(ol, "reconstruction_connections", 
+    declareOption(ol, "reconstruction_connections",
                   &StackedAutoassociatorsNet::reconstruction_connections,
                   OptionBase::buildoption,
                   "The weights of the reconstruction connections between the "
                   "layers");
 
-    declareOption(ol, "correlation_connections", 
+    declareOption(ol, "correlation_connections",
                   &StackedAutoassociatorsNet::correlation_connections,
                   OptionBase::buildoption,
                   "Optional weights to capture correlation and anti-correlation\n"
                   "in the hidden layers. They must have the same input and\n"
                   "output sizes, compatible with their corresponding layers.");
 
-    declareOption(ol, "direct_connections", 
+    declareOption(ol, "direct_connections",
                   &StackedAutoassociatorsNet::direct_connections,
                   OptionBase::buildoption,
                   "Optional weights from each inputs to all other inputs'\n"
@@ -188,58 +188,58 @@
                   "If true then all unsupervised training stages (as well as\n"
                   "the fine-tuning stage) are done simultaneously.\n");
 
-    declareOption(ol, "partial_costs_weights", 
+    declareOption(ol, "partial_costs_weights",
                   &StackedAutoassociatorsNet::partial_costs_weights,
                   OptionBase::buildoption,
                   "Relative weights of the partial costs. If not defined,\n"
                   "weights of 1 will be assumed for all partial costs.\n"
         );
 
-    declareOption(ol, "compute_all_test_costs", 
+    declareOption(ol, "compute_all_test_costs",
                   &StackedAutoassociatorsNet::compute_all_test_costs,
                   OptionBase::buildoption,
                   "Indication that, at test time, all costs for all layers \n"
                   "(up to the currently trained layer) should be computed.\n"
         );
 
-    declareOption(ol, "reconstruct_hidden", 
+    declareOption(ol, "reconstruct_hidden",
                   &StackedAutoassociatorsNet::reconstruct_hidden,
                   OptionBase::buildoption,
                   "Indication that the autoassociators are also trained to\n"
                   "reconstruct their hidden layers (inspired from CD1 in an RBM).\n"
         );
 
-    declareOption(ol, "fraction_of_masked_inputs", 
+    declareOption(ol, "fraction_of_masked_inputs",
                   &StackedAutoassociatorsNet::fraction_of_masked_inputs,
                   OptionBase::buildoption,
                   "Random fraction of the autoassociators' input components that\n"
                   "masked, i.e. unsused to reconstruct the input.\n"
         );
 
-    declareOption(ol, "unsupervised_nstages", 
+    declareOption(ol, "unsupervised_nstages",
                   &StackedAutoassociatorsNet::unsupervised_nstages,
                   OptionBase::buildoption,
                   "Number of samples to use for unsupervised fine-tuning.\n");
 
-    declareOption(ol, "unsupervised_fine_tuning_learning_rate", 
+    declareOption(ol, "unsupervised_fine_tuning_learning_rate",
                   &StackedAutoassociatorsNet::unsupervised_fine_tuning_learning_rate,
                   OptionBase::buildoption,
                   "The learning rate used during the unsupervised "
                   "fine tuning gradient descent");
 
-    declareOption(ol, "unsupervised_fine_tuning_decrease_ct", 
+    declareOption(ol, "unsupervised_fine_tuning_decrease_ct",
                   &StackedAutoassociatorsNet::unsupervised_fine_tuning_decrease_ct,
                   OptionBase::buildoption,
                   "The decrease constant of the learning rate used during\n"
                   "unsupervised fine tuning gradient descent.\n");
 
-    declareOption(ol, "mask_input_layer_only_in_unsupervised_fine_tuning", 
+    declareOption(ol, "mask_input_layer_only_in_unsupervised_fine_tuning",
                   &StackedAutoassociatorsNet::mask_input_layer_only_in_unsupervised_fine_tuning,
                   OptionBase::buildoption,
                   "Indication that only the input layer should be masked\n"
                   "during unsupervised fine-tuning.\n");
 
-    declareOption(ol, "greedy_stages", 
+    declareOption(ol, "greedy_stages",
                   &StackedAutoassociatorsNet::greedy_stages,
                   OptionBase::learntoption,
                   "Number of training samples seen in the different greedy "
@@ -251,13 +251,13 @@
                   "Number of layers"
         );
 
-    declareOption(ol, "unsupervised_stage", 
+    declareOption(ol, "unsupervised_stage",
                   &StackedAutoassociatorsNet::unsupervised_stage,
                   OptionBase::learntoption,
                   "Number of samples visited so far during unsupervised "
                   "fine-tuning.\n");
 
-    declareOption(ol, "correlation_layers", 
+    declareOption(ol, "correlation_layers",
                   &StackedAutoassociatorsNet::correlation_layers,
                   OptionBase::learntoption,
                   "Hidden layers for the correlation connections"
@@ -286,17 +286,17 @@
     {
         // Initialize some learnt variables
         n_layers = layers.length();
-        
+
         if( weightsize_ > 0 )
             PLERROR("StackedAutoassociatorsNet::build_() - \n"
                     "usage of weighted samples (weight size > 0) is not\n"
                     "implemented yet.\n");
 
-        if( !online && training_schedule.length() != n_layers-1 )        
+        if( !online && training_schedule.length() != n_layers-1 )
             PLERROR("StackedAutoassociatorsNet::build_() - \n"
                     "training_schedule should have %d elements.\n",
                     n_layers-1);
-        
+
         if( partial_costs && partial_costs.length() != n_layers-1 )
             PLERROR("StackedAutoassociatorsNet::build_() - \n"
                     "partial_costs should have %d elements.\n",
@@ -335,11 +335,11 @@
                 greedy_stages.resize(n_layers-1);
                 greedy_stages.clear();
             }
-            
+
             if(stage > 0)
                 currently_trained_layer = n_layers;
             else
-            {            
+            {
                 currently_trained_layer = n_layers-1;
                 while(currently_trained_layer>1
                       && greedy_stages[currently_trained_layer-1] <= 0)
@@ -350,7 +350,7 @@
         {
             currently_trained_layer = n_layers;
         }
-    
+
         build_layers_and_connections();
         build_costs();
     }
@@ -369,13 +369,13 @@
         PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() - \n"
                 "there should be %d reconstruction connections.\n",
                 n_layers-1);
-    
+
     if( correlation_connections.length() != 0 &&
         correlation_connections.length() != n_layers-1 )
         PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() - \n"
                 "there should be either %d correlation connections or none.\n",
                 n_layers-1);
-    
+
     if( direct_connections.length() != 0 &&
         direct_connections.length() != n_layers-1 )
         PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() - \n"
@@ -387,7 +387,7 @@
                 "compute_all_test_costs option is not implemented for\n"
                 "reconstruct_hidden option.");
 
-    
+
     if(correlation_connections.length() != 0)
     {
         if( compute_all_test_costs )
@@ -400,7 +400,7 @@
             if( greedy_stages[i] == 0 )
             {
                 CopiesMap map;
-                correlation_layers[i] = 
+                correlation_layers[i] =
                     layers[i+1]->deepCopy(map);
             }
         }
@@ -414,7 +414,7 @@
         PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() - \n"
                 "layers[0] should have a size of %d.\n",
                 inputsize_);
-    
+
     activations.resize( n_layers );
     expectations.resize( n_layers );
     activation_gradients.resize( n_layers );
@@ -523,7 +523,7 @@
         {
             reconstruction_connections[i]->random_gen = random_gen;
             reconstruction_connections[i]->forget();
-        }        
+        }
 
         activations[i].resize( layers[i]->size );
         expectations[i].resize( layers[i]->size );
@@ -562,15 +562,15 @@
     if( !final_module )
         PLERROR("StackedAutoassociatorsNet::build_costs() - \n"
                 "final_module should be provided.\n");
-    
+
     if( layers[n_layers-1]->size != final_module->input_size )
         PLERROR("StackedAutoassociatorsNet::build_costs() - \n"
-                "final_module should have an input_size of %d.\n", 
+                "final_module should have an input_size of %d.\n",
                 layers[n_layers-1]->size);
-    
+
     if( final_module->output_size != final_cost->input_size )
         PLERROR("StackedAutoassociatorsNet::build_costs() - \n"
-                "final_module should have an output_size of %d.\n", 
+                "final_module should have an output_size of %d.\n",
                 final_cost->input_size);
 
     final_module->setLearningRate( fine_tuning_learning_rate );
@@ -585,14 +585,14 @@
     if(targetsize_ != 1)
         PLERROR("StackedAutoassociatorsNet::build_costs() - \n"
                 "target size of %d is not supported.\n", targetsize_);
-    
+
     if(partial_costs)
     {
 
         if( correlation_connections.length() != 0 )
             PLERROR("StackedAutoassociatorsNet::build_costs() - \n"
                     "correlation_connections cannot be used with partial costs.");
-            
+
         partial_costs_positions.resize(partial_costs.length());
         partial_costs_positions.clear();
         for(int i=0; i<partial_costs.length(); i++)
@@ -602,7 +602,7 @@
                         "partial_costs[%i] should be provided.\n",i);
             if( layers[i+1]->size != partial_costs[i]->input_size )
                 PLERROR("StackedAutoassociatorsNet::build_costs() - \n"
-                        "partial_costs[%i] should have an input_size of %d.\n", 
+                        "partial_costs[%i] should have an input_size of %d.\n",
                         i,layers[i+1]->size);
             if(i==0)
                 partial_costs_positions[i] = n_layers-1;
@@ -705,13 +705,13 @@
 
     for( int i=0 ; i<n_layers ; i++ )
         layers[i]->forget();
-    
+
     for( int i=0 ; i<n_layers-1 ; i++ )
     {
         connections[i]->forget();
         reconstruction_connections[i]->forget();
     }
-    
+
     final_module->forget();
     final_cost->forget();
 
@@ -720,16 +720,16 @@
             partial_costs[i]->forget();
 
     if(correlation_connections.length() != 0)
-    {        
+    {
         for( int i=0 ; i<n_layers-1 ; i++)
         {
             correlation_connections[i]->forget();
             correlation_layers[i]->forget();
-        }        
+        }
     }
 
     if(direct_connections.length() != 0)
-    {        
+    {
         for( int i=0 ; i<n_layers-1 ; i++)
             direct_connections[i]->forget();
     }
@@ -762,7 +762,7 @@
         train_stats = new VecStatsCollector();
         train_stats->setFieldNames(getTrainCostNames());
     }
-    
+
     // clear stats of previous epoch
     train_stats->forget();
 
@@ -838,8 +838,8 @@
             {
                 if( !fast_exact_is_equal( greedy_decrease_ct , 0 ) )
                 {
-                    lr = greedy_learning_rate/(1 + greedy_decrease_ct 
-                                               * (*this_stage)); 
+                    lr = greedy_learning_rate/(1 + greedy_decrease_ct
+                                               * (*this_stage));
                     layers[i]->setLearningRate( lr );
                     connections[i]->setLearningRate( lr );
                     reconstruction_connections[i]->setLearningRate( lr );
@@ -873,13 +873,13 @@
 //                PLERROR("StackedAutoassociatorsNet::train()"
 //                        " - \n"
 //                        "cannot use unsupervised fine-tuning with correlation connections.\n");
-            
+
             MODULE_LOG << "Unsupervised fine-tuning all parameters, ";
             MODULE_LOG << "by gradient descent" << endl;
             MODULE_LOG << "  unsupervised_stage = " << unsupervised_stage << endl;
-            MODULE_LOG << "  unsupervised_nstages = " << 
+            MODULE_LOG << "  unsupervised_nstages = " <<
                 unsupervised_nstages << endl;
-            MODULE_LOG << "  unsupervised_fine_tuning_learning_rate = " << 
+            MODULE_LOG << "  unsupervised_fine_tuning_learning_rate = " <<
                 unsupervised_fine_tuning_learning_rate << endl;
 
             init_stage = unsupervised_stage;
@@ -894,13 +894,13 @@
             fine_tuning_reconstruction_expectation_gradients.resize( n_layers );
             for( int i=0 ; i<n_layers ; i++ )
             {
-                fine_tuning_reconstruction_activations[i].resize( 
+                fine_tuning_reconstruction_activations[i].resize(
                     layers[i]->size );
-                fine_tuning_reconstruction_expectations[i].resize( 
+                fine_tuning_reconstruction_expectations[i].resize(
                     layers[i]->size );
-                fine_tuning_reconstruction_activation_gradients[i].resize( 
+                fine_tuning_reconstruction_activation_gradients[i].resize(
                     layers[i]->size );
-                fine_tuning_reconstruction_expectation_gradients[i].resize( 
+                fine_tuning_reconstruction_expectation_gradients[i].resize(
                     layers[i]->size );
             }
 
@@ -908,7 +908,7 @@
             {
                 masked_autoassociator_expectations.resize( n_layers-1 );
                 autoassociator_expectation_indices.resize( n_layers-1 );
-                
+
                 for( int i=0 ; i<n_layers-1 ; i++ )
                 {
                     masked_autoassociator_expectations[i].resize( layers[i]->size );
@@ -924,9 +924,9 @@
             {
                 sample = unsupervised_stage % nsamples;
                 if( !fast_exact_is_equal( unsupervised_fine_tuning_decrease_ct, 0. ) )
-                    setLearningRate( 
+                    setLearningRate(
                         unsupervised_fine_tuning_learning_rate
-                        / (1. + unsupervised_fine_tuning_decrease_ct 
+                        / (1. + unsupervised_fine_tuning_decrease_ct
                            * unsupervised_stage ) );
 
                 train_set->getExample( sample, input, target, weight );
@@ -945,7 +945,7 @@
             MODULE_LOG << "Fine-tuning all parameters, by gradient descent" << endl;
             MODULE_LOG << "  stage = " << stage << endl;
             MODULE_LOG << "  nstages = " << nstages << endl;
-            MODULE_LOG << "  fine_tuning_learning_rate = " << 
+            MODULE_LOG << "  fine_tuning_learning_rate = " <<
                 fine_tuning_learning_rate << endl;
 
             init_stage = stage;
@@ -971,7 +971,7 @@
                     pb->update( stage - init_stage + 1 );
             }
         }
-    
+
         train_stats->finalize();
         MODULE_LOG << "  train costs = " << train_stats->getMean() << endl;
 
@@ -979,9 +979,9 @@
         if(stage > 0)
             currently_trained_layer = n_layers;
         else
-        {            
+        {
             currently_trained_layer = n_layers-1;
-            while(currently_trained_layer>1 
+            while(currently_trained_layer>1
                   && greedy_stages[currently_trained_layer-1] <= 0)
                 currently_trained_layer--;
         }
@@ -993,24 +993,24 @@
             PLERROR("StackedAutoassociatorsNet::train()"
                     " - \n"
                     "unsupervised fine-tuning with online=true is not implemented.\n");
-        
+
         // Train all layers simultaneously AND fine-tuning as well!
         if( stage < nstages )
         {
 
             MODULE_LOG << "Training all layers greedy layer-wise AND "
-                       << "fine-tuning all parameters, by gradient descent" 
+                       << "fine-tuning all parameters, by gradient descent"
                        << endl;
             MODULE_LOG << "  stage = " << stage << endl;
             MODULE_LOG << "  nstages = " << nstages << endl;
-            MODULE_LOG << "  fine_tuning_learning_rate = " 
+            MODULE_LOG << "  fine_tuning_learning_rate = "
                        << fine_tuning_learning_rate << endl;
-            MODULE_LOG << "  greedy_learning_rate = " 
+            MODULE_LOG << "  greedy_learning_rate = "
                        << greedy_learning_rate << endl;
 
             init_stage = stage;
             if( report_progress && stage < nstages )
-                pb = new ProgressBar( 
+                pb = new ProgressBar(
                     "Greedy layer-wise training AND fine-tuning parameters of "
                                       + classname(),
                                       nstages - init_stage );
@@ -1052,16 +1052,16 @@
             {
                 masked_autoassociator_input << expectations[i];
                 for( int j=0 ; j < round(fraction_of_masked_inputs*layers[index]->size) ; j++)
-                    masked_autoassociator_input[ autoassociator_input_indices[j] ] = 0; 
+                    masked_autoassociator_input[ autoassociator_input_indices[j] ] = 0;
                 connections[i]->fprop( masked_autoassociator_input, correlation_activations[i] );
             }
             else
                 connections[i]->fprop( expectations[i], correlation_activations[i] );
             layers[i+1]->fprop( correlation_activations[i],
                                 correlation_expectations[i] );
-            correlation_connections[i]->fprop( correlation_expectations[i], 
+            correlation_connections[i]->fprop( correlation_expectations[i],
                                                activations[i+1] );
-            correlation_layers[i]->fprop( activations[i+1], 
+            correlation_layers[i]->fprop( activations[i+1],
                                           expectations[i+1] );
         }
     }
@@ -1073,7 +1073,7 @@
             {
                 masked_autoassociator_input << expectations[i];
                 for( int j=0 ; j < round(fraction_of_masked_inputs*layers[index]->size) ; j++)
-                    masked_autoassociator_input[ autoassociator_input_indices[j] ] = 0; 
+                    masked_autoassociator_input[ autoassociator_input_indices[j] ] = 0;
                 connections[i]->fprop( masked_autoassociator_input, activations[i+1] );
             }
             else
@@ -1123,52 +1123,52 @@
     if(direct_connections.length() != 0)
     {
         if( fraction_of_masked_inputs > 0 )
-            direct_connections[ index ]->fprop( masked_autoassociator_input, 
+            direct_connections[ index ]->fprop( masked_autoassociator_input,
                                                 direct_activations );
         else
-            direct_connections[ index ]->fprop( expectations[ index ], 
-                                                direct_activations );            
+            direct_connections[ index ]->fprop( expectations[ index ],
+                                                direct_activations );
         direct_and_reconstruction_activations.clear();
         direct_and_reconstruction_activations += direct_activations;
         direct_and_reconstruction_activations += reconstruction_activations;
 
         layers[ index ]->fprop( direct_and_reconstruction_activations,
                                 layers[ index ]->expectation);
-        
+
         layers[ index ]->activation << direct_and_reconstruction_activations;
         //layers[ index ]->expectation_is_up_to_date = true;  // Won't work for certain RBMLayers
         layers[ index ]->setExpectationByRef( layers[ index ]->expectation );
         train_costs[index] = layers[ index ]->fpropNLL(expectations[index]);
-        
+
         layers[ index ]->bpropNLL(expectations[index], train_costs[index],
                                   direct_and_reconstruction_activation_gradients);
 
         layers[ index ]->update(direct_and_reconstruction_activation_gradients);
 
         if( fraction_of_masked_inputs > 0 )
-            direct_connections[ index ]->bpropUpdate( 
+            direct_connections[ index ]->bpropUpdate(
                 masked_autoassociator_input,
                 direct_activations,
                 reconstruction_expectation_gradients, // Will be overwritten later
                 direct_and_reconstruction_activation_gradients);
         else
-            direct_connections[ index ]->bpropUpdate( 
+            direct_connections[ index ]->bpropUpdate(
                 expectations[ index ],
                 direct_activations,
                 reconstruction_expectation_gradients, // Will be overwritten later
                 direct_and_reconstruction_activation_gradients);
-            
-        reconstruction_connections[ index ]->bpropUpdate( 
-            expectations[ index + 1], 
-            reconstruction_activations, 
-            reconstruction_expectation_gradients, 
+
+        reconstruction_connections[ index ]->bpropUpdate(
+            expectations[ index + 1],
+            reconstruction_activations,
+            reconstruction_expectation_gradients,
             direct_and_reconstruction_activation_gradients);
     }
     else
     {
         layers[ index ]->fprop( reconstruction_activations,
                                 layers[ index ]->expectation);
-        
+
         layers[ index ]->activation << reconstruction_activations;
         //layers[ index ]->expectation_is_up_to_date = true;
         layers[ index ]->setExpectationByRef( layers[ index ]->expectation );
@@ -1180,7 +1180,7 @@
 
         if(reconstruct_hidden)
         {
-            connections[ index ]->fprop( layers[ index ]->expectation, 
+            connections[ index ]->fprop( layers[ index ]->expectation,
                                          hidden_reconstruction_activations );
             layers[ index+1 ]->fprop( hidden_reconstruction_activations,
                 layers[ index+1 ]->expectation );
@@ -1193,20 +1193,20 @@
             layers[ index+1 ]->bpropNLL(expectations[index+1], hid_rec_err,
                                         hidden_reconstruction_activation_gradients);
             layers[ index+1 ]->update(hidden_reconstruction_activation_gradients);
-            
-            connections[ index ]->bpropUpdate( 
-                layers[ index ]->expectation, 
+
+            connections[ index ]->bpropUpdate(
+                layers[ index ]->expectation,
                 hidden_reconstruction_activations,
                 reconstruction_expectation_gradients_from_hid_rec,
                 hidden_reconstruction_activation_gradients);
 
-            layers[ index ]->bpropUpdate( 
+            layers[ index ]->bpropUpdate(
                 reconstruction_activations,
                 layers[ index ]->expectation,
                 reconstruction_activation_gradients_from_hid_rec,
                 reconstruction_expectation_gradients_from_hid_rec);
         }
-        
+
         layers[ index ]->update(reconstruction_activation_gradients);
 
         if(reconstruct_hidden)
@@ -1214,20 +1214,20 @@
                 reconstruction_activation_gradients_from_hid_rec;
 
         // // This is a bad update! Propagates gradient through sigmoid again!
-        // layers[ index ]->bpropUpdate( reconstruction_activations, 
+        // layers[ index ]->bpropUpdate( reconstruction_activations,
         //                                   layers[ index ]->expectation,
         //                                   reconstruction_activation_gradients,
         //                                   reconstruction_expectation_gradients);
-        
-        reconstruction_connections[ index ]->bpropUpdate( 
-            expectations[ index + 1], 
-            reconstruction_activations, 
-            reconstruction_expectation_gradients, 
+
+        reconstruction_connections[ index ]->bpropUpdate(
+            expectations[ index + 1],
+            reconstruction_activations,
+            reconstruction_expectation_gradients,
             reconstruction_activation_gradients);
 
     }
 
-    
+
     if(!fast_exact_is_equal(l1_neuron_decay,0))
     {
         // Compute L1 penalty gradient on neurons
@@ -1256,26 +1256,26 @@
             reconstruction_expectation_gradients
             );
 
-        correlation_connections[ index ]->bpropUpdate( 
+        correlation_connections[ index ]->bpropUpdate(
             correlation_expectations[ index ],
             activations[ index+1 ],
-            correlation_expectation_gradients[ index ], 
+            correlation_expectation_gradients[ index ],
             reconstruction_activation_gradients);
-        
-        layers[ index+1 ]->bpropUpdate( 
+
+        layers[ index+1 ]->bpropUpdate(
             correlation_activations[ index ],
             correlation_expectations[ index ],
             correlation_activation_gradients [ index ],
-            correlation_expectation_gradients [ index ]);    
-        
+            correlation_expectation_gradients [ index ]);
+
         if( fraction_of_masked_inputs > 0 )
-            connections[ index ]->bpropUpdate( 
+            connections[ index ]->bpropUpdate(
                 masked_autoassociator_input,
                 correlation_activations[ index ],
                 reconstruction_expectation_gradients, //reused
                 correlation_activation_gradients [ index ]);
         else
-            connections[ index ]->bpropUpdate( 
+            connections[ index ]->bpropUpdate(
                 expectations[ index ],
                 correlation_activations[ index ],
                 reconstruction_expectation_gradients, //reused
@@ -1286,17 +1286,17 @@
         layers[ index+1 ]->bpropUpdate( activations[ index + 1 ],
                                         expectations[ index + 1 ],
                                         // reused
-                                        reconstruction_activation_gradients, 
-                                        reconstruction_expectation_gradients);    
-        
+                                        reconstruction_activation_gradients,
+                                        reconstruction_expectation_gradients);
+
         if( fraction_of_masked_inputs > 0 )
-            connections[ index ]->bpropUpdate( 
+            connections[ index ]->bpropUpdate(
                 masked_autoassociator_input,
                 activations[ index + 1 ],
                 reconstruction_expectation_gradients, //reused
                 reconstruction_activation_gradients);
         else
-            connections[ index ]->bpropUpdate( 
+            connections[ index ]->bpropUpdate(
                 expectations[ index ],
                 activations[ index + 1 ],
                 reconstruction_expectation_gradients, //reused
@@ -1305,7 +1305,7 @@
 
 }
 
-void StackedAutoassociatorsNet::unsupervisedFineTuningStep( const Vec& input, 
+void StackedAutoassociatorsNet::unsupervisedFineTuningStep( const Vec& input,
                                                             const Vec& target,
                                                             Vec& train_costs )
 {
@@ -1318,21 +1318,21 @@
         {
             for( int i=0; i<autoassociator_expectation_indices.length(); i++ )
                 random_gen->shuffleElements(autoassociator_expectation_indices[i]);
-            
+
             for( int i=0 ; i<n_layers-1; i++ )
             {
                 masked_autoassociator_expectations[i] << expectations[i];
                 if( !(mask_input_layer_only_in_unsupervised_fine_tuning && i > 0) )
                     for( int j=0 ; j < round(fraction_of_masked_inputs*layers[i]->size) ; j++)
-                        masked_autoassociator_expectations[i][ autoassociator_expectation_indices[i][j] ] = 0; 
-                
-                connections[i]->fprop( masked_autoassociator_expectations[i], 
+                        masked_autoassociator_expectations[i][ autoassociator_expectation_indices[i][j] ] = 0;
+
+                connections[i]->fprop( masked_autoassociator_expectations[i],
                                        correlation_activations[i] );
                 layers[i+1]->fprop( correlation_activations[i],
                                     correlation_expectations[i] );
-                correlation_connections[i]->fprop( correlation_expectations[i], 
+                correlation_connections[i]->fprop( correlation_expectations[i],
                                                    activations[i+1] );
-                correlation_layers[i]->fprop( activations[i+1], 
+                correlation_layers[i]->fprop( activations[i+1],
                                               expectations[i+1] );
             }
         }
@@ -1343,9 +1343,9 @@
                 connections[i]->fprop( expectations[i], correlation_activations[i]);
                 layers[i+1]->fprop( correlation_activations[i],
                                     correlation_expectations[i] );
-                correlation_connections[i]->fprop( correlation_expectations[i], 
+                correlation_connections[i]->fprop( correlation_expectations[i],
                                                    activations[i+1] );
-                correlation_layers[i]->fprop( activations[i+1], 
+                correlation_layers[i]->fprop( activations[i+1],
                                               expectations[i+1] );
             }
         }
@@ -1356,15 +1356,15 @@
         {
             for( int i=0; i<autoassociator_expectation_indices.length(); i++ )
                 random_gen->shuffleElements(autoassociator_expectation_indices[i]);
-            
+
             for( int i=0 ; i<n_layers-1; i++ )
             {
                 masked_autoassociator_expectations[i] << expectations[i];
                 if( !(mask_input_layer_only_in_unsupervised_fine_tuning && i > 0) )
                     for( int j=0 ; j < round(fraction_of_masked_inputs*layers[i]->size) ; j++)
-                        masked_autoassociator_expectations[i][ autoassociator_expectation_indices[i][j] ] = 0; 
-                
-                connections[i]->fprop( masked_autoassociator_expectations[i], 
+                        masked_autoassociator_expectations[i][ autoassociator_expectation_indices[i][j] ] = 0;
+
+                connections[i]->fprop( masked_autoassociator_expectations[i],
                                        activations[i+1] );
                 layers[i+1]->fprop(activations[i+1],expectations[i+1]);
             }
@@ -1378,23 +1378,23 @@
             }
         }
     }
-    fine_tuning_reconstruction_expectations[ n_layers-1 ] << 
+    fine_tuning_reconstruction_expectations[ n_layers-1 ] <<
         expectations[ n_layers-1 ];
 
     for( int i=n_layers-2 ; i>=0; i-- )
     {
-        reconstruction_connections[i]->fprop( 
-            fine_tuning_reconstruction_expectations[i+1], 
+        reconstruction_connections[i]->fprop(
+            fine_tuning_reconstruction_expectations[i+1],
             fine_tuning_reconstruction_activations[i] );
         layers[i]->fprop( fine_tuning_reconstruction_activations[i],
                           fine_tuning_reconstruction_expectations[i]);
     }
-    
+
     layers[ 0 ]->setExpectation( fine_tuning_reconstruction_expectations[ 0 ] );
     layers[ 0 ]->activation << fine_tuning_reconstruction_activations[0];
     real rec_err = layers[ 0 ]->fpropNLL( input );
     train_costs[n_layers-1] = rec_err;
-    
+
     layers[ 0 ]->bpropNLL( input, rec_err,
                            fine_tuning_reconstruction_activation_gradients[ 0 ] );
 
@@ -1407,16 +1407,16 @@
                                     fine_tuning_reconstruction_expectations[i],
                                     fine_tuning_reconstruction_activation_gradients[i],
                                     fine_tuning_reconstruction_expectation_gradients[i]);
-        reconstruction_connections[i]->bpropUpdate( 
-            fine_tuning_reconstruction_expectations[i+1], 
+        reconstruction_connections[i]->bpropUpdate(
+            fine_tuning_reconstruction_expectations[i+1],
             fine_tuning_reconstruction_activations[i],
-            fine_tuning_reconstruction_expectation_gradients[i+1], 
+            fine_tuning_reconstruction_expectation_gradients[i+1],
             fine_tuning_reconstruction_activation_gradients[i]);
     }
 
-    expectation_gradients[ n_layers-1 ] << 
+    expectation_gradients[ n_layers-1 ] <<
         fine_tuning_reconstruction_expectation_gradients[ n_layers-1 ];
-    
+
     for( int i=n_layers-2 ; i>=0; i-- )
     {
 
@@ -1445,27 +1445,27 @@
                 activation_gradients[ i + 1 ],
                 expectation_gradients[ i + 1 ]
                 );
-            
-            correlation_connections[ i ]->bpropUpdate( 
+
+            correlation_connections[ i ]->bpropUpdate(
                 correlation_expectations[ i ],
                 activations[ i + 1 ],
-                correlation_expectation_gradients[ i ], 
+                correlation_expectation_gradients[ i ],
                 activation_gradients[ i + 1 ] );
-            
-            layers[ i + 1 ]->bpropUpdate( 
+
+            layers[ i + 1 ]->bpropUpdate(
                 correlation_activations[ i ],
                 correlation_expectations[ i ],
                 correlation_activation_gradients [ i ],
-                correlation_expectation_gradients [ i ]);    
-            
+                correlation_expectation_gradients [ i ]);
+
             if( fraction_of_masked_inputs > 0 )
-                connections[ i ]->bpropUpdate( 
+                connections[ i ]->bpropUpdate(
                     masked_autoassociator_expectations[ i ],
                     correlation_activations[ i ],
-                    expectation_gradients[i], 
+                    expectation_gradients[i],
                     correlation_activation_gradients [ i ]);
             else
-                connections[ i ]->bpropUpdate( 
+                connections[ i ]->bpropUpdate(
                     expectations[ i ],
                     correlation_activations[ i ],
                     expectation_gradients[i],
@@ -1478,11 +1478,11 @@
                 activations[i+1],expectations[i+1],
                 activation_gradients[i+1],expectation_gradients[i+1]);
             if( fraction_of_masked_inputs > 0 )
-                connections[i]->bpropUpdate( 
+                connections[i]->bpropUpdate(
                     masked_autoassociator_expectations[i], activations[i+1],
                     expectation_gradients[i], activation_gradients[i+1] );
             else
-                connections[i]->bpropUpdate( 
+                connections[i]->bpropUpdate(
                     expectations[i], activations[i+1],
                     expectation_gradients[i], activation_gradients[i+1] );
         }
@@ -1502,9 +1502,9 @@
             connections[i]->fprop( expectations[i], correlation_activations[i] );
             layers[i+1]->fprop( correlation_activations[i],
                                 correlation_expectations[i] );
-            correlation_connections[i]->fprop( correlation_expectations[i], 
+            correlation_connections[i]->fprop( correlation_expectations[i],
                                                activations[i+1] );
-            correlation_layers[i]->fprop( activations[i+1], 
+            correlation_layers[i]->fprop( activations[i+1],
                                           expectations[i+1] );
         }
     }
@@ -1537,13 +1537,13 @@
     {
         for( int i=n_layers-1 ; i>0 ; i-- )
         {
-            correlation_layers[i-1]->bpropUpdate( 
+            correlation_layers[i-1]->bpropUpdate(
                 activations[i],
                 expectations[i],
                 activation_gradients[i],
                 expectation_gradients[i] );
 
-            correlation_connections[i-1]->bpropUpdate( 
+            correlation_connections[i-1]->bpropUpdate(
                 correlation_expectations[i-1],
                 activations[i],
                 correlation_expectation_gradients[i-1],
@@ -1553,7 +1553,7 @@
                                     correlation_expectations[i-1],
                                     correlation_activation_gradients[i-1],
                                     correlation_expectation_gradients[i-1] );
-            
+
             connections[i-1]->bpropUpdate( expectations[i-1],
                                            correlation_activations[i-1],
                                            expectation_gradients[i-1],
@@ -1568,16 +1568,16 @@
                                     expectations[i],
                                     activation_gradients[i],
                                     expectation_gradients[i] );
-            
+
             connections[i-1]->bpropUpdate( expectations[i-1],
                                            activations[i],
                                            expectation_gradients[i-1],
                                            activation_gradients[i] );
-        }        
+        }
     }
 }
 
-void StackedAutoassociatorsNet::onlineStep( const Vec& input, 
+void StackedAutoassociatorsNet::onlineStep( const Vec& input,
                                             const Vec& target,
                                             Vec& train_costs )
 {
@@ -1592,9 +1592,9 @@
             connections[i]->fprop( expectations[i], correlation_activations[i] );
             layers[i+1]->fprop( correlation_activations[i],
                                 correlation_expectations[i] );
-            correlation_connections[i]->fprop( correlation_expectations[i], 
+            correlation_connections[i]->fprop( correlation_expectations[i],
                                                activations[i+1] );
-            correlation_layers[i]->fprop( activations[i+1], 
+            correlation_layers[i]->fprop( activations[i+1],
                                           expectations[i+1] );
         }
     }
@@ -1604,57 +1604,61 @@
         {
             connections[i]->fprop( expectations[i], activations[i+1] );
             layers[i+1]->fprop(activations[i+1],expectations[i+1]);
-            
+
             if( partial_costs.length() != 0 && partial_costs[ i ] )
             {
                 // Set learning rates
-
-                if( !fast_exact_is_equal( greedy_decrease_ct , 0 ) )
-                    lr = greedy_learning_rate / 
-                        (1 + greedy_decrease_ct * stage);
+                if( !fast_exact_is_equal(fine_tuning_decrease_ct , 0 ) )
+                    lr = fine_tuning_learning_rate /
+                        (1 + fine_tuning_decrease_ct * stage);
                 else
-                    lr = greedy_learning_rate;
+                    lr = fine_tuning_learning_rate;
 
                 partial_costs[ i ]->setLearningRate( lr );
+                /* No, learning rate should already be OK in layers and
+                 * connections
                 layers[ i+1 ]->setLearningRate( lr );
                 connections[ i ]->setLearningRate( lr );
+                */
 
                 partial_costs[ i ]->fprop( expectations[ i + 1],
                                            target, partial_cost_value );
-                
+
                 // Update partial cost (might contain some weights for example)
-                partial_costs[ i ]->bpropUpdate( 
+                partial_costs[ i ]->bpropUpdate(
                     expectations[ i + 1 ],
                     target, partial_cost_value[0],
                     expectation_gradients[ i + 1 ]
                     );
-                
+
                 train_costs.subVec(partial_costs_positions[i]+1,
-                                   partial_cost_value.length()) 
+                                   partial_cost_value.length())
                     << partial_cost_value;
-                
+
                 if( !fast_exact_is_equal( partial_costs_weights.length(), 0 ) )
                     expectation_gradients[ i + 1 ] *= partial_costs_weights[i];
-                
+
                 // Update hidden layer bias and weights
                 layers[ i+1 ]->bpropUpdate( activations[ i + 1 ],
                                             expectations[ i + 1 ],
                                             activation_gradients[ i + 1 ],
                                             expectation_gradients[ i + 1 ] );
-                
+
                 connections[ i ]->bpropUpdate( expectations[ i ],
                                                activations[ i + 1 ],
                                                expectation_gradients[ i ],
                                                activation_gradients[ i + 1 ] );
 
+                /* no need
                 if( !fast_exact_is_equal( fine_tuning_decrease_ct , 0 ) )
-                    lr = fine_tuning_learning_rate / 
+                    lr = fine_tuning_learning_rate /
                         (1 + fine_tuning_decrease_ct * stage);
                 else
                     lr = fine_tuning_learning_rate;
 
                 layers[ i+1 ]->setLearningRate( lr );
                 connections[ i ]->setLearningRate( lr );
+                */
             }
         }
     }
@@ -1699,13 +1703,13 @@
     // Backpropagate unsupervised gradient, layer-wise
     for( int i=n_layers-1 ; i>0 ; i-- )
     {
-        reconstruction_connections[ i-1 ]->fprop( 
+        reconstruction_connections[ i-1 ]->fprop(
             expectations[ i ],
             reconstruction_activations);
 
         layers[ i-1 ]->fprop( reconstruction_activations,
                                 layers[ i-1 ]->expectation);
-        
+
         layers[ i-1 ]->activation << reconstruction_activations;
         //layers[ i-1 ]->expectation_is_up_to_date = true;
         layers[ i-1 ]->setExpectationByRef( layers[ i-1 ]->expectation );
@@ -1717,10 +1721,10 @@
 
         layers[ i-1 ]->update(reconstruction_activation_gradients);
 
-        reconstruction_connections[ i-1 ]->bpropUpdate( 
-            expectations[ i ], 
-            reconstruction_activations, 
-            reconstruction_expectation_gradients, 
+        reconstruction_connections[ i-1 ]->bpropUpdate(
+            expectations[ i ],
+            reconstruction_activations,
+            reconstruction_expectation_gradients,
             reconstruction_activation_gradients);
 
         if(!fast_exact_is_equal(l1_neuron_decay,0))
@@ -1742,13 +1746,13 @@
 
         if( correlation_connections.length() != 0 )
         {
-            correlation_layers[i-1]->bpropUpdate( 
+            correlation_layers[i-1]->bpropUpdate(
                 activations[i],
                 expectations[i],
                 reconstruction_activation_gradients,
                 reconstruction_expectation_gradients );
-            
-            correlation_connections[i-1]->bpropUpdate( 
+
+            correlation_connections[i-1]->bpropUpdate(
                 correlation_expectations[i-1],
                 activations[i],
                 correlation_expectation_gradients[i-1],
@@ -1758,7 +1762,7 @@
                                     correlation_expectations[i-1],
                                     correlation_activation_gradients[i-1],
                                     correlation_expectation_gradients[i-1] );
-            
+
             connections[i-1]->bpropUpdate( expectations[i-1],
                                            correlation_activations[i-1],
                                            reconstruction_expectation_gradients,
@@ -1766,13 +1770,13 @@
         }
         else
         {
-            layers[i]->bpropUpdate( 
+            layers[i]->bpropUpdate(
                 activations[i],
                 expectations[i],
                 reconstruction_activation_gradients,
                 reconstruction_expectation_gradients );
-            
-            connections[i-1]->bpropUpdate( 
+
+            connections[i-1]->bpropUpdate(
                 expectations[i-1],
                 activations[i],
                 reconstruction_expectation_gradients,
@@ -1783,7 +1787,7 @@
     // Put back fine-tuning learning rate
     // Set learning rates
     if( !fast_exact_is_equal( fine_tuning_decrease_ct , 0 ) )
-        lr = fine_tuning_learning_rate 
+        lr = fine_tuning_learning_rate
             / (1 + fine_tuning_decrease_ct * stage) ;
     else
         lr = fine_tuning_learning_rate ;
@@ -1807,13 +1811,13 @@
     {
         for( int i=n_layers-1 ; i>0 ; i-- )
         {
-            correlation_layers[i-1]->bpropUpdate( 
+            correlation_layers[i-1]->bpropUpdate(
                 activations[i],
                 expectations[i],
                 activation_gradients[i],
                 expectation_gradients[i] );
 
-            correlation_connections[i-1]->bpropUpdate( 
+            correlation_connections[i-1]->bpropUpdate(
                 correlation_expectations[i-1],
                 activations[i],
                 correlation_expectation_gradients[i-1],
@@ -1823,8 +1827,8 @@
                                     correlation_expectations[i-1],
                                     correlation_activation_gradients[i-1],
                                     correlation_expectation_gradients[i-1] );
-            
-            connections[i-1]->bpropUpdate( 
+
+            connections[i-1]->bpropUpdate(
                 expectations[i-1],
                 correlation_activations[i-1],
                 expectation_gradients[i-1],
@@ -1839,12 +1843,12 @@
                                     expectations[i],
                                     activation_gradients[i],
                                     expectation_gradients[i] );
-            
+
             connections[i-1]->bpropUpdate( expectations[i-1],
                                            activations[i],
                                            expectation_gradients[i-1],
                                            activation_gradients[i] );
-        }        
+        }
     }
 }
 
@@ -1861,14 +1865,14 @@
             connections[i]->fprop( expectations[i], correlation_activations[i] );
             layers[i+1]->fprop( correlation_activations[i],
                                 correlation_expectations[i] );
-            correlation_connections[i]->fprop( correlation_expectations[i], 
+            correlation_connections[i]->fprop( correlation_expectations[i],
                                                activations[i+1] );
-            correlation_layers[i]->fprop( activations[i+1], 
+            correlation_layers[i]->fprop( activations[i+1],
                                           expectations[i+1] );
         }
     }
     else
-    {   
+    {
         for(int i=0 ; i<currently_trained_layer-1 ; i++ )
         {
             connections[i]->fprop( expectations[i], activations[i+1] );
@@ -1880,33 +1884,33 @@
     {
         if(correlation_connections.length() != 0)
         {
-            connections[currently_trained_layer-1]->fprop( 
-                expectations[currently_trained_layer-1], 
+            connections[currently_trained_layer-1]->fprop(
+                expectations[currently_trained_layer-1],
                 correlation_activations[currently_trained_layer-1] );
 
             layers[currently_trained_layer]->fprop(
                 correlation_activations[currently_trained_layer-1],
                 correlation_expectations[currently_trained_layer-1] );
 
-            correlation_connections[currently_trained_layer-1]->fprop( 
-                correlation_expectations[currently_trained_layer-1], 
+            correlation_connections[currently_trained_layer-1]->fprop(
+                correlation_expectations[currently_trained_layer-1],
                 activations[currently_trained_layer] );
 
-            correlation_layers[currently_trained_layer-1]->fprop( 
-                activations[currently_trained_layer], 
+            correlation_layers[currently_trained_layer-1]->fprop(
+                activations[currently_trained_layer],
                 output );
         }
         else
         {
-            connections[currently_trained_layer-1]->fprop( 
-                expectations[currently_trained_layer-1], 
+            connections[currently_trained_layer-1]->fprop(
+                expectations[currently_trained_layer-1],
                 activations[currently_trained_layer] );
             layers[currently_trained_layer]->fprop(
                 activations[currently_trained_layer],
                 output);
         }
     }
-    else        
+    else
         final_module->fprop( expectations[ currently_trained_layer - 1],
                              output );
 }
@@ -1927,15 +1931,15 @@
                                                     reconstruction_activations);
             if( direct_connections.length() != 0 )
             {
-                direct_connections[ i ]->fprop( 
-                    expectations[ i ], 
+                direct_connections[ i ]->fprop(
+                    expectations[ i ],
                     direct_activations );
                 reconstruction_activations += direct_activations;
             }
 
             layers[ i ]->fprop( reconstruction_activations,
                                 layers[ i ]->expectation);
-            
+
             layers[ i ]->activation << reconstruction_activations;
             //layers[ i ]->expectation_is_up_to_date = true;
             layers[ i ]->setExpectationByRef( layers[ i ]->expectation );
@@ -1947,7 +1951,7 @@
                 partial_costs[ i ]->fprop( expectations[ i + 1],
                                            target, partial_cost_value );
                 costs.subVec(partial_costs_positions[i],
-                             partial_cost_value.length()) << 
+                             partial_cost_value.length()) <<
                     partial_cost_value;
             }
         }
@@ -1955,50 +1959,50 @@
 
     if( currently_trained_layer<n_layers )
     {
-        reconstruction_connections[ currently_trained_layer-1 ]->fprop( 
+        reconstruction_connections[ currently_trained_layer-1 ]->fprop(
             output,
             reconstruction_activations);
         if( direct_connections.length() != 0 )
         {
-            direct_connections[ currently_trained_layer-1 ]->fprop( 
-                expectations[ currently_trained_layer-1 ], 
+            direct_connections[ currently_trained_layer-1 ]->fprop(
+                expectations[ currently_trained_layer-1 ],
                 direct_activations );
             reconstruction_activations += direct_activations;
         }
-        layers[ currently_trained_layer-1 ]->fprop( 
+        layers[ currently_trained_layer-1 ]->fprop(
             reconstruction_activations,
             layers[ currently_trained_layer-1 ]->expectation);
-        
-        layers[ currently_trained_layer-1 ]->activation << 
+
+        layers[ currently_trained_layer-1 ]->activation <<
             reconstruction_activations;
         //layers[ currently_trained_layer-1 ]->expectation_is_up_to_date = true;
-        layers[ currently_trained_layer-1 ]->setExpectationByRef( 
+        layers[ currently_trained_layer-1 ]->setExpectationByRef(
             layers[ currently_trained_layer-1 ]->expectation );
-        costs[ currently_trained_layer-1 ] = 
+        costs[ currently_trained_layer-1 ] =
             layers[ currently_trained_layer-1 ]->fpropNLL(
                 expectations[ currently_trained_layer-1 ]);
 
         if(reconstruct_hidden)
         {
-            connections[ currently_trained_layer-1 ]->fprop( 
-                layers[ currently_trained_layer-1 ]->expectation, 
+            connections[ currently_trained_layer-1 ]->fprop(
+                layers[ currently_trained_layer-1 ]->expectation,
                 hidden_reconstruction_activations );
-            layers[ currently_trained_layer ]->fprop( 
+            layers[ currently_trained_layer ]->fprop(
                 hidden_reconstruction_activations,
                 layers[ currently_trained_layer ]->expectation );
-            layers[ currently_trained_layer ]->activation << 
+            layers[ currently_trained_layer ]->activation <<
                 hidden_reconstruction_activations;
             //layers[ currently_trained_layer ]->expectation_is_up_to_date = true;
-            layers[ currently_trained_layer ]->setExpectationByRef( 
+            layers[ currently_trained_layer ]->setExpectationByRef(
                 layers[ currently_trained_layer ]->expectation );
-            costs[ currently_trained_layer-1 ] += 
+            costs[ currently_trained_layer-1 ] +=
                 layers[ currently_trained_layer ]->fpropNLL(
                     output);
         }
 
         if( partial_costs && partial_costs[ currently_trained_layer-1 ] )
         {
-            partial_costs[ currently_trained_layer-1 ]->fprop( 
+            partial_costs[ currently_trained_layer-1 ]->fprop(
                 output,
                 target, partial_cost_value );
             costs.subVec(partial_costs_positions[currently_trained_layer-1],
@@ -2007,7 +2011,7 @@
     }
     else
     {
-        final_cost->fprop( output, target, final_cost_value );        
+        final_cost->fprop( output, target, final_cost_value );
         costs.subVec(costs.length()-final_cost_value.length(),
                      final_cost_value.length()) <<
             final_cost_value;
@@ -2024,12 +2028,12 @@
 
     for( int i=0; i<layers.size()-1; i++)
         cost_names.push_back("reconstruction_error_" + tostring(i+1));
-    
+
     for( int i=0 ; i<partial_costs.size() ; i++ )
     {
         TVec<string> names = partial_costs[i]->costNames();
         for(int j=0; j<names.length(); j++)
-            cost_names.push_back("partial" + tostring(i) + "." + 
+            cost_names.push_back("partial" + tostring(i) + "." +
                 names[j]);
     }
 
@@ -2046,12 +2050,12 @@
         cost_names.push_back("reconstruction_error_" + tostring(i+1));
 
     cost_names.push_back("global_reconstruction_error");
-    
+
     for( int i=0 ; i<partial_costs.size() ; i++ )
     {
         TVec<string> names = partial_costs[i]->costNames();
         for(int j=0; j<names.length(); j++)
-            cost_names.push_back("partial" + tostring(i) + "." + 
+            cost_names.push_back("partial" + tostring(i) + "." +
                 names[j]);
     }
 



From lamblin at mail.berlios.de  Wed Jun  4 19:25:58 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 4 Jun 2008 19:25:58 +0200
Subject: [Plearn-commits] r9105 - trunk/plearn_learners/online
Message-ID: <200806041725.m54HPwPs027145@sheep.berlios.de>

Author: lamblin
Date: 2008-06-04 19:25:50 +0200 (Wed, 04 Jun 2008)
New Revision: 9105

Modified:
   trunk/plearn_learners/online/LogaddOnBagsModule.cc
   trunk/plearn_learners/online/LogaddOnBagsModule.h
   trunk/plearn_learners/online/OnBagsModule.cc
   trunk/plearn_learners/online/OnBagsModule.h
Log:
remove "svn:executable" flag



Property changes on: trunk/plearn_learners/online/LogaddOnBagsModule.cc
___________________________________________________________________
Name: svn:executable
   - *


Property changes on: trunk/plearn_learners/online/LogaddOnBagsModule.h
___________________________________________________________________
Name: svn:executable
   - *


Property changes on: trunk/plearn_learners/online/OnBagsModule.cc
___________________________________________________________________
Name: svn:executable
   - *


Property changes on: trunk/plearn_learners/online/OnBagsModule.h
___________________________________________________________________
Name: svn:executable
   - *



From larocheh at mail.berlios.de  Thu Jun  5 01:36:55 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 5 Jun 2008 01:36:55 +0200
Subject: [Plearn-commits] r9106 - trunk/plearn_learners/online
Message-ID: <200806042336.m54Natcv002616@sheep.berlios.de>

Author: larocheh
Date: 2008-06-05 01:36:54 +0200 (Thu, 05 Jun 2008)
New Revision: 9106

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
   trunk/plearn_learners/online/StackedAutoassociatorsNet.h
Log:
Added a remote method that ignores the correlation connections...


Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2008-06-04 17:25:50 UTC (rev 9105)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2008-06-04 23:36:54 UTC (rev 9106)
@@ -267,6 +267,21 @@
     inherited::declareOptions(ol);
 }
 
+void StackedAutoassociatorsNet::declareMethods(RemoteMethodMap& rmm)
+{
+    // Insert a backpointer to remote methods; note that this is different from
+    // declareOptions().
+    rmm.inherited(inherited::_getRemoteMethodMap_());
+
+    declareMethod(
+        rmm, "computeOutputWithoutCorrelationConnections", 
+        &StackedAutoassociatorsNet::remote_computeOutputWithoutCorrelationConnections,
+        (BodyDoc("On a trained learner, this computes the output from the input without using the correlation_connections"),
+         ArgDoc ("input", "Input vector (should have width inputsize)"),
+         RetDoc ("Computed output (will have width outputsize)")));
+
+}
+
 void StackedAutoassociatorsNet::build_()
 {
     // ### This method should do the real building of the object,
@@ -1915,6 +1930,32 @@
                              output );
 }
 
+void StackedAutoassociatorsNet::computeOutputWithoutCorrelationConnections(const Vec& input, Vec& output) const
+{
+    // fprop
+
+    expectations[0] << input;
+
+    for(int i=0 ; i<currently_trained_layer-1 ; i++ )
+    {
+        connections[i]->fprop( expectations[i], activations[i+1] );
+        layers[i+1]->fprop(activations[i+1],expectations[i+1]);
+    }
+
+    if( currently_trained_layer<n_layers )
+    {
+        connections[currently_trained_layer-1]->fprop( 
+            expectations[currently_trained_layer-1], 
+            activations[currently_trained_layer] );
+        layers[currently_trained_layer]->fprop(
+            activations[currently_trained_layer],
+            output);
+    }
+    else        
+        final_module->fprop( expectations[ currently_trained_layer - 1],
+                             output );
+}
+
 void StackedAutoassociatorsNet::computeCostsFromOutputs(const Vec& input, const Vec& output,
                                            const Vec& target, Vec& costs) const
 {
@@ -2090,6 +2131,13 @@
     final_module->setLearningRate( the_learning_rate );
 }
 
+//! Version of computeOutput that returns a result by value
+Vec StackedAutoassociatorsNet::remote_computeOutputWithoutCorrelationConnections(const Vec& input) const
+{
+    tmp_output.resize(outputsize());
+    computeOutput(input, tmp_output);
+    return tmp_output;
+}
 
 } // end of namespace PLearn
 

Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.h
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2008-06-04 17:25:50 UTC (rev 9105)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2008-06-04 23:36:54 UTC (rev 9106)
@@ -192,6 +192,10 @@
     virtual void train();
 
     //! Computes the output from the input.
+    virtual void computeOutputWithoutCorrelationConnections(const Vec& input, 
+                                                        Vec& output) const;
+
+    //! Computes the output from the input.
     virtual void computeOutput(const Vec& input, Vec& output) const;
 
     //! Computes the costs from already computed output.
@@ -352,6 +356,9 @@
     //! Declares the class options.
     static void declareOptions(OptionList& ol);
 
+    //! Declare the methods that are remote-callable
+    static void declareMethods(RemoteMethodMap& rmm);
+
 private:
     //#####  Private Member Functions  ########################################
 
@@ -366,6 +373,14 @@
 
     void setLearningRate( real the_learning_rate );
 
+    // List of remote methods
+    
+    Vec remote_computeOutputWithoutCorrelationConnections(const Vec& input) const;
+
+    //! Global storage to save memory allocations.
+    mutable Vec tmp_output;
+
+
 private:
     //#####  Private Data Members  ############################################
 



From dumitruerhan at mail.berlios.de  Thu Jun  5 19:33:47 2008
From: dumitruerhan at mail.berlios.de (dumitruerhan at BerliOS)
Date: Thu, 5 Jun 2008 19:33:47 +0200
Subject: [Plearn-commits] r9107 - trunk/plearn_learners/online
Message-ID: <200806051733.m55HXlVM005882@sheep.berlios.de>

Author: dumitruerhan
Date: 2008-06-05 19:33:46 +0200 (Thu, 05 Jun 2008)
New Revision: 9107

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
   trunk/plearn_learners/online/StackedAutoassociatorsNet.h
Log:
bugfix + added functionality to compute the outputs without correlation connections for a matrix of inputs (not just one input)

Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2008-06-04 23:36:54 UTC (rev 9106)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2008-06-05 17:33:46 UTC (rev 9107)
@@ -280,6 +280,14 @@
          ArgDoc ("input", "Input vector (should have width inputsize)"),
          RetDoc ("Computed output (will have width outputsize)")));
 
+    declareMethod(
+        rmm, "computeOutputsWithoutCorrelationConnections", 
+        &StackedAutoassociatorsNet::remote_computeOutputsWithoutCorrelationConnections,
+        (BodyDoc("On a trained learner, this computes the outputs from the inputs without using the correlation_connections"),
+         ArgDoc ("input", "Input matrix (should have width inputsize)"),
+         RetDoc ("Computed outputs (will have width outputsize)")));
+
+
 }
 
 void StackedAutoassociatorsNet::build_()
@@ -1956,6 +1964,21 @@
                              output );
 }
 
+void StackedAutoassociatorsNet::computeOutputsWithoutCorrelationConnections(const Mat& inputs, Mat& outputs) const
+{
+
+    int n=inputs.length();
+    PLASSERT(outputs.length()==n);
+    for (int i=0;i<n;i++)
+    {
+        Vec in_i = inputs(i);
+        Vec out_i = outputs(i);
+        computeOutputWithoutCorrelationConnections(in_i,out_i);
+    }
+
+}
+
+
 void StackedAutoassociatorsNet::computeCostsFromOutputs(const Vec& input, const Vec& output,
                                            const Vec& target, Vec& costs) const
 {
@@ -2131,14 +2154,23 @@
     final_module->setLearningRate( the_learning_rate );
 }
 
-//! Version of computeOutput that returns a result by value
+//! Version of computeOutputWithoutCorrelationConnections(Vec,Vec) that returns a result by value
 Vec StackedAutoassociatorsNet::remote_computeOutputWithoutCorrelationConnections(const Vec& input) const
 {
     tmp_output.resize(outputsize());
-    computeOutput(input, tmp_output);
+    computeOutputWithoutCorrelationConnections(input, tmp_output);
     return tmp_output;
 }
 
+//! Version of computeOutputsWithoutCorrelationConnections(Mat,Mat) that returns a result by value
+Mat StackedAutoassociatorsNet::remote_computeOutputsWithoutCorrelationConnections(const Mat& inputs) const
+{
+    tmp_output_mat.resize(inputs.length(),outputsize());
+    computeOutputsWithoutCorrelationConnections(inputs, tmp_output_mat);
+    return tmp_output_mat;
+}
+
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.h
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2008-06-04 23:36:54 UTC (rev 9106)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2008-06-05 17:33:46 UTC (rev 9107)
@@ -194,6 +194,9 @@
     //! Computes the output from the input.
     virtual void computeOutputWithoutCorrelationConnections(const Vec& input, 
                                                         Vec& output) const;
+    //! Computes the output from the input
+    virtual void computeOutputsWithoutCorrelationConnections(const Mat& input, 
+                                                        Mat& output) const;
 
     //! Computes the output from the input.
     virtual void computeOutput(const Vec& input, Vec& output) const;
@@ -373,14 +376,15 @@
 
     void setLearningRate( real the_learning_rate );
 
-    // List of remote methods
+    // List of remote methods    
+    Vec remote_computeOutputWithoutCorrelationConnections(const Vec& input) const;
     
-    Vec remote_computeOutputWithoutCorrelationConnections(const Vec& input) const;
+    Mat remote_computeOutputsWithoutCorrelationConnections(const Mat& inputs) const;
 
     //! Global storage to save memory allocations.
     mutable Vec tmp_output;
+    mutable Mat tmp_output_mat;
 
-
 private:
     //#####  Private Data Members  ############################################
 



From louradou at mail.berlios.de  Fri Jun  6 22:46:45 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Fri, 6 Jun 2008 22:46:45 +0200
Subject: [Plearn-commits] r9108 - in trunk/plearn_learners: generic hyper
Message-ID: <200806062046.m56KkjEc011300@sheep.berlios.de>

Author: louradou
Date: 2008-06-06 22:46:44 +0200 (Fri, 06 Jun 2008)
New Revision: 9108

Modified:
   trunk/plearn_learners/generic/EmbeddedLearner.cc
   trunk/plearn_learners/generic/EmbeddedLearner.h
   trunk/plearn_learners/hyper/HyperLearner.h
Log:
moved the function getLearner() to EmbeddedLearner (previously in subclass HyperLearner),
and make this function callable from python



Modified: trunk/plearn_learners/generic/EmbeddedLearner.cc
===================================================================
--- trunk/plearn_learners/generic/EmbeddedLearner.cc	2008-06-05 17:33:46 UTC (rev 9107)
+++ trunk/plearn_learners/generic/EmbeddedLearner.cc	2008-06-06 20:46:44 UTC (rev 9108)
@@ -96,6 +96,20 @@
     inherited::declareOptions(ol);
 }
 
+////////////////////
+// declareMethods //
+////////////////////
+void EmbeddedLearner::declareMethods(RemoteMethodMap& rmm)
+{
+    // Insert a backpointer to remote methods; note that this is different from
+    // declareOptions().
+    rmm.inherited(inherited::_getRemoteMethodMap_());
+    declareMethod(
+        rmm, "getLearner", &EmbeddedLearner::getLearner,
+        (BodyDoc("Returns the learnt embedded learner.\n"),
+         RetDoc ("the learner")));
+}
+
 void EmbeddedLearner::build_()
 {
     if (!learner_)

Modified: trunk/plearn_learners/generic/EmbeddedLearner.h
===================================================================
--- trunk/plearn_learners/generic/EmbeddedLearner.h	2008-06-05 17:33:46 UTC (rev 9107)
+++ trunk/plearn_learners/generic/EmbeddedLearner.h	2008-06-06 20:46:44 UTC (rev 9108)
@@ -57,6 +57,9 @@
     //! Inner learner which is embedded into the current learner
     PP<PLearner> learner_;
 
+    inline PP<PLearner> getLearner() const
+    { return learner_; }
+
     //! A string which should be appended to the expdir for the inner learner
     string expdir_append;
 
@@ -85,6 +88,9 @@
     //! Declares this class' options
     static void declareOptions(OptionList& ol);
 
+    //! Declare the methods that are remote-callable
+    static void declareMethods(RemoteMethodMap& rmm);
+
     //! Set training set of the inner learner.
     void setInnerLearnerTrainingSet(VMat training_set, bool call_forget);
     

Modified: trunk/plearn_learners/hyper/HyperLearner.h
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.h	2008-06-05 17:33:46 UTC (rev 9107)
+++ trunk/plearn_learners/hyper/HyperLearner.h	2008-06-06 20:46:44 UTC (rev 9108)
@@ -82,9 +82,6 @@
 
     HyperLearner();
 
-    inline PP<PLearner> getLearner() const
-    { return learner_; }
-
     inline void setLearner(PP<PLearner> learner)
     { tester->learner = learner; learner_ = learner; }
 



From lamblin at mail.berlios.de  Sun Jun  8 00:28:08 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Sun, 8 Jun 2008 00:28:08 +0200
Subject: [Plearn-commits] r9109 - trunk/python_modules/plearn/pymake
Message-ID: <200806072228.m57MS8Mu027808@sheep.berlios.de>

Author: lamblin
Date: 2008-06-08 00:28:07 +0200 (Sun, 08 Jun 2008)
New Revision: 9109

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
Get the correct tootal number of  cc files whhen compiling without linking.


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-06-06 20:46:44 UTC (rev 9108)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-06-07 22:28:07 UTC (rev 9109)
@@ -905,22 +905,24 @@
     (base,ext) = os.path.splitext(filepath)
     return ext in cpp_exts
 
-def get_ccfiles_to_compile_and_link(target, ccfiles_to_compile, executables_to_link,linkname):
+def get_ccfiles_to_compile_and_link(target, ccfiles_to_compile, ccfiles_to_link, executables_to_link,linkname):
     """A target can be a .cc file, a binary target, or a directory.
     The function updates (by appending to them) ccfiles_to_compile and ofiles_to_link
     ccfiles_to_compile is a dictionary containing FileInfo of all .cc files to compile
+    ccfiles_to_link is a dictionary containing FileInfo of all .cc files on
+    which target depends
     executables_to_link is a dictionary containing FileInfo of all .cc files containing a
     main whose corresponding executable should be made."""
     target = abspath(target) # get absolute path
     if os.path.basename(target)[0] == '.': # ignore files and directories starting with a dot
-        return 
+        return
     if os.path.isdir(target):
         if os.path.basename(target) not in ['OBJS','CVS']: # skip OBJS and CVS directories
             print "Entering " + target
             for direntry in os.listdir(target):
                 newtarget = join(target,direntry)
                 if os.path.isdir(newtarget) or isccfile(newtarget):
-                    get_ccfiles_to_compile_and_link(newtarget,ccfiles_to_compile,executables_to_link, linkname)
+                    get_ccfiles_to_compile_and_link(newtarget, ccfiles_to_compile, ccfiles_to_link, executables_to_link, linkname)
 
     else:
         if isccfile(target):
@@ -932,6 +934,7 @@
             print "Warning: bad target", target
         else:
             info = file_info(cctarget)
+            ccfiles_to_link[info] = 1
             if info.hasmain or create_dll or create_so or create_pyso:
                 if not force_link and not force_recompilation and info.corresponding_output_is_up_to_date() and not create_dll:
                     # Refresh symbolic link.
@@ -940,6 +943,7 @@
                 else:
                     executables_to_link[info] = 1
                     for ccfile in info.get_ccfiles_to_link():
+                        ccfiles_to_link[ccfile] = 1
                         if force_recompilation or not ccfile.corresponding_ofile_is_up_to_date():
                             #print ccfile.filebase
                             ccfiles_to_compile[ccfile] = 1
@@ -2861,6 +2865,7 @@
         sourcedirs = unique(sourcedirs)
 
         ccfiles_to_compile = {}
+        ccfiles_to_link = {}
         executables_to_link = {}
 
         options = getOptions(options_choices,optionargs)
@@ -2884,7 +2889,7 @@
         print '*** Running pymake on '+os.path.basename(target)+' using configuration file: ' + configpath
         print '*** Running pymake on '+os.path.basename(target)+' using options: ' + string.join(map(lambda o: '-'+o, options))
         print '++++ Computing dependencies of '+target
-        get_ccfiles_to_compile_and_link(target, ccfiles_to_compile, executables_to_link, linkname)
+        get_ccfiles_to_compile_and_link(target, ccfiles_to_compile, ccfiles_to_link, executables_to_link, linkname)
         print '++++ Dependencies computed'
 
         if distribute:
@@ -2896,19 +2901,18 @@
             generate_vcproj_files(target, ccfiles_to_compile, executables_to_link, linkname)
 
         else:
-            l=reduce(lambda x,y:x+y.get_ccfiles_to_link(),
-                     executables_to_link,[])
             if verbose >=4:
                 print "Link files:"
-                for i in l:
+                for i in ccfiles_to_link:
                     print i.filebase
-                print 
                 print
+                print
                 print "Files to compile: "
                 for i in ccfiles_to_compile:
                     print i.filebase
             print '++++ Compiling',
-            print str(len(ccfiles_to_compile))+'/'+str(len(l)),'files...'
+            print str(len(ccfiles_to_compile))+'/'+str(len(ccfiles_to_link))
+            print 'files...'
 
             if platform=='win32':
                 win32_parallel_compile(ccfiles_to_compile.keys())



From saintmlx at mail.berlios.de  Tue Jun 10 20:21:37 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 10 Jun 2008 20:21:37 +0200
Subject: [Plearn-commits] r9110 - trunk/python_modules/plearn/math
Message-ID: <200806101821.m5AILbck019599@sheep.berlios.de>

Author: saintmlx
Date: 2008-06-10 20:21:36 +0200 (Tue, 10 Jun 2008)
New Revision: 9110

Added:
   trunk/python_modules/plearn/math/spearman.py
Log:
- added fn. to calculate Spearman's rank correlation coefficient



Added: trunk/python_modules/plearn/math/spearman.py
===================================================================
--- trunk/python_modules/plearn/math/spearman.py	2008-06-07 22:28:07 UTC (rev 9109)
+++ trunk/python_modules/plearn/math/spearman.py	2008-06-10 18:21:36 UTC (rev 9110)
@@ -0,0 +1,107 @@
+# spearman.py
+# Copyright (C) 2008 Xavier Saint-Mleux, ApSTAT Technologies inc.
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+
+# Author: Xavier Saint-Mleux
+
+import math
+
+def spearmans_rho(xs, ys):
+    """
+    Calculate Spearman's rank correlation coefficient on two
+    lists or arrays.
+    """
+    n= len(xs)
+    assert(n==len(ys))
+
+    # sort xs and ys while keeping original indices
+    def add_idx_and_sort(l):
+        r= zip(l, range(len(l)))
+        r.sort()
+        return r
+    xs2= add_idx_and_sort(xs)
+    ys2= add_idx_and_sort(ys)
+
+    # assign ranks, averaging if necessary
+    def assign_ranks(li):
+        n= len(li)
+        r= []# list of (idx, val, rank) to be built
+        k= 0
+        while k < n:
+            k0= k
+            v= li[k0][0]
+            while k < n and li[k][0]==v:
+                k+= 1
+            rank= 1.+float(k0+k-1)/2.
+            for i in range(k0,k):
+                r+= [(li[i][1], li[i][0], rank)]
+        r.sort()#re-sort in orig. order
+        return r
+    xs3= assign_ranks(xs2)
+    ys3= assign_ranks(ys2)
+
+    # calc. correlation
+    sumx= 0.
+    sumy= 0.
+    sumxy= 0.
+    sumx2= 0.
+    sumy2= 0.
+    ds= []
+    ds2= []
+    for i in range(n):
+        x= xs3[i][2]
+        y= ys3[i][2]
+        ds+= [x-y]
+        ds2+= [(x-y)**2]
+        sumx+= x
+        sumy+= y
+        sumxy+= x*y
+        sumx2+= x*x
+        sumy2+= y*y
+    rho= (n*sumxy - sumx*sumy) / (math.sqrt(n*sumx2 - sumx*sumx) * math.sqrt(n*sumy2 - sumy*sumy))
+    return rho
+
+
+if __name__ == '__main__':
+    #####
+    # test example from Wikipedia (http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient)
+    #(IQ - Xi, 	Hours of TV per week - Yi)
+    xys= [(106,	7),
+          (86,	0),
+          (100,	27),
+          (101,	50),
+          (99,	28),
+          (103,	29),
+          (97,	20),
+          (113,	12),
+          (112,	6),
+          (110,	17)]
+    print spearmans_rho([xy[0] for xy in xys], [xy[1] for xy in xys])
+    



From lamblin at mail.berlios.de  Wed Jun 11 04:10:25 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 11 Jun 2008 04:10:25 +0200
Subject: [Plearn-commits] r9111 - trunk/plearn_learners/online
Message-ID: <200806110210.m5B2APIm023102@sheep.berlios.de>

Author: lamblin
Date: 2008-06-11 04:10:22 +0200 (Wed, 11 Jun 2008)
New Revision: 9111

Modified:
   trunk/plearn_learners/online/RBMBinomialLayer.cc
   trunk/plearn_learners/online/RBMBinomialLayer.h
   trunk/plearn_learners/online/RBMGaussianLayer.cc
   trunk/plearn_learners/online/RBMLateralBinomialLayer.cc
   trunk/plearn_learners/online/RBMLateralBinomialLayer.h
   trunk/plearn_learners/online/RBMLayer.cc
   trunk/plearn_learners/online/RBMLayer.h
   trunk/plearn_learners/online/RBMMixedLayer.cc
   trunk/plearn_learners/online/RBMMixedLayer.h
   trunk/plearn_learners/online/RBMWoodsLayer.cc
   trunk/plearn_learners/online/RBMWoodsLayer.h
Log:
Correct prototype for minibatch fprop() (without ports).
Implement a version in base class RBMLayer.


Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2008-06-10 18:21:36 UTC (rev 9110)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2008-06-11 02:10:22 UTC (rev 9111)
@@ -87,7 +87,7 @@
             sample[i] = 2*random_gen->binomial_sample( (expectation[i]+1)/2 )-1;
     else
         for( int i=0 ; i<size ; i++ )
-            sample[i] = random_gen->binomial_sample( expectation[i] );        
+            sample[i] = random_gen->binomial_sample( expectation[i] );
 }
 
 /////////////////////
@@ -115,7 +115,7 @@
             for (int i=0 ; i<size ; i++)
                 samples(k, i) = random_gen->binomial_sample( expectations(k, i) );
         }
-        
+
 }
 
 ////////////////////////
@@ -149,6 +149,7 @@
 /////////////////////////
 void RBMBinomialLayer::computeExpectations()
 {
+    PLASSERT( activations.length() == batch_size );
     if( expectations_are_up_to_date )
         return;
 
@@ -201,7 +202,7 @@
 
 }
 
-void RBMBinomialLayer::fprop( const Mat& inputs, Mat& outputs ) const
+void RBMBinomialLayer::fprop( const Mat& inputs, Mat& outputs )
 {
     int mbatch_size = inputs.length();
     PLASSERT( inputs.width() == size );
@@ -285,7 +286,7 @@
             real in_grad_i;
             in_grad_i = (1 -  output_i * output_i) * output_gradient[i];
             input_gradient[i] += in_grad_i;
-            
+
             if( momentum == 0. )
             {
                 // update the bias: bias -= learning_rate * input_gradient
@@ -309,7 +310,7 @@
             real in_grad_i;
             in_grad_i = output_i * (1-output_i) * output_gradient[i];
             input_gradient[i] += in_grad_i;
-            
+
             if( momentum == 0. )
             {
                 // update the bias: bias -= learning_rate * input_gradient
@@ -371,7 +372,7 @@
                 real in_grad_i;
                 in_grad_i = (1 - output_i * output_i) * output_gradients(j, i);
                 input_gradients(j, i) += in_grad_i;
-                
+
                 if( momentum == 0. )
                 {
                     // update the bias: bias -= learning_rate * input_gradient
@@ -400,7 +401,7 @@
                 real in_grad_i;
                 in_grad_i = output_i * (1-output_i) * output_gradients(j, i);
                 input_gradients(j, i) += in_grad_i;
-                
+
                 if( momentum == 0. )
                 {
                     // update the bias: bias -= learning_rate * input_gradient
@@ -441,7 +442,7 @@
         for( int i=0 ; i<size ; i++ )
         {
             real output_i = output[i];
-            
+
             input_gradient[i] = ( 1 - output_i * output_i ) * output_gradient[i];
         }
     }
@@ -451,7 +452,7 @@
         {
             real output_i = output[i];
             input_gradient[i] = output_i * (1-output_i) * output_gradient[i];
-        }   
+        }
     }
 
     rbm_bias_gradient << input_gradient;
@@ -470,19 +471,19 @@
             {
                 target_i = (target[i]+1)/2;
                 activation_i = 2*activation[i];
-                
+
                 ret += tabulated_softplus(activation_i) - target_i * activation_i;
                 // nll = - target*log(sigmoid(act)) -(1-target)*log(1-sigmoid(act))
                 // but it is numerically unstable, so use instead the following identity:
                 //     = target*softplus(-act) +(1-target)*(act+softplus(-act))
-                //     = act + softplus(-act) - target*act 
+                //     = act + softplus(-act) - target*act
                 //     = softplus(act) - target*act
             }
         } else {
             for( int i=0 ; i<size ; i++ )
             {
                 target_i = (target[i]+1)/2;
-                activation_i = 2*activation[i];                
+                activation_i = 2*activation[i];
                 ret += softplus(activation_i) - target_i * activation_i;
             }
         }
@@ -498,7 +499,7 @@
                 // nll = - target*log(sigmoid(act)) -(1-target)*log(1-sigmoid(act))
                 // but it is numerically unstable, so use instead the following identity:
                 //     = target*softplus(-act) +(1-target)*(act+softplus(-act))
-                //     = act + softplus(-act) - target*act 
+                //     = act + softplus(-act) - target*act
                 //     = softplus(act) - target*act
             }
         } else {
@@ -516,8 +517,6 @@
 
 void RBMBinomialLayer::fpropNLL(const Mat& targets, const Mat& costs_column)
 {
-    // computeExpectations(); // why?
-
     PLASSERT( targets.width() == input_size );
     PLASSERT( targets.length() == batch_size );
     PLASSERT( costs_column.width() == 1 );
@@ -703,13 +702,13 @@
 {
     PLASSERT( output.length() == size );
     PLASSERT( conf_index >= 0 && conf_index < getConfigurationCount() );
-    
+
     if( use_signed_samples )
     {
         for ( int i = 0; i < size; ++i ) {
             output[i] = 2 * (conf_index & 1) - 1;
             conf_index >>= 1;
-        }        
+        }
     }
     else
     {

Modified: trunk/plearn_learners/online/RBMBinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.h	2008-06-10 18:21:36 UTC (rev 9110)
+++ trunk/plearn_learners/online/RBMBinomialLayer.h	2008-06-11 02:10:22 UTC (rev 9111)
@@ -84,7 +84,7 @@
     virtual void fprop( const Vec& input, Vec& output ) const;
 
     //! Batch forward propagation
-    virtual void fprop( const Mat& inputs, Mat& outputs ) const;
+    virtual void fprop( const Mat& inputs, Mat& outputs );
 
     //! forward propagation with provided bias
     virtual void fprop( const Vec& input, const Vec& rbm_bias,

Modified: trunk/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.cc	2008-06-10 18:21:36 UTC (rev 9110)
+++ trunk/plearn_learners/online/RBMGaussianLayer.cc	2008-06-11 02:10:22 UTC (rev 9111)
@@ -827,7 +827,7 @@
             }
     }
 }
-    
+
 void RBMGaussianLayer::bpropNLL(const Vec& target, real nll, Vec& bias_gradient)
 {
     computeExpectation();
@@ -839,7 +839,7 @@
     substract(expectation, target, bias_gradient);
 
     if( compute_mse_instead_of_nll )
-        bias_gradient *= 2;
+        bias_gradient *= 2.;
     addBiasDecay(bias_gradient);
 
 }
@@ -859,7 +859,7 @@
     substract(expectations, targets, bias_gradients);
 
     if( compute_mse_instead_of_nll )
-        bias_gradients *= 2;
+        bias_gradients *= 2.;
     addBiasDecay(bias_gradients);
 
 }

Modified: trunk/plearn_learners/online/RBMLateralBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLateralBinomialLayer.cc	2008-06-10 18:21:36 UTC (rev 9110)
+++ trunk/plearn_learners/online/RBMLateralBinomialLayer.cc	2008-06-11 02:10:22 UTC (rev 9111)
@@ -450,7 +450,7 @@
     }
 }
 
-void RBMLateralBinomialLayer::fprop( const Mat& inputs, Mat& outputs ) const
+void RBMLateralBinomialLayer::fprop( const Mat& inputs, Mat& outputs )
 {
     int mbatch_size = inputs.length();
     PLASSERT( inputs.width() == size );

Modified: trunk/plearn_learners/online/RBMLateralBinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLateralBinomialLayer.h	2008-06-10 18:21:36 UTC (rev 9110)
+++ trunk/plearn_learners/online/RBMLateralBinomialLayer.h	2008-06-11 02:10:22 UTC (rev 9111)
@@ -141,7 +141,7 @@
     virtual void fprop( const Vec& input, Vec& output ) const;
 
     //! Batch forward propagation
-    virtual void fprop( const Mat& inputs, Mat& outputs ) const;
+    virtual void fprop( const Mat& inputs, Mat& outputs );
 
     //! forward propagation with provided bias
     virtual void fprop( const Vec& input, const Vec& rbm_bias,

Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2008-06-10 18:21:36 UTC (rev 9110)
+++ trunk/plearn_learners/online/RBMLayer.cc	2008-06-11 02:10:22 UTC (rev 9111)
@@ -319,6 +319,23 @@
     output << This->expectation;
 }
 
+void RBMLayer::fprop(const Mat& inputs, Mat& outputs)
+{
+    // Note: inefficient.
+    PLASSERT( inputs.width() == input_size );
+    int mbatch_size = inputs.length();
+    outputs.resize(mbatch_size, output_size);
+
+    setBatchSize(mbatch_size);
+    activations << inputs;
+    for (int k = 0; k < mbatch_size; k++)
+        activations(k) += bias;
+
+    expectations_are_up_to_date = false;
+    computeExpectations();
+    outputs << expectations;
+}
+
 void RBMLayer::fprop( const Vec& input, const Vec& rbm_bias,
                       Vec& output ) const
 {
@@ -365,7 +382,7 @@
         selectRows( activations, TVec<int>(1, k), tmp );
         activation << tmp;
         selectRows( targets, TVec<int>(1, k), tmp );
-	target << tmp;
+        target << tmp;
         costs_column(k,0) = fpropNLL( target );
     }
 }

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2008-06-10 18:21:36 UTC (rev 9110)
+++ trunk/plearn_learners/online/RBMLayer.h	2008-06-11 02:10:22 UTC (rev 9111)
@@ -182,7 +182,8 @@
 
     //! Adds the bias to input, consider this as the activation, then compute
     //! the expectation
-    virtual void fprop( const Vec& input, Vec& output ) const;
+    virtual void fprop(const Vec& input, Vec& output) const;
+    virtual void fprop(const Mat& inputs, Mat& outputs);
 
     //! computes the expectation given the conditional input
     //! and the given bias

Modified: trunk/plearn_learners/online/RBMMixedLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.cc	2008-06-10 18:21:36 UTC (rev 9110)
+++ trunk/plearn_learners/online/RBMMixedLayer.cc	2008-06-11 02:10:22 UTC (rev 9111)
@@ -258,7 +258,7 @@
 ///////////
 // fprop //
 ///////////
-void RBMMixedLayer::fprop( const Mat& inputs, Mat& outputs ) const
+void RBMMixedLayer::fprop( const Mat& inputs, Mat& outputs )
 {
     int mbatch_size = inputs.length();
     PLASSERT( inputs.width() == size );

Modified: trunk/plearn_learners/online/RBMMixedLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.h	2008-06-10 18:21:36 UTC (rev 9110)
+++ trunk/plearn_learners/online/RBMMixedLayer.h	2008-06-11 02:10:22 UTC (rev 9111)
@@ -120,7 +120,7 @@
     virtual void fprop( const Vec& input, Vec& output ) const;
 
     //! Batch forward propagation
-    virtual void fprop( const Mat& inputs, Mat& outputs ) const;
+    virtual void fprop( const Mat& inputs, Mat& outputs );
 
     //! forward propagation with provided bias
     virtual void fprop( const Vec& input, const Vec& rbm_bias,

Modified: trunk/plearn_learners/online/RBMWoodsLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMWoodsLayer.cc	2008-06-10 18:21:36 UTC (rev 9110)
+++ trunk/plearn_learners/online/RBMWoodsLayer.cc	2008-06-11 02:10:22 UTC (rev 9111)
@@ -473,7 +473,7 @@
     }
 }
 
-void RBMWoodsLayer::fprop( const Mat& inputs, Mat& outputs ) const
+void RBMWoodsLayer::fprop( const Mat& inputs, Mat& outputs )
 {
     int mbatch_size = inputs.length();
     PLASSERT( inputs.width() == size );

Modified: trunk/plearn_learners/online/RBMWoodsLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMWoodsLayer.h	2008-06-10 18:21:36 UTC (rev 9110)
+++ trunk/plearn_learners/online/RBMWoodsLayer.h	2008-06-11 02:10:22 UTC (rev 9111)
@@ -89,7 +89,7 @@
     virtual void fprop( const Vec& input, Vec& output ) const;
 
     //! Batch forward propagation
-    virtual void fprop( const Mat& inputs, Mat& outputs ) const;
+    virtual void fprop( const Mat& inputs, Mat& outputs );
 
     //! forward propagation with provided bias
     virtual void fprop( const Vec& input, const Vec& rbm_bias,



From lamblin at mail.berlios.de  Wed Jun 11 04:11:56 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 11 Jun 2008 04:11:56 +0200
Subject: [Plearn-commits] r9112 - trunk/plearn_learners/online
Message-ID: <200806110211.m5B2BuCe023267@sheep.berlios.de>

Author: lamblin
Date: 2008-06-11 04:11:56 +0200 (Wed, 11 Jun 2008)
New Revision: 9112

Modified:
   trunk/plearn_learners/online/RBMConnection.cc
   trunk/plearn_learners/online/RBMConnection.h
Log:
Add minibatch version of fprop() (without ports).


Modified: trunk/plearn_learners/online/RBMConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMConnection.cc	2008-06-11 02:10:22 UTC (rev 9111)
+++ trunk/plearn_learners/online/RBMConnection.cc	2008-06-11 02:11:56 UTC (rev 9112)
@@ -295,6 +295,15 @@
     computeProduct( 0, output_size, output );
 }
 
+void RBMConnection::fprop(const Mat& inputs, Mat& outputs)
+{
+    int batch_size = inputs.length();
+    // propagates the activations.
+    setAsDownInputs(inputs);
+    outputs.resize(batch_size, output_size);
+    computeProducts(0, output_size, outputs);
+}
+
 void RBMConnection::getAllWeights(Mat& rbm_weights) const
 {
     PLERROR("In RBMConnection::getAllWeights(): not implemented");

Modified: trunk/plearn_learners/online/RBMConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMConnection.h	2008-06-11 02:10:22 UTC (rev 9111)
+++ trunk/plearn_learners/online/RBMConnection.h	2008-06-11 02:11:56 UTC (rev 9112)
@@ -187,6 +187,7 @@
 
     //! given the input, compute the output (possibly resize it  appropriately)
     virtual void fprop(const Vec& input, Vec& output) const;
+    virtual void fprop(const Mat& inputs, Mat& outputs);
 
     //! provide the internal weight values (not a copy)
     virtual void getAllWeights(Mat& rbm_weights) const;



From lamblin at mail.berlios.de  Wed Jun 11 04:14:11 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 11 Jun 2008 04:14:11 +0200
Subject: [Plearn-commits] r9113 - trunk/plearn_learners/online
Message-ID: <200806110214.m5B2EBok023477@sheep.berlios.de>

Author: lamblin
Date: 2008-06-11 04:14:10 +0200 (Wed, 11 Jun 2008)
New Revision: 9113

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
Log:
Some cleanup.


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2008-06-11 02:11:56 UTC (rev 9112)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2008-06-11 02:14:10 UTC (rev 9113)
@@ -62,9 +62,9 @@
     up_down_learning_rate( 0. ),
     up_down_decrease_ct( 0. ),
     grad_learning_rate( 0. ),
-    batch_size( 1 ),
     grad_decrease_ct( 0. ),
     // grad_weight_decay( 0. ),
+    batch_size( 1 ),
     n_classes( -1 ),
     up_down_nstages( 0 ),
     use_classification_cost( true ),
@@ -78,8 +78,6 @@
     train_stats_window( -1 ),
     minibatch_size( 0 ),
     initialize_gibbs_chain( false ),
-    final_module_has_learning_rate( false ),
-    final_cost_has_learning_rate( false ),
     nll_cost_index( -1 ),
     class_cost_index( -1 ),
     final_cost_index( -1 ),
@@ -110,7 +108,7 @@
                   "The decrease constant of the learning rate used during"
                   " contrastive divergence");
 
-    declareOption(ol, "up_down_learning_rate", 
+    declareOption(ol, "up_down_learning_rate",
                   &DeepBeliefNet::up_down_learning_rate,
                   OptionBase::buildoption,
                   "The learning rate used in the up-down algorithm during the\n"
@@ -305,17 +303,21 @@
 
     declareOption(ol, "minibatch_size", &DeepBeliefNet::minibatch_size,
                   OptionBase::learntoption,
-                  "Size of a mini-batch.");
+                  "Actual size of a mini-batch (size of the training set if"
+                  " batch_size==1).");
 
     declareOption(ol, "gibbs_down_state", &DeepBeliefNet::gibbs_down_state,
                   OptionBase::learntoption,
-                  "State of visible units of RBMs at each layer in background Gibbs chain.");
+                  "State of visible units of RBMs at each layer in background"
+                  " Gibbs chain.");
 
-    declareOption(ol, "cumulative_training_time", &DeepBeliefNet::cumulative_training_time,
+    declareOption(ol, "cumulative_training_time",
+                  &DeepBeliefNet::cumulative_training_time,
                   OptionBase::learntoption | OptionBase::nosave,
                   "Cumulative training time since age=0, in seconds.\n");
 
-    declareOption(ol, "cumulative_testing_time", &DeepBeliefNet::cumulative_testing_time,
+    declareOption(ol, "cumulative_testing_time",
+                  &DeepBeliefNet::cumulative_testing_time,
                   OptionBase::learntoption | OptionBase::nosave,
                   "Cumulative testing time since age=0, in seconds.\n");
 
@@ -324,7 +326,7 @@
                   "Number of samples visited so far during unsupervised\n"
                   "fine-tuning.\n");
 
-    declareOption(ol, "generative_connections", 
+    declareOption(ol, "generative_connections",
                   &DeepBeliefNet::generative_connections,
                   OptionBase::learntoption,
                   "The untied generative weights of the connections"
@@ -791,7 +793,7 @@
 
     if( final_module )
         out_size += final_module->output_size;
-    
+
     if( !use_classification_cost && !final_module )
         out_size += layers[i_output_layer]->size;
 
@@ -907,9 +909,10 @@
     if (online)
     {
         // Train all layers simultaneously AND fine-tuning as well!
+        int init_stage = stage;
         if( report_progress && stage < nstages )
             pb = new ProgressBar( "Training "+classname(),
-                                  nstages - stage );
+                                  nstages - init_stage );
 
         setLearningRate( grad_learning_rate );
         train_stats->forget();
@@ -926,7 +929,7 @@
                     setLearningRate( grad_learning_rate
                                      / (1. + grad_decrease_ct * stage ));
 
-                if (batch_size > 1 || minibatch_hack)
+                if (minibatch_size > 1 || minibatch_hack)
                 {
                     train_set->getExamples(sample_start, minibatch_size,
                                            inputs, targets, weights, NULL, true);
@@ -953,7 +956,7 @@
             }
 
             if( pb )
-                pb->update( stage + 1 );
+                pb->update( stage - init_stage + 1 );
         }
     }
     else // Greedy learning, one layer at a time.
@@ -1000,7 +1003,7 @@
                 // Do a step every 'minibatch_size' examples.
                 if (stage % minibatch_size == 0) {
                     int sample_start = stage % nsamples;
-                    if (batch_size > 1 || minibatch_hack) {
+                    if (minibatch_size > 1 || minibatch_hack) {
                         train_set->getExamples(sample_start, minibatch_size,
                                 inputs, targets, weights, NULL, true);
                         train_costs_m.fill(MISSING_VALUE);
@@ -1194,8 +1197,8 @@
 ////////////////
 // onlineStep //
 ////////////////
-void DeepBeliefNet::onlineStep( const Vec& input, const Vec& target,
-                                Vec& train_costs)
+void DeepBeliefNet::onlineStep(const Vec& input, const Vec& target,
+                               Vec& train_costs)
 {
     real lr;
     PLASSERT(batch_size == 1);
@@ -1600,7 +1603,7 @@
                 lr = grad_learning_rate / (1. + grad_decrease_ct * stage );
             else
                 lr = grad_learning_rate;
-            
+
             connections[ i ]->setLearningRate( lr );
             layers[ i+1 ]->setLearningRate( lr );
 
@@ -1672,7 +1675,7 @@
 ////////////////
 // greedyStep //
 ////////////////
-void DeepBeliefNet::greedyStep( const Vec& input, const Vec& target, int index )
+void DeepBeliefNet::greedyStep(const Vec& input, const Vec& target, int index)
 {
     real lr;
     PLASSERT( index < n_layers );
@@ -1689,8 +1692,8 @@
     {
         // put appropriate learning rate
         if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
-            lr = grad_learning_rate / 
-                (1. + grad_decrease_ct * 
+            lr = grad_learning_rate /
+                (1. + grad_decrease_ct *
                  (stage - cumulative_schedule[index]));
         else
             lr = grad_learning_rate;
@@ -1721,7 +1724,7 @@
 
         // put back old learning rate
         if( !fast_exact_is_equal( cd_decrease_ct, 0. ) )
-            lr = cd_learning_rate / (1. + cd_decrease_ct * 
+            lr = cd_learning_rate / (1. + cd_decrease_ct *
                                      (stage - cumulative_schedule[index]));
         else
             lr = cd_learning_rate;
@@ -1739,7 +1742,8 @@
 /////////////////
 // greedySteps //
 /////////////////
-void DeepBeliefNet::greedyStep( const Mat& inputs, const Mat& targets, int index, Mat& train_costs_m )
+void DeepBeliefNet::greedyStep(const Mat& inputs, const Mat& targets,
+                               int index, Mat& train_costs_m)
 {
     real lr;
     PLASSERT( index < n_layers );
@@ -1756,8 +1760,8 @@
     {
         // put appropriate learning rate
         if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
-            lr = grad_learning_rate / 
-                (1. + grad_decrease_ct * 
+            lr = grad_learning_rate /
+                (1. + grad_decrease_ct *
                  (stage - cumulative_schedule[index]));
         else
             lr = grad_learning_rate;
@@ -1788,7 +1792,7 @@
 
         // put back old learning rate
         if( !fast_exact_is_equal( cd_decrease_ct, 0. ) )
-            lr = cd_learning_rate / (1. + cd_decrease_ct * 
+            lr = cd_learning_rate / (1. + cd_decrease_ct *
                                      (stage - cumulative_schedule[index]));
         else
             lr = cd_learning_rate;
@@ -1842,17 +1846,17 @@
 
         // put appropriate learning rate
         if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
-            lr = grad_learning_rate 
-                / (1. + grad_decrease_ct * 
+            lr = grad_learning_rate
+                / (1. + grad_decrease_ct *
                    (stage - cumulative_schedule[n_layers-2]));
         else
             lr = grad_learning_rate;
-        
+
         partial_costs[ n_layers-2 ]->setLearningRate( lr );
         connections[ n_layers-2 ]->setLearningRate( lr );
         layers[ n_layers-1 ]->setLearningRate( lr );
-        
 
+
         // Backward pass
         real cost;
         partial_costs[ n_layers-2 ]->fprop( layers[ n_layers-1 ]->expectation,
@@ -1876,8 +1880,8 @@
 
         // put back old learning rate
         if( !fast_exact_is_equal( cd_decrease_ct, 0. ) )
-            lr = cd_learning_rate 
-                / (1. + cd_decrease_ct * 
+            lr = cd_learning_rate
+                / (1. + cd_decrease_ct *
                    (stage - cumulative_schedule[n_layers-2]));
         else
             lr = cd_learning_rate;
@@ -1895,6 +1899,12 @@
         layers[ n_layers-1 ], n_layers-2);
 }
 
+void DeepBeliefNet::jointGreedyStep(const Mat& inputs, const Mat& targets)
+{
+    PLCHECK_MSG(false, "Not implemented for mini-batches");
+}
+
+
 ////////////////
 // upDownStep //
 ////////////////
@@ -1970,6 +1980,12 @@
     }
 }
 
+void DeepBeliefNet::upDownStep(const Mat& inputs, const Mat& targets,
+                               Mat& train_costs)
+{
+    PLCHECK_MSG(false, "Not implemented for mini-batches");
+}
+
 ////////////////////
 // fineTuningStep //
 ////////////////////
@@ -2079,7 +2095,7 @@
 }
 
 void DeepBeliefNet::fineTuningStep(const Mat& inputs, const Mat& targets,
-                                   Mat& train_costs )
+                                   Mat& train_costs)
 {
     final_cost_values.resize(0, 0);
     // fprop
@@ -2239,7 +2255,8 @@
         }
     }
 
-    if (mbatch) {
+    if (mbatch)
+    {
         // accumulate positive stats using the expectation
         // we deep-copy because the value will change during negative phase
         pos_down_vals.resize(minibatch_size, down_layer->size);
@@ -2366,7 +2383,7 @@
     } else {
         if( !use_mean_field_contrastive_divergence )
             up_layer->generateSample();
-        
+
         // accumulate positive stats using the expectation
         // we deep-copy because the value will change during negative phase
         pos_down_val.resize( down_layer->size );
@@ -2572,7 +2589,7 @@
 
 //! This function is usefull when the NLL CostModule AND/OR the final_cost Module
 //! are more efficient with batch computation (or need to be computed on a bunch of examples, as LayerCostModule)
-void DeepBeliefNet::computeOutputsAndCosts(const Mat& inputs, const Mat& targets, 
+void DeepBeliefNet::computeOutputsAndCosts(const Mat& inputs, const Mat& targets,
                                       Mat& outputs, Mat& costs) const
 {
     int nsamples = inputs.length();
@@ -2583,7 +2600,7 @@
     for (int isample = 0; isample < nsamples; isample++ )
     {
         Vec in_i = inputs(isample);
-        Vec out_i = outputs(isample); 
+        Vec out_i = outputs(isample);
         computeOutput(in_i, out_i);
         if( !partial_costs.isEmpty() )
         {
@@ -2627,7 +2644,7 @@
         costs.subMat( 0, nll_cost_index, nsamples, 1) << pcosts;
 
         for (int isample = 0; isample < nsamples; isample++ )
-	    costs(isample,class_cost_index) =
+            costs(isample,class_cost_index) =
                 (argmax(outputs(isample).subVec(0, n_classes)) == (int) round(targets(isample,0))) ? 0 : 1;
     }
 
@@ -2643,7 +2660,7 @@
 
     if( !partial_costs.isEmpty() )
         PLERROR("cannot compute partial costs in DeepBeliefNet::computeCostsFromOutputs(Mat&, Mat&, Mat&, Mat&)"
-	        "(expectations are not up to date in the batch version)");
+                "(expectations are not up to date in the batch version)");
 }
 
 void DeepBeliefNet::test(VMat testset, PP<VecStatsCollector> test_stats, VMat testoutputs, VMat testcosts) const
@@ -2660,7 +2677,7 @@
     //  E[testN-1.E[cumulative_test_time] will basically be the cumulative test
     //  time until (and including) the N-1th split! So it's a pretty
     //  meaningless number (more or less).
-      
+
     Profiler::reset("testing");
     Profiler::start("testing");
 

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2008-06-11 02:11:56 UTC (rev 9112)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2008-06-11 02:14:10 UTC (rev 9113)
@@ -68,11 +68,11 @@
     //! The learning rate used during contrastive divergence learning
     real cd_learning_rate;
 
-    //! The decrease constant of the learning rate used during 
+    //! The decrease constant of the learning rate used during
     //! contrastive divergence learning
     real cd_decrease_ct;
 
-    //! The learning rate used in the up-down algorithm during the 
+    //! The learning rate used in the up-down algorithm during the
     //! unsupervised fine tuning gradient descent
     real up_down_learning_rate;
 
@@ -83,8 +83,6 @@
     //! The learning rate used during the gradient descent
     real grad_learning_rate;
 
-    int batch_size;
-
     //! The decrease constant of the learning rate used during gradient descent
     real grad_decrease_ct;
 
@@ -93,6 +91,9 @@
     real grad_weight_decay;
     */
 
+    //! Training batch size (1=stochastic learning, 0=full batch learning)
+    int batch_size;
+
     //! Number of classes in the training set
     //!   - 0 means we are doing regression,
     //!   - 1 means we have two classes, but only one output,
@@ -230,13 +231,11 @@
     //! (Re-)initializes the PLearner in its fresh state (that state may depend
     //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
     //! a fresh learner!).
-    // (PLEASE IMPLEMENT IN .cc)
     virtual void forget();
 
     //! The role of the train method is to bring the learner up to
     //! stage==nstages, updating the train_stats collector with training costs
     //! measured on-line in the process.
-    // (PLEASE IMPLEMENT IN .cc)
     virtual void train();
 
     //! Re-implementation of the PLearner test() for profiling purposes
@@ -244,18 +243,17 @@
                       VMat testoutputs=0, VMat testcosts=0) const;
 
     //! Computes the output from the input.
-    // (PLEASE IMPLEMENT IN .cc)
     virtual void computeOutput(const Vec& input, Vec& output) const;
 
-    virtual void computeOutputsAndCosts(const Mat& inputs, const Mat& targets, 
+    virtual void computeOutputsAndCosts(const Mat& inputs, const Mat& targets,
                                         Mat& outputs, Mat& costs) const;
 
     //! Computes the costs from already computed output.
-    // (PLEASE IMPLEMENT IN .cc)
     virtual void computeCostsFromOutputs(const Vec& input, const Vec& output,
                                          const Vec& target, Vec& costs) const;
-    virtual void computeClassifAndFinalCostsFromOutputs(const Mat& inputs, const Mat& outputs,
-                                                        const Mat& targets, Mat& costs) const;
+    virtual void computeClassifAndFinalCostsFromOutputs(
+            const Mat& inputs, const Mat& outputs,
+            const Mat& targets, Mat& costs) const;
 
     //! Returns the names of the costs computed by computeCostsFromOutpus (and
     //! thus the test method).
@@ -268,27 +266,24 @@
     virtual TVec<std::string> getTrainCostNames() const;
 
 
-    void onlineStep( const Vec& input, const Vec& target, Vec& train_costs );
+    void onlineStep(const Vec& input, const Vec& target, Vec& train_costs);
+    void onlineStep(const Mat& inputs, const Mat& targets, Mat& train_costs);
 
-    void onlineStep( const Mat& inputs, const Mat& targets, Mat& train_costs );
+    void greedyStep(const Vec& input, const Vec& target, int index);
+    void greedyStep(const Mat& inputs, const Mat& targets, int index,
+                    Mat& train_costs_m);
 
-    void greedyStep( const Vec& input, const Vec& target, int index );
 
-    //! Greedy step with mini-batches.
-    void greedyStep(const Mat& inputs, const Mat& targets, int index, Mat& train_costs_m);
+    void jointGreedyStep(const Vec& input, const Vec& target);
+    void jointGreedyStep(const Mat& inputs, const Mat& targets);
 
-    void jointGreedyStep( const Vec& input, const Vec& target );
+    void upDownStep(const Vec& input, const Vec& target, Vec& train_costs);
+    void upDownStep(const Mat& inputs, const Mat& targets, Mat& train_costs);
 
-    void upDownStep( const Vec& input, const Vec& target,
-                     Vec& train_costs );
+    void fineTuningStep(const Vec& input, const Vec& target, Vec& train_costs);
+    void fineTuningStep(const Mat& inputs, const Mat& targets,
+                        Mat& train_costs);
 
-    void fineTuningStep( const Vec& input, const Vec& target,
-                         Vec& train_costs );
-
-    //! Fine tuning step with mini-batches.
-    void fineTuningStep( const Mat& inputs, const Mat& targets,
-                         Mat& train_costs );
-
     //! Perform a step of contrastive divergence, assuming that
     //! down_layer->expectation(s) is set.
     void contrastiveDivergenceStep( const PP<RBMLayer>& down_layer,
@@ -317,19 +312,16 @@
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.
-    // ### If your class is not instantiatable (it has pure virtual methods)
-    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
     PLEARN_DECLARE_OBJECT(DeepBeliefNet);
 
     // Simply calls inherited::build() then build_()
     virtual void build();
 
     //! Transforms a shallow copy into a deep copy
-    // (PLEASE IMPLEMENT IN .cc)
     virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
 protected:
-
+    //! Actual size of a mini-batch (size of the training set if batch_size==1)
     int minibatch_size;
 
     //#####  Not Options  #####################################################
@@ -363,22 +355,14 @@
     mutable Vec final_cost_gradient;
     mutable Mat final_cost_gradients; //!< For mini-batch.
 
-    //! buffers bottom layer activation during onlineStep 
+    //! buffers bottom layer activation during onlineStep
     mutable Vec save_layer_activation;
-
     Mat save_layer_activations; //!< For mini-batches.
 
-    //! buffers bottom layer expectation during onlineStep 
+    //! buffers bottom layer expectation during onlineStep
     mutable Vec save_layer_expectation;
-
     Mat save_layer_expectations; //!< For mini-batches.
 
-    //! Does final_module exist and have a "learning_rate" option
-    bool final_module_has_learning_rate;
-
-    //! Does final_cost exist and have a "learning_rate" option
-    bool final_cost_has_learning_rate;
-
     //! Store a copy of the positive phase values
     mutable Vec pos_down_val;
     mutable Vec pos_up_val;
@@ -414,7 +398,7 @@
     //! Index of the cpu time cost (per each call of train())
     int training_cpu_time_cost_index;
 
-    //! The index of the cumulative training time cost 
+    //! The index of the cumulative training time cost
     int cumulative_training_time_cost_index;
 
     //! The index of the cumulative testing time cost
@@ -423,7 +407,7 @@
     //! Holds the total training (cpu)time
     real cumulative_training_time;
 
-    //! Holds the total testing (cpu)time 
+    //! Holds the total testing (cpu)time
     mutable real cumulative_testing_time;
 
     //! Cumulative training schedule



From lamblin at mail.berlios.de  Wed Jun 11 04:17:59 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 11 Jun 2008 04:17:59 +0200
Subject: [Plearn-commits] r9114 - trunk/plearn_learners/online
Message-ID: <200806110217.m5B2HxoZ024120@sheep.berlios.de>

Author: lamblin
Date: 2008-06-11 04:17:58 +0200 (Wed, 11 Jun 2008)
New Revision: 9114

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
   trunk/plearn_learners/online/StackedAutoassociatorsNet.h
Log:
 - partial_cost and final_costs now have the same learning rate.
 - Added support for online minibatch.


Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2008-06-11 02:14:10 UTC (rev 9113)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2008-06-11 02:17:58 UTC (rev 9114)
@@ -42,6 +42,8 @@
 #include "StackedAutoassociatorsNet.h"
 #include <plearn/io/pl_log.h>
 
+#define minibatch_hack 0 // Do we force the minibatch setting? (debug hack)
+
 namespace PLearn {
 using namespace std;
 
@@ -59,6 +61,7 @@
     fine_tuning_decrease_ct( 0. ),
     l1_neuron_decay( 0. ),
     l1_neuron_decay_center( 0 ),
+    batch_size( 1 ),
     online( false ),
     compute_all_test_costs( false ),
     reconstruct_hidden( false ),
@@ -67,8 +70,10 @@
     unsupervised_fine_tuning_learning_rate( 0. ),
     unsupervised_fine_tuning_decrease_ct( 0. ),
     mask_input_layer_only_in_unsupervised_fine_tuning( false ),
+    train_stats_window( -1 ),
     n_layers( 0 ),
     unsupervised_stage( 0 ),
+    minibatch_size( 0 ),
     currently_trained_layer( 0 )
 {
     // random_gen will be initialized in PLearner::build_()
@@ -183,6 +188,11 @@
                   "previous layers.\n"
         );
 
+    declareOption(ol, "batch_size", &StackedAutoassociatorsNet::batch_size,
+                  OptionBase::buildoption,
+                  "Training batch size (1=stochastic learning, 0=full batch"
+                  " learning)");
+
     declareOption(ol, "online", &StackedAutoassociatorsNet::online,
                   OptionBase::buildoption,
                   "If true then all unsupervised training stages (as well as\n"
@@ -239,6 +249,12 @@
                   "Indication that only the input layer should be masked\n"
                   "during unsupervised fine-tuning.\n");
 
+    declareOption(ol, "train_stats_window",
+                  &StackedAutoassociatorsNet::train_stats_window,
+                  OptionBase::buildoption,
+                  "The number of samples to use to compute training stats.\n"
+                  "-1 (default) means the number of training samples.\n");
+
     declareOption(ol, "greedy_stages",
                   &StackedAutoassociatorsNet::greedy_stages,
                   OptionBase::learntoption,
@@ -274,14 +290,14 @@
     rmm.inherited(inherited::_getRemoteMethodMap_());
 
     declareMethod(
-        rmm, "computeOutputWithoutCorrelationConnections", 
+        rmm, "computeOutputWithoutCorrelationConnections",
         &StackedAutoassociatorsNet::remote_computeOutputWithoutCorrelationConnections,
         (BodyDoc("On a trained learner, this computes the output from the input without using the correlation_connections"),
          ArgDoc ("input", "Input vector (should have width inputsize)"),
          RetDoc ("Computed output (will have width outputsize)")));
 
     declareMethod(
-        rmm, "computeOutputsWithoutCorrelationConnections", 
+        rmm, "computeOutputsWithoutCorrelationConnections",
         &StackedAutoassociatorsNet::remote_computeOutputsWithoutCorrelationConnections,
         (BodyDoc("On a trained learner, this computes the outputs from the inputs without using the correlation_connections"),
          ArgDoc ("input", "Input matrix (should have width inputsize)"),
@@ -292,17 +308,6 @@
 
 void StackedAutoassociatorsNet::build_()
 {
-    // ### This method should do the real building of the object,
-    // ### according to set 'options', in *any* situation.
-    // ### Typical situations include:
-    // ###  - Initial building of an object from a few user-specified options
-    // ###  - Building of a "reloaded" object: i.e. from the complete set of
-    // ###    all serialised options.
-    // ###  - Updating or "re-building" of an object after a few "tuning"
-    // ###    options have been modified.
-    // ### You should assume that the parent class' build_() has already been
-    // ### called.
-
     MODULE_LOG << "build_() called" << endl;
 
     if(inputsize_ > 0 && targetsize_ > 0)
@@ -428,9 +433,13 @@
             }
         }
         correlation_activations.resize( n_layers-1 );
+        correlation_activations_m.resize( n_layers-1 );
         correlation_expectations.resize( n_layers-1 );
+        correlation_expectations_m.resize( n_layers-1 );
         correlation_activation_gradients.resize( n_layers-1 );
+        correlation_activation_gradients_m.resize( n_layers-1 );
         correlation_expectation_gradients.resize( n_layers-1 );
+        correlation_expectation_gradients_m.resize( n_layers-1 );
     }
 
     if(layers[0]->size != inputsize_)
@@ -439,9 +448,13 @@
                 inputsize_);
 
     activations.resize( n_layers );
+    activations_m.resize( n_layers );
     expectations.resize( n_layers );
+    expectations_m.resize( n_layers );
     activation_gradients.resize( n_layers );
+    activation_gradients_m.resize( n_layers );
     expectation_gradients.resize( n_layers );
+    expectation_gradients_m.resize( n_layers );
 
 
     for( int i=0 ; i<n_layers-1 ; i++ )
@@ -642,7 +655,6 @@
     }
 }
 
-// ### Nothing to add here, simply calls build_
 void StackedAutoassociatorsNet::build()
 {
     inherited::build();
@@ -670,12 +682,19 @@
 
     // Protected options
     deepCopyField(activations, copies);
+    deepCopyField(activations_m, copies);
     deepCopyField(expectations, copies);
+    deepCopyField(expectations_m, copies);
     deepCopyField(activation_gradients, copies);
+    deepCopyField(activation_gradients_m, copies);
     deepCopyField(expectation_gradients, copies);
+    deepCopyField(expectation_gradients_m, copies);
     deepCopyField(reconstruction_activations, copies);
+    deepCopyField(reconstruction_activations_m, copies);
     deepCopyField(reconstruction_activation_gradients, copies);
+    deepCopyField(reconstruction_activation_gradients_m, copies);
     deepCopyField(reconstruction_expectation_gradients, copies);
+    deepCopyField(reconstruction_expectation_gradients_m, copies);
     deepCopyField(fine_tuning_reconstruction_activations, copies);
     deepCopyField(fine_tuning_reconstruction_expectations, copies);
     deepCopyField(fine_tuning_reconstruction_activation_gradients, copies);
@@ -685,18 +704,28 @@
     deepCopyField(hidden_reconstruction_activations, copies);
     deepCopyField(hidden_reconstruction_activation_gradients, copies);
     deepCopyField(correlation_activations, copies);
+    deepCopyField(correlation_activations_m, copies);
     deepCopyField(correlation_expectations, copies);
+    deepCopyField(correlation_expectations_m, copies);
     deepCopyField(correlation_activation_gradients, copies);
+    deepCopyField(correlation_activation_gradients_m, copies);
     deepCopyField(correlation_expectation_gradients, copies);
+    deepCopyField(correlation_expectation_gradients_m, copies);
     deepCopyField(correlation_layers, copies);
     deepCopyField(direct_activations, copies);
     deepCopyField(direct_and_reconstruction_activations, copies);
     deepCopyField(direct_and_reconstruction_activation_gradients, copies);
     deepCopyField(partial_costs_positions, copies);
     deepCopyField(partial_cost_value, copies);
+    deepCopyField(partial_cost_values, copies);
+    deepCopyField(partial_cost_values_0, copies);
     deepCopyField(final_cost_input, copies);
+    deepCopyField(final_cost_inputs, copies);
     deepCopyField(final_cost_value, copies);
+    deepCopyField(final_cost_values, copies);
+    deepCopyField(final_cost_values_0, copies);
     deepCopyField(final_cost_gradient, copies);
+    deepCopyField(final_cost_gradients, copies);
     deepCopyField(masked_autoassociator_input, copies);
     deepCopyField(masked_autoassociator_expectations, copies);
     deepCopyField(autoassociator_input_indices, copies);
@@ -767,13 +796,22 @@
     MODULE_LOG << "train() called " << endl;
     MODULE_LOG << "  training_schedule = " << training_schedule << endl;
 
-    Vec input( inputsize() );
-    Vec target( targetsize() );
+    minibatch_size = batch_size > 0 ? batch_size : train_set->length();
+    int n_train_stats_samples = (train_stats_window >= 0)
+        ? train_stats_window
+        : train_set->length();
+
+    Vec input(inputsize());
+    Mat inputs(minibatch_size, inputsize());
+    Vec target(targetsize());
+    Mat targets(minibatch_size, inputsize());
     real weight; // unused
+    Vec weights(minibatch_size);
 
-    TVec<string> train_cost_names = getTrainCostNames() ;
-    Vec train_costs( train_cost_names.length() );
-    train_costs.fill(MISSING_VALUE) ;
+    TVec<string> train_cost_names = getTrainCostNames();
+    Vec train_costs(train_cost_names.length(), MISSING_VALUE);
+    Mat train_costs_m(minibatch_size, train_cost_names.length(),
+                      MISSING_VALUE);
 
     int nsamples = train_set->length();
     int sample;
@@ -834,8 +872,14 @@
 
             // Make sure that storage not null, will be resized anyways by bprop calls
             reconstruction_activations.resize(layers[i]->size);
+            reconstruction_activations_m.resize(minibatch_size,
+                                                layers[i]->size);
             reconstruction_activation_gradients.resize(layers[i]->size);
+            reconstruction_activation_gradients_m.resize(minibatch_size,
+                                                         layers[i]->size);
             reconstruction_expectation_gradients.resize(layers[i]->size);
+            reconstruction_expectation_gradients_m.resize(minibatch_size,
+                                                          layers[i]->size);
             masked_autoassociator_input.resize(layers[i]->size);
             autoassociator_input_indices.resize(layers[i]->size);
             for( int j=0 ; j < autoassociator_input_indices.length() ; j++ )
@@ -1009,9 +1053,8 @@
                 currently_trained_layer--;
         }
     }
-    else
+    else // online==true
     {
-
         if( unsupervised_nstages > 0 )
             PLERROR("StackedAutoassociatorsNet::train()"
                     " - \n"
@@ -1042,24 +1085,46 @@
             train_costs.fill(MISSING_VALUE);
             for( ; stage<nstages ; stage++ )
             {
-                sample = stage % nsamples;
-                if( !fast_exact_is_equal( fine_tuning_decrease_ct, 0. ) )
-                    setLearningRate( fine_tuning_learning_rate
-                                     / (1. + fine_tuning_decrease_ct * stage ) );
+                // Do a step every 'minibatch_size' examples
+                if (stage % minibatch_size == 0)
+                {
+                    sample = stage % nsamples;
+                    if( !fast_exact_is_equal(fine_tuning_decrease_ct, 0.) )
+                        setLearningRate(fine_tuning_learning_rate
+                                        /(1. + fine_tuning_decrease_ct*stage));
 
-                train_set->getExample( sample, input, target, weight );
-                onlineStep( input, target, train_costs );
-                train_stats->update( train_costs );
+                    if (minibatch_size > 1 || minibatch_hack)
+                    {
+                        train_set->getExamples(sample, minibatch_size,
+                                               inputs, targets, weights,
+                                               NULL, true );
+                        onlineStep(inputs, targets, train_costs_m);
+                    }
+                    else
+                    {
+                        train_set->getExample(sample, input, target, weight);
+                        onlineStep(input, target, train_costs);
+                    }
 
-                if( pb )
-                    pb->update( stage - init_stage + 1 );
+                    // Update stats if we are in the last n_train_stats_samples
+                    if (stage >= nstages - n_train_stats_samples)
+                        if (minibatch_size > 1 || minibatch_hack)
+                            for (int k = 0; k < minibatch_size; k++)
+                                train_stats->update(train_costs_m(k));
+                        else
+                            train_stats->update(train_costs);
+                }
+
+                if (pb)
+                    pb->update(stage - init_stage + 1);
             }
         }
 
     }
 }
 
-void StackedAutoassociatorsNet::greedyStep( const Vec& input, const Vec& target, int index, Vec train_costs )
+void StackedAutoassociatorsNet::greedyStep(const Vec& input, const Vec& target,
+                                           int index, Vec train_costs)
 {
     PLASSERT( index < n_layers );
 
@@ -1328,10 +1393,17 @@
 
 }
 
-void StackedAutoassociatorsNet::unsupervisedFineTuningStep( const Vec& input,
-                                                            const Vec& target,
-                                                            Vec& train_costs )
+void StackedAutoassociatorsNet::greedyStep(const Mat& inputs,
+                                           const Mat& targets,
+                                           int index, Mat& train_costs)
 {
+    PLCHECK_MSG(false, "Mini-batch not implemented yet.");
+}
+
+void StackedAutoassociatorsNet::unsupervisedFineTuningStep(const Vec& input,
+                                                           const Vec& target,
+                                                           Vec& train_costs)
+{
     // fprop
     expectations[0] << input;
 
@@ -1512,9 +1584,17 @@
     }
 }
 
-void StackedAutoassociatorsNet::fineTuningStep( const Vec& input, const Vec& target,
-                                                Vec& train_costs )
+void StackedAutoassociatorsNet::unsupervisedFineTuningStep(const Mat& inputs,
+                                                           const Mat& targets,
+                                                           Mat& train_costs)
 {
+    PLCHECK_MSG(false, "Mini-batch not implemented yet.");
+}
+
+void StackedAutoassociatorsNet::fineTuningStep(const Vec& input,
+                                               const Vec& target,
+                                               Vec& train_costs)
+{
     // fprop
     expectations[0] << input;
 
@@ -1600,10 +1680,19 @@
     }
 }
 
-void StackedAutoassociatorsNet::onlineStep( const Vec& input,
-                                            const Vec& target,
-                                            Vec& train_costs )
+void StackedAutoassociatorsNet::fineTuningStep(const Mat& inputs,
+                                               const Mat& targets,
+                                               Mat& train_costs)
 {
+    PLCHECK_MSG(false, "Mini-batch not implemented yet.");
+}
+
+
+
+void StackedAutoassociatorsNet::onlineStep(const Vec& input,
+                                           const Vec& target,
+                                           Vec& train_costs)
+{
     real lr;
     // fprop
     expectations[0] << input;
@@ -1731,7 +1820,7 @@
             reconstruction_activations);
 
         layers[ i-1 ]->fprop( reconstruction_activations,
-                                layers[ i-1 ]->expectation);
+                              layers[ i-1 ]->expectation);
 
         layers[ i-1 ]->activation << reconstruction_activations;
         //layers[ i-1 ]->expectation_is_up_to_date = true;
@@ -1875,6 +1964,283 @@
     }
 }
 
+void StackedAutoassociatorsNet::onlineStep(const Mat& inputs,
+                                           const Mat& targets,
+                                           Mat& train_costs)
+{
+    real lr;
+    int mbatch_size = inputs.length();
+    PLASSERT( targets.length() == mbatch_size );
+    train_costs.resize(mbatch_size, train_costs.width());
+
+    // fprop
+    expectations_m[0].resize(mbatch_size, inputsize());
+    expectations_m[0] << inputs;
+
+    if(correlation_connections.length() != 0)
+    {
+        for( int i=0 ; i<n_layers-1; i++ )
+        {
+            connections[i]->fprop(expectations_m[i],
+                                  correlation_activations_m[i]);
+            layers[i+1]->fprop(correlation_activations_m[i],
+                               correlation_expectations_m[i]);
+            correlation_connections[i]->fprop(correlation_expectations_m[i],
+                                              activations_m[i+1] );
+            correlation_layers[i]->fprop(activations_m[i+1],
+                                         expectations_m[i+1]);
+        }
+    }
+    else
+    {
+        for( int i=0 ; i<n_layers-1; i++ )
+        {
+            connections[i]->fprop( expectations_m[i], activations_m[i+1] );
+            layers[i+1]->fprop(activations_m[i+1], expectations_m[i+1]);
+
+            if( partial_costs.length() != 0 && partial_costs[ i ] )
+            {
+                // Set learning rates
+                if( !fast_exact_is_equal(fine_tuning_decrease_ct, 0 ) )
+                    lr = fine_tuning_learning_rate /
+                        (1 + fine_tuning_decrease_ct * stage);
+                else
+                    lr = fine_tuning_learning_rate;
+
+                partial_costs[ i ]->setLearningRate( lr );
+                partial_costs[ i ]->fprop( expectations_m[i + 1],
+                                           targets, partial_cost_values );
+                // Update partial cost (might contain some weights for example)
+                partial_cost_values_0.resize(mbatch_size);
+                partial_cost_values_0 << partial_cost_values.column(0);
+                partial_costs[ i ]->bpropUpdate(
+                    expectations_m[ i + 1 ],
+                    targets,
+                    partial_cost_values_0,
+                    expectation_gradients_m[ i + 1 ]
+                    );
+
+                train_costs.subMatColumns(partial_costs_positions[i]+1,
+                                          partial_cost_values.width())
+                    << partial_cost_values;
+
+                if( partial_costs_weights.length() != 0 )
+                    expectation_gradients_m[i + 1] *= partial_costs_weights[i];
+
+                // Update hidden layer bias and weights
+                layers[ i+1 ]->bpropUpdate( activations_m[ i + 1 ],
+                                            expectations_m[ i + 1 ],
+                                            activation_gradients_m[ i + 1 ],
+                                            expectation_gradients_m[ i + 1 ] );
+
+                connections[ i ]->bpropUpdate( expectations_m[ i ],
+                                               activations_m[ i + 1 ],
+                                               expectation_gradients_m[ i ],
+                                               activation_gradients_m[ i + 1 ]
+                                             );
+            }
+        }
+    }
+
+    final_module->fprop( expectations_m[ n_layers-1 ],
+                         final_cost_inputs );
+
+    final_cost->fprop( final_cost_inputs, targets, final_cost_values );
+
+    train_costs.subMatColumns(train_costs.width() - final_cost_values.width(),
+                              final_cost_values.width())
+        << final_cost_values;
+
+    final_cost_values_0.resize(mbatch_size);
+    final_cost_values_0 << final_cost_values.column(0);
+    final_cost->bpropUpdate( final_cost_inputs, targets,
+                             final_cost_values_0,
+                             final_cost_gradients );
+    final_module->bpropUpdate( expectations_m[ n_layers-1 ],
+                               final_cost_inputs,
+                               expectation_gradients_m[ n_layers-1 ],
+                               final_cost_gradients );
+
+    // Unsupervised greedy layer-wise cost
+
+    // Set learning rates
+    if( !fast_exact_is_equal( greedy_decrease_ct, 0 ) )
+        lr = greedy_learning_rate / (1 + greedy_decrease_ct * stage) ;
+    else
+        lr = greedy_learning_rate;
+
+    for( int i=0 ; i<n_layers-1 ; i++ )
+    {
+        layers[i]->setLearningRate( lr );
+        connections[i]->setLearningRate( lr );
+        reconstruction_connections[i]->setLearningRate( lr );
+        if(correlation_layers.length() != 0)
+        {
+            correlation_layers[i]->setLearningRate( lr );
+            correlation_connections[i]->setLearningRate( lr );
+        }
+    }
+    layers[n_layers-1]->setLearningRate( lr );
+
+    // Backpropagate unsupervised gradient, layer-wise
+    for( int i=n_layers-1 ; i>0 ; i-- )
+    {
+        reconstruction_connections[ i-1 ]->fprop(
+            expectations_m[ i ],
+            reconstruction_activations_m);
+
+        layers[ i-1 ]->activations.resize(mbatch_size, layers[i-1]->size);
+        layers[ i-1 ]->activations << reconstruction_activations_m;
+
+        Mat layer_exp = layers[i-1]->getExpectations();
+        layers[ i-1 ]->fprop(reconstruction_activations_m,
+                             layer_exp);
+        layers[ i-1 ]->setExpectationsByRef(layer_exp);
+
+        layers[ i-1 ]->fpropNLL(expectations_m[i-1],
+                                train_costs.column(i-1));
+
+        layers[ i-1 ]->bpropNLL(expectations_m[i-1], train_costs.column(i-1),
+                                reconstruction_activation_gradients_m);
+
+        layers[ i-1 ]->update(reconstruction_activation_gradients_m);
+
+        reconstruction_connections[ i-1 ]->bpropUpdate(
+            expectations_m[ i ],
+            reconstruction_activations_m,
+            reconstruction_expectation_gradients_m,
+            reconstruction_activation_gradients_m);
+
+        if(!fast_exact_is_equal(l1_neuron_decay,0))
+        {
+            // Compute L1 penalty gradient on neurons
+            for (int k = 0; k < mbatch_size; k++)
+            {
+                real* hid = expectations_m[i](k).data();
+                real* grad = reconstruction_expectation_gradients_m(k).data();
+                int width = expectations_m[i].width();
+                for(int j = 0; j < width; j++)
+                {
+                    if(*hid > l1_neuron_decay_center)
+                        *grad += l1_neuron_decay;
+                    else if(*hid < l1_neuron_decay_center)
+                        *grad -= l1_neuron_decay;
+                    hid++;
+                    grad++;
+                }
+            }
+        }
+
+        if( correlation_connections.length() != 0 )
+        {
+            correlation_layers[i-1]->bpropUpdate(
+                activations_m[i],
+                expectations_m[i],
+                reconstruction_activation_gradients_m,
+                reconstruction_expectation_gradients_m);
+
+            correlation_connections[i-1]->bpropUpdate(
+                correlation_expectations_m[i-1],
+                activations_m[i],
+                correlation_expectation_gradients_m[i-1],
+                reconstruction_activation_gradients_m);
+
+            layers[i]->bpropUpdate(
+                correlation_activations_m[i-1],
+                correlation_expectations_m[i-1],
+                correlation_activation_gradients_m[i-1],
+                correlation_expectation_gradients_m[i-1]);
+
+            connections[i-1]->bpropUpdate(
+                expectations_m[i-1],
+                correlation_activations_m[i-1],
+                reconstruction_expectation_gradients_m,
+                correlation_activation_gradients_m[i-1]);
+        }
+        else
+        {
+            layers[i]->bpropUpdate(
+                activations_m[i],
+                expectations_m[i],
+                reconstruction_activation_gradients_m,
+                reconstruction_expectation_gradients_m);
+
+            connections[i-1]->bpropUpdate(
+                expectations_m[i-1],
+                activations_m[i],
+                reconstruction_expectation_gradients_m,
+                reconstruction_activation_gradients_m);
+        }
+    }
+
+    // Put back fine-tuning learning rate
+    // Set learning rates
+    if( !fast_exact_is_equal(fine_tuning_decrease_ct, 0) )
+        lr = fine_tuning_learning_rate
+            / (1 + fine_tuning_decrease_ct * stage) ;
+    else
+        lr = fine_tuning_learning_rate ;
+
+    // Set learning rate back for fine-tuning
+    for( int i=0 ; i<n_layers-1 ; i++ )
+    {
+        layers[i]->setLearningRate( lr );
+        connections[i]->setLearningRate( lr );
+        //reconstruction_connections[i]->setLearningRate( lr );
+        if(correlation_layers.length() != 0)
+        {
+            correlation_layers[i]->setLearningRate( lr );
+            correlation_connections[i]->setLearningRate( lr );
+        }
+    }
+    layers[n_layers-1]->setLearningRate( lr );
+
+    // Fine-tuning backpropagation
+    if( correlation_connections.length() != 0 )
+    {
+        for( int i=n_layers-1 ; i>0 ; i-- )
+        {
+            correlation_layers[i-1]->bpropUpdate(
+                activations_m[i],
+                expectations_m[i],
+                activation_gradients_m[i],
+                expectation_gradients_m[i] );
+
+            correlation_connections[i-1]->bpropUpdate(
+                correlation_expectations_m[i-1],
+                activations_m[i],
+                correlation_expectation_gradients_m[i-1],
+                activation_gradients_m[i] );
+
+            layers[i]->bpropUpdate( correlation_activations_m[i-1],
+                                    correlation_expectations_m[i-1],
+                                    correlation_activation_gradients_m[i-1],
+                                    correlation_expectation_gradients_m[i-1] );
+
+            connections[i-1]->bpropUpdate(
+                expectations_m[i-1],
+                correlation_activations_m[i-1],
+                expectation_gradients_m[i-1],
+                correlation_activation_gradients_m[i-1] );
+        }
+    }
+    else
+    {
+        for( int i=n_layers-1 ; i>0 ; i-- )
+        {
+            layers[i]->bpropUpdate( activations_m[i],
+                                    expectations_m[i],
+                                    activation_gradients_m[i],
+                                    expectation_gradients_m[i] );
+
+            connections[i-1]->bpropUpdate( expectations_m[i-1],
+                                           activations_m[i],
+                                           expectation_gradients_m[i-1],
+                                           activation_gradients_m[i] );
+        }
+    }
+}
+
 void StackedAutoassociatorsNet::computeOutput(const Vec& input, Vec& output) const
 {
     // fprop
@@ -1952,14 +2318,14 @@
 
     if( currently_trained_layer<n_layers )
     {
-        connections[currently_trained_layer-1]->fprop( 
-            expectations[currently_trained_layer-1], 
+        connections[currently_trained_layer-1]->fprop(
+            expectations[currently_trained_layer-1],
             activations[currently_trained_layer] );
         layers[currently_trained_layer]->fprop(
             activations[currently_trained_layer],
             output);
     }
-    else        
+    else
         final_module->fprop( expectations[ currently_trained_layer - 1],
                              output );
 }

Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.h
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2008-06-11 02:14:10 UTC (rev 9113)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2008-06-11 02:17:58 UTC (rev 9114)
@@ -64,12 +64,14 @@
 public:
     //#####  Public Build Options  ############################################
 
-    //! The learning rate used during the autoassociator gradient descent training
+    //! The learning rate used during the autoassociator gradient descent
+    //! training
     real greedy_learning_rate;
 
-    //! The decrease constant of the learning rate used during the autoassociator
-    //! gradient descent training. When a hidden layer has finished its training,
-    //! the learning rate is reset to it's initial value.
+    //! The decrease constant of the learning rate used during the
+    //! autoassociator gradient descent training. When a hidden layer has
+    //! finished its training, the learning rate is reset to it's initial
+    //! value.
     real greedy_decrease_ct;
 
     //! The learning rate used during the fine tuning gradient descent
@@ -88,6 +90,9 @@
     //! where h is the value of the neurons.
     real l1_neuron_decay_center;
 
+    //! Training batch size (1=stochastic learning, 0=full batch learning)
+    int batch_size;
+
     //! Number of examples to use during each phase of greedy pre-training.
     //! The number of fine-tunig steps is defined by nstages.
     TVec<int> training_schedule;
@@ -120,13 +125,13 @@
     PP<OnlineLearningModule> final_module;
 
     //! The cost function to be applied on top of the neural network
-    //! (i.e. at the output of final_module). Its gradients will be 
+    //! (i.e. at the output of final_module). Its gradients will be
     //! backpropagated to final_module and then backpropagated to
     //! the layers.
     PP<CostModule> final_cost;
 
-    //! Corresponding additional supervised cost function to be applied on 
-    //! top of each hidden layer during the autoassociator training stages. 
+    //! Corresponding additional supervised cost function to be applied on
+    //! top of each hidden layer during the autoassociator training stages.
     //! The gradient for these costs are not backpropagated to previous layers.
     TVec< PP<CostModule> > partial_costs;
 
@@ -149,17 +154,23 @@
     //! Number of samples to use for unsupervised fine-tuning
     int unsupervised_nstages;
 
-    //! The learning rate used during the unsupervised fine tuning gradient descent
+    //! The learning rate used during the unsupervised fine tuning gradient
+    //! descent
     real unsupervised_fine_tuning_learning_rate;
 
-    //! The decrease constant of the learning rate used during 
+    //! The decrease constant of the learning rate used during
     //! unsupervised fine tuning gradient descent
     real unsupervised_fine_tuning_decrease_ct;
 
-    //! Indication that only the input layer should be masked 
+    //! Indication that only the input layer should be masked
     //! during unsupervised fine-tuning
     bool mask_input_layer_only_in_unsupervised_fine_tuning;
 
+    //! The number of samples to use to compute training stats.
+    //! -1 (default) means the number of training samples.
+    int train_stats_window;
+
+
     //#####  Public Learnt Options  ###########################################
 
     //! Number of layers
@@ -214,17 +225,25 @@
     virtual TVec<std::string> getTrainCostNames() const;
 
 
-    void greedyStep( const Vec& input, const Vec& target, int index, 
-                     Vec train_costs );
+    void greedyStep(const Vec& input, const Vec& target, int index,
+                    Vec train_costs);
+    void greedyStep(const Mat& inputs, const Mat& targets, int index,
+                    Mat& train_costs);
 
-    void unsupervisedFineTuningStep( const Vec& input, const Vec& target,
-                                     Vec& train_costs );
+    void unsupervisedFineTuningStep(const Vec& input, const Vec& target,
+                                    Vec& train_costs);
+    void unsupervisedFineTuningStep(const Mat& inputs, const Mat& targets,
+                                    Mat& train_costs);
 
-    void fineTuningStep( const Vec& input, const Vec& target,
-                         Vec& train_costs );
+    void fineTuningStep(const Vec& input, const Vec& target,
+                        Vec& train_costs);
+    void fineTuningStep(const Mat& inputs, const Mat& targets,
+                        Mat& train_costs);
 
-    void onlineStep( const Vec& input, const Vec& target,
-                         Vec& train_costs );
+    void onlineStep(const Vec& input, const Vec& target,
+                    Vec& train_costs);
+    void onlineStep(const Mat& inputs, const Mat& targets,
+                    Mat& train_costs);
 
     //#####  PLearn::Object Protocol  #########################################
 
@@ -237,74 +256,88 @@
     virtual void build();
 
     //! Transforms a shallow copy into a deep copy
-    // (PLEASE IMPLEMENT IN .cc)
     virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
 protected:
+
+    //! Actual size of a mini-batch (size of the training set if batch_size==1)
+    int minibatch_size;
+
     //#####  Not Options  #####################################################
 
     //! Stores the activations of the input and hidden layers
     //! (at the input of the layers)
     mutable TVec<Vec> activations;
+    mutable TVec<Mat> activations_m;
 
     //! Stores the expectations of the input and hidden layers
     //! (at the output of the layers)
     mutable TVec<Vec> expectations;
+    mutable TVec<Mat> expectations_m;
 
-    //! Stores the gradient of the cost wrt the activations of 
+    //! Stores the gradient of the cost wrt the activations of
     //! the input and hidden layers
     //! (at the input of the layers)
     mutable TVec<Vec> activation_gradients;
+    mutable TVec<Mat> activation_gradients_m;
 
-    //! Stores the gradient of the cost wrt the expectations of 
+    //! Stores the gradient of the cost wrt the expectations of
     //! the input and hidden layers
     //! (at the output of the layers)
     mutable TVec<Vec> expectation_gradients;
+    mutable TVec<Mat> expectation_gradients_m;
 
     //! Reconstruction activations
     mutable Vec reconstruction_activations;
-    
+    mutable Mat reconstruction_activations_m;
+
     //! Reconstruction activation gradients
     mutable Vec reconstruction_activation_gradients;
+    mutable Mat reconstruction_activation_gradients_m;
 
     //! Reconstruction expectation gradients
     mutable Vec reconstruction_expectation_gradients;
+    mutable Mat reconstruction_expectation_gradients_m;
 
     //! Unsupervised fine-tuning reconstruction activations
     TVec< Vec > fine_tuning_reconstruction_activations;
-    
+
     //! Unsupervised fine-tuning reconstruction expectations
     TVec< Vec > fine_tuning_reconstruction_expectations;
 
     //! Unsupervised fine-tuning reconstruction activations gradients
     TVec< Vec > fine_tuning_reconstruction_activation_gradients;
-    
+
     //! Unsupervised fine-tuning reconstruction expectations gradients
     TVec< Vec > fine_tuning_reconstruction_expectation_gradients;
 
     //! Reconstruction activation gradients coming from hidden reconstruction
     mutable Vec reconstruction_activation_gradients_from_hid_rec;
-    
+
     //! Reconstruction expectation gradients coming from hidden reconstruction
     mutable Vec reconstruction_expectation_gradients_from_hid_rec;
 
     //! Hidden reconstruction activations
     mutable Vec hidden_reconstruction_activations;
-    
+
     //! Hidden reconstruction activation gradients
     mutable Vec hidden_reconstruction_activation_gradients;
-    
+
     //! Activations before the correlation layer
     mutable TVec<Vec> correlation_activations;
-    
+    mutable TVec<Mat> correlation_activations_m;
+
     //! Expectations before the correlation layer
     mutable TVec<Vec> correlation_expectations;
-    
+    mutable TVec<Mat> correlation_expectations_m;
+
     //! Gradients of activations before the correlation layer
     mutable TVec<Vec> correlation_activation_gradients;
-    
+    mutable TVec<Mat> correlation_activation_gradients_m;
+
     //! Gradients of expectations before the correlation layer
     mutable TVec<Vec> correlation_expectation_gradients;
+    mutable TVec<Mat> correlation_expectation_gradients_m;
 
     //! Hidden layers for the correlation connections
     mutable TVec< PP<RBMLayer> > correlation_layers;
@@ -315,23 +348,30 @@
     //! Sum of activations from the direct and reconstruction connections
     mutable Vec direct_and_reconstruction_activations;
 
-    //! Gradient of sum of activations from the direct and reconstruction connections
+    //! Gradient of sum of activations from the direct and reconstruction
+    //! connections
     mutable Vec direct_and_reconstruction_activation_gradients;
 
     //! Position in the total cost vector of the different partial costs
     mutable TVec<int> partial_costs_positions;
-    
+
     //! Cost value of partial_costs
     mutable Vec partial_cost_value;
+    mutable Mat partial_cost_values;
+    mutable Vec partial_cost_values_0;
 
     //! Input of the final_cost
     mutable Vec final_cost_input;
+    mutable Mat final_cost_inputs;
 
     //! Cost value of final_cost
     mutable Vec final_cost_value;
+    mutable Mat final_cost_values;
+    mutable Vec final_cost_values_0;
 
     //! Stores the gradient of the cost at the input of final_cost
     mutable Vec final_cost_gradient;
+    mutable Mat final_cost_gradients;
 
     //! Input of autoassociator where some of the components
     //! have been masked (set to 0) randomly.
@@ -388,7 +428,7 @@
 private:
     //#####  Private Data Members  ############################################
 
-    // The rest of the private stuff goes here    
+    // The rest of the private stuff goes here
 };
 
 // Declares a few other classes and functions related to this class



From lamblin at mail.berlios.de  Wed Jun 11 04:37:29 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 11 Jun 2008 04:37:29 +0200
Subject: [Plearn-commits] r9115 - trunk/plearn_learners/online
Message-ID: <200806110237.m5B2bTO5026001@sheep.berlios.de>

Author: lamblin
Date: 2008-06-11 04:37:22 +0200 (Wed, 11 Jun 2008)
New Revision: 9115

Modified:
   trunk/plearn_learners/online/BinarizeModule.cc
   trunk/plearn_learners/online/CombiningCostsModule.cc
   trunk/plearn_learners/online/CostModule.h
   trunk/plearn_learners/online/CrossEntropyCostModule.cc
   trunk/plearn_learners/online/CrossEntropyCostModule.h
   trunk/plearn_learners/online/ForwardModule.cc
   trunk/plearn_learners/online/ForwardModule.h
   trunk/plearn_learners/online/GradNNetLayerModule.cc
   trunk/plearn_learners/online/GradNNetLayerModule.h
   trunk/plearn_learners/online/LayerCostModule.cc
   trunk/plearn_learners/online/LayerCostModule.h
   trunk/plearn_learners/online/LinearCombinationModule.cc
   trunk/plearn_learners/online/LinearCombinationModule.h
   trunk/plearn_learners/online/LinearFilterModule.cc
   trunk/plearn_learners/online/LinearFilterModule.h
   trunk/plearn_learners/online/LogaddOnBagsModule.cc
   trunk/plearn_learners/online/LogaddOnBagsModule.h
   trunk/plearn_learners/online/ModuleLearner.cc
   trunk/plearn_learners/online/ModuleLearner.h
   trunk/plearn_learners/online/ModuleStackModule.h
   trunk/plearn_learners/online/ModuleTester.cc
   trunk/plearn_learners/online/NetworkConnection.h
   trunk/plearn_learners/online/NetworkModule.cc
   trunk/plearn_learners/online/NetworkModule.h
   trunk/plearn_learners/online/OnBagsModule.cc
   trunk/plearn_learners/online/OnBagsModule.h
   trunk/plearn_learners/online/OnlineLearningModule.cc
   trunk/plearn_learners/online/OnlineLearningModule.h
   trunk/plearn_learners/online/RBMBinomialLayer.h
   trunk/plearn_learners/online/RBMDiagonalMatrixConnection.cc
   trunk/plearn_learners/online/RBMGaussianLayer.cc
   trunk/plearn_learners/online/RBMLateralBinomialLayer.cc
   trunk/plearn_learners/online/RBMLateralBinomialLayer.h
   trunk/plearn_learners/online/RBMLayer.cc
   trunk/plearn_learners/online/RBMLayer.h
   trunk/plearn_learners/online/RBMMatrixConnection.cc
   trunk/plearn_learners/online/RBMMatrixConnectionNatGrad.cc
   trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc
   trunk/plearn_learners/online/RBMMatrixTransposeConnection.h
   trunk/plearn_learners/online/RBMMixedLayer.cc
   trunk/plearn_learners/online/RBMModule.cc
   trunk/plearn_learners/online/RBMModule.h
   trunk/plearn_learners/online/RBMMultitaskClassificationModule.cc
   trunk/plearn_learners/online/RBMMultitaskClassificationModule.h
   trunk/plearn_learners/online/RBMWoodsLayer.cc
   trunk/plearn_learners/online/SoftmaxModule.h
   trunk/plearn_learners/online/SplitModule.cc
   trunk/plearn_learners/online/SplitModule.h
   trunk/plearn_learners/online/StackedAutoassociatorsNet.h
   trunk/plearn_learners/online/TanhModule.cc
   trunk/plearn_learners/online/VBoundDBN2.cc
   trunk/plearn_learners/online/VBoundDBN2.h
Log:
Whitespace and tabs fix.


Modified: trunk/plearn_learners/online/BinarizeModule.cc
===================================================================
--- trunk/plearn_learners/online/BinarizeModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/BinarizeModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -110,7 +110,7 @@
     Mat* output = ports_value[1];
     Mat* input_gradient = ports_gradient[0];
     Mat* output_gradient = ports_gradient[1];
-    
+
     int mbs=output->length();
     if (input_gradient)
     {

Modified: trunk/plearn_learners/online/CombiningCostsModule.cc
===================================================================
--- trunk/plearn_learners/online/CombiningCostsModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/CombiningCostsModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -81,7 +81,7 @@
 
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
-    
+
     redeclareOption(ol, "input_size", &CombiningCostsModule::input_size,
                     OptionBase::learntoption,
                     "Is set to sub_costs[0]->input_size.");
@@ -127,13 +127,13 @@
             PLERROR( "CombiningCostsModule::build_(): sub_costs[%d]->input_size"
                      " (%d)\n"
                      "should be equal to %d.\n",
-                     i,sub_costs[i]->input_size, input_size);  
+                     i,sub_costs[i]->input_size, input_size);
 
         if(sub_costs[i]->target_size != target_size)
             PLERROR( "CombiningCostsModule::build_(): sub_costs[%d]->target_size"
                      " (%d)\n"
                      "should be equal to %d.\n",
-                     i,sub_costs[i]->target_size, target_size);  
+                     i,sub_costs[i]->target_size, target_size);
     }
 
     sub_costs_values.resize( n_sub_costs );

Modified: trunk/plearn_learners/online/CostModule.h
===================================================================
--- trunk/plearn_learners/online/CostModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/CostModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -113,7 +113,7 @@
     // TODO Had to override to compile, this is weird.
     virtual void bpropUpdate(const Mat& input, const Mat& output,
                              Mat& input_gradient, const Mat& output_gradient,
-                             bool accumulate=false) 
+                             bool accumulate=false)
     {
         inherited::bpropUpdate(input, output, input_gradient, output_gradient,
                 accumulate);

Modified: trunk/plearn_learners/online/CrossEntropyCostModule.cc
===================================================================
--- trunk/plearn_learners/online/CrossEntropyCostModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/CrossEntropyCostModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -125,7 +125,7 @@
 
     for (int i = 0; i < inputs.length(); i++)
         fprop(inputs(i), targets(i), costs(i,0));
- 
+
 }
 
 void CrossEntropyCostModule::bpropAccUpdate(const TVec<Mat*>& ports_value,
@@ -166,8 +166,8 @@
         prediction_grad->resize(batch_size, getPortSizes()(0,1));
 
         for( int i=0; i < batch_size; i++ )
-            for ( int j=0; j < target->width(); j++ ) 
-                (*prediction_grad)(i, j) += 
+            for ( int j=0; j < target->width(); j++ )
+                (*prediction_grad)(i, j) +=
                 (*cost_grad)(i,0)*((*target)(i,j) - sigmoid(-(*prediction)(i,j) ));
     }
 

Modified: trunk/plearn_learners/online/CrossEntropyCostModule.h
===================================================================
--- trunk/plearn_learners/online/CrossEntropyCostModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/CrossEntropyCostModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -67,7 +67,7 @@
     virtual void fprop(const Vec& input, const Vec& target, real& cost) const;
     virtual void fprop(const Vec& input, const Vec& target, Vec& cost) const;
     virtual void fprop(const Mat& input, const Mat& target, Mat& cost) const;
-   
+
     //! New version of backpropagation
     virtual void bpropAccUpdate(const TVec<Mat*>& ports_value,
                                 const TVec<Mat*>& ports_gradient);

Modified: trunk/plearn_learners/online/ForwardModule.cc
===================================================================
--- trunk/plearn_learners/online/ForwardModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/ForwardModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -87,7 +87,7 @@
         "will be forgotten.");
 
     // 'nosave' options.
-    
+
     // 'current' is an option only so it can be modified in server mode for
     // instance, in order to bypass a call to build (yes, this is a hack!).
     declareOption(ol, "current", &ForwardModule::current,

Modified: trunk/plearn_learners/online/ForwardModule.h
===================================================================
--- trunk/plearn_learners/online/ForwardModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/ForwardModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -156,7 +156,7 @@
 
     //! Index of the module currently being used in the 'modules' list.
     int current;
-    
+
     //#####  Protected Member Functions  ######################################
 
     //! Declares the class options.

Modified: trunk/plearn_learners/online/GradNNetLayerModule.cc
===================================================================
--- trunk/plearn_learners/online/GradNNetLayerModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/GradNNetLayerModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -95,7 +95,7 @@
 
     // Add bias.
     resizeOnes(n);
-    externalProductAcc(outputs, ones, bias); // could be more efficient, but not critical 
+    externalProductAcc(outputs, ones, bias); // could be more efficient, but not critical
 }
 
 /////////////////
@@ -282,8 +282,8 @@
     }
     step_number += n;
 }
-    
 
+
 //////////////////
 // bbpropUpdate //
 //////////////////

Modified: trunk/plearn_learners/online/GradNNetLayerModule.h
===================================================================
--- trunk/plearn_learners/online/GradNNetLayerModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/GradNNetLayerModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -122,7 +122,7 @@
                              Mat& input_gradients,
                              const Mat& output_gradients,
                              bool accumulate = false);
-    
+
     virtual void bbpropUpdate(const Vec& input, const Vec& output,
                               const Vec& output_gradient,
                               const Vec& output_diag_hessian);
@@ -155,7 +155,7 @@
 
     //! A vector filled with all ones.
     Vec ones;
-    
+
     //#####  Protected Options  ###############################################
 
 protected:

Modified: trunk/plearn_learners/online/LayerCostModule.cc
===================================================================
--- trunk/plearn_learners/online/LayerCostModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/LayerCostModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -95,15 +95,15 @@
     declareOption(ol, "nstages_max", &LayerCostModule::nstages_max,
                   OptionBase::buildoption,
         "Maximal number of updates for which the gradient of the cost function will be propagated.\n"
-	"-1 means: always train without limit.\n"
+        "-1 means: always train without limit.\n"
         );
 
     declareOption(ol, "optimization_strategy", &LayerCostModule::optimization_strategy,
                   OptionBase::buildoption,
         "Strategy to compute the gradient:\n"
-	"- \"standard\": standard computation\n"
-	"- \"half\": we will propagate the gradient only on units tagged as i < j.\n"
-	"- \"random_half\": idem than 'half' with the order of the indices that changes randomly during training.\n"
+        "- \"standard\": standard computation\n"
+        "- \"half\": we will propagate the gradient only on units tagged as i < j.\n"
+        "- \"random_half\": idem than 'half' with the order of the indices that changes randomly during training.\n"
         );
 
     declareOption(ol, "momentum", &LayerCostModule::momentum,
@@ -202,22 +202,22 @@
             inputs_histo.resize(input_size,histo_size);
         HISTO_STEP = 1.0/(real)histo_size;
 
-	if( cost_function == "kl_div" )
-	{
-	    cache_differ_count_i.resize(input_size);
-	    cache_differ_count_j.resize(input_size);
-	    cache_n_differ.resize(input_size);
-	    for( int i = 0; i < input_size; i ++)
-	    {
-	        cache_differ_count_i[i].resize(i);
-	        cache_differ_count_j[i].resize(i);
-	        cache_n_differ[i].resize(i);
-  	        for( int j = 0; j < i; j ++)
-	        {
-	            cache_differ_count_i[i][j].resize(histo_size);
-		    cache_differ_count_j[i][j].resize(histo_size);
-		    cache_n_differ[i][j].resize(histo_size);
-	        }
+        if( cost_function == "kl_div" )
+        {
+            cache_differ_count_i.resize(input_size);
+            cache_differ_count_j.resize(input_size);
+            cache_n_differ.resize(input_size);
+            for( int i = 0; i < input_size; i ++)
+            {
+                cache_differ_count_i[i].resize(i);
+                cache_differ_count_j[i].resize(i);
+                cache_n_differ[i].resize(i);
+                for( int j = 0; j < i; j ++)
+                {
+                    cache_differ_count_i[i][j].resize(histo_size);
+                    cache_differ_count_j[i][j].resize(histo_size);
+                    cache_n_differ[i][j].resize(histo_size);
+                }
             }
         }
     }
@@ -279,7 +279,7 @@
 
     inputs_expectation.clear();
     inputs_stds.clear();
-    
+
     inputs_correlations.clear();
     inputs_cross_quadratic_mean.clear();
     if( momentum > 0.0)
@@ -310,7 +310,7 @@
     deepCopyField(cache_differ_count_i, copies);
     deepCopyField(cache_differ_count_j, copies);
     deepCopyField(cache_n_differ, copies);
-    
+
     deepCopyField(ports, copies);
 }
 
@@ -355,7 +355,7 @@
     }
     else
         costs.clear();
-    
+
     if( !is_cost_function_stochastic )
     {
         PLASSERT( inputs.width() == input_size );
@@ -388,8 +388,8 @@
         //! ************************************************************
 
 
-	    Mat histo;
-	    computeHisto( inputs, histo );
+            Mat histo;
+            computeHisto( inputs, histo );
             costs(0,0) = computeKLdiv( histo );
         }
         else if( cost_function == "kl_div_simple" )
@@ -404,7 +404,7 @@
         //! ************************************************************
 
             Mat histo;
-	    computeSafeHisto( inputs, histo );
+            computeSafeHisto( inputs, histo );
 
             // Computing the KL divergence
             for (int i = 0; i < input_size; i++)
@@ -430,8 +430,8 @@
         //! ************************************************************
 
             Vec expectation;
-	    Mat cross_quadratic_mean;
-	    computePascalStatistics( inputs, expectation, cross_quadratic_mean );
+            Mat cross_quadratic_mean;
+            computePascalStatistics( inputs, expectation, cross_quadratic_mean );
 
             // Computing the cost
             for (int i = 0; i < input_size; i++)
@@ -461,9 +461,9 @@
         //! ************************************************************
 
             Vec expectation;
-	    Mat cross_quadratic_mean;
+            Mat cross_quadratic_mean;
             Vec stds;
-	    Mat correlations;
+            Mat correlations;
             computeCorrelationStatistics( inputs, expectation, cross_quadratic_mean, stds, correlations );
 
             // Computing the cost
@@ -605,18 +605,18 @@
     if( p_inputs_grad && p_inputs_grad->isEmpty()
         && p_cost_grad && !p_cost_grad->isEmpty() )
     {
-	PLASSERT( p_inputs && !p_inputs->isEmpty());
+        PLASSERT( p_inputs && !p_inputs->isEmpty());
         int n_samples = p_inputs->length();
-	PLASSERT( p_cost_grad->length() == n_samples );
-	PLASSERT( p_cost_grad->width() == 1 );
+        PLASSERT( p_cost_grad->length() == n_samples );
+        PLASSERT( p_cost_grad->width() == 1 );
 
         bpropUpdate( *p_inputs, *p_inputs_grad);
 
         for( int isample = 0; isample < n_samples; isample++ )
-	    for( int i = 0; i < input_size; i++ )
-	        (*p_inputs_grad)(isample, i) *= (*p_cost_grad)(isample,0);
+            for( int i = 0; i < input_size; i++ )
+                (*p_inputs_grad)(isample, i) *= (*p_cost_grad)(isample,0);
 
-	checkProp(ports_gradient);
+        checkProp(ports_gradient);
     }
     else if( !p_inputs_grad && !p_cost_grad )
         return;
@@ -653,8 +653,8 @@
         for (int isample = 0; isample < n_samples; isample++)
         {
             real qi, qj, comp_qi, comp_qj;
-	    Vec comp_q(input_size), log_term(input_size);
-	    for (int i = 0 ; i < input_size ; i++ )
+            Vec comp_q(input_size), log_term(input_size);
+            for (int i = 0 ; i < input_size ; i++ )
             {
                 qi = inputs(isample,i);
                 comp_qi = 1.0 - qi;
@@ -673,7 +673,7 @@
                     inputs(isample,i) += log_term[j] + qj/qi - comp_qi/comp_qj;
                     // The symetric part (loop  j=i+1...input_size)
                     if( bprop_all_terms )
-		        inputs(isample,j) += log_term[i] + qi/qj - comp_qj/comp_qi;
+                        inputs(isample,j) += log_term[i] + qi/qj - comp_qj/comp_qi;
                 }
             }
                 for (int i = 0; i < input_size; i++ )
@@ -686,7 +686,7 @@
         for (int isample = 0; isample < n_samples; isample++)
         {
             real qi, qj, comp_qi, comp_qj;
-	    Vec comp_q(input_size), log_term(input_size);
+            Vec comp_q(input_size), log_term(input_size);
             for (int i = 0; i < input_size; i++ )
             {
                 qi = inputs(isample,i);
@@ -710,7 +710,7 @@
                     inputs_grad(isample,i) += (qj - qi)*comp_qi - log_term[i] + log_term[j];
                     // The symetric part (loop  j=i+1...input_size)
                     if( bprop_all_terms )
-		        inputs_grad(isample,j) += (qi - qj)*comp_qj - log_term[j] + log_term[i];
+                        inputs_grad(isample,j) += (qi - qj)*comp_qj - log_term[j] + log_term[i];
                 }
             }
             for (int i = 0; i < input_size; i++ )
@@ -724,8 +724,8 @@
         real cost_before = computeKLdiv( true );
 
         if( !bprop_all_terms )
-	    PLERROR("kl_div with bprop_all_terms=false not implemented yet");
-    
+            PLERROR("kl_div with bprop_all_terms=false not implemented yet");
+
         for (int isample = 0; isample < n_samples; isample++)
         {
             real qi, qj;
@@ -735,11 +735,11 @@
             {
                 qi=inputs(isample,i);
                 if( histo_index(qi) < histo_size-1 )
-                { 
+                {
                     inputs(isample,i) += dq(qi);
                     computeHisto(inputs);
                     real cost_after = computeKLdiv( false );
-                    inputs(isample,i) -= dq(qi); 
+                    inputs(isample,i) -= dq(qi);
                     inputs_grad(isample, i) = (cost_after - cost_before)*1./dq(qi);
                 }
                 //else inputs_grad(isample, i) = 0.;
@@ -747,7 +747,7 @@
                 continue;
 
                 inputs_grad(isample, i) = 0.;
-                    
+
                 qi = inputs(isample,i);
                 int index_i = histo_index(qi);
                 if( ( index_i == histo_size-1 ) ) // we do not care about this...
@@ -755,7 +755,7 @@
                 real over_dqi=1.0/dq(qi);
                 // qi + dq(qi) ==> | p_inputs_histo(i,index_i)   - one_count
                 //                 \ p_inputs_histo(i,index_i+shift_i) + one_count
-                    		    
+
                 for (int j = 0; j < i; j++)
                 {
                     inputs_grad(isample, i) += delta_KLdivTerm(i, j, index_i, over_dqi);
@@ -767,17 +767,17 @@
                     real over_dqj=1.0/dq(qj);
                     // qj + dq(qj) ==> | p_inputs_histo(j,index_j)   - one_count
                     //                 \ p_inputs_histo(j,index_j+shift_j) + one_count
-                        
+
                     inputs_grad(isample, j) += delta_KLdivTerm(j, i, index_j, over_dqj);
                 }
             }
-        }            
+        }
     } // END cost_function == "kl_div"
 
     else if( cost_function == "kl_div_simple" )
     {
         computeSafeHisto(inputs);
-            
+
         for (int isample = 0; isample < n_samples; isample++)
         {
             // Computing the difference of KL divergence
@@ -800,7 +800,7 @@
                     inputs_grad(isample, i) += delta_SafeKLdivTerm(i, j, index_i, over_dqi);
 
                     if( bprop_all_terms )
-		    {
+                    {
                         qj = inputs(isample,j);
                         int index_j = histo_index(qj);
                         if( ( index_j == histo_size-1 ) || ( index_j == 0 ) )
@@ -808,9 +808,9 @@
                         real over_dqj=1.0/dq(qj);
                         // qj + dq(qj) ==> | p_inputs_histo(j,index_j)   - one_count
                         //                 \ p_inputs_histo(j,index_j+shift_j) + one_count
-                        
-		        inputs_grad(isample, j) += delta_SafeKLdivTerm(j, i, index_j, over_dqj);
-		    }
+
+                        inputs_grad(isample, j) += delta_SafeKLdivTerm(j, i, index_j, over_dqj);
+                    }
                 }
             }
 
@@ -838,7 +838,7 @@
                     real d_temp = deriv_func_(inputs_cross_quadratic_mean(i,j));
                     qj = inputs(isample,j);
                     inputs_grad(isample, i) += d_temp *qj;
-	            if( bprop_all_terms )
+                    if( bprop_all_terms )
                         inputs_grad(isample, j) += d_temp *qi;
                 }
             }
@@ -885,7 +885,7 @@
                     continue;
                 }
                 //!  dCROSSij_dqj[i] = d[ E(QiQj)-E(Qi)E(Qj) ]/d[qj(t)]
-                //!                  = ( qi(t) - E(Qi) ) / n_samples 
+                //!                  = ( qi(t) - E(Qi) ) / n_samples
                 //!
                 //!  dSTDi_dqi[i] = d[ STD(Qi) ]/d[qi(t)]
                 //!               = d[ sqrt( E(Qi^2) -E(Qi)^2 ]/d[qi(t)]
@@ -901,14 +901,14 @@
                 for (int j = 0; j < i; j++)
                 {
                     if( fast_exact_is_equal( inputs_correlations(i,j), 0.) )
-		    {
-			if (isample == 0)
-			    PLWARNING("correlation(i,j)=0 for i=%d, j=%d", i, j);
+                    {
+                        if (isample == 0)
+                            PLWARNING("correlation(i,j)=0 for i=%d, j=%d", i, j);
                         continue;
                     }
                     qj = inputs(isample,j);
                     real correlation_denum = inputs_stds[i]*inputs_stds[j];
-		    real squared_correlation_denum = correlation_denum * correlation_denum;
+                    real squared_correlation_denum = correlation_denum * correlation_denum;
                     if( fast_exact_is_equal( squared_correlation_denum, 0. ) )
                         continue;
                     real dfunc_dCorr = deriv_func_( inputs_correlations(i,j) );
@@ -916,16 +916,16 @@
                                              - inputs_expectation[i]*inputs_expectation[j] );
 
                     if( correlation_num/correlation_denum - inputs_correlations(i,j) > 0.0000001 )
-			PLERROR( "num/denum (%f) <> correlation (%f) for (i,j)=(%d,%d)",
-				 correlation_num/correlation_denum, inputs_correlations(i,j),i,j);
+                        PLERROR( "num/denum (%f) <> correlation (%f) for (i,j)=(%d,%d)",
+                                 correlation_num/correlation_denum, inputs_correlations(i,j),i,j);
 
-                    inputs_grad(isample, i) += dfunc_dCorr * ( 
+                    inputs_grad(isample, i) += dfunc_dCorr * (
                                                  correlation_denum * dCROSSij_dqj[j]
                                                - correlation_num * dSTDi_dqi[i] * inputs_stds[j]
                                                  ) / squared_correlation_denum;
 
                     if( bprop_all_terms )
-			inputs_grad(isample, j) += dfunc_dCorr * ( 
+                        inputs_grad(isample, j) += dfunc_dCorr * (
                                                      correlation_denum * dCROSSij_dqj[i]
                                                    - correlation_num * dSTDi_dqi[j] * inputs_stds[i]
                                                      ) / squared_correlation_denum;
@@ -962,11 +962,11 @@
     int n_samples = inputs.length();
     one_count = 1. / (real)n_samples;
     Vec input;
-    
+
     expectation.resize( input_size );
-    expectation.clear(); 
+    expectation.clear();
     cross_quadratic_mean.resize(input_size,input_size);
-    cross_quadratic_mean.clear(); 
+    cross_quadratic_mean.clear();
 
     inputs_expectation.clear();
     inputs_cross_quadratic_mean.clear();
@@ -1065,9 +1065,9 @@
     Vec input;
 
     expectation.resize( input_size );
-    expectation.clear(); 
+    expectation.clear();
     cross_quadratic_mean.resize(input_size,input_size);
-    cross_quadratic_mean.clear(); 
+    cross_quadratic_mean.clear();
     stds.resize( input_size );
     stds.clear();
     correlations.resize(input_size,input_size);
@@ -1092,27 +1092,27 @@
         cross_quadratic_mean(i,i) *= one_count;
 
         if( fast_is_equal(momentum, 0.)
-	||  !during_training )
+            ||  !during_training )
         {
- 	    //! Computation of the standard deviations
-	    //! requires temporary variable because of numerical imprecision
-	    real tmp = cross_quadratic_mean(i,i) - expectation[i] * expectation[i];
-	    if( tmp > 0. )  //  std[i] = 0 by default
-	        stds[i] = sqrt( tmp );
+            //! Computation of the standard deviations
+            //! requires temporary variable because of numerical imprecision
+            real tmp = cross_quadratic_mean(i,i) - expectation[i] * expectation[i];
+            if( tmp > 0. )  //  std[i] = 0 by default
+                stds[i] = sqrt( tmp );
         }
-	
+
         for (int j = 0; j < i; j++)
         {
             //! Normalization (2/2)
             cross_quadratic_mean(i,j) *= one_count;
 
             if( fast_is_equal(momentum, 0.)
-	    ||  !during_training )
-            {	    
-	        //! Correlations
-	        real tmp = stds[i] * stds[j];
+                ||  !during_training )
+            {
+                //! Correlations
+                real tmp = stds[i] * stds[j];
                 if( !fast_is_equal(tmp, 0.) )  //  correlations(i,j) = 1 by default
-	            correlations(i,j) = ( cross_quadratic_mean(i,j)
+                    correlations(i,j) = ( cross_quadratic_mean(i,j)
                                           - expectation[i]*expectation[j] ) / tmp;
             }
         }
@@ -1131,19 +1131,19 @@
                                         +(1.0-momentum)*cross_quadratic_mean(i,i);
             inputs_cross_quadratic_mean_trainMemory(i,i) = cross_quadratic_mean(i,i);
 
-	    real tmp = cross_quadratic_mean(i,i) - expectation[i] * expectation[i];
-	    if( tmp > 0. )  //  std[i] = 0 by default
-	        stds[i] = sqrt( tmp );
-	    
+            real tmp = cross_quadratic_mean(i,i) - expectation[i] * expectation[i];
+            if( tmp > 0. )  //  std[i] = 0 by default
+                stds[i] = sqrt( tmp );
+
             for (int j = 0; j < i; j++)
             {
-                 cross_quadratic_mean(i,j) = momentum*inputs_cross_quadratic_mean_trainMemory(i,j)
-                                             +(1.0-momentum)*cross_quadratic_mean(i,j);
-                 inputs_cross_quadratic_mean_trainMemory(i,j) = cross_quadratic_mean(i,j);
+                cross_quadratic_mean(i,j) = momentum*inputs_cross_quadratic_mean_trainMemory(i,j)
+                    +(1.0-momentum)*cross_quadratic_mean(i,j);
+                inputs_cross_quadratic_mean_trainMemory(i,j) = cross_quadratic_mean(i,j);
 
-	         tmp = stds[i] * stds[j];
+                tmp = stds[i] * stds[j];
                  if( !fast_is_equal(tmp, 0.) )  //  correlations(i,j) = 1 by default
-	             correlations(i,j) = ( cross_quadratic_mean(i,j)
+                     correlations(i,j) = ( cross_quadratic_mean(i,j)
                                          - expectation[i]*expectation[j] ) / tmp;
 
             }
@@ -1162,7 +1162,7 @@
     for (int i = 0; i < input_size; i++)
         for (int j = 0; j < i; j++)
         {
-            // These variables are used in case one bin of 
+            // These variables are used in case one bin of
             // the histogram is empty for one unit
             // and not for another one ( (Nj-Ni).log(Ni/Nj) = nan ).
             // In such case, we ''differ'' the count for the next bin and so on.
@@ -1197,14 +1197,14 @@
                 }
             }
 //                    if( differ_count_i > 0.0 )
-//                    {   
-//                        "cas ou on regroupe avec le dernier";   
+//                    {
+//                        "cas ou on regroupe avec le dernier";
 //                        cost -= KLdivTerm(last_positive_Ni_k,last_positive_Nj_k)
 //                                  *(real)(1+last_n_differ) *HISTO_STEP;
 //                        cost += KLdivTerm(last_positive_Ni_k+differ_count_i,last_positive_Nj_k)
-//                                 *(real)(1+last_n_differ+n_differ) *HISTO_STEP; 
+//                                 *(real)(1+last_n_differ+n_differ) *HISTO_STEP;
 //                    }
-//                     
+//
 //                    else if ( differ_count_j > 0.0 )
 //                    {
 //                        "cas ou on regroupe avec le dernier";
@@ -1212,7 +1212,7 @@
 //                                 *(real)(1+last_n_differ) *HISTO_STEP;
 //                        cost += KLdivTerm(last_positive_Ni_k,last_positive_Nj_k+differ_count_j)
 //                                 *(real)(1+last_n_differ+n_differ) *HISTO_STEP;
-//                    }    
+//                    }
         }
     // Normalization w.r.t. number of units
     return cost *norm_factor;
@@ -1226,12 +1226,12 @@
             for (int i = 0; i < input_size; i++)
                 for (int j = 0; j < i; j++)
                 {
-                    // These variables are used in case one bin of 
+                    // These variables are used in case one bin of
                     // the histogram is empty for one unit
                     // and not for another one ( (Nj-Ni).log(Ni/Nj) = nan ).
                     // In such case, we ''differ'' the count for the next bin and so on.
-		    cache_differ_count_i[ i ][ j ].clear();
-		    cache_differ_count_j[ i ][ j ].clear();
+                    cache_differ_count_i[ i ][ j ].clear();
+                    cache_differ_count_j[ i ][ j ].clear();
                     cache_n_differ[i][j].fill( 0. );
 //                    real last_positive_Ni_k, last_positive_Nj_k;
 //                    real last_n_differ;
@@ -1242,17 +1242,17 @@
 
                         if( fast_exact_is_equal(Ni_k, 0.0) )
                         {
-			    if( k < histo_size - 1 ) // "cas ou on regroupe avec le dernier";
-			    {
-			        cache_differ_count_j[i][j][ k+1 ] = Nj_k;
+                            if( k < histo_size - 1 ) // "cas ou on regroupe avec le dernier";
+                            {
+                                cache_differ_count_j[i][j][ k+1 ] = Nj_k;
                                 cache_n_differ[i][j][ k+1 ] = cache_n_differ[i][j][ k ] + 1;
                             }
-			}
+                        }
                         else if( fast_exact_is_equal(Nj_k, 0.0) )
                         {
-			    if( k < histo_size - 1 ) // "cas ou on regroupe avec le dernier";
-			    {
-			        cache_differ_count_i[i][j][ k+1 ] = Ni_k;
+                            if( k < histo_size - 1 ) // "cas ou on regroupe avec le dernier";
+                            {
+                                cache_differ_count_i[i][j][ k+1 ] = Ni_k;
                                 cache_n_differ[i][j][ k+1 ] = cache_n_differ[i][j][ k ] + 1;
                             }
                         }
@@ -1268,7 +1268,7 @@
 //                    else if ( cache_differ_count_j[i][j][ histo_size - 1 ] > 0.0 )
 //                        "cas ou on regroupe avec le dernier";
                     }
-		}
+                }
             // Normalization w.r.t. number of units
             return cost *norm_factor;
     }
@@ -1288,7 +1288,7 @@
     //   => ) the input(isample,i) has been counted
 
     real grad_update = 0.0;
-    
+
     real Ni_ki, Nj_ki, Ni_ki_shift1, Nj_ki_shift1;
     real n_differ_before_ki, n_differ_before_ki_shift1;
 
@@ -1322,42 +1322,42 @@
     {
         // removing the term of the sum that will be modified
         grad_update -= KLdivTerm( Ni_ki,
-	                          Nj_ki )
-	               * ( 1 + n_differ_before_ki);
+                                  Nj_ki )
+                       * ( 1 + n_differ_before_ki);
 
         if( fast_exact_is_equal(Ni_ki, one_count) )
         {
             additional_differ_count_j_after = Nj_ki;
-	    n_differ_after_ki_shift1 = n_differ_after_ki + 1;
-	                          // = n_differ_before_ki + 1;
+            n_differ_after_ki_shift1 = n_differ_after_ki + 1;
+                                  // = n_differ_before_ki + 1;
         }
         else
-	{
+        {
             // adding the term of the sum with its modified value
             grad_update += KLdivTerm( Ni_ki - one_count,
-	                              Nj_ki )
-	                   * ( 1 + n_differ_after_ki );
-	}
+                                      Nj_ki )
+                           * ( 1 + n_differ_after_ki );
+        }
 
         if( !fast_exact_is_equal(Nj_ki_shift1,0.0) )
         {
             // adding the term of the sum with its modified value
             grad_update += KLdivTerm( Ni_ki_shift1 + one_count,
-	                                  Nj_ki_shift1 + additional_differ_count_j_after )
-	                       * ( 1 + n_differ_after_ki_shift1 );
+                                          Nj_ki_shift1 + additional_differ_count_j_after )
+                               * ( 1 + n_differ_after_ki_shift1 );
 
             if( !fast_exact_is_equal(Ni_ki_shift1, 0.0) ) // "cas ou on regroupe avec le dernier";
             {
                 // removing the term of the sum that will be modified
                 grad_update -= KLdivTerm( Ni_ki_shift1,
-		                          Nj_ki_shift1 )
-		               * ( 1 + n_differ_before_ki_shift1 );                
+                                          Nj_ki_shift1 )
+                               * ( 1 + n_differ_before_ki_shift1 );
             }
             else // ( Ni_ki_shift1 == 0.0 )
             {
                 // We search   ki' > k(i)+1   such that   n(i,ki') > 0
                 real additional_differ_count_j_before = 0.;
-		real additional_n_differ_before_ki_shift1 = 0.;
+                real additional_n_differ_before_ki_shift1 = 0.;
                 int ki;
                 for (ki = index_i+2; ki < histo_size; ki++)
                 {
@@ -1369,15 +1369,15 @@
                 if( ki < histo_size )
                 {
                     grad_update -= KLdivTerm( inputs_histo( i, ki ),
-		                              Nj_ki_shift1 + additional_differ_count_j_before )
-		                   * ( 1 + n_differ_before_ki_shift1 + additional_n_differ_before_ki_shift1 );
+                                              Nj_ki_shift1 + additional_differ_count_j_before )
+                                   * ( 1 + n_differ_before_ki_shift1 + additional_n_differ_before_ki_shift1 );
 
                     if( additional_differ_count_j_before > 0. )
-		    // We have to report the additional count for unit j
+                    // We have to report the additional count for unit j
                     {
                         grad_update += KLdivTerm( inputs_histo( i, ki ),
-			                          additional_differ_count_j_before )
-			               * ( additional_n_differ_before_ki_shift1 );
+                                                  additional_differ_count_j_before )
+                                       * ( additional_n_differ_before_ki_shift1 );
                     }
                 }
             }
@@ -1385,7 +1385,7 @@
         else // ( Nj_ki_shift1 == 0.0 )
         {
             real additional_differ_count_i_before = 0.;
-	    // We search kj > ki+1 tq inputs_histo( j, kj ) > 0.
+            // We search kj > ki+1 tq inputs_histo( j, kj ) > 0.
             int kj;
             for( kj = index_i+2; kj < histo_size; kj++)
             {
@@ -1394,64 +1394,64 @@
                 if( inputs_histo( j, kj ) > 0. )
                     break;
             }
-	    if ( !fast_exact_is_equal(additional_differ_count_j_after, 0. ) )
-	        n_differ_after_ki_shift1 = n_differ_before_ki_shift1;
+            if ( !fast_exact_is_equal(additional_differ_count_j_after, 0. ) )
+                n_differ_after_ki_shift1 = n_differ_before_ki_shift1;
             if( kj < histo_size )
             {
                 if ( fast_exact_is_equal(n_differ_after_ki_shift1, n_differ_before_ki_shift1) )
-		{
-		    // ( no qj were differed after we changed count at bin ki )
-		    // OR ( some qj were differed to bin ki+1 AND the bin were not empty )
+                {
+                    // ( no qj were differed after we changed count at bin ki )
+                    // OR ( some qj were differed to bin ki+1 AND the bin were not empty )
                     grad_update += KLdivTerm( Ni_ki_shift1 + additional_differ_count_i_before + one_count,
-		                             inputs_histo( j, kj ) + additional_differ_count_j_after )
-		                   * ( 1 + n_differ_after_ki_shift1 );
-                }	   		
-		else
-		{
-		    PLASSERT( n_differ_before_ki_shift1 > n_differ_after_ki_shift1 );
+                                              inputs_histo( j, kj ) + additional_differ_count_j_after )
+                                   * ( 1 + n_differ_after_ki_shift1 );
+                }
+                else
+                {
+                    PLASSERT( n_differ_before_ki_shift1 > n_differ_after_ki_shift1 );
                     grad_update += KLdivTerm( Ni_ki_shift1 + one_count,
-		                              additional_differ_count_j_after )
-		                   * ( 1 + n_differ_after_ki_shift1 );
+                                              additional_differ_count_j_after )
+                                   * ( 1 + n_differ_after_ki_shift1 );
                     grad_update += KLdivTerm( additional_differ_count_i_before,
-		                              inputs_histo( j, kj ) )
-		                   * ( n_differ_before_ki_shift1 - n_differ_after_ki_shift1 );
+                                              inputs_histo( j, kj ) )
+                                   * ( n_differ_before_ki_shift1 - n_differ_after_ki_shift1 );
                 }
 
                 if( !fast_exact_is_equal(Ni_ki_shift1 + additional_differ_count_i_before,0.0) )
-		{
+                {
                     grad_update -= KLdivTerm( Ni_ki_shift1 + additional_differ_count_i_before,
-		                              inputs_histo( j, kj ) )
-		                   * ( 1 + n_differ_before_ki_shift1 );
-	        }
-		else // ( Ni_ki_shift1' == 0 == Nj_ki_shift1 ) && ( pas de q[i] avant q[j']... )
-		{
-		    // We search ki' > kj+1 tq inputs_histo( i, ki' ) > 0.
+                                              inputs_histo( j, kj ) )
+                                   * ( 1 + n_differ_before_ki_shift1 );
+                }
+                else // ( Ni_ki_shift1' == 0 == Nj_ki_shift1 ) && ( pas de q[i] avant q[j']... )
+                {
+                    // We search ki' > kj+1 tq inputs_histo( i, ki' ) > 0.
                     real additional_differ_count_j_before = 0.;
-		    real additional_n_differ_before_ki_shift1 = 0.;
-		    int kj2;
+                    real additional_n_differ_before_ki_shift1 = 0.;
+                    int kj2;
                     for( kj2 = kj+1; kj2 < histo_size; kj2++)
                     {
-			additional_differ_count_j_before += inputs_histo( j, kj2 );
+                        additional_differ_count_j_before += inputs_histo( j, kj2 );
                         additional_n_differ_before_ki_shift1 += 1;
                         if( inputs_histo( i, kj2 ) > 0. )
                             break;
-		    }
-		    if ( fast_exact_is_equal(additional_differ_count_j_before, 0. ) )
-		        n_differ_after_ki_shift1 = n_differ_before_ki_shift1;
+                    }
+                    if ( fast_exact_is_equal(additional_differ_count_j_before, 0. ) )
+                        n_differ_after_ki_shift1 = n_differ_before_ki_shift1;
                     if( kj2 < histo_size )
-		    {
-		        grad_update -= KLdivTerm( inputs_histo( i, kj2 ),
-			                          Nj_ki_shift1 + additional_differ_count_j_before )
-		                       * ( 1 + n_differ_before_ki_shift1 + additional_n_differ_before_ki_shift1 );
+                    {
+                        grad_update -= KLdivTerm( inputs_histo( i, kj2 ),
+                                                  Nj_ki_shift1 + additional_differ_count_j_before )
+                                       * ( 1 + n_differ_before_ki_shift1 + additional_n_differ_before_ki_shift1 );
 
                         if( additional_differ_count_j_before > 0. )
-			{
+                        {
                             grad_update += KLdivTerm( inputs_histo( i, kj2 ),
-			                              additional_differ_count_j_before )
-		                           * ( additional_n_differ_before_ki_shift1 );
+                                                      additional_differ_count_j_before )
+                                           * ( additional_n_differ_before_ki_shift1 );
                         }
                     }
-	        }
+                }
             }
         }
     }
@@ -1505,15 +1505,15 @@
 {
     int n_samples = inputs.length();
     one_count = 1. / (real)n_samples;
-    
+
     histo.resize(input_size,histo_size);
-    histo.clear(); 
+    histo.clear();
     for (int isample = 0; isample < n_samples; isample++)
     {
         Vec input = inputs(isample);
         for (int i = 0; i < input_size; i++)
-	{
-	    PLASSERT( histo_index(input[i]) < histo_size);
+        {
+            PLASSERT( histo_index(input[i]) < histo_size);
             histo( i, histo_index(input[i]) ) += one_count;
         }
     }

Modified: trunk/plearn_learners/online/LayerCostModule.h
===================================================================
--- trunk/plearn_learners/online/LayerCostModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/LayerCostModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -58,7 +58,7 @@
     //! Generic name of the cost function
     string cost_function;
 
-    //! Maximum number of stages we want to propagate the gradient    
+    //! Maximum number of stages we want to propagate the gradient
     int nstages_max;
 
     //! Parameter to compute moving means in non stochastic cost functions
@@ -66,9 +66,9 @@
 
     //! Kind of optimization
     string optimization_strategy;
-    
+
     //! Parameter in pascal's cost function
-    real alpha;    
+    real alpha;
 
     //! For non stochastic KL divergence cost function
     int histo_size;
@@ -91,7 +91,7 @@
 
     //! Variables for (non stochastic) Pascal's/correlation function with momentum
     //! -------------------------------------------------------------
-    //! Statistics on outputs (estimated empiricially on the data)    
+    //! Statistics on outputs (estimated empiricially on the data)
     Vec inputs_expectation_trainMemory;
     Mat inputs_cross_quadratic_mean_trainMemory;
 
@@ -146,7 +146,7 @@
     virtual void computeCorrelationStatistics(const Mat& inputs);
     virtual void computeCorrelationStatistics(const Mat& inputs,
                                               Vec& expectation, Mat& cross_quadratic_mean,
-                                              Vec& stds, Mat& correlations) const;    
+                                              Vec& stds, Mat& correlations) const;
     //! Returns all ports in a RBMModule.
     virtual const TVec<string>& getPorts();
 
@@ -210,7 +210,7 @@
     real HISTO_STEP;
     //! the weight of a sample within a batch (usually, 1/n_samples)
 
-    mutable real one_count; 
+    mutable real one_count;
     TVec< TVec< Vec > > cache_differ_count_i;
     TVec< TVec< Vec > > cache_differ_count_j;
     TVec< TVec< Vec > > cache_n_differ;

Modified: trunk/plearn_learners/online/LinearCombinationModule.cc
===================================================================
--- trunk/plearn_learners/online/LinearCombinationModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/LinearCombinationModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -138,7 +138,7 @@
         // has build completed? there should be at least one input port + the output port
         PLERROR("LinearCombinationModule should have at least 2 ports (one input port and one output port)\n");
     PLASSERT( ports_value.length() == n_ports ); // is the input coherent with expected nPorts
-    
+
     const TVec<Mat*>& inputs = ports_value;
     Mat* output = ports_value[n_ports-1];
     if (output) {

Modified: trunk/plearn_learners/online/LinearCombinationModule.h
===================================================================
--- trunk/plearn_learners/online/LinearCombinationModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/LinearCombinationModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -57,7 +57,7 @@
     //#####  Public Build Options  ############################################
 
     //! ### declare public option fields (such as build options) here
-    
+
     //! the weights of the linear combination: one per input port
     Vec weights;
     //! whether to adapt the weights, and whether to clear them to 0 upon forget()

Modified: trunk/plearn_learners/online/LinearFilterModule.cc
===================================================================
--- trunk/plearn_learners/online/LinearFilterModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/LinearFilterModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -99,7 +99,7 @@
 
     // Add bias.
     resizeOnes(n);
-    externalProductAcc(outputs, ones, bias); // could be more efficient, but not critical 
+    externalProductAcc(outputs, ones, bias); // could be more efficient, but not critical
 }
 
 /////////////////
@@ -148,7 +148,7 @@
                 else
                     weights[i] = 0.;
             }
-            
+
             if( between_0_and_1 )
             {
                 if( weights[i] > 1. )
@@ -274,7 +274,7 @@
         resizeOnes(n);
         transposeProductScaleAcc(bias, output_gradients, ones, -avg_lr, real(1));
     }
-    
+
     // Update weights.
     for(int i_sample = 0; i_sample < outputs.length() ;i_sample++)
         for(int i = 0; i < output_size; i++ )
@@ -304,8 +304,8 @@
     }
     step_number += n;
 }
-    
 
+
 //////////////////
 // bbpropUpdate //
 //////////////////

Modified: trunk/plearn_learners/online/LinearFilterModule.h
===================================================================
--- trunk/plearn_learners/online/LinearFilterModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/LinearFilterModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -97,7 +97,7 @@
 
     //! The bias
     Vec bias;
-    
+
     bool no_bias;
     bool between_0_and_1;
 
@@ -125,7 +125,7 @@
                              Mat& input_gradients,
                              const Mat& output_gradients,
                              bool accumulate = false);
-    
+
     virtual void bbpropUpdate(const Vec& input, const Vec& output,
                               const Vec& output_gradient,
                               const Vec& output_diag_hessian);
@@ -158,7 +158,7 @@
 
     //! A vector filled with all ones.
     Vec ones;
-    
+
     //#####  Protected Options  ###############################################
 
 protected:

Modified: trunk/plearn_learners/online/LogaddOnBagsModule.cc
===================================================================
--- trunk/plearn_learners/online/LogaddOnBagsModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/LogaddOnBagsModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -97,7 +97,7 @@
 {
     for( int i = 0; i < input_size; i++ )
         accumulated_output[i] = logadd(accumulated_output[i],
-                                       input[i]); 
+                                       input[i]);
 }
 void LogaddOnBagsModule::fpropOutput(Vec& output)
 {

Modified: trunk/plearn_learners/online/LogaddOnBagsModule.h
===================================================================
--- trunk/plearn_learners/online/LogaddOnBagsModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/LogaddOnBagsModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -87,7 +87,7 @@
 protected:
 
     Vec accumulated_output;
-    
+
     //#####  Protected Member Functions  ######################################
 
     //! Declares the class options.

Modified: trunk/plearn_learners/online/ModuleLearner.cc
===================================================================
--- trunk/plearn_learners/online/ModuleLearner.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/ModuleLearner.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -342,17 +342,17 @@
                     bag_info = int(round(target.lastElement()));
                     isample ++;
                 }
-                isample = isample % train_set->length();                 
+                isample = isample % train_set->length();
             }
             if( stage + inputs.length() > nstages )
                 break;
             // Perform a training step.
-            trainingStep(inputs, targets, weights);              
+            trainingStep(inputs, targets, weights);
             // Handle training progress.
             stage += inputs.length();
             if (report_progress)
                 pb->update(stage - stage_init);
-        }    
+        }
     else
         while (stage + mbatch_size <= nstages) {
             // Obtain training samples.

Modified: trunk/plearn_learners/online/ModuleLearner.h
===================================================================
--- trunk/plearn_learners/online/ModuleLearner.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/ModuleLearner.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -190,7 +190,7 @@
     //! Matrix that contains only ones (used to fill weights at test time).
     mutable Mat all_ones;
 
-    //! Matrix that stores a copy of the costs 
+    //! Matrix that stores a copy of the costs
     //! (used to update the cost statistics).
     mutable Mat tmp_costs;
 

Modified: trunk/plearn_learners/online/ModuleStackModule.h
===================================================================
--- trunk/plearn_learners/online/ModuleStackModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/ModuleStackModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -95,7 +95,7 @@
                              Mat& input_gradients,
                              const Mat& output_gradients,
                              bool accumulate = false);
-    
+
     //! This version does not obtain the input gradient.
     virtual void bpropUpdate(const Vec& input, const Vec& output,
                              const Vec& output_gradient);

Modified: trunk/plearn_learners/online/ModuleTester.cc
===================================================================
--- trunk/plearn_learners/online/ModuleTester.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/ModuleTester.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -434,7 +434,7 @@
                             PLASSERT( out_val && out_prev && out_grad_ );
                             for (int oi = 0; oi < out_val->length(); oi++)
                                 for (int oj = 0; oj < out_val->width(); oj++) {
-                                    real diff = (*out_val)(oi, oj) - 
+                                    real diff = (*out_val)(oi, oj) -
                                         (*out_prev)(oi, oj);
                                     (*grad)(p, q) +=
                                         diff * (*out_grad_)(oi, oj) / step;

Modified: trunk/plearn_learners/online/NetworkConnection.h
===================================================================
--- trunk/plearn_learners/online/NetworkConnection.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/NetworkConnection.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -121,7 +121,7 @@
 
     //! Destination port.
     string dst_port;
-    
+
     //#####  Protected Options  ###############################################
 
 protected:

Modified: trunk/plearn_learners/online/NetworkModule.cc
===================================================================
--- trunk/plearn_learners/online/NetworkModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/NetworkModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -328,7 +328,7 @@
                                     tostring(count_null++));
                             all_modules.append(null);
                         /*} else {
-                            PLASSERT( all_modules.length() == 
+                            PLASSERT( all_modules.length() ==
                                         modules.length() + 1 );
                             null = all_modules.lastElement();
                         }*/
@@ -350,7 +350,7 @@
             }
         }
     }
-    
+
     // Construct fprop and bprop paths from the list of modules and
     // connections.
 
@@ -383,7 +383,7 @@
     map<const OnlineLearningModule*, int> module_to_index;
     for (int i = 0; i < all_modules.length(); i++)
         module_to_index[all_modules[i]] = i;
-    
+
     // Analyze the list of ports.
     // The 'port_correspondances' lists, for each module, the correspondances
     // between the modules' ports and the ports of the NetworkModule.

Modified: trunk/plearn_learners/online/NetworkModule.h
===================================================================
--- trunk/plearn_learners/online/NetworkModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/NetworkModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -141,7 +141,7 @@
     //! p1 the index of the port in this NetworkModule, and p2 the index of the
     //! corresponding port in the i-th module during fprop.
     TVec< TMat<int> > fprop_toplug;
-    
+
     //! The i-th element is the list of Mat* pointers being provided to the
     //! i-th module in a bprop step.
     TVec< TVec<Mat*> > bprop_data;

Modified: trunk/plearn_learners/online/OnBagsModule.cc
===================================================================
--- trunk/plearn_learners/online/OnBagsModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/OnBagsModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -72,7 +72,7 @@
 {
     PLASSERT( bagtarget_size > 0 );
     PLASSERT( input_size > 0 );
-    
+
     // The port story...
     ports.resize(0);
     portname_to_index.clear();
@@ -156,7 +156,7 @@
         wait_new_bag = false;
     }
     else
-    {   
+    {
         PLASSERT( !wait_new_bag );
         fpropAcc( input );
     }
@@ -206,19 +206,19 @@
     if( p_inputs_grad
         && p_output_grad && !p_output_grad->isEmpty() )
     {
-	    PLASSERT( p_inputs && !p_inputs->isEmpty());
-	    PLASSERT( p_bagtargets && !p_bagtargets->isEmpty());
+        PLASSERT( p_inputs && !p_inputs->isEmpty());
+        PLASSERT( p_bagtargets && !p_bagtargets->isEmpty());
         int n_samples = p_inputs->length();
-    	PLASSERT( p_output_grad->length() == n_samples );
-    	PLASSERT( p_output_grad->width() == output_size );
+        PLASSERT( p_output_grad->length() == n_samples );
+        PLASSERT( p_output_grad->width() == output_size );
         if( p_inputs_grad->isEmpty() )
         {
             p_inputs_grad->resize( n_samples, input_size);
             p_inputs_grad->clear();
         }
-        bpropUpdate( *p_inputs, *p_bagtargets, 
+        bpropUpdate( *p_inputs, *p_bagtargets,
                      *p_inputs_grad, *p_output_grad, true );
-    	checkProp(ports_gradient);
+        checkProp(ports_gradient);
     }
     else if( !p_inputs_grad && !p_output_grad )
         return;

Modified: trunk/plearn_learners/online/OnBagsModule.h
===================================================================
--- trunk/plearn_learners/online/OnBagsModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/OnBagsModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -64,7 +64,7 @@
     OnBagsModule();
 
     //!### Main functions to be coded in subclasses ###
-    
+
     //! update internal statistics needed to compute the output of bag
     virtual void fpropAcc(const Vec& input);
     virtual void fpropInit(const Vec& input);
@@ -74,7 +74,7 @@
     virtual void bprop( const Mat& baginputs,
                           const Vec& bagoutput_gradient,
                           Mat& baginputs_gradients);
-                          
+
    //!#################################################
 
     //! given the input and target, compute the cost
@@ -122,7 +122,7 @@
     virtual void forget();
 
 protected:
-    
+
     bool wait_new_bag;
 
     //! Map from a port name to its index in the 'ports' vector.

Modified: trunk/plearn_learners/online/OnlineLearningModule.cc
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/OnlineLearningModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -413,8 +413,8 @@
     return outputs;
 }
 
-map<string,Mat> OnlineLearningModule::namedBpropAccUpdate(map<string,Mat>& values, 
-                                                          map<string,Mat>& gradients, 
+map<string,Mat> OnlineLearningModule::namedBpropAccUpdate(map<string,Mat>& values,
+                                                          map<string,Mat>& gradients,
                                                           TVec<string> additional_input_gradients)
 {
     map<string,Mat> all_gradients;
@@ -439,7 +439,7 @@
     for (it=gradients.begin();it!=gradients.end();++it)
         all_gradients[it->first]=it->second;
     for (int i=0;i<additional_input_gradients.length();i++)
-        all_gradients[additional_input_gradients[i]]= 
+        all_gradients[additional_input_gradients[i]]=
             *ports_gradient[getPortIndex(additional_input_gradients[i])];
     return all_gradients;
 }

Modified: trunk/plearn_learners/online/OnlineLearningModule.h
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/OnlineLearningModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -133,8 +133,8 @@
     virtual void fprop(const TVec<Mat*>& ports_value);
 
     virtual map<string,Mat> namedFprop(map<string,Mat>& inputs, TVec<string> wanted_outputs);
-    virtual map<string,Mat> namedBpropAccUpdate(map<string,Mat>& values, 
-                                                map<string,Mat>& gradients, 
+    virtual map<string,Mat> namedBpropAccUpdate(map<string,Mat>& values,
+                                                map<string,Mat>& gradients,
                                                 TVec<string> additional_input_gradients);
 
     //! SOON TO BE DEPRECATED, USE bpropAccUpdate(const TVec<Mat*>& ports_value,

Modified: trunk/plearn_learners/online/RBMBinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMBinomialLayer.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -133,9 +133,9 @@
     virtual void freeEnergyContributionGradient(const Vec& unit_activations,
                                                 Vec& unit_activations_gradient,
                                                 real output_gradient = 1,
-                                                bool accumulate = false) 
+                                                bool accumulate = false)
         const;
-    
+
     virtual int getConfigurationCount();
 
     virtual void getConfiguration(int conf_index, Vec& output);

Modified: trunk/plearn_learners/online/RBMDiagonalMatrixConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMDiagonalMatrixConnection.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMDiagonalMatrixConnection.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -60,33 +60,33 @@
                   OptionBase::learntoption,
                   "Vector containing the diagonal of the weight matrix.\n");
 
-    declareOption(ol, "L1_penalty_factor", 
+    declareOption(ol, "L1_penalty_factor",
                   &RBMDiagonalMatrixConnection::L1_penalty_factor,
                   OptionBase::buildoption,
                   "Optional (default=0) factor of L1 regularization term, i.e.\n"
                   "minimize L1_penalty_factor * sum_{ij} |weights(i,j)| "
                   "during training.\n");
 
-    declareOption(ol, "L2_penalty_factor", 
+    declareOption(ol, "L2_penalty_factor",
                   &RBMDiagonalMatrixConnection::L2_penalty_factor,
                   OptionBase::buildoption,
                   "Optional (default=0) factor of L2 regularization term, i.e.\n"
                   "minimize 0.5 * L2_penalty_factor * sum_{ij} weights(i,j)^2 "
                   "during training.\n");
 
-    declareOption(ol, "L2_decrease_constant", 
+    declareOption(ol, "L2_decrease_constant",
                   &RBMDiagonalMatrixConnection::L2_decrease_constant,
                   OptionBase::buildoption,
         "Parameter of the L2 penalty decrease (see L2_decrease_type).",
         OptionBase::advanced_level);
 
-    declareOption(ol, "L2_shift", 
+    declareOption(ol, "L2_shift",
                   &RBMDiagonalMatrixConnection::L2_shift,
                   OptionBase::buildoption,
         "Parameter of the L2 penalty decrease (see L2_decrease_type).",
         OptionBase::advanced_level);
 
-    declareOption(ol, "L2_decrease_type", 
+    declareOption(ol, "L2_decrease_type",
                   &RBMDiagonalMatrixConnection::L2_decrease_type,
                   OptionBase::buildoption,
         "The kind of L2 decrease that is being applied. The decrease\n"
@@ -97,7 +97,7 @@
         " - 'sigmoid_like': sigmoid((L2_shift - t) * L2_decrease_constant)",
         OptionBase::advanced_level);
 
-    declareOption(ol, "L2_n_updates", 
+    declareOption(ol, "L2_n_updates",
                   &RBMDiagonalMatrixConnection::L2_n_updates,
                   OptionBase::learntoption,
         "Number of times that weights have been changed by the L2 penalty\n"
@@ -262,7 +262,7 @@
         }
     }
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         applyWeightPenalty();
 
     clearStats();
@@ -306,7 +306,7 @@
         }
     }
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         applyWeightPenalty();
 }
 
@@ -352,7 +352,7 @@
         PLERROR("RBMDiagonalMatrixConnection::update minibatch with momentum - Not implemented");
     }
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         applyWeightPenalty();
 }
 
@@ -455,7 +455,7 @@
         }
     }
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         applyWeightPenalty();
 }
 
@@ -509,7 +509,7 @@
             w[i] -= avg_lr * in[i] * outg[i];
     }
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         applyWeightPenalty();
 }
 
@@ -533,7 +533,7 @@
     {
         if( delta_L2 != 0. )
             w_[i] *= (1 - delta_L2);
-        
+
         if( delta_L1 != 0. )
         {
             if( w_[i] > delta_L1 )
@@ -544,7 +544,7 @@
                 w_[i] = 0.;
         }
     }
-    
+
     if (delta_L2 > 0)
         L2_n_updates++;
 }
@@ -565,7 +565,7 @@
     {
         if( delta_L2 != 0. )
             gw_[i] += delta_L2*w_[i];
-        
+
         if( delta_L1 != 0. )
         {
             if( w_[i] > 0 )

Modified: trunk/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMGaussianLayer.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -404,7 +404,7 @@
 
     if ( fixed_std_deviation > 0 && share_quad_coeff )
     {
-        if( share_quad_coeff ) 
+        if( share_quad_coeff )
             PLERROR("In RBMGaussianLayer::build_(): fixed_std_deviation should not "
                     "be > 0 when share_quad_coeff is true.");
         quad_coeff.fill( 1 / ( M_SQRT2 * fixed_std_deviation ) );
@@ -491,7 +491,7 @@
     {
         real pos_factor = -learning_rate / pos_count;
         real neg_factor = learning_rate / neg_count;
-        
+
         real* a = quad_coeff.data();
         real* aps = quad_coeff_pos_stats.data();
         real* ans = quad_coeff_neg_stats.data();
@@ -549,7 +549,7 @@
         // We will need to recompute sigma
         sigma_is_up_to_date = false;
     }
-    
+
     // will update the bias, and clear the statistics
     inherited::update();
 }
@@ -619,7 +619,7 @@
         // We will need to recompute sigma
         sigma_is_up_to_date = false;
     }
-    
+
     // update the bias
     inherited::update( pos_values, neg_values );
 }
@@ -740,7 +740,7 @@
             r = (target[i] - expectation[i]);
             ret += r * r;
         }
-    } 
+    }
     else
     {
         if(share_quad_coeff)
@@ -756,7 +756,7 @@
                 //      + log(sqrt(2*Pi) * sigma[i])
                 real r = (target[i] - expectation[i]) * quad_coeff[i];
                 ret += r * r + pl_log(sigma[i]);
-                
+
             }
         ret += 0.5*size*Log2Pi;
     }

Modified: trunk/plearn_learners/online/RBMLateralBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLateralBinomialLayer.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMLateralBinomialLayer.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -93,8 +93,8 @@
     // Set diagonal to 0
     if( lateral_weights.length() != 0 )
     {
-        real *d = lateral_weights.data();        
-        for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+        real *d = lateral_weights.data();
+        for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1)
             *d = 0;
     }
 
@@ -107,7 +107,7 @@
         mean_field_output_weights(i,i) = 1;
     for( int i=0; i<mean_field_output_bias.length(); i++ )
         mean_field_output_bias[i] = -0.5;
-    
+
 }
 
 ////////////////////
@@ -122,7 +122,7 @@
             "before calling generateSample()");
 
     for( int i=0 ; i<size ; i++ )
-        sample[i] = random_gen->binomial_sample( expectation[i] );    
+        sample[i] = random_gen->binomial_sample( expectation[i] );
 }
 
 /////////////////////
@@ -160,7 +160,7 @@
         else
             for( int i=0 ; i<size ; i++ )
                 mean_field_input[i] = sigmoid( activation[i] );
-        
+
         product(pre_sigmoid_mean_field_output, mean_field_output_weights, mean_field_input);
         pre_sigmoid_mean_field_output += mean_field_output_bias;
 
@@ -182,34 +182,34 @@
         for( int i=0 ; i<size ; i++ )
         {
             mean_field_i = expectation[i];
-            temp_mean_field_gradient[i] = (pre_sigmoid_mean_field_output[i] 
-                                           - temp_mean_field_gradient[i]) 
+            temp_mean_field_gradient[i] = (pre_sigmoid_mean_field_output[i]
+                                           - temp_mean_field_gradient[i])
                 * mean_field_i * (1 - mean_field_i);
         }
 
-        externalProductScaleAcc( mean_field_output_weights, temp_mean_field_gradient, 
+        externalProductScaleAcc( mean_field_output_weights, temp_mean_field_gradient,
                                  mean_field_input, -learning_rate );
         multiplyScaledAdd( temp_mean_field_gradient, 1.0, -learning_rate, mean_field_output_bias);
     }
     else
-    {        
+    {
         if( temp_output.length() != n_lateral_connections_passes+1 )
         {
             temp_output.resize(n_lateral_connections_passes+1);
             for( int i=0 ; i<n_lateral_connections_passes+1 ; i++ )
                 temp_output[i].resize(size);
-        }       
-        
+        }
+
         current_temp_output = temp_output[0];
         temp_output.last() = expectation;
-        
+
         if (use_fast_approximations)
             for( int i=0 ; i<size ; i++ )
                 current_temp_output[i] = fastsigmoid( activation[i] );
         else
             for( int i=0 ; i<size ; i++ )
                 current_temp_output[i] = sigmoid( activation[i] );
-        
+
         for( int t=0; t<n_lateral_connections_passes; t++ )
         {
             previous_temp_output = current_temp_output;
@@ -229,8 +229,8 @@
                 else
                 {
                     for( int i=0 ; i<size ; i++ )
-                        current_temp_output[i] = 
-                            (1-dampening_factor) * fastsigmoid( dampening_expectation[i] ) 
+                        current_temp_output[i] =
+                            (1-dampening_factor) * fastsigmoid( dampening_expectation[i] )
                             + dampening_factor * previous_temp_output[i];
                 }
             }
@@ -244,12 +244,12 @@
                 else
                 {
                     for( int i=0 ; i<size ; i++ )
-                        current_temp_output[i] = 
-                            (1-dampening_factor) * sigmoid( dampening_expectation[i] ) 
+                        current_temp_output[i] =
+                            (1-dampening_factor) * sigmoid( dampening_expectation[i] )
                             + dampening_factor * previous_temp_output[i];
                 }
             }
-            if( !fast_exact_is_equal(mean_field_precision_threshold, 0.) && 
+            if( !fast_exact_is_equal(mean_field_precision_threshold, 0.) &&
                 dist(current_temp_output, previous_temp_output,2)/size < mean_field_precision_threshold )
             {
                 expectation << current_temp_output;
@@ -283,17 +283,17 @@
     else
     {
         dampening_expectations.resize( batch_size, size );
-        
+
         if( temp_outputs.length() != n_lateral_connections_passes+1 )
         {
             temp_outputs.resize(n_lateral_connections_passes+1);
             for( int i=0 ; i<n_lateral_connections_passes+1 ; i++ )
                 temp_outputs[i].resize( batch_size, size);
-        }       
-        
+        }
+
         current_temp_outputs = temp_outputs[0];
         temp_outputs.last() = expectations;
-        
+
         if (use_fast_approximations)
             for (int k = 0; k < batch_size; k++)
                 for (int i = 0 ; i < size ; i++)
@@ -308,11 +308,11 @@
             previous_temp_outputs = current_temp_outputs;
             current_temp_outputs = temp_outputs[t+1];
             if( topographic_lateral_weights.length() == 0 )
-                productTranspose(dampening_expectations, previous_temp_outputs, 
+                productTranspose(dampening_expectations, previous_temp_outputs,
                                  lateral_weights);
             else
                 for( int b = 0; b<dampening_expectations.length(); b++)
-                    productTopoLateralWeights( dampening_expectations(b), 
+                    productTopoLateralWeights( dampening_expectations(b),
                                                previous_temp_outputs(b) );
 
             dampening_expectations += activations;
@@ -322,7 +322,7 @@
                 {
                     for(int k = 0; k < batch_size; k++)
                         for( int i=0 ; i<size ; i++ )
-                            current_temp_outputs(k, i) = 
+                            current_temp_outputs(k, i) =
                                 fastsigmoid( dampening_expectations(k, i) );
                 }
                 else
@@ -330,7 +330,7 @@
                     for(int k = 0; k < batch_size; k++)
                         for( int i=0 ; i<size ; i++ )
                             current_temp_outputs(k, i) = (1-dampening_factor)
-                                * fastsigmoid( dampening_expectations(k, i) ) 
+                                * fastsigmoid( dampening_expectations(k, i) )
                                 + dampening_factor * previous_temp_outputs(k, i);
                 }
             }
@@ -340,15 +340,15 @@
                 {
                     for(int k = 0; k < batch_size; k++)
                         for( int i=0 ; i<size ; i++ )
-                            current_temp_outputs(k, i) = 
+                            current_temp_outputs(k, i) =
                                 sigmoid( dampening_expectations(k, i) );
                 }
                 else
                 {
                     for(int k = 0; k < batch_size; k++)
                         for( int i=0 ; i<size ; i++ )
-                            current_temp_outputs(k, i) = (1-dampening_factor) 
-                                * sigmoid( dampening_expectations(k, i) ) 
+                            current_temp_outputs(k, i) = (1-dampening_factor)
+                                * sigmoid( dampening_expectations(k, i) )
                                 + dampening_factor * previous_temp_outputs(k, i);
                 }
             }
@@ -376,7 +376,7 @@
         else
             for( int i=0 ; i<size ; i++ )
                 mean_field_input[i] = sigmoid( bias_plus_input[i] );
-        
+
         product(pre_sigmoid_mean_field_output, mean_field_output_weights, mean_field_input);
         pre_sigmoid_mean_field_output += mean_field_output_bias;
 
@@ -388,14 +388,14 @@
                 output[i] = sigmoid( pre_sigmoid_mean_field_output[i] );
     }
     else
-    {        
+    {
 
         if( temp_output.length() != n_lateral_connections_passes+1 )
         {
             temp_output.resize(n_lateral_connections_passes+1);
             for( int i=0 ; i<n_lateral_connections_passes+1 ; i++ )
                 temp_output[i].resize(size);
-        }       
+        }
 
         temp_output.last() = output;
         current_temp_output = temp_output[0];
@@ -426,8 +426,8 @@
                 else
                 {
                     for( int i=0 ; i<size ; i++ )
-                        current_temp_output[i] = 
-                            (1-dampening_factor) * fastsigmoid( dampening_expectation[i] ) 
+                        current_temp_output[i] =
+                            (1-dampening_factor) * fastsigmoid( dampening_expectation[i] )
                             + dampening_factor * previous_temp_output[i];
                 }
             }
@@ -441,8 +441,8 @@
                 else
                 {
                     for( int i=0 ; i<size ; i++ )
-                        current_temp_output[i] = 
-                            (1-dampening_factor) * sigmoid( dampening_expectation[i] ) 
+                        current_temp_output[i] =
+                            (1-dampening_factor) * sigmoid( dampening_expectation[i] )
                             + dampening_factor * previous_temp_output[i];
                 }
             }
@@ -476,7 +476,7 @@
             temp_outputs.resize(n_lateral_connections_passes+1);
             for( int i=0 ; i<n_lateral_connections_passes+1 ; i++ )
                 temp_outputs[i].resize(mbatch_size,size);
-        }       
+        }
 
         temp_outputs.last() = outputs;
         current_temp_outputs = temp_outputs[0];
@@ -495,11 +495,11 @@
             previous_temp_outputs = current_temp_outputs;
             current_temp_outputs = temp_outputs[t+1];
             if( topographic_lateral_weights.length() == 0 )
-                productTranspose(dampening_expectations, previous_temp_outputs, 
+                productTranspose(dampening_expectations, previous_temp_outputs,
                                  lateral_weights);
             else
                 for( int b = 0; b<dampening_expectations.length(); b++)
-                    productTopoLateralWeights( dampening_expectations(b), 
+                    productTopoLateralWeights( dampening_expectations(b),
                                                previous_temp_outputs(b) );
 
             dampening_expectations += bias_plus_inputs;
@@ -509,7 +509,7 @@
                 {
                     for(int k = 0; k < batch_size; k++)
                         for( int i=0 ; i<size ; i++ )
-                            current_temp_outputs(k, i) = 
+                            current_temp_outputs(k, i) =
                                 fastsigmoid( dampening_expectations(k, i) );
                 }
                 else
@@ -517,7 +517,7 @@
                     for(int k = 0; k < batch_size; k++)
                         for( int i=0 ; i<size ; i++ )
                             current_temp_outputs(k, i) = (1-dampening_factor)
-                                * fastsigmoid( dampening_expectations(k, i) ) 
+                                * fastsigmoid( dampening_expectations(k, i) )
                                 + dampening_factor * previous_temp_outputs(k, i);
                 }
             }
@@ -527,7 +527,7 @@
                 {
                     for(int k = 0; k < batch_size; k++)
                         for( int i=0 ; i<size ; i++ )
-                            current_temp_outputs(k, i) = 
+                            current_temp_outputs(k, i) =
                                 sigmoid( dampening_expectations(k, i) );
                 }
                 else
@@ -535,7 +535,7 @@
                     for(int k = 0; k < batch_size; k++)
                         for( int i=0 ; i<size ; i++ )
                             current_temp_outputs(k, i) = (1-dampening_factor)
-                                * sigmoid( dampening_expectations(k, i) ) 
+                                * sigmoid( dampening_expectations(k, i) )
                                 + dampening_factor * previous_temp_outputs(k, i);
                 }
             }
@@ -565,7 +565,7 @@
             temp_output.resize(n_lateral_connections_passes+1);
             for( int i=0 ; i<n_lateral_connections_passes+1 ; i++ )
                 temp_output[i].resize(size);
-        }       
+        }
 
         temp_output.last() = output;
         current_temp_output = temp_output[0];
@@ -596,8 +596,8 @@
                 else
                 {
                     for( int i=0 ; i<size ; i++ )
-                        current_temp_output[i] = 
-                            (1-dampening_factor) * fastsigmoid( dampening_expectation[i] ) 
+                        current_temp_output[i] =
+                            (1-dampening_factor) * fastsigmoid( dampening_expectation[i] )
                             + dampening_factor * previous_temp_output[i];
                 }
             }
@@ -611,8 +611,8 @@
                 else
                 {
                     for( int i=0 ; i<size ; i++ )
-                        current_temp_output[i] = 
-                            (1-dampening_factor) * sigmoid( dampening_expectation[i] ) 
+                        current_temp_output[i] =
+                            (1-dampening_factor) * sigmoid( dampening_expectation[i] )
                             + dampening_factor * previous_temp_output[i];
                 }
             }
@@ -625,7 +625,7 @@
 void RBMLateralBinomialLayer::externalSymetricProductAcc(const Mat& mat, const Vec& v1, const Vec& v2)
 {
 #ifdef BOUNDCHECK
-    if (v1.length()!=mat.length() || mat.width()!=v2.length() 
+    if (v1.length()!=mat.length() || mat.width()!=v2.length()
         || v1.length() != v2.length())
         PLERROR("externalSymetricProductAcc(Mat,Vec,Vec), incompatible "
                 "arguments sizes");
@@ -667,7 +667,7 @@
     }
 }
 
-void RBMLateralBinomialLayer::productTopoLateralWeights(const Vec& result, 
+void RBMLateralBinomialLayer::productTopoLateralWeights(const Vec& result,
                                                         const Vec& input ) const
 {
     // Could be made faster, in terms of memory access
@@ -683,21 +683,21 @@
         neuron_h = i%topographic_width;
         wi = 0;
         current_weights = topographic_lateral_weights[i].data();
-        
-        vmin = neuron_v < topographic_patch_vradius ? 
+
+        vmin = neuron_v < topographic_patch_vradius ?
             - neuron_v : - topographic_patch_vradius;
-        vmax = topographic_length - neuron_v - 1 < topographic_patch_vradius ? 
+        vmax = topographic_length - neuron_v - 1 < topographic_patch_vradius ?
             topographic_length - neuron_v - 1: topographic_patch_vradius;
 
-        hmin = neuron_h < topographic_patch_hradius ? 
+        hmin = neuron_h < topographic_patch_hradius ?
             - neuron_h : - topographic_patch_hradius;
-        hmax = topographic_width - neuron_h - 1 < topographic_patch_hradius ? 
+        hmax = topographic_width - neuron_h - 1 < topographic_patch_hradius ?
             topographic_width - neuron_h - 1: topographic_patch_hradius;
 
-        for( int j = -1 * topographic_patch_vradius; 
-             j <= topographic_patch_vradius ; j++ ) 
+        for( int j = -1 * topographic_patch_vradius;
+             j <= topographic_patch_vradius ; j++ )
         {
-            for( int k = -1 * topographic_patch_hradius; 
+            for( int k = -1 * topographic_patch_hradius;
                  k <= topographic_patch_hradius; k++ )
             {
                 connected_neuron = (i+j*topographic_width)+k;
@@ -738,23 +738,23 @@
         current_weights = topographic_lateral_weights[i].data();
         current_weights_gradient = weights_gradient[i].data();
 
-        vmin = neuron_v < topographic_patch_vradius ? 
+        vmin = neuron_v < topographic_patch_vradius ?
             - neuron_v : - topographic_patch_vradius;
-        vmax = topographic_length - neuron_v - 1 < topographic_patch_vradius ? 
+        vmax = topographic_length - neuron_v - 1 < topographic_patch_vradius ?
             topographic_length - neuron_v - 1: topographic_patch_vradius;
 
-        hmin = neuron_h < topographic_patch_hradius ? 
+        hmin = neuron_h < topographic_patch_hradius ?
             - neuron_h : - topographic_patch_hradius;
-        hmax = topographic_width - neuron_h - 1 < topographic_patch_hradius ? 
+        hmax = topographic_width - neuron_h - 1 < topographic_patch_hradius ?
             topographic_width - neuron_h - 1: topographic_patch_hradius;
 
         result_gradient_i = result_gradient[i];
         input_i = input[i];
 
-        for( int j = -1 * topographic_patch_vradius; 
+        for( int j = -1 * topographic_patch_vradius;
              j <= topographic_patch_vradius ; j++ )
         {
-            for( int k = -1 * topographic_patch_hradius; 
+            for( int k = -1 * topographic_patch_hradius;
                  k <= topographic_patch_hradius; k++ )
             {
                 connected_neuron = (i+j*topographic_width)+k;
@@ -763,9 +763,9 @@
                     if( j >= vmin && j <= vmax &&
                         k >= hmin && k <= hmax )
                     {
-                        input_gradient[connected_neuron] += 
+                        input_gradient[connected_neuron] +=
                             result_gradient_i * current_weights[wi];
-                        current_weights_gradient[wi] += 
+                        current_weights_gradient[wi] +=
                             //0.5 * ( result_gradient_i * input[connected_neuron] +
                             ( result_gradient_i * input[connected_neuron] +
                               input_i * result_gradient[connected_neuron] );
@@ -783,7 +783,7 @@
 {
     if( !do_not_learn_topographic_lateral_weights )
     {
-        
+
         // Could be made faster, in terms of memory access
         int connected_neuron;
         int wi;
@@ -797,25 +797,25 @@
             neuron_v = i/topographic_width;
             neuron_h = i%topographic_width;
             wi = 0;
-            
-            vmin = neuron_v < topographic_patch_vradius ? 
+
+            vmin = neuron_v < topographic_patch_vradius ?
                 - neuron_v : - topographic_patch_vradius;
-            vmax = topographic_length - neuron_v - 1 < topographic_patch_vradius ? 
+            vmax = topographic_length - neuron_v - 1 < topographic_patch_vradius ?
                 topographic_length - neuron_v - 1: topographic_patch_vradius;
-            
-            hmin = neuron_h < topographic_patch_hradius ? 
+
+            hmin = neuron_h < topographic_patch_hradius ?
                 - neuron_h : - topographic_patch_hradius;
-            hmax = topographic_width - neuron_h - 1 < topographic_patch_hradius ? 
+            hmax = topographic_width - neuron_h - 1 < topographic_patch_hradius ?
                 topographic_width - neuron_h - 1: topographic_patch_hradius;
-            
+
             current_weights = topographic_lateral_weights[i].data();
             pos_values_i = pos_values[i];
             neg_values_i = neg_values[i];
-            
-            for( int j = - topographic_patch_vradius; 
+
+            for( int j = - topographic_patch_vradius;
                  j <= topographic_patch_vradius ; j++ )
             {
-                for( int k = -topographic_patch_hradius; 
+                for( int k = -topographic_patch_hradius;
                      k <= topographic_patch_hradius; k++ )
                 {
                     connected_neuron = (i+j*topographic_width)+k;
@@ -824,9 +824,9 @@
                         if( j >= vmin && j <= vmax &&
                             k >= hmin && k <= hmax )
                         {
-                            current_weights[wi] += 
-                                //learning_rate * 0.5 * ( 
-                                learning_rate * ( 
+                            current_weights[wi] +=
+                                //learning_rate * 0.5 * (
+                                learning_rate * (
                                     pos_values_i * pos_values[connected_neuron] -
                                     neg_values_i * neg_values[connected_neuron] );
                         }
@@ -873,7 +873,7 @@
 
         transposeProductAcc( input_gradient, mean_field_output_weights, temp_mean_field_gradient );
 
-        externalProductScaleAcc( mean_field_output_weights, temp_mean_field_gradient, 
+        externalProductScaleAcc( mean_field_output_weights, temp_mean_field_gradient,
                                  mean_field_input, -learning_rate );
         multiplyScaledAdd( temp_mean_field_gradient, 1.0, -learning_rate, mean_field_output_bias);
 
@@ -903,7 +903,7 @@
                 // Contribution from the mean field approximation
                 temp_mean_field_gradient2[i] =  (1-dampening_factor)*
                     output_i * (1-output_i) * temp_mean_field_gradient[i];
-            
+
                 // Contribution from the dampening
                 temp_mean_field_gradient[i] *= dampening_factor;
             }
@@ -914,16 +914,16 @@
             // Lateral weights gradient contribution
             if( topographic_lateral_weights.length() == 0)
             {
-                externalSymetricProductAcc( lateral_weights_gradient, 
+                externalSymetricProductAcc( lateral_weights_gradient,
                                             temp_mean_field_gradient2,
                                             temp_output[t] );
-            
-                transposeProductAcc(temp_mean_field_gradient, lateral_weights, 
+
+                transposeProductAcc(temp_mean_field_gradient, lateral_weights,
                                     temp_mean_field_gradient2);
             }
             else
             {
-                productTopoLateralWeightsGradients( 
+                productTopoLateralWeightsGradients(
                     temp_output[t],
                     temp_mean_field_gradient,
                     temp_mean_field_gradient2,
@@ -932,7 +932,7 @@
 
             current_temp_output = temp_output[t];
         }
-    
+
         for( int i=0 ; i<size ; i++ )
         {
             output_i = current_temp_output[i];
@@ -983,10 +983,10 @@
             {
                 if( momentum == 0. )
                     for( int i=0; i<topographic_lateral_weights.length(); i++ )
-                        multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0, 
+                        multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0,
                                            -learning_rate,
                                            topographic_lateral_weights[i]);
-            
+
                 else
                     PLERROR("In RBMLateralBinomialLayer:bpropUpdate - Not implemented for "
                             "topographic weights");
@@ -996,8 +996,8 @@
         // Set diagonal to 0
         if( lateral_weights.length() != 0 )
         {
-            real *d = lateral_weights.data();        
-            for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+            real *d = lateral_weights.data();
+            for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1)
                 *d = 0;
         }
     }
@@ -1057,32 +1057,32 @@
                 for( int i=0 ; i<size ; i++ )
                 {
                     output_i = current_temp_output[i];
-                
+
                     // Contribution from the mean field approximation
                     temp_mean_field_gradient2[i] =  (1-dampening_factor)*
                         output_i * (1-output_i) * temp_mean_field_gradient[i];
-                
+
                     // Contribution from the dampening
                     temp_mean_field_gradient[i] *= dampening_factor;
                 }
-            
+
                 // Input gradient contribution
                 temp_input_gradient += temp_mean_field_gradient2;
-            
+
                 // Lateral weights gradient contribution
                 if( topographic_lateral_weights.length() == 0)
                 {
-                
-                    externalSymetricProductAcc( lateral_weights_gradient, 
+
+                    externalSymetricProductAcc( lateral_weights_gradient,
                                                 temp_mean_field_gradient2,
                                                 temp_outputs[t](j) );
-                
-                    transposeProductAcc(temp_mean_field_gradient, lateral_weights, 
+
+                    transposeProductAcc(temp_mean_field_gradient, lateral_weights,
                                         temp_mean_field_gradient2);
                 }
                 else
                 {
-                    productTopoLateralWeightsGradients( 
+                    productTopoLateralWeightsGradients(
                         temp_outputs[t](j),
                         temp_mean_field_gradient,
                         temp_mean_field_gradient2,
@@ -1091,7 +1091,7 @@
 
                 current_temp_output = temp_outputs[t](j);
             }
-    
+
             for( int i=0 ; i<size ; i++ )
             {
                 output_i = current_temp_output[i];
@@ -1099,7 +1099,7 @@
             }
 
             temp_input_gradient += temp_mean_field_gradient;
-        
+
             input_gradients(j) += temp_input_gradient;
 
             // Update bias
@@ -1115,7 +1115,7 @@
                 else
                     PLERROR("In RBMLateralBinomialLayer:bpropUpdate - Not implemented for "
                             "momentum with mini-batches");
-            }        
+            }
         }
 
         if( topographic_lateral_weights.length() == 0)
@@ -1133,10 +1133,10 @@
             {
                 if( momentum == 0. )
                     for( int i=0; i<topographic_lateral_weights.length(); i++ )
-                        multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0, 
+                        multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0,
                                            -learning_rate,
                                            topographic_lateral_weights[i]);
-            
+
                 else
                     PLERROR("In RBMLateralBinomialLayer:bpropUpdate - Not implemented for "
                             "topographic weights");
@@ -1148,7 +1148,7 @@
         if( lateral_weights.length() != 0 )
         {
             real *d = lateral_weights.data();
-            for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+            for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1)
                 *d = 0;
         }
     }
@@ -1191,7 +1191,7 @@
                 // Contribution from the mean field approximation
                 temp_mean_field_gradient2[i] =  (1-dampening_factor)*
                     output_i * (1-output_i) * temp_mean_field_gradient[i];
-            
+
                 // Contribution from the dampening
                 temp_mean_field_gradient[i] *= dampening_factor;
             }
@@ -1203,16 +1203,16 @@
             if( topographic_lateral_weights.length() == 0)
             {
 
-                externalSymetricProductAcc( lateral_weights_gradient, 
+                externalSymetricProductAcc( lateral_weights_gradient,
                                             temp_mean_field_gradient2,
                                             temp_output[t] );
-            
-                transposeProductAcc(temp_mean_field_gradient, lateral_weights, 
+
+                transposeProductAcc(temp_mean_field_gradient, lateral_weights,
                                     temp_mean_field_gradient2);
             }
             else
             {
-                productTopoLateralWeightsGradients( 
+                productTopoLateralWeightsGradients(
                     temp_output[t],
                     temp_mean_field_gradient,
                     temp_mean_field_gradient2,
@@ -1221,7 +1221,7 @@
 
             current_temp_output = temp_output[t];
         }
-    
+
         for( int i=0 ; i<size ; i++ )
         {
             output_i = current_temp_output[i];
@@ -1253,21 +1253,21 @@
             {
                 if( momentum == 0. )
                     for( int i=0; i<topographic_lateral_weights.length(); i++ )
-                        multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0, 
+                        multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0,
                                            -learning_rate,
                                            topographic_lateral_weights[i]);
-            
+
                 else
                     PLERROR("In RBMLateralBinomialLayer:bpropUpdate - Not implemented for "
                             "topographic weights");
             }
         }
-        
+
         // Set diagonal to 0
         if( lateral_weights.length() != 0 )
         {
             real *d = lateral_weights.data();
-            for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+            for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1)
                 *d = 0;
         }
     }
@@ -1276,7 +1276,7 @@
 real RBMLateralBinomialLayer::fpropNLL(const Vec& target)
 {
     PLASSERT( target.size() == input_size );
-    computeExpectation(); 
+    computeExpectation();
 
     real ret = 0;
     real target_i, expectation_i;
@@ -1295,7 +1295,7 @@
 
 void RBMLateralBinomialLayer::fpropNLL(const Mat& targets, const Mat& costs_column)
 {
-    computeExpectations(); 
+    computeExpectations();
 
     PLASSERT( targets.width() == input_size );
     PLASSERT( targets.length() == batch_size );
@@ -1350,7 +1350,7 @@
                 // Contribution from the mean field approximation
                 temp_mean_field_gradient2[i] =  (1-dampening_factor)*
                     output_i * (1-output_i) * temp_mean_field_gradient[i];
-            
+
                 // Contribution from the dampening
                 temp_mean_field_gradient[i] *= dampening_factor;
             }
@@ -1361,16 +1361,16 @@
             // Lateral weights gradient contribution
             if( topographic_lateral_weights.length() == 0)
             {
-                externalSymetricProductAcc( lateral_weights_gradient, 
+                externalSymetricProductAcc( lateral_weights_gradient,
                                             temp_mean_field_gradient2,
                                             temp_output[t] );
-            
-                transposeProductAcc(temp_mean_field_gradient, lateral_weights, 
+
+                transposeProductAcc(temp_mean_field_gradient, lateral_weights,
                                     temp_mean_field_gradient2);
             }
             else
             {
-                productTopoLateralWeightsGradients( 
+                productTopoLateralWeightsGradients(
                     temp_output[t],
                     temp_mean_field_gradient,
                     temp_mean_field_gradient2,
@@ -1379,7 +1379,7 @@
 
             current_temp_output = temp_output[t];
         }
-    
+
         for( int i=0 ; i<size ; i++ )
         {
             output_i = current_temp_output[i];
@@ -1409,10 +1409,10 @@
             {
                 if( momentum == 0. )
                     for( int i=0; i<topographic_lateral_weights.length(); i++ )
-                        multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0, 
+                        multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0,
                                            -learning_rate,
                                            topographic_lateral_weights[i]);
-            
+
                 else
                     PLERROR("In RBMLateralBinomialLayer:bpropNLL - Not implemented for "
                             "topographic weights");
@@ -1422,7 +1422,7 @@
         if( lateral_weights.length() != 0 )
         {
             real *d = lateral_weights.data();
-            for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+            for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1)
                 *d = 0;
         }
     }
@@ -1465,32 +1465,32 @@
                 for( int i=0 ; i<size ; i++ )
                 {
                     output_i = current_temp_output[i];
-                
+
                     // Contribution from the mean field approximation
                     temp_mean_field_gradient2[i] =  (1-dampening_factor)*
                         output_i * (1-output_i) * temp_mean_field_gradient[i];
-                
+
                     // Contribution from the dampening
                     temp_mean_field_gradient[i] *= dampening_factor;
                 }
-            
+
                 // Input gradient contribution
                 bias_gradients(j) += temp_mean_field_gradient2;
-            
+
                 // Lateral weights gradient contribution
                 if( topographic_lateral_weights.length() == 0)
                 {
 
-                    externalSymetricProductAcc( lateral_weights_gradient, 
+                    externalSymetricProductAcc( lateral_weights_gradient,
                                                 temp_mean_field_gradient2,
                                                 temp_outputs[t](j) );
-                
-                    transposeProductAcc(temp_mean_field_gradient, lateral_weights, 
+
+                    transposeProductAcc(temp_mean_field_gradient, lateral_weights,
                                         temp_mean_field_gradient2);
                 }
                 else
                 {
-                    productTopoLateralWeightsGradients( 
+                    productTopoLateralWeightsGradients(
                         temp_outputs[t](j),
                         temp_mean_field_gradient,
                         temp_mean_field_gradient2,
@@ -1498,7 +1498,7 @@
                 }
                 current_temp_output = temp_outputs[t](j);
             }
-    
+
             for( int i=0 ; i<size ; i++ )
             {
                 output_i = current_temp_output[i];
@@ -1524,10 +1524,10 @@
             {
                 if( momentum == 0. )
                     for( int i=0; i<topographic_lateral_weights.length(); i++ )
-                        multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0, 
+                        multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0,
                                            -learning_rate,
                                            topographic_lateral_weights[i]);
-            
+
                 else
                     PLERROR("In RBMLateralBinomialLayer:bpropNLL - Not implemented for "
                             "topographic weights");
@@ -1538,7 +1538,7 @@
         if( lateral_weights.length() != 0 )
         {
             real *d = lateral_weights.data();
-            for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+            for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1)
                 *d = 0;
         }
     }
@@ -1570,7 +1570,7 @@
 
 
 void RBMLateralBinomialLayer::update()
-{ 
+{
     //real pos_factor = 0.5 * learning_rate / pos_count;
     //real neg_factor = - 0.5 * learning_rate / neg_count;
     real pos_factor = learning_rate / pos_count;
@@ -1585,7 +1585,7 @@
     {
         multiplyScaledAdd( lateral_weights_pos_stats, neg_factor, pos_factor,
                            lateral_weights_neg_stats);
-        lateral_weights += lateral_weights_neg_stats; 
+        lateral_weights += lateral_weights_neg_stats;
     }
     else
     {
@@ -1600,7 +1600,7 @@
     if( lateral_weights.length() != 0 )
     {
         real *d = lateral_weights.data();
-        for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+        for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1)
             *d = 0;
     }
 
@@ -1639,13 +1639,13 @@
                                     //- 0.5 * learning_rate);
                                     - learning_rate);
             lateral_weights += lateral_weights_inc;
-        }    
+        }
 
         // Set diagonal to 0
         if( lateral_weights.length() != 0 )
         {
             real *d = lateral_weights.data();
-            for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+            for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1)
                 *d = 0;
         }
     }
@@ -1694,7 +1694,7 @@
         if( lateral_weights.length() != 0 )
         {
             real *d = lateral_weights.data();
-            for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+            for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1)
                 *d = 0;
         }
     }
@@ -1704,7 +1704,7 @@
         {
             for(int b=0; b<pos_values.length(); b++)
                 updateTopoLateralWeightsCD(pos_values(b), neg_values(b));
-            
+
         }
         else
             PLERROR("In RBMLateralBinomialLayer:bpropNLL - Not implemented for "
@@ -1733,18 +1733,18 @@
 
 void RBMLateralBinomialLayer::declareOptions(OptionList& ol)
 {
-    declareOption(ol, "n_lateral_connections_passes", 
+    declareOption(ol, "n_lateral_connections_passes",
                   &RBMLateralBinomialLayer::n_lateral_connections_passes,
                   OptionBase::buildoption,
                   "Number of passes through the lateral connections.\n");
 
-    declareOption(ol, "dampening_factor", 
+    declareOption(ol, "dampening_factor",
                   &RBMLateralBinomialLayer::dampening_factor,
                   OptionBase::buildoption,
                   "Dampening factor ( expectation_t = (1-df) * currrent mean field"
                   " + df * expectation_{t-1}).\n");
 
-    declareOption(ol, "mean_field_precision_threshold", 
+    declareOption(ol, "mean_field_precision_threshold",
                   &RBMLateralBinomialLayer::mean_field_precision_threshold,
                   OptionBase::buildoption,
                   "Mean-field precision threshold that, once reached, stops the mean-field\n"
@@ -1752,59 +1752,59 @@
                   "Precision is computed as:\n"
                   "  dist(last_mean_field, current_mean_field) / size\n");
 
-    declareOption(ol, "topographic_length", 
+    declareOption(ol, "topographic_length",
                   &RBMLateralBinomialLayer::topographic_length,
                   OptionBase::buildoption,
                   "Length of the topographic map.\n");
 
-    declareOption(ol, "topographic_width", 
+    declareOption(ol, "topographic_width",
                   &RBMLateralBinomialLayer::topographic_width,
                   OptionBase::buildoption,
                   "Width of the topographic map.\n");
 
-    declareOption(ol, "topographic_patch_vradius", 
+    declareOption(ol, "topographic_patch_vradius",
                   &RBMLateralBinomialLayer::topographic_patch_vradius,
                   OptionBase::buildoption,
                   "Vertical radius of the topographic local weight patches.\n");
 
-    declareOption(ol, "topographic_patch_hradius", 
+    declareOption(ol, "topographic_patch_hradius",
                   &RBMLateralBinomialLayer::topographic_patch_hradius,
                   OptionBase::buildoption,
                   "Horizontal radius of the topographic local weight patches.\n");
 
-    declareOption(ol, "topographic_lateral_weights_init_value", 
+    declareOption(ol, "topographic_lateral_weights_init_value",
                   &RBMLateralBinomialLayer::topographic_lateral_weights_init_value,
                   OptionBase::buildoption,
                   "Initial value for the topographic_lateral_weights.\n");
 
-    declareOption(ol, "do_not_learn_topographic_lateral_weights", 
+    declareOption(ol, "do_not_learn_topographic_lateral_weights",
                   &RBMLateralBinomialLayer::do_not_learn_topographic_lateral_weights,
                   OptionBase::buildoption,
                   "Indication that the topographic_lateral_weights should\n"
                   "be fixed at their initial value.\n");
 
-    declareOption(ol, "lateral_weights", 
+    declareOption(ol, "lateral_weights",
                   &RBMLateralBinomialLayer::lateral_weights,
                   OptionBase::learntoption,
                   "Lateral connections.\n");
 
-    declareOption(ol, "topographic_lateral_weights", 
+    declareOption(ol, "topographic_lateral_weights",
                   &RBMLateralBinomialLayer::topographic_lateral_weights,
                   OptionBase::learntoption,
                   "Local topographic lateral connections.\n");
 
-    declareOption(ol, "use_parametric_mean_field", 
+    declareOption(ol, "use_parametric_mean_field",
                   &RBMLateralBinomialLayer::use_parametric_mean_field,
                   OptionBase::buildoption,
                   "Indication that a parametric predictor of the mean-field\n"
                   "approximation of the hidden layer conditional distribution.\n");
 
-    declareOption(ol, "mean_field_output_weights", 
+    declareOption(ol, "mean_field_output_weights",
                   &RBMLateralBinomialLayer::mean_field_output_weights,
                   OptionBase::learntoption,
                   "Output weights of the mean field predictor.\n");
 
-    declareOption(ol, "mean_field_output_bias", 
+    declareOption(ol, "mean_field_output_bias",
                   &RBMLateralBinomialLayer::mean_field_output_bias,
                   OptionBase::learntoption,
                   "Output bias of the mean field predictor.\n");
@@ -1827,11 +1827,11 @@
     if( n_lateral_connections_passes < 0 )
         PLERROR("In RBMLateralBinomialLayer::build_(): n_lateral_connections_passes\n"
                 " should be >= 0.");
- 
+
     if( use_parametric_mean_field && topographic_length > 0 && topographic_width > 0 )
         PLERROR("RBMLateralBinomialLayer::build_(): can't use parametric mean field "
             "and topographic lateral connections.");
-    
+
     if( use_parametric_mean_field )
     {
         mean_field_output_weights.resize(size,size);
@@ -1851,7 +1851,7 @@
         {
             bias_inc.resize( size );
             lateral_weights_inc.resize(size,size);
-        }   
+        }
     }
     else
     {
@@ -1871,10 +1871,10 @@
         topographic_lateral_weights_gradient.resize(size);
         for( int i=0; i<size; i++ )
         {
-            topographic_lateral_weights[i].resize( 
+            topographic_lateral_weights[i].resize(
                 ( 2 * topographic_patch_hradius + 1 ) *
                 ( 2 * topographic_patch_vradius + 1 ) - 1 );
-            topographic_lateral_weights_gradient[i].resize( 
+            topographic_lateral_weights_gradient[i].resize(
                 ( 2 * topographic_patch_hradius + 1 ) *
                 ( 2 * topographic_patch_vradius + 1 ) - 1 );
         }

Modified: trunk/plearn_learners/online/RBMLateralBinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLateralBinomialLayer.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMLateralBinomialLayer.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -56,15 +56,15 @@
 public:
     //#####  Public Build Options  ############################################
 
-    //! Number of passes through the lateral connections 
+    //! Number of passes through the lateral connections
     int n_lateral_connections_passes;
 
-    //! Dampening factor 
+    //! Dampening factor
     //! ( expectation_t = (1-df) * currrent mean field + df * expectation_{t-1})
     real dampening_factor;
 
-    //! Mean-field precision threshold that, once reached, stops the mean-field 
-    //! expectation approximation computation. Used only in computeExpectation(). 
+    //! Mean-field precision threshold that, once reached, stops the mean-field
+    //! expectation approximation computation. Used only in computeExpectation().
     //! Precision is computed as:
     //!   dist(last_mean_field, current_mean_field) / size
     real mean_field_precision_threshold;
@@ -87,7 +87,7 @@
     //! Indication that the topographic_lateral_weights should
     //! be fixed at their initial value.
     bool do_not_learn_topographic_lateral_weights;
-    
+
     //! Lateral connections
     Mat lateral_weights;
 
@@ -100,7 +100,7 @@
     //! Accumulates negative contribution to the gradient of lateral weights
     Mat lateral_weights_neg_stats;
 
-    //! Indication that a parametric predictor of the mean-field 
+    //! Indication that a parametric predictor of the mean-field
     //! approximation of the hidden layer conditional distribution.
     bool use_parametric_mean_field;
 
@@ -186,18 +186,18 @@
     virtual void accumulateNegStats( const Vec& neg_values );
     virtual void accumulateNegStats( const Mat& neg_values );
 
-    //! Update bias and lateral connections parameters 
+    //! Update bias and lateral connections parameters
     //! according to accumulated statistics
     virtual void update();
 
     //! Updates ONLY the bias parameters according to the given gradient
     virtual void update( const Vec& grad );
 
-    //! Update bias and lateral connections 
+    //! Update bias and lateral connections
     //! parameters according to one pair of vectors
     virtual void update( const Vec& pos_values, const Vec& neg_values );
 
-    //! Update bias and lateral connections 
+    //! Update bias and lateral connections
     //! parameters according to one pair of matrices.
     virtual void update( const Mat& pos_values, const Mat& neg_values );
 
@@ -275,13 +275,13 @@
     static void declareOptions(OptionList& ol);
 
     //! Computes mat[i][j] += 0.5 * (v1[i] * v2[j] +  v1[j] * v2[i])
-    void externalSymetricProductAcc(const Mat& mat, const Vec& v1, 
+    void externalSymetricProductAcc(const Mat& mat, const Vec& v1,
                                     const Vec& v2);
 
     void productTopoLateralWeights( const Vec& result, const Vec& input ) const;
 
     void productTopoLateralWeightsGradients( const Vec& input, const Vec& input_gradient,
-                                             const Vec& result_gradient, 
+                                             const Vec& result_gradient,
                                              const TVec< Vec >& weights_gradient );
 
     void updateTopoLateralWeightsCD( const Vec& pos_values, const Vec& neg_values );

Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMLayer.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -393,7 +393,7 @@
             this->classname().c_str());
 }
 
-void RBMLayer::bpropNLL(const Mat& targets,  const Mat& costs_column, 
+void RBMLayer::bpropNLL(const Mat& targets,  const Mat& costs_column,
                         Mat& bias_gradients)
 {
     PLERROR("In RBMLayer::bpropNLL(): not implemented in subclass %s",
@@ -469,7 +469,7 @@
     }
 
     applyBiasDecay();
-    
+
     clearStats();
 }
 
@@ -656,7 +656,7 @@
     columnSum(gibbs_neg_values,tmp);
     if (neg_count==0)
         multiply(tmp, normalize_factor, bias_neg_stats);
-    else // bias_neg_stats <-- tmp*(1-gibbs_chain_statistics_forgetting_factor)/minibatch_size 
+    else // bias_neg_stats <-- tmp*(1-gibbs_chain_statistics_forgetting_factor)/minibatch_size
         //                    +gibbs_chain_statistics_forgetting_factor*bias_neg_stats
         multiplyScaledAdd(tmp,gibbs_ma_coefficient,
                           normalize_factor*(1-gibbs_ma_coefficient),
@@ -674,7 +674,7 @@
         gibbs_ma_coefficient = sigmoid(gibbs_ma_increment + inverse_sigmoid(gibbs_ma_coefficient));
 
 
-    // delta w = lrate * ( meanoverrows(pos_values) - neg_stats ) 
+    // delta w = lrate * ( meanoverrows(pos_values) - neg_stats )
     columnSum(pos_values,tmp);
     multiplyAcc(bias, tmp, learning_rate*normalize_factor);
     multiplyAcc(bias, bias_neg_stats, -learning_rate);
@@ -761,7 +761,7 @@
 
     for( int i=0 ; i<size ; i++ )
         bg[i] = -bps[i] + bns[i];
-    
+
     addBiasDecay(bias_gradient);
 
 }
@@ -832,7 +832,7 @@
         real *bg = bias_gradients[b];
         real *b = bias.data();
         bias_decay_type = lowerstring(bias_decay_type);
-        
+
         if (bias_decay_type=="negative")  // Pushes the biases towards -\infty
             for( int i=0 ; i<size ; i++ )
                 bg[i] += avg_lr * bias_decay_parameter;
@@ -847,7 +847,7 @@
 
 void RBMLayer::applyBiasDecay()
 {
-    
+
     PLASSERT(bias.size()==size);
 
     real* b = bias.data();
@@ -860,11 +860,11 @@
             b[i] -= learning_rate * bias_decay_parameter;
     else if (bias_decay_type=="l2") // L2 penalty on the biases
         bias *= (1 - learning_rate * bias_decay_parameter);
-    else 
+    else
         PLERROR("RBMLayer::applyBiasDecay(string) bias_decay_type %s is not in"
                 " the list, in subclass %s\n",bias_decay_type.c_str(),classname().c_str());
 
-}   
+}
 
 } // end of namespace PLearn
 

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMLayer.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -297,7 +297,7 @@
     virtual void freeEnergyContributionGradient(const Vec& unit_activations,
                                                 Vec& unit_activations_gradient,
                                                 real output_gradient = 1,
-                                                bool accumulate = false ) 
+                                                bool accumulate = false )
         const;
 
     //! Returns a number of different configurations the layer can be in.

Modified: trunk/plearn_learners/online/RBMMatrixConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMMatrixConnection.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -76,7 +76,7 @@
                   "its inverse sigmoid by gibbs_ma_increment). After the last\n"
                   "increase has been made, the moving average coefficient stays constant.\n");
 
-    declareOption(ol, "gibbs_ma_increment", 
+    declareOption(ol, "gibbs_ma_increment",
                   &RBMMatrixConnection::gibbs_ma_increment,
                   OptionBase::buildoption,
                   "The increment in the inverse sigmoid of the moving "
@@ -84,39 +84,39 @@
                   "to apply after the number of updates reaches an element "
                   "of the gibbs_ma_schedule.\n");
 
-    declareOption(ol, "gibbs_initial_ma_coefficient", 
+    declareOption(ol, "gibbs_initial_ma_coefficient",
                   &RBMMatrixConnection::gibbs_initial_ma_coefficient,
                   OptionBase::buildoption,
                   "Initial moving average coefficient for the negative phase "
                   "statistics in the Gibbs chain.\n");
 
-    declareOption(ol, "L1_penalty_factor", 
+    declareOption(ol, "L1_penalty_factor",
                   &RBMMatrixConnection::L1_penalty_factor,
                   OptionBase::buildoption,
                   "Optional (default=0) factor of L1 regularization term, i.e.\n"
                   "minimize L1_penalty_factor * sum_{ij} |weights(i,j)| "
                   "during training.\n");
 
-    declareOption(ol, "L2_penalty_factor", 
+    declareOption(ol, "L2_penalty_factor",
                   &RBMMatrixConnection::L2_penalty_factor,
                   OptionBase::buildoption,
                   "Optional (default=0) factor of L2 regularization term, i.e.\n"
                   "minimize 0.5 * L2_penalty_factor * sum_{ij} weights(i,j)^2 "
                   "during training.\n");
 
-    declareOption(ol, "L2_decrease_constant", 
+    declareOption(ol, "L2_decrease_constant",
                   &RBMMatrixConnection::L2_decrease_constant,
                   OptionBase::buildoption,
         "Parameter of the L2 penalty decrease (see L2_decrease_type).",
         OptionBase::advanced_level);
 
-    declareOption(ol, "L2_shift", 
+    declareOption(ol, "L2_shift",
                   &RBMMatrixConnection::L2_shift,
                   OptionBase::buildoption,
         "Parameter of the L2 penalty decrease (see L2_decrease_type).",
         OptionBase::advanced_level);
 
-    declareOption(ol, "L2_decrease_type", 
+    declareOption(ol, "L2_decrease_type",
                   &RBMMatrixConnection::L2_decrease_type,
                   OptionBase::buildoption,
         "The kind of L2 decrease that is being applied. The decrease\n"
@@ -127,7 +127,7 @@
         " - 'sigmoid_like': sigmoid((L2_shift - t) * L2_decrease_constant)",
         OptionBase::advanced_level);
 
-    declareOption(ol, "L2_n_updates", 
+    declareOption(ol, "L2_n_updates",
                   &RBMMatrixConnection::L2_n_updates,
                   OptionBase::learntoption,
         "Number of times that weights have been changed by the L2 penalty\n"
@@ -273,7 +273,7 @@
             }
     }
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         applyWeightPenalty();
 
     clearStats();
@@ -332,7 +332,7 @@
             }
     }
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         applyWeightPenalty();
 }
 
@@ -386,7 +386,7 @@
          */
     }
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         applyWeightPenalty();
 }
 
@@ -427,7 +427,7 @@
         -learning_rate*(1-background_gibbs_update_ratio)*normalize_factor,
         real(1));
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         applyWeightPenalty();
 }
 
@@ -472,7 +472,7 @@
                              learning_rate*normalize_factor, real(1));
     multiplyAcc(weights, weights_neg_stats, -learning_rate);
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         applyWeightPenalty();
 }
 
@@ -621,7 +621,7 @@
     // weights -= learning_rate * output_gradient * input'
     externalProductScaleAcc( weights, output_gradient, input, -learning_rate );
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         applyWeightPenalty();
 }
 
@@ -654,7 +654,7 @@
     transposeProductScaleAcc(weights, output_gradients, inputs,
                              -learning_rate / inputs.length(), real(1));
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         applyWeightPenalty();
 }
 
@@ -695,7 +695,7 @@
                          input);
     }
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         addWeightPenalty(rbm_weights, rbm_weights_gradient);
 }
 
@@ -766,7 +766,7 @@
         PLCHECK_MSG( false,
                      "Unknown port configuration" );
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         applyWeightPenalty();
 }
 
@@ -800,7 +800,7 @@
                 w_i[j] = wns_i[j]/pos_count - wps_i[j]/neg_count;
     }
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         addWeightPenalty(weights, weights_gradient);
 }
 
@@ -841,7 +841,7 @@
                 w_i[j] =  *nuv_i * ndv[j] - *puv_i * pdv[j] ;
     }
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         addWeightPenalty(weights, weights_gradient);
 }
 
@@ -867,7 +867,7 @@
         {
             if( delta_L2 != 0. )
                 w_[j] *= (1 - delta_L2);
-        
+
             if( delta_L1 != 0. )
             {
                 if( w_[j] > delta_L1 )
@@ -901,7 +901,7 @@
         {
             if( delta_L2 != 0. )
                 gw_[j] += delta_L2*w_[j];
-        
+
             if( delta_L1 != 0. )
             {
                 if( w_[j] > 0 )

Modified: trunk/plearn_learners/online/RBMMatrixConnectionNatGrad.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnectionNatGrad.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMMatrixConnectionNatGrad.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -135,7 +135,7 @@
 }
 
 
-void RBMMatrixConnectionNatGrad::bpropUpdate(const Mat& inputs, 
+void RBMMatrixConnectionNatGrad::bpropUpdate(const Mat& inputs,
                                              const Mat& outputs,
                                              Mat& input_gradients,
                                              const Mat& output_gradients,
@@ -171,9 +171,9 @@
     }
     pos_count++;
 }
- 
 
 
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -48,7 +48,7 @@
     "RBMMatrixConnection's weights",
     "");
 
-RBMMatrixTransposeConnection::RBMMatrixTransposeConnection( 
+RBMMatrixTransposeConnection::RBMMatrixTransposeConnection(
     PP<RBMMatrixConnection> the_rbm_matrix_connection,
     real the_learning_rate,
     bool call_build_) :
@@ -63,18 +63,18 @@
 
 void RBMMatrixTransposeConnection::declareOptions(OptionList& ol)
 {
-    declareOption(ol, "rbm_matrix_connection", 
+    declareOption(ol, "rbm_matrix_connection",
                   &RBMMatrixTransposeConnection::rbm_matrix_connection,
                   OptionBase::buildoption,
                   "RBMMatrixConnection from which the weights are taken");
 
-    declareOption(ol, "learn_scale", 
+    declareOption(ol, "learn_scale",
                   &RBMMatrixTransposeConnection::learn_scale,
                   OptionBase::buildoption,
                   "Indication that the scale of the weight matrix should be "
                   "learned.\n");
 
-    declareOption(ol, "scale", 
+    declareOption(ol, "scale",
                   &RBMMatrixTransposeConnection::scale,
                   OptionBase::learntoption,
                   "Learned scale for weight matrix.\n");
@@ -353,7 +353,7 @@
 }
 
 //! this version allows to obtain the input gradient as well
-void RBMMatrixTransposeConnection::bpropUpdate(const Vec& input, 
+void RBMMatrixTransposeConnection::bpropUpdate(const Vec& input,
                                                const Vec& output,
                                                Vec& input_gradient,
                                                const Vec& output_gradient,
@@ -375,11 +375,11 @@
     else
     {
         input_gradient.resize( down_size );
-        
+
         // input_gradient = weights' * output_gradient
         product( input_gradient, weights, output_gradient );
     }
-    
+
     // weights -= learning_rate * output_gradient * input'
     externalProductScaleAcc( weights, input, output_gradient, -learning_rate );
     if( learn_scale )

Modified: trunk/plearn_learners/online/RBMMatrixTransposeConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMMatrixTransposeConnection.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMMatrixTransposeConnection.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -86,7 +86,7 @@
     //#####  Public Member Functions  #########################################
 
     //! Default constructor.
-    RBMMatrixTransposeConnection( 
+    RBMMatrixTransposeConnection(
         PP<RBMMatrixConnection> the_rbm_matrix_connection = 0,
         real the_learning_rate = 0,
         bool call_build_ = false);

Modified: trunk/plearn_learners/online/RBMMixedLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMMixedLayer.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -661,7 +661,7 @@
                                                               layer_size));
 
         bias.subVec(init_pos, layer_size) << layer->bias;
-        layer->bias = bias.subVec(init_pos, layer_size);        
+        layer->bias = bias.subVec(init_pos, layer_size);
 
         // We changed fields of layer, so we need to rebuild it (especially
         // if it is another RBMMixedLayer)
@@ -745,7 +745,7 @@
         Vec output_i = output.subVec( begin, size_i );
         sub_layers[i]->getConfiguration(conf_i % conf_layer_i, output_i);
         conf_i /= conf_layer_i;
-    }    
+    }
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -135,7 +135,7 @@
 void RBMModule::declareOptions(OptionList& ol)
 {
     // Build options.
-    
+
     declareOption(ol, "visible_layer", &RBMModule::visible_layer,
                   OptionBase::buildoption,
         "Visible layer of the RBM.");
@@ -183,12 +183,12 @@
                   &RBMModule::deterministic_reconstruction_in_cd,
                   OptionBase::buildoption,
         "Whether to use the expectation of the visible (given a hidden sample)\n"
-	"or a sample of the visible in the contrastive divergence learning.\n"
+        "or a sample of the visible in the contrastive divergence learning.\n"
         "In other words, instead of the classical Gibbs sampling\n"
         "   v_0 --> h_0 ~ p(h|v_0) --> v_1 ~ p(v|h_0) -->  p(h|v_1)\n"
         "we will have by setting 'deterministic_reconstruction_in_cd=1'\n"
         "   v_0 --> h_0 ~ p(h|v_0) --> v_1 = E(v|h_0) -->  p(h|E(v|h_0)).");
- 
+
     declareOption(ol, "standard_cd_grad",
                   &RBMModule::standard_cd_grad,
                   OptionBase::buildoption,
@@ -253,7 +253,7 @@
                   "of w.r.t. the contrastive divergence.\n");
 
     // Learnt options.
-    
+
     declareOption(ol, "Gibbs_step",
                   &RBMModule::Gibbs_step,
                   OptionBase::learntoption,
@@ -352,13 +352,13 @@
     addPortName("hidden_bias");
     addPortName("weights");
     addPortName("neg_log_likelihood");
-    // a column matrix with one element -log P(h) for each row h of "hidden", 
+    // a column matrix with one element -log P(h) for each row h of "hidden",
     // used as an input port, with neg_log_pvisible_given_phidden as output
-    addPortName("neg_log_phidden"); 
+    addPortName("neg_log_phidden");
     // compute column matrix with one entry -log P(x) = -log( sum_h P(x|h) P(h) ) for
-    // each row x of "visible", and where {P(h)}_h is provided 
+    // each row x of "visible", and where {P(h)}_h is provided
     // in "neg_log_phidden" for the set of h's in "hidden".
-    addPortName("neg_log_pvisible_given_phidden"); 
+    addPortName("neg_log_pvisible_given_phidden");
     if(reconstruction_connection)
     {
         addPortName("visible_reconstruction.state");
@@ -779,7 +779,7 @@
     if (compute_contrastive_divergence)
     {
         contrastive_divergence = ports_value[getPortIndex("contrastive_divergence")];
-/* YB: I don't agree with this error message: the behavior should be adapted to the provided ports. 
+/* YB: I don't agree with this error message: the behavior should be adapted to the provided ports.
       if (!contrastive_divergence || !contrastive_divergence->isEmpty())
             PLERROR("In RBMModule::fprop - When option "
                     "'compute_contrastive_divergence' is 'true', the "
@@ -1117,7 +1117,7 @@
                 n_samples = hidden_sample->length();
             }
             PLCHECK( n_samples > 0 );
-            
+
             // the visible_layer->expectations contain the "state" from which we
             // start or continue the chain
             if (visible_layer->samples.length() != n_samples)
@@ -1417,7 +1417,7 @@
     Mat* contrastive_divergence = NULL;
     if (compute_contrastive_divergence)
         contrastive_divergence = ports_value[getPortIndex("contrastive_divergence")];
-    bool computed_contrastive_divergence = compute_contrastive_divergence && 
+    bool computed_contrastive_divergence = compute_contrastive_divergence &&
         contrastive_divergence && !contrastive_divergence->isEmpty();
 
     // Ensure the gradient w.r.t. contrastive divergence is 1 (if provided).
@@ -1550,9 +1550,9 @@
         PLASSERT(hidden && !hidden->isEmpty());
         setAllLearningRates(grad_learning_rate);
         visible_layer->bpropUpdate(*visible_activations,
-                                   *visible, visible_act_grad, *visible_grad, 
+                                   *visible, visible_act_grad, *visible_grad,
                                    false);
-        
+
 //        PLASSERT_MSG(!visible_bias_grad,"back-prop into visible bias  not implemented for downward fprop");
 //        PLASSERT_MSG(!weights_grad,"back-prop into weights  not implemented for downward fprop");
 //        hidden_grad->resize(mbs,hidden_layer->size);
@@ -1695,7 +1695,7 @@
                 vis_expect_ptr = visible_layer->getExpectations();
                 negative_phase_visible_samples = &vis_expect_ptr;
             }
-	    else // classical CD learning
+            else // classical CD learning
                negative_phase_visible_samples = &(visible_layer->samples);
             negative_phase_hidden_activations = &(hidden_layer->activations);
             negative_phase_hidden_expectations = &(hidden_layer->getExpectations());
@@ -1906,7 +1906,7 @@
 
         // UGLY HACK WHICH BREAKS THE RULE THAT RBMMODULE CAN BE CALLED IN DIFFERENT CONTEXTS AND fprop/bprop ORDERS
         // BUT NECESSARY WHEN hidden WAS AN INPUT
-        if (hidden_is_output) 
+        if (hidden_is_output)
         {
             // Hidden layer bias update
             hidden_layer->bpropUpdate(*hidden_act,
@@ -1960,7 +1960,7 @@
                  visible_layer->classname()=="RBMGaussianlLayer");
         PLASSERT(connection->classname()=="RBMMatrixConnection");
         PLASSERT(hidden && !hidden->isEmpty());
-        // FE(x) = -b'x - sum_i softplus(hidden_layer->activation[i])        
+        // FE(x) = -b'x - sum_i softplus(hidden_layer->activation[i])
         // dFE(x)/dx = -b - sum_i sigmoid(hidden_layer->activation[i]) W_i
         // dC/dxt = -b dC/dFE - dC/dFE sum_i p_ti W_i
         int mbs=energy_grad->length();

Modified: trunk/plearn_learners/online/RBMModule.h
===================================================================
--- trunk/plearn_learners/online/RBMModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -284,7 +284,7 @@
 public:
     //! Forward the given learning rate to all elements of this module.
     void setAllLearningRates(real lr);
-        
+
     //! Compute activations on the hidden layer based on the provided
     //! visible input.
     //! If 'hidden_bias' is not null nor empty, then it is used as an

Modified: trunk/plearn_learners/online/RBMMultitaskClassificationModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMMultitaskClassificationModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMMultitaskClassificationModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -165,7 +165,7 @@
         PLERROR("In RBMMultitaskClassificationModule::build_(): "
                 "n_mean_field_iterations should be > 0\n");
 
-    last_to_target_gradient.resize( last_to_target->up_size, 
+    last_to_target_gradient.resize( last_to_target->up_size,
                                     last_to_target->down_size );
 
     // If we have a random_gen, share it with the ones who do not
@@ -233,27 +233,27 @@
     output.resize( output_size );
 
     previous_to_last->fprop( input, mean_field_activations_hidden[0] );
-    last_layer->fprop( mean_field_activations_hidden[0], 
+    last_layer->fprop( mean_field_activations_hidden[0],
                        mean_field_approximations_hidden[0] );
 
     Mat weights = last_to_target->weights;
     for( int t=0; t<n_mean_field_iterations; t++ )
     {
-        transposeProduct( mean_field_activations_target[t], weights, 
+        transposeProduct( mean_field_activations_target[t], weights,
                           mean_field_approximations_hidden[t] );
         target_layer->fprop( mean_field_activations_target[t],
                              mean_field_approximations_target[t] );
-        
+
         if( t != n_mean_field_iterations -1 )
         {
-            product( mean_field_activations_hidden[t+1], weights, 
+            product( mean_field_activations_hidden[t+1], weights,
                      mean_field_approximations_target[t] );
             mean_field_activations_hidden[t+1] += mean_field_activations_hidden[0];
             last_layer->fprop( mean_field_activations_hidden[t+1],
                                mean_field_approximations_hidden[t+1] );
         }
     }
-    
+
     if( fprop_outputs_activation )
     {
         output << mean_field_activations_target.last();
@@ -317,9 +317,9 @@
                             mean_field_approximations_hidden[t],
                             mean_field_activations_gradient_target);
 
-        product( mean_field_approximations_gradient_hidden, weights, 
+        product( mean_field_approximations_gradient_hidden, weights,
                           mean_field_activations_gradient_target);
-        
+
         if( t != 0 )
         {
             last_layer->bpropUpdate( mean_field_activations_hidden[t],
@@ -333,7 +333,7 @@
                                 mean_field_approximations_target[t-1]
                                 );
 
-            transposeProduct( mean_field_approximations_gradient_target, weights, 
+            transposeProduct( mean_field_approximations_gradient_target, weights,
                               mean_field_activations_gradient_hidden);
         }
     }
@@ -345,11 +345,11 @@
         );
 
     previous_to_last->bpropUpdate( input, mean_field_activations_hidden[0],
-                                   input_gradient, 
+                                   input_gradient,
                                    mean_field_activations_gradient_hidden,
                                    accumulate);
 
-    multiplyAcc( weights, last_to_target_gradient, 
+    multiplyAcc( weights, last_to_target_gradient,
                  - (last_to_target->learning_rate) );
 }
 

Modified: trunk/plearn_learners/online/RBMMultitaskClassificationModule.h
===================================================================
--- trunk/plearn_learners/online/RBMMultitaskClassificationModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMMultitaskClassificationModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -79,7 +79,7 @@
     int n_mean_field_iterations;
 
     //! Indication that fprop should output the value of the activation
-    //! before the squashing function and the application of the bias, 
+    //! before the squashing function and the application of the bias,
     //! instead of the mean-field approximation.
     bool fprop_outputs_activation;
 
@@ -194,7 +194,7 @@
     mutable Vec mean_field_approximations_gradient_target;
     mutable Vec mean_field_activations_gradient_hidden;
     mutable Vec mean_field_approximations_gradient_hidden;
-    
+
 };
 
 // Declares a few other classes and functions related to this class

Modified: trunk/plearn_learners/online/RBMWoodsLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMWoodsLayer.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMWoodsLayer.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -74,7 +74,7 @@
     else
         sample.clear();
 
-    int n_nodes_per_tree = size / n_trees;    
+    int n_nodes_per_tree = size / n_trees;
     int node, depth, node_sample, sub_tree_size;
     int offset = 0;
 
@@ -85,13 +85,13 @@
         sub_tree_size = node;
         while( depth < tree_depth )
         {
-            node_sample = random_gen->binomial_sample( 
+            node_sample = random_gen->binomial_sample(
                 local_node_expectation[ node + offset ] );
             if( use_signed_samples )
                 sample[node + offset] = 2*node_sample-1;
             else
                 sample[node + offset] = node_sample;
-            
+
             // Descending in the tree
             sub_tree_size /= 2;
             if ( node_sample > 0.5 )
@@ -144,14 +144,14 @@
     if( expectation_is_up_to_date )
         return;
 
-    int n_nodes_per_tree = size / n_trees;    
+    int n_nodes_per_tree = size / n_trees;
     int node, depth, sub_tree_size, grand_parent;
     int offset = 0;
     bool left_of_grand_parent;
     real grand_parent_prob;
 
     // Get local expectations at every node
-    
+
     // Divide and conquer computation of local (conditional) free energies
     for( int t=0; t<n_trees; t++ )
     {
@@ -178,12 +178,12 @@
         {
             for( int n=sub_tree_size; n<n_nodes_per_tree; n += 2*sub_tree_size + 2 )
             {
-                //on_free_energy[ n + offset ] = safeexp(activation[n+offset]) * 
+                //on_free_energy[ n + offset ] = safeexp(activation[n+offset]) *
                 //    ( on_free_energy[n + offset - sub_tree_size] + off_free_energy[n + offset - sub_tree_size] ) ;
-                //off_free_energy[ n + offset ] = 
+                //off_free_energy[ n + offset ] =
                 //    ( on_free_energy[n + offset + sub_tree_size] + off_free_energy[n + offset + sub_tree_size] ) ;
                 // Now working in log-domain
-                on_free_energy[ n + offset ] = activation[n+offset] + 
+                on_free_energy[ n + offset ] = activation[n+offset] +
                     logadd( on_free_energy[n + offset - (sub_tree_size/2+1)],
                             off_free_energy[n + offset - (sub_tree_size/2+1)] ) ;
                 if( use_signed_samples )
@@ -191,28 +191,28 @@
                         logadd( on_free_energy[n + offset + (sub_tree_size/2+1)],
                                 off_free_energy[n + offset + (sub_tree_size/2+1)] ) ;
                 else
-                    off_free_energy[ n + offset ] = 
+                    off_free_energy[ n + offset ] =
                         logadd( on_free_energy[n + offset + (sub_tree_size/2+1)],
                                 off_free_energy[n + offset + (sub_tree_size/2+1)] ) ;
-                
+
             }
             sub_tree_size = 2 * ( sub_tree_size + 1 ) - 1;
             depth--;
         }
         offset += n_nodes_per_tree;
-    }    
-    
+    }
+
     for( int i=0 ; i<size ; i++ )
         //local_node_expectation[i] = on_free_energy[i] / ( on_free_energy[i] + off_free_energy[i] );
         // Now working in log-domain
-        local_node_expectation[i] = safeexp(on_free_energy[i] 
+        local_node_expectation[i] = safeexp(on_free_energy[i]
                                             - logadd(on_free_energy[i], off_free_energy[i]));
 
     // Compute marginal expectations
     offset = 0;
     for( int t=0; t<n_trees; t++ )
     {
-        // Initialize root        
+        // Initialize root
         node = n_nodes_per_tree / 2;
         expectation[ node + offset ] = local_node_expectation[ node + offset ];
         off_expectation[ node + offset ] = (1 - local_node_expectation[ node + offset ]);
@@ -321,14 +321,14 @@
     PLASSERT( input.size() == input_size );
     output.resize( output_size );
 
-    int n_nodes_per_tree = size / n_trees;    
+    int n_nodes_per_tree = size / n_trees;
     int node, depth, sub_tree_size, grand_parent;
-    int offset = 0;    
+    int offset = 0;
     bool left_of_grand_parent;
     real grand_parent_prob;
 
     // Get local expectations at every node
-    
+
     // Divide and conquer computation of local (conditional) free energies
     for( int t=0; t<n_trees; t++ )
     {
@@ -355,40 +355,40 @@
         {
             for( int n=sub_tree_size; n<n_nodes_per_tree; n += 2*sub_tree_size + 2 )
             {
-                //on_free_energy[ n + offset ] = safeexp(input[n+offset] + bias[n+offset]) * 
+                //on_free_energy[ n + offset ] = safeexp(input[n+offset] + bias[n+offset]) *
                 //    ( on_free_energy[n + offset - sub_tree_size] + off_free_energy[n + offset - sub_tree_size] ) ;
-                //off_free_energy[ n + offset ] = 
+                //off_free_energy[ n + offset ] =
                 //    ( on_free_energy[n + offset + sub_tree_size] + off_free_energy[n + offset + sub_tree_size] ) ;
                 // Now working in the log-domain
                 on_free_energy[ n + offset ] = input[n+offset] + bias[n+offset] +
-                    logadd( on_free_energy[n + offset - (sub_tree_size/2+1)], 
+                    logadd( on_free_energy[n + offset - (sub_tree_size/2+1)],
                             off_free_energy[n + offset - (sub_tree_size/2+1)] ) ;
                 if( use_signed_samples )
                     off_free_energy[ n + offset ] = -(input[n+offset] + bias[n+offset]) +
-                        logadd( on_free_energy[n + offset + (sub_tree_size/2+1)], 
+                        logadd( on_free_energy[n + offset + (sub_tree_size/2+1)],
                                 off_free_energy[n + offset + (sub_tree_size/2+1)] ) ;
                 else
-                    off_free_energy[ n + offset ] = 
-                        logadd( on_free_energy[n + offset + (sub_tree_size/2+1)], 
+                    off_free_energy[ n + offset ] =
+                        logadd( on_free_energy[n + offset + (sub_tree_size/2+1)],
                                 off_free_energy[n + offset + (sub_tree_size/2+1)] ) ;
             }
             sub_tree_size = 2 * ( sub_tree_size + 1 ) - 1;
             depth--;
         }
         offset += n_nodes_per_tree;
-    }    
-    
+    }
+
     for( int i=0 ; i<size ; i++ )
         //local_node_expectation[i] = on_free_energy[i] / ( on_free_energy[i] + off_free_energy[i] );
         // Now working in log-domain
-        local_node_expectation[i] = safeexp(on_free_energy[i] 
+        local_node_expectation[i] = safeexp(on_free_energy[i]
                                             - logadd(on_free_energy[i], off_free_energy[i]));
 
     // Compute marginal expectations
-    offset = 0;    
+    offset = 0;
     for( int t=0; t<n_trees; t++ )
     {
-        // Initialize root        
+        // Initialize root
         node = n_nodes_per_tree / 2;
         output[ node + offset ] = local_node_expectation[ node + offset ];
         off_expectation[ node + offset ] = (1 - local_node_expectation[ node + offset ]);
@@ -516,7 +516,7 @@
     }
 
     // Compute gradient on marginal expectations
-    int n_nodes_per_tree = size / n_trees;    
+    int n_nodes_per_tree = size / n_trees;
     int node, depth, sub_tree_size, grand_parent;
     int offset = 0;
     bool left_of_grand_parent;
@@ -537,7 +537,7 @@
             left_of_grand_parent = true;
             for( int n=sub_tree_size; n<n_nodes_per_tree; n += 4*sub_tree_size + 4 )
             {
-                out_grad = output_gradient[ n + offset ] + 
+                out_grad = output_gradient[ n + offset ] +
                     on_tree_gradient[ n + offset ] ;
                 off_grad = off_tree_gradient[ n + offset ] ;
                 node_exp = local_node_expectation[ n + offset ];
@@ -548,9 +548,9 @@
                     grand_parent = n + offset + 3*sub_tree_size + 3;
                     grand_parent_prob = output[ grand_parent ];
                     // Gradient for rest of the tree
-                    on_tree_gradient[ grand_parent ] += 
-                        ( out_grad * node_exp 
-                          + off_grad * (1 - node_exp) ) 
+                    on_tree_gradient[ grand_parent ] +=
+                        ( out_grad * node_exp
+                          + off_grad * (1 - node_exp) )
                         * parent_exp;
                     left_of_grand_parent = false;
                 }
@@ -559,20 +559,20 @@
                     grand_parent = n + offset - sub_tree_size - 1;
                     grand_parent_prob = off_expectation[ grand_parent ];
                     // Gradient for rest of the tree
-                    off_tree_gradient[ grand_parent ] += 
-                        ( out_grad * node_exp 
+                    off_tree_gradient[ grand_parent ] +=
+                        ( out_grad * node_exp
                           + off_grad * (1 - node_exp) )
                         * parent_exp;
                     left_of_grand_parent = true;
                 }
 
                 // Gradient w/r current node
-                local_node_expectation_gradient[ n + offset ] += 
+                local_node_expectation_gradient[ n + offset ] +=
                     ( out_grad - off_grad ) * parent_exp * grand_parent_prob;
                     //* node_exp * ( 1 - node_exp );
 
                 // Gradient w/r parent node
-                local_node_expectation_gradient[ n + offset + sub_tree_size + 1 ] += 
+                local_node_expectation_gradient[ n + offset + sub_tree_size + 1 ] +=
                     ( out_grad * node_exp + off_grad * (1 - node_exp) )  * grand_parent_prob;
                     //* parent_exp * (1-parent_exp) ;
 
@@ -582,7 +582,7 @@
             left_of_grand_parent = true;
             for( int n=3*sub_tree_size+2; n<n_nodes_per_tree; n += 4*sub_tree_size + 4 )
             {
-                out_grad = output_gradient[ n + offset ] + 
+                out_grad = output_gradient[ n + offset ] +
                     on_tree_gradient[ n + offset ] ;
                 off_grad = off_tree_gradient[ n + offset ] ;
                 node_exp = local_node_expectation[ n + offset ];
@@ -593,9 +593,9 @@
                     grand_parent = n + offset + sub_tree_size + 1;
                     grand_parent_prob = output[ grand_parent ];
                     // Gradient for rest of the tree
-                    on_tree_gradient[ grand_parent ] += 
-                        ( out_grad * node_exp 
-                          + off_grad * (1 - node_exp) ) 
+                    on_tree_gradient[ grand_parent ] +=
+                        ( out_grad * node_exp
+                          + off_grad * (1 - node_exp) )
                         * ( 1 - parent_exp );
                     left_of_grand_parent = false;
                 }
@@ -604,20 +604,20 @@
                     grand_parent = n + offset - 3*sub_tree_size - 3;
                     grand_parent_prob = off_expectation[ grand_parent ];
                     // Gradient for rest of the tree
-                    off_tree_gradient[ grand_parent ] += 
-                        ( out_grad * node_exp 
-                          + off_grad * (1 - node_exp) ) 
+                    off_tree_gradient[ grand_parent ] +=
+                        ( out_grad * node_exp
+                          + off_grad * (1 - node_exp) )
                         * ( 1 - parent_exp );
                     left_of_grand_parent = true;
                 }
 
                 // Gradient w/r current node
-                local_node_expectation_gradient[ n + offset ] += 
+                local_node_expectation_gradient[ n + offset ] +=
                     ( out_grad - off_grad ) * ( 1 - parent_exp ) * grand_parent_prob;
                     //* node_exp * ( 1 - node_exp );
 
                 // Gradient w/r parent node
-                local_node_expectation_gradient[ n + offset - sub_tree_size - 1 ] -= 
+                local_node_expectation_gradient[ n + offset - sub_tree_size - 1 ] -=
                     ( out_grad * node_exp + off_grad * (1 - node_exp) )  * grand_parent_prob;
                     //* parent_exp * (1-parent_exp) ;
             }
@@ -630,49 +630,49 @@
 
         //// Left child
         node = sub_tree_size;
-        out_grad = output_gradient[ node + offset ] + 
+        out_grad = output_gradient[ node + offset ] +
             on_tree_gradient[ node + offset ] ;
         off_grad = off_tree_gradient[ node + offset ] ;
         node_exp = local_node_expectation[ node + offset ];
         parent_exp = local_node_expectation[ node + offset + sub_tree_size + 1 ];
-        
+
         // Gradient w/r current node
-        local_node_expectation_gradient[ node + offset ] += 
+        local_node_expectation_gradient[ node + offset ] +=
             ( out_grad - off_grad ) * parent_exp;
             //* node_exp * ( 1 - node_exp );
-        
+
         // Gradient w/r parent node
-        local_node_expectation_gradient[ node + offset + sub_tree_size + 1 ] += 
+        local_node_expectation_gradient[ node + offset + sub_tree_size + 1 ] +=
             ( out_grad * node_exp  + off_grad * (1 - node_exp) );
             //* parent_exp * (1-parent_exp) ;
 
         //// Right child
         node = 3*sub_tree_size+2;
-        out_grad = output_gradient[ node + offset ] + 
+        out_grad = output_gradient[ node + offset ] +
             on_tree_gradient[ node + offset ] ;
         off_grad = off_tree_gradient[ node + offset ] ;
         node_exp = local_node_expectation[ node + offset ];
         parent_exp = local_node_expectation[ node + offset - sub_tree_size - 1 ];
 
         // Gradient w/r current node
-        local_node_expectation_gradient[ node + offset ] += 
+        local_node_expectation_gradient[ node + offset ] +=
             ( out_grad - off_grad ) * ( 1 - parent_exp ) ;
             //* node_exp * ( 1 - node_exp );
-        
+
         // Gradient w/r parent node
-        local_node_expectation_gradient[ node + offset - sub_tree_size - 1 ] -= 
+        local_node_expectation_gradient[ node + offset - sub_tree_size - 1 ] -=
             ( out_grad * node_exp + off_grad * (1 - node_exp) ) ;
             //* parent_exp * (1-parent_exp) ;
-        
+
         ////// Root
         node = n_nodes_per_tree / 2;
         sub_tree_size = 2 * ( sub_tree_size + 1 ) - 1;
 
-        out_grad = output_gradient[ node + offset ] + 
+        out_grad = output_gradient[ node + offset ] +
             on_tree_gradient[ node + offset ] ;
         off_grad = off_tree_gradient[ node + offset ] ;
         node_exp = local_node_expectation[ node + offset ];
-        local_node_expectation_gradient[ node + offset ] += 
+        local_node_expectation_gradient[ node + offset ] +=
             ( out_grad - off_grad );// * node_exp * ( 1 - node_exp );
 
         offset += n_nodes_per_tree;
@@ -699,16 +699,16 @@
                 out_grad = on_free_energy_gradient[ n + offset ];
                 node_exp = local_node_expectation[n + offset - (sub_tree_size/2+1)];
                 input_gradient[n+offset] += out_grad;
-                on_free_energy_gradient[n + offset - (sub_tree_size/2+1)] += out_grad * node_exp; 
-                off_free_energy_gradient[n + offset - (sub_tree_size/2+1)] += out_grad * (1 - node_exp); 
+                on_free_energy_gradient[n + offset - (sub_tree_size/2+1)] += out_grad * node_exp;
+                off_free_energy_gradient[n + offset - (sub_tree_size/2+1)] += out_grad * (1 - node_exp);
 
                 out_grad = off_free_energy_gradient[ n + offset ];
                 node_exp = local_node_expectation[n + offset + (sub_tree_size/2+1)];
                 if( use_signed_samples )
                     input_gradient[n+offset] -= out_grad;
-                on_free_energy_gradient[n + offset + (sub_tree_size/2+1)] += out_grad * node_exp; 
-                off_free_energy_gradient[n + offset + (sub_tree_size/2+1)] += 
-                    out_grad * (1 - node_exp); 
+                on_free_energy_gradient[n + offset + (sub_tree_size/2+1)] += out_grad * node_exp;
+                off_free_energy_gradient[n + offset + (sub_tree_size/2+1)] +=
+                    out_grad * (1 - node_exp);
             }
             sub_tree_size /= 2;
             depth++;
@@ -721,7 +721,7 @@
         {
             input_gradient[n+offset] += on_free_energy_gradient[ n + offset ];
             if( use_signed_samples )
-                input_gradient[n+offset] -= off_free_energy_gradient[ n + offset ];                
+                input_gradient[n+offset] -= off_free_energy_gradient[ n + offset ];
         }
 
         offset += n_nodes_per_tree;
@@ -858,7 +858,7 @@
             // nll = - target*log(sigmoid(act)) -(1-target)*log(1-sigmoid(act))
             // but it is numerically unstable, so use instead the following identity:
             //     = target*softplus(-act) +(1-target)*(act+softplus(-act))
-            //     = act + softplus(-act) - target*act 
+            //     = act + softplus(-act) - target*act
             //     = softplus(act) - target*act
         }
     } else {
@@ -892,7 +892,7 @@
             for( int i=0 ; i<size ; i++ ) // loop over outputs
             {
                 if(!fast_exact_is_equal(target[i],0.0))
-                    // nll -= target[i] * pl_log(expectations[i]); 
+                    // nll -= target[i] * pl_log(expectations[i]);
                     // but it is numerically unstable, so use instead
                     // log (1/(1+exp(-x))) = -log(1+exp(-x)) = -softplus(-x)
                     nll += target[i] * tabulated_softplus(-activation[i]);
@@ -908,7 +908,7 @@
             for( int i=0 ; i<size ; i++ ) // loop over outputs
             {
                 if(!fast_exact_is_equal(target[i],0.0))
-                    // nll -= target[i] * pl_log(expectations[i]); 
+                    // nll -= target[i] * pl_log(expectations[i]);
                     // but it is numerically unstable, so use instead
                     // log (1/(1+exp(-x))) = -log(1+exp(-x)) = -softplus(-x)
                     nll += target[i] * softplus(-activation[i]);

Modified: trunk/plearn_learners/online/SoftmaxModule.h
===================================================================
--- trunk/plearn_learners/online/SoftmaxModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/SoftmaxModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -79,7 +79,7 @@
                              Mat& input_gradients,
                              const Mat& output_gradients,
                              bool accumulate = false);
-    
+
     //! this version allows to obtain the input gradient and diag_hessian
     virtual void bbpropUpdate(const Vec& input, const Vec& output,
                               Vec& input_gradient,

Modified: trunk/plearn_learners/online/SplitModule.cc
===================================================================
--- trunk/plearn_learners/online/SplitModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/SplitModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -59,7 +59,7 @@
     "can be seen as performed on each row of these matrices.\n"
     );
 
-SplitModule::SplitModule() 
+SplitModule::SplitModule()
     : down_port_name("down_port")
 /* ### Initialize all fields to their default value here */
 {
@@ -194,7 +194,7 @@
             start += width;
         }
         return;
-    } 
+    }
     bool one_of_the_up_ports_wants_a_gradient = false;
     for (int i=0;i<up_port_sizes.length();i++)
         if (ports_gradient[i+1] && ports_gradient[i+1]->isEmpty())
@@ -211,7 +211,7 @@
         for (int i=0;i<up_port_sizes.length();i++)
         {
             int width = up_port_sizes[i];
-            if (ports_gradient[i+1] && ports_gradient[i+1]->isEmpty()) 
+            if (ports_gradient[i+1] && ports_gradient[i+1]->isEmpty())
             {
                 ports_gradient[i+1]->resize(output_gradient.length(),width);
                 (*ports_gradient[i+1]) += output_gradient.subMatColumns(start,width);
@@ -242,7 +242,7 @@
 //////////////////
 const TMat<int>& SplitModule::getPortSizes() {
     if (sizes.isEmpty())
-    { 
+    {
         sizes.resize(nPorts(),2);
         sizes.column(0).fill(-1);
         sizes(0,1) = input_size;

Modified: trunk/plearn_learners/online/SplitModule.h
===================================================================
--- trunk/plearn_learners/online/SplitModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/SplitModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -67,7 +67,7 @@
     string down_port_name;
     TVec<int> up_port_sizes;
     TVec<string> up_port_names;
-     
+
 public:
     //#####  Public Member Functions  #########################################
 

Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.h
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -203,10 +203,10 @@
     virtual void train();
 
     //! Computes the output from the input.
-    virtual void computeOutputWithoutCorrelationConnections(const Vec& input, 
+    virtual void computeOutputWithoutCorrelationConnections(const Vec& input,
                                                         Vec& output) const;
     //! Computes the output from the input
-    virtual void computeOutputsWithoutCorrelationConnections(const Mat& input, 
+    virtual void computeOutputsWithoutCorrelationConnections(const Mat& input,
                                                         Mat& output) const;
 
     //! Computes the output from the input.
@@ -416,9 +416,9 @@
 
     void setLearningRate( real the_learning_rate );
 
-    // List of remote methods    
+    // List of remote methods
     Vec remote_computeOutputWithoutCorrelationConnections(const Vec& input) const;
-    
+
     Mat remote_computeOutputsWithoutCorrelationConnections(const Mat& inputs) const;
 
     //! Global storage to save memory allocations.

Modified: trunk/plearn_learners/online/TanhModule.cc
===================================================================
--- trunk/plearn_learners/online/TanhModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/TanhModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -98,7 +98,7 @@
                              bool accumulate)
 {
     int mbs=inputs.length();
-    PLASSERT(mbs==outputs.length() && 
+    PLASSERT(mbs==outputs.length() &&
              mbs==output_gradients.length());
     input_gradients.resize(mbs,input_size);
     for (int i=0;i<mbs;i++)

Modified: trunk/plearn_learners/online/VBoundDBN2.cc
===================================================================
--- trunk/plearn_learners/online/VBoundDBN2.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/VBoundDBN2.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -183,7 +183,7 @@
     substract(delta_h, *sampled_h_, delta_h);
     columnSum(delta_h,delta_hb);
     multiplyAcc(rbm1->hidden_layer->bias,delta_hb,rbm1->cd_learning_rate);
-    
+
     //  dlogbound/dji sampling approx = v[j] - reconstructed_v[j]
     columnSum(reconstructed_v,delta_vb1);
     columnSum(*input,delta_vb2);
@@ -247,7 +247,7 @@
     //
 
     // for learning or testing
-    if (input && !input->isEmpty()) 
+    if (input && !input->isEmpty())
     {
         int mbs=input->length();
         FE1v.resize(mbs,1);
@@ -261,8 +261,8 @@
         global_improvement->resize(mbs,1);
         ph_given_v->resize(mbs,rbm1->hidden_layer->size);
 
-        // compute things needed for everything else 
-    
+        // compute things needed for everything else
+
         rbm1->sampleHiddenGivenVisible(*input);
         *ph_given_v << rbm1->hidden_layer->getExpectations();
         *sampled_h << rbm1->hidden_layer->samples;

Modified: trunk/plearn_learners/online/VBoundDBN2.h
===================================================================
--- trunk/plearn_learners/online/VBoundDBN2.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/VBoundDBN2.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -47,7 +47,7 @@
 
 /**
  * 2-RBM DBN trained using Hinton's new variational bound of global likelihood:
- * 
+ *
  * log P(x) >= -FE1(x) + E_{P1(h|x)}[ FE1(h) - FE2(h) ] - log Z2
  *
  * where P1 and P2 are RBMs with Pi(x) = exp(-FEi(x))/Zi.



From louradou at mail.berlios.de  Wed Jun 11 17:00:10 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Wed, 11 Jun 2008 17:00:10 +0200
Subject: [Plearn-commits] r9116 - trunk/plearn_learners/online
Message-ID: <200806111500.m5BF0A89018030@sheep.berlios.de>

Author: louradou
Date: 2008-06-11 17:00:09 +0200 (Wed, 11 Jun 2008)
New Revision: 9116

Added:
   trunk/plearn_learners/online/RBMSparse1DMatrixConnection.cc
   trunk/plearn_learners/online/RBMSparse1DMatrixConnection.h
Log:
Added: sparse RBM connections



Added: trunk/plearn_learners/online/RBMSparse1DMatrixConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMSparse1DMatrixConnection.cc	2008-06-11 02:37:22 UTC (rev 9115)
+++ trunk/plearn_learners/online/RBMSparse1DMatrixConnection.cc	2008-06-11 15:00:09 UTC (rev 9116)
@@ -0,0 +1,440 @@
+// -*- C++ -*-
+
+// RBMSparse1DMatrixConnection.cc
+//
+// Copyright (C) 2008 Jerome Louradour
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Jerome Louradour
+
+/*! \file RBMSparse1DMatrixConnection.cc */
+
+
+
+#include "RBMSparse1DMatrixConnection.h"
+#include <plearn/math/TMat_maths.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    RBMSparse1DMatrixConnection,
+    "RBM connections with sparses weights, designed for 1D inputs.",
+    "");
+
+RBMSparse1DMatrixConnection::RBMSparse1DMatrixConnection( real the_learning_rate ) :
+    filter_size(-1),
+    enforce_positive_weights(false)
+{
+}
+
+void RBMSparse1DMatrixConnection::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "filter_size", &RBMSparse1DMatrixConnection::filter_size,
+                  OptionBase::buildoption,
+                  "Length of each filter. If -1 then input_size is taken (RBMMatrixConnection).");
+
+    declareOption(ol, "enforce_positive_weights", &RBMSparse1DMatrixConnection::enforce_positive_weights,
+                  OptionBase::buildoption,
+                  "Whether or not to enforce having positive weights.");
+
+    declareOption(ol, "step_size", &RBMSparse1DMatrixConnection::step_size,
+                  OptionBase::learntoption,
+                  "Step between each filter.");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+////////////////////
+// declareMethods //
+////////////////////
+void RBMSparse1DMatrixConnection::declareMethods(RemoteMethodMap& rmm)
+{
+    // Insert a backpointer to remote methods; note that this is different from
+    // declareOptions().
+    rmm.inherited(inherited::_getRemoteMethodMap_());
+    declareMethod(
+        rmm, "getWeights", &RBMSparse1DMatrixConnection::getWeights,
+        (BodyDoc("Returns the full weights (including 0s).\n"),
+         RetDoc ("Matrix of weights (n_hidden x input_size)")));
+}
+
+void RBMSparse1DMatrixConnection::build_()
+{
+    if( up_size <= 0 || down_size <= 0 )
+        return;
+
+    if( filter_size < 0 )
+        filter_size = down_size;
+        
+    step_size = (int)((real)(down_size-filter_size)/(real)(up_size-1));
+        
+    PLASSERT( filter_size <= down_size );
+
+    bool needs_forget = false; // do we need to reinitialize the parameters?
+
+    if( weights.length() != up_size ||
+        weights.width() != filter_size )
+    {
+        weights.resize( up_size, filter_size );
+        needs_forget = true;
+    }
+
+    weights_pos_stats.resize( up_size, filter_size );
+    weights_neg_stats.resize( up_size, filter_size );
+
+    if( momentum != 0. )
+        weights_inc.resize( up_size, filter_size );
+
+    if( needs_forget ) {
+        forget();
+    }
+    
+    clearStats();
+}
+
+void RBMSparse1DMatrixConnection::build()
+{
+    RBMConnection::build();
+    build_();
+}
+
+int RBMSparse1DMatrixConnection::filterStart(int idx) const
+{
+    return step_size*idx;
+}
+
+int RBMSparse1DMatrixConnection::filterSize(int idx) const
+{
+    return filter_size;
+}
+
+Mat RBMSparse1DMatrixConnection::getWeights() const
+{
+    Mat w( up_size, down_size);
+    w.clear();
+    for ( int i=0; i<up_size; i++)
+        w(i).subVec( filterStart(i), filterSize(i) ) << weights(i);
+    return w;
+}
+
+////////////////////////
+// accumulateStats //
+////////////////////////
+void RBMSparse1DMatrixConnection::accumulatePosStats( const Mat& down_values,
+                                              const Mat& up_values )
+{
+    int mbs=down_values.length();
+    PLASSERT(up_values.length()==mbs);
+    // weights_pos_stats += up_values * down_values'
+    for ( int i=0; i<up_size; i++)
+        transposeProductAcc( weights_pos_stats(i),
+                             down_values.subMatColumns( filterStart(i), filterSize(i) ),
+                             up_values(i));
+    pos_count+=mbs;
+}
+
+void RBMSparse1DMatrixConnection::accumulateNegStats( const Mat& down_values,
+                                              const Mat& up_values )
+{
+    int mbs=down_values.length();
+    PLASSERT(up_values.length()==mbs);
+    // weights_neg_stats += up_values * down_values'
+    for ( int i=0; i<up_size; i++)
+        transposeProductAcc( weights_neg_stats(i),
+                             down_values.subMatColumns( filterStart(i), filterSize(i) ),
+                             up_values(i));
+    neg_count+=mbs;
+}
+
+////////////////////
+// computeProduct //
+////////////////////
+void RBMSparse1DMatrixConnection::computeProducts(int start, int length,
+                                          Mat& activations,
+                                          bool accumulate ) const
+{
+    PLASSERT( activations.width() == length );
+    activations.resize(inputs_mat.length(), length);
+    if( going_up )
+    {
+        PLASSERT( start+length <= up_size );
+        // activations(k, i-start) += sum_j weights(i,j) inputs_mat(k, j)
+        if( accumulate )
+            for (int i=start; i<start+length; i++)
+                productAcc( activations.column(i-start).toVec(),
+                            inputs_mat.subMatColumns( filterStart(i), filterSize(i) ),
+                            weights(i) );
+        else
+            for (int i=start; i<start+length; i++)
+                product( activations.column(i-start).toVec(),
+                         inputs_mat.subMatColumns( filterStart(i), filterSize(i) ),
+                         weights(i) );
+    }
+    else
+    {
+        PLASSERT( start+length <= down_size );
+        if( !accumulate )
+            activations.clear();
+        // activations(k, i-start) += sum_j weights(j,i) inputs_mat(k, j)
+        Mat all_activations(inputs_mat.length(), down_size);
+        all_activations.subMatColumns( start, length ) << activations;
+        for (int i=0; i<up_size; i++)
+        {
+            externalProductAcc( all_activations.subMatColumns( filterStart(i), filterSize(i) ),
+                                inputs_mat.column(i).toVec(),
+                                weights(i) );
+        }
+        activations << all_activations.subMatColumns( start, length );
+    }
+}
+
+///////////
+// fprop //
+///////////
+void RBMSparse1DMatrixConnection::fprop(const Vec& input, const Mat& rbm_weights,
+                          Vec& output) const
+{
+    PLERROR("RBMSparse1DMatrixConnection::fprop not implemented.");
+}
+
+/////////////////
+// bpropUpdate //
+/////////////////
+void RBMSparse1DMatrixConnection::bpropUpdate(const Mat& inputs, const Mat& outputs,
+                                      Mat& input_gradients,
+                                      const Mat& output_gradients,
+                                      bool accumulate)
+{
+    PLASSERT( inputs.width() == down_size );
+    PLASSERT( outputs.width() == up_size );
+    PLASSERT( output_gradients.width() == up_size );
+
+    if( accumulate )
+        PLASSERT_MSG( input_gradients.width() == down_size &&
+                      input_gradients.length() == inputs.length(),
+                      "Cannot resize input_gradients and accumulate into it" );
+    else {
+        input_gradients.resize(inputs.length(), down_size);
+        input_gradients.clear();
+    }  
+        
+    for (int i=0; i<up_size; i++) {
+        int filter_start= filterStart(i), length= filterSize(i);
+        
+        // input_gradients = output_gradient * weights
+        externalProductAcc( input_gradients.subMatColumns( filter_start, length ),
+                            output_gradients.column(i).toVec(),
+                            weights(i));
+
+        // weights -= learning_rate/n * output_gradients' * inputs
+        transposeProductScaleAcc( weights(i),
+                                  inputs.subMatColumns( filter_start, length ),
+                                  output_gradients.column(i).toVec(),
+                                  -learning_rate / inputs.length(), real(1));
+
+        if( enforce_positive_weights )
+            for (int j=0; j<filter_size; j++)
+                weights(i,j)= max( real(0), weights(i,j) );
+   }
+
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+        applyWeightPenalty();
+}
+
+
+
+////////////////////
+// bpropAccUpdate //
+////////////////////
+void RBMSparse1DMatrixConnection::bpropAccUpdate(const TVec<Mat*>& ports_value,
+                                         const TVec<Mat*>& ports_gradient)
+{
+    //TODO: add weights as port?
+    PLASSERT( ports_value.length() == nPorts()
+              && ports_gradient.length() == nPorts() );
+
+    Mat* down = ports_value[0];
+    //Mat* up = ports_value[1];
+    Mat* down_grad = ports_gradient[0];
+    Mat* up_grad = ports_gradient[1];
+
+    PLASSERT( down && !down->isEmpty() );
+    PLASSERT( up && !up->isEmpty() );
+
+    int batch_size = down->length();
+    PLASSERT( up->length() == batch_size );
+
+    // If we have up_grad
+    if( up_grad && !up_grad->isEmpty() )
+    {
+        // down_grad should not be provided
+        PLASSERT( !down_grad || down_grad->isEmpty() );
+        PLASSERT( up_grad->length() == batch_size );
+        PLASSERT( up_grad->width() == up_size );
+
+        bool compute_down_grad = false;
+        if( down_grad && down_grad->isEmpty() )
+        {
+            compute_down_grad = true;
+            PLASSERT( down_grad->width() == down_size );
+            down_grad->resize(batch_size, down_size);
+        }
+        
+        for (int i=0; i<up_size; i++) {
+            int filter_start= filterStart(i), length= filterSize(i);
+            
+            // propagate gradient
+            // input_gradients = output_gradient * weights
+            if( compute_down_grad )
+                externalProductAcc( down_grad->subMatColumns( filter_start, length ),
+                                    up_grad->column(i).toVec(),
+                                    weights(i));
+
+            // update weights
+            // weights -= learning_rate/n * output_gradients' * inputs
+            transposeProductScaleAcc( weights(i),
+                                      down->subMatColumns( filter_start, length ),
+                                      up_grad->column(i).toVec(),
+                                      -learning_rate / batch_size, real(1));
+
+            if( enforce_positive_weights )
+                for (int j=0; j<filter_size; j++)
+                    weights(i,j)= max( real(0), weights(i,j) );
+       }
+    }
+    else if( down_grad && !down_grad->isEmpty() )
+    {
+        PLERROR("down-up gradient not implemented in RBMSparse1DMatrixConnection::bpropAccUpdate.");
+
+        PLASSERT( down_grad->length() == batch_size );
+        PLASSERT( down_grad->width() == down_size );
+    }
+    else
+        PLCHECK_MSG( false,
+                     "Unknown port configuration" );
+
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+        applyWeightPenalty();
+}
+
+void RBMSparse1DMatrixConnection::update( const Mat& pos_down_values, // v_0
+                                  const Mat& pos_up_values,   // h_0
+                                  const Mat& neg_down_values, // v_1
+                                  const Mat& neg_up_values )  // h_1
+{
+    // weights += learning_rate * ( h_0 v_0' - h_1 v_1' );
+    // or:
+    // weights[i][j] += learning_rate * (h_0[i] v_0[j] - h_1[i] v_1[j]);
+
+    PLASSERT( pos_up_values.width() == weights.length() );
+    PLASSERT( neg_up_values.width() == weights.length() );
+    PLASSERT( pos_down_values.width() == down_size );
+    PLASSERT( neg_down_values.width() == down_size );
+
+    if( momentum == 0. )
+    {
+        // We use the average gradient over a mini-batch.
+        real avg_lr = learning_rate / pos_down_values.length();
+
+        for (int i=0; i<up_size; i++) {
+            int filter_start= filterStart(i), length= filterSize(i);
+
+            transposeProductScaleAcc( weights(i),
+                                      pos_down_values.subMatColumns( filter_start, length ),
+                                      pos_up_values.column(i).toVec(), 
+                                      avg_lr, real(1));
+
+            transposeProductScaleAcc( weights(i),
+                                      neg_down_values.subMatColumns( filter_start, length ),
+                                      neg_up_values.column(i).toVec(),
+                                      -avg_lr, real(1));
+
+            if( enforce_positive_weights )
+                for (int j=0; j<filter_size; j++)
+                    weights(i,j)= max( real(0), weights(i,j) );
+        }
+    }
+    else
+        PLERROR("RBMSparse1DMatrixConnection::update minibatch with momentum - Not implemented");
+
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+        applyWeightPenalty();
+}
+
+////////////
+// forget //
+////////////
+// Reset the parameters to the state they would be BEFORE starting training.
+void RBMSparse1DMatrixConnection::forget()
+{
+    clearStats();
+    if( initialization_method == "zero" )
+        weights.clear();
+    else
+    {
+        if( !random_gen ) {
+            PLWARNING( "RBMSparse1DMatrixConnection: cannot forget() without"
+                       " random_gen" );
+            return;
+        }
+        real d = 1. / max( filter_size, up_size );
+        if( initialization_method == "uniform_sqrt" )
+            d = sqrt( d );
+
+        if( enforce_positive_weights )
+            random_gen->fill_random_uniform( weights, real(0), d );
+        else
+            random_gen->fill_random_uniform( weights, -d, d );
+    }
+    L2_n_updates = 0;
+}
+
+//! return the number of parameters
+int RBMSparse1DMatrixConnection::nParameters() const
+{
+    return weights.size();
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


Property changes on: trunk/plearn_learners/online/RBMSparse1DMatrixConnection.cc
___________________________________________________________________
Name: svn:executable
   + *

Added: trunk/plearn_learners/online/RBMSparse1DMatrixConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMSparse1DMatrixConnection.h	2008-06-11 02:37:22 UTC (rev 9115)
+++ trunk/plearn_learners/online/RBMSparse1DMatrixConnection.h	2008-06-11 15:00:09 UTC (rev 9116)
@@ -0,0 +1,163 @@
+// -*- C++ -*-
+
+// RBMSparse1DMatrixConnection.h
+//
+// Copyright (C) 2008 Jerome Louradour
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Jerome Louradour
+
+/*! \file RBMSparse1DMatrixConnection.h */
+
+
+#ifndef RBMSparse1DMatrixConnection_INC
+#define RBMSparse1DMatrixConnection_INC
+
+#include "RBMMatrixConnection.h"
+
+namespace PLearn {
+using namespace std;
+
+
+/**
+ * Stores and learns the parameters between two linear layers of an RBM.
+ *
+ */
+class RBMSparse1DMatrixConnection: public RBMMatrixConnection
+{
+    typedef RBMMatrixConnection inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    int filter_size;
+
+    bool enforce_positive_weights;
+
+    //#####  Learned Options  #################################################
+
+    int step_size;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    RBMSparse1DMatrixConnection( real the_learning_rate=0 );
+
+    // Your other public member functions go here
+    virtual void accumulatePosStats( const Mat& down_values,
+                                     const Mat& up_values );
+
+    virtual void accumulateNegStats( const Mat& down_values,
+                                     const Mat& up_values );
+
+    //! Computes the vectors of activation of "length" units,
+    //! starting from "start", and stores (or add) them into "activations".
+    //! "start" indexes an up unit if "going_up", else a down unit.
+    virtual void computeProducts(int start, int length,
+                                 Mat& activations,
+                                 bool accumulate=false ) const;
+
+
+    //! given the input and the connection weights,
+    //! compute the output (possibly resize it  appropriately)
+    virtual void fprop(const Vec& input, const Mat& rbm_weights,
+                       Vec& output) const;
+
+    virtual void bpropUpdate(const Mat& inputs, const Mat& outputs,
+                             Mat& input_gradients,
+                             const Mat& output_gradients,
+                             bool accumulate = false);
+
+    virtual void bpropAccUpdate(const TVec<Mat*>& ports_value,
+                                const TVec<Mat*>& ports_gradient);
+
+    virtual void update( const Mat& pos_down_values,
+                         const Mat& pos_up_values,
+                         const Mat& neg_down_values,
+                         const Mat& neg_up_values);
+
+    //! reset the parameters to the state they would be BEFORE starting
+    //! training.  Note that this method is necessarily called from
+    //! build().
+    virtual void forget();
+
+    //! return the number of parameters
+    virtual int nParameters() const;
+
+    virtual Mat getWeights() const;
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(RBMSparse1DMatrixConnection);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+protected:
+    //#####  Protected Member Functions  ######################################
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+    //! Declare the methods that are remote-callable
+    static void declareMethods(RemoteMethodMap& rmm);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+    int filterStart(int idx) const;
+    int filterSize(int idx) const;
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(RBMSparse1DMatrixConnection);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


Property changes on: trunk/plearn_learners/online/RBMSparse1DMatrixConnection.h
___________________________________________________________________
Name: svn:executable
   + *



From tihocan at mail.berlios.de  Wed Jun 11 17:29:02 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 11 Jun 2008 17:29:02 +0200
Subject: [Plearn-commits] r9117 - trunk/plearn/ker
Message-ID: <200806111529.m5BFT2lc022692@sheep.berlios.de>

Author: tihocan
Date: 2008-06-11 17:29:02 +0200 (Wed, 11 Jun 2008)
New Revision: 9117

Modified:
   trunk/plearn/ker/GeodesicDistanceKernel.cc
Log:
More explicit error message

Modified: trunk/plearn/ker/GeodesicDistanceKernel.cc
===================================================================
--- trunk/plearn/ker/GeodesicDistanceKernel.cc	2008-06-11 15:00:09 UTC (rev 9116)
+++ trunk/plearn/ker/GeodesicDistanceKernel.cc	2008-06-11 15:29:02 UTC (rev 9117)
@@ -336,7 +336,9 @@
                 pb->update(k+1);
         }
     } else {
-        PLERROR("In GeodesicDistanceKernel::setDataForKernelMatrix - Unknown value for 'shortest_algo'");
+        PLERROR("In GeodesicDistanceKernel::setDataForKernelMatrix - Unknown "
+                "value for 'shortest_algo': %s",
+                shortest_algo.c_str());
     }
     // Save the result in geo_distances.
     if (geodesic_file.isEmpty()) {



From tihocan at mail.berlios.de  Wed Jun 11 17:29:17 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 11 Jun 2008 17:29:17 +0200
Subject: [Plearn-commits] r9118 - trunk/plearn_learners/unsupervised
Message-ID: <200806111529.m5BFTHAU022743@sheep.berlios.de>

Author: tihocan
Date: 2008-06-11 17:29:17 +0200 (Wed, 11 Jun 2008)
New Revision: 9118

Modified:
   trunk/plearn_learners/unsupervised/Isomap.cc
Log:
Typo fix

Modified: trunk/plearn_learners/unsupervised/Isomap.cc
===================================================================
--- trunk/plearn_learners/unsupervised/Isomap.cc	2008-06-11 15:29:02 UTC (rev 9117)
+++ trunk/plearn_learners/unsupervised/Isomap.cc	2008-06-11 15:29:17 UTC (rev 9118)
@@ -52,7 +52,7 @@
 ////////////
 Isomap::Isomap() 
     : geodesic_file(""),
-      geodesic_method("djikstra"),
+      geodesic_method("dijkstra"),
       knn(10)
 {
     kernel_is_distance = true;



From chapados at mail.berlios.de  Wed Jun 11 20:34:17 2008
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Wed, 11 Jun 2008 20:34:17 +0200
Subject: [Plearn-commits] r9119 - in trunk/python_modules/plearn: . gui_tools
Message-ID: <200806111834.m5BIYH5J029127@sheep.berlios.de>

Author: chapados
Date: 2008-06-11 20:34:17 +0200 (Wed, 11 Jun 2008)
New Revision: 9119

Added:
   trunk/python_modules/plearn/gui_tools/
   trunk/python_modules/plearn/gui_tools/console_logger.py
   trunk/python_modules/plearn/gui_tools/mpl_figure_editor.py
   trunk/python_modules/plearn/gui_tools/xp_workbench.py
Log:
Added support for figures

Added: trunk/python_modules/plearn/gui_tools/console_logger.py
===================================================================
--- trunk/python_modules/plearn/gui_tools/console_logger.py	2008-06-11 15:29:17 UTC (rev 9118)
+++ trunk/python_modules/plearn/gui_tools/console_logger.py	2008-06-11 18:34:17 UTC (rev 9119)
@@ -0,0 +1,189 @@
+# xp_workbench.py
+# Copyright (C) 2008 by Nicolas Chapados
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+
+# Author: Nicolas Chapados
+
+"""Provide a console-like object that supports stdin/stdout redirection.
+"""
+
+import fcntl
+import os
+import sys
+import threading
+import time
+import wx
+from   select import select
+
+from enthought.traits.api       import *
+from enthought.traits.ui.api    import CodeEditor, Group, Item, View
+from enthought.traits.ui.menu   import NoButtons
+
+
+## Raw stdout/stderr as global objects, for ease of debugging (python streams)
+_raw_stdout = None
+_raw_stderr = None
+
+
+class ConsoleLogger(HasTraits):
+
+    ## Title of logger object
+    title = "Output"
+
+    ## Contents of the actual logging window
+    data = Str
+
+    ## Whether this logger is currently active
+    is_active = false
+
+    ## Class variable containing which logger is currently active; cannot
+    ## have more than one active logger simultaneously.  NOTE: we assume
+    ## that this variable is only accessed within the UI thread.
+    active_logger = None
+
+    ## Default view
+    traits_view = View(Item('data~', editor=CodeEditor(foldable=False), show_label=False))
+
+    def activate_stdouterr_redirect(self, streams_to_watch={}):
+        """Redirect standard output and error to be sent to the log pane
+        instead of the console.
+        """
+        ## If we already have an active logger, raise
+        if ConsoleLogger.active_logger is not None:
+            raise RuntimeError, "Trying to activate redirection for a ConsoleLogger " +\
+                  "while it is already active for a different one"
+        ConsoleLogger.active_logger = self
+
+        ## Ensure that any pending writes are expelled before redirection
+        ## (Python only)
+        sys.stdout.flush()
+        sys.stderr.flush()
+        
+        ## Redirect standard output and standard error to display to the
+        ## log pane area.  Keep around old stdout/stderr in order to
+        ## display debugging messages.  They are called, respectively,
+        ## raw_stdout and raw_stderr (Python file objects); make them
+        ## global for ease of debugging.
+        global _raw_stdout, _raw_stderr
+        if _raw_stdout is None:
+            old_stdout_fd = os.dup(sys.stdout.fileno())
+            _raw_stdout   = os.fdopen(old_stdout_fd, 'w')
+        if _raw_stderr is None:
+            old_stderr_fd = os.dup(sys.stderr.fileno())
+            _raw_stderr   = os.fdopen(old_stderr_fd, 'w')
+
+        # print >>sys.stderr, "Original stderr"
+        # print >>_raw_stderr, "Redirected stderr"
+        # sys.stderr.flush()
+        # _raw_stderr.flush()
+        
+        (self.stdout_read, self.stdout_write) = os.pipe()
+        (self.stderr_read, self.stderr_write) = os.pipe()
+        os.dup2(self.stdout_write, sys.stdout.fileno())
+        os.dup2(self.stderr_write, sys.stderr.fileno())
+        
+        out_flags = fcntl.fcntl(self.stdout_read, fcntl.F_GETFL)
+        err_flags = fcntl.fcntl(self.stderr_read, fcntl.F_GETFL)
+        fcntl.fcntl(self.stdout_read, fcntl.F_SETFL, out_flags | os.O_NONBLOCK)
+        fcntl.fcntl(self.stderr_read, fcntl.F_SETFL, err_flags | os.O_NONBLOCK)
+
+        streams_to_watch.update({ self.stdout_read:'stdout',
+                                  self.stderr_read:'stderr' })
+
+        ## We're going to create a thread whose purpose is to indefinitely
+        ## wait (using 'select') on the stdout/stderr file descriptors.
+        ## When text arrives read it and update the log window.
+        class _ListenerThread(threading.Thread):
+            def run(inner_self):
+                listen_fds = streams_to_watch.keys()
+                eof = False
+                while not eof and self.is_active:   # Note: traits are thread-safe
+                    ready = select(listen_fds, [], [])
+                    for fd in listen_fds:
+                        if fd in ready[0]:
+                            chunk = os.read(fd, 65536)
+                            if chunk == '':
+                                eof = True
+                            self.data += chunk
+                    if eof:
+                        break
+                    select([],[],[],.1) # Give a little time for buffers to fill
+
+        listener = _ListenerThread()
+        self.is_active = True
+        listener.start()
+        
+
+    def desactivate_stdout_err_redirect(self):
+        """Bring back redirected file descriptors to their original state
+        and relinquish control as the active logger.
+        """
+        global _raw_stdout, _raw_stderr
+        if ConsoleLogger.active_logger == self:
+            ## Wait a little bit to ensure that the last data printed to
+            ## stdout/stderr has had time to be processed by the logging
+            ## thread
+            sys.stdout.flush()
+            sys.stderr.flush()
+            time.sleep(0.1)
+            
+            if _raw_stdout is not None:
+                os.dup2(_raw_stdout.fileno(), sys.stdout.fileno())
+                _raw_stdout = None
+
+            if _raw_stderr is not None:
+                os.dup2(_raw_stderr.fileno(), sys.stderr.fileno())
+                _raw_stderr = None
+
+            ConsoleLogger.active_logger = None
+            self.is_active = False
+
+
+#####  Test Case  ###########################################################
+
+if __name__ == "__main__":
+    class X(HasTraits):
+        log = Instance(ConsoleLogger,())
+        go  = Button("Go!")
+
+        def _go_fired(self):
+            self.log.activate_stdouterr_redirect()
+            print "This is printed to stdout but should go to logger"
+            print "Another print"
+            print >>sys.stderr, "This one goes to stderr"
+            self.log.desactivate_stdout_err_redirect()
+            print "Normal print to stdout"
+            print >>sys.stderr, "Normal print to stderr"
+
+        traits_view = View(Group(Item('log@'), Item('go'),
+                                 show_labels=False),
+                           resizable=True, buttons=NoButtons, width=0.5, height=0.5)
+
+    X().configure_traits()

Added: trunk/python_modules/plearn/gui_tools/mpl_figure_editor.py
===================================================================
--- trunk/python_modules/plearn/gui_tools/mpl_figure_editor.py	2008-06-11 15:29:17 UTC (rev 9118)
+++ trunk/python_modules/plearn/gui_tools/mpl_figure_editor.py	2008-06-11 18:34:17 UTC (rev 9119)
@@ -0,0 +1,91 @@
+#!/usr/bin/python
+
+"""Provide a traits-friendly container for Matplotlib figures.
+
+Code from Gael Varoquaux, modifications by Nicolas Chapados
+"""
+
+import wx
+import matplotlib
+matplotlib.use('WXAgg')          # We want matplotlib to use a wxPython backend
+
+from matplotlib.backends.backend_wxagg import FigureCanvasWxAgg as FigureCanvas
+from matplotlib.figure import Figure
+from matplotlib.backends.backend_wx import NavigationToolbar2Wx
+
+from enthought.traits.api          import Any, HasTraits, Instance
+from enthought.traits.ui.api       import Item, View
+from enthought.traits.ui.wx.editor import Editor
+from enthought.traits.ui.wx.basic_editor_factory import BasicEditorFactory
+
+class _MPLFigureEditor(Editor):
+    """Traits editor that contains the actual Matplotlib canvas for drawing.
+    """
+
+    scrollable = True
+
+    def init(self, parent):
+        self.control = self._create_canvas(parent)
+        self.set_tooltip()
+        
+    def update_editor(self):
+        pass
+
+    def _create_canvas(self, parent):
+        """ Create the MPL canvas. """
+        # The panel lets us add additional controls.
+        panel = wx.Panel(parent, -1, style=wx.CLIP_CHILDREN)
+        sizer = wx.BoxSizer(wx.VERTICAL)
+        panel.SetSizer(sizer)
+        # matplotlib commands to create a canvas
+        mpl_control = FigureCanvas(panel, -1, self.value)
+        sizer.Add(mpl_control, 1, wx.LEFT | wx.TOP | wx.GROW)
+        toolbar = NavigationToolbar2Wx(mpl_control)
+        sizer.Add(toolbar, 0, wx.EXPAND)
+        self.value.canvas.SetMinSize((50,50))
+        return panel
+
+
+class MPLFigureEditor(BasicEditorFactory):
+
+    klass = _MPLFigureEditor
+
+
+class TraitedFigure( HasTraits ):
+    """Traited object acting as a container for a single figure.
+    """
+    figure = Instance(Figure, editor=MPLFigureEditor())
+    title  = "Figure"
+
+    def _figure_default(self):
+        return Figure()
+
+    traits_view = View(Item('figure', show_label=False))
+
+
+
+#####  Test Case  ###########################################################
+
+if __name__ == "__main__":
+    # Create a window to demo the editor
+    from enthought.traits.api import HasTraits
+    from enthought.traits.ui.api import View, Item
+    from numpy import sin, cos, linspace, pi
+
+    class Test(HasTraits):
+
+        figure = Instance(Figure, ())
+
+        view = View(Item('figure', editor=MPLFigureEditor(),
+                                show_label=False),
+                        width=600,
+                        height=450,
+                        resizable=True)
+
+        def __init__(self):
+            super(Test, self).__init__()
+            axes = self.figure.add_subplot(111)
+            t = linspace(0, 2*pi, 200)
+            axes.plot(sin(t)*(1+0.5*cos(11*t)), cos(t)*(1+0.5*cos(11*t)))
+
+    Test().configure_traits()

Added: trunk/python_modules/plearn/gui_tools/xp_workbench.py
===================================================================
--- trunk/python_modules/plearn/gui_tools/xp_workbench.py	2008-06-11 15:29:17 UTC (rev 9118)
+++ trunk/python_modules/plearn/gui_tools/xp_workbench.py	2008-06-11 18:34:17 UTC (rev 9119)
@@ -0,0 +1,338 @@
+# xp_workbench.py
+# Copyright (C) 2008 by Nicolas Chapados
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+
+# Author: Nicolas Chapados
+
+## System imports
+import ctypes
+import inspect
+import os.path
+import sys
+import threading
+from   datetime import datetime
+
+## Traits
+from enthought.traits.api          import *
+from enthought.traits.ui.api       import *
+from enthought.traits.ui.menu      import NoButtons
+
+from console_logger    import ConsoleLogger
+from mpl_figure_editor import TraitedFigure
+
+
+#####  ExperimentContext  ###################################################
+
+class ExperimentContext(HasTraits):
+    """Contains the context of a single experiment -- either a running one
+    or a reloaded one.
+    """
+    expdir          = Directory(desc="Complete path to experiment directory")
+    
+    _all_tabs       = List(HasTraits, desc="Set of tabs associated with experiment",
+                           editor=ListEditor(use_notebook=True,
+                                             deletable=False,
+                                             dock_style="tab",
+                                             page_name=".title"))
+    
+    @property
+    def console_logger(self):
+        """Return a reference to the context's console logger."""
+        logger = self._all_tabs[0]
+        assert isinstance(logger, ConsoleLogger)
+        return logger
+
+    @property
+    def _display_expdir(self):
+        """Shortened version of expdir suitable for display."""
+        return os.path.basename(self.expdir)
+
+    def Figure(self, title="Figure"):
+        """Return a new Matplotlib figure and add it to the notebook.
+
+        Note that you must paint on this figure using the Matplotlib OO
+        API, not pylab.
+        """
+        f = TraitedFigure()
+        f.title = title
+        self._all_tabs.append(f)
+        return f.figure
+    
+    ## Default view
+    traits_view = View(Item('_all_tabs@', show_label=False))
+
+
+class _ConsoleOutput(HasTraits):
+    title    = "Output"
+    contents = Str
+
+    traits_view = View(Item("contents~",show_label=False))
+
+
+class _WorkerThread(threading.Thread):
+    """Utility class to perform experiment in a separate thread.
+    It notifies the workbench when it's done.
+
+    Note: this thread can (theoretically) be killed from outside, which is
+    quite nonstandard in Python.  See
+    http://sebulba.wikispaces.com/recipe+thread2 for details of how this is
+    done.  However it does not seem to work very well for now...
+    """
+    def __init__(self, func, params, context, wkbench):
+        def work():
+            ## Call user-defined work function, and before quitting
+            ## notify that we are done
+            func(params, context)
+            wkbench.curworker = None
+
+        super(_WorkerThread,self).__init__(target=work)
+        self.context = context
+
+    def _get_my_tid(self):
+        """determines this (self's) thread id"""
+        if not self.isAlive():
+            raise threading.ThreadError("the thread is not active")
+        
+        # do we have it cached?
+        if hasattr(self, "_thread_id"):
+            return self._thread_id
+        
+        # no, look for it in the _active dict
+        for tid, tobj in threading._active.items():
+            if tobj is self:
+                self._thread_id = tid
+                return tid
+        
+        raise AssertionError("could not determine the thread's id")
+    
+    def raise_exc(self, exctype):
+        """raises the given exception type in the context of this thread"""
+        _async_raise(self._get_my_tid(), exctype)
+    
+    def terminate(self):
+        """raises SystemExit in the context of the given thread, which should 
+        cause the thread to exit silently (unless caught)"""
+        self.raise_exc(SystemExit)    
+
+
+#####  ExperimentWorkbench  #################################################
+
+class ExperimentWorkbench(HasTraits) :
+    """Manage the interface of a traits-based experiment.
+    """
+    ## Data traits
+    script_params = Instance(HasTraits, (), desc="Script Parameters")
+    experiments   = List(ExperimentContext, desc="Set of experiments",
+                         editor=ListEditor(use_notebook=True,
+                                           deletable=True,
+                                           dock_style="tab",
+                                           page_name="._display_expdir"))
+
+    expfunc   = Function(desc="Function to run when experiment is running")
+    curworker = Instance(threading.Thread, desc="Worker thread, if any is running")
+
+    ## Active traits
+    launch = Button("Launch Experiment")
+    cancel = Button("Cancel")
+
+    ## The main view
+    traits_view  = View(
+        HSplit(Group(Group(Item("script_params@", show_label=False, springy=True),
+                           springy=True),
+                     Group(Item("launch", springy=True, enabled_when="curworker is None"),
+                           Item("cancel", springy=True, enabled_when="curworker is not None"),
+                           show_labels=False, orientation="horizontal")),
+               Group(Item("experiments@", dock="fixed", show_label=False, width=300),
+                     springy=True)),
+        resizable=True,
+        height=0.75, width=0.75,
+        buttons=NoButtons)
+
+    ## Experiment management
+    def _launch_fired(self):
+        expdir  = self.expdir_name(self.script_params.expdir_root)
+        logger  = ConsoleLogger()
+        context = ExperimentContext(expdir    = expdir,
+                                    _all_tabs = [ logger ])
+        self.experiments.append(context)
+        self.curworker = _WorkerThread(self.expfunc, self.script_params, context, self)
+        self.curworker.start()
+
+    def _cancel_fired(self):
+        if self.curworker is not None:
+            self.curworker.terminate()
+            self.curworker.join()
+            self.curworker = None
+
+    def _curworker_changed(self, old, new):
+        ## If curworker had an active console logger, disable it
+        if old is not None:
+            context = old.context
+            logger  = context.console_logger
+            if logger is not None:
+                logger.desactivate_stdout_err_redirect()
+
+        ## And redirect output to the new logger...
+        if new is not None:
+            context = new.context
+            logger = context.console_logger
+            if logger is not None:
+                logger.activate_stdouterr_redirect()
+
+    
+    def run(self, params, func, gui="__AUTO__"):
+        """Bind the command-line arguments to the params, show the gui (if
+        requested) and run the experiment by calling 'func' in a separate
+        thread.
+
+        Arguments:
+
+        - params: Either a class or instance inheriting from HasTraits
+          and specifying the parameters (hierarchically) for the experiment.
+
+        - func: Function to be called to run the experiment.  The function
+          is called with two arguments: a filled-out params instance
+          containing the experiment parameters, and an ExperimentContext
+          instance giving, among other things, an experiment directory and
+          facility to create Matplotlib figures to be added to the workbench.
+
+        - gui: Either a boolean, or if '__AUTO__', taken from the
+          command-line switch --no-gui or --gui.
+        """
+
+        ## Instantiate the params container if a class was passed
+        if isinstance(params,type):
+            params = params()
+        assert isinstance(params, HasTraits), \
+               "The 'params' argument must be an instance or a class inheriting from HasTraits."
+
+        ## Determine whether a GUI should be used
+        if gui == "__AUTO__":
+            if "--no-gui" in sys.argv:
+                gui = False
+            else:
+                gui = True
+
+        ## Bind the command-line arguments to the parameters
+        self.script_params = self.bind(params, sys.argv)
+        self.expfunc = func
+
+        ## Run the thing
+        self.configure_traits()
+
+
+    @staticmethod
+    def bind(params, argv):
+        """Bind any arguments in argv that does not start with a '-' to
+        elements in params and has the form 'K=V'.  This assumes that
+        'params' (and the classes contained therin) inherits from
+        HasStrictTraits so that any assignment to inexistant options raises
+        an exception.
+        """
+        for arg in argv:
+            if arg.startswith('-'):
+                continue
+            if '=' in arg:
+                (k,v) = arg.split('=')
+                exec("params.%s = %s" % (k,v))
+
+        return params
+
+
+    @staticmethod
+    def expdir_name(expdir_root):
+        return os.path.join(expdir_root,
+                            datetime.now().strftime("expdir_%Y%m%d_%H%m%S"))
+
+
+#####  Utilities  ###########################################################
+
+def _async_raise(tid, exctype):
+    """raises the exception, performs cleanup if needed"""
+    if not inspect.isclass(exctype):
+        raise TypeError("Only types can be raised (not instances)")
+    res = ctypes.pythonapi.PyThreadState_SetAsyncExc(tid, ctypes.py_object(exctype))
+    if res == 0:
+        raise ValueError("invalid thread id")
+    elif res != 1:
+        # """if it returns a number greater than one, you're in trouble, 
+        # and you should call it again with exc=NULL to revert the effect"""
+        ctypes.pythonapi.PyThreadState_SetAsyncExc(tid, 0)
+        raise SystemError("PyThreadState_SetAsyncExc failed")
+
+
+
+#####  Test Case  ###########################################################
+
+if __name__ == "__main__":
+    class GlobalOpt(HasStrictTraits):
+        expdir_root       = Directory(".", auto_set=True,
+                                         desc="Where the experiment directory should be created")
+        max_train_size    = Trait( -1 ,  desc="maximum size of training set (in days)")
+        nhidden           = Trait(3,     desc="number of hidden units")
+        weight_decay      = Trait(1e-8,  desc="weight decay to use for neural-net training")
+
+    class MinorOpt(HasStrictTraits):
+        earlystop_fraction  = Trait(  0.0, Desc="fraction of training set to use for early stopping")
+        earlystop_check     = Trait(    1, Desc="check early-stopping criterion every N epochs")
+        earlystop_minstage  = Trait(    1, Desc="minimum optimization stage after which early-stopping can kick in")
+
+    class AllOpt(HasStrictTraits):
+        expdir_root = Delegate("GlobalOpt")
+        GlobalOpt   = Instance(GlobalOpt, ())
+        MinorOpt    = Instance(MinorOpt,  ())
+
+        traits_view = View(
+            Group(Item("GlobalOpt@"),
+                  Item("MinorOpt@" ),
+                  show_labels=False, layout="tabbed"))
+
+    def f(params, context):
+        print "Now running a very complex experiment"
+        print "params.GlobalOpt.nhidden = ", params.GlobalOpt.nhidden
+        print "context.expdir           = ", context.expdir
+
+        from numpy import sin, cos, linspace, pi
+        print "Drawing a figure..."
+        f = context.Figure()
+        axes = f.add_subplot(111)
+        t = linspace(0, 2*pi, 200)
+        axes.plot(sin(t)*(1+0.5*cos(11*t)), cos(t)*(1+0.5*cos(11*t)))
+
+        print "Sleeping during a long computation..."
+        sys.stdout.flush()
+        try:
+            import time
+            time.sleep(10)
+        finally:
+            print "Done."
+
+    ExperimentWorkbench().run(AllOpt, f)
+    



From larocheh at mail.berlios.de  Wed Jun 11 22:56:32 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 11 Jun 2008 22:56:32 +0200
Subject: [Plearn-commits] r9120 - trunk/plearn_learners/online
Message-ID: <200806112056.m5BKuWX1010064@sheep.berlios.de>

Author: larocheh
Date: 2008-06-11 22:56:31 +0200 (Wed, 11 Jun 2008)
New Revision: 9120

Modified:
   trunk/plearn_learners/online/RBMMixedLayer.cc
   trunk/plearn_learners/online/RBMMixedLayer.h
   trunk/plearn_learners/online/RBMMultinomialLayer.cc
   trunk/plearn_learners/online/RBMMultinomialLayer.h
Log:
Coded methods for free energy computations and gradient descent...


Modified: trunk/plearn_learners/online/RBMMixedLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.cc	2008-06-11 18:34:17 UTC (rev 9119)
+++ trunk/plearn_learners/online/RBMMixedLayer.cc	2008-06-11 20:56:31 UTC (rev 9120)
@@ -717,6 +717,39 @@
     return energy;
 }
 
+real RBMMixedLayer::freeEnergyContribution(const Vec& unit_activations)
+    const
+{
+    real freeEnergy = 0;
+
+    Vec act;
+    for ( int i = 0; i < n_layers; ++i ) {
+        int begin = init_positions[i];
+        int size_i = sub_layers[i]->size;
+        act = unit_activations.subVec( begin, size_i );
+        freeEnergy += sub_layers[i]->freeEnergyContribution(act);
+    }
+
+    return freeEnergy;
+}
+
+void RBMMixedLayer::freeEnergyContributionGradient(
+    const Vec& unit_activations,
+    Vec& unit_activations_gradient,
+    real output_gradient, bool accumulate) const
+{
+    Vec act;
+    Vec gact;
+    for ( int i = 0; i < n_layers; ++i ) {
+        int begin = init_positions[i];
+        int size_i = sub_layers[i]->size;
+        act = unit_activations.subVec( begin, size_i );
+        gact = unit_activations_gradient.subVec( begin, size_i );
+        sub_layers[i]->freeEnergyContributionGradient(
+            act, gact, output_gradient, accumulate);
+    }
+}
+
 int RBMMixedLayer::getConfigurationCount()
 {
     int count = 1;

Modified: trunk/plearn_learners/online/RBMMixedLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.h	2008-06-11 18:34:17 UTC (rev 9119)
+++ trunk/plearn_learners/online/RBMMixedLayer.h	2008-06-11 20:56:31 UTC (rev 9120)
@@ -183,6 +183,21 @@
     //! Compute -bias' unit_values
     virtual real energy(const Vec& unit_values) const;
 
+    //! Computes -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! This quantity is used for computing the free energy of a sample x in
+    //! the OTHER layer of an RBM, from which unit_activations was computed.
+    virtual real freeEnergyContribution(const Vec& unit_activations) const;
+
+    //! Computes gradient of the result of freeEnergyContribution
+    //! -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! with respect to unit_activations. Optionally, a gradient
+    //! with respect to freeEnergyContribution can be given
+    virtual void freeEnergyContributionGradient(const Vec& unit_activations,
+                                                Vec& unit_activations_gradient,
+                                                real output_gradient = 1,
+                                                bool accumulate = false) 
+        const;
+
     virtual int getConfigurationCount();
 
     virtual void getConfiguration(int conf_index, Vec& output);

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.cc	2008-06-11 18:34:17 UTC (rev 9119)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.cc	2008-06-11 20:56:31 UTC (rev 9120)
@@ -426,6 +426,7 @@
 void RBMMultinomialLayer::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
+    deepCopyField(tmp_softmax, copies);
 }
 
 real RBMMultinomialLayer::energy(const Vec& unit_values) const
@@ -440,6 +441,22 @@
     return -logadd(unit_activations);
 }
 
+void RBMMultinomialLayer::freeEnergyContributionGradient(
+    const Vec& unit_activations,
+    Vec& unit_activations_gradient,
+    real output_gradient, bool accumulate) const
+{
+    PLASSERT( unit_activations.size() == size );
+    unit_activations_gradient.resize( size );
+    if( !accumulate ) unit_activations_gradient.clear();
+    tmp_softmax.resize( size );
+    softmax(unit_activations, tmp_softmax);
+    real* ga = unit_activations_gradient.data();
+    real* s = tmp_softmax.data();
+    for (int i=0; i<size; i++)
+        ga[i] -= output_gradient * s[i];
+}
+
 int RBMMultinomialLayer::getConfigurationCount()
 {
     return size;

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.h	2008-06-11 18:34:17 UTC (rev 9119)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.h	2008-06-11 20:56:31 UTC (rev 9120)
@@ -124,6 +124,17 @@
     //! the OTHER layer of an RBM, from which unit_activations was computed.
     virtual real freeEnergyContribution(const Vec& unit_activations) const;
 
+    //! Computes gradient of the result of freeEnergyContribution
+    //! -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! with respect to unit_activations. Optionally, a gradient
+    //! with respect to freeEnergyContribution can be given
+    virtual void freeEnergyContributionGradient(const Vec& unit_activations,
+                                                Vec& unit_activations_gradient,
+                                                real output_gradient = 1,
+                                                bool accumulate = false) 
+        const;
+
+
     virtual int getConfigurationCount();
 
     virtual void getConfiguration(int conf_index, Vec& output);
@@ -141,6 +152,7 @@
 
 protected:
     //#####  Not Options  #####################################################
+    mutable Vec tmp_softmax;
 
 protected:
     //#####  Protected Member Functions  ######################################



From louradou at mail.berlios.de  Wed Jun 11 23:55:23 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Wed, 11 Jun 2008 23:55:23 +0200
Subject: [Plearn-commits] r9121 - in trunk: commands plearn/ker
	plearn_learners/online
Message-ID: <200806112155.m5BLtNUr018071@sheep.berlios.de>

Author: louradou
Date: 2008-06-11 23:55:21 +0200 (Wed, 11 Jun 2008)
New Revision: 9121

Added:
   trunk/plearn/ker/CosKernel.cc
   trunk/plearn/ker/CosKernel.h
   trunk/plearn_learners/online/ShuntingNNetLayerModule.cc
   trunk/plearn_learners/online/ShuntingNNetLayerModule.h
Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
- added Cosinus kernel (distance kernel, good with kNN)
- addded Shunting Layer in online



Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2008-06-11 20:56:31 UTC (rev 9120)
+++ trunk/commands/plearn_noblas_inc.h	2008-06-11 21:55:21 UTC (rev 9121)
@@ -107,6 +107,7 @@
  **********/
 #include <plearn/ker/AdditiveNormalizationKernel.h>
 #include <plearn/ker/BetaKernel.h>
+#include <plearn/ker/CosKernel.h>
 #include <plearn/ker/DistanceKernel.h>
 #include <plearn/ker/DotProductKernel.h>
 #include <plearn/ker/EpanechnikovKernel.h>
@@ -242,9 +243,11 @@
 #include <plearn_learners/online/RBMMixedLayer.h>
 #include <plearn_learners/online/RBMModule.h>
 #include <plearn_learners/online/RBMMultinomialLayer.h>
+#include <plearn_learners/online/RBMSparse1DMatrixConnection.h>
 #include <plearn_learners/online/RBMTrainer.h>
 #include <plearn_learners/online/RBMTruncExpLayer.h>
 #include <plearn_learners/online/ScaleGradientModule.h>
+#include <plearn_learners/online/ShuntingNNetLayerModule.h>
 #include <plearn_learners/online/SoftmaxModule.h>
 #include <plearn_learners/online/SplitModule.h>
 #include <plearn_learners/online/SquaredErrorCostModule.h>

Added: trunk/plearn/ker/CosKernel.cc
===================================================================
--- trunk/plearn/ker/CosKernel.cc	2008-06-11 20:56:31 UTC (rev 9120)
+++ trunk/plearn/ker/CosKernel.cc	2008-06-11 21:55:21 UTC (rev 9121)
@@ -0,0 +1,120 @@
+// -*- C++ -*-
+
+// PLearn (A C++ Machine Learning Library)
+// Copyright (C) 2007 Jerome Louradour
+
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* *******************************************************      
+ * $Id: CosKernel.cc 7675 2007-06-29 19:50:49Z tihocan $
+ * This file is part of the PLearn library.
+ ******************************************************* */
+
+#include "CosKernel.h"
+
+namespace PLearn {
+using namespace std;
+
+
+PLEARN_IMPLEMENT_OBJECT(
+    CosKernel,
+    "Implements a distance based on a cosinus.",
+    "Output k(x,y)=(0.5 - 0.5*cos(x,y)) (between 0 and 1)\n");
+
+////////////////////
+// CosKernel //
+////////////////////
+CosKernel::CosKernel()
+{
+    n=2.0;
+    optimized=false;
+    pow_distance=false;
+    ignore_missing=false;
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void CosKernel::declareOptions(OptionList& ol)
+{
+    inherited::declareOptions(ol);
+
+    redeclareOption(ol, "n", &CosKernel::n, OptionBase::nosave, 
+                  "Obsolete option for cosinus kernel.");
+
+    redeclareOption(ol, "pow_distance", &CosKernel::pow_distance, OptionBase::nosave, 
+                  "Obsolete option for cosinus kernel.");
+
+    redeclareOption(ol, "optimized", &CosKernel::optimized, OptionBase::nosave, 
+                  "Obsolete option for cosinus kernel.");
+
+    redeclareOption(ol, "ignore_missing", &CosKernel::ignore_missing, OptionBase::nosave, 
+                  "Obsolete option for cosinus kernel.");
+}
+
+//////////////
+// evaluate //
+//////////////
+real CosKernel::evaluate(const Vec& x1, const Vec& x2) const {
+    if (ignore_missing && !pow_distance)
+        PLERROR("In CosKernel::evaluate(int i, int j) - 'ignore_missing' "
+                "implemented only if pow_distance is set");
+    static real cosinus;
+    cosinus = dot(x1, x2) / ( norm(x1) * norm(x2) );
+    return (1.-cosinus)*.5;
+}
+
+//////////////////
+// evaluate_i_j //
+//////////////////
+real CosKernel::evaluate_i_j(int i, int j) const {
+    static real cosinus;
+    if (i == j)
+        // The case 'i == j' can cause precision issues because of the optimized
+        // formula below. Thus we make sure we always return 0.
+        return 0;
+    cosinus = data->dot(i, j, data_inputsize) / sqrt(squarednorms[i] * squarednorms[j]);
+    return (1.-cosinus)*.5;
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/ker/CosKernel.h
===================================================================
--- trunk/plearn/ker/CosKernel.h	2008-06-11 20:56:31 UTC (rev 9120)
+++ trunk/plearn/ker/CosKernel.h	2008-06-11 21:55:21 UTC (rev 9121)
@@ -0,0 +1,89 @@
+// -*- C++ -*-
+
+// PLearn (A C++ Machine Learning Library)
+// Copyright (C) 2007 Jerome Louradour
+
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* *******************************************************      
+ * $Id: CosKernel.h 7664 2007-06-28 19:47:30Z nouiz $
+ * This file is part of the PLearn library.
+ ******************************************************* */
+
+#ifndef CosKernel_INC
+#define CosKernel_INC
+
+#include "DistanceKernel.h"
+
+namespace PLearn {
+using namespace std;
+
+//! This class implements an Ln distance (defaults to L2 i.e. euclidean distance).
+class CosKernel: public DistanceKernel
+{
+
+private:
+
+    typedef DistanceKernel inherited;
+
+protected:
+
+public:
+
+    CosKernel();
+    
+    PLEARN_DECLARE_OBJECT(CosKernel);
+
+    virtual real evaluate(const Vec& x1, const Vec& x2) const;
+    virtual real evaluate_i_j(int i, int j) const;
+
+
+protected:
+    static void declareOptions(OptionList& ol);
+};
+
+DECLARE_OBJECT_PTR(CosKernel);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/ShuntingNNetLayerModule.cc
===================================================================
--- trunk/plearn_learners/online/ShuntingNNetLayerModule.cc	2008-06-11 20:56:31 UTC (rev 9120)
+++ trunk/plearn_learners/online/ShuntingNNetLayerModule.cc	2008-06-11 21:55:21 UTC (rev 9121)
@@ -0,0 +1,542 @@
+// -*- C++ -*-
+
+// ShuntingNNetLayerModule.cc
+//
+// Copyright (C) 2008 Jerome Louradour
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+/* *******************************************************
+   * $Id: ShuntingNNetLayerModule.cc,v 1.3 2006/01/18 04:04:06 lamblinp Exp $
+   ******************************************************* */
+
+// Authors: Jerome Louradour
+
+/*! \file ShuntingNNetLayerModule.cc */
+
+
+#include "ShuntingNNetLayerModule.h"
+#include <plearn/math/TMat_maths.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    ShuntingNNetLayerModule,
+    "Affine transformation module, with stochastic gradient descent updates",
+    "Neural Network layer, using stochastic gradient to update neuron weights\n"
+    "       Output = weights * Input + bias\n"
+    "Weights and bias are updated by online gradient descent, with learning\n"
+    "rate possibly decreasing in 1/(1 + n_updates_done * decrease_constant).\n"
+    "An L1 and L2 regularization penalty can be added to push weights to 0.\n"
+    "Weights can be initialized to 0, to a given initial matrix, or randomly\n"
+    "from a uniform distribution.\n"
+    );
+
+/////////////////////////
+// ShuntingNNetLayerModule //
+/////////////////////////
+ShuntingNNetLayerModule::ShuntingNNetLayerModule():
+    start_learning_rate( .001 ),
+    decrease_constant( 0. ),
+    init_weights_random_scale( 1. ),
+    init_quad_weights_random_scale( 1. ),
+    n_filters( 1 ),
+    n_filters_inhib( -1 ),
+    step_number( 0 )
+{}
+
+////////////////////
+// declareOptions //
+////////////////////
+
+void ShuntingNNetLayerModule::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "start_learning_rate",
+                  &ShuntingNNetLayerModule::start_learning_rate,
+                  OptionBase::buildoption,
+                  "Learning-rate of stochastic gradient optimization");
+
+    declareOption(ol, "decrease_constant",
+                  &ShuntingNNetLayerModule::decrease_constant,
+                  OptionBase::buildoption,
+                  "Decrease constant of stochastic gradient optimization");
+
+    declareOption(ol, "init_weights_random_scale",
+                  &ShuntingNNetLayerModule::init_weights_random_scale,
+                  OptionBase::buildoption,
+                  "Weights of the excitation (softplus part) are initialized randomly\n"
+                  "from a uniform in [-r,r], with r = init_weights_random_scale/input_size.\n"
+                  "To clear the weights initially, just set this option to 0.");
+                  
+    declareOption(ol, "init_quad_weights_random_scale",
+                  &ShuntingNNetLayerModule::init_quad_weights_random_scale,
+                  OptionBase::buildoption,
+                  "Weights of the quadratic part (of excitation, as well as inhibition) are initialized randomly\n"
+                  "from a uniform in [-r,r], with r = init_weights_random_scale/input_size.\n"
+                  "To clear the weights initially, just set this option to 0.");
+                  
+    declareOption(ol, "n_filters",
+                  &ShuntingNNetLayerModule::n_filters,
+                  OptionBase::buildoption,
+                  "Number of synapses per neuron for excitation.\n");
+
+    declareOption(ol, "n_filters_inhib",
+                  &ShuntingNNetLayerModule::n_filters_inhib,
+                  OptionBase::buildoption,
+                  "Number of synapses per neuron for inhibition.\n"
+                  "Must be lower or equal to n_filters in the current implementation (!).\n"
+                  "If -1, then it is taken equal to n_filters.");
+
+    declareOption(ol, "excit_quad_weights", &ShuntingNNetLayerModule::excit_quad_weights,
+                  OptionBase::learntoption,
+                  "List of weights vectors of the neurons"
+                  "contributing to the excitation -- quadratic part)");
+
+    declareOption(ol, "inhib_quad_weights", &ShuntingNNetLayerModule::inhib_quad_weights,
+                  OptionBase::learntoption,
+                  "List of weights vectors of the neurons (inhibation -- quadratic part)\n");
+
+    declareOption(ol, "excit_weights", &ShuntingNNetLayerModule::excit_weights,
+                  OptionBase::learntoption,
+                  "Input weights vectors of the neurons (excitation -- softplus part)\n");
+
+    declareOption(ol, "bias", &ShuntingNNetLayerModule::bias,
+                  OptionBase::learntoption,
+                  "Bias of the neurons (in the softplus of the excitations)\n");
+
+    declareOption(ol, "excit_num_coeff", &ShuntingNNetLayerModule::excit_num_coeff,
+                  OptionBase::learntoption,
+                  "Multiplicative Coefficient applied on the excitation\n"
+                  "in the numerator of the activation closed form.\n");
+
+    declareOption(ol, "inhib_num_coeff", &ShuntingNNetLayerModule::inhib_num_coeff,
+                  OptionBase::learntoption,
+                  "Multiplicative Coefficient applied on the inhibition\n"
+                  "in the numerator of the activation closed form.\n");
+
+    inherited::declareOptions(ol);
+}
+///////////
+// build //
+///////////
+
+void ShuntingNNetLayerModule::build_()
+{
+    if( input_size < 0 ) // has not been initialized
+        return;
+
+    if( output_size < 0 )
+        PLERROR("ShuntingNNetLayerModule::build_: 'output_size' is < 0 (%i),\n"
+                " you should set it to a positive integer (the number of"
+                " neurons).\n", output_size);
+
+    if (n_filters_inhib < 0)
+        n_filters_inhib= n_filters;
+    PLASSERT( n_filters>0 );
+    
+    if(    excit_quad_weights.length() != n_filters
+        || inhib_quad_weights.length() != n_filters_inhib
+        || excit_weights.length() != output_size
+        || excit_weights.width() != input_size
+        || bias.size() != output_size )
+    {
+        forget();
+    }
+}
+void ShuntingNNetLayerModule::build()
+{
+    inherited::build();
+    build_();
+}
+
+////////////
+// forget //
+////////////
+
+void ShuntingNNetLayerModule::forget()
+{
+    learning_rate = start_learning_rate;
+    step_number = 0;
+
+    bias.resize( output_size );
+    bias.clear();
+    
+    excit_num_coeff.resize( output_size );
+    inhib_num_coeff.resize( output_size );
+    excit_num_coeff.fill(1.);
+    inhib_num_coeff.fill(1.);
+
+    excit_weights.resize( output_size, input_size );
+    excit_quad_weights.resize( n_filters );
+    PLASSERT( n_filters_inhib >= 0 && n_filters_inhib <= n_filters );
+    inhib_quad_weights.resize( n_filters_inhib );
+    
+    if( !random_gen )
+    {
+        PLWARNING( "ShuntingNNetLayerModule: cannot forget() without random_gen" );
+        return;
+    }
+    
+    real r = init_weights_random_scale / (real)input_size;
+    if( r > 0. )
+        random_gen->fill_random_uniform(excit_weights, -r, r);
+    else
+        excit_weights.clear();
+      
+    r = init_quad_weights_random_scale / (real)input_size;    
+    if( r > 0. )
+        for( int k = 0; k < n_filters; k++ )
+        {
+            excit_quad_weights[k].resize( output_size, input_size );
+            random_gen->fill_random_uniform(excit_quad_weights[k], -r, r);
+            if ( k < n_filters_inhib ) {
+                inhib_quad_weights[k].resize( output_size, input_size );
+                random_gen->fill_random_uniform(inhib_quad_weights[k], -r, r);
+            }
+        }
+    else
+        for( int k = 0; k < n_filters; k++ )
+        {
+            excit_quad_weights[k].resize(output_size, input_size );
+            excit_quad_weights[k].clear();
+            if ( k < n_filters_inhib ) {
+                inhib_quad_weights[k].resize(output_size, input_size );
+                inhib_quad_weights[k].clear();
+            }
+        }
+}
+
+///////////
+// fprop //
+///////////
+
+void ShuntingNNetLayerModule::fprop(const Vec& input, Vec& output) const
+{
+    PLASSERT_MSG( input.size() == input_size,
+                  "input.size() should be equal to this->input_size" );
+
+    output.resize( output_size );
+
+    if( during_training )
+    {
+        batch_excitations.resize(1, output_size);
+        batch_inhibitions.resize(1, output_size);
+    }
+//    if( use_fast_approximations )
+
+        for( int i = 0; i < output_size; i++ )
+        {
+            real excitation = 0.;
+            real inhibition = 0.;
+            for ( int k=0; k < n_filters; k++ )
+            {
+                excitation += square( dot( excit_quad_weights[k](i), input ) );
+                if ( k < n_filters_inhib )
+                    inhibition += square( dot( inhib_quad_weights[k](i), input ) );
+            }
+            excitation = sqrt( excitation + tabulated_softplus( dot( excit_weights(i), input ) + bias[i] ) );
+            inhibition = sqrt( inhibition );
+            if( during_training )
+            {
+                    batch_excitations(0,i) = excitation;
+                    batch_inhibitions(0,i) = inhibition;
+            }
+
+            output[i] = ( excit_num_coeff[i]* excitation - inhib_num_coeff[i]* inhibition ) /
+                        (1. + excitation + inhibition );
+        }
+//    else
+}
+
+void ShuntingNNetLayerModule::fprop(const Mat& inputs, Mat& outputs)
+{
+    PLASSERT( inputs.width() == input_size );
+    int n = inputs.length();
+    outputs.resize(n, output_size);
+    
+
+    Mat excitations_part2(n, output_size);
+    excitations_part2.clear();
+    productTranspose(excitations_part2, inputs, excit_weights);
+    resizeOnes(n);
+    externalProductAcc(excitations_part2, ones, bias);
+
+    Mat excitations(n, output_size), inhibitions(n, output_size);
+    excitations.clear();
+    inhibitions.clear();
+
+        for ( int k=0; k < n_filters; k++ )
+        {
+            Mat tmp_sample_output(n, output_size);
+
+            tmp_sample_output.clear();
+            productTranspose(tmp_sample_output, inputs, excit_quad_weights[k]);
+            squareElements(tmp_sample_output);
+            multiplyAcc(excitations, tmp_sample_output, 1.);
+
+            if ( k < n_filters_inhib ) {
+                tmp_sample_output.clear();
+                productTranspose(tmp_sample_output, inputs, inhib_quad_weights[k]);
+                squareElements(tmp_sample_output);
+                multiplyAcc(inhibitions, tmp_sample_output, 1.);
+            }
+        }
+        for( int i_sample = 0; i_sample < n; i_sample ++)
+        {
+            for( int i = 0; i < output_size; i++ )
+            {
+                excitations(i_sample,i) = sqrt( excitations(i_sample,i) + tabulated_softplus( excitations_part2(i_sample,i) ) );
+                inhibitions(i_sample,i) = sqrt( inhibitions(i_sample,i) );
+
+                real E = excitations(i_sample,i);
+                real S = inhibitions(i_sample,i);
+                    
+                outputs(i_sample,i) = ( excit_num_coeff[i]* E - inhib_num_coeff[i]* S ) /
+                                       (1. + E + S );
+            }
+        }
+
+    if( during_training )
+    {
+        batch_excitations.resize(n, output_size);
+        batch_inhibitions.resize(n, output_size);
+        batch_excitations << excitations;
+        batch_inhibitions << inhibitions;
+    }
+}
+
+/////////////////
+// bpropUpdate //
+/////////////////
+
+void ShuntingNNetLayerModule::bpropUpdate(const Vec& input, const Vec& output,
+                                      const Vec& output_gradient)
+{
+    learning_rate = start_learning_rate / (1+decrease_constant*step_number);
+
+    for( int i=0; i<output_size; i++ )
+    {
+        real tmp = square(1 + batch_excitations(0,i) + batch_inhibitions(0,i) );
+        
+        real Dactivation_Dexcit =   ( excit_num_coeff[i]  +  batch_inhibitions(0,i)*(excit_num_coeff[i] + inhib_num_coeff[i]) ) / tmp;
+        real Dactivation_Dinhib = - ( inhib_num_coeff[i]  +  batch_excitations(0,i)*(excit_num_coeff[i] + inhib_num_coeff[i]) ) / tmp;
+
+        real lr_og_excit = learning_rate * output_gradient[i];
+        PLASSERT( batch_excitations(0,i)>0. );
+        PLASSERT( batch_inhibitions(0,i)>0. );
+        real lr_og_inhib = lr_og_excit * Dactivation_Dinhib / batch_inhibitions(0,i);
+        lr_og_excit *= Dactivation_Dexcit / batch_excitations(0,i);
+        
+        tmp = lr_og_excit * sigmoid( dot( excit_weights(i), input ) + bias[i] ) * .5;
+
+        bias[i] -= tmp;
+        multiplyAcc( excit_weights(i), input, -tmp);
+
+        for( int k = 0; k < n_filters; k++ )
+        {
+            real tmp_excit2 = lr_og_excit * dot( excit_quad_weights[k](i), input );
+            real tmp_inhib2 = 0;
+            if (k < n_filters_inhib)
+                tmp_inhib2 = lr_og_inhib * dot( inhib_quad_weights[k](i), input );
+            for( int j=0; j<input_size; j++ )
+            {
+                excit_quad_weights[k](i,j) -= tmp_excit2 * input[j];
+                if (k < n_filters_inhib)
+                    inhib_quad_weights[k](i,j) -= tmp_inhib2 * input[j];
+            }   
+        }
+    }
+
+    step_number++;
+}
+
+void ShuntingNNetLayerModule::bpropUpdate(const Mat& inputs, const Mat& outputs,
+        Mat& input_gradients,
+        const Mat& output_gradients,
+        bool accumulate)
+{
+    PLASSERT( inputs.width() == input_size );
+    PLASSERT( outputs.width() == output_size );
+    PLASSERT( output_gradients.width() == output_size );
+
+    //fprop(inputs);
+
+    int n = inputs.length();
+
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradients.width() == input_size &&
+                input_gradients.length() == n,
+                "Cannot resize input_gradients and accumulate into it" );
+    }
+    else
+    {
+        input_gradients.resize(n, input_size);
+        input_gradients.fill(0);
+    }
+
+    learning_rate = start_learning_rate / (1+decrease_constant*step_number);
+    real avg_lr = learning_rate / n; // To obtain an average on a mini-batch.
+
+    if ( avg_lr == 0. )
+        return ; 
+
+        Mat tmp(n, output_size);
+        // tmp = (1 + E + S ).^2;
+        tmp.fill(1.);
+        multiplyAcc(tmp, batch_excitations, 1);
+        multiplyAcc(tmp, batch_inhibitions, 1);
+        squareElements(tmp);
+        
+        Vec bias_updates(output_size);
+        Mat excit_weights_updates( output_size, input_size);
+        TVec<Mat> excit_quad_weights_updates(n_filters);
+        TVec<Mat> inhib_quad_weights_updates(n_filters_inhib);
+        // Initialisation 
+        bias_updates.clear();
+        excit_weights_updates.clear();
+        for( int k=0; k < n_filters; k++ )
+        {
+            excit_quad_weights_updates[k].resize( output_size, input_size);
+            excit_quad_weights_updates[k].clear();
+            if (k < n_filters_inhib) {
+                inhib_quad_weights_updates[k].resize( output_size, input_size);
+                inhib_quad_weights_updates[k].clear();
+            }
+        }
+
+        for( int i_sample = 0; i_sample < n; i_sample++ )
+        for( int i=0; i<output_size; i++ )
+        {
+            real Dactivation_Dexcit =   ( excit_num_coeff[i]  +  batch_inhibitions(i_sample,i)*(excit_num_coeff[i] + inhib_num_coeff[i]) ) / tmp(i_sample,i);
+            real Dactivation_Dinhib = - ( inhib_num_coeff[i]  +  batch_excitations(i_sample,i)*(excit_num_coeff[i] + inhib_num_coeff[i]) ) / tmp(i_sample,i);
+            
+            real lr_og_excit = avg_lr * output_gradients(i_sample,i);
+            PLASSERT( batch_excitations(i_sample,i)>0. );
+            PLASSERT( n_filters_inhib==0 || batch_inhibitions(i_sample,i)>0. );
+            real lr_og_inhib = lr_og_excit * Dactivation_Dinhib / batch_inhibitions(i_sample,i);
+            lr_og_excit *= Dactivation_Dexcit / batch_excitations(i_sample,i);
+                
+            real tmp2 = lr_og_excit * sigmoid( dot( excit_weights(i), inputs(i_sample) ) + bias[i] ) * .5;
+
+            bias_updates[i] -= tmp2;
+            multiplyAcc( excit_weights_updates(i), inputs(i_sample), -tmp2);
+
+            for( int k = 0; k < n_filters; k++ )
+            {
+                real tmp_excit2 = lr_og_excit   * dot( excit_quad_weights[k](i), inputs(i_sample) );
+                real tmp_inhib2 = 0;
+                if (k < n_filters_inhib)
+                    tmp_inhib2 = lr_og_inhib   * dot( inhib_quad_weights[k](i), inputs(i_sample) );
+                //for( int j=0; j<input_size; j++ )
+                //{
+                //    excit_quad_weights_updates[k](i,j) -= tmp_excit2 * inputs(i_sample,j);
+                //    if (k < n_filters_inhib)
+                //        inhib_quad_weights_updates[k](i,j) -= tmp_inhib2 * inputs(i_sample,j);
+                //}
+                multiplyAcc( excit_quad_weights_updates[k](i), inputs(i_sample), -tmp_excit2);
+                if (k < n_filters_inhib)
+                    multiplyAcc( inhib_quad_weights_updates[k](i), inputs(i_sample), -tmp_inhib2);
+            }
+        }
+
+        multiplyAcc( bias, bias_updates, 1.);
+        multiplyAcc( excit_weights, excit_weights_updates, 1.);
+        for( int k = 0; k < n_filters; k++ )
+        {
+            multiplyAcc( excit_quad_weights[k], excit_quad_weights_updates[k], 1.);
+            if (k < n_filters_inhib)
+                multiplyAcc( inhib_quad_weights[k], inhib_quad_weights_updates[k], 1.);
+        }
+        batch_excitations.clear();
+        batch_inhibitions.clear();
+
+    step_number += n;
+}
+
+
+
+
+void ShuntingNNetLayerModule::setLearningRate( real dynamic_learning_rate )
+{
+    start_learning_rate = dynamic_learning_rate;
+    step_number = 0;
+    // learning_rate will automatically be set in bpropUpdate()
+}
+
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+
+void ShuntingNNetLayerModule::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(excit_weights,            copies);
+    deepCopyField(excit_quad_weights,       copies);
+    deepCopyField(inhib_quad_weights,       copies);
+    deepCopyField(bias,                     copies);
+    deepCopyField(excit_num_coeff,          copies);
+    deepCopyField(inhib_num_coeff,          copies);
+    deepCopyField(ones,                     copies);
+}
+
+
+
+
+////////////////
+// resizeOnes //
+////////////////
+void ShuntingNNetLayerModule::resizeOnes(int n) const
+{
+    if (ones.length() < n) {
+        ones.resize(n);
+        ones.fill(1);
+    } else if (ones.length() > n)
+        ones.resize(n);
+}
+
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/ShuntingNNetLayerModule.h
===================================================================
--- trunk/plearn_learners/online/ShuntingNNetLayerModule.h	2008-06-11 20:56:31 UTC (rev 9120)
+++ trunk/plearn_learners/online/ShuntingNNetLayerModule.h	2008-06-11 21:55:21 UTC (rev 9121)
@@ -0,0 +1,189 @@
+// -*- C++ -*-
+
+// ShuntingNNetLayerModule.h
+//
+// Copyright (C) 2008 Jerome Louradour
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+/* *******************************************************
+   * $Id: ShuntingNNetLayerModule.h,v 1.3 2006/01/08 00:14:53 lamblinp Exp $
+   ******************************************************* */
+
+// Authors: Jerome Louradour
+
+/*! \file ShuntingNNetLayerModule.h */
+
+
+#ifndef ShuntingNNetLayerModule_INC
+#define ShuntingNNetLayerModule_INC
+
+#include <plearn/base/Object.h>
+#include <plearn/math/TMat_maths.h>
+#include "OnlineLearningModule.h"
+
+namespace PLearn {
+
+/**
+ * Affine transformation module, with stochastic gradient descent updates.
+ *
+ * Neural Network layer, using stochastic gradient to update neuron weights,
+ *      Output = weights * Input + bias
+ * Weights and bias are updated by online gradient descent, with learning
+ * rate possibly decreasing in 1/(1 + n_updates_done * decrease_constant).
+ * An L1 and L2 regularization penalty can be added to push weights to 0.
+ * Weights can be initialized to 0, to a given initial matrix, or randomly
+ * from a uniform distribution.
+ *
+ */
+class ShuntingNNetLayerModule : public OnlineLearningModule
+{
+    typedef OnlineLearningModule inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! Starting learning-rate, by which we multiply the gradient step
+    real start_learning_rate;
+
+    //! learning_rate = start_learning_rate / (1 + decrease_constant*t),
+    //! where t is the number of updates since the beginning
+    real decrease_constant;
+
+    //! If init_weights is not provided, the weights are initialized randomly
+    //! from a uniform in [-r,r], with r = init_weights_random_scale/input_size
+    real init_weights_random_scale;
+    real init_quad_weights_random_scale;
+
+    //! Number of excitation/inhibition quadratic weights
+    int n_filters;
+    int n_filters_inhib;
+
+    //! The weights, one neuron per line
+    TVec<Mat> excit_quad_weights;
+    TVec<Mat> inhib_quad_weights;
+    Mat excit_weights;
+    
+    //! The bias
+    Vec bias;
+
+    //! The multiplicative coefficients of excitation and inhibition
+    //! (in the numerator of the output activation)
+    Vec excit_num_coeff;
+    Vec inhib_num_coeff;
+
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    ShuntingNNetLayerModule();
+
+    // Your other public member functions go here
+
+    virtual void fprop(const Vec& input, Vec& output) const;
+
+    //! Overridden.
+    virtual void fprop(const Mat& inputs, Mat& outputs);
+
+    virtual void bpropUpdate(const Vec& input, const Vec& output,
+                             const Vec& output_gradient);
+
+    virtual void bpropUpdate(const Mat& inputs, const Mat& outputs,
+                             Mat& input_gradients,
+                             const Mat& output_gradients,
+                             bool accumulate = false);
+    
+    virtual void forget();
+
+    virtual void setLearningRate(real dynamic_learning_rate);
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(ShuntingNNetLayerModule);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+
+    //! A vector filled with all ones.
+    mutable Vec ones;
+    
+    mutable Mat batch_excitations;
+    mutable Mat batch_inhibitions;
+    
+    //#####  Protected Options  ###############################################
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+    //! Resize vector 'ones'.
+    void resizeOnes(int n) const;
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+    real learning_rate;
+    int step_number;
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(ShuntingNNetLayerModule);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From louradou at mail.berlios.de  Thu Jun 12 00:00:29 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Thu, 12 Jun 2008 00:00:29 +0200
Subject: [Plearn-commits] r9122 - trunk/plearn_learners/online
Message-ID: <200806112200.m5BM0Tx6018524@sheep.berlios.de>

Author: louradou
Date: 2008-06-12 00:00:28 +0200 (Thu, 12 Jun 2008)
New Revision: 9122

Modified:
   trunk/plearn_learners/online/RBMSparse1DMatrixConnection.cc
Log:
fixed a compiling error



Modified: trunk/plearn_learners/online/RBMSparse1DMatrixConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMSparse1DMatrixConnection.cc	2008-06-11 21:55:21 UTC (rev 9121)
+++ trunk/plearn_learners/online/RBMSparse1DMatrixConnection.cc	2008-06-11 22:00:28 UTC (rev 9122)
@@ -288,10 +288,10 @@
     Mat* up_grad = ports_gradient[1];
 
     PLASSERT( down && !down->isEmpty() );
-    PLASSERT( up && !up->isEmpty() );
+    //PLASSERT( up && !up->isEmpty() );
 
     int batch_size = down->length();
-    PLASSERT( up->length() == batch_size );
+    //PLASSERT( up->length() == batch_size );
 
     // If we have up_grad
     if( up_grad && !up_grad->isEmpty() )



From chapados at mail.berlios.de  Thu Jun 12 00:52:11 2008
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Thu, 12 Jun 2008 00:52:11 +0200
Subject: [Plearn-commits] r9123 - trunk/python_modules/plearn/gui_tools
Message-ID: <200806112252.m5BMqBYo020508@sheep.berlios.de>

Author: chapados
Date: 2008-06-12 00:52:10 +0200 (Thu, 12 Jun 2008)
New Revision: 9123

Modified:
   trunk/python_modules/plearn/gui_tools/console_logger.py
   trunk/python_modules/plearn/gui_tools/xp_workbench.py
Log:
Popup dialog if experiment is still running when closing the window

Modified: trunk/python_modules/plearn/gui_tools/console_logger.py
===================================================================
--- trunk/python_modules/plearn/gui_tools/console_logger.py	2008-06-11 22:00:28 UTC (rev 9122)
+++ trunk/python_modules/plearn/gui_tools/console_logger.py	2008-06-11 22:52:10 UTC (rev 9123)
@@ -137,6 +137,7 @@
                     select([],[],[],.1) # Give a little time for buffers to fill
 
         listener = _ListenerThread()
+        listener.setDaemon(True)        # Allow quitting Python even if thread running
         self.is_active = True
         listener.start()
         

Modified: trunk/python_modules/plearn/gui_tools/xp_workbench.py
===================================================================
--- trunk/python_modules/plearn/gui_tools/xp_workbench.py	2008-06-11 22:00:28 UTC (rev 9122)
+++ trunk/python_modules/plearn/gui_tools/xp_workbench.py	2008-06-11 22:52:10 UTC (rev 9123)
@@ -43,6 +43,7 @@
 from enthought.traits.api          import *
 from enthought.traits.ui.api       import *
 from enthought.traits.ui.menu      import NoButtons
+from enthought.pyface.api          import confirm, YES
 
 from console_logger    import ConsoleLogger
 from mpl_figure_editor import TraitedFigure
@@ -54,8 +55,15 @@
     """Contains the context of a single experiment -- either a running one
     or a reloaded one.
     """
-    expdir          = Directory(desc="Complete path to experiment directory")
+    ## Complete path to experiment directory
+    expdir = Directory
+
+    ## The console logger
+    console_logger = ConsoleLogger
+
     
+
+    ## All tabs
     _all_tabs       = List(HasTraits, desc="Set of tabs associated with experiment",
                            editor=ListEditor(use_notebook=True,
                                              deletable=False,
@@ -63,13 +71,6 @@
                                              page_name=".title"))
     
     @property
-    def console_logger(self):
-        """Return a reference to the context's console logger."""
-        logger = self._all_tabs[0]
-        assert isinstance(logger, ConsoleLogger)
-        return logger
-
-    @property
     def _display_expdir(self):
         """Shortened version of expdir suitable for display."""
         return os.path.basename(self.expdir)
@@ -113,6 +114,7 @@
             wkbench.curworker = None
 
         super(_WorkerThread,self).__init__(target=work)
+        self.setDaemon(True)     # Allow quitting Python even if thread still running
         self.context = context
 
     def _get_my_tid(self):
@@ -142,6 +144,27 @@
         self.raise_exc(SystemExit)    
 
 
+#####  Custom Handlers  #####################################################
+
+class WorkbenchHandler(Handler):
+    """Some custom UI code for the main window.
+    """
+    def close(self, info, is_ok):
+        """If user is trying to close the main window, look at whether an
+        experiment is currently running.  If so, pop a dialog to ask
+        whether it's really OK to close the thing.
+        """
+        workbench = info.ui.context['object']
+        if workbench.curworker is not None:
+            msg = "An experiment is currently running.\n" \
+                  "Do you really want to interrupt it and\n" \
+                  "close the workbench window?"
+            parent = info.ui.control
+            return confirm(parent, msg) == YES
+        else:
+            return True
+
+
 #####  ExperimentWorkbench  #################################################
 
 class ExperimentWorkbench(HasTraits) :
@@ -164,8 +187,8 @@
 
     ## The main view
     traits_view  = View(
-        HSplit(Group(Group(Item("script_params@", show_label=False, springy=True),
-                           springy=True),
+        HSplit(Group(Item("script_params@", show_label=False, springy=True),
+                     spring, spring,
                      Group(Item("launch", springy=True, enabled_when="curworker is None"),
                            Item("cancel", springy=True, enabled_when="curworker is not None"),
                            show_labels=False, orientation="horizontal")),
@@ -173,13 +196,15 @@
                      springy=True)),
         resizable=True,
         height=0.75, width=0.75,
-        buttons=NoButtons)
+        buttons=NoButtons,
+        handler=WorkbenchHandler() )
 
     ## Experiment management
     def _launch_fired(self):
         expdir  = self.expdir_name(self.script_params.expdir_root)
         logger  = ConsoleLogger()
-        context = ExperimentContext(expdir    = expdir,
+        context = ExperimentContext(expdir = expdir,
+                                    console_logger = logger,
                                     _all_tabs = [ logger ])
         self.experiments.append(context)
         self.curworker = _WorkerThread(self.expfunc, self.script_params, context, self)



From louradou at mail.berlios.de  Thu Jun 12 15:36:53 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Thu, 12 Jun 2008 15:36:53 +0200
Subject: [Plearn-commits] r9124 - trunk/python_modules/plearn/learners
Message-ID: <200806121336.m5CDarO4021103@sheep.berlios.de>

Author: louradou
Date: 2008-06-12 15:36:52 +0200 (Thu, 12 Jun 2008)
New Revision: 9124

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:
- added the number of train/valid/test samples in the table of results
- added an initial seed in the run to have always the same results



Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2008-06-11 22:52:10 UTC (rev 9123)
+++ trunk/python_modules/plearn/learners/SVM.py	2008-06-12 13:36:52 UTC (rev 9124)
@@ -1,3 +1,10 @@
+#! /usr/bin/env python
+#
+# This python code is to use with PLearn (A C++ Machine Learning Library)
+# Copyright (C) 2008 Jerome Louradour
+#
+# For any information or problem regarding this code, please contact jeromelouradour at gmail.com
+
 import os
 
 from libsvm import *
@@ -26,7 +33,6 @@
 
     user options:
 
-
         'C_initvalue': <float> Default value for 'C'.
 
         'verbosity': <int> Level of verbosity.
@@ -61,8 +67,6 @@
                        always includes at first the positive     
                        constant 'C' (trade-off bias/variance).   
 
-    private:
-
         'trials_param_list': <list> of <dict> hyperparameter values
                             that have been already tried.
 
@@ -1013,7 +1017,10 @@
                         valid_stats,
                         test_stats = None,
                         train_stats= None,
-                        only_stdout=False ):
+                        ntrain = None,
+                        nvalid = None,
+                        ntest = None,
+                        only_stdout= False ):
         if valid_stats==None and param == self.best_param:
                 valid_stats = self.valid_stats
         if test_stats==None and param == self.best_param:
@@ -1102,6 +1109,11 @@
                 else:
                     costvalues_string += "None "
         
+        for variable_name in ['ntrain','nvalid','ntest']:
+            if eval(variable_name) <> None:
+                preproc_optionnames += ' %s ' % variable_name
+                preproc_optionvalues += ' %s ' % eval(variable_name)
+        
         # Write the result in the file specified by 'results_filename'
         os.system('makeresults  %s %s %s %s;' % \
                             (self.results_filename,
@@ -1549,7 +1561,10 @@
         self.update_trials( self.best_param,
                             None, test_stats, train_stats )
         self.write_results( self.best_param,
-                            self.valid_stats, self.test_stats, self.train_stats )
+                            self.valid_stats, self.test_stats, self.train_stats,
+                            ntrain = (trainset <> None and trainset.length or 0),
+                            nvalid = (validset <> None and validset.length or 0),
+                            ntest = (testset <> None and testset.length or 0) )
         return dataspec
 
     """ THE interesting function of the class.
@@ -1559,6 +1574,7 @@
         cf. train_inputspec(), valid_inputspec(), and test_inputspec().
     """
     def run(self, dataspec, param = None, L0 = None):
+        random.seed(17)
         assert self.testlevel >= 0
         assert self.max_ntrials > 0
         trainset = self.train_inputspec(dataspec)
@@ -1599,7 +1615,11 @@
 
                 # We reject the model (to avoid testing on it)
                 self.model = None
-                self.write_results( param, valid_stats, None, None, self.testlevel < 2  )
+                self.write_results( param, valid_stats, None, None,
+                                    ntrain = (trainset <> None and trainset.length or 0),
+                                    nvalid = (validset <> None and validset.length or 0),
+                                    ntest = (testset <> None and testset.length or 0),
+                                    only_stdout = self.testlevel < 2  )
 
             # Better valid cost is obtained!
             else:
@@ -1610,7 +1630,11 @@
                 if self.testlevel > 0:
                     self.retrain_and_writeresults(dataspec)
                 else:
-                    self.write_results( param, valid_stats, None, None, True  )
+                    self.write_results( param, valid_stats, None, None,
+                                        ntrain = (trainset <> None and trainset.length or 0),
+                                        nvalid = (validset <> None and validset.length or 0),
+                                        ntest = (testset <> None and testset.length or 0),
+                                        only_stdout = True  )
 
             if( len(expert.trials_param_list)-L0 >= self.max_ntrials
             or  ( self.min_cost <> None and expert.best_cost <= self.min_cost ) ):



From chrish at mail.berlios.de  Mon Jun 16 16:27:29 2008
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Mon, 16 Jun 2008 16:27:29 +0200
Subject: [Plearn-commits] r9125 - trunk/python_modules/plearn/utilities
Message-ID: <200806161427.m5GERTSF024328@sheep.berlios.de>

Author: chrish
Date: 2008-06-16 16:27:28 +0200 (Mon, 16 Jun 2008)
New Revision: 9125

Modified:
   trunk/python_modules/plearn/utilities/toolkit.py
Log:
Use @deprecated to mark command_output as deprecated, so it shows up as such in generated documentation.

Modified: trunk/python_modules/plearn/utilities/toolkit.py
===================================================================
--- trunk/python_modules/plearn/utilities/toolkit.py	2008-06-12 13:36:52 UTC (rev 9124)
+++ trunk/python_modules/plearn/utilities/toolkit.py	2008-06-16 14:27:28 UTC (rev 9125)
@@ -52,8 +52,7 @@
 def command_output(command, stderr = True, stdout = True):
     """Returns the output lines of a shell command.    
     
-    Deprecated: please consider using the new python 2.4 subprocess module
-    instead.
+    @deprecated Please use the subprocess module instead.
 
     @param command: The shell command to execute.
     @type command: String



From larocheh at mail.berlios.de  Mon Jun 16 18:39:24 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Mon, 16 Jun 2008 18:39:24 +0200
Subject: [Plearn-commits] r9126 - trunk/plearn_learners_experimental
Message-ID: <200806161639.m5GGdOJ8018239@sheep.berlios.de>

Author: larocheh
Date: 2008-06-16 18:39:23 +0200 (Mon, 16 Jun 2008)
New Revision: 9126

Modified:
   trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc
   trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.h
Log:
Added an option to not learn sigma_noise...


Modified: trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc
===================================================================
--- trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc	2008-06-16 14:27:28 UTC (rev 9125)
+++ trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc	2008-06-16 16:39:23 UTC (rev 9126)
@@ -1,4 +1,4 @@
-// -*- C++ -*-
+// -*- C ++ -*-
 
 // DeepNonLocalManifoldParzen.cc
 //
@@ -69,6 +69,7 @@
     output_connections_l1_penalty_factor( 0 ),
     output_connections_l2_penalty_factor( 0 ),
     save_manifold_parzen_parameters( false ),
+    do_not_learn_sigma_noise( false ),
     n_layers( 0 ),
     currently_trained_layer( 0 ),
     manifold_parzen_parameters_are_up_to_date( false )
@@ -183,6 +184,11 @@
                   "windows estimator should be saved during test, to speed up "
                   "testing.");
 
+    declareOption(ol, "do_not_learn_sigma_noise", 
+                  &DeepNonLocalManifoldParzen::do_not_learn_sigma_noise,
+                  OptionBase::buildoption,
+                  "Indication that the value of sigma noise should not be learned.\n");
+
     declareOption(ol, "greedy_stages", 
                   &DeepNonLocalManifoldParzen::greedy_stages,
                   OptionBase::learntoption,
@@ -362,7 +368,7 @@
     activation_gradients[n_layers-1].resize( layers[n_layers-1]->size );
     expectation_gradients[n_layers-1].resize( layers[n_layers-1]->size );
 
-    int output_size = n_components*inputsize() + inputsize() + 1;
+    int output_size = n_components*inputsize() + inputsize() + (do_not_learn_sigma_noise ? 0 : 1);
     all_outputs.resize( output_size );
 
     if( !output_connections || output_connections->output_size != output_size)
@@ -785,7 +791,10 @@
     F << all_outputs.subVec(0,n_components * inputsize()).toMat(
         n_components, inputsize());
     mu << all_outputs.subVec(n_components * inputsize(),inputsize());
-    pre_sigma_noise << all_outputs.subVec( (n_components+1) * inputsize(), 1 );
+    if( do_not_learn_sigma_noise )
+        pre_sigma_noise.clear();
+    else
+        pre_sigma_noise << all_outputs.subVec( (n_components+1) * inputsize(), 1 );
 
     F_copy.resize(F.length(),F.width());
     sm_svd.resize(n_components);
@@ -872,9 +881,11 @@
         }
     }
 
+    all_outputs_gradient.resize((n_components+1) * inputsize()+ 
+                                (do_not_learn_sigma_noise ? 0 : 1));
     all_outputs_gradient.clear();
-    coef = 1.0/train_set->length();
-    all_outputs_gradient.resize((n_components+1) * inputsize()+1);
+    //coef = 1.0/train_set->length();
+    coef = 1.0/k_neighbors;
     for(int neighbor=0; neighbor<k_neighbors; neighbor++)
     {
         // dNLL/dF
@@ -890,10 +901,13 @@
                     inv_Sigma_z(neighbor),
                     -coef) ;
 
-        // dNLL/dsn
-        all_outputs_gradient[(n_components + 1 )* inputsize()] += coef* 
-            0.5*(tr_inv_Sigma - pownorm(inv_Sigma_z(neighbor))) * 
-            2 * pre_sigma_noise[0];
+        if( !do_not_learn_sigma_noise )
+        {
+            // dNLL/dsn
+            all_outputs_gradient[(n_components + 1 )* inputsize()] += coef* 
+                0.5*(tr_inv_Sigma - pownorm(inv_Sigma_z(neighbor))) * 
+                2 * pre_sigma_noise[0];
+        }
     }
 
     // Propagating supervised gradient
@@ -953,6 +967,7 @@
         return;
     }
 
+    expectations[0] << input;
     for( int i=0 ; i<layer; i++ )
     {
         connections[i]->fprop( expectations[i], activations[i+1] );

Modified: trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.h
===================================================================
--- trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.h	2008-06-16 14:27:28 UTC (rev 9125)
+++ trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.h	2008-06-16 16:39:23 UTC (rev 9126)
@@ -121,6 +121,9 @@
     //! windows estimator should be saved during test, to speed up testing.
     bool save_manifold_parzen_parameters;
 
+    //! Indication that the value of sigma noise should not be learned.
+    bool do_not_learn_sigma_noise;
+
     //#####  Public Learnt Options  ###########################################
 
     //! Number of layers



From larocheh at mail.berlios.de  Mon Jun 16 19:43:04 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Mon, 16 Jun 2008 19:43:04 +0200
Subject: [Plearn-commits] r9127 - trunk/plearn_learners_experimental
Message-ID: <200806161743.m5GHh4aN032565@sheep.berlios.de>

Author: larocheh
Date: 2008-06-16 19:43:03 +0200 (Mon, 16 Jun 2008)
New Revision: 9127

Modified:
   trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc
   trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.h
Log:
Corrected a bug which made the density estimator not integrate to one...



Modified: trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc
===================================================================
--- trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc	2008-06-16 16:39:23 UTC (rev 9126)
+++ trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc	2008-06-16 17:43:03 UTC (rev 9127)
@@ -66,6 +66,7 @@
     n_components( 1 ),
     min_sigma_noise( 0 ),
     n_classes( -1 ),
+    train_one_network_per_class( false ),
     output_connections_l1_penalty_factor( 0 ),
     output_connections_l2_penalty_factor( 0 ),
     save_manifold_parzen_parameters( false ),
@@ -85,7 +86,7 @@
                   &DeepNonLocalManifoldParzen::cd_learning_rate,
                   OptionBase::buildoption,
                   "The learning rate used during the RBM "
-                  "contrastive divergence training");
+                  "contrastive divergence training.\n");
 
     declareOption(ol, "cd_decrease_ct", 
                   &DeepNonLocalManifoldParzen::cd_decrease_ct,
@@ -100,7 +101,7 @@
                   &DeepNonLocalManifoldParzen::greedy_learning_rate,
                   OptionBase::buildoption,
                   "The learning rate used during the autoassociator "
-                  "gradient descent training");
+                  "gradient descent training.\n");
 
     declareOption(ol, "greedy_decrease_ct", 
                   &DeepNonLocalManifoldParzen::greedy_decrease_ct,
@@ -114,7 +115,7 @@
     declareOption(ol, "fine_tuning_learning_rate", 
                   &DeepNonLocalManifoldParzen::fine_tuning_learning_rate,
                   OptionBase::buildoption,
-                  "The learning rate used during the fine tuning gradient descent");
+                  "The learning rate used during the fine tuning gradient descent.\n");
 
     declareOption(ol, "fine_tuning_decrease_ct", 
                   &DeepNonLocalManifoldParzen::fine_tuning_decrease_ct,
@@ -139,50 +140,57 @@
 
     declareOption(ol, "connections", &DeepNonLocalManifoldParzen::connections,
                   OptionBase::buildoption,
-                  "The weights of the connections between the layers");
+                  "The weights of the connections between the layers.\n");
 
     declareOption(ol, "reconstruction_connections", 
                   &DeepNonLocalManifoldParzen::reconstruction_connections,
                   OptionBase::buildoption,
-                  "The reconstruction weights of the autoassociators");
+                  "The reconstruction weights of the autoassociators.\n");
 
     declareOption(ol, "k_neighbors", 
                   &DeepNonLocalManifoldParzen::k_neighbors,
                   OptionBase::buildoption,
                   "Number of good nearest neighbors to attract and bad nearest "
-                  "neighbors to repel.");
+                  "neighbors to repel.\n");
 
     declareOption(ol, "n_components", 
                   &DeepNonLocalManifoldParzen::n_components,
                   OptionBase::buildoption,
-                  "Dimensionality of the manifold");
+                  "Dimensionality of the manifold.\n");
 
     declareOption(ol, "min_sigma_noise", 
                   &DeepNonLocalManifoldParzen::min_sigma_noise,
                   OptionBase::buildoption,
-                  "Minimum value for the noise variance");
+                  "Minimum value for the noise variance.\n");
 
     declareOption(ol, "n_classes", 
                   &DeepNonLocalManifoldParzen::n_classes,
                   OptionBase::buildoption,
-                  "Number of classes.");
+                  "Number of classes. If n_classes = 1, learner will output\n"
+                  "log likelihood of a given input. If n_classes > 1,\n"
+                  "classification will be performed.\n");
 
+    declareOption(ol, "train_one_network_per_class", 
+                  &DeepNonLocalManifoldParzen::train_one_network_per_class,
+                  OptionBase::buildoption,
+                  "Indication that one network per class should be trained.\n");
+
     declareOption(ol, "output_connections_l1_penalty_factor", 
                   &DeepNonLocalManifoldParzen::output_connections_l1_penalty_factor,
                   OptionBase::buildoption,
-                  "Output weights L1 penalty factor");
+                  "Output weights L1 penalty factor.\n");
 
     declareOption(ol, "output_connections_l2_penalty_factor", 
                   &DeepNonLocalManifoldParzen::output_connections_l2_penalty_factor,
                   OptionBase::buildoption,
-                  "Output weights L2 penalty factor");
+                  "Output weights L2 penalty factor.\n");
 
     declareOption(ol, "save_manifold_parzen_parameters", 
                   &DeepNonLocalManifoldParzen::save_manifold_parzen_parameters,
                   OptionBase::buildoption,
                   "Indication that the parameters for the manifold parzen\n"
                   "windows estimator should be saved during test, to speed up "
-                  "testing.");
+                  "testing.\n");
 
     declareOption(ol, "do_not_learn_sigma_noise", 
                   &DeepNonLocalManifoldParzen::do_not_learn_sigma_noise,
@@ -198,19 +206,19 @@
 
     declareOption(ol, "n_layers", &DeepNonLocalManifoldParzen::n_layers,
                   OptionBase::learntoption,
-                  "Number of layers"
+                  "Number of layers.\n"
         );
 
     declareOption(ol, "output_connections", 
                   &DeepNonLocalManifoldParzen::output_connections,
                   OptionBase::learntoption,
-                  "Output weights"
+                  "Output weights.\n"
         );
 
     declareOption(ol, "train_set", 
                   &DeepNonLocalManifoldParzen::train_set,
                   OptionBase::learntoption,
-                  "Training set"
+                  "Training set.\n"
         );
 
     // Now call the parent class' declareOptions
@@ -232,7 +240,7 @@
 
     MODULE_LOG << "build_() called" << endl;
 
-    if(inputsize_ > 0 && targetsize_ > 0)
+    if(inputsize_ > 0 )
     {
         // Initialize some learnt variables
         n_layers = layers.length();
@@ -284,6 +292,52 @@
         }
 
         build_layers_and_connections();
+
+        if( train_one_network_per_class )
+        {
+            if( n_classes == 1 )
+                PLERROR("DeepNonLocalManifoldParzen::build_() - \n"
+                        "train_one_network_per_class is useless for\n"
+                        "n_classes == 1.\n");
+            if( all_layers.length() != n_classes )
+            {
+                all_layers.resize( n_classes);
+                for( int i=0; i<all_layers.length(); i++ )
+                {
+                    CopiesMap copies;
+                    all_layers[i] = layers->deepCopy(copies);
+                }
+            }
+            if( all_connections.length() != n_classes )
+            {
+                all_connections.resize( n_classes);
+                for( int i=0; i<all_connections.length(); i++ )
+                {
+                    CopiesMap copies;
+                    all_connections[i] = connections->deepCopy(copies);
+                }
+            }
+            if( all_reconstruction_connections.length() != n_classes )
+            {
+                all_reconstruction_connections.resize( n_classes);
+                for( int i=0; i<all_reconstruction_connections.length(); i++ )
+                {
+                    CopiesMap copies;
+                    all_reconstruction_connections[i] = 
+                        reconstruction_connections->deepCopy(copies);
+                }
+            }
+            if( all_output_connections.length() != n_classes )
+            {
+                all_output_connections.resize( n_classes);
+                for( int i=0; i<all_output_connections.length(); i++ )
+                {
+                    CopiesMap copies;
+                    all_output_connections[i] = 
+                        output_connections->deepCopy(copies);
+                }
+            }
+        }
     }
 }
 
@@ -419,6 +473,10 @@
     deepCopyField(reconstruction_activation_gradients, copies);
     deepCopyField(reconstruction_expectation_gradients, copies);
     deepCopyField(output_connections, copies);
+    deepCopyField(all_layers, copies);
+    deepCopyField(all_connections, copies);
+    deepCopyField(all_reconstruction_connections, copies);
+    deepCopyField(all_output_connections, copies);
     deepCopyField(input_representation, copies);
     deepCopyField(previous_input_representation, copies);
     deepCopyField(all_outputs, copies);
@@ -463,7 +521,7 @@
     //if(currently_trained_layer < n_layers)
     //    return layers[currently_trained_layer]->size;
     //return layers[n_layers-1]->size;
-    return n_classes;
+    return 1;
 }
 
 void DeepNonLocalManifoldParzen::forget()
@@ -482,17 +540,38 @@
 
     manifold_parzen_parameters_are_up_to_date = false;
 
-    for( int i=0 ; i<n_layers-1 ; i++ )
-        connections[i]->forget();
-    
-    for( int i=0 ; i<n_layers ; i++ )
-        layers[i]->forget();
-    
-    for( int i=0; i<reconstruction_connections.length(); i++)
-        reconstruction_connections[i]->forget();
+    if( train_one_network_per_class )
+    {
+        for(int c = 0; c<n_classes; c++ )
+        {
+            for( int i=0 ; i<n_layers-1 ; i++ )
+                all_connections[c][i]->forget();
+            
+            for( int i=0 ; i<n_layers ; i++ )
+                all_layers[c][i]->forget();
+            
+            for( int i=0; i<all_reconstruction_connections[c].length(); i++)
+                all_reconstruction_connections[c][i]->forget();
+            
+            if( all_output_connections[c] )
+                all_output_connections[c]->forget();
+        }
+    }
+    else
+    {
+        for( int i=0 ; i<n_layers-1 ; i++ )
+            connections[i]->forget();
+        
+        for( int i=0 ; i<n_layers ; i++ )
+            layers[i]->forget();
+        
+        for( int i=0; i<reconstruction_connections.length(); i++)
+            reconstruction_connections[i]->forget();
+        
+        if( output_connections )
+            output_connections->forget();
 
-    if( output_connections )
-        output_connections->forget();
+    }
 
     stage = 0;
     greedy_stages.clear();
@@ -558,6 +637,15 @@
         {
             sample = *this_stage % nsamples;
             train_set->getExample(sample, input, target, weight);
+
+            if( train_one_network_per_class )
+            {
+                int c = (int) target[0];
+                layers = all_layers[c];
+                connections = all_connections[c];
+                reconstruction_connections = all_reconstruction_connections[c];
+                output_connections = all_output_connections[c];
+            }
             greedyStep( input, target, i, train_costs, *this_stage);
             train_stats->update( train_costs );
 
@@ -576,19 +664,32 @@
             // Find training nearest neighbors
             TVec<int> nearest_neighbors_indices_row;
             nearest_neighbors_indices.resize(train_set->length(), k_neighbors);
-            for(int k=0; k<n_classes; k++)
-            {
-                for(int i=0; i<class_datasets[k]->length(); i++)
+            if( n_classes > 1 )
+                for(int k=0; k<n_classes; k++)
                 {
-                    class_datasets[k]->getExample(i,input,target,weight);
-                    nearest_neighbors_indices_row = nearest_neighbors_indices(
-                        class_datasets[k]->indices[i]);
+                    for(int i=0; i<class_datasets[k]->length(); i++)
+                    {
+                        class_datasets[k]->getExample(i,input,target,weight);
+                        nearest_neighbors_indices_row = nearest_neighbors_indices(
+                            class_datasets[k]->indices[i]);
+                        
+                        computeNearestNeighbors(
+                            new GetInputVMatrix((VMatrix *)class_datasets[k]),input,
+                            nearest_neighbors_indices_row,
+                            i);
+                    }
+                }
+            else
+                for(int i=0; i<train_set->length(); i++)
+                {
+                    train_set->getExample(i,input,target,weight);
+                    nearest_neighbors_indices_row = nearest_neighbors_indices(i);
                     computeNearestNeighbors(
-                        new GetInputVMatrix((VMatrix *)class_datasets[k]),input,
+                        train_set,input,
                         nearest_neighbors_indices_row,
                         i);
                 }
-            }
+                
         }
 
         MODULE_LOG << "Fine-tuning all parameters, by gradient descent" << endl;
@@ -603,31 +704,50 @@
                                   + classname(),
                                   nstages - init_stage );
 
-        setLearningRate( fine_tuning_learning_rate );
         train_costs.fill(MISSING_VALUE);
 
         for( ; stage<nstages ; stage++ )
         {
             sample = stage % nsamples;
-            if( !fast_exact_is_equal( fine_tuning_decrease_ct, 0. ) )
-                setLearningRate( fine_tuning_learning_rate
-                                 / (1. + fine_tuning_decrease_ct * stage ) );
-
             train_set->getExample( sample, input, target, weight );
 
             // Find nearest neighbors
-            for( int k=0; k<k_neighbors; k++ )
+            if( n_classes > 1 )
+                for( int k=0; k<k_neighbors; k++ )
+                {
+                    class_datasets[(int)round(target[0])]->getExample(
+                        nearest_neighbors_indices(sample,k),
+                        nearest_neighbor, target2, weight2);
+                    
+                    if(round(target[0]) != round(target2[0]))
+                        PLERROR("DeepNonLocalManifoldParzen::train(): similar"
+                                " example is not from same class!");
+                    nearest_neighbors(k) << nearest_neighbor;
+                }
+            else
+                for( int k=0; k<k_neighbors; k++ )
+                {
+                    train_set->getExample(
+                        nearest_neighbors_indices(sample,k),
+                        nearest_neighbor, target2, weight2);
+                    nearest_neighbors(k) << nearest_neighbor;
+                }
+                
+
+            if( train_one_network_per_class )
             {
-                class_datasets[(int)round(target[0])]->getExample(
-                    nearest_neighbors_indices(sample,k),
-                    nearest_neighbor, target2, weight2);
-
-                if(round(target[0]) != round(target2[0]))
-                    PLERROR("DeepNonLocalManifoldParzen::train(): similar"
-                            " example is not from same class!");
-                nearest_neighbors(k) << nearest_neighbor;
+                int c = (int) target[0];
+                layers = all_layers[c];
+                connections = all_connections[c];
+                reconstruction_connections = all_reconstruction_connections[c];
+                output_connections = all_output_connections[c];
             }
 
+            if( !fast_exact_is_equal( fine_tuning_decrease_ct, 0. ) )
+                setLearningRate( fine_tuning_learning_rate
+                                 / (1. + fine_tuning_decrease_ct * stage ) );
+            else
+                setLearningRate( fine_tuning_learning_rate );
 
             fineTuningStep( input, target, train_costs, 
                             nearest_neighbors);
@@ -775,8 +895,17 @@
 
 void DeepNonLocalManifoldParzen::computeManifoldParzenParameters( 
     const Vec& input, Mat& F, Vec& mu, 
-    Vec& pre_sigma_noise, Mat& U, Vec& sm_svd) const
+    Vec& pre_sigma_noise, Mat& U, Vec& sm_svd, int target_class) const
 {
+    if( train_one_network_per_class )
+    {
+        PLASSERT( target_class >= 0 );
+        layers = all_layers[target_class];
+        connections = all_connections[target_class];
+        reconstruction_connections = all_reconstruction_connections[target_class];
+        output_connections = all_output_connections[target_class];
+    }
+
     // Get example representation
     computeRepresentation(input, input_representation, 
                           n_layers-1);
@@ -816,7 +945,11 @@
 {
     manifold_parzen_parameters_are_up_to_date = false;
 
-    computeManifoldParzenParameters( input, F, mu, pre_sigma_noise, U, sm_svd );
+    if( n_classes > 1 )
+        computeManifoldParzenParameters( input, F, mu, pre_sigma_noise, U, sm_svd,
+                                         (int)target[0]);
+    else
+        computeManifoldParzenParameters( input, F, mu, pre_sigma_noise, U, sm_svd);
 
     real sigma_noise = pre_sigma_noise[0]* pre_sigma_noise[0] + min_sigma_noise;
 
@@ -1011,11 +1144,23 @@
         int input_j_index;
         for( int i=0; i<n_classes; i++ )
         {
-            for( int j=0; j<class_datasets[i]->length(); j++ )
+            for( int j=0; 
+                 j<(n_classes > 1 ? 
+                    class_datasets[i]->length() 
+                    : train_set->length()); 
+                 j++ )
             {
-                class_datasets[i]->getExample(j,input_j,target,weight);
+                if( n_classes > 1 )
+                {
+                    class_datasets[i]->getExample(j,input_j,target,weight);
+                    input_j_index = class_datasets[i]->indices[j];
+                }
+                else
+                {
+                    train_set->getExample(j,input_j,target,weight);
+                    input_j_index = j;
+                }
 
-                input_j_index = class_datasets[i]->indices[j];
                 U << eigenvectors[input_j_index];
                 sm_svd << eigenvalues(input_j_index);
                 sigma_noise = sigma_noises[input_j_index];
@@ -1034,7 +1179,7 @@
                     dotp = dot(diff,uk);
                     coef = (1.0/(sm_svd[k]+sigma_noise) - 1.0/sigma_noise);
                     mahal -= dotp*dotp*0.5*coef;
-                    norm_term -= 0.5*pl_log(sm_svd[k]);
+                    norm_term -= 0.5*pl_log(sm_svd[k]+sigma_noise);
                 }
                 
                 if( j==0 )
@@ -1051,12 +1196,26 @@
 
         for( int i=0; i<n_classes; i++ )
         {
-            for( int j=0; j<class_datasets[i]->length(); j++ )
+            for( int j=0; 
+                 j<(n_classes > 1 ? 
+                    class_datasets[i]->length() 
+                    : train_set->length()); 
+                 j++ )
             {
-                class_datasets[i]->getExample(j,input_j,target,weight);
+                if( n_classes > 1 )
+                {
+                    class_datasets[i]->getExample(j,input_j,target,weight);
+                    computeManifoldParzenParameters( input_j, F, mu, 
+                                                     pre_sigma_noise, U, sm_svd,
+                                                     (int) target[0]);
+                }
+                else
+                {
+                    train_set->getExample(j,input_j,target,weight);
+                    computeManifoldParzenParameters( input_j, F, mu, 
+                                                     pre_sigma_noise, U, sm_svd );
+                }
 
-                computeManifoldParzenParameters( input_j, F, mu, 
-                                                 pre_sigma_noise, U, sm_svd );
                 
                 sigma_noise = pre_sigma_noise[0]*pre_sigma_noise[0] 
                     + min_sigma_noise;
@@ -1074,7 +1233,7 @@
                     dotp = dot(diff,uk);
                     coef = (1.0/(sm_svd[k]+sigma_noise) - 1.0/sigma_noise);
                     mahal -= dotp*dotp*0.5*coef;
-                    norm_term -= 0.5*pl_log(sm_svd[k]);
+                    norm_term -= 0.5*pl_log(sm_svd[k]+sigma_noise);
                 }
                 
                 if( j==0 )
@@ -1087,8 +1246,10 @@
         }
     }
 
-
-    output[0] = argmax(test_votes);
+    if( n_classes > 1 )
+        output[0] = argmax(test_votes);
+    else
+        output[0] = test_votes[0]-pl_log(train_set->length());
 }
 
 void DeepNonLocalManifoldParzen::computeCostsFromOutputs(const Vec& input, const Vec& output,
@@ -1100,6 +1261,15 @@
     costs.resize( getTestCostNames().length() );
     costs.fill( MISSING_VALUE );
 
+    if( train_one_network_per_class )
+    {
+        int c = (int) target[0];
+        layers = all_layers[c];
+        connections = all_connections[c];
+        reconstruction_connections = all_reconstruction_connections[c];
+        output_connections = all_output_connections[c];
+    }
+
     if( currently_trained_layer<n_layers 
         && reconstruction_connections.length() != 0 )
     {
@@ -1120,12 +1290,20 @@
     }
     else
     {
-        int target_class = ((int)round(target[0]));
-        if( ((int)round(output[0])) == target_class )
-            costs[n_layers-1] = 0;
+        if( n_classes > 1 )
+        {
+            int target_class = ((int)round(target[0]));
+            if( ((int)round(output[0])) == target_class )
+                costs[n_layers-1] = 0;
+            else
+                costs[n_layers-1] = 1;
+            costs[n_layers] = - test_votes[target_class]
+                +pl_log(class_datasets[target_class]->length()); // Must take into account the 1/n normalization
+        }
         else
-            costs[n_layers-1] = 1;
-        costs[n_layers] = - test_votes[target_class]+pl_log(class_datasets[target_class]->length());
+        {
+            costs[n_layers] = - output[0]; // 1/n normalization already accounted for
+        }
     }
 }
 
@@ -1151,8 +1329,13 @@
         {
             train_set->getExample(i,input,target,weight);
 
-            computeManifoldParzenParameters( input, F, mu, 
-                                             pre_sigma_noise, U, sm_svd );
+            if( n_classes > 1 )
+                computeManifoldParzenParameters( input, F, mu, 
+                                                 pre_sigma_noise, U, sm_svd,
+                                                 (int) target[0]);
+            else
+                computeManifoldParzenParameters( input, F, mu, 
+                                                 pre_sigma_noise, U, sm_svd);
             
             sigma_noise = pre_sigma_noise[0]*pre_sigma_noise[0] + min_sigma_noise;
 
@@ -1198,16 +1381,19 @@
     manifold_parzen_parameters_are_up_to_date = false;
 
     // Separate classes
-    class_datasets.resize(n_classes);
-    for(int k=0; k<n_classes; k++)
+    if( n_classes > 1 )
     {
-        class_datasets[k] = new ClassSubsetVMatrix();
-        class_datasets[k]->classes.resize(1);
-        class_datasets[k]->classes[0] = k;
-        class_datasets[k]->source = training_set;
-        class_datasets[k]->build();
+        class_datasets.resize(n_classes);
+        for(int k=0; k<n_classes; k++)
+        {
+            class_datasets[k] = new ClassSubsetVMatrix();
+            class_datasets[k]->classes.resize(1);
+            class_datasets[k]->classes[0] = k;
+            class_datasets[k]->source = training_set;
+            class_datasets[k]->build();
+        }
     }
-    
+
     //// Find other classes proportions
     //class_proportions.resize(n_classes);
     //class_proportions.fill(0);

Modified: trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.h
===================================================================
--- trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.h	2008-06-16 16:39:23 UTC (rev 9126)
+++ trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.h	2008-06-16 17:43:03 UTC (rev 9127)
@@ -90,13 +90,13 @@
     TVec<int> training_schedule;
 
     //! The layers of units in the network
-    TVec< PP<RBMLayer> > layers;
+    mutable TVec< PP<RBMLayer> > layers;
 
     //! The weights of the connections between the layers
-    TVec< PP<RBMConnection> > connections;
+    mutable TVec< PP<RBMConnection> > connections;
 
     //! The reconstruction weights of the autoassociators
-    TVec< PP<RBMConnection> > reconstruction_connections;
+    mutable TVec< PP<RBMConnection> > reconstruction_connections;
 
     //! Number of nearest neighbors to use to learn
     //! the manifold structure.
@@ -111,6 +111,9 @@
     //! Number of classes
     int n_classes;
 
+    //! Indication that one network per class should be trained
+    bool train_one_network_per_class;
+
     //! Output weights L1 penalty factor
     real output_connections_l1_penalty_factor;
 
@@ -192,7 +195,8 @@
 
     void computeManifoldParzenParameters( const Vec& input, 
                                           Mat& F, Vec& mu, Vec& pre_sigma_noise,
-                                          Mat& U, Vec& sm_svd) const;
+                                          Mat& U, Vec& sm_svd, 
+                                          int target_class = -1) const;
                                           
 
     //#####  PLearn::Object Protocol  #########################################
@@ -242,6 +246,12 @@
     //! Output weights
     mutable PP<OnlineLearningModule> output_connections;
     
+    //! Parameters for all networks, when training one network per class
+    mutable TVec< TVec< PP<RBMLayer> > > all_layers;
+    mutable TVec< TVec<PP<RBMConnection> > > all_connections;
+    mutable TVec< TVec<PP<RBMConnection> > > all_reconstruction_connections;
+    mutable TVec< PP<OnlineLearningModule> > all_output_connections;
+
     //! Example representation
     mutable Vec input_representation;
 



From saintmlx at mail.berlios.de  Mon Jun 16 20:07:01 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Mon, 16 Jun 2008 20:07:01 +0200
Subject: [Plearn-commits] r9128 - trunk/plearn/python
Message-ID: <200806161807.m5GI71rj002144@sheep.berlios.de>

Author: saintmlx
Date: 2008-06-16 20:07:00 +0200 (Mon, 16 Jun 2008)
New Revision: 9128

Modified:
   trunk/plearn/python/PythonExtension.cc
Log:
- add class and options help text to python help



Modified: trunk/plearn/python/PythonExtension.cc
===================================================================
--- trunk/plearn/python/PythonExtension.cc	2008-06-16 17:43:03 UTC (rev 9127)
+++ trunk/plearn/python/PythonExtension.cc	2008-06-16 18:07:00 UTC (rev 9128)
@@ -258,13 +258,15 @@
 
         // create new python type deriving from WrappedPLearnObject
         string classname= tit->first;
+        string class_help_text= HelpSystem::helpOnClass(classname);
+        search_replace(class_help_text, "\"\"\"", "\\\"\\\"\\\"");
         string pyclassname= classname;
         search_replace(pyclassname, " ", "_");
         search_replace(pyclassname, "<", "_");
         search_replace(pyclassname, ">", "_");
         string derivcode= string("\nclass ")
             + pyclassname + "(" + actual_wrapper_name + "):\n"
-            "  \"\"\" ... \"\"\"\n"
+            "  \"\"\" \n" + class_help_text + "\n \"\"\"\n"
             "  def __new__(cls,*args,**kwargs):\n"
             "    #print '** "+pyclassname+".__new__',kwargs\n"
             "    obj= object.__new__(cls,*args,**kwargs)\n"



From saintmlx at mail.berlios.de  Mon Jun 16 20:08:46 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Mon, 16 Jun 2008 20:08:46 +0200
Subject: [Plearn-commits] r9129 - trunk/plearn/math
Message-ID: <200806161808.m5GI8kg4002431@sheep.berlios.de>

Author: saintmlx
Date: 2008-06-16 20:08:46 +0200 (Mon, 16 Jun 2008)
New Revision: 9129

Modified:
   trunk/plearn/math/TMat_maths_impl.h
Log:
- allow op+= on empty TMats



Modified: trunk/plearn/math/TMat_maths_impl.h
===================================================================
--- trunk/plearn/math/TMat_maths_impl.h	2008-06-16 18:07:00 UTC (rev 9128)
+++ trunk/plearn/math/TMat_maths_impl.h	2008-06-16 18:08:46 UTC (rev 9129)
@@ -5921,11 +5921,14 @@
         PLERROR("IN operator+=(const TMat<T>& m1(%d,%d), const TMat<T>& m2(%d,%d)): m1 and m2 must have same dimensions",
                 m1.length(),m1.width(),m2.length(),m2.width());
 #endif
-    T* m1_i = m1.data();
-    T* m2_i = m2.data();
-    for(int i=0; i<n; i++, m1_i+=m1.mod(),m2_i+=m2.mod())
-        for(int j=0; j<l; j++)
-            m1_i[j] -= m2_i[j];
+    if(m1.isNotEmpty()) // calc only if some data
+    {
+        T* m1_i = m1.data();
+        T* m2_i = m2.data();
+        for(int i=0; i<n; i++, m1_i+=m1.mod(),m2_i+=m2.mod())
+            for(int j=0; j<l; j++)
+                m1_i[j] -= m2_i[j];
+    }
 }
 
 template<class T>



From saintmlx at mail.berlios.de  Mon Jun 16 20:21:56 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Mon, 16 Jun 2008 20:21:56 +0200
Subject: [Plearn-commits] r9130 - in trunk: commands/PLearnCommands
	plearn/io plearn/io/test plearn/misc plearn/sys plearn/vmat
Message-ID: <200806161821.m5GILupN004352@sheep.berlios.de>

Author: saintmlx
Date: 2008-06-16 20:21:54 +0200 (Mon, 16 Jun 2008)
New Revision: 9130

Modified:
   trunk/commands/PLearnCommands/Plide.cc
   trunk/commands/PLearnCommands/RunCommand.cc
   trunk/plearn/io/MatIO.h
   trunk/plearn/io/PPath.cc
   trunk/plearn/io/PStream.h
   trunk/plearn/io/PyPLearnScript.cc
   trunk/plearn/io/fileutils.cc
   trunk/plearn/io/test/PStreamBufTest.cc
   trunk/plearn/misc/PLearnServer.cc
   trunk/plearn/misc/PLearnService.cc
   trunk/plearn/misc/RemotePLearnServer.cc
   trunk/plearn/sys/Popen.cc
   trunk/plearn/vmat/VMField.cc
   trunk/plearn/vmat/VMField.h
   trunk/plearn/vmat/VMatLanguage.cc
   trunk/plearn/vmat/VMatrix.cc
Log:
- removed implicit conversion of PStream to bool, fixed affected code



Modified: trunk/commands/PLearnCommands/Plide.cc
===================================================================
--- trunk/commands/PLearnCommands/Plide.cc	2008-06-16 18:08:46 UTC (rev 9129)
+++ trunk/commands/PLearnCommands/Plide.cc	2008-06-16 18:21:54 UTC (rev 9130)
@@ -501,7 +501,7 @@
     string pyplearn_script_string = pyplearn_script->getScript();
     PStream plearn_in = openString(pyplearn_script_string,
                                    PStream::plearn_ascii);
-    while (plearn_in) {
+    while (plearn_in.good()) {
         PP<Object> o = readObject(plearn_in);
         o->run();
         plearn_in.skipBlanksAndCommentsAndSeparators();

Modified: trunk/commands/PLearnCommands/RunCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/RunCommand.cc	2008-06-16 18:08:46 UTC (rev 9129)
+++ trunk/commands/PLearnCommands/RunCommand.cc	2008-06-16 18:21:54 UTC (rev 9130)
@@ -134,7 +134,7 @@
     else
         PLERROR("Invalid extension for script file. Must be one of .pyplearn .plearn .psave");
 
-    while ( in )
+    while ( in.good() )
     {
         PP<Object> o = readObject(in);
         o->run();

Modified: trunk/plearn/io/MatIO.h
===================================================================
--- trunk/plearn/io/MatIO.h	2008-06-16 18:08:46 UTC (rev 9129)
+++ trunk/plearn/io/MatIO.h	2008-06-16 18:21:54 UTC (rev 9130)
@@ -306,7 +306,7 @@
         skipBlanksAndComments(loadmat);
         for(int j=0; j<width; j++) 
         {
-            if (loadmat) 
+            if (loadmat.good()) 
             {
                 loadmat >> inp_element;
                 if (pl_isnumber(inp_element)) 

Modified: trunk/plearn/io/PPath.cc
===================================================================
--- trunk/plearn/io/PPath.cc	2008-06-16 18:08:46 UTC (rev 9129)
+++ trunk/plearn/io/PPath.cc	2008-06-16 18:21:54 UTC (rev 9130)
@@ -265,10 +265,10 @@
 
             string  next_metaprotocol;
             PPath   next_metapath;    
-            while (ppath_config) {
+            while (ppath_config.good()) {
                 ppath_config >> next_metaprotocol >> next_metapath;
                 if (next_metaprotocol.empty())
-                    if (ppath_config)
+                    if (ppath_config.good())
                         PLERROR("In PPath::metaprotocolToMetapath - Error while parsing PPath config file (%s): read "
                                 "a blank line before reaching the end of the file",
                                 config_file_path.errorDisplay().c_str());

Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2008-06-16 18:08:46 UTC (rev 9129)
+++ trunk/plearn/io/PStream.h	2008-06-16 18:21:54 UTC (rev 9130)
@@ -424,8 +424,23 @@
     bool good() const
     { return ptr && ptr->good(); }
 
+    /*****
+     * DEPRECATED: this implicit conversion is too dangerous!
+     *             use .good() instead.
     operator bool() const
     { return good(); }
+    */
+    /*****
+     * operator void*() causes a compile error ON PURPOSE! (ambiguous w/ PP<T>::operator T*())
+     * The implicit conversion of a PStream to a bool or pointer is
+     * too dangerous to be allowed; please use PStream::good() (or PP<T>::isNotNull()) instead.
+     * EXAMPLE of a flawed conversion to bool:
+     *  enum Machin { Truc, Bidule };
+     *  Machin mon_machin;
+     *  pin >> mon_machin; // OOPS!!! converts pin to bool and applies operator>>(int,int), a shift... 
+     */
+    /** !!! **/ operator void*() const // DO NOT USE IT; FAILS ON PURPOSE!
+                { PLERROR("!?! this should have failed at compile time !?!"); return 0; } 
 
     //! Test whether a PStream is in an invalid state.
     // This operator is required for compilation under Visual C++.

Modified: trunk/plearn/io/PyPLearnScript.cc
===================================================================
--- trunk/plearn/io/PyPLearnScript.cc	2008-06-16 18:08:46 UTC (rev 9129)
+++ trunk/plearn/io/PyPLearnScript.cc	2008-06-16 18:21:54 UTC (rev 9130)
@@ -224,7 +224,7 @@
 
     PStream in = openString( plearn_script, PStream::plearn_ascii );
 
-    while ( in )
+    while ( in.good() )
     {
         PP<Object> o = readObject(in);
         o->run();

Modified: trunk/plearn/io/fileutils.cc
===================================================================
--- trunk/plearn/io/fileutils.cc	2008-06-16 18:08:46 UTC (rev 9129)
+++ trunk/plearn/io/fileutils.cc	2008-06-16 18:21:54 UTC (rev 9130)
@@ -463,7 +463,7 @@
 /////////////////////////
 void getNextNonBlankLine(PStream& in, string& line)
 {
-    while (in) {
+    while (in.good()) {
         in.getline(line);
         size_t l = line.size();
         bool ok = false;
@@ -722,7 +722,7 @@
     string text; // the processed text to return
     bool inside_a_quoted_string=false; // inside a quoted string we don't skip characters following a #
     int c=EOF, last_c=EOF;
-    while(in)
+    while(in.good())
     {
         last_c = c;
         c = in.get();

Modified: trunk/plearn/io/test/PStreamBufTest.cc
===================================================================
--- trunk/plearn/io/test/PStreamBufTest.cc	2008-06-16 18:08:46 UTC (rev 9129)
+++ trunk/plearn/io/test/PStreamBufTest.cc	2008-06-16 18:21:54 UTC (rev 9130)
@@ -187,7 +187,8 @@
 
     test("Peek", s.peek(), 'v');
 
-    test("Conversion to bool, not on EOF", bool(s), 1);
+    //test("Conversion to bool, not on EOF", bool(s), 1);//DEPRECATED
+    test("stream.good(), not on EOF", s.good(), 1);
   
     s.putback('U');
     test("Putback of a single char", s.get(), 'U');
@@ -204,7 +205,8 @@
     s.read(temp, 10);
     test("EOF, reading into a string", temp, "");
 
-    test("Conversion to bool on EOF", bool(s), 0);
+    //test("Conversion to bool on EOF", bool(s), 0);//DEPRECATED
+    test("stream.good() on EOF", s.good(), 0);
   
     test("EOF, second time", s.get(), EOF);
 

Modified: trunk/plearn/misc/PLearnServer.cc
===================================================================
--- trunk/plearn/misc/PLearnServer.cc	2008-06-16 18:08:46 UTC (rev 9129)
+++ trunk/plearn/misc/PLearnServer.cc	2008-06-16 18:21:54 UTC (rev 9130)
@@ -247,7 +247,7 @@
         int c = -1;
         do 
             c = io.get(); 
-        while(io && c!='!' && c!=EOF);
+        while(io.good() && c!='!' && c!=EOF);
         
         if(c==EOF || !io)
             return true;

Modified: trunk/plearn/misc/PLearnService.cc
===================================================================
--- trunk/plearn/misc/PLearnService.cc	2008-06-16 18:08:46 UTC (rev 9129)
+++ trunk/plearn/misc/PLearnService.cc	2008-06-16 18:21:54 UTC (rev 9130)
@@ -89,13 +89,13 @@
 
     TVec< pair<string,int> > hostname_and_port;
 
-    while(in)
+    while(in.good())
     {
         in.skipBlanksAndComments();
         if(!in)
             break;
         in >> hostname >> tcpport >> pid;
-        if(in)
+        if(in.good())
             hostname_and_port.append(pair<string,int>(hostname,tcpport));
     }
     connectToServers(hostname_and_port);

Modified: trunk/plearn/misc/RemotePLearnServer.cc
===================================================================
--- trunk/plearn/misc/RemotePLearnServer.cc	2008-06-16 18:08:46 UTC (rev 9129)
+++ trunk/plearn/misc/RemotePLearnServer.cc	2008-06-16 18:21:54 UTC (rev 9130)
@@ -207,7 +207,7 @@
 
 void RemotePLearnServer::deleteAllObjectsAsync()
 {
-    if(io)
+    if(io.good())
     {
         io.write("!Z "); 
         io << endl;

Modified: trunk/plearn/sys/Popen.cc
===================================================================
--- trunk/plearn/sys/Popen.cc	2008-06-16 18:08:46 UTC (rev 9129)
+++ trunk/plearn/sys/Popen.cc	2008-06-16 18:21:54 UTC (rev 9130)
@@ -200,7 +200,7 @@
 {
     Popen p(command, redirect_stderr);
     vector<string> result;
-    while(p.in)
+    while(p.in.good())
     {
         string line = p.in.getline();
         result.push_back(line);

Modified: trunk/plearn/vmat/VMField.cc
===================================================================
--- trunk/plearn/vmat/VMField.cc	2008-06-16 18:08:46 UTC (rev 9129)
+++ trunk/plearn/vmat/VMField.cc	2008-06-16 18:21:54 UTC (rev 9130)
@@ -59,7 +59,13 @@
     return !((*this)==other);
 }
 
+PStream& operator>>(PStream& in, VMField::FieldType& x) // dummy placeholder; do not call
+{
+    PLERROR("operator>> for VMField::FieldType not implemented (yet).");
+    return in; // shut up compiler
+}
 
+
 /** VMFieldStat **/
 
 VMFieldStat::VMFieldStat(int the_maxndiscrete)

Modified: trunk/plearn/vmat/VMField.h
===================================================================
--- trunk/plearn/vmat/VMField.h	2008-06-16 18:08:46 UTC (rev 9129)
+++ trunk/plearn/vmat/VMField.h	2008-06-16 18:21:54 UTC (rev 9130)
@@ -78,6 +78,7 @@
 
 };
 
+PStream& operator>>(PStream& in, VMField::FieldType& x); // dummy placeholder; do not call
 
 //!  this class holds simple statistics about a field
 class VMFieldStat

Modified: trunk/plearn/vmat/VMatLanguage.cc
===================================================================
--- trunk/plearn/vmat/VMatLanguage.cc	2008-06-16 18:08:46 UTC (rev 9129)
+++ trunk/plearn/vmat/VMatLanguage.cc	2008-06-16 18:21:54 UTC (rev 9130)
@@ -291,7 +291,7 @@
     string token;
     size_t spos;
     map<string, string>::iterator pos;
-    while (in)
+    while (in.good())
     {
         in >> token;
         pos = defines.find(token);
@@ -708,7 +708,7 @@
 {
     string token;
     map<string,string>::iterator pos;
-    while(in)
+    while(in.good())
     {
         in >> token;
         pos=defines.find(token);

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-06-16 18:08:46 UTC (rev 9129)
+++ trunk/plearn/vmat/VMatrix.cc	2008-06-16 18:21:54 UTC (rev 9130)
@@ -217,6 +217,12 @@
         (BodyDoc("Returns the field names.\n"),
          RetDoc ("TVec of field names.\n")));
 
+    declareMethod(
+        rmm, "fieldName", &VMatrix::fieldName,
+        (BodyDoc("Returns the field name for a given column.\n"),
+         ArgDoc ("col", "column index.\n"),
+         RetDoc ("Field name.\n")));
+
      declareMethod(
         rmm, "findFieldIndex", &VMatrix::fieldIndex,
         (BodyDoc("Returns the index of a field, or -1 if the field does not "
@@ -285,7 +291,14 @@
          ArgDoc ("w", "width"),
          RetDoc ("The sub-VMatrix")));
 
+    declareMethod(
+        rmm, "get", &VMatrix::get,
+        (BodyDoc("Returns the element at position (i,j)\n"),
+         ArgDoc ("i", "row"),
+         ArgDoc ("j", "col"),
+         RetDoc ("Value at (i,j)")));
 
+
     declareMethod(
         rmm, "getStats", &VMatrix::remote_getStats,
         (BodyDoc("Returns the unconditonal statistics for all fields\n"),
@@ -1364,7 +1377,7 @@
     if(!hasMetaDataDir())
         PLERROR("In VMatrix::lockMetaDataDir() subclass %s -"
                 " metadatadir was not set", classname().c_str());
-    if(lockf_) // Already locked by this object!
+    if(lockf_.good()) // Already locked by this object!
         PLERROR("VMatrix::lockMetaDataDir() subclass %s -"
                 " called while already locked by this object.",
                 classname().c_str());
@@ -1454,12 +1467,12 @@
         PLERROR( string("File "+fname+" is not a valid String mapping file.\nShould start with #SMAP on first line (this is to prevent inopportunely overwritting another type of file)").c_str());
 #endif
 
-    while(f)
+    while(f.good())
     {
         string s;
         real val;
         f >> s >> val;
-        if(f)
+        if(f.good())
         {
             map_sr[col][s]   = val;
             map_rs[col][val] = s;



From lamblin at mail.berlios.de  Mon Jun 16 21:10:12 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Mon, 16 Jun 2008 21:10:12 +0200
Subject: [Plearn-commits] r9131 - trunk/plearn_learners/online
Message-ID: <200806161910.m5GJACmm011187@sheep.berlios.de>

Author: lamblin
Date: 2008-06-16 21:10:11 +0200 (Mon, 16 Jun 2008)
New Revision: 9131

Modified:
   trunk/plearn_learners/online/RBMSparse1DMatrixConnection.cc
   trunk/plearn_learners/online/RBMSparse1DMatrixConnection.h
Log:

Remove "executable" property



Property changes on: trunk/plearn_learners/online/RBMSparse1DMatrixConnection.cc
___________________________________________________________________
Name: svn:executable
   - *


Property changes on: trunk/plearn_learners/online/RBMSparse1DMatrixConnection.h
___________________________________________________________________
Name: svn:executable
   - *



From nouiz at mail.berlios.de  Mon Jun 16 21:40:26 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 16 Jun 2008 21:40:26 +0200
Subject: [Plearn-commits] r9132 - trunk/scripts
Message-ID: <200806161940.m5GJeQhT014272@sheep.berlios.de>

Author: nouiz
Date: 2008-06-16 21:40:26 +0200 (Mon, 16 Jun 2008)
New Revision: 9132

Modified:
   trunk/scripts/collectres
Log:
bugfix. Allow to work with matrix that have a different number or row.


Modified: trunk/scripts/collectres
===================================================================
--- trunk/scripts/collectres	2008-06-16 19:10:11 UTC (rev 9131)
+++ trunk/scripts/collectres	2008-06-16 19:40:26 UTC (rev 9132)
@@ -239,7 +239,7 @@
         alist.append(concatenate([a,b],0))
       else:
         alist.append(a)
-    a = concatenate(arrays,1)
+    a = concatenate(alist,1)
     # write the array to file, without any []
     write_array(f,a)
   else:



From plearner at mail.berlios.de  Mon Jun 16 22:41:54 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Mon, 16 Jun 2008 22:41:54 +0200
Subject: [Plearn-commits] r9133 - trunk/plearn_learners/hyper
Message-ID: <200806162041.m5GKfsmv023499@sheep.berlios.de>

Author: plearner
Date: 2008-06-16 22:41:53 +0200 (Mon, 16 Jun 2008)
New Revision: 9133

Modified:
   trunk/plearn_learners/hyper/HyperCommand.cc
   trunk/plearn_learners/hyper/HyperCommand.h
   trunk/plearn_learners/hyper/HyperOptimize.cc
   trunk/plearn_learners/hyper/HyperOptimize.h
Log:
Added forget() method to reinitialize hypercommands with a state such as HyperOptimize, called before doing a substrategy.
This corrects a bug introduced with the recent changes to hyperoptimization framework to allow resuming prematurely stopped hyperoptimizations (see Fred).
The bug manifested when using a substrategy. (encountered by Hugo)



Modified: trunk/plearn_learners/hyper/HyperCommand.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperCommand.cc	2008-06-16 19:40:26 UTC (rev 9132)
+++ trunk/plearn_learners/hyper/HyperCommand.cc	2008-06-16 20:41:53 UTC (rev 9133)
@@ -97,6 +97,10 @@
     build_();
 }
 
+void HyperCommand::forget()
+{}
+
+
 void HyperCommand::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);

Modified: trunk/plearn_learners/hyper/HyperCommand.h
===================================================================
--- trunk/plearn_learners/hyper/HyperCommand.h	2008-06-16 19:40:26 UTC (rev 9132)
+++ trunk/plearn_learners/hyper/HyperCommand.h	2008-06-16 20:41:53 UTC (rev 9133)
@@ -112,6 +112,10 @@
     PPath getExperimentDirectory() const
     { return expdir; }
 
+    //! Resets the command's internal state as if freshly constructed
+    //! (default does nothing)
+    virtual void forget();
+
     //! Executes the command, returning the resulting costvec of its optimization
     //! (or an empty vec if it didn't do any testng).
     virtual Vec optimize() =0;

Modified: trunk/plearn_learners/hyper/HyperOptimize.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperOptimize.cc	2008-06-16 19:40:26 UTC (rev 9132)
+++ trunk/plearn_learners/hyper/HyperOptimize.cc	2008-06-16 20:41:53 UTC (rev 9133)
@@ -336,6 +336,15 @@
     return hlearner->tester->getStatNames();
 }
 
+void HyperOptimize::forget()
+{
+    trialnum = 0;    
+    option_vals.resize(0);
+    best_objective = REAL_MAX;
+    best_results = Vec();
+    best_learner = 0;
+}
+
 Vec HyperOptimize::optimize()
 {
 //in the case when auto_save is true. This function can be called even
@@ -383,10 +392,12 @@
             for(int commandnum=0; commandnum<sub_strategy.length(); commandnum++)
             {
                 sub_strategy[commandnum]->setHyperLearner(hlearner);
+                sub_strategy[commandnum]->forget();
                 if(!expdir.isEmpty() && provide_sub_expdir)
                     sub_strategy[commandnum]->setExperimentDirectory(
                         expdir / ("Trials"+tostring(trialnum)) / ("Step"+tostring(commandnum))
                         );
+                
                 best_sub_results = sub_strategy[commandnum]->optimize();
             }
             if(rerun_after_sub)

Modified: trunk/plearn_learners/hyper/HyperOptimize.h
===================================================================
--- trunk/plearn_learners/hyper/HyperOptimize.h	2008-06-16 19:40:26 UTC (rev 9132)
+++ trunk/plearn_learners/hyper/HyperOptimize.h	2008-06-16 20:41:53 UTC (rev 9133)
@@ -178,6 +178,8 @@
     //! Returns the names of the results returned by the optimize() method.
     virtual TVec<string> getResultNames() const;
 
+    virtual void forget();
+
     virtual Vec optimize();
 
     // simply calls inherited::build() then build_()



From louradou at mail.berlios.de  Tue Jun 17 17:51:55 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Tue, 17 Jun 2008 17:51:55 +0200
Subject: [Plearn-commits] r9134 - trunk/python_modules/plearn/utilities
Message-ID: <200806171551.m5HFptQb011782@sheep.berlios.de>

Author: louradou
Date: 2008-06-17 17:51:53 +0200 (Tue, 17 Jun 2008)
New Revision: 9134

Added:
   trunk/python_modules/plearn/utilities/write_results.py
Log:
a function writeResults to write results in a amat from python



Added: trunk/python_modules/plearn/utilities/write_results.py
===================================================================
--- trunk/python_modules/plearn/utilities/write_results.py	2008-06-16 20:41:53 UTC (rev 9133)
+++ trunk/python_modules/plearn/utilities/write_results.py	2008-06-17 15:51:53 UTC (rev 9134)
@@ -0,0 +1,117 @@
+import time, random, os
+
+def writeResults(argdict, costdict, results_amat):
+    """ Write the results of an experiment in a amat file,
+        managing the fact that several scripts can try yo write
+        at the same time in the amat.
+        - argdict: list of hyperparameter names and values.
+                   Must be a dictionary such as {'n_hidden':50, 'learning_rate':0.1}
+                   OR a list of lists, as [['n_hidden', 'learning_rate'],[50, 0.1]]
+                   (the interest of this last format is to keep a given order, while
+                    alphabetical order is taken with dicts).
+        - costdict: list of costs names and values (same format as argdict)
+        - results_amat: names of the amat (string)
+                        (with or without the extension 'amat')
+    """
+    argnames, argvals = getSortedValues(argdict)
+    cost_names, cost_vals = getSortedValues(costdict)
+    if not results_amat.endswith('.amat'):
+        results_amat += '.amat'
+
+    # Create .amat if it does not exist.
+    if not os.path.exists(results_amat):
+        lockFile(results_amat)
+        f = open(results_amat, "w")
+        f.write('TO_CREATE')
+        f.close()
+        unlockFile(results_amat)
+
+    # Fill data in the .amat file.
+    lockFile(results_amat)
+    f = open(results_amat, 'r+')
+    if f.readline() == 'TO_CREATE':
+        # Need to write the header.
+        f.seek(0)
+        f.write('# Fieldnames:\n')
+        f.write('#: %s %s\n' % \
+                (' '.join(argnames), ' '.join(cost_names)))
+    f.seek(0, 2)   # End of file.
+    f.write('%s\n' % ' '.join( str(x) for x in argvals + cost_vals ) )
+    f.close()
+    unlockFile(results_amat)
+
+def getSortedValues(names_and_vals):
+    if type(names_and_vals) == dict:
+        keys = names_and_vals.keys()
+        keys.sort()
+        return keys, [names_and_vals[key] for key in keys]
+    elif type(names_and_vals) == list:
+        assert len(names_and_vals) == 2
+        assert type(names_and_vals[0]) in [list,str]
+        if type(names_and_vals[0]) == str:
+            names_and_vals[0] = names_and_vals[0].split()
+        assert type(names_and_vals[1]) == list
+        assert len(names_and_vals[0]) == len(names_and_vals[1])
+        return names_and_vals[0], names_and_vals[1]
+    else:
+        raise TypeError, "argument of getSortedValues (type %s) must be a dict" % type(names_and_vals)+\
+                         " or a list of length 2 (of lists of the same length)."
+
+def lockFile(file, timeout = 30, min_wait = 1, max_wait = 5, verbosity = 0):
+    """Obtain lock access to the given file. If access is refused by the same
+    lock owner during more than 'timeout' seconds, then overrides the current
+    lock. If timeout is None, then no timeout is performed.
+    The lock is performed by created a 'file.lock' file that contains a unique
+    id identifying the owner of the lock.
+    When there is already a lock, the process sleeps for a random amount of
+    time between min_wait and max_wait seconds before trying again.
+    If 'verbosity' is set to 1, then a message will be displayed when we need
+    to wait for the lock. If it is set to a value >1, then this message will
+    be displayed each time we re-check for the presence of the lock."""
+    lock_file = file + '.lock'
+    random.seed()
+    unique_id = '%s_%s' % (os.getpid(),
+                             ''.join( [ str(random.randint(0,9)) \
+                                        for i in range(10) ]))
+    no_display = verbosity == 0
+    while True:
+        last_owner = 'no_owner'
+        time_start = time.time()
+        while os.path.isfile(lock_file):
+            try:
+                read_owner = open(lock_file).readlines()[0].strip()
+            except:
+                read_owner = 'failure'
+            if last_owner == read_owner:
+                if timeout is not None and time.time() - time_start >= timeout:
+                    # Timeout exceeded.
+                    break
+            else:
+                last_owner = read_owner
+                time_start = time.time()
+            if not no_display:
+                print 'Waiting for existing lock (by %s)' % read_owner
+                if verbosity <= 1:
+                    no_display = True
+            time.sleep(random.uniform(min_wait, max_wait))
+
+        # Write own id into lock file.
+        lock_write = open(lock_file, 'w')
+        lock_write.write(unique_id + '\n')
+        lock_write.close()
+        time.sleep(1) # Safety wait.
+        # Verify noone else tried to claim the lock at the same time.
+        owner = open(lock_file).readlines()[0].strip()
+        if owner != unique_id:
+            # Too bad, try again.
+            continue
+        else:
+            # We got the lock!
+            return
+
+
+def unlockFile(file):
+    """Remove current lock on file."""
+    lock_file = file + '.lock'
+    if os.path.exists(lock_file):
+        os.remove(lock_file)



From laulysta at mail.berlios.de  Tue Jun 17 19:10:02 2008
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Tue, 17 Jun 2008 19:10:02 +0200
Subject: [Plearn-commits] r9135 - trunk/plearn_learners_experimental
Message-ID: <200806171710.m5HHA2Nl029635@sheep.berlios.de>

Author: laulysta
Date: 2008-06-17 19:10:00 +0200 (Tue, 17 Jun 2008)
New Revision: 9135

Added:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
Log:


Added: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-06-17 15:51:53 UTC (rev 9134)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-06-17 17:10:00 UTC (rev 9135)
@@ -0,0 +1,1864 @@
+// -*- C++ -*-
+
+// DenoisingRecurrentNet.cc
+//
+// Copyright (C) 2006 Stanislas Lauly
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Stanislas Lauly
+
+/*! \file DenoisingRecurrentNet.cc */
+
+
+#define PL_LOG_MODULE_NAME "DenoisingRecurrentNet"
+#include <plearn/io/pl_log.h>
+
+#include "DenoisingRecurrentNet.h"
+#include "plearn/math/plapack.h"
+
+// - commiter mse
+// - ajouter denoising recurrent net. Deux possibilit?s:
+//   1) on ajoute du bruit ? l'input, et on reconstruit les targets avec des poids
+//      possiblement diff?rents
+//     * option denoising_target_layers_weights (c'est l? qu'on met l'input)
+//     * version de clamp_units qui ajoute le bruit
+//   2) on reconstruit l'input directement (sans 2e couche cach?e)
+//     * toujours clamp_units qui ajoute le bruit
+//     * une option qui dit quelle partie de l'input reconstruire et du code 
+//       pour bloquer le gradient qui ne doit pas passer (pas tr?s propre, 
+//       mais bon...)
+//     * une option donnant les connections de reconstruction
+//     * du code pour entra?ner s?par?ment les hidden_connections (si pr?sentes)
+// - pourrait avoir le gradient du denoising recurrent net en m?me temps que
+//   celui du "fine-tuning"
+// - add dynamic_activations_list and use it in recurrent_update
+
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    DenoisingRecurrentNet,
+    "Model made of RBMs linked through time",
+    ""
+    );
+
+
+DenoisingRecurrentNet::DenoisingRecurrentNet() :
+    //rbm_learning_rate( 0.01 ),
+    recurrent_net_learning_rate( 0.01),
+    use_target_layers_masks( false ),
+    end_of_sequence_symbol( -1000 )
+    //rbm_nstages( 0 ),
+{
+    random_gen = new PRandom();
+}
+
+void DenoisingRecurrentNet::declareOptions(OptionList& ol)
+{
+//    declareOption(ol, "rbm_learning_rate", &DenoisingRecurrentNet::rbm_learning_rate,
+//                  OptionBase::buildoption,
+//                  "The learning rate used during RBM contrastive "
+//                  "divergence learning phase.\n");
+
+    declareOption(ol, "recurrent_net_learning_rate", 
+                  &DenoisingRecurrentNet::recurrent_net_learning_rate,
+                  OptionBase::buildoption,
+                  "The learning rate used during the recurrent phase.\n");
+
+//    declareOption(ol, "rbm_nstages", &DenoisingRecurrentNet::rbm_nstages,
+//                  OptionBase::buildoption,
+//                  "Number of epochs for rbm phase.\n");
+
+
+    declareOption(ol, "target_layers_weights", 
+                  &DenoisingRecurrentNet::target_layers_weights,
+                  OptionBase::buildoption,
+                  "The training weights of each target layers.\n");
+
+    declareOption(ol, "use_target_layers_masks", 
+                  &DenoisingRecurrentNet::use_target_layers_masks,
+                  OptionBase::buildoption,
+                  "Indication that a mask indicating which target to predict\n"
+                  "is present in the input part of the VMatrix dataset.\n");
+
+    declareOption(ol, "end_of_sequence_symbol", 
+                  &DenoisingRecurrentNet::end_of_sequence_symbol,
+                  OptionBase::buildoption,
+                  "Value of the first input component for end-of-sequence "
+                  "delimiter.\n");
+
+    declareOption(ol, "input_layer", &DenoisingRecurrentNet::input_layer,
+                  OptionBase::buildoption,
+                  "The input layer of the model.\n");
+
+    declareOption(ol, "target_layers", &DenoisingRecurrentNet::target_layers,
+                  OptionBase::buildoption,
+                  "The target layers of the model.\n");
+
+    declareOption(ol, "hidden_layer", &DenoisingRecurrentNet::hidden_layer,
+                  OptionBase::buildoption,
+                  "The hidden layer of the model.\n");
+
+    declareOption(ol, "hidden_layer2", &DenoisingRecurrentNet::hidden_layer2,
+                  OptionBase::buildoption,
+                  "The second hidden layer of the model (optional).\n");
+
+    declareOption(ol, "dynamic_connections", 
+                  &DenoisingRecurrentNet::dynamic_connections,
+                  OptionBase::buildoption,
+                  "The RBMConnection between the first hidden layers, "
+                  "through time (optional).\n");
+
+    declareOption(ol, "hidden_connections", 
+                  &DenoisingRecurrentNet::hidden_connections,
+                  OptionBase::buildoption,
+                  "The RBMConnection between the first and second "
+                  "hidden layers (optional).\n");
+
+    declareOption(ol, "input_connections", 
+                  &DenoisingRecurrentNet::input_connections,
+                  OptionBase::buildoption,
+                  "The RBMConnection from input_layer to hidden_layer.\n");
+
+    declareOption(ol, "target_connections", 
+                  &DenoisingRecurrentNet::target_connections,
+                  OptionBase::buildoption,
+                  "The RBMConnection from input_layer to hidden_layer.\n");
+
+    /*
+    declareOption(ol, "", 
+                  &DenoisingRecurrentNet::,
+                  OptionBase::buildoption,
+                  "");
+    */
+
+
+    declareOption(ol, "target_layers_n_of_target_elements", 
+                  &DenoisingRecurrentNet::target_layers_n_of_target_elements,
+                  OptionBase::learntoption,
+                  "Number of elements in the target part of a VMatrix associated\n"
+                  "to each target layer.\n");
+
+    declareOption(ol, "input_symbol_sizes", 
+                  &DenoisingRecurrentNet::input_symbol_sizes,
+                  OptionBase::learntoption,
+                  "Number of symbols for each symbolic field of train_set.\n");
+
+    declareOption(ol, "target_symbol_sizes", 
+                  &DenoisingRecurrentNet::target_symbol_sizes,
+                  OptionBase::learntoption,
+                  "Number of symbols for each symbolic field of train_set.\n");
+
+    /*
+    declareOption(ol, "", &DenoisingRecurrentNet::,
+                  OptionBase::learntoption,
+                  "");
+     */
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void DenoisingRecurrentNet::build_()
+{
+    // ### This method should do the real building of the object,
+    // ### according to set 'options', in *any* situation.
+    // ### Typical situations include:
+    // ###  - Initial building of an object from a few user-specified options
+    // ###  - Building of a "reloaded" object: i.e. from the complete set of
+    // ###    all serialised options.
+    // ###  - Updating or "re-building" of an object after a few "tuning"
+    // ###    options have been modified.
+    // ### You should assume that the parent class' build_() has already been
+    // ### called.
+
+    MODULE_LOG << "build_() called" << endl;
+
+    if(train_set)
+    {
+        PLASSERT( target_layers_weights.length() == target_layers.length() );
+        PLASSERT( target_connections.length() == target_layers.length() );
+        PLASSERT( target_layers.length() > 0 );
+        PLASSERT( input_layer );
+        PLASSERT( hidden_layer );
+        PLASSERT( input_connections );
+
+        // Parsing symbols in input
+        int input_layer_size = 0;
+        input_symbol_sizes.resize(0);
+        PP<Dictionary> dict;
+        int inputsize_without_masks = inputsize() 
+            - ( use_target_layers_masks ? targetsize() : 0 );
+        for(int i=0; i<inputsize_without_masks; i++)
+        {
+            dict = train_set->getDictionary(i);
+            if(dict)
+            {
+                if( dict->size() == 0 )
+                    PLERROR("DenoisingRecurrentNet::build_(): dictionary "
+                        "of field %d is empty", i);
+                input_symbol_sizes.push_back(dict->size());
+                // Adjust size to include one-hot vector
+                input_layer_size += dict->size();
+            }
+            else
+            {
+                input_symbol_sizes.push_back(-1);
+                input_layer_size++;
+            }
+        }
+
+        if( input_layer->size != input_layer_size )
+            PLERROR("DenoisingRecurrentNet::build_(): input_layer->size %d "
+                    "should be %d", input_layer->size, input_layer_size);
+
+        // Parsing symbols in target
+        int tar_layer = 0;
+        int tar_layer_size = 0;
+        target_symbol_sizes.resize(target_layers.length());
+        for( int tar_layer=0; tar_layer<target_layers.length(); 
+             tar_layer++ )
+            target_symbol_sizes[tar_layer].resize(0);
+        target_layers_n_of_target_elements.resize( targetsize() );
+        target_layers_n_of_target_elements.clear();
+
+        for( int tar=0; tar<targetsize(); tar++)
+        {
+            if( tar_layer > target_layers.length() )
+                PLERROR("DenoisingRecurrentNet::build_(): target layers "
+                        "does not cover all targets.");            
+
+            dict = train_set->getDictionary(tar+inputsize());
+            if(dict)
+            {
+                if( use_target_layers_masks )
+                    PLERROR("DenoisingRecurrentNet::build_(): masks for "
+                            "symbolic targets is not implemented.");
+                if( dict->size() == 0 )
+                    PLERROR("DenoisingRecurrentNet::build_(): dictionary "
+                            "of field %d is empty", tar);
+
+                target_symbol_sizes[tar_layer].push_back(dict->size());
+                target_layers_n_of_target_elements[tar_layer]++;
+                tar_layer_size += dict->size();
+            }
+            else
+            {
+                target_symbol_sizes[tar_layer].push_back(-1);
+                target_layers_n_of_target_elements[tar_layer]++;
+                tar_layer_size++;
+            }
+
+            if( target_layers[tar_layer]->size == tar_layer_size )
+            {
+                tar_layer++;
+                tar_layer_size = 0;
+            }
+        }
+
+        if( tar_layer != target_layers.length() )
+            PLERROR("DenoisingRecurrentNet::build_(): target layers "
+                    "does not cover all targets.");
+
+
+        // Building weights and layers
+        if( !input_layer->random_gen )
+        {
+            input_layer->random_gen = random_gen;
+            input_layer->forget();
+        }
+
+        if( !hidden_layer->random_gen )
+        {
+            hidden_layer->random_gen = random_gen;
+            hidden_layer->forget();
+        }
+
+        input_connections->down_size = input_layer->size;
+        input_connections->up_size = hidden_layer->size;
+        if( !input_connections->random_gen )
+        {
+            input_connections->random_gen = random_gen;
+            input_connections->forget();
+        }
+        input_connections->build();
+
+
+        if( dynamic_connections )
+        {
+            dynamic_connections->down_size = hidden_layer->size;
+            dynamic_connections->up_size = hidden_layer->size;
+            if( !dynamic_connections->random_gen )
+            {
+                dynamic_connections->random_gen = random_gen;
+                dynamic_connections->forget();
+            }
+            dynamic_connections->build();
+        }
+
+        if( hidden_layer2 )
+        {
+            if( !hidden_layer2->random_gen )
+            {
+                hidden_layer2->random_gen = random_gen;
+                hidden_layer2->forget();
+            }
+
+            PLASSERT( hidden_connections );
+
+            hidden_connections->down_size = hidden_layer->size;
+            hidden_connections->up_size = hidden_layer2->size;
+            if( !hidden_connections->random_gen )
+            {
+                hidden_connections->random_gen = random_gen;
+                hidden_connections->forget();
+            }
+            hidden_connections->build();
+        }
+
+        for( int tar_layer = 0; tar_layer < target_layers.length(); tar_layer++ )
+        {
+            PLASSERT( target_layers[tar_layer] );
+            PLASSERT( target_connections[tar_layer] );
+
+            if( !target_layers[tar_layer]->random_gen )
+            {
+                target_layers[tar_layer]->random_gen = random_gen;
+                target_layers[tar_layer]->forget();
+            }
+
+            if( hidden_layer2 )
+                target_connections[tar_layer]->down_size = hidden_layer2->size;
+            else
+                target_connections[tar_layer]->down_size = hidden_layer->size;
+
+            target_connections[tar_layer]->up_size = target_layers[tar_layer]->size;
+            if( !target_connections[tar_layer]->random_gen )
+            {
+                target_connections[tar_layer]->random_gen = random_gen;
+                target_connections[tar_layer]->forget();
+            }
+            target_connections[tar_layer]->build();
+        }
+
+    }
+}
+
+// ### Nothing to add here, simply calls build_
+void DenoisingRecurrentNet::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void DenoisingRecurrentNet::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField( input_layer, copies);
+    deepCopyField( target_layers , copies);
+    deepCopyField( hidden_layer, copies);
+    deepCopyField( hidden_layer2 , copies);
+    deepCopyField( dynamic_connections , copies);
+    deepCopyField( hidden_connections , copies);
+    deepCopyField( input_connections , copies);
+    deepCopyField( target_connections , copies);
+    deepCopyField( target_layers_n_of_target_elements, copies);
+    deepCopyField( input_symbol_sizes, copies);
+    deepCopyField( target_symbol_sizes, copies);
+    
+
+    deepCopyField( bias_gradient , copies);
+    deepCopyField( visi_bias_gradient , copies);
+    deepCopyField( hidden_gradient , copies);
+    deepCopyField( hidden_temporal_gradient , copies);
+    deepCopyField( hidden_list , copies);
+    deepCopyField( hidden_act_no_bias_list , copies);
+    deepCopyField( hidden2_list , copies);
+    deepCopyField( hidden2_act_no_bias_list , copies);
+    deepCopyField( target_prediction_list , copies);
+    deepCopyField( target_prediction_act_no_bias_list , copies);
+    deepCopyField( input_list , copies);
+    deepCopyField( targets_list , copies);
+    deepCopyField( nll_list , copies);
+    deepCopyField( masks_list , copies);
+    deepCopyField( dynamic_act_no_bias_contribution, copies);
+
+
+    // deepCopyField(, copies);
+
+    //PLERROR("DenoisingRecurrentNet::makeDeepCopyFromShallowCopy(): "
+    //"not implemented yet");
+}
+
+
+int DenoisingRecurrentNet::outputsize() const
+{
+    int out_size = 0;
+    for( int i=0; i<target_layers.length(); i++ )
+        out_size += target_layers[i]->size;
+    return out_size;
+}
+
+void DenoisingRecurrentNet::forget()
+{
+    inherited::forget();
+
+    input_layer->forget();
+    hidden_layer->forget();
+    input_connections->forget();
+    if( dynamic_connections )
+        dynamic_connections->forget();
+    if( hidden_layer2 )
+    {
+        hidden_layer2->forget();
+        hidden_connections->forget();
+    }
+
+    for( int i=0; i<target_layers.length(); i++ )
+    {
+        target_layers[i]->forget();
+        target_connections[i]->forget();
+    }
+
+    stage = 0;
+}
+
+void DenoisingRecurrentNet::train()
+{
+    MODULE_LOG << "train() called " << endl;
+
+    Vec input( inputsize() );
+    Vec target( targetsize() );
+    real weight = 0; // Unused
+    Vec train_costs( getTrainCostNames().length() );
+    train_costs.clear();
+    Vec train_n_items( getTrainCostNames().length() );
+
+    if( !initTrain() )
+    {
+        MODULE_LOG << "train() aborted" << endl;
+        return;
+    }
+
+    ProgressBar* pb = 0;
+
+    // clear stats of previous epoch
+    train_stats->forget();
+
+
+    /***** RBM training phase *****/
+//    if(rbm_stage < rbm_nstages)
+//    {
+//    }
+
+
+    /***** Recurrent phase *****/
+    if( stage >= nstages )
+        return;
+
+    if( stage < nstages )
+    {        
+
+        MODULE_LOG << "Training the whole model" << endl;
+
+        int init_stage = stage;
+        //int end_stage = max(0,nstages-(rbm_nstages + dynamic_nstages));
+        int end_stage = nstages;
+
+        MODULE_LOG << "  stage = " << stage << endl;
+        MODULE_LOG << "  end_stage = " << end_stage << endl;
+        MODULE_LOG << "  recurrent_net_learning_rate = " << recurrent_net_learning_rate << endl;
+
+        if( report_progress && stage < end_stage )
+            pb = new ProgressBar( "Recurrent training phase of "+classname(),
+                                  end_stage - init_stage );
+
+        setLearningRate( recurrent_net_learning_rate );
+
+        int ith_sample_in_sequence = 0;
+        int inputsize_without_masks = inputsize() 
+            - ( use_target_layers_masks ? targetsize() : 0 );
+        int sum_target_elements = 0;
+        while(stage < end_stage)
+        {
+/*
+                TMat<real> U,V;//////////crap James
+                TVec<real> S;
+                U.resize(hidden_layer->size,hidden_layer->size);
+                V.resize(hidden_layer->size,hidden_layer->size);
+                S.resize(hidden_layer->size);
+                U << dynamic_connections->weights;
+                
+                SVD(U,dynamic_connections->weights,S,V);
+                S.fill(-0.5);
+                productScaleAcc(dynamic_connections->bias,dynamic_connections->weights,S,1,0);
+*/
+            train_costs.clear();
+            train_n_items.clear();
+            for(int sample=0 ; sample<train_set->length() ; sample++ )
+            {
+                train_set->getExample(sample, input, target, weight);
+
+                if( fast_exact_is_equal(input[0],end_of_sequence_symbol) )
+                {
+                    //update
+                    recurrent_update();
+                    
+                    ith_sample_in_sequence = 0;
+                    hidden_list.resize(0);
+                    hidden_act_no_bias_list.resize(0);
+                    hidden2_list.resize(0);
+                    hidden2_act_no_bias_list.resize(0);
+                    target_prediction_list.resize(0);
+                    target_prediction_act_no_bias_list.resize(0);
+                    input_list.resize(0);
+                    targets_list.resize(0);
+                    nll_list.resize(0,0);
+                    masks_list.resize(0);
+                    continue;
+                }
+
+                // Resize internal variables
+                hidden_list.resize(ith_sample_in_sequence+1);
+                hidden_act_no_bias_list.resize(ith_sample_in_sequence+1);
+                if( hidden_layer2 )
+                {
+                    hidden2_list.resize(ith_sample_in_sequence+1);
+                    hidden2_act_no_bias_list.resize(ith_sample_in_sequence+1);
+                }
+                 
+                input_list.resize(ith_sample_in_sequence+1);
+                input_list[ith_sample_in_sequence].resize(input_layer->size);
+
+                targets_list.resize( target_layers.length() );
+                target_prediction_list.resize( target_layers.length() );
+                target_prediction_act_no_bias_list.resize( target_layers.length() );
+                for( int tar=0; tar < target_layers.length(); tar++ )
+                {
+                    if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                    {                        
+                        targets_list[tar].resize( ith_sample_in_sequence+1);
+                        targets_list[tar][ith_sample_in_sequence].resize( 
+                            target_layers[tar]->size);
+                        target_prediction_list[tar].resize(
+                            ith_sample_in_sequence+1);
+                        target_prediction_act_no_bias_list[tar].resize(
+                            ith_sample_in_sequence+1);
+                    }
+                }
+                nll_list.resize(ith_sample_in_sequence+1,target_layers.length());
+                if( use_target_layers_masks )
+                {
+                    masks_list.resize( target_layers.length() );
+                    for( int tar=0; tar < target_layers.length(); tar++ )
+                        if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                            masks_list[tar].resize( ith_sample_in_sequence+1 );
+                }
+
+                // Forward propagation
+
+                // Fetch right representation for input
+                clamp_units(input.subVec(0,inputsize_without_masks),
+                            input_layer,
+                            input_symbol_sizes);                
+                input_list[ith_sample_in_sequence] << input_layer->expectation;
+
+                // Fetch right representation for target
+                sum_target_elements = 0;
+                for( int tar=0; tar < target_layers.length(); tar++ )
+                {
+                    if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                    {
+                        if( use_target_layers_masks )
+                        {
+                            clamp_units(target.subVec(
+                                            sum_target_elements,
+                                            target_layers_n_of_target_elements[tar]),
+                                        target_layers[tar],
+                                        target_symbol_sizes[tar],
+                                        input.subVec(
+                                            inputsize_without_masks 
+                                            + sum_target_elements, 
+                                            target_layers_n_of_target_elements[tar]),
+                                        masks_list[tar][ith_sample_in_sequence]
+                                );
+                            
+                        }
+                        else
+                        {
+                            clamp_units(target.subVec(
+                                            sum_target_elements,
+                                            target_layers_n_of_target_elements[tar]),
+                                        target_layers[tar],
+                                        target_symbol_sizes[tar]);
+                        }
+                        targets_list[tar][ith_sample_in_sequence] << 
+                            target_layers[tar]->expectation;
+                    }
+                    sum_target_elements += target_layers_n_of_target_elements[tar];
+                }
+                
+                input_connections->fprop( input_list[ith_sample_in_sequence], 
+                                          hidden_act_no_bias_list[ith_sample_in_sequence]);
+                
+                if( ith_sample_in_sequence > 0 && dynamic_connections )
+                {
+                    dynamic_connections->fprop( 
+                        hidden_list[ith_sample_in_sequence-1],
+                        dynamic_act_no_bias_contribution );
+
+                    hidden_act_no_bias_list[ith_sample_in_sequence] += 
+                        dynamic_act_no_bias_contribution;
+                }
+                 
+                hidden_layer->fprop( hidden_act_no_bias_list[ith_sample_in_sequence], 
+                                     hidden_list[ith_sample_in_sequence] );
+                 
+                if( hidden_layer2 )
+                {
+                    hidden_connections->fprop( 
+                        hidden_list[ith_sample_in_sequence],
+                        hidden2_act_no_bias_list[ith_sample_in_sequence]);
+
+                    hidden_layer2->fprop( 
+                        hidden2_act_no_bias_list[ith_sample_in_sequence],
+                        hidden2_list[ith_sample_in_sequence] 
+                        );
+
+                    for( int tar=0; tar < target_layers.length(); tar++ )
+                    {
+                        if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                        {
+                            target_connections[tar]->fprop(
+                                hidden2_list[ith_sample_in_sequence],
+                                target_prediction_act_no_bias_list[tar][
+                                    ith_sample_in_sequence]
+                                );
+                            target_layers[tar]->fprop(
+                                target_prediction_act_no_bias_list[tar][
+                                    ith_sample_in_sequence],
+                                target_prediction_list[tar][
+                                    ith_sample_in_sequence] );
+                            if( use_target_layers_masks )
+                                target_prediction_list[tar][ ith_sample_in_sequence] *= 
+                                    masks_list[tar][ith_sample_in_sequence];
+                        }
+                    }
+                }
+                else
+                {
+                    for( int tar=0; tar < target_layers.length(); tar++ )
+                    {
+                        if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                        {
+                            target_connections[tar]->fprop(
+                                hidden_list[ith_sample_in_sequence],
+                                target_prediction_act_no_bias_list[tar][
+                                    ith_sample_in_sequence]
+                                );
+                            target_layers[tar]->fprop(
+                                target_prediction_act_no_bias_list[tar][
+                                    ith_sample_in_sequence],
+                                target_prediction_list[tar][
+                                    ith_sample_in_sequence] );
+                            if( use_target_layers_masks )
+                                target_prediction_list[tar][ ith_sample_in_sequence] *= 
+                                    masks_list[tar][ith_sample_in_sequence];
+                        }
+                    }
+                }
+
+                sum_target_elements = 0;
+                for( int tar=0; tar < target_layers.length(); tar++ )
+                {
+                    if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                    {
+                        target_layers[tar]->activation << 
+                            target_prediction_act_no_bias_list[tar][
+                                ith_sample_in_sequence];
+                        target_layers[tar]->activation += target_layers[tar]->bias;
+                        target_layers[tar]->setExpectation(
+                            target_prediction_list[tar][
+                                ith_sample_in_sequence]);
+                        nll_list(ith_sample_in_sequence,tar) = 
+                            target_layers[tar]->fpropNLL( 
+                                targets_list[tar][ith_sample_in_sequence] ); 
+                        train_costs[tar] += nll_list(ith_sample_in_sequence,tar);
+                        
+                        // Normalize by the number of things to predict
+                        if( use_target_layers_masks )
+                        {
+                            train_n_items[tar] += sum(
+                                input.subVec( inputsize_without_masks 
+                                              + sum_target_elements, 
+                                              target_layers_n_of_target_elements[tar]) );
+                        }
+                        else
+                            train_n_items[tar]++;
+                    }
+                    if( use_target_layers_masks )
+                        sum_target_elements += 
+                            target_layers_n_of_target_elements[tar];
+                    
+                }
+                ith_sample_in_sequence++;
+            }
+            if( pb )
+                pb->update( stage + 1 - init_stage);
+            
+            for(int i=0; i<train_costs.length(); i++)
+            {
+                if( !fast_exact_is_equal(target_layers_weights[i],0) )
+                    train_costs[i] /= train_n_items[i];
+                else
+                    train_costs[i] = MISSING_VALUE;
+            }
+
+            if(verbosity>0)
+                cout << "mean costs at stage " << stage << 
+                    " = " << train_costs << endl;
+            stage++;
+            train_stats->update(train_costs);
+        }    
+        if( pb )
+        {
+            delete pb;
+            pb = 0;
+        }
+
+    }
+
+
+    train_stats->finalize();
+}
+
+
+
+void DenoisingRecurrentNet::clamp_units(const Vec layer_vector,
+                                             PP<RBMLayer> layer,
+                                             TVec<int> symbol_sizes) const
+{
+    int it = 0;
+    int ss = -1;
+    for(int i=0; i<layer_vector.length(); i++)
+    {
+        ss = symbol_sizes[i];
+        // If input is a real ...
+        if(ss < 0) 
+        {
+            layer->expectation[it++] = layer_vector[i];
+        }
+        else // ... or a symbol
+        {
+            // Convert to one-hot vector
+            layer->expectation.subVec(it,ss).clear();
+            layer->expectation[it+(int)layer_vector[i]] = 1;
+            it += ss;
+        }
+    }
+    layer->setExpectation( layer->expectation );
+}
+
+void DenoisingRecurrentNet::clamp_units(const Vec layer_vector,
+                                             PP<RBMLayer> layer,
+                                             TVec<int> symbol_sizes,
+                                             const Vec original_mask,
+                                             Vec& formated_mask) const
+{
+    int it = 0;
+    int ss = -1;
+    PLASSERT( original_mask.length() == layer_vector.length() );
+    formated_mask.resize(layer->size);
+    for(int i=0; i<layer_vector.length(); i++)
+    {
+        ss = symbol_sizes[i];
+        // If input is a real ...
+        if(ss < 0) 
+        {
+            formated_mask[it] = original_mask[i];
+            layer->expectation[it++] = layer_vector[i];
+        }
+        else // ... or a symbol
+        {
+            // Convert to one-hot vector
+            layer->expectation.subVec(it,ss).clear();
+            formated_mask.subVec(it,ss).fill(original_mask[i]);
+            layer->expectation[it+(int)layer_vector[i]] = 1;
+            it += ss;
+        }
+    }
+    layer->setExpectation( layer->expectation );
+}
+
+void DenoisingRecurrentNet::setLearningRate( real the_learning_rate )
+{
+    input_layer->setLearningRate( the_learning_rate );
+    hidden_layer->setLearningRate( the_learning_rate );
+    input_connections->setLearningRate( the_learning_rate );
+    if( dynamic_connections )
+        dynamic_connections->setLearningRate( the_learning_rate ); //HUGO: multiply by dynamic_connections_learning_weight;
+    if( hidden_layer2 )
+    {
+        hidden_layer2->setLearningRate( the_learning_rate );
+        hidden_connections->setLearningRate( the_learning_rate );
+    }
+
+    for( int i=0; i<target_layers.length(); i++ )
+    {
+        target_layers[i]->setLearningRate( the_learning_rate );
+        target_connections[i]->setLearningRate( the_learning_rate );
+    }
+}
+
+void DenoisingRecurrentNet::recurrent_update()
+{
+        hidden_temporal_gradient.resize(hidden_layer->size);
+        hidden_temporal_gradient.clear();
+        for(int i=hidden_list.length()-1; i>=0; i--){   
+
+            if( hidden_layer2 )
+                hidden_gradient.resize(hidden_layer2->size);
+            else
+                hidden_gradient.resize(hidden_layer->size);
+            hidden_gradient.clear();
+            if(use_target_layers_masks)
+            {
+                for( int tar=0; tar<target_layers.length(); tar++)
+                {
+                    if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                    {
+                        target_layers[tar]->activation << target_prediction_act_no_bias_list[tar][i];
+                        target_layers[tar]->activation += target_layers[tar]->bias;
+                        target_layers[tar]->setExpectation(target_prediction_list[tar][i]);
+                        target_layers[tar]->bpropNLL(targets_list[tar][i],nll_list(i,tar),bias_gradient);
+                        bias_gradient *= target_layers_weights[tar];
+                        bias_gradient *= masks_list[tar][i];
+                        target_layers[tar]->update(bias_gradient);
+                        if( hidden_layer2 )
+                            target_connections[tar]->bpropUpdate(hidden2_list[i],target_prediction_act_no_bias_list[tar][i],
+                                                                 hidden_gradient, bias_gradient,true);
+                        else
+                            target_connections[tar]->bpropUpdate(hidden_list[i],target_prediction_act_no_bias_list[tar][i],
+                                                                 hidden_gradient, bias_gradient,true);
+                    }
+                }
+            }
+            else
+            {
+                for( int tar=0; tar<target_layers.length(); tar++)
+                {
+                    if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                    {
+                        target_layers[tar]->activation << target_prediction_act_no_bias_list[tar][i];
+                        target_layers[tar]->activation += target_layers[tar]->bias;
+                        target_layers[tar]->setExpectation(target_prediction_list[tar][i]);
+                        target_layers[tar]->bpropNLL(targets_list[tar][i],nll_list(i,tar),bias_gradient);
+                        bias_gradient *= target_layers_weights[tar];
+                        target_layers[tar]->update(bias_gradient);
+                        if( hidden_layer2 )
+                            target_connections[tar]->bpropUpdate(hidden2_list[i],target_prediction_act_no_bias_list[tar][i],
+                                                                 hidden_gradient, bias_gradient,true); 
+                        else
+                            target_connections[tar]->bpropUpdate(hidden_list[i],target_prediction_act_no_bias_list[tar][i],
+                                                                 hidden_gradient, bias_gradient,true); 
+                        
+                    }
+                }
+            }
+
+            if (hidden_layer2)
+            {
+                hidden_layer2->bpropUpdate(
+                    hidden2_act_no_bias_list[i], hidden2_list[i],
+                    bias_gradient, hidden_gradient);
+                
+                hidden_connections->bpropUpdate(
+                    hidden_list[i],
+                    hidden2_act_no_bias_list[i], 
+                    hidden_gradient, bias_gradient);
+            }
+            
+            if(i!=0 && dynamic_connections )
+            {   
+                hidden_gradient += hidden_temporal_gradient;
+                
+                hidden_layer->bpropUpdate(
+                    hidden_act_no_bias_list[i], hidden_list[i],
+                    hidden_temporal_gradient, hidden_gradient);
+                
+                dynamic_connections->bpropUpdate(
+                    hidden_list[i-1],
+                    hidden_act_no_bias_list[i], // Here, it should be cond_bias, but doesn't matter
+                    hidden_gradient, hidden_temporal_gradient);
+                
+                hidden_temporal_gradient << hidden_gradient;
+                
+                input_connections->bpropUpdate(
+                    input_list[i],
+                    hidden_act_no_bias_list[i], 
+                    visi_bias_gradient, hidden_temporal_gradient);// Here, it should be activations - cond_bias, but doesn't matter
+                
+            }
+            else
+            {
+                hidden_layer->bpropUpdate(
+                    hidden_act_no_bias_list[i], hidden_list[i],
+                    hidden_temporal_gradient, hidden_gradient); // Not really temporal gradient, but this is the final iteration...
+                input_connections->bpropUpdate(
+                    input_list[i],
+                    hidden_act_no_bias_list[i], 
+                    visi_bias_gradient, hidden_temporal_gradient);// Here, it should be activations - cond_bias, but doesn't matter
+
+            }
+        }
+    
+}
+
+void DenoisingRecurrentNet::computeOutput(const Vec& input, Vec& output) const
+{
+    PLERROR("DenoisingRecurrentNet::computeOutput(): this is a dynamic, "
+            "generative model, that can only compute negative log-likelihood "
+            "costs for a whole VMat");
+}
+
+void DenoisingRecurrentNet::computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                           const Vec& target, Vec& costs) const
+{
+    PLERROR("DenoisingRecurrentNet::computeCostsFromOutputs(): this is a "
+            "dynamic, generative model, that can only compute negative "
+            "log-likelihooh costs for a whole VMat");
+}
+
+void DenoisingRecurrentNet::test(VMat testset, PP<VecStatsCollector> test_stats,
+                  VMat testoutputs, VMat testcosts)const
+{ 
+
+    int len = testset.length();
+    Vec input;
+    Vec target;
+    real weight;
+
+    Vec output(outputsize());
+    output.clear();
+    Vec costs(nTestCosts());
+    costs.clear();
+    Vec n_items(nTestCosts());
+    n_items.clear();
+
+    PP<ProgressBar> pb;
+    if (report_progress) 
+        pb = new ProgressBar("Testing learner", len);
+
+    if (len == 0) {
+        // Empty test set: we give -1 cost arbitrarily.
+        costs.fill(-1);
+        test_stats->update(costs);
+    }
+    
+    int ith_sample_in_sequence = 0;
+    int inputsize_without_masks = inputsize() 
+        - ( use_target_layers_masks ? targetsize() : 0 );
+    int sum_target_elements = 0;
+    for (int i = 0; i < len; i++)
+    {
+        testset.getExample(i, input, target, weight);
+
+        if( fast_exact_is_equal(input[0],end_of_sequence_symbol) )
+        {
+            ith_sample_in_sequence = 0;
+            hidden_list.resize(0);
+            hidden_act_no_bias_list.resize(0);
+            hidden2_list.resize(0);
+            hidden2_act_no_bias_list.resize(0);
+            target_prediction_list.resize(0);
+            target_prediction_act_no_bias_list.resize(0);
+            input_list.resize(0);
+            targets_list.resize(0);
+            nll_list.resize(0,0);
+            masks_list.resize(0);
+
+            if (testoutputs)
+            {
+                output.fill(end_of_sequence_symbol);
+                testoutputs->putOrAppendRow(i, output);
+            }
+
+            continue;
+        }
+
+        // Resize internal variables
+        hidden_list.resize(ith_sample_in_sequence+1);
+        hidden_act_no_bias_list.resize(ith_sample_in_sequence+1);
+        if( hidden_layer2 )
+        {
+            hidden2_list.resize(ith_sample_in_sequence+1);
+            hidden2_act_no_bias_list.resize(ith_sample_in_sequence+1);
+        }
+                 
+        input_list.resize(ith_sample_in_sequence+1);
+        input_list[ith_sample_in_sequence].resize(input_layer->size);
+
+        targets_list.resize( target_layers.length() );
+        target_prediction_list.resize( target_layers.length() );
+        target_prediction_act_no_bias_list.resize( target_layers.length() );
+        for( int tar=0; tar < target_layers.length(); tar++ )
+        {
+            if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+            {
+                targets_list[tar].resize( ith_sample_in_sequence+1);
+                targets_list[tar][ith_sample_in_sequence].resize( 
+                    target_layers[tar]->size);
+                target_prediction_list[tar].resize(
+                    ith_sample_in_sequence+1);
+                target_prediction_act_no_bias_list[tar].resize(
+                    ith_sample_in_sequence+1);
+            }
+        }
+        nll_list.resize(ith_sample_in_sequence+1,target_layers.length());
+        if( use_target_layers_masks )
+        {
+            masks_list.resize( target_layers.length() );
+            for( int tar=0; tar < target_layers.length(); tar++ )
+                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                    masks_list[tar].resize( ith_sample_in_sequence+1 );
+        }
+
+        // Forward propagation
+
+        // Fetch right representation for input
+        clamp_units(input.subVec(0,inputsize_without_masks),
+                    input_layer,
+                    input_symbol_sizes);                
+        input_list[ith_sample_in_sequence] << input_layer->expectation;
+
+        // Fetch right representation for target
+        sum_target_elements = 0;
+        for( int tar=0; tar < target_layers.length(); tar++ )
+        {
+            if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+            {
+                if( use_target_layers_masks )
+                {
+                    clamp_units(target.subVec(
+                                    sum_target_elements,
+                                    target_layers_n_of_target_elements[tar]),
+                                target_layers[tar],
+                                target_symbol_sizes[tar],
+                                input.subVec(
+                                    inputsize_without_masks 
+                                    + sum_target_elements, 
+                                    target_layers_n_of_target_elements[tar]),
+                                masks_list[tar][ith_sample_in_sequence]
+                        );
+                    
+                }
+                else
+                {
+                    clamp_units(target.subVec(
+                                    sum_target_elements,
+                                    target_layers_n_of_target_elements[tar]),
+                                target_layers[tar],
+                                target_symbol_sizes[tar]);
+                }
+                targets_list[tar][ith_sample_in_sequence] << 
+                    target_layers[tar]->expectation;
+            }
+            sum_target_elements += target_layers_n_of_target_elements[tar];
+        }
+                
+        input_connections->fprop( input_list[ith_sample_in_sequence], 
+                                  hidden_act_no_bias_list[ith_sample_in_sequence]);
+                
+        if( ith_sample_in_sequence > 0 && dynamic_connections )
+        {
+            dynamic_connections->fprop( 
+                hidden_list[ith_sample_in_sequence-1],
+                dynamic_act_no_bias_contribution );
+
+            hidden_act_no_bias_list[ith_sample_in_sequence] += 
+                dynamic_act_no_bias_contribution;
+        }
+                 
+        hidden_layer->fprop( hidden_act_no_bias_list[ith_sample_in_sequence], 
+                             hidden_list[ith_sample_in_sequence] );
+                 
+        if( hidden_layer2 )
+        {
+            hidden_connections->fprop( 
+                hidden_list[ith_sample_in_sequence],
+                hidden2_act_no_bias_list[ith_sample_in_sequence]);
+
+            hidden_layer2->fprop( 
+                hidden2_act_no_bias_list[ith_sample_in_sequence],
+                hidden2_list[ith_sample_in_sequence] 
+                );
+
+            for( int tar=0; tar < target_layers.length(); tar++ )
+            {
+                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                {
+                    target_connections[tar]->fprop(
+                        hidden2_list[ith_sample_in_sequence],
+                        target_prediction_act_no_bias_list[tar][
+                            ith_sample_in_sequence]
+                        );
+                    target_layers[tar]->fprop(
+                        target_prediction_act_no_bias_list[tar][
+                            ith_sample_in_sequence],
+                        target_prediction_list[tar][
+                            ith_sample_in_sequence] );
+                    if( use_target_layers_masks )
+                        target_prediction_list[tar][ ith_sample_in_sequence] *= 
+                            masks_list[tar][ith_sample_in_sequence];
+                }
+            }
+        }
+        else
+        {
+            for( int tar=0; tar < target_layers.length(); tar++ )
+            {
+                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                {
+                    target_connections[tar]->fprop(
+                        hidden_list[ith_sample_in_sequence],
+                        target_prediction_act_no_bias_list[tar][
+                            ith_sample_in_sequence]
+                        );
+                    target_layers[tar]->fprop(
+                        target_prediction_act_no_bias_list[tar][
+                            ith_sample_in_sequence],
+                        target_prediction_list[tar][
+                            ith_sample_in_sequence] );
+                    if( use_target_layers_masks )
+                        target_prediction_list[tar][ ith_sample_in_sequence] *= 
+                            masks_list[tar][ith_sample_in_sequence];
+                }
+            }
+        }
+
+        if (testoutputs)
+        {
+            int sum_target_layers_size = 0;
+            for( int tar=0; tar < target_layers.length(); tar++ )
+            {
+                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                {
+                    output.subVec(sum_target_layers_size,target_layers[tar]->size)
+                        << target_prediction_list[tar][ ith_sample_in_sequence ];
+                }
+                sum_target_layers_size += target_layers[tar]->size;
+            }
+            testoutputs->putOrAppendRow(i, output);
+        }
+
+        sum_target_elements = 0;
+        for( int tar=0; tar < target_layers.length(); tar++ )
+        {
+            if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+            {
+                target_layers[tar]->activation << 
+                    target_prediction_act_no_bias_list[tar][
+                        ith_sample_in_sequence];
+                target_layers[tar]->activation += target_layers[tar]->bias;
+                target_layers[tar]->setExpectation(
+                    target_prediction_list[tar][
+                        ith_sample_in_sequence]);
+                nll_list(ith_sample_in_sequence,tar) = 
+                    target_layers[tar]->fpropNLL( 
+                        targets_list[tar][ith_sample_in_sequence] ); 
+                costs[tar] += nll_list(ith_sample_in_sequence,tar);
+                
+                // Normalize by the number of things to predict
+                if( use_target_layers_masks )
+                {
+                    n_items[tar] += sum(
+                        input.subVec( inputsize_without_masks 
+                                      + sum_target_elements, 
+                                      target_layers_n_of_target_elements[tar]) );
+                }
+                else
+                    n_items[tar]++;
+            }
+            if( use_target_layers_masks )
+                sum_target_elements += 
+                    target_layers_n_of_target_elements[tar];
+        }
+        ith_sample_in_sequence++;
+
+        if (report_progress)
+            pb->update(i);
+
+    }
+
+    for(int i=0; i<costs.length(); i++)
+    {
+        if( !fast_exact_is_equal(target_layers_weights[i],0) )
+            costs[i] /= n_items[i];
+        else
+            costs[i] = MISSING_VALUE;
+    }
+    if (testcosts)
+        testcosts->putOrAppendRow(0, costs);
+    
+    if (test_stats)
+        test_stats->update(costs, weight);
+    
+    ith_sample_in_sequence = 0;
+    hidden_list.resize(0);
+    hidden_act_no_bias_list.resize(0);
+    hidden2_list.resize(0);
+    hidden2_act_no_bias_list.resize(0);
+    target_prediction_list.resize(0);
+    target_prediction_act_no_bias_list.resize(0);
+    input_list.resize(0);
+    targets_list.resize(0);
+    nll_list.resize(0,0);
+    masks_list.resize(0);   
+}
+
+
+TVec<string> DenoisingRecurrentNet::getTestCostNames() const
+{
+    TVec<string> cost_names(0);
+    for( int i=0; i<target_layers.length(); i++ )
+        cost_names.append("target" + tostring(i) + ".NLL");
+    return cost_names;
+}
+
+TVec<string> DenoisingRecurrentNet::getTrainCostNames() const
+{
+    return getTestCostNames();
+}
+
+void DenoisingRecurrentNet::generate(int t, int n)
+{
+    //PPath* the_filename = "/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/generate/scoreGen.amat";
+    data = new AutoVMatrix();
+    //data->filename = "/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/listData/target_tm12_input_t_tm12_tp12/scoreGen_tar_tm12__in_tm12_tp12.amat";
+    data->filename = "/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/create_data/scoreGenSuitePerf.amat";
+
+    data->defineSizes(208,16,0);
+    //data->inputsize = 21;
+    //data->targetsize = 0;
+    //data->weightsize = 0;
+    data->build();
+
+    
+    
+   
+   
+
+    int len = data->length();
+    int tarSize = outputsize();
+    int partTarSize;
+    Vec input;
+    Vec target;
+    real weight;
+
+    Vec output(outputsize());
+    output.clear();
+    /*Vec costs(nTestCosts());
+    costs.clear();
+    Vec n_items(nTestCosts());
+    n_items.clear();*/
+
+    int r,r2;
+    
+    int ith_sample_in_sequence = 0;
+    int inputsize_without_masks = inputsize() 
+        - ( use_target_layers_masks ? targetsize() : 0 );
+    int sum_target_elements = 0;
+    for (int i = 0; i < len; i++)
+    {
+        data->getExample(i, input, target, weight);
+        if(i>n)
+        {
+            for (int k = 1; k <= t; k++)
+            {
+                if(k<=i){
+                    partTarSize = outputsize();
+                    for( int tar=0; tar < target_layers.length(); tar++ )
+                    {
+                        
+                        input.subVec(inputsize_without_masks-(tarSize*(t-k))-partTarSize-1,target_layers[tar]->size) << target_prediction_list[tar][ith_sample_in_sequence-k];
+                        partTarSize -= target_layers[tar]->size;
+                        
+                        
+                    }
+                }
+            }       
+        }
+    
+/*
+        for (int k = 1; k <= t; k++)
+        {
+            partTarSize = outputsize();
+            for( int tar=0; tar < target_layers.length(); tar++ )
+            {
+                if(i>=t){
+                    input.subVec(inputsize_without_masks-(tarSize*(t-k))-partTarSize-1,target_layers[tar]->size) << target_prediction_list[tar][ith_sample_in_sequence-k];
+                    partTarSize -= target_layers[tar]->size;
+                }
+            }
+        }
+*/
+        if( fast_exact_is_equal(input[0],end_of_sequence_symbol) )
+        {
+            /*  ith_sample_in_sequence = 0;
+            hidden_list.resize(0);
+            hidden_act_no_bias_list.resize(0);
+            hidden2_list.resize(0);
+            hidden2_act_no_bias_list.resize(0);
+            target_prediction_list.resize(0);
+            target_prediction_act_no_bias_list.resize(0);
+            input_list.resize(0);
+            targets_list.resize(0);
+            nll_list.resize(0,0);
+            masks_list.resize(0);*/
+
+            
+
+            continue;
+        }
+
+        // Resize internal variables
+        hidden_list.resize(ith_sample_in_sequence+1);
+        hidden_act_no_bias_list.resize(ith_sample_in_sequence+1);
+        if( hidden_layer2 )
+        {
+            hidden2_list.resize(ith_sample_in_sequence+1);
+            hidden2_act_no_bias_list.resize(ith_sample_in_sequence+1);
+        }
+                 
+        input_list.resize(ith_sample_in_sequence+1);
+        input_list[ith_sample_in_sequence].resize(input_layer->size);
+
+        targets_list.resize( target_layers.length() );
+        target_prediction_list.resize( target_layers.length() );
+        target_prediction_act_no_bias_list.resize( target_layers.length() );
+        for( int tar=0; tar < target_layers.length(); tar++ )
+        {
+            if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+            {
+                targets_list[tar].resize( ith_sample_in_sequence+1);
+                targets_list[tar][ith_sample_in_sequence].resize( 
+                    target_layers[tar]->size);
+                target_prediction_list[tar].resize(
+                    ith_sample_in_sequence+1);
+                target_prediction_act_no_bias_list[tar].resize(
+                    ith_sample_in_sequence+1);
+            }
+        }
+        nll_list.resize(ith_sample_in_sequence+1,target_layers.length());
+        if( use_target_layers_masks )
+        {
+            masks_list.resize( target_layers.length() );
+            for( int tar=0; tar < target_layers.length(); tar++ )
+                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                    masks_list[tar].resize( ith_sample_in_sequence+1 );
+        }
+
+        // Forward propagation
+
+        // Fetch right representation for input
+        clamp_units(input.subVec(0,inputsize_without_masks),
+                    input_layer,
+                    input_symbol_sizes);                
+        input_list[ith_sample_in_sequence] << input_layer->expectation;
+
+        // Fetch right representation for target
+        sum_target_elements = 0;
+        for( int tar=0; tar < target_layers.length(); tar++ )
+        {
+            if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+            {
+                if( use_target_layers_masks )
+                {
+                    clamp_units(target.subVec(
+                                    sum_target_elements,
+                                    target_layers_n_of_target_elements[tar]),
+                                target_layers[tar],
+                                target_symbol_sizes[tar],
+                                input.subVec(
+                                    inputsize_without_masks 
+                                    + sum_target_elements, 
+                                    target_layers_n_of_target_elements[tar]),
+                                masks_list[tar][ith_sample_in_sequence]
+                        );
+                    
+                }
+                else
+                {
+                    clamp_units(target.subVec(
+                                    sum_target_elements,
+                                    target_layers_n_of_target_elements[tar]),
+                                target_layers[tar],
+                                target_symbol_sizes[tar]);
+                }
+                targets_list[tar][ith_sample_in_sequence] << 
+                    target_layers[tar]->expectation;
+            }
+            sum_target_elements += target_layers_n_of_target_elements[tar];
+        }
+                
+        input_connections->fprop( input_list[ith_sample_in_sequence], 
+                                  hidden_act_no_bias_list[ith_sample_in_sequence]);
+                
+        if( ith_sample_in_sequence > 0 && dynamic_connections )
+        {
+            dynamic_connections->fprop( 
+                hidden_list[ith_sample_in_sequence-1],
+                dynamic_act_no_bias_contribution );
+
+            hidden_act_no_bias_list[ith_sample_in_sequence] += 
+                dynamic_act_no_bias_contribution;
+        }
+                 
+        hidden_layer->fprop( hidden_act_no_bias_list[ith_sample_in_sequence], 
+                             hidden_list[ith_sample_in_sequence] );
+                 
+        if( hidden_layer2 )
+        {
+            hidden_connections->fprop( 
+                hidden_list[ith_sample_in_sequence],
+                hidden2_act_no_bias_list[ith_sample_in_sequence]);
+
+            hidden_layer2->fprop( 
+                hidden2_act_no_bias_list[ith_sample_in_sequence],
+                hidden2_list[ith_sample_in_sequence] 
+                );
+
+            for( int tar=0; tar < target_layers.length(); tar++ )
+            {
+                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                {
+                    target_connections[tar]->fprop(
+                        hidden2_list[ith_sample_in_sequence],
+                        target_prediction_act_no_bias_list[tar][
+                            ith_sample_in_sequence]
+                        );
+                    target_layers[tar]->fprop(
+                        target_prediction_act_no_bias_list[tar][
+                            ith_sample_in_sequence],
+                        target_prediction_list[tar][
+                            ith_sample_in_sequence] );
+                    if( use_target_layers_masks )
+                        target_prediction_list[tar][ ith_sample_in_sequence] *= 
+                            masks_list[tar][ith_sample_in_sequence];
+                }
+            }
+        }
+        else
+        {
+            for( int tar=0; tar < target_layers.length(); tar++ )
+            {
+                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                {
+                    target_connections[tar]->fprop(
+                        hidden_list[ith_sample_in_sequence],
+                        target_prediction_act_no_bias_list[tar][
+                            ith_sample_in_sequence]
+                        );
+                    target_layers[tar]->fprop(
+                        target_prediction_act_no_bias_list[tar][
+                            ith_sample_in_sequence],
+                        target_prediction_list[tar][
+                            ith_sample_in_sequence] );
+                    if( use_target_layers_masks )
+                        target_prediction_list[tar][ ith_sample_in_sequence] *= 
+                            masks_list[tar][ith_sample_in_sequence];
+                }
+            }
+        }
+
+        
+
+        sum_target_elements = 0;
+        for( int tar=0; tar < target_layers.length(); tar++ )
+        {
+            if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+            {
+                target_layers[tar]->activation << 
+                    target_prediction_act_no_bias_list[tar][
+                        ith_sample_in_sequence];
+                target_layers[tar]->activation += target_layers[tar]->bias;
+                target_layers[tar]->setExpectation(
+                    target_prediction_list[tar][
+                        ith_sample_in_sequence]);
+                nll_list(ith_sample_in_sequence,tar) = 
+                    target_layers[tar]->fpropNLL( 
+                        targets_list[tar][ith_sample_in_sequence] ); 
+                /*costs[tar] += nll_list(ith_sample_in_sequence,tar);
+                
+                // Normalize by the number of things to predict
+                if( use_target_layers_masks )
+                {
+                    n_items[tar] += sum(
+                        input.subVec( inputsize_without_masks 
+                                      + sum_target_elements, 
+                                      target_layers_n_of_target_elements[tar]) );
+                }
+                else
+                n_items[tar]++;*/
+            }
+            if( use_target_layers_masks )
+                sum_target_elements += 
+                    target_layers_n_of_target_elements[tar];
+        }
+        ith_sample_in_sequence++;
+
+        
+
+    }
+
+    /*  
+    ith_sample_in_sequence = 0;
+    hidden_list.resize(0);
+    hidden_act_no_bias_list.resize(0);
+    hidden2_list.resize(0);
+    hidden2_act_no_bias_list.resize(0);
+    target_prediction_list.resize(0);
+    target_prediction_act_no_bias_list.resize(0);
+    input_list.resize(0);
+    targets_list.resize(0);
+    nll_list.resize(0,0);
+    masks_list.resize(0);   
+
+
+    */
+
+
+
+
+
+
+
+
+
+    
+    //Vec tempo;
+    //TVec<real> tempo;
+    //tempo.resize(visible_layer->size);
+    ofstream myfile;
+    myfile.open ("/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/generate/test.txt");
+    
+    for (int i = 0; i < target_prediction_list[0].length() ; i++ ){
+       
+       
+        for( int tar=0; tar < target_layers.length(); tar++ )
+        {
+            for (int j = 0; j < target_prediction_list[tar][i].length() ; j++ ){
+                
+                if(i>n){
+                    myfile << target_prediction_list[tar][i][j] << " ";
+                }
+                else{
+                    myfile << targets_list[tar][i][j] << " ";
+                }
+                       
+           
+            }
+        }
+        myfile << "\n";
+    }
+     
+
+     myfile.close();
+
+}
+/*
+void DenoisingRecurrentNet::gen()
+{
+    //PPath* the_filename = "/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/generate/scoreGen.amat";
+    data = new AutoVMatrix();
+    data->filename = "/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/generate/scoreGen.amat";
+    data->defineSizes(21,0,0);
+    //data->inputsize = 21;
+    //data->targetsize = 0;
+    //data->weightsize = 0;
+    data->build();
+
+    
+    int len = data->length();
+    Vec score;
+    Vec target;
+    real weight;
+    Vec bias_tempo;
+    Vec visi_bias_tempo;
+   
+   
+    
+    previous_hidden_layer.resize(hidden_layer->size);
+    connections_idem = connections;
+
+    for (int ith_sample = 0; ith_sample < len ; ith_sample++ ){
+        
+        data->getExample(ith_sample, score, target, weight);
+        //score << data(ith_sample);
+        input_prediction_list.resize(
+            ith_sample+1,visible_layer->size);
+        if(ith_sample > 0)
+        {
+            
+            //input_list(ith_sample_in_sequence) << previous_input;
+            //h*_{t-1}
+            //////////////////////////////////
+            dynamic_connections->fprop(previous_hidden_layer, cond_bias);
+            hidden_layer->setAllBias(cond_bias); 
+            
+            
+            
+            //up phase
+            connections->setAsDownInput( input_prediction_list(ith_sample-1) );
+            hidden_layer->getAllActivations( connections_idem );
+            hidden_layer->computeExpectation();
+            //////////////////////////////////
+            
+            //previous_hidden_layer << hidden_layer->expectation;//h_{t-2} au prochain tour
+            //previous_hidden_layer_act_no_bias << hidden_layer->activation;
+            
+            
+            //h*_{t}
+            ////////////
+            if(dynamic_connections_copy)
+                dynamic_connections_copy->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+            else
+                dynamic_connections->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+            //dynamic_connections_copy->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+            hidden_layer->expectation_is_not_up_to_date();
+            hidden_layer->computeExpectation();//h_{t}
+            ///////////
+            
+            //previous_input << visible_layer->expectation;//v_{t-1}
+            
+        }
+        else
+        {
+            
+            previous_hidden_layer.clear();//h_{t-1}
+            if(dynamic_connections_copy)
+                dynamic_connections_copy->fprop( previous_hidden_layer ,
+                                                 hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+            else
+                dynamic_connections->fprop(previous_hidden_layer,
+                                           hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+            
+            hidden_layer->expectation_is_not_up_to_date();
+            hidden_layer->computeExpectation();//h_{t}
+            //previous_input.resize(data->inputsize);
+            //previous_input << data(ith_sample);
+            
+        }
+        
+        //connections_transpose->setAsDownInput( hidden_layer->expectation );
+        //visible_layer->getAllActivations( connections_idem_t );
+        
+        connections->setAsUpInput( hidden_layer->expectation );
+        visible_layer->getAllActivations( connections_idem );
+        
+        visible_layer->computeExpectation();
+        //visible_layer->generateSample();
+        partition(score.subVec(14,taillePart), visible_layer->activation.subVec(14+taillePart,taillePart), visible_layer->activation.subVec(14+(taillePart*2),taillePart));
+        partition(score.subVec(14,taillePart), visible_layer->expectation.subVec(14+taillePart,taillePart), visible_layer->expectation.subVec(14+(taillePart*2),taillePart));
+
+
+        visible_layer->activation.subVec(0,14+taillePart) << score;
+        visible_layer->expectation.subVec(0,14+taillePart) << score;
+
+        input_prediction_list(ith_sample) << visible_layer->expectation;
+        
+    }
+    
+    //Vec tempo;
+    TVec<real> tempo;
+    tempo.resize(visible_layer->size);
+    ofstream myfile;
+    myfile.open ("/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/generate/test.txt");
+    
+    for (int i = 0; i < len ; i++ ){
+        tempo << input_prediction_list(i);
+        
+        //cout << tempo[2] << endl;
+       
+        for (int j = 0; j < tempo.length() ; j++ ){
+            
+            
+                
+                
+               myfile << tempo[j] << " ";
+               
+
+               
+           
+        }
+        myfile << "\n";
+    }
+     
+
+     myfile.close();
+
+}*/
+//void DenoisingRecurrentNet::generate(int nbNotes)
+//{
+//    
+//    previous_hidden_layer.resize(hidden_layer->size);
+//    connections_idem = connections;
+//
+//    for (int ith_sample = 0; ith_sample < nbNotes ; ith_sample++ ){
+//        
+//        input_prediction_list.resize(
+//            ith_sample+1,visible_layer->size);
+//        if(ith_sample > 0)
+//        {
+//            
+//            //input_list(ith_sample_in_sequence) << previous_input;
+//            //h*_{t-1}
+//            //////////////////////////////////
+//            dynamic_connections->fprop(previous_hidden_layer, cond_bias);
+//            hidden_layer->setAllBias(cond_bias); //**************************
+//            
+//            
+//            
+//            //up phase
+//            connections->setAsDownInput( input_prediction_list(ith_sample-1) );
+//            hidden_layer->getAllActivations( connections_idem );
+//            hidden_layer->computeExpectation();
+//            //////////////////////////////////
+//            
+//            //previous_hidden_layer << hidden_layer->expectation;//h_{t-2} au prochain tour//******************************
+//            //previous_hidden_layer_act_no_bias << hidden_layer->activation;
+//            
+//            
+//            //h*_{t}
+//            ////////////
+//            if(dynamic_connections_copy)
+//                dynamic_connections_copy->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+//            else
+//                dynamic_connections->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+//            //dynamic_connections_copy->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+//            hidden_layer->expectation_is_not_up_to_date();
+//            hidden_layer->computeExpectation();//h_{t}
+//            ///////////
+//            
+//            //previous_input << visible_layer->expectation;//v_{t-1}
+//            
+//        }
+//        else
+//        {
+//            
+//            previous_hidden_layer.clear();//h_{t-1}
+//            if(dynamic_connections_copy)
+//                dynamic_connections_copy->fprop( previous_hidden_layer ,
+//                                                 hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+//            else
+//                dynamic_connections->fprop(previous_hidden_layer,
+//                                           hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+//            
+//            hidden_layer->expectation_is_not_up_to_date();
+//            hidden_layer->computeExpectation();//h_{t}
+//            
+//            
+//        }
+//        
+//        //connections_transpose->setAsDownInput( hidden_layer->expectation );
+//        //visible_layer->getAllActivations( connections_idem_t );
+//        
+//        connections->setAsUpInput( hidden_layer->expectation );
+//        visible_layer->getAllActivations( connections_idem );
+//        
+//        visible_layer->computeExpectation();
+//        visible_layer->generateSample();
+//        
+//        input_prediction_list(ith_sample) << visible_layer->sample;
+//        
+//    }
+//    
+//    //Vec tempo;
+//    TVec<int> tempo;
+//    tempo.resize(visible_layer->size);
+//    int theNote;
+//    //int nbNoteVisiLayer = input_prediction_list(1).length()/13;
+//    ofstream myfile;
+//    int theLayer;
+//    myfile.open ("/home/stan/Documents/recherche_maitrise/DDBN_musicGeneration/data/generate/test.txt");
+//    
+//    for (int i = 0; i < nbNotes ; i++ ){
+//        tempo << input_prediction_list(i);
+//        
+//        //cout << tempo[2] << endl;
+//       
+//        for (int j = 0; j < tempo.length() ; j++ ){
+//            
+//            if (tempo[j] == 1){
+//                theLayer = (j/13);
+//                
+//                theNote = j - (13*theLayer);
+//               
+//
+//                if (theNote<=11){
+//                    //print theNote
+//                    //cout << theNote+50 << " ";
+//                    myfile << theNote << " ";
+//                }
+//                else{
+//                    //print #
+//                    //cout << "# ";
+//                    myfile << "# ";
+//                    
+//                }
+//     
+//            }
+//           
+//        }
+//        myfile << "\n";
+//    }
+//     myfile << "<oov> <oov> \n";
+//
+//     myfile.close();
+//
+//}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-06-17 15:51:53 UTC (rev 9134)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-06-17 17:10:00 UTC (rev 9135)
@@ -0,0 +1,308 @@
+// -*- C++ -*-
+
+// DenoisingRecurrentNet.h
+//
+// Copyright (C) 2006 Stanislas Lauly
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Stanislas Lauly
+
+/*! \file DenoisingRecurrentNet.h */
+
+
+
+#ifndef DenoisingRecurrentNet_INC
+#define DenoisingRecurrentNet_INC
+
+#include <plearn_learners/generic/PLearner.h>
+#include <plearn_learners/online/OnlineLearningModule.h>
+#include <plearn_learners/online/RBMClassificationModule.h>
+#include <plearn_learners/online/RBMLayer.h>
+#include <plearn_learners/online/RBMMixedLayer.h>
+#include <plearn_learners/online/RBMConnection.h>
+#include <plearn_learners/online/RBMMatrixConnection.h>
+#include <plearn/vmat/AutoVMatrix.h>
+#include <plearn_learners/online/RBMMatrixTransposeConnection.h>
+
+#include <plearn_learners/online/GradNNetLayerModule.h>
+
+namespace PLearn {
+
+/**
+ * Model made of RBMs linked through time.
+ */
+class DenoisingRecurrentNet : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    ////! The learning rate used during RBM contrastive divergence learning phase
+    //real rbm_learning_rate;
+
+    //! The learning rate used during the recurrent phase
+    real recurrent_net_learning_rate;
+
+    ////! Number of epochs for rbm phase
+    //int rbm_nstages;
+
+    //! The training weights of each target layers
+    Vec target_layers_weights;
+    
+    //! Indication that a mask indicating which target to predict
+    //! is present in the input part of the VMatrix dataset.
+    bool use_target_layers_masks;
+
+    //! Value of the first input component for end-of-sequence delimiter
+    real end_of_sequence_symbol;
+
+    //! The input layer of the model
+    PP<RBMLayer> input_layer;
+
+    //! The target layers of the model
+    TVec< PP<RBMLayer> > target_layers;
+
+    //! The hidden layer of the model
+    PP<RBMLayer> hidden_layer;
+
+    //! The second hidden layer of the model (optional) 
+    PP<RBMLayer> hidden_layer2;
+
+    //! The RBMConnection between the first hidden layers, through time
+    PP<RBMConnection> dynamic_connections;
+
+    //! The RBMConnection between the first and second hidden layers (optional)
+    PP<RBMConnection> hidden_connections;
+
+    //! The RBMConnection from input_layer to hidden_layer
+    PP<RBMConnection> input_connections;
+
+    //! The RBMConnection from input_layer to hidden_layer
+    TVec< PP<RBMConnection> > target_connections;
+
+    //#####  Public Learnt Options  ###########################################
+
+    //! Number of elements in the target part of a VMatrix associated
+    //! to each target layer
+    TVec<int> target_layers_n_of_target_elements;
+
+    //! Number of symbols for each symbolic field of train_set
+    TVec<int> input_symbol_sizes;
+    
+    //! Number of symbols for each symbolic field of train_set
+    TVec< TVec<int> > target_symbol_sizes;
+
+    
+    
+    //#####  Not Options  #####################################################
+
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    DenoisingRecurrentNet();
+
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Returns the size of this learner's output, (which typically
+    //! may depend on its inputsize(), targetsize() and set options).
+    virtual int outputsize() const;
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training costs
+    //! measured on-line in the process.
+    virtual void train();
+
+    //! Sets the learning of all layers and connections
+    void setLearningRate( real the_learning_rate );
+
+    //! Computes the output from the input.
+    virtual void computeOutput(const Vec& input, Vec& output) const;
+
+    //! Computes the costs from already computed output.
+    virtual void computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                         const Vec& target, Vec& costs) const;
+
+    //! Returns the names of the costs computed by computeCostsFromOutpus (and
+    //! thus the test method).
+    virtual TVec<std::string> getTestCostNames() const;
+
+    
+
+//    //! Generate music in a folder
+    void generate(int t, int n);
+//
+//    //! Generate a part of the data in a folder
+//    void gen();
+
+    //! Returns the names of the objective costs that the train method computes
+    //! and  for which it updates the VecStatsCollector train_stats.
+    virtual TVec<std::string> getTrainCostNames() const;
+
+    //! Use the partition
+    void partition(TVec<double> part, TVec<double> periode, TVec<double> vel ) const;
+    
+    //! Clamps the layer units based on a layer vector
+    void clamp_units(const Vec layer_vector, PP<RBMLayer> layer,
+                     TVec<int> symbol_sizes) const;
+
+    //! Clamps the layer units based on a layer vector
+    //! and provides the associated mask in the correct format.
+    void clamp_units(const Vec layer_vector, PP<RBMLayer> layer,
+                     TVec<int> symbol_sizes, const Vec original_mask,
+                     Vec &formated_mask) const;
+    
+    //! Updates both the RBM parameters and the 
+    //! dynamic connections in the recurrent tuning phase,
+    //! after the visible units have been clamped
+    void recurrent_update();
+
+    virtual void test(VMat testset, PP<VecStatsCollector> test_stats,
+                      VMat testoutputs=0, VMat testcosts=0) const;
+
+    
+
+
+    // *** SUBCLASS WRITING: ***
+    // While in general not necessary, in case of particular needs
+    // (efficiency concerns for ex) you may also want to overload
+    // some of the following methods:
+    // virtual void computeOutputAndCosts(const Vec& input, const Vec& target,
+    //                                    Vec& output, Vec& costs) const;
+    // virtual void computeCostsOnly(const Vec& input, const Vec& target,
+    //                               Vec& costs) const;
+    // virtual int nTestCosts() const;
+    // virtual int nTrainCosts() const;
+    // virtual void resetInternalState();
+    // virtual bool isStatefulLearner() const;
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(DenoisingRecurrentNet);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Not Options  #####################################################
+
+
+    //! Store external data;
+    AutoVMatrix*  data;
+   
+    //! Stores bias gradient
+    mutable Vec bias_gradient;
+    
+     //! Stores bias gradient
+    mutable Vec visi_bias_gradient;
+
+    //! Stores hidden gradient of dynamic connections
+    mutable Vec hidden_gradient;
+    
+    //! Stores hidden gradient of dynamic connections coming from time t+1
+    mutable Vec hidden_temporal_gradient;
+        
+    //! List of hidden layers values
+    mutable TVec< Vec > hidden_list;
+    mutable TVec< Vec > hidden_act_no_bias_list;
+
+    //! List of second hidden layers values
+    mutable TVec< Vec > hidden2_list;
+    mutable TVec< Vec > hidden2_act_no_bias_list;
+
+    //! List of target prediction values
+    mutable TVec< TVec< Vec > > target_prediction_list;
+    mutable TVec< TVec< Vec > > target_prediction_act_no_bias_list;
+
+    //! List of inputs values
+    mutable TVec< Vec > input_list;
+
+    //! List of inputs values
+    mutable TVec< TVec< Vec > > targets_list;
+
+    //! List of the nll of the input samples in a sequence
+    mutable Mat nll_list;
+
+    //! List of all targets' masks
+    mutable TVec< TVec< Vec > > masks_list;
+
+    //! Contribution of dynamic weights to hidden layer activation
+    mutable Vec dynamic_act_no_bias_contribution;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(DenoisingRecurrentNet);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From tihocan at mail.berlios.de  Tue Jun 17 20:17:00 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 17 Jun 2008 20:17:00 +0200
Subject: [Plearn-commits] r9136 - trunk/plearn_learners/online
Message-ID: <200806171817.m5HIH0HI014116@sheep.berlios.de>

Author: tihocan
Date: 2008-06-17 20:17:00 +0200 (Tue, 17 Jun 2008)
New Revision: 9136

Modified:
   trunk/plearn_learners/online/ModuleLearner.cc
   trunk/plearn_learners/online/ModuleLearner.h
Log:
- The output port can now be given a name different from 'output'
- Does not crash anymore when computing the output and there is none


Modified: trunk/plearn_learners/online/ModuleLearner.cc
===================================================================
--- trunk/plearn_learners/online/ModuleLearner.cc	2008-06-17 17:10:00 UTC (rev 9135)
+++ trunk/plearn_learners/online/ModuleLearner.cc	2008-06-17 18:17:00 UTC (rev 9136)
@@ -53,8 +53,8 @@
     "That module should have ports that can be fed with the input, target\n"
     "and weight of an example (defined by the 'input_ports', 'target_ports'\n"
     "and 'weight_ports' options), ports that compute costs (defined by the\n"
-    "'cost_ports' option), and a port named 'output' that computes the\n"
-    "output of this learner.\n"
+    "'cost_ports' option), and a port that computes the output of this\n"
+    "learner (whose name is given by the 'output_port' option).\n"
     "\n"
     "For example one can use a NetworkModule, which can define such ports.\n"
     "\n"
@@ -78,6 +78,7 @@
     cost_ports(TVec<string>(1, "cost")),
     input_ports(TVec<string>(1, "input")),
     target_ports(TVec<string>(1, "target")),
+    output_port("output"),
     // Note: many learners do not use weights, thus the default behavior is not
     // to have a 'weight' port in 'weight_ports'.
     operate_on_bags(false),
@@ -125,6 +126,10 @@
                   OptionBase::buildoption,
        "List of ports that take the weight part of a sample as input.");
 
+    declareOption(ol, "output_port", &ModuleLearner::output_port,
+                  OptionBase::buildoption,
+       "The port that will contain the output of the learner.");
+
     declareOption(ol, "operate_on_bags", &ModuleLearner::operate_on_bags,
                   OptionBase::buildoption,
        "If true, then each training step will be done on batch_size *bags*\n"
@@ -194,11 +199,11 @@
                     module, weight_ports[i], false));
     }
 
-    if (ports.find("output") >= 0) {
+    if (ports.find(output_port) >= 0) {
         store_outputs = new MatrixModule("store_outputs", true);
         all_modules.append(get_pointer(store_outputs));
         all_connections.append(new NetworkConnection(
-                    module, "output",
+                    module, output_port,
                     get_pointer(store_outputs), "data", false));
     } else
         store_outputs = NULL;
@@ -266,7 +271,7 @@
 int ModuleLearner::outputsize() const
 {
     if ( module && store_outputs )
-        return module->getPortWidth("output");
+        return module->getPortWidth(output_port);
     else
         return -1; // Undefined.
 }
@@ -449,11 +454,13 @@
     network->fprop(null_pointers);
 
     // Store output.
-    PLASSERT( store_outputs );
-    const Mat& net_out = store_outputs->getData();
-    PLASSERT( net_out.length() == 1 );
-    output.resize(net_out.width());
-    output << net_out;
+    if (store_outputs) {
+        const Mat& net_out = store_outputs->getData();
+        PLASSERT( net_out.length() == 1 );
+        output.resize(net_out.width());
+        output << net_out;
+    } else
+        output.resize(0);
 
     // Store costs.
     costs.resize(0);

Modified: trunk/plearn_learners/online/ModuleLearner.h
===================================================================
--- trunk/plearn_learners/online/ModuleLearner.h	2008-06-17 17:10:00 UTC (rev 9135)
+++ trunk/plearn_learners/online/ModuleLearner.h	2008-06-17 18:17:00 UTC (rev 9136)
@@ -62,6 +62,7 @@
     TVec<string> input_ports;
     TVec<string> target_ports;
     TVec<string> weight_ports;
+    string output_port;
 
     bool operate_on_bags;
 



From tihocan at mail.berlios.de  Tue Jun 17 20:21:45 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 17 Jun 2008 20:21:45 +0200
Subject: [Plearn-commits] r9137 - trunk/plearn_learners/online
Message-ID: <200806171821.m5HILjFL014499@sheep.berlios.de>

Author: tihocan
Date: 2008-06-17 20:21:44 +0200 (Tue, 17 Jun 2008)
New Revision: 9137

Modified:
   trunk/plearn_learners/online/RBMModule.cc
   trunk/plearn_learners/online/RBMModule.h
Log:
Added new experimental code to compare the CD update with the NLL gradient

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2008-06-17 18:17:00 UTC (rev 9136)
+++ trunk/plearn_learners/online/RBMModule.cc	2008-06-17 18:21:44 UTC (rev 9137)
@@ -87,7 +87,7 @@
     "     values (expectations) through the conditional expectations of hidden | visible.\n"
     "  - 'reconstruction_error.state' : the auto-associator reconstruction error (NLL)\n"
     "    obtained by matching the visible_reconstruction with the given visible.\n"
-    "Note that the above determnistic reconstruction may be made stochastic\n"
+    "Note that the above deterministic reconstruction may be made stochastic\n"
     "by using the advanced option 'stochastic_reconstruction'.\n"
     "If compute_contrastive_divergence is true, then the RBM also has these ports\n"
     "  - 'contrastive_divergence' : the quantity minimized by contrastive-divergence training.\n"
@@ -109,6 +109,8 @@
     grad_learning_rate(0),
     tied_connection_weights(false),
     compute_contrastive_divergence(false),
+    compare_true_gradient_with_cd(false),
+    n_steps_compare(1),
     n_Gibbs_steps_CD(1),
     min_n_Gibbs_steps(1),
     n_Gibbs_steps_per_generated_sample(-1),
@@ -252,6 +254,21 @@
                   "i.e. take stochastic gradient steps w.r.t. the log-likelihood instead\n"
                   "of w.r.t. the contrastive divergence.\n");
 
+    declareOption(ol, "compare_true_gradient_with_cd",
+                  &RBMModule::compare_true_gradient_with_cd,
+                  OptionBase::buildoption,
+        "If true, then will compute the true gradient (of the NLL) as well\n"
+        "as the exact non-stochastic CD update, and compare them.",
+                  OptionBase::advanced_level);
+
+    declareOption(ol, "n_steps_compare",
+                  &RBMModule::n_steps_compare,
+                  OptionBase::buildoption,
+        "Number of steps for which we want to compare CD with the true\n"
+        "gradient (when 'compare_true_gradient_with_cd' is true). This will\n"
+        "compute P(x_t|x) for t from 1 to 'n_steps_compare'.",
+                  OptionBase::advanced_level);
+
     // Learnt options.
 
     declareOption(ol, "Gibbs_step",
@@ -574,6 +591,25 @@
     }
 }
 
+///////////////////////////////////
+// computeAllHiddenProbabilities //
+///////////////////////////////////
+void RBMModule::computeAllHiddenProbabilities(const Mat& visible,
+                                              const Mat& p_hidden)
+{
+    Vec hidden(hidden_layer->size);
+    computeHiddenActivations(visible);
+    int n_conf = hidden_layer->getConfigurationCount();
+    for (int i = 0; i < n_conf; i++) {
+        hidden_layer->getConfiguration(i, hidden);
+        for (int j = 0; j < visible.length(); j++) {
+            hidden_layer->activation = hidden_layer->activations(j);
+            real neg_log_p_h_given_v = hidden_layer->fpropNLL(hidden);
+            p_hidden(i, j) = exp(-neg_log_p_h_given_v);
+        }
+    }
+}
+
 ///////////////////////////////////////////
 // computePositivePhaseHiddenActivations //
 ///////////////////////////////////////////
@@ -660,6 +696,12 @@
 
     deepCopyField(ports, copies);
     deepCopyField(energy_inputs, copies);
+
+    deepCopyField(all_p_visible,            copies);
+    deepCopyField(all_hidden_cond_prob,     copies);
+    deepCopyField(all_visible_cond_prob,    copies);
+    deepCopyField(p_ht_given_x,             copies);
+    deepCopyField(p_xt_given_x,             copies);
 }
 
 ///////////
@@ -681,29 +723,70 @@
                  "or visible layer must be less than 2^31.");
 
     // Compute partition function
-    if (hidden_configurations > visible_configurations)
+    if (hidden_configurations > visible_configurations ||
+        compare_true_gradient_with_cd)
         // do it by log-summing minus-free-energy of visible configurations
     {
+        if (compare_true_gradient_with_cd) {
+            all_p_visible.resize(visible_configurations);
+            all_visible_cond_prob.resize(visible_configurations,
+                                         hidden_configurations);
+            all_hidden_cond_prob.resize(hidden_configurations,
+                                        visible_configurations);
+        }
         energy_inputs.resize(1, visible_layer->size);
         Vec input = energy_inputs(0);
         // COULD BE DONE MORE EFFICIENTLY BY DOING MANY CONFIGURATIONS
         // AT ONCE IN A 'MINIBATCH'
         Mat free_energy(1, 1);
         log_partition_function = 0;
+        PP<ProgressBar> pb;
+        if (verbosity >= 2)
+            pb = new ProgressBar("Computing partition function",\
+                                 visible_configurations);
         for (int c = 0; c < visible_configurations; c++)
         {
             visible_layer->getConfiguration(c, input);
             computeFreeEnergyOfVisible(energy_inputs, free_energy, false);
+            real fe = free_energy(0,0);
             if (c==0)
-                log_partition_function = -free_energy(0,0);
+                log_partition_function = -fe;
             else
-                log_partition_function = logadd(log_partition_function,-free_energy(0,0));
+                log_partition_function = logadd(log_partition_function, -fe);
+            if (compare_true_gradient_with_cd) {
+                all_p_visible[c] = -fe;
+                // Compute P(visible | hidden) and P(hidden | visible) for all
+                // values of hidden.
+                computeAllHiddenProbabilities(input.toMat(1, input.length()),
+                                              all_hidden_cond_prob.column(c));
+                Vec hidden(hidden_layer->size);
+                for (int d = 0; d < hidden_configurations; d++) {
+                    hidden_layer->getConfiguration(d, hidden);
+                    computeVisibleActivations(hidden.toMat(1, hidden.length()),
+                                              false);
+                    visible_layer->activation = visible_layer->activations(0);
+                    real neg_log_p_v_given_h = visible_layer->fpropNLL(input);
+                    all_visible_cond_prob(c, d) = exp(-neg_log_p_v_given_h);
+                }
+            }
+            if (pb)
+                pb->update(c + 1);
         }
+        pb = NULL;
         hidden_activations_are_computed = false;
+        if (compare_true_gradient_with_cd) {
+            // Normalize probabilities.
+            for (int i = 0; i < all_p_visible.length(); i++)
+                all_p_visible[i] =
+                    exp(all_p_visible[i] - log_partition_function);
+            pout << "All P(x): " << all_p_visible << endl;
+            pout << "Sum_x P(x) = " << sum(all_p_visible) << endl;
+        }
     }
     else
         // do it by summing free-energy of hidden configurations
     {
+        PLASSERT( !compare_true_gradient_with_cd );
         energy_inputs.resize(1, hidden_layer->size);
         Vec input = energy_inputs(0);
         // COULD BE DONE MORE EFFICIENTLY BY DOING MANY CONFIGURATIONS
@@ -713,13 +796,18 @@
         for (int c = 0; c < hidden_configurations; c++)
         {
             hidden_layer->getConfiguration(c, input);
+            //pout << "Input = " << input << endl;
             computeFreeEnergyOfHidden(energy_inputs, free_energy);
+            //pout << "FE = " << free_energy(0, 0) << endl;
+            real fe = free_energy(0,0);
             if (c==0)
-                log_partition_function = -free_energy(0,0);
+                log_partition_function = -fe;
             else
-                log_partition_function = logadd(log_partition_function,-free_energy(0,0));
+                log_partition_function = logadd(log_partition_function, -fe);
         }
     }
+    if (false)
+        pout << "Log Z(" << name << ") = " << log_partition_function << endl;
 }
 
 void RBMModule::fprop(const TVec<Mat*>& ports_value)
@@ -1327,6 +1415,40 @@
         found_a_valid_configuration = true;
     }
 
+    if (compare_true_gradient_with_cd) {
+        PLCHECK_MSG(!partition_function_is_stale,
+                "The partition function must be computed for the comparison "
+                "between true gradient and contrastive divergence to work.");
+        PLCHECK_MSG(visible && !visible_is_output, "Visible must be as input");
+        // Compute P(x_t|x) for all t and inputs x.
+        int n_visible_conf = visible_layer->getConfigurationCount();
+        int n_hidden_conf = hidden_layer->getConfigurationCount();
+        p_xt_given_x.resize(n_visible_conf, visible->length());
+        p_ht_given_x.resize(n_hidden_conf, visible->length());
+        for (int i = 0; i < visible->length(); i++) {
+            // First compute P(h|x) for inputs x.
+            computeAllHiddenProbabilities(*visible, p_ht_given_x);
+            for (int t = 0; t < n_steps_compare; t++) {
+                // Compute P(x_t|x).
+                product(p_xt_given_x, all_visible_cond_prob, p_ht_given_x);
+                pout << "P(x_" << (t + 1) << "|x) = " << endl << p_xt_given_x
+                     << endl;
+                Vec colsum(p_xt_given_x.width());
+                columnSum(p_xt_given_x, colsum);
+                pout << "Sum = " << endl << colsum << endl;
+                int best_idx = argmax(p_xt_given_x.column(0).toVecCopy());
+                Vec tmp(visible_layer->size);
+                visible_layer->getConfiguration(best_idx, tmp);
+                pout << "Best (P = " << p_xt_given_x.column(0)(best_idx, 0) <<
+                    ") for x = " << (*visible)(0) << ":" <<
+                    endl << tmp << endl;
+                // If it is not the last step, update P(h_t|x).
+                if (t < n_steps_compare - 1)
+                    product(p_ht_given_x, all_hidden_cond_prob, p_xt_given_x);
+            }
+        }
+    }
+
     // UGLY HACK TO DEAL WITH THE PROBLEM THAT XXX.state MAY NOT BE NEEDED
     // BUT IS ALWAYS EXPECTED BECAUSE IT IS A STATE (!@#$%!!!)
     if (hidden_act && hidden_act->isEmpty())
@@ -2014,6 +2136,7 @@
         // We avoid to call forget() twice if the connections are the same.
         reconstruction_connection->forget();
     Gibbs_step = 0;
+    partition_function_is_stale = true;
 }
 
 //////////////////

Modified: trunk/plearn_learners/online/RBMModule.h
===================================================================
--- trunk/plearn_learners/online/RBMModule.h	2008-06-17 18:17:00 UTC (rev 9136)
+++ trunk/plearn_learners/online/RBMModule.h	2008-06-17 18:21:44 UTC (rev 9137)
@@ -74,6 +74,8 @@
     bool tied_connection_weights;
 
     bool compute_contrastive_divergence;
+    bool compare_true_gradient_with_cd;
+    int n_steps_compare;
 
     //! Number of Gibbs sampling steps in negative phase
     //! of contrastive divergence.
@@ -269,6 +271,27 @@
     //! Used to store inputs generated to compute the free energy.
     Mat energy_inputs;
 
+    //! P(x) for all possible configurations x of visible layer. Used only when
+    //! 'compare_true_gradient_with_cd' is true (computed at the same time as
+    //! the partition function).
+    Vec all_p_visible;
+
+    //! Used to store P(h|x) for all values of h and all values of x.
+    //! Element (i,j) is P(h_i | x_j).
+    Mat all_hidden_cond_prob;
+
+    //! Used to store P(x|h) for all values of h and all values of x.
+    //! Element (i,j) is P(x_i | h_j).
+    Mat all_visible_cond_prob;
+
+    //! Used to store P(h_t|x) for all values of h_t and some values of x.
+    //! Element (i,j) is P(h_ti | x_j).
+    Mat p_ht_given_x;
+
+    //! Used to store P(x_t|x) for all values of x_t and some values of x.
+    //! Element (i,j) is P(x_ti | x_j).
+    Mat p_xt_given_x;
+
     //#####  Protected Member Functions  ######################################
 
     //! Add a new port to the 'portname_to_index' map and 'ports' vector.
@@ -332,6 +355,12 @@
 
     void computePartitionFunction();
 
+    //! Compute probabilities of all hidden configurations given some visible
+    //! inputs. The 'p_hidden' matrix is filled such that the (i,j)-th element
+    //! is P(hidden_configuration_i | visible_j).
+    void computeAllHiddenProbabilities(const Mat& visible,
+                                       const Mat& p_hidden);
+
     void computeNegLogPVisibleGivenPHidden(Mat visible, Mat hidden, Mat* neg_log_phidden, Mat& neg_log_pvisible_given_phidden);
 
 protected:



From larocheh at mail.berlios.de  Tue Jun 17 21:47:00 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Tue, 17 Jun 2008 21:47:00 +0200
Subject: [Plearn-commits] r9138 - trunk/plearn_learners_experimental
Message-ID: <200806171947.m5HJl0Tr021634@sheep.berlios.de>

Author: larocheh
Date: 2008-06-17 21:46:59 +0200 (Tue, 17 Jun 2008)
New Revision: 9138

Added:
   trunk/plearn_learners_experimental/NeuralProbabilisticLanguageModel.cc
   trunk/plearn_learners_experimental/NeuralProbabilisticLanguageModel.h
Log:
For Doomie...



Added: trunk/plearn_learners_experimental/NeuralProbabilisticLanguageModel.cc
===================================================================
--- trunk/plearn_learners_experimental/NeuralProbabilisticLanguageModel.cc	2008-06-17 18:21:44 UTC (rev 9137)
+++ trunk/plearn_learners_experimental/NeuralProbabilisticLanguageModel.cc	2008-06-17 19:46:59 UTC (rev 9138)
@@ -0,0 +1,3143 @@
+// -*- C++ -*-
+
+// NeuralProbabilisticLanguageModel.cc
+// Copyright (c) 1998-2002 Pascal Vincent
+// Copyright (C) 1999-2002 Yoshua Bengio and University of Montreal
+// Copyright (c) 2002 Jean-Sebastien Senecal, Xavier Saint-Mleux, Rejean Ducharme
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+/*! \file PLearnLibrary/PLearnAlgo/NeuralProbabilisticLanguageModel.h */
+
+
+#include "NeuralProbabilisticLanguageModel.h"
+#include <plearn/vmat/SubVMatrix.h>
+//#include <plearn/sys/Profiler.h>
+#include <time.h>
+#include <stdio.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(NeuralProbabilisticLanguageModel, 
+                        "Feedforward neural network for language modeling",
+                        "Implementation of the Neural Probabilistic Language "
+                        "Model proposed by \n"
+                        "Bengio, Ducharme, Vincent and Jauvin (JMLR 2003), "
+                        "with extentensions to speedup\n"
+                        "the model (Bengio and S?n?cal, AISTATS 2003) and "
+                        "to include prior information\n"
+                        "about the distributed representation and permit "
+                        "generalization of these\n"
+                        "distributed representations to out-of-vocabulary "
+                        "words using features \n"
+                        "(Larochelle and Bengio, Tech Report 2006).\n");
+
+NeuralProbabilisticLanguageModel::NeuralProbabilisticLanguageModel() 
+// DEFAULT VALUES FOR ALL OPTIONS
+    :
+rgen(new PRandom()),
+nhidden(0),
+nhidden2(0),
+weight_decay(0),
+bias_decay(0),
+layer1_weight_decay(0),
+layer1_bias_decay(0),
+layer2_weight_decay(0),
+layer2_bias_decay(0),
+output_layer_weight_decay(0),
+output_layer_bias_decay(0),
+direct_in_to_out_weight_decay(0),
+output_layer_dist_rep_weight_decay(0),
+output_layer_dist_rep_bias_decay(0),
+fixed_output_weights(0),
+direct_in_to_out(0),
+penalty_type("L2_square"),
+output_transfer_func(""),
+hidden_transfer_func("tanh"),
+start_learning_rate(0.01),
+decrease_constant(0),
+batch_size(1),
+stochastic_gradient_descent_speedup(true),
+initialization_method("uniform_linear"),
+dist_rep_dim(-1),
+possible_targets_vary(false),
+train_proposal_distribution(true),
+sampling_block_size(50),
+minimum_effective_sample_size(100)
+{}
+
+NeuralProbabilisticLanguageModel::~NeuralProbabilisticLanguageModel()
+{
+}
+
+void NeuralProbabilisticLanguageModel::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "nhidden", &NeuralProbabilisticLanguageModel::nhidden, 
+                  OptionBase::buildoption, 
+                  "Number of hidden units in first hidden layer (0 means no "
+                  "hidden layer).\n");
+    
+    declareOption(ol, "nhidden2", &NeuralProbabilisticLanguageModel::nhidden2, 
+                  OptionBase::buildoption, 
+                  "Number of hidden units in second hidden layer (0 means no "
+                  "hidden layer).\n");
+    
+    declareOption(ol, "weight_decay", 
+                  &NeuralProbabilisticLanguageModel::weight_decay, 
+                  OptionBase::buildoption, 
+                  "Global weight decay for all layers.\n");
+    
+    declareOption(ol, "bias_decay", &NeuralProbabilisticLanguageModel::bias_decay,
+                  OptionBase::buildoption, 
+                  "Global bias decay for all layers.\n");
+    
+    declareOption(ol, "layer1_weight_decay", 
+                  &NeuralProbabilisticLanguageModel::layer1_weight_decay, 
+                  OptionBase::buildoption, 
+                  "Additional weight decay for the first hidden layer. "
+                  "Is added to weight_decay.\n");
+    
+    declareOption(ol, "layer1_bias_decay", 
+                  &NeuralProbabilisticLanguageModel::layer1_bias_decay, 
+                  OptionBase::buildoption, 
+                  "Additional bias decay for the first hidden layer. "
+                  "Is added to bias_decay.\n");
+    
+    declareOption(ol, "layer2_weight_decay", 
+                  &NeuralProbabilisticLanguageModel::layer2_weight_decay, 
+                  OptionBase::buildoption, 
+                  "Additional weight decay for the second hidden layer. "
+                  "Is added to weight_decay.\n");
+    
+    declareOption(ol, "layer2_bias_decay", 
+                  &NeuralProbabilisticLanguageModel::layer2_bias_decay, 
+                  OptionBase::buildoption, 
+                  "Additional bias decay for the second hidden layer. "
+                  "Is added to bias_decay.\n");
+    
+    declareOption(ol, "output_layer_weight_decay", 
+                  &NeuralProbabilisticLanguageModel::output_layer_weight_decay, 
+                  OptionBase::buildoption, 
+                  "Additional weight decay for the output layer. "
+                  "Is added to 'weight_decay'.\n");
+    
+    declareOption(ol, "output_layer_bias_decay", 
+                  &NeuralProbabilisticLanguageModel::output_layer_bias_decay, 
+                  OptionBase::buildoption, 
+                  "Additional bias decay for the output layer. "
+                  "Is added to 'bias_decay'.\n");
+    
+    declareOption(ol, "direct_in_to_out_weight_decay", 
+                  &NeuralProbabilisticLanguageModel::direct_in_to_out_weight_decay,
+                  OptionBase::buildoption,
+                  "Additional weight decay for the weights going from the "
+                  "input directly to the \n output layer.  Is added to "
+                  "'weight_decay'.\n");
+    
+    declareOption(ol, "output_layer_dist_rep_weight_decay", 
+                  &NeuralProbabilisticLanguageModel::output_layer_dist_rep_weight_decay, 
+                  OptionBase::buildoption, 
+                  "Additional weight decay for the output layer of distributed"
+                  "representation\n"
+                  "predictor.  Is added to 'weight_decay'.\n");
+    
+    declareOption(ol, "output_layer_dist_rep_bias_decay", 
+                  &NeuralProbabilisticLanguageModel::output_layer_dist_rep_bias_decay, 
+                  OptionBase::buildoption, 
+                  "Additional bias decay for the output layer of distributed"
+                  "representation\n"
+                  "predictor.  Is added to 'bias_decay'.\n");
+    
+    declareOption(ol, "fixed_output_weights", 
+                  &NeuralProbabilisticLanguageModel::fixed_output_weights, 
+                  OptionBase::buildoption, 
+                  "If true then the output weights are not learned. They are"
+                  "initialized to +1 or -1 randomly.\n");
+    
+    declareOption(ol, "direct_in_to_out", 
+                  &NeuralProbabilisticLanguageModel::direct_in_to_out, 
+                  OptionBase::buildoption, 
+                  "If true then direct input to output weights will be added "
+                  "(if nhidden > 0).\n");
+    
+    declareOption(ol, "penalty_type", 
+                  &NeuralProbabilisticLanguageModel::penalty_type,
+                  OptionBase::buildoption,
+                  "Penalty to use on the weights (for weight and bias decay).\n"
+                  "Can be any of:\n"
+                  "  - \"L1\": L1 norm,\n"
+                  "  - \"L2_square\" (default): square of the L2 norm.\n");
+    
+    declareOption(ol, "output_transfer_func", 
+                  &NeuralProbabilisticLanguageModel::output_transfer_func, 
+                  OptionBase::buildoption, 
+                  "what transfer function to use for ouput layer? One of: \n"
+                  "  - \"tanh\" \n"
+                  "  - \"sigmoid\" \n"
+                  "  - \"softmax\" \n"
+                  "An empty string or \"none\" means no output transfer function \n");
+    
+    declareOption(ol, "hidden_transfer_func", 
+                  &NeuralProbabilisticLanguageModel::hidden_transfer_func, 
+                  OptionBase::buildoption, 
+                  "What transfer function to use for hidden units? One of \n"
+                  "  - \"linear\" \n"
+                  "  - \"tanh\" \n"
+                  "  - \"sigmoid\" \n"
+                  "  - \"softmax\" \n");
+    
+    declareOption(ol, "cost_funcs", &NeuralProbabilisticLanguageModel::cost_funcs, 
+                  OptionBase::buildoption, 
+                  "A list of cost functions to use\n"
+                  "in the form \"[ cf1; cf2; cf3; ... ]\" where each function "
+                  "is one of: \n"
+                  "  - \"NLL\" (negative log likelihood -log(p[c]) for "
+                  "classification) \n"
+                  "  - \"class_error\" (classification error) \n"
+                  "The FIRST function of the list will be used as \n"
+                  "the objective function to optimize \n"
+                  "(possibly with an added weight decay penalty) \n");
+    
+    declareOption(ol, "start_learning_rate", 
+                  &NeuralProbabilisticLanguageModel::start_learning_rate, 
+                  OptionBase::buildoption, 
+                  "Start learning rate of gradient descent.\n");
+                  
+    declareOption(ol, "decrease_constant", 
+                  &NeuralProbabilisticLanguageModel::decrease_constant, 
+                  OptionBase::buildoption, 
+                  "Decrease constant of gradient descent.\n");
+
+    declareOption(ol, "batch_size", 
+                  &NeuralProbabilisticLanguageModel::batch_size, 
+                  OptionBase::buildoption, 
+                  "How many samples to use to estimate the avergage gradient before updating the weights\n"
+                  "0 is equivalent to specifying training_set->length() \n");
+
+    declareOption(ol, "stochastic_gradient_descent_speedup", 
+                  &NeuralProbabilisticLanguageModel::stochastic_gradient_descent_speedup, 
+                  OptionBase::buildoption, 
+                  "Indication that a trick to speedup stochastic "
+                  "gradient descent\n"
+                  "should be used.\n");
+
+    declareOption(ol, "initialization_method", 
+                  &NeuralProbabilisticLanguageModel::initialization_method, 
+                  OptionBase::buildoption, 
+                  "The method used to initialize the weights:\n"
+                  " - \"normal_linear\"  = a normal law with variance "
+                  "1/n_inputs\n"
+                  " - \"normal_sqrt\"    = a normal law with variance "
+                  "1/sqrt(n_inputs)\n"
+                  " - \"uniform_linear\" = a uniform law in [-1/n_inputs,"
+                  "1/n_inputs]\n"
+                  " - \"uniform_sqrt\"   = a uniform law in [-1/sqrt(n_inputs),"
+                  "1/sqrt(n_inputs)]\n"
+                  " - \"zero\"           = all weights are set to 0\n");
+    
+    declareOption(ol, "dist_rep_dim", 
+                  &NeuralProbabilisticLanguageModel::dist_rep_dim, 
+                  OptionBase::buildoption, 
+                  " Dimensionality (number of components) of distributed "
+                  "representations.\n"
+                  "If <= 0, than distributed representations will not be used.\n"
+        );
+    
+    declareOption(ol, "possible_targets_vary", 
+                  &NeuralProbabilisticLanguageModel::possible_targets_vary, 
+                  OptionBase::buildoption, 
+                  "Indication that the set of possible targets vary from\n"
+                  "one input vector to another.\n"
+        );
+    
+    declareOption(ol, "feat_sets", &NeuralProbabilisticLanguageModel::feat_sets, 
+                                OptionBase::buildoption, 
+                  "FeatureSets to apply on input. The number of feature\n"
+                  "sets should be a divisor of inputsize(). The feature\n"
+                  "sets applied to the ith input field is the feature\n"
+                  "set at position i % feat_sets.length().\n"
+        );
+
+    declareOption(ol, "train_proposal_distribution", 
+                  &NeuralProbabilisticLanguageModel::train_proposal_distribution
+                  OptionBase::buildoption, 
+                  "Indication that the proposal distribution must be trained\n"
+                  "(using train_set).\n"
+        );
+
+    declareOption(ol, "sampling_block_size", 
+                  &NeuralProbabilisticLanguageModel::sampling_block_size, 
+                  OptionBase::buildoption, 
+                  "Size of the sampling blocks.\n"
+        );
+
+    declareOption(ol, "minimum_effective_sample_size", 
+                  &NeuralProbabilisticLanguageModel::minimum_effective_sample_size, 
+                  OptionBase::buildoption, 
+                  "Minimum effective sample size.\n"
+        );
+
+    declareOption(ol, "train_set", &NeuralProbabilisticLanguageModel::train_set, 
+                  OptionBase::learntoption, 
+                  "VMatrix used for training, that also provides information about the data (e.g. Dictionary objects for the different fields).\n");
+
+
+                  // Networks' learnt parameters
+    declareOption(ol, "w1", &NeuralProbabilisticLanguageModel::w1, 
+                  OptionBase::learntoption, 
+                  "Weights of first hidden layer.\n");
+    declareOption(ol, "b1", &NeuralProbabilisticLanguageModel::b1, 
+                  OptionBase::learntoption, 
+                  "Bias of first hidden layer.\n");
+    declareOption(ol, "w2", &NeuralProbabilisticLanguageModel::w2, 
+                  OptionBase::learntoption, 
+                  "Weights of second hidden layer.\n");
+    declareOption(ol, "b2", &NeuralProbabilisticLanguageModel::b2, 
+                  OptionBase::learntoption, 
+                  "Bias of second hidden layer.\n");
+    declareOption(ol, "wout", &NeuralProbabilisticLanguageModel::wout, 
+                  OptionBase::learntoption, 
+                  "Weights of output layer.\n");
+    declareOption(ol, "bout", &NeuralProbabilisticLanguageModel::bout, 
+                  OptionBase::learntoption, 
+                  "Bias of output layer.\n");
+    declareOption(ol, "direct_wout", 
+                  &NeuralProbabilisticLanguageModel::direct_wout, 
+                  OptionBase::learntoption, 
+                  "Direct input to output weights.\n");
+    declareOption(ol, "direct_bout", 
+                  &NeuralProbabilisticLanguageModel::direct_bout, 
+                  OptionBase::learntoption, 
+                  "Direct input to output bias.\n");
+    declareOption(ol, "wout_dist_rep", 
+                  &NeuralProbabilisticLanguageModel::wout_dist_rep, 
+                  OptionBase::learntoption, 
+                  "Weights of output layer for distributed representation "
+                  "predictor.\n");
+    declareOption(ol, "bout_dist_rep", 
+                  &NeuralProbabilisticLanguageModel::bout_dist_rep, 
+                  OptionBase::learntoption, 
+                  "Bias of output layer for distributed representation "
+                  "predictor.\n");
+
+    inherited::declareOptions(ol);
+
+}
+
+///////////
+// build //
+///////////
+void NeuralProbabilisticLanguageModel::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+////////////
+// build_ //
+////////////
+void NeuralProbabilisticLanguageModel::build_()
+{
+    // Don't do anything if we don't have a train_set
+    // It's the only one who knows the inputsize, targetsize and weightsize
+
+    if(inputsize_>=0 && targetsize_>=0 && weightsize_>=0)
+    {
+        if(targetsize_ != 1)
+            PLERROR("In NeuralProbabilisticLanguageModel::build_(): "
+                    "targetsize_ must be 1, not %d",targetsize_);
+
+        n_feat_sets = feat_sets.length();
+
+        if(n_feat_sets == 0)
+            PLERROR("In NeuralProbabilisticLanguageModel::build_(): "
+                    "at least one FeatureSet must be provided\n");
+        
+        if(inputsize_ % n_feat_sets != 0)
+            PLERROR("In NeuralProbabilisticLanguageModel::build_(): "
+                    "feat_sets.length() must be a divisor of inputsize()");
+        
+        // Process penalty type option
+        string pt = lowerstring( penalty_type );
+        if( pt == "l1" )
+            penalty_type = "L1";
+        else if( pt == "l2_square" || pt == "l2 square" || pt == "l2square" )
+            penalty_type = "L2_square";
+        else if( pt == "l2" )
+        {
+            PLWARNING("In NeuralProbabilisticLanguageModel::build_(): "
+                      "L2 penalty not supported, assuming you want L2 square");
+            penalty_type = "L2_square";
+        }
+        else
+            PLERROR("In NeuralProbabilisticLanguageModel::build_(): "
+                    "penalty_type \"%s\" not supported", penalty_type.c_str());
+        
+        int ncosts = cost_funcs.size();  
+        if(ncosts<=0)
+            PLERROR("In NeuralProbabilisticLanguageModel::build_(): "
+                    "Empty cost_funcs : must at least specify the cost "
+                    "function to optimize!");
+        
+        if(stage <= 0 ) // Training hasn't started
+        {
+            // Initialize parameters
+            initializeParams();                        
+        }
+        
+        output_comp.resize(total_output_size);
+        row.resize(train_set->width());
+        row.fill(MISSING_VALUE);
+        feats.resize(inputsize_);
+        // Making sure that all feats[i] have non null storage...
+        for(int i=0; i<feats.length(); i++)
+        {
+            feats[i].resize(1);
+            feats[i].resize(0);
+        }
+        if(fixed_output_weights && stochastic_gradient_descent_speedup)
+            PLERROR("In NeuralProbabilisticLanguageModel::build_(): "
+                    "cannot use stochastic gradient descent speedup with "
+                    "fixed output weights");
+        val_string_reference_set = train_set;
+        target_values_reference_set = train_set;
+
+        if(proposal_distribution)
+        {
+            if(batch_size != 1)
+                PLERROR("In NeuralProbabilisticLanguageModel::build_(): "
+                        "importance sampling speedup is not implemented for"
+                        "batch size != 1");
+            sample.resize(1);            
+            if(train_proposal_distribution)
+            {
+                proposal_distribution->setTrainingSet(train_set);
+                proposal_distribution->train();
+            }
+        }
+    }
+}
+
+void NeuralProbabilisticLanguageModel::fprop(const Vec& inputv, Vec& outputv, 
+                                             const Vec& targetv, Vec& costsv, 
+                                             real sampleweight) const
+{
+    
+    fpropOutput(inputv,outputv);
+    //if(is_missing(outputv[0]))
+    //    cout << "What the fuck" << endl;
+    fpropCostsFromOutput(inputv, outputv, targetv, costsv, sampleweight);
+    //if(is_missing(costsv[0]))
+    //    cout << "Re-What the fuck" << endl;
+
+}
+
+void NeuralProbabilisticLanguageModel::fpropOutput(const Vec& inputv, 
+                                                   Vec& outputv) const
+{
+    // Forward propagation until reaches output weights, sets last_layer
+    fpropBeforeOutputWeights(inputv);
+    
+    if(dist_rep_dim > 0) // x -> d(x)
+    {        
+        // d(x),h1(d(x)),h2(h1(d(x))) -> o(x)
+
+        add_affine_transform(last_layer,wout,bout,outputv,false,
+                             possible_targets_vary,target_values);            
+        if(direct_in_to_out && nhidden>0)
+            add_affine_transform(nnet_input,direct_wout,direct_bout,
+                                 outputv,false,possible_targets_vary,
+                                 target_values);
+    }
+    else
+    {
+        // x, h1(x),h2(h1(x)) -> o(x)
+        add_affine_transform(last_layer,wout,bout,outputv,nhidden<=0,
+                             possible_targets_vary,target_values);            
+        if(direct_in_to_out && nhidden>0)
+            add_affine_transform(feat_input,direct_wout,direct_bout,
+                                 outputv,true,possible_targets_vary,
+                                 target_values);
+    }
+                               
+    if (nhidden2>0 && nhidden<=0)
+        PLERROR("NeuralProbabilisticLanguageModel::fprop(): "
+                "can't have nhidden2 (=%d) > 0 while nhidden=0",nhidden2);
+    
+    if(output_transfer_func!="" && output_transfer_func!="none")
+       add_transfer_func(outputv, output_transfer_func);
+}
+
+void NeuralProbabilisticLanguageModel::fpropBeforeOutputWeights(
+    const Vec& inputv) const
+{
+    // Get possible target values
+    if(possible_targets_vary) 
+    {
+        row.subVec(0,inputsize_) << inputv;
+        target_values_reference_set->getValues(row,inputsize_,target_values);
+        outputv.resize(target_values.length());
+    }
+
+    // Get features
+    ni = inputsize_;
+    nfeats = 0;
+    for(int i=0; i<ni; i++)
+    {
+        str = val_string_reference_set->getValString(i,inputv[i]);
+        feat_sets[i%n_feat_sets]->getFeatures(str,feats[i]);
+        nfeats += feats[i].length();
+    }
+    
+    feat_input.resize(nfeats);
+    offset = 0;
+    id = 0;
+    for(int i=0; i<ni; i++)
+    {
+        f = feats[i].data();
+        nj = feats[i].length();
+        for(int j=0; j<nj; j++)
+            feat_input[id++] = offset + *f++;
+        if(dist_rep_dim <= 0 || ((i+1) % n_feat_sets != 0))
+            offset += feat_sets[i % n_feat_sets]->size();
+        else
+            offset = 0;
+    }
+
+    // Fprop up to output weights
+    if(dist_rep_dim > 0) // x -> d(x)
+    {        
+        nfeats = 0;
+        id = 0;
+        for(int i=0; i<inputsize_;)
+        {
+            ifeats = 0;
+            for(int j=0; j<n_feat_sets; j++,i++)
+                ifeats += feats[i].length();
+            
+            add_affine_transform(feat_input.subVec(nfeats,ifeats),
+                                 wout_dist_rep, bout_dist_rep,
+                                 nnet_input.subVec(id*dist_rep_dim,dist_rep_dim),
+                                      true, false);
+            nfeats += ifeats;
+            id++;
+        }
+
+        if(nhidden>0) // d(x) -> h1(d(x))
+        {
+            add_affine_transform(nnet_input,w1,b1,hiddenv,false,false);
+            add_transfer_func(hiddenv);
+
+            if(nhidden2>0) // h1(d(x)) -> h2(h1(d(x)))
+            {
+                add_affine_transform(hiddenv,w2,b2,hidden2v,false,false);
+                add_transfer_func(hidden2v);
+                last_layer = hidden2v;
+            }
+            else
+                last_layer = hiddenv;
+        }
+        else
+            last_layer = nnet_input;
+
+    }
+    else
+    {        
+        if(nhidden>0) // x -> h1(x)
+        {
+            add_affine_transform(feat_input,w1,b1,hiddenv,true,false);
+            // Transfert function
+            add_transfer_func(hiddenv);
+
+            if(nhidden2>0) // h1(x) -> h2(h1(x))
+            {
+                add_affine_transform(hiddenv,w2,b2,hidden2v,true,false);
+                add_transfer_func(hidden2v);
+                last_layer = hidden2v;
+            }
+            else
+                last_layer = hiddenv;
+        }
+        else
+            last_layer = feat_input;
+    }
+}
+
+void NeuralProbabilisticLanguageModel::fpropCostsFromOutput(const Vec& inputv, const Vec& outputv, const Vec& targetv, Vec& costsv, real sampleweight) const
+{
+    //Compute cost
+
+    if(possible_targets_vary)
+    {
+        reind_target = target_values.find(targetv[0]);
+        if(reind_target<0)
+            PLERROR("In NeuralProbabilisticLanguageModel::fprop(): target %d is not in possible targets", targetv[0]);
+    }
+    else
+        reind_target = (int)targetv[0];
+
+    // Build cost function
+
+    int ncosts = cost_funcs.size();
+    for(int k=0; k<ncosts; k++)
+    {
+        if(cost_funcs[k]=="NLL") 
+        {
+            costsv[k] = sampleweight*nll(outputv,reind_target);
+        }
+        else if(cost_funcs[k]=="class_error")
+            costsv[k] = sampleweight*classification_loss(outputv, reind_target);
+        else 
+            PLERROR("In NeuralProbabilisticLanguageModel::fprop(): "
+                    "unknown cost_func option: %s",cost_funcs[k].c_str());        
+    }
+}
+
+void NeuralProbabilisticLanguageModel::bprop(Vec& inputv, Vec& outputv, 
+                                             Vec& targetv, Vec& costsv, 
+                                             real learning_rate, 
+                                             real sampleweight)
+{
+    if(possible_targets_vary) 
+    {
+        gradient_outputv.resize(target_values.length());
+        gradient_act_outputv.resize(target_values.length());
+        if(!stochastic_gradient_descent_speedup)
+            target_values_since_last_update.append(target_values);
+    }
+
+    if(!stochastic_gradient_descent_speedup)
+        feats_since_last_update.append(feat_input);
+
+    // Gradient through cost
+    if(cost_funcs[0]=="NLL") 
+    {
+        // Permits to avoid numerical precision errors
+        if(output_transfer_func == "softmax")
+            gradient_outputv[reind_target] = learning_rate*sampleweight;
+        else
+            gradient_outputv[reind_target] = learning_rate*sampleweight/(outputv[reind_target]);            
+    }
+    else if(cost_funcs[0]=="class_error")
+    {
+        PLERROR("NeuralProbabilisticLanguageModel::bprop(): gradient "
+                "cannot be computed for \"class_error\" cost");
+    }
+
+    // Gradient through output transfer function
+    if(output_transfer_func != "linear")
+    {
+        if(cost_funcs[0]=="NLL" && output_transfer_func == "softmax")
+            gradient_transfer_func(outputv,gradient_act_outputv, gradient_outputv,
+                                    output_transfer_func, reind_target);
+        else
+            gradient_transfer_func(outputv,gradient_act_outputv, gradient_outputv,
+                                    output_transfer_func);
+        gradient_last_layer = gradient_act_outputv;
+    }
+    else
+        gradient_last_layer = gradient_act_outputv;
+    
+    // Gradient through output affine transform
+
+
+    if(nhidden2 > 0) {
+        gradient_affine_transform(hidden2v, wout, bout, gradient_hidden2v, 
+                                  gradient_wout, gradient_bout, 
+                                  gradient_last_layer,
+                                  false, possible_targets_vary, 
+                                  learning_rate*sampleweight, 
+                                  weight_decay+output_layer_weight_decay,
+                                  bias_decay+output_layer_bias_decay,
+                                  target_values);
+    }
+    else if(nhidden > 0) 
+    {
+        gradient_affine_transform(hiddenv, wout, bout, gradient_hiddenv,
+                                  gradient_wout, gradient_bout, 
+                                  gradient_last_layer,
+                                  false, possible_targets_vary, 
+                                  learning_rate*sampleweight, 
+                                  weight_decay+output_layer_weight_decay,
+                                  bias_decay+output_layer_bias_decay, 
+                                  target_values);
+    }
+    else
+    {
+        gradient_affine_transform(nnet_input, wout, bout, gradient_nnet_input, 
+                                  gradient_wout, gradient_bout, 
+                                  gradient_last_layer,
+                                  (dist_rep_dim <= 0), possible_targets_vary, 
+                                  learning_rate*sampleweight, 
+                                  weight_decay+output_layer_weight_decay,
+                                  bias_decay+output_layer_bias_decay, 
+                                  target_values);
+    }
+
+
+    if(nhidden>0 && direct_in_to_out)
+    {
+        gradient_affine_transform(nnet_input, direct_wout, direct_bout,
+                                  gradient_nnet_input, 
+                                  gradient_direct_wout, gradient_direct_bout,
+                                  gradient_last_layer,
+                                  dist_rep_dim<=0, possible_targets_vary,
+                                  learning_rate*sampleweight, 
+                                  weight_decay+direct_in_to_out_weight_decay,
+                                  0,
+                                  target_values);
+    }
+
+
+    if(nhidden2 > 0)
+    {
+        gradient_transfer_func(hidden2v,gradient_act_hidden2v,gradient_hidden2v);
+        gradient_affine_transform(hiddenv, w2, b2, gradient_hiddenv, 
+                                  gradient_w2, gradient_b2, gradient_act_hidden2v,
+                                  false, false,learning_rate*sampleweight, 
+                                  weight_decay+layer2_weight_decay,
+                                  bias_decay+layer2_bias_decay);
+    }
+    if(nhidden > 0)
+    {
+        gradient_transfer_func(hiddenv,gradient_act_hiddenv,gradient_hiddenv);  
+        gradient_affine_transform(nnet_input, w1, b1, gradient_nnet_input, 
+                                  gradient_w1, gradient_b1, gradient_act_hiddenv,
+                                  dist_rep_dim<=0, false,learning_rate*sampleweight, 
+                                  weight_decay+layer1_weight_decay,
+                                  bias_decay+layer1_bias_decay);
+    }
+
+    if(dist_rep_dim > 0)
+    {
+        nfeats = 0;
+        id = 0;
+        for(int i=0; i<inputsize_; )
+        {
+            ifeats = 0;
+            for(int j=0; j<n_feat_sets; j++,i++)
+                ifeats += feats[i].length();
+            gradient_affine_transform(feat_input.subVec(nfeats,ifeats),
+                                      wout_dist_rep, bout_dist_rep,
+                                      //gradient_feat_input.subVec(nfeats,feats[i].length()),
+                                      gradient_feat_input,// Useless anyways...
+                                      gradient_wout_dist_rep,
+                                      gradient_bout_dist_rep,
+                                      gradient_nnet_input.subVec(
+                                          id*dist_rep_dim,dist_rep_dim),
+                                      true, false, learning_rate*sampleweight, 
+                                      weight_decay+
+                                      output_layer_dist_rep_weight_decay,
+                                      bias_decay+output_layer_dist_rep_bias_decay);
+            nfeats += ifeats;
+            id++;
+        }
+    }
+
+    clearProppathGradient();
+}
+
+void NeuralProbabilisticLanguageModel::bpropBeforeOutputWeights(
+    real learning_rate, 
+    real sampleweight)
+{
+}
+
+
+void NeuralProbabilisticLanguageModel::update()
+{
+
+    if(dist_rep_dim > 0)
+    {
+        update_affine_transform(feats_since_last_update, wout_dist_rep, 
+                                bout_dist_rep, gradient_wout_dist_rep,
+                                gradient_bout_dist_rep, true, false,
+                                target_values_since_last_update);
+    }
+
+    if(nhidden>0) 
+    {
+        update_affine_transform(feats_since_last_update, w1, b1, 
+                                gradient_w1, gradient_b1,
+                                dist_rep_dim<=0, false,
+                                target_values_since_last_update);
+        if(nhidden2>0) 
+        {
+            update_affine_transform(feats_since_last_update, w2, b2, 
+                                    gradient_w2, gradient_b2,
+                                    false, false,
+                                    target_values_since_last_update);
+        }
+
+        update_affine_transform(feats_since_last_update, wout, bout, 
+                                gradient_wout, gradient_bout,
+                                false, possible_targets_vary,
+                                target_values_since_last_update);
+        if(direct_in_to_out)
+        {
+            update_affine_transform(feats_since_last_update, direct_wout, 
+                                    direct_bout, 
+                                    gradient_direct_wout, gradient_direct_bout,
+                                    false, possible_targets_vary,
+                                    target_values_since_last_update);
+        }
+    }
+    else
+    {
+        update_affine_transform(feats_since_last_update, wout, bout, 
+                                gradient_wout, gradient_bout,
+                                dist_rep_dim<=0, possible_targets_vary,
+                                target_values_since_last_update);
+    }
+
+    feats_since_last_update.resize(0);
+    target_values_since_last_update.resize(0);
+}
+
+void NeuralProbabilisticLanguageModel::update_affine_transform(
+    Vec input, Mat weights, Vec bias,
+    Mat gweights, Vec gbias,
+    bool input_is_sparse, bool output_is_sparse,
+    Vec output_indices) 
+{
+    // Bias
+    if(bias.length() != 0)
+    {
+        if(output_is_sparse)
+        {
+            pval1 = gbias.data();
+            pval2 = bias.data();
+            pval3 = output_indices.data();
+            ni = output_indices.length();
+            for(int i=0; i<ni; i++)
+            {
+                pval2[(int)*pval3] += pval1[(int)*pval3];
+                pval1[(int)*pval3] = 0;
+                pval3++;
+            }
+        }
+        else
+        {
+            pval1 = gbias.data();
+            pval2 = bias.data();
+            ni = bias.length();
+            for(int i=0; i<ni; i++)
+            {
+                *pval2 += *pval1;
+                *pval1 = 0;
+                pval1++; 
+                pval2++;
+            }
+        }
+    }
+
+    // Weights
+    if(!input_is_sparse && !output_is_sparse)
+    {
+        if(!gweights.isCompact() || !weights.isCompact())
+            PLERROR("In NeuralProbabilisticLanguageModel::"
+                    "update_affine_transform(): weights or gweights is"
+                    "not a compact TMat");
+        ni = weights.length();
+        nj = weights.width();
+        pval1 = gweights.data();
+        pval2 = weights.data();
+        for(int i=0; i<ni; i++)
+            for(int j=0; j<nj; j++)
+            {
+                *pval2 += *pval1;
+                *pval1 = 0;
+                pval1++;
+                pval2++;
+            }
+    }
+    else if(!input_is_sparse && output_is_sparse)
+    {
+        ni = output_indices.length();
+        nj = input.length();
+        pval3 = output_indices.data();
+        for(int i=0; i<ni; i++)
+        {
+            for(int j=0; j<nj; j++)
+            {
+                weights(j,(int)*pval3) += gweights(j,(int)*pval3);
+                gweights(j,(int)*pval3) = 0;
+            }
+            pval3++;
+        }
+    }
+    else if(input_is_sparse && !output_is_sparse)
+    {
+        ni = input.length();
+        nj = weights.width();
+        pval3 = input.data();
+        for(int i=0; i<ni; i++)
+        {
+            pval1 = gweights[(int)(*pval3)];
+            pval2 = weights[(int)(*pval3++)];
+            for(int j=0; j<nj;j++)
+            {
+                *pval2 += *pval1;
+                *pval1 = 0;
+                pval1++;
+                pval2++;
+            }
+        }
+    }
+    else if(input_is_sparse && output_is_sparse)
+    {
+        // Weights
+        ni = input.length();
+        nj = output_indices.length();
+        pval2 = input.data();
+        for(int i=0; i<ni; i++)
+        {
+            pval3 = output_indices.data();
+            for(int j=0; j<nj; j++)
+            {
+                weights((int)(*pval2),(int)*pval3) += 
+                    gweights((int)(*pval2),(int)*pval3);
+                gweights((int)(*pval2),(int)*pval3) = 0;
+                pval3++;
+            }
+            pval2++;
+        }
+    }
+}
+
+//! Clear network's gradient fields
+void NeuralProbabilisticLanguageModel::clearProppathGradient()
+{
+    // Trick to make clearProppathGradient faster...
+    if(cost_funcs[0]=="NLL") 
+        gradient_outputv[reind_target] = 0;
+    else
+        gradient_outputv.clear();
+    gradient_act_outputv.clear();
+    
+    if(dist_rep_dim>0)
+        gradient_nnet_input.clear();
+
+    if(nhidden>0) 
+    {
+        gradient_hiddenv.clear();
+        gradient_act_hiddenv.clear();
+        if(nhidden2>0) 
+        {
+            gradient_hidden2v.clear();
+            gradient_act_hidden2v.clear();
+        }
+    }
+}
+
+
+/////////////////////////////
+// computeCostsFromOutputs //
+/////////////////////////////
+void NeuralProbabilisticLanguageModel::computeCostsFromOutputs(const Vec& inputv, 
+                                                               const Vec& outputv,
+                                                               const Vec& targetv,
+                                                               Vec& costsv) const
+{
+    PLERROR("In NeuralProbabilisticLanguageModel::computeCostsFromOutputs():"
+            "output is not enough to compute costs");
+}
+
+int NeuralProbabilisticLanguageModel::my_argmax(const Vec& vec, 
+                                                int default_compare) const
+{
+#ifdef BOUNDCHECK
+    if(vec.length()==0)
+        PLERROR("IN int argmax(const TVec<T>& vec) vec has zero length");
+#endif
+    real* v = vec.data();
+    int indexmax = default_compare;
+    real maxval = v[default_compare];
+    for(int i=0; i<vec.length(); i++)
+        if(v[i]>maxval)
+        {
+            maxval = v[i];
+            indexmax = i;
+        }
+    return indexmax;
+}
+
+///////////////////
+// computeOutput //
+///////////////////
+void NeuralProbabilisticLanguageModel::computeOutput(const Vec& inputv, 
+                                                     Vec& outputv) const
+{
+    fpropOutput(inputv, output_comp);
+    if(possible_targets_vary)
+    {
+        //row.subVec(0,inputsize_) << inputv;
+        //target_values_reference_set->getValues(row,inputsize_,target_values);
+        outputv[0] = target_values[
+            my_argmax(output_comp,rgen->uniform_multinomial_sample(
+                          output_comp.length()))];
+    }
+    else
+        outputv[0] = argmax(output_comp);
+}
+
+///////////////////////////
+// computeOutputAndCosts //
+///////////////////////////
+void NeuralProbabilisticLanguageModel::computeOutputAndCosts(const Vec& inputv, 
+                                                             const Vec& targetv, 
+                                                             Vec& outputv, 
+                                                             Vec& costsv) const
+{
+    fprop(inputv,output_comp,targetv,costsv);
+    if(possible_targets_vary)
+    {
+        //row.subVec(0,inputsize_) << inputv;
+        //target_values_reference_set->getValues(row,inputsize_,target_values);
+        outputv[0] = 
+            target_values[
+                my_argmax(output_comp,rgen->uniform_multinomial_sample(
+                              output_comp.length()))];
+    }
+    else
+        outputv[0] = argmax(output_comp);
+}
+
+/////////////////
+// fillWeights //
+/////////////////
+void NeuralProbabilisticLanguageModel::fillWeights(const Mat& weights) {
+    if (initialization_method == "zero") {
+        weights.clear();
+        return;
+    }
+    real delta;
+    int is = weights.length();
+    if (initialization_method.find("linear") != string::npos)
+        delta = 1.0 / real(is);
+    else
+        delta = 1.0 / sqrt(real(is));
+    if (initialization_method.find("normal") != string::npos)
+        rgen->fill_random_normal(weights, 0, delta);
+    else
+        rgen->fill_random_uniform(weights, -delta, delta);
+}
+
+////////////
+// forget //
+////////////
+void NeuralProbabilisticLanguageModel::forget()
+{
+    if (train_set) build();
+    total_updates=0;
+    stage = 0;
+}
+
+///////////////////////
+// getTrainCostNames //
+///////////////////////
+TVec<string> NeuralProbabilisticLanguageModel::getTrainCostNames() const
+{
+    return cost_funcs;
+}
+
+//////////////////////
+// getTestCostNames //
+//////////////////////
+TVec<string> NeuralProbabilisticLanguageModel::getTestCostNames() const
+{ 
+    return cost_funcs;
+}
+
+///////////////////////
+// add_transfer_func //
+///////////////////////
+void NeuralProbabilisticLanguageModel::add_transfer_func(const Vec& input, 
+                                                         string transfer_func) 
+    const
+{
+    if (transfer_func == "default")
+        transfer_func = hidden_transfer_func;
+    if(transfer_func=="linear")
+        return;
+    else if(transfer_func=="tanh")
+    {
+        compute_tanh(input,input);
+        return;
+    }        
+    else if(transfer_func=="sigmoid")
+    {
+        compute_sigmoid(input,input);
+        return;
+    }
+    else if(transfer_func=="softmax")
+    {
+        compute_softmax(input,input);
+        return;
+    }
+    else PLERROR("In NeuralProbabilisticLanguageModel::add_transfer_func(): "
+                 "Unknown value for transfer_func: %s",transfer_func.c_str());
+}
+
+////////////////////////////
+// gradient_transfer_func //
+////////////////////////////
+void NeuralProbabilisticLanguageModel::gradient_transfer_func(
+    Vec& output, 
+    Vec& gradient_input,
+    Vec& gradient_output,
+    string transfer_func,
+    int nll_softmax_speed_up_target) 
+{
+    if (transfer_func == "default")        
+        transfer_func = hidden_transfer_func;
+    if(transfer_func=="linear")
+    {
+        pval1 = gradient_output.data();
+        pval2 = gradient_input.data();
+        ni = output.length();
+        for(int i=0; i<ni; i++)
+            *pval2++ += *pval1++;
+        return;
+    }
+    else if(transfer_func=="tanh")
+    {
+        pval1 = gradient_output.data();
+        pval2 = output.data();
+        pval3 = gradient_input.data();
+        ni = output.length();
+        for(int i=0; i<ni; i++)
+            *pval3++ += (*pval1++)*(1.0-square(*pval2++));
+        return;
+    }        
+    else if(transfer_func=="sigmoid")
+    {
+        pval1 = gradient_output.data();
+        pval2 = output.data();
+        pval3 = gradient_input.data();
+        ni = output.length();
+        for(int i=0; i<ni; i++)
+        {
+            *pval3++ += (*pval1++)*(*pval2)*(1.0-*pval2);
+            pval2++;
+        }   
+        return;
+    }
+    else if(transfer_func=="softmax")
+    {
+        if(nll_softmax_speed_up_target<0)
+        {            
+            pval3 = gradient_input.data();
+            ni = nk = output.length();
+            for(int i=0; i<ni; i++)
+            {
+                val = output[i];
+                pval1 = gradient_output.data();
+                pval2 = output.data();
+                for(int k=0; k<nk; k++)
+                    if(k!=i)
+                        *pval3 -= *pval1++ * val * (*pval2++);
+                    else
+                    {
+                        *pval3 += *pval1++ * val * (1.0-val);
+                        pval2++;
+                    }
+                pval3++;                
+            }   
+        }
+        else // Permits speedup and avoids numerical precision errors
+        {
+            pval2 = output.data();
+            pval3 = gradient_input.data();
+            ni = output.length();
+            grad = gradient_output[nll_softmax_speed_up_target];
+            val = output[nll_softmax_speed_up_target];
+            for(int i=0; i<ni; i++)
+            {
+                if(nll_softmax_speed_up_target!=i)
+                    //*pval3++ -= grad * val * (*pval2++);
+                    *pval3++ -= grad * (*pval2++);
+                else
+                {
+                    //*pval3++ += grad * val * (1.0-val);
+                    *pval3++ += grad * (1.0-val);
+                    pval2++;
+                }
+            }   
+        }
+        return;
+    }
+    else PLERROR("In NeuralProbabilisticLanguageModel::gradient_transfer_func():"
+                 "Unknown value for transfer_func: %s",transfer_func.c_str());
+}
+
+void NeuralProbabilisticLanguageModel::add_affine_transform(
+    Vec input, 
+    Mat weights, 
+    Vec bias, Vec output, 
+    bool input_is_sparse, bool output_is_sparse,
+    Vec output_indices) const
+{
+    // Bias
+    if(bias.length() != 0)
+    {
+        if(output_is_sparse)
+        {
+            pval1 = output.data();
+            pval2 = bias.data();
+            pval3 = output_indices.data();
+            ni = output.length();
+            for(int i=0; i<ni; i++)
+                *pval1++ = pval2[(int)*pval3++];
+        }
+        else
+        {
+            pval1 = output.data();
+            pval2 = bias.data();
+            ni = output.length();
+            for(int i=0; i<ni; i++)
+                *pval1++ = *pval2++;
+        }
+    }
+
+    // Weights
+    if(!input_is_sparse && !output_is_sparse)
+    {
+        transposeProductAcc(output,weights,input);
+    }
+    else if(!input_is_sparse && output_is_sparse)
+    {
+        ni = output.length();
+        nj = input.length();
+        pval1 = output.data();
+        pval3 = output_indices.data();
+        for(int i=0; i<ni; i++)
+        {
+            pval2 = input.data();
+            for(int j=0; j<nj; j++)
+                *pval1 += (*pval2++)*weights(j,(int)*pval3);
+            pval1++;
+            pval3++;
+        }
+    }
+    else if(input_is_sparse && !output_is_sparse)
+    {
+        ni = input.length();
+        nj = output.length();
+        if(ni != 0)
+        {
+            pval3 = input.data();
+            for(int i=0; i<ni; i++)
+            {
+                pval1 = output.data();
+                pval2 = weights[(int)(*pval3++)];
+                for(int j=0; j<nj;j++)
+                    *pval1++ += *pval2++;
+            }
+        }
+    }
+    else if(input_is_sparse && output_is_sparse)
+    {
+        // Weights
+        ni = input.length();
+        nj = output.length();
+        if(ni != 0)
+        {
+            pval2 = input.data();
+            for(int i=0; i<ni; i++)
+            {
+                pval1 = output.data();
+                pval3 = output_indices.data();
+                for(int j=0; j<nj; j++)
+                    *pval1++ += weights((int)(*pval2),(int)*pval3++);
+                pval2++;
+            }
+        }
+    }
+}
+
+void NeuralProbabilisticLanguageModel::gradient_affine_transform(
+    Vec input, Mat weights, Vec bias, 
+    Vec ginput, Mat gweights, Vec gbias,
+    Vec goutput, bool input_is_sparse, 
+    bool output_is_sparse,
+    real learning_rate,
+    real weight_decay, real bias_decay,
+    Vec output_indices)
+{
+    // Bias
+    if(bias.length() != 0)
+    {
+        if(output_is_sparse)
+        {
+            pval1 = gbias.data();
+            pval2 = goutput.data();
+            pval3 = output_indices.data();
+            ni = goutput.length();
+            
+            if(fast_exact_is_equal(bias_decay, 0))
+            {
+                // Without bias decay
+                for(int i=0; i<ni; i++)
+                    pval1[(int)*pval3++] += *pval2++;
+            }
+            else
+            {
+                // With bias decay
+                if(penalty_type == "L2_square")
+                {
+                    pval4 = bias.data();
+                    val = -two(learning_rate)*bias_decay;
+                    for(int i=0; i<ni; i++)
+                    {
+                        pval1[(int)*pval3] += *pval2++ + val*(pval4[(int)*pval3]);
+                        pval3++;
+                    }
+                }
+                else if(penalty_type == "L1")
+                {
+                    pval4 = bias.data();
+                    val = -learning_rate*bias_decay;
+                    for(int i=0; i<ni; i++)
+                    {
+                        val2 = pval4[(int)*pval3];
+                        if(val2 > 0 )
+                            pval1[(int)*pval3] += *pval2 + val;
+                        else if(val2 < 0)
+                            pval1[(int)*pval3] += *pval2 - val;
+                        pval2++;
+                        pval3++;
+                    }
+                }
+            }
+        }
+        else
+        {
+            pval1 = gbias.data();
+            pval2 = goutput.data();
+            ni = goutput.length();
+            if(fast_exact_is_equal(bias_decay, 0))
+            {
+                // Without bias decay
+                for(int i=0; i<ni; i++)
+                    *pval1++ += *pval2++;
+            }
+            else
+            {
+                // With bias decay
+                if(penalty_type == "L2_square")
+                {
+                    pval3 = bias.data();
+                    val = -two(learning_rate)*bias_decay;
+                    for(int i=0; i<ni; i++)
+                    {
+                        *pval1++ += *pval2++ + val * (*pval3++);
+                    }
+                }
+                else if(penalty_type == "L1")
+                {
+                    pval3 = bias.data();
+                    val = -learning_rate*bias_decay;
+                    for(int i=0; i<ni; i++)
+                    {
+                        if(*pval3 > 0)
+                            *pval1 += *pval2 + val;
+                        else if(*pval3 < 0)
+                            *pval1 += *pval2 - val;
+                        pval1++;
+                        pval2++;
+                        pval3++;
+                    }
+                }
+            }
+        }
+    }
+
+    // Weights and input (when appropriate)
+    if(!input_is_sparse && !output_is_sparse)
+    {        
+        // Input
+        //productAcc(ginput, weights, goutput);
+        // Weights
+        //externalProductAcc(gweights, input, goutput);
+
+        // Faster code to do this, which limits the accesses
+        // to memory
+
+        ni = input.length();
+        nj = goutput.length();
+        pval3 = ginput.data();
+        pval5 = input.data();
+        
+        if(fast_exact_is_equal(weight_decay, 0))
+        {
+            // Without weight decay
+            for(int i=0; i<ni; i++) {
+                
+                pval1 = goutput.data();
+                pval2 = weights[i];
+                pval4 = gweights[i];
+                for(int j=0; j<nj; j++) {
+                    *pval3 += *pval2 * (*pval1);
+                    *pval4 += *pval5 * (*pval1);
+                    pval1++;
+                    pval2++;
+                    pval4++;
+                }
+                pval3++;
+                pval5++;
+            }   
+        }
+        else
+        {
+            //With weight decay            
+            if(penalty_type == "L2_square")
+            {
+                val = -two(learning_rate)*weight_decay;
+                for(int i=0; i<ni; i++) {   
+                    pval1 = goutput.data();
+                    pval2 = weights[i];
+                    pval4 = gweights[i];
+                    for(int j=0; j<nj; j++) {
+                        *pval3 += *pval2 * (*pval1);
+                        *pval4 += *pval5 * (*pval1) + val * (*pval2);
+                        pval1++;
+                        pval2++;
+                        pval4++;
+                    }
+                    pval3++;
+                    pval5++;
+                }
+            }
+            else if(penalty_type == "L1")
+            {
+                val = -learning_rate*weight_decay;
+                for(int i=0; i<ni; i++) {
+                    
+                    pval1 = goutput.data();
+                    pval2 = weights[i];
+                    pval4 = gweights[i];
+                    for(int j=0; j<nj; j++) {
+                        *pval3 += *pval2 * (*pval1);
+                        if(*pval2 > 0)
+                            *pval4 += *pval5 * (*pval1) + val;
+                        else if(*pval2 < 0)
+                            *pval4 += *pval5 * (*pval1) - val;
+                        pval1++;
+                        pval2++;
+                        pval4++;
+                    }
+                    pval3++;
+                    pval5++;
+                }
+            }
+        }
+    }
+    else if(!input_is_sparse && output_is_sparse)
+    {
+        ni = goutput.length();
+        nj = input.length();
+        pval1 = goutput.data();
+        pval3 = output_indices.data();
+        
+        if(fast_exact_is_equal(weight_decay, 0))
+        {
+            // Without weight decay
+            for(int i=0; i<ni; i++)
+            {
+                pval2 = input.data();
+                pval4 = ginput.data();
+                for(int j=0; j<nj; j++)
+                {
+                    // Input
+                    *pval4++ += weights(j,(int)(*pval3))*(*pval1);
+                    // Weights
+                    gweights(j,(int)(*pval3)) += (*pval2++)*(*pval1);
+                }
+                pval1++;
+                pval3++;
+            }
+        }
+        else
+        {
+            // With weight decay
+            if(penalty_type == "L2_square")
+            {
+                val = -two(learning_rate)*weight_decay;
+                for(int i=0; i<ni; i++)
+                {
+                    pval2 = input.data();
+                    pval4 = ginput.data();
+                    for(int j=0; j<nj; j++)
+                    {
+                        val2 = weights(j,(int)(*pval3));
+                        // Input
+                        *pval4++ += val2*(*pval1);
+                        // Weights
+                        gweights(j,(int)(*pval3)) += (*pval2++)*(*pval1) 
+                            + val*val2;
+                    }
+                    pval1++;
+                    pval3++;
+                }
+            }
+            else if(penalty_type == "L1")
+            {
+                val = -learning_rate*weight_decay;
+                for(int i=0; i<ni; i++)
+                {
+                    pval2 = input.data();
+                    pval4 = ginput.data();
+                    for(int j=0; j<nj; j++)
+                    {
+                        val2 = weights(j,(int)(*pval3));
+                        // Input
+                        *pval4++ += val2*(*pval1);
+                        // Weights
+                        if(val2 > 0)
+                            gweights(j,(int)(*pval3)) += (*pval2)*(*pval1) + val;
+                        else if(val2 < 0)
+                            gweights(j,(int)(*pval3)) += (*pval2)*(*pval1) - val;
+                        pval2++;
+                    }
+                    pval1++;
+                    pval3++;
+                }
+            }
+        }
+    }
+    else if(input_is_sparse && !output_is_sparse)
+    {
+        ni = input.length();
+        nj = goutput.length();
+
+        if(fast_exact_is_equal(weight_decay, 0))
+        {
+            // Without weight decay
+            if(ni != 0)
+            {
+                pval3 = input.data();
+                for(int i=0; i<ni; i++)
+                {
+                    pval1 = goutput.data();
+                    pval2 = gweights[(int)(*pval3++)];
+                    for(int j=0; j<nj;j++)
+                        *pval2++ += *pval1++;
+                }
+            }
+        }
+        else
+        {
+            // With weight decay
+            if(penalty_type == "L2_square")
+            {
+                if(ni != 0)
+                {
+                    pval3 = input.data();                    
+                    val = -two(learning_rate)*weight_decay;
+                    for(int i=0; i<ni; i++)
+                    {
+                        pval1 = goutput.data();
+                        pval2 = gweights[(int)(*pval3)];
+                        pval4 = weights[(int)(*pval3++)];
+                        for(int j=0; j<nj;j++)
+                        {
+                            *pval2++ += *pval1++ + val * (*pval4++);
+                        }
+                    }
+                }
+            }
+            else if(penalty_type == "L1")
+            {
+                if(ni != 0)
+                {
+                    pval3 = input.data();
+                    val = learning_rate*weight_decay;
+                    for(int i=0; i<ni; i++)
+                    {
+                        pval1 = goutput.data();
+                        pval2 = gweights[(int)(*pval3)];
+                        pval4 = weights[(int)(*pval3++)];
+                        for(int j=0; j<nj;j++)
+                        {
+                            if(*pval4 > 0)
+                                *pval2 += *pval1 + val;
+                            else if(*pval4 < 0)
+                                *pval2 += *pval1 - val;
+                            pval1++;
+                            pval2++;
+                            pval4++;
+                        }
+                    }
+                }
+            }
+        }
+    }
+    else if(input_is_sparse && output_is_sparse)
+    {
+        ni = input.length();
+        nj = goutput.length();
+
+        if(fast_exact_is_equal(weight_decay, 0))
+        {
+            // Without weight decay
+            if(ni != 0)
+            {
+                pval2 = input.data();
+                for(int i=0; i<ni; i++)
+                {
+                    pval1 = goutput.data();
+                    pval3 = output_indices.data();
+                    for(int j=0; j<nj; j++)
+                        gweights((int)(*pval2),(int)*pval3++) += *pval1++;
+                    pval2++;
+                }
+            }
+        }
+        else
+        {
+            // With weight decay
+            if(penalty_type == "L2_square")
+            {
+                if(ni != 0)
+                {
+                    pval2 = input.data();
+                    val = -two(learning_rate)*weight_decay;                    
+                    for(int i=0; i<ni; i++)
+                    {
+                        pval1 = goutput.data();
+                        pval3 = output_indices.data();
+                        for(int j=0; j<nj; j++)
+                        {
+                            gweights((int)(*pval2),(int)*pval3) 
+                                += *pval1++ 
+                                + val * weights((int)(*pval2),(int)*pval3);
+                            pval3++;
+                        }
+                        pval2++;
+                    }
+                }
+            }
+            else if(penalty_type == "L1")
+            {
+                if(ni != 0)
+                {
+                    pval2 = input.data();
+                    val = -learning_rate*weight_decay;                    
+                    for(int i=0; i<ni; i++)
+                    {
+                        pval1 = goutput.data();
+                        pval3 = output_indices.data();
+                        for(int j=0; j<nj; j++)
+                        {
+                            val2 = weights((int)(*pval2),(int)*pval3);
+                            if(val2 > 0)
+                                gweights((int)(*pval2),(int)*pval3) 
+                                    += *pval1 + val;
+                            else if(val2 < 0)
+                                gweights((int)(*pval2),(int)*pval3) 
+                                    += *pval1 - val;
+                            pval1++;
+                            pval3++;
+                        }
+                        pval2++;
+                    }
+                }
+            }
+        }
+    }
+
+//    gradient_penalty(input,weights,bias,gweights,gbias,input_is_sparse,output_is_sparse,
+//                     learning_rate,weight_decay,bias_decay,output_indices);
+}
+
+void NeuralProbabilisticLanguageModel::gradient_penalty(
+    Vec input, Mat weights, Vec bias, 
+    Mat gweights, Vec gbias,
+    bool input_is_sparse, bool output_is_sparse,
+    real learning_rate,
+    real weight_decay, real bias_decay,
+    Vec output_indices)
+{
+    // Bias
+    if(!fast_exact_is_equal(bias_decay, 0) && !fast_exact_is_equal(bias.length(),
+                                                                   0) )
+    {
+        if(output_is_sparse)
+        {
+            pval1 = gbias.data();
+            pval2 = bias.data();
+            pval3 = output_indices.data();
+            ni = output_indices.length();            
+            if(penalty_type == "L2_square")
+            {
+                val = -two(learning_rate)*bias_decay;
+                for(int i=0; i<ni; i++)
+                {
+                    pval1[(int)*pval3] += val*(pval2[(int)*pval3]);
+                    pval3++;
+                }
+            }
+            else if(penalty_type == "L1")
+            {
+                val = -learning_rate*bias_decay;
+                for(int i=0; i<ni; i++)
+                {
+                    val2 = pval2[(int)*pval3];
+                    if(val2 > 0 )
+                        pval1[(int)*pval3++] += val;
+                    else if(val2 < 0)
+                        pval1[(int)*pval3++] -= val;
+                }
+            }
+        }
+        else
+        {
+            pval1 = gbias.data();
+            pval2 = bias.data();
+            ni = output_indices.length();            
+            if(penalty_type == "L2_square")
+            {
+                val = -two(learning_rate)*bias_decay;
+                for(int i=0; i<ni; i++)
+                    *pval1++ += val*(*pval2++);
+            }
+            else if(penalty_type == "L1")
+            {
+                val = -learning_rate*bias_decay;
+                for(int i=0; i<ni; i++)
+                {
+                    if(*pval2 > 0)
+                        *pval1 += val;
+                    else if(*pval2 < 0)
+                        *pval1 -= val;
+                    pval1++;
+                    pval2++;
+                }
+            }
+        }
+    }
+
+    // Weights
+    if(!fast_exact_is_equal(weight_decay, 0))
+    {
+        if(!input_is_sparse && !output_is_sparse)
+        {      
+            if(penalty_type == "L2_square")
+            {
+                multiplyAcc(gweights, weights,-two(learning_rate)*weight_decay);
+            }
+            else if(penalty_type == "L1")
+            {
+                val = -learning_rate*weight_decay;
+                if(gweights.isCompact() && weights.isCompact())
+                {
+                    Mat::compact_iterator itm = gweights.compact_begin();
+                    Mat::compact_iterator itmend = gweights.compact_end();
+                    Mat::compact_iterator itx = weights.compact_begin();
+                    for(; itm!=itmend; ++itm, ++itx)
+                    {
+                        if(*itx > 0)
+                            *itm += val;
+                        else if(*itx < 0)
+                            *itm -= val;
+                    }
+                }
+                else // use non-compact iterators
+                {
+                    Mat::iterator itm = gweights.begin();
+                    Mat::iterator itmend = gweights.end();
+                    Mat::iterator itx = weights.begin();
+                    for(; itm!=itmend; ++itm, ++itx)
+                    {
+                        if(*itx > 0)
+                            *itm += val;
+                        else if(*itx < 0)
+                            *itm -= val;
+                    }
+                }
+            }
+        }
+        else if(!input_is_sparse && output_is_sparse)
+        {
+            ni = output_indices.length();
+            nj = input.length();
+            pval1 = output_indices.data();
+
+            if(penalty_type == "L2_square")
+            {
+                val = -two(learning_rate)*weight_decay;
+                for(int i=0; i<ni; i++)
+                {
+                    for(int j=0; j<nj; j++)
+                    {
+                        gweights(j,(int)(*pval1)) += val * 
+                            weights(j,(int)(*pval1));
+                    }
+                    pval1++;
+                }
+            }
+            else if(penalty_type == "L1")
+            {
+                val = -learning_rate*weight_decay;
+                for(int i=0; i<ni; i++)
+                {
+                    for(int j=0; j<nj; j++)
+                    {
+                        val2 = weights(j,(int)(*pval1));
+                        if(val2 > 0)
+                            gweights(j,(int)(*pval1)) +=  val;
+                        else if(val2 < 0)
+                            gweights(j,(int)(*pval1)) -=  val;
+                    }
+                    pval1++;
+                }
+            }
+        }
+        else if(input_is_sparse && !output_is_sparse)
+        {
+            ni = input.length();
+            nj = output_indices.length();
+            if(ni != 0)
+            {
+                pval3 = input.data();
+                if(penalty_type == "L2_square")
+                {
+                    val = -two(learning_rate)*weight_decay;
+                    for(int i=0; i<ni; i++)
+                    {
+                        pval1 = weights[(int)(*pval3)];
+                        pval2 = gweights[(int)(*pval3++)];
+                        for(int j=0; j<nj;j++)
+                            *pval2++ += val * *pval1++;
+                    }
+                }
+                else if(penalty_type == "L1")
+                {
+                    val = -learning_rate*weight_decay;
+                    for(int i=0; i<ni; i++)
+                    {
+                        pval1 = weights[(int)(*pval3)];
+                        pval2 = gweights[(int)(*pval3++)];
+                        for(int j=0; j<nj;j++)
+                        {
+                            if(*pval1 > 0)
+                                *pval2 += val;
+                            else if(*pval1 < 0)
+                                *pval2 -= val;
+                            pval2++;
+                            pval1++;
+                        }
+                    }                
+                }
+            }
+        }
+        else if(input_is_sparse && output_is_sparse)
+        {
+            ni = input.length();
+            nj = output_indices.length();
+            if(ni != 0)
+            {
+                pval1 = input.data();
+                if(penalty_type == "L2_square")
+                {
+                    val = -two(learning_rate)*weight_decay;
+                    for(int i=0; i<ni; i++)
+                    {
+                        pval2 = output_indices.data();
+                        for(int j=0; j<nj; j++)
+                        {
+                            gweights((int)(*pval1),(int)*pval2) += val*
+                                weights((int)(*pval1),(int)*pval2);
+                        pval2++;
+                        }
+                        pval1++;
+                    }
+                }
+                else if(penalty_type == "L1")
+                {
+                    val = -learning_rate*weight_decay;
+                    for(int i=0; i<ni; i++)
+                    {
+                        pval2 = output_indices.data();
+                        for(int j=0; j<nj; j++)
+                        {
+                            val2 = weights((int)(*pval1),(int)*pval2);
+                            if(val2 > 0)
+                                gweights((int)(*pval1),(int)*pval2) += val;
+                            else if(val2 < 0)
+                                gweights((int)(*pval1),(int)*pval2) -= val;
+                            pval2++;
+                        }
+                        pval1++;
+                    }
+                    
+                }
+            }
+        }
+    }
+}
+
+void NeuralProbabilisticLanguageModel::importance_sampling_gradient_update(
+    Vec& inputv, Vec& targetv, 
+    real learning_rate, int n_samples, 
+    real train_sample_weight=1)
+{
+    // TODO: implement NGramDistribution::generate()
+    //       adjust deepcopy(...)
+
+    // Do forward propagation that is common to all computations
+    fpropBeforeOutputWeights(inputv);
+
+    // Generate the n_samples samples from proposal_distribution
+    generated_samples.resize(n_samples+1);
+    densities.resize(n_samples);
+    
+    proposal_distribution->setPredictor(inputv);
+    pval1 = generated_samples.data();
+    pval2 = sample.data();
+    pval3 = densities.data();
+    for(int i=0; i<n_samples; i++)
+    {
+        proposal_distribution->generate(sample);        
+        *pval1++ = *pval2;
+        *pval3++ = proposal_distribution->density(sample);        
+    }
+
+    real sum = 0;
+    generated_samples[n_samples] = targetv[0];
+    neg_energies.resize(n_samples+1);
+    getNegativeEnergyValues(generated_samples, neg_energies);
+    
+    importance_sampling_ratios.resize(
+        importance_sampling_ratios.length() + n_samples);
+    pval1 = importance_sampling_ratios.subVec(
+        importance_sampling_ratios.length() - n_samples).data();
+    pval2 = neg_energies.data();
+    pval3 = densities.data();
+    for(int i=0; i<n_samples; i++)
+    {
+        *pval1 = exp(*pval2++)/ (*pval3++);
+        sum += *pval1;
+    }
+
+    // Compute importance sampling estimate of the gradient
+
+    // Training sample contribution...
+    gradient_last_layer.resize(1);
+    gradient_last_layer[0] = learning_rate*train_sample_weight;
+
+    if(nhidden2 > 0) {
+        gradient_affine_transform(hidden2v, wout, bout, gradient_hidden2v, 
+                                  gradient_wout, gradient_bout, 
+                                  gradient_last_layer,
+                                  false, true, learning_rate*train_sample_weight, 
+                                  weight_decay+output_layer_weight_decay,
+                                  bias_decay+output_layer_bias_decay,
+                                  generated_samples.subVec(n_samples,1));
+    }
+    else if(nhidden > 0) 
+    {
+        gradient_affine_transform(hiddenv, wout, bout, gradient_hiddenv,
+                                  gradient_wout, gradient_bout, 
+                                  gradient_last_layer,
+                                  false, true, learning_rate*train_sample_weight, 
+                                  weight_decay+output_layer_weight_decay,
+                                  bias_decay+output_layer_bias_decay, 
+                                  generated_samples.subVec(n_samples,1));
+    }
+    else
+    {
+        gradient_affine_transform(nnet_input, wout, bout, gradient_nnet_input, 
+                                  gradient_wout, gradient_bout, 
+                                  gradient_last_layer,
+                                  (dist_rep_dim <= 0), true, 
+                                  learning_rate*train_sample_weight, 
+                                  weight_decay+output_layer_weight_decay,
+                                  bias_decay+output_layer_bias_decay, 
+                                  generated_samples.subVec(n_samples,1));
+    }
+
+
+    if(nhidden>0 && direct_in_to_out)
+    {
+        gradient_affine_transform(nnet_input, direct_wout, direct_bout,
+                                  gradient_nnet_input, 
+                                  gradient_direct_wout, gradient_direct_bout,
+                                  gradient_last_layer,
+                                  dist_rep_dim<=0, true,
+                                  learning_rate*train_sample_weight, 
+                                  weight_decay+direct_in_to_out_weight_decay,
+                                  0,
+                                  generated_samples.subVec(n_samples,1));
+    }
+
+    // Importance sampling contributions
+    for(int i=0; i<n_samples; i++)
+    {
+        gradient_last_layer.resize(1);
+        gradient_last_layer[0] = -learning_rate*train_sample_weight*
+            importance_sampling_ratios[i]/sum;
+
+        if(nhidden2 > 0) {
+            gradient_affine_transform(hidden2v, wout, bout, gradient_hidden2v, 
+                                      gradient_wout, gradient_bout, 
+                                      gradient_last_layer,
+                                      false, true, 
+                                      learning_rate*train_sample_weight, 
+                                      weight_decay+output_layer_weight_decay,
+                                      bias_decay+output_layer_bias_decay,
+                                      generated_samples.subVec(i,1));
+        }
+        else if(nhidden > 0) 
+        {
+            gradient_affine_transform(hiddenv, wout, bout, gradient_hiddenv,
+                                      gradient_wout, gradient_bout, 
+                                      gradient_last_layer,
+                                      false, true, 
+                                      learning_rate*train_sample_weight, 
+                                      weight_decay+output_layer_weight_decay,
+                                      bias_decay+output_layer_bias_decay, 
+                                      generated_samples.subVec(i,1));
+        }
+        else
+        {
+            gradient_affine_transform(nnet_input, wout, bout, 
+                                      gradient_nnet_input, 
+                                      gradient_wout, gradient_bout, 
+                                      gradient_last_layer,
+                                      (dist_rep_dim <= 0), true, 
+                                      learning_rate*train_sample_weight, 
+                                      weight_decay+output_layer_weight_decay,
+                                      bias_decay+output_layer_bias_decay, 
+                                      generated_samples.subVec(i,1));
+        }
+
+
+        if(nhidden>0 && direct_in_to_out)
+        {
+            gradient_affine_transform(nnet_input, direct_wout, direct_bout,
+                                      gradient_nnet_input, 
+                                      gradient_direct_wout, gradient_direct_bout,
+                                      gradient_last_layer,
+                                      dist_rep_dim<=0, true,
+                                      learning_rate*train_sample_weight, 
+                                      weight_decay+direct_in_to_out_weight_decay,
+                                      0,
+                                      generated_samples.subVec(i,1));
+        }
+
+    }
+
+    // Propagate all contributions through rest of the network
+
+    if(nhidden2 > 0)
+    {
+        gradient_transfer_func(hidden2v,gradient_act_hidden2v,gradient_hidden2v);
+        gradient_affine_transform(hiddenv, w2, b2, gradient_hiddenv, 
+                                  gradient_w2, gradient_b2, gradient_act_hidden2v,
+                                  false, false,learning_rate*train_sample_weight, 
+                                  weight_decay+layer2_weight_decay,
+                                  bias_decay+layer2_bias_decay);
+    }
+    if(nhidden > 0)
+    {
+        gradient_transfer_func(hiddenv,gradient_act_hiddenv,gradient_hiddenv);  
+        gradient_affine_transform(nnet_input, w1, b1, gradient_nnet_input, 
+                                  gradient_w1, gradient_b1, gradient_act_hiddenv,
+                                  dist_rep_dim<=0, false,learning_rate*train_sample_weight, 
+                                  weight_decay+layer1_weight_decay,
+                                  bias_decay+layer1_bias_decay);
+    }
+
+    if(dist_rep_dim > 0)
+    {
+        nfeats = 0;
+        id = 0;
+        for(int i=0; i<inputsize_; )
+        {
+            ifeats = 0;
+            for(int j=0; j<n_feat_sets; j++,i++)
+                ifeats += feats[i].length();
+            gradient_affine_transform(feat_input.subVec(nfeats,ifeats),
+                                      wout_dist_rep, bout_dist_rep,
+                                      gradient_feat_input,// Useless anyways...
+                                      gradient_wout_dist_rep,
+                                      gradient_bout_dist_rep,
+                                      gradient_nnet_input.subVec(
+                                          id*dist_rep_dim,dist_rep_dim),
+                                      true, false, 
+                                      learning_rate*train_sample_weight, 
+                                      weight_decay+
+                                      output_layer_dist_rep_weight_decay,
+                                      bias_decay
+                                      +output_layer_dist_rep_bias_decay);
+            nfeats += ifeats;
+            id++;
+        }
+    }
+    clearProppathGradient();
+
+    // Update parameters and clear gradient
+    if(!stochastic_gradient_descent_speedup)
+        update();
+}
+
+void NeuralProbabilisticLanguageModel::getNegativeEnergyValues(
+    Vec samples, Vec neg_energies)
+{
+    if(dist_rep_dim > 0) // x -> d(x)
+    {        
+        // d(x),h1(d(x)),h2(h1(d(x))) -> o(x)
+
+        add_affine_transform(last_layer,wout,bout,neg_energies,false,
+                             true,samples);            
+        if(direct_in_to_out && nhidden>0)
+            add_affine_transform(nnet_input,direct_wout,direct_bout,
+                                 neg_energies,false,true,
+                                 samples);
+    }
+    else
+    {
+        // x, h1(x),h2(h1(x)) -> o(x)
+        add_affine_transform(last_layer,wout,bout,samples,nhidden<=0,
+                             true,samples);            
+        if(direct_in_to_out && nhidden>0)
+            add_affine_transform(feat_input,direct_wout,direct_bout,
+                                 neg_energies,true,true,
+                                 samples);
+    }
+}
+
+void NeuralProbabilisticLanguageModel::compute_softmax(const Vec& x, 
+                                                       const Vec& y) const
+{
+    int n = x.length();
+    
+//    real* yp = y.data();
+//    real* xp = x.data();
+//    for(int i=0; i<n; i++)
+//    {
+//        *yp++ = *xp > 1e-5 ? *xp : 1e-5;
+//        xp++;
+//    }
+
+    if (n>0)
+    {
+        real* yp = y.data();
+        real* xp = x.data();
+        real maxx = max(x);
+        real s = 0;
+        for (int i=0;i<n;i++)
+            s += (*yp++ = safeexp(*xp++-maxx));
+        if (s == 0) PLERROR("trying to divide by 0 in softmax");
+        s = 1.0 / s;
+        yp = y.data();
+        for (int i=0;i<n;i++)
+            *yp++ *= s;
+    }
+}
+
+real NeuralProbabilisticLanguageModel::nll(const Vec& outputv, int target) const
+{
+    return -safeflog(outputv[target]);
+}
+    
+real NeuralProbabilisticLanguageModel::classification_loss(const Vec& outputv, 
+                                                           int target) const
+{
+    return (argmax(outputv) == target ? 0 : 1);
+}
+
+void NeuralProbabilisticLanguageModel::initializeParams(bool set_seed)
+{
+    if (set_seed) {
+        if (seed_>=0)
+            rgen->manual_seed(seed_);
+    }
+
+
+    PP<Dictionary> dict = train_set->getDictionary(inputsize_);
+    total_output_size = dict->size();
+
+    total_feats_per_token = 0;
+    for(int i=0; i<n_feat_sets; i++)
+        total_feats_per_token += feat_sets[i]->size();
+
+    int nnet_inputsize;
+    if(dist_rep_dim > 0)
+    {
+        wout_dist_rep.resize(total_feats_per_token,dist_rep_dim);
+        bout_dist_rep.resize(dist_rep_dim);
+        nnet_inputsize = dist_rep_dim*inputsize_/n_feat_sets;
+        nnet_input.resize(nnet_inputsize);
+
+        fillWeights(wout_dist_rep);
+        bout_dist_rep.clear();
+
+        gradient_wout_dist_rep.resize(total_feats_per_token,dist_rep_dim);
+        gradient_bout_dist_rep.resize(dist_rep_dim);
+        gradient_nnet_input.resize(nnet_inputsize);
+        gradient_wout_dist_rep.clear();
+        gradient_bout_dist_rep.clear();
+        gradient_nnet_input.clear();
+    }
+    else
+    {
+        nnet_inputsize = total_feats_per_token*inputsize_/n_feat_sets;
+        nnet_input = feat_input;
+    }
+
+    if(nhidden>0) 
+    {
+        w1.resize(nnet_inputsize,nhidden);
+        b1.resize(nhidden);
+        hiddenv.resize(nhidden);
+
+        fillWeights(w1);
+        b1.clear();
+
+        gradient_w1.resize(nnet_inputsize,nhidden);
+        gradient_b1.resize(nhidden);
+        gradient_hiddenv.resize(nhidden);
+        gradient_act_hiddenv.resize(nhidden);
+        gradient_w1.clear();
+        gradient_b1.clear();
+        gradient_hiddenv.clear();
+        gradient_act_hiddenv.clear();
+        if(nhidden2>0) 
+        {
+            w2.resize(nhidden,nhidden2);
+            b2.resize(nhidden2);
+            hidden2v.resize(nhidden2);
+            wout.resize(nhidden2,total_output_size);
+            bout.resize(total_output_size);
+
+            fillWeights(w2);
+            b2.clear();
+
+            gradient_w2.resize(nhidden,nhidden2);
+            gradient_b2.resize(nhidden2);
+            gradient_hidden2v.resize(nhidden2);
+            gradient_act_hidden2v.resize(nhidden2);
+            gradient_wout.resize(nhidden2,total_output_size);
+            gradient_bout.resize(total_output_size);
+            gradient_w2.clear();
+            gradient_b2.clear();
+            gradient_hidden2v.clear();
+            gradient_act_hidden2v.clear();
+            gradient_wout.clear();
+            gradient_bout.clear();
+        }
+        else
+        {
+            wout.resize(nhidden,total_output_size);
+            bout.resize(total_output_size);
+
+            gradient_wout.resize(nhidden,total_output_size);
+            gradient_bout.resize(total_output_size);
+            gradient_wout.clear();
+            gradient_bout.clear();
+        }
+            
+        if(direct_in_to_out)
+        {
+            direct_wout.resize(nnet_inputsize,total_output_size);
+            direct_bout.resize(0); // Because it is not used
+
+            fillWeights(direct_wout);
+                
+            gradient_direct_wout.resize(nnet_inputsize,total_output_size);
+            gradient_direct_wout.clear();
+            gradient_direct_bout.resize(0); // idem
+        }
+    }
+    else
+    {
+        wout.resize(nnet_inputsize,total_output_size);
+        bout.resize(total_output_size);
+
+        gradient_wout.resize(nnet_inputsize,total_output_size);
+        gradient_bout.resize(total_output_size);
+        gradient_wout.clear();
+        gradient_bout.clear();
+    }
+
+    //fillWeights(wout);
+    
+    if (fixed_output_weights) {
+        static Vec values;
+        if (values.size()==0)
+        {
+            values.resize(2);
+            values[0]=-1;
+            values[1]=1;
+        }
+        rgen->fill_random_discrete(wout.toVec(), values);
+    }
+    else 
+        fillWeights(wout);
+
+    bout.clear();
+
+    gradient_outputv.resize(total_output_size);
+    gradient_act_outputv.resize(total_output_size);
+    gradient_outputv.clear();
+    gradient_act_outputv.clear();
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void NeuralProbabilisticLanguageModel::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // Private variables
+    deepCopyField(target_values,copies);
+    deepCopyField(output_comp,copies);
+    deepCopyField(row,copies);
+    deepCopyField(last_layer,copies);
+    deepCopyField(gradient_last_layer,copies);
+    deepCopyField(feats,copies);
+    deepCopyField(gradient,copies);
+    deepCopyField(neg_energies,copies);
+    deepCopyField(densities,copies);
+
+    // Protected variables
+    deepCopyField(feat_input,copies);
+    deepCopyField(gradient_feat_input,copies);
+    deepCopyField(nnet_input,copies);
+    deepCopyField(gradient_nnet_input,copies);
+    deepCopyField(hiddenv,copies);
+    deepCopyField(gradient_hiddenv,copies);
+    deepCopyField(gradient_act_hiddenv,copies);
+    deepCopyField(hidden2v,copies);
+    deepCopyField(gradient_hidden2v,copies);
+    deepCopyField(gradient_act_hidden2v,copies);
+    deepCopyField(gradient_outputv,copies);
+    deepCopyField(gradient_act_outputv,copies);
+    deepCopyField(rgen,copies);
+    deepCopyField(feats_since_last_update,copies);
+    deepCopyField(target_values_since_last_update,copies);
+    deepCopyField(val_string_reference_set,copies);
+    deepCopyField(target_values_reference_set,copies);
+    deepCopyField(importance_sampling_ratios,copies);
+    deepCopyField(sample,copies);
+    deepCopyField(generated_samples,copies);
+
+    // Public variables
+    deepCopyField(w1,copies);
+    deepCopyField(gradient_w1,copies);
+    deepCopyField(b1,copies);
+    deepCopyField(gradient_b1,copies);
+    deepCopyField(w2,copies);
+    deepCopyField(gradient_w2,copies);
+    deepCopyField(b2,copies);
+    deepCopyField(gradient_b2,copies);
+    deepCopyField(wout,copies);
+    deepCopyField(gradient_wout,copies);
+    deepCopyField(bout,copies);
+    deepCopyField(gradient_bout,copies);
+    deepCopyField(direct_wout,copies);
+    deepCopyField(gradient_direct_wout,copies);
+    deepCopyField(direct_bout,copies);
+    deepCopyField(gradient_direct_bout,copies);
+    deepCopyField(wout_dist_rep,copies);
+    deepCopyField(gradient_wout_dist_rep,copies);
+    deepCopyField(bout_dist_rep,copies);
+    deepCopyField(gradient_bout_dist_rep,copies);
+
+    // Public build options
+    deepCopyField(cost_funcs,copies);
+    deepCopyField(feat_sets,copies);
+    deepCopyField(proposal_distribution,copies);
+
+    PLERROR("not up to date");
+}
+
+////////////////
+// outputsize //
+////////////////
+int NeuralProbabilisticLanguageModel::outputsize() const {
+    return targetsize_;
+}
+
+///////////
+// train //
+///////////
+void NeuralProbabilisticLanguageModel::train()
+{
+    //Profiler::activate();
+    if(!train_set)
+        PLERROR("In NeuralProbabilisticLanguageModel::train, "
+                "you did not setTrainingSet");
+
+    if(!train_stats)
+        PLERROR("In NeuralProbabilisticLanguageModel::train, "
+                "you did not setTrainStatsCollector");
+ 
+    Vec outputv(total_output_size);
+    Vec costsv(getTrainCostNames().length());
+    Vec inputv(train_set->inputsize());
+    Vec targetv(train_set->targetsize());
+    real sample_weight = 1;
+
+    int l = train_set->length();  
+    int bs = batch_size>0 ? batch_size : l;
+
+    // Importance sampling speedup variables
+    
+    // Effective sample size statistics
+    real effective_sample_size_sum = 0;
+    real effective_sample_size_square_sum = 0;
+    real importance_sampling_ratio_k = 0;
+    // Current true sample size;
+    int n_samples = 0;
+
+    real 
+
+    PP<ProgressBar> pb;
+    if(report_progress)
+        pb = new ProgressBar("Training " + classname() + " from stage " 
+                             + tostring(stage) + " to " 
+                             + tostring(nstages), nstages-stage);
+
+    //if(stage == 0)
+    //{
+    //    for(int t=0; t<l;t++)
+    //    {
+    //        cout << "t=" << t << " ";
+    //        train_set->getExample(t,inputv,targetv,sample_weight);
+    //        row.subVec(0,inputsize_) << inputv;
+    //        train_set->getValues(row,inputsize_,target_values);
+    //        if(target_values.length() != 1)
+    //            verify_gradient(inputv,targetv,1e-6);
+    //    }
+    //    return;
+    //}
+
+    Mat old_gradient_wout;
+    Vec old_gradient_bout;
+    Mat old_gradient_wout_dist_rep;
+    Vec old_gradient_bout_dist_rep;
+    Mat old_gradient_w1;
+    Vec old_gradient_b1;
+    Mat old_gradient_w2;
+    Vec old_gradient_b2;
+    Mat old_gradient_direct_wout;
+
+    if(stochastic_gradient_descent_speedup)
+    {
+        // Trick to make stochastic gradient descent faster
+
+        old_gradient_wout = gradient_wout;
+        old_gradient_bout = gradient_bout;
+        gradient_wout = wout;
+        gradient_bout = bout;
+        
+        if(dist_rep_dim > 0)
+        {
+            old_gradient_wout_dist_rep = gradient_wout_dist_rep;
+            old_gradient_bout_dist_rep = gradient_bout_dist_rep;
+            gradient_wout_dist_rep = wout_dist_rep;
+            gradient_bout_dist_rep = bout_dist_rep;
+        }
+
+        if(nhidden>0) 
+        {
+            old_gradient_w1 = gradient_w1;
+            old_gradient_b1 = gradient_b1;
+            gradient_w1 = w1;
+            gradient_b1 = b1;
+            if(nhidden2>0) 
+            {
+                old_gradient_w2 = gradient_w2;
+                old_gradient_b2 = gradient_b2;
+                gradient_w2 = w2;
+                gradient_b2 = b2;
+            }
+            
+            if(direct_in_to_out)
+            {
+                old_gradient_direct_wout = gradient_direct_wout;
+                gradient_direct_wout = direct_wout;
+            }
+        }
+    }
+
+    int initial_stage = stage;
+    while(stage<nstages)
+    {
+        for(int t=0; t<l;)
+        {
+            //if(t%1000 == 0)
+            //{
+            //    cout << "Time: " << clock()/CLOCKS_PER_SEC << " seconds." << endl;
+            //}
+            for(int i=0; i<bs; i++)
+            {
+                //if(t == 71705)
+                //    cout << "It's going to fuck !!!" << endl;
+                
+                //if(t == 71704)
+                //    cout << "It's going to fuck !!!" << endl;
+                
+                train_set->getExample(t%l,inputv,targetv,sample_weight);
+
+                if(proposal_distributions)
+                {
+                    n_samples = 0;
+                    importance_sampling_ratios.resize(0);
+                    effective_sample_size_sum = 0;
+                    effective_sample_size_square_sum = 0;                    
+                    while(effective_sample_size < minimum_effective_sample_size)
+                    {
+                        if(n_samples >= total_output_size)
+                        {
+                            gradient_last_layer.resize(total_output_size);
+                            
+                            fprop(inputv,outputv,targetv,costsv,sample_weight);
+                            bprop(inputv,outputv,targetv,costsv,
+                                  start_learning_rate/
+                                  (bs*(1.0+decrease_constant*total_updates)),
+                                  sample_weight);
+                            train_stats->update(costsv);
+                            break;
+                        }
+                        
+                        importance_sampling_gradient_update(
+                            inputv,targetv,
+                            start_learning_rate/
+                            (bs*(1.0+decrease_constant*total_updates)),
+                            sampling_block_size,
+                            sampleweight
+                            );
+
+                        // Update effective sample size
+                        pval1 = importance_sampling_ratios.subVec(
+                            nsamples,sampling_block_size).data();
+                        for(int k=0; k<sampling_block_size; k++)
+                        {                            
+                            effective_sample_size_sum += *pval1;
+                            effective_sample_size_square_sum += *pval1 * (*pval1);
+                            pval1++;
+                        }
+                        
+                        effective_sample_size = 
+                            (effective_sample_size_sum*effective_sample_size_sum)/
+                            effective_sample_size_square_sum;
+                        n_samples += sampling_block_size;
+                    }
+                }
+                else
+                {
+                    //Profiler::start("fprop()");
+                    fprop(inputv,outputv,targetv,costsv,sample_weight);
+                    //Profiler::end("fprop()");
+                    //Profiler::start("bprop()");
+                    bprop(inputv,outputv,targetv,costsv,
+                          start_learning_rate/
+                          (bs*(1.0+decrease_constant*total_updates)),
+                          sample_weight);
+                    //Profiler::end("bprop()");
+                    train_stats->update(costsv);
+                }
+                t++;
+            }
+            // Update
+            if(!stochastic_gradient_descent_speedup)
+                update();
+            total_updates++;
+        }
+        train_stats->finalize();
+        ++stage;
+        if(verbosity>2)
+            cout << "Epoch " << stage << " train objective: " 
+                 << train_stats->getMean() << endl;
+        if(pb) pb->update(stage-initial_stage);
+    }
+
+    if(stochastic_gradient_descent_speedup)
+    {
+        // Trick to make stochastic gradient descent faster
+
+        gradient_wout = old_gradient_wout;
+        gradient_bout = old_gradient_bout;
+        
+        if(dist_rep_dim > 0)
+        {
+            gradient_wout_dist_rep = old_gradient_wout_dist_rep;
+            gradient_bout_dist_rep = old_gradient_bout_dist_rep;
+        }
+
+        if(nhidden>0) 
+        {
+            gradient_w1 = old_gradient_w1;
+            gradient_b1 = old_gradient_b1;
+            if(nhidden2>0) 
+            {
+                gradient_w2 = old_gradient_w2;
+                gradient_b2 = old_gradient_b2;
+            }
+            
+            if(direct_in_to_out)
+            {
+                gradient_direct_wout = old_gradient_direct_wout;
+            }
+        }
+    }
+    //Profiler::report(cout);
+}
+
+void NeuralProbabilisticLanguageModel::verify_gradient(
+    Vec& input, Vec targetv, real step)
+{
+    Vec costsv(getTrainCostNames().length());
+    real sampleweight = 1;
+    real verify_step = step;
+    
+    // To avoid the interaction between fprop and this function
+    int nfeats = 0;
+    int id = 0;
+    int ifeats = 0;
+
+    Vec est_gradient_bout;
+    Mat est_gradient_wout;
+    Vec est_gradient_bout_dist_rep;
+    Mat est_gradient_wout_dist_rep;
+    Vec est_gradient_b1;
+    Mat est_gradient_w1;
+    Vec est_gradient_b2;
+    Mat est_gradient_w2;
+    Vec est_gradient_direct_bout;
+    Mat est_gradient_direct_wout;
+
+    int nnet_inputsize;
+    if(dist_rep_dim > 0)
+    {
+        nnet_inputsize = dist_rep_dim*inputsize_/n_feat_sets;
+        est_gradient_wout_dist_rep.resize(total_feats_per_token,dist_rep_dim);
+        est_gradient_bout_dist_rep.resize(dist_rep_dim);
+        est_gradient_wout_dist_rep.clear();
+        est_gradient_bout_dist_rep.clear();
+        gradient_wout_dist_rep.clear();
+        gradient_bout_dist_rep.clear();
+    }
+    else
+    {
+        nnet_inputsize = total_feats_per_token*inputsize_/n_feat_sets;
+    }
+    
+    if(nhidden>0) 
+    {
+        est_gradient_w1.resize(nnet_inputsize,nhidden);
+        est_gradient_b1.resize(nhidden);
+        est_gradient_w1.clear();
+        est_gradient_b1.clear();
+        gradient_w1.clear();
+        gradient_b1.clear();
+        if(nhidden2>0) 
+        {
+            est_gradient_w2.resize(nhidden,nhidden2);
+            est_gradient_b2.resize(nhidden2);
+            est_gradient_wout.resize(nhidden2,total_output_size);
+            est_gradient_bout.resize(total_output_size);
+            est_gradient_w2.clear();
+            est_gradient_b2.clear();
+            est_gradient_wout.clear();
+            est_gradient_bout.clear();
+            gradient_w2.clear();
+            gradient_b2.clear();
+            gradient_wout.clear();
+            gradient_bout.clear();
+        }
+        else
+        {
+            est_gradient_wout.resize(nhidden,total_output_size);
+            est_gradient_bout.resize(total_output_size);
+            est_gradient_wout.clear();
+            est_gradient_bout.clear();
+            gradient_wout.clear();
+            gradient_bout.clear();
+        }
+            
+        if(direct_in_to_out)
+        {
+            est_gradient_direct_wout.resize(nnet_inputsize,total_output_size);
+            est_gradient_direct_wout.clear();
+            est_gradient_direct_bout.resize(0); // idem
+            gradient_direct_wout.clear();                        
+        }
+    }
+    else
+    {
+        est_gradient_wout.resize(nnet_inputsize,total_output_size);
+        est_gradient_bout.resize(total_output_size);
+        est_gradient_wout.clear();
+        est_gradient_bout.clear();
+        gradient_wout.clear();
+        gradient_bout.clear();
+    }
+
+    fprop(input, output_comp, targetv, costsv);
+    bprop(input,output_comp,targetv,costsv,
+          -1, sampleweight);
+    clearProppathGradient();
+    
+    // Compute estimated gradient
+
+    if(dist_rep_dim > 0) 
+    {        
+        nfeats = 0;
+        id = 0;
+        for(int i=0; i<inputsize_;)
+        {
+            ifeats = 0;
+            for(int j=0; j<n_feat_sets; j++,i++)
+                ifeats += feats[i].length();
+            verify_gradient_affine_transform(
+                input,output_comp, targetv, costsv, sampleweight,
+                feat_input.subVec(nfeats,ifeats),
+                wout_dist_rep, bout_dist_rep,
+                est_gradient_wout_dist_rep, est_gradient_bout_dist_rep,
+                true, false, verify_step);
+            nfeats += ifeats;
+            id++;
+        }
+
+        cout << "Verify wout_dist_rep" << endl;
+        output_gradient_verification(gradient_wout_dist_rep.toVec(), 
+                                     est_gradient_wout_dist_rep.toVec());
+        cout << "Verify bout_dist_rep" << endl;
+        output_gradient_verification(gradient_bout_dist_rep, 
+                                     est_gradient_bout_dist_rep);
+        gradient_wout_dist_rep.clear();
+        gradient_bout_dist_rep.clear();
+
+        if(nhidden>0) 
+        {
+            verify_gradient_affine_transform(
+                input,output_comp, targetv, costsv, sampleweight,
+                nnet_input,w1,b1,
+                est_gradient_w1, est_gradient_b1, false,false, verify_step);
+
+            cout << "Verify w1" << endl;
+            output_gradient_verification(gradient_w1.toVec(), 
+                                         est_gradient_w1.toVec());
+            cout << "Verify b1" << endl;
+            output_gradient_verification(gradient_b1, est_gradient_b1);
+            
+            if(nhidden2>0) 
+            {
+                verify_gradient_affine_transform(
+                    input,output_comp, targetv, costsv, sampleweight,    
+                    hiddenv,w2,b2,
+                    est_gradient_w2, est_gradient_b2,
+                    false,false, verify_step);
+                cout << "Verify w2" << endl;
+                output_gradient_verification(gradient_w2.toVec(), 
+                                             est_gradient_w2.toVec());
+                cout << "Verify b2" << endl;
+                output_gradient_verification(gradient_b2, est_gradient_b2);
+
+                last_layer = hidden2v;
+            }
+            else
+                last_layer = hiddenv;
+        }
+        else
+            last_layer = nnet_input;
+
+        verify_gradient_affine_transform(
+            input,output_comp, targetv, costsv, sampleweight,
+            last_layer,wout,bout,
+            est_gradient_wout, est_gradient_bout, false,
+            possible_targets_vary,verify_step,target_values);
+
+        cout << "Verify wout" << endl;
+        output_gradient_verification(gradient_wout.toVec(), 
+                                     est_gradient_wout.toVec());
+        cout << "Verify bout" << endl;
+        output_gradient_verification(gradient_bout, est_gradient_bout);
+ 
+        if(direct_in_to_out && nhidden>0)
+        {
+            verify_gradient_affine_transform(
+                input,output_comp, targetv, costsv, sampleweight,
+                nnet_input,direct_wout,direct_bout,
+                est_gradient_direct_wout, est_gradient_direct_bout,false,
+                possible_targets_vary, verify_step, target_values);
+            cout << "Verify direct_wout" << endl;
+            output_gradient_verification(gradient_direct_wout.toVec(), 
+                                         est_gradient_direct_wout.toVec());
+            //cout << "Verify direct_bout" << endl;
+            //output_gradient_verification(gradient_direct_bout, est_gradient_direct_bout);
+        }
+    }
+    else
+    {        
+        if(nhidden>0)
+        {
+            verify_gradient_affine_transform(
+                input,output_comp, targetv, costsv, sampleweight,
+                feat_input,w1,b1,
+                est_gradient_w1, est_gradient_b1,
+                true,false, verify_step);
+
+            cout << "Verify w1" << endl;
+            output_gradient_verification(gradient_w1.toVec(), 
+                                         est_gradient_w1.toVec());
+            cout << "Verify b1" << endl;
+            output_gradient_verification(gradient_b1, est_gradient_b1);
+
+            if(nhidden2>0)
+            {
+                verify_gradient_affine_transform(
+                    input,output_comp, targetv, costsv, sampleweight,
+                    hiddenv,w2,b2,
+                    est_gradient_w2, est_gradient_b2,true,false,
+                    verify_step);
+
+                cout << "Verify w2" << endl;
+                output_gradient_verification(gradient_w2.toVec(), 
+                                             est_gradient_w2.toVec());
+                cout << "Verify b2" << endl;
+                output_gradient_verification(gradient_b2, est_gradient_b2);
+                
+                last_layer = hidden2v;
+            }
+            else
+                last_layer = hiddenv;
+        }
+        else
+            last_layer = feat_input;
+        
+        verify_gradient_affine_transform(
+            input,output_comp, targetv, costsv, sampleweight,
+            last_layer,wout,bout,
+            est_gradient_wout, est_gradient_bout, nhidden<=0,
+            possible_targets_vary,verify_step, target_values);
+
+        cout << "Verify wout" << endl;
+        output_gradient_verification(gradient_wout.toVec(), 
+                                     est_gradient_wout.toVec());
+        cout << "Verify bout" << endl;
+        output_gradient_verification(gradient_bout, est_gradient_bout);
+        
+        if(direct_in_to_out && nhidden>0)
+        {
+            verify_gradient_affine_transform(
+                input,output_comp, targetv, costsv, sampleweight,
+                feat_input,direct_wout,direct_bout,
+                est_gradient_wout, est_gradient_bout,true,
+                possible_targets_vary, verify_step,target_values);
+            cout << "Verify direct_wout" << endl;
+            output_gradient_verification(gradient_direct_wout.toVec(), 
+                                         est_gradient_direct_wout.toVec());
+            cout << "Verify direct_bout" << endl;
+            output_gradient_verification(gradient_direct_bout, 
+                                         est_gradient_direct_bout);
+        }
+    }
+
+}
+
+void NeuralProbabilisticLanguageModel::verify_gradient_affine_transform(
+    Vec global_input, Vec& global_output, Vec& global_targetv,
+    Vec& global_costs, real sampleweight,
+    Vec input, Mat weights, Vec bias,
+    Mat est_gweights, Vec est_gbias,  
+    bool input_is_sparse, bool output_is_sparse,
+    real step,
+    Vec output_indices) const
+{
+    real *pval1, *pval2, *pval3;
+    int ni,nj;
+    real out1,out2;
+    // Bias
+    if(bias.length() != 0)
+    {
+        if(output_is_sparse)
+        {
+            pval1 = est_gbias.data();
+            pval2 = bias.data();
+            pval3 = output_indices.data();
+            ni = output_indices.length();
+            for(int i=0; i<ni; i++)
+            {
+                pval2[(int)*pval3] += step;
+                fprop(global_input, global_output, global_targetv, 
+                      global_costs, sampleweight);
+                out1 = global_costs[0];
+                pval2[(int)*pval3] -= 2*step;
+                fprop(global_input, global_output, global_targetv, 
+                      global_costs, sampleweight);
+                out2 = global_costs[0];
+                pval1[(int)*pval3] = (out1-out2)/(2*step);
+                pval2[(int)*pval3] += step;
+                pval3++;
+            }
+        }
+        else
+        {
+            pval1 = est_gbias.data();
+            pval2 = bias.data();
+            ni = bias.length();
+            for(int i=0; i<ni; i++)
+            {
+                *pval2 += step;
+                fprop(global_input, global_output, global_targetv, 
+                      global_costs, sampleweight);
+                out1 = global_costs[0];
+                *pval2 -= 2*step;
+                fprop(global_input, global_output, global_targetv, 
+                      global_costs, sampleweight);
+                out2 = global_costs[0];
+                *pval1 = (out1-out2)/(2*step);
+                *pval2 += step;
+                pval1++; 
+                pval2++;
+            }
+        }
+    }
+
+    // Weights
+    if(!input_is_sparse && !output_is_sparse)
+    {
+        ni = weights.length();
+        nj = weights.width();
+        for(int i=0; i<ni; i++)
+            for(int j=0; j<nj; j++)
+            {
+                weights(i,j) += step;
+                fprop(global_input, global_output, global_targetv, 
+                      global_costs, sampleweight);
+                out1 = global_costs[0];
+                weights(i,j) -= 2*step;
+                fprop(global_input, global_output, global_targetv, 
+                      global_costs, sampleweight);
+                out2 = global_costs[0];
+                weights(i,j) += step;
+                est_gweights(i,j) = (out1-out2)/(2*step);
+            }
+    }
+    else if(!input_is_sparse && output_is_sparse)
+    {
+        ni = output_indices.length();
+        nj = input.length();
+        pval3 = output_indices.data();
+        for(int i=0; i<ni; i++)
+        {
+            for(int j=0; j<nj; j++)
+            {
+                weights(j,(int)*pval3) += step;
+                fprop(global_input, global_output, global_targetv, 
+                      global_costs, sampleweight);
+                out1 = global_costs[0];
+                weights(j,(int)*pval3) -= 2*step;
+                fprop(global_input, global_output, global_targetv, 
+                      global_costs, sampleweight);
+                out2 = global_costs[0];
+                weights(j,(int)*pval3) += step;
+                est_gweights(j,(int)*pval3) = (out1-out2)/(2*step);
+//                if(target_values.length() != 1 && input[j] != 0 && (out1-out2)/(2*step) == 0)
+//                {                    
+//                    print_what_the_fuck();
+//                    weights(j,(int)*pval3) += 1;
+//                    fprop(global_input, global_output, global_targetv, global_costs, sampleweight);
+//                    weights(j,(int)*pval3) -= 1;
+//                    cout << "out1 - global_costs[0] =" << out1-global_costs[0] << endl;
+//                }
+            }
+            pval3++;
+        }
+    }
+    else if(input_is_sparse && !output_is_sparse)
+    {
+        ni = input.length();
+        nj = weights.width();
+        if(ni != 0 )
+        {
+            pval3 = input.data();
+            for(int i=0; i<ni; i++)
+            {
+                pval1 = est_gweights[(int)(*pval3)];
+                pval2 = weights[(int)(*pval3++)];
+                for(int j=0; j<nj;j++)
+                {
+                    *pval2 += step;
+                    fprop(global_input, global_output, global_targetv, 
+                          global_costs, sampleweight);
+                    out1 = global_costs[0];
+                    *pval2 -= 2*step;
+                    fprop(global_input, global_output, global_targetv, 
+                          global_costs, sampleweight);
+                    out2 = global_costs[0];
+                    *pval1 = (out1-out2)/(2*step);
+                    *pval2 += step;
+                    pval1++;
+                    pval2++;
+                }
+            }
+        }
+    }
+    else if(input_is_sparse && output_is_sparse)
+    {
+        // Weights
+        ni = input.length();
+        nj = output_indices.length();
+        if(ni != 0)
+        {
+            pval2 = input.data();
+            for(int i=0; i<ni; i++)
+            {
+                pval3 = output_indices.data();
+                for(int j=0; j<nj; j++)
+                {
+                    weights((int)(*pval2),(int)*pval3) += step;
+                    fprop(global_input, global_output, global_targetv, 
+                          global_costs, sampleweight);
+                    out1 = global_costs[0];
+                    weights((int)(*pval2),(int)*pval3) -= 2*step;
+                    fprop(global_input, global_output, global_targetv, 
+                          global_costs, sampleweight);
+                    out2 = global_costs[0];
+                    est_gweights((int)(*pval2),(int)*pval3)  = 
+                        (out1-out2)/(2*step);
+                    weights((int)(*pval2),(int)*pval3) += step;
+                    pval3++;
+                }
+                pval2++;
+            }
+        }
+    }
+}
+
+
+void NeuralProbabilisticLanguageModel::output_gradient_verification(
+    Vec grad, Vec est_grad)
+{
+    // Inspired from Func::verifyGradient()
+
+    Vec num = apply(grad - est_grad,(tRealFunc)FABS);
+    Vec denom = real(0.5)*apply(grad + est_grad,(tRealFunc)FABS);
+    for (int i = 0; i < num.length(); i++)
+    {
+        if (!fast_exact_is_equal(num[i], 0))
+            num[i] /= denom[i];
+        else
+            if(!fast_exact_is_equal(denom[i],0))
+                cout << "at position " << i << " num[i] == 0 but denom[i] = " 
+                     << denom[i] << endl;
+    }
+    int pos = argmax(num);
+    cout << max(num) << " (at position " << pos << "/" << num.length()
+         << ", computed = " << grad[pos] << " and estimated = "
+         << est_grad[pos] << ")" << endl;
+
+    real norm_grad = norm(grad);
+    real norm_est_grad = norm(est_grad);
+    real cos_angle = fast_exact_is_equal(norm_grad*norm_est_grad,
+                                         0)
+        ? MISSING_VALUE
+        : dot(grad,est_grad) /
+        (norm_grad*norm_est_grad);
+    if (cos_angle > 1)
+        cos_angle = 1;      // Numerical imprecisions can lead to such situation.
+    cout << "grad.length() = " << grad.length() << endl;
+    cout << "cos(angle) : " << cos_angle << endl;
+    cout << "angle : " << ( is_missing(cos_angle) ? MISSING_VALUE
+                            : acos(cos_angle) ) << endl;
+}
+
+void NeuralProbabilisticLanguageModel::batchComputeOutputAndConfidence(
+    VMat inputs, real probability,
+    VMat outputs_and_confidence) const
+{
+    val_string_reference_set = inputs;
+    inherited::batchComputeOutputAndConfidence(inputs,
+                                               probability,
+                                               outputs_and_confidence);
+    val_string_reference_set = train_set;
+}
+
+void NeuralProbabilisticLanguageModel::use(VMat testset, VMat outputs) const
+{
+    val_string_reference_set = testset;
+    if(testset->width() > train_set->inputsize())
+        target_values_reference_set = testset;
+    target_values_reference_set = testset;
+    inherited::use(testset,outputs);
+    val_string_reference_set = train_set;
+    if(testset->width() > train_set->inputsize())
+        target_values_reference_set = train_set;
+}
+
+void NeuralProbabilisticLanguageModel::test(VMat testset, 
+                                            PP<VecStatsCollector> test_stats, 
+                      VMat testoutputs, VMat testcosts) const
+{
+    val_string_reference_set = testset;
+    target_values_reference_set = testset;
+    inherited::test(testset,test_stats,testoutputs,testcosts);
+    val_string_reference_set = train_set;
+    target_values_reference_set = train_set;
+}
+
+VMat NeuralProbabilisticLanguageModel::processDataSet(VMat dataset) const
+{
+    VMat ret;
+    val_string_reference_set = dataset;
+    // Assumes it contains the target part information
+    if(dataset->width() > train_set->inputsize())
+        target_values_reference_set = dataset;
+    ret = inherited::processDataSet(dataset);
+    val_string_reference_set = train_set;
+    if(dataset->width() > train_set->inputsize())
+        target_values_reference_set = train_set;
+    return ret;
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners_experimental/NeuralProbabilisticLanguageModel.h
===================================================================
--- trunk/plearn_learners_experimental/NeuralProbabilisticLanguageModel.h	2008-06-17 18:21:44 UTC (rev 9137)
+++ trunk/plearn_learners_experimental/NeuralProbabilisticLanguageModel.h	2008-06-17 19:46:59 UTC (rev 9138)
@@ -0,0 +1,482 @@
+// -*- C++ -*-
+
+// NeuralProbabilisticLanguageModel.h
+// Copyright (c) 1998-2002 Pascal Vincent
+// Copyright (C) 1999-2002 Yoshua Bengio and University of Montreal
+// Copyright (c) 2002 Jean-Sebastien Senecal, Xavier Saint-Mleux, Rejean Ducharme
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+/*! \file PLearnLibrary/PLearnAlgo/NeuralProbabilisticLanguageModel.h */
+
+#ifndef NeuralProbabilisticLanguageModel_INC
+#define NeuralProbabilisticLanguageModel_INC
+
+#include "PLearner.h"
+#include <plearn/math/PRandom.h>
+#include <plearn/feat/FeatureSet.h>
+
+namespace PLearn {
+using namespace std;
+
+/**
+ * Feedforward neural network for language modeling
+ *
+ * Implementation of the Neural Probabilistic Language Model proposed by
+ * Bengio, Ducharme, Vincent and Jauvin (JMLR 2003), with extentensions to speedup
+ * the model (Bengio and S?n?cal, AISTATS 2003) and to include prior information
+ * about the distributed representation and permit generalization of these
+ * distributed representations to out-of-vocabulary words using features
+ * (Larochelle and Bengio, Tech Report 2006).
+ */
+class NeuralProbabilisticLanguageModel: public PLearner
+{
+
+private:
+
+    typedef PLearner inherited;
+    
+    //! Vector of possible target values
+    mutable Vec target_values;
+    //! Vector for output computations 
+    mutable Vec output_comp;
+    //! Row vector
+    mutable Vec row;
+    //! Last layer of network (pointer to either nnet_input,
+    //! vnhidden or vnhidden2)
+    mutable Vec last_layer;
+    //! Gradient of last layer in back propagation
+    mutable Vec gradient_last_layer;
+    //! Features for each token
+    mutable TVec< TVec<int> > feats;
+
+    //! Temporary computations variable, used in fprop() and bprop()
+    //! Care must be taken when using these variables,
+    //! since they are used by many different functions
+    mutable Vec gradient, neg_energies, densities;
+    mutable string str;
+    mutable real * pval1, * pval2, * pval3, * pval4, * pval5;
+    mutable real val, val2, grad;
+    mutable int offset;
+    mutable int ni,nj,nk,id,nfeats,ifeats;
+    mutable int* f;
+
+protected:
+
+    //! Total output size
+    int total_output_size;
+    //! Total updates so far;
+    int total_updates;
+    //! Number of feature sets
+    int n_feat_sets;
+    //! Number of features per input token
+    //! for which a distributed representation
+    //! is computed.
+    int total_feats_per_token;
+    //! Reindexed target
+    mutable int reind_target;
+    //! Feature input;
+    mutable Vec feat_input;
+    //! Gradient on feature input (useless for now)
+    Vec gradient_feat_input;
+    //! Input vector to NNet (after mapping into distributed representations)
+    Vec nnet_input;
+    //! Gradient for vector to NNet
+    Vec gradient_nnet_input;
+    //! First hidden layer value
+    Vec hiddenv;
+    //! Gradient of first hidden layer
+    Vec gradient_hiddenv;
+    //! Gradient through first hidden layer activation
+    Vec gradient_act_hiddenv;
+    //! Second hidden layer value
+    Vec hidden2v;
+    //! Gradient of second hidden layer
+    Vec gradient_hidden2v;
+    //! Gradient through second hidden layer activation
+    Vec gradient_act_hidden2v;
+    //! Gradient on output
+    Vec gradient_outputv;
+    //! Gradient throught output layer activation
+    Vec gradient_act_outputv;
+    //! Random number generator for parameters initialization
+    PP<PRandom> rgen;
+    //! Features seen in input since last update
+    Vec feats_since_last_update;
+    //! Possible target values seen since last update
+    Vec target_values_since_last_update;
+    //! VMatrix used to get values to string mapping for input tokens
+    mutable VMat val_string_reference_set;
+    //! Possible target values mapping.
+    mutable VMat target_values_reference_set;
+    //! Importance sampling ratios of the samples
+    Vec importance_sampling_ratios;
+    //! Generated sample from proposal distribution
+    Vec sample;
+    //! Set of generated samples from the proposal distribution
+    Vec generated_samples;
+
+public: 
+    //! Weights of first hidden layer
+    Mat w1;
+    //! Gradient on weights of first hidden layer
+    Mat gradient_w1;
+    //! Bias of first hidden layer
+    Vec b1;
+    //! Gradient on bias of first hidden layer
+    Vec gradient_b1;
+    //! Weights of second hidden layer
+    Mat w2;
+    //! gradient on weights of second hidden layer
+    Mat gradient_w2;
+    //! Bias of second hidden layer
+    Vec b2;
+    //! Gradient on bias of second hidden layer
+    Vec gradient_b2;
+    //! Weights of output layer
+    Mat wout;
+    //! Gradient on weights of output layer
+    Mat gradient_wout;
+    //! Bias of output layer
+    Vec bout;
+    //! Gradient on bias of output layer
+    Vec gradient_bout;
+    //! Direct input to output weights
+    Mat direct_wout;
+    //! Gradient on direct input to output weights
+    Mat gradient_direct_wout;
+    //! Direct input to output bias (empty, since no bias is used)
+    Vec direct_bout;
+    //! Gradient on direct input to output bias (empty, since no bias is used)
+    Vec gradient_direct_bout;
+    //! Weights of output layer for distributed
+    //! representation predictor
+    Mat wout_dist_rep;
+    //! Gradient on weights of output layer for distributed
+    //! representation predictor
+    Mat gradient_wout_dist_rep;
+    //! Bias of output layer for distributed
+    //! representation predictor
+    Vec bout_dist_rep;
+    //! Gradient on bias of output layer for distributed
+    //! representation predictor
+    Vec gradient_bout_dist_rep;
+
+public:
+
+    // Build options:
+
+    //! Number of hidden nunits in first hidden layer (default:0)
+    int nhidden;
+    //! Number of hidden units in second hidden layer (default:0)
+    int nhidden2; 
+    //! Weight decay (default:0)
+    real weight_decay; 
+    //! Bias decay (default:0)
+    real bias_decay; 
+    //! Weight decay for weights from input layer to first hidden layer 
+    //! (default:0)
+    real layer1_weight_decay; 
+    //! Bias decay for weights from input layer to first hidden layer 
+    //! (default:0)
+    real layer1_bias_decay;   
+    //! Weight decay for weights from first hidden layer to second hidden
+    //! layer (default:0)
+    real layer2_weight_decay; 
+    //! Bias decay for weights from first hidden layer to second hidden 
+    //! layer (default:0)
+    real layer2_bias_decay;   
+    //! Weight decay for weights from last hidden layer to output layer 
+    //! (default:0)
+    real output_layer_weight_decay; 
+    //! Bias decay for weights from last hidden layer to output layer 
+    //! (default:0)
+    real output_layer_bias_decay;
+    //! Weight decay for weights from input directly to output layer 
+    //! (default:0)
+    real direct_in_to_out_weight_decay;
+    //! Weight decay for weights from last hidden layer to output layer
+    //! of distributed representation predictor (default:0)
+    real output_layer_dist_rep_weight_decay; 
+    //! Bias decay for weights from last hidden layer to output layer of 
+    //! distributed representation predictor (default:0)
+    real output_layer_dist_rep_bias_decay;
+    //! Margin requirement, used only with the margin_perceptron_cost 
+    //! cost function (default:1)
+    real margin; 
+    //! If true then the output weights are not learned. 
+    //! They are initialized to +1 or -1 randomly (default:false)
+    bool fixed_output_weights;
+    //! If true then direct input to output weights will be added 
+    //! (if nhidden > 0)
+    bool direct_in_to_out;
+    //! Penalty to use on the weights (for weight and bias decay) 
+    //! (default:"L2_square")
+    string penalty_type; 
+    //! Transfer function to use for ouput layer (default:"")
+    string output_transfer_func; 
+    //! Transfer function to use for hidden units (default:"tanh")
+    //! tanh, sigmoid, softplus, softmax, etc...  
+    string hidden_transfer_func; 
+    //! Cost functions.
+    TVec<string> cost_funcs;  
+    //! Start learning rate of gradient descent
+    real start_learning_rate;
+    //! Decrease constant of gradietn descent
+    real decrease_constant;
+    //! Number of samples to use to estimate gradient before an update.
+    //! 0 means the whole training set (default: 1)
+    int batch_size; 
+    //! Indication that a trick to speedup stochastic gradient descent
+    //! should be used. 
+    bool stochastic_gradient_descent_speedup;
+    //! Method of initialization for neural network's weights
+    string initialization_method;
+    //! Dimensionality (number of components) of distributed representations
+    //! If <= 0, than distributed representations will not be used.
+    int dist_rep_dim;
+    //! Indication that the set of possible targets vary from
+    //! one input vector to another.
+    bool possible_targets_vary;
+    //! FeatureSets to apply on input
+    TVec<PP<FeatureSet> > feat_sets;
+    //! Proposal distribution for importance sampling
+    //! speedup method (Bengio and Senecal 2006).
+    //! If NULL, then this speedup method won't be used.
+    //! This proposal distribution should use the same
+    //! symbol int/string mapping as this class
+    //! uses.
+    PP<PDistribution> proposal_distribution;
+    //! Indication that the proposal distribution
+    //! must be trained (using train_set).
+    bool train_proposal_distribution;
+    //! Size of the sampling blocks
+    int sampling_block_size;
+    //! Minimum effective sample size
+    int minimum_effective_sample_size;
+
+private:
+    void build_();
+
+    //! Softmax vector y obtained on x
+    //! This implementation is such that 
+    //! compute_softmax(x,x) is such that x
+    //! becomes its softmax value.
+    void compute_softmax(const Vec& x, const Vec& y) const;
+
+    //! Negative log-likelihood loss
+    real nll(const Vec& outputv, int target) const;
+    
+    //! Classification loss
+    real classification_loss(const Vec& outputv, int target) const;
+    
+    //! Argmax function that lets you define the default (first)
+    //! component used for comparisons. This is useful to avoid bias in the prediction
+    //! when the getValues() provides some information about
+    //! the prior distribution of the targets (e.g. the first target given by 
+    //! getValues() is the most likely) and the output of the model is
+    //! the same for all targets.
+    int my_argmax(const Vec& vec, int default_compare=0) const;
+
+public:
+
+    NeuralProbabilisticLanguageModel();
+    virtual ~NeuralProbabilisticLanguageModel();
+    PLEARN_DECLARE_OBJECT(NeuralProbabilisticLanguageModel);
+
+    virtual void build();
+    virtual void forget(); // simply calls initializeParams()
+
+    virtual int outputsize() const;
+    virtual TVec<string> getTrainCostNames() const;
+    virtual TVec<string> getTestCostNames() const;
+
+    virtual void train();
+
+    virtual void computeOutput(const Vec& input, Vec& output) const;
+
+    virtual void computeOutputAndCosts(const Vec& input, const Vec& target,
+                                       Vec& output, Vec& costs) const;
+
+    virtual void computeCostsFromOutputs(const Vec& input, 
+                                         const Vec& output, 
+                                         const Vec& target, 
+                                         Vec& costs) const;
+
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap &copies);
+
+protected:
+    static void declareOptions(OptionList& ol);
+
+    //! Forward propagation in the network
+    void fprop(const Vec& inputv, Vec& outputv, const Vec& targetv, Vec& costsv, real sampleweight=1) const;
+
+    //! Forward propagation to compute the output
+    void fpropOutput(const Vec& inputv, Vec& outputv) const;
+
+    //! Forward propagation until output weights are reached
+    //! (called by fpropOutput(...) and importance_sampling_gradient_update(...)
+    void fpropBeforeOutputWeights(const Vec& inputv) const;
+
+    //! Forward propagation to compute the costs from the output
+    void fpropCostsFromOutput(const Vec& inputv, const Vec& outputv, const Vec& targetv, Vec& costsv, real sampleweight=1) const;
+
+    //! Backward propagation in the network,
+    //! which assumes that a forward propagation has been done
+    //! before.
+    //! A learning rate needs to be provided because it is
+    //! -learning_rate * gradient that is propagated, not just the gradient.
+    void bprop(Vec& inputv, Vec& outputv, Vec& targetv, Vec& costsv, real learning_rate, real sampleweight=1);
+
+    //! Update network's parameters
+    void update();
+
+    //! Update affine transformation's parameters
+    void update_affine_transform(Vec input, Mat weights, Vec bias,
+                                 Mat gweights, Vec gbias,
+                                 bool input_is_sparse, bool output_is_sparse,
+                                 Vec output_indices);
+    
+    //! Clear network's propagation path gradient fields
+    //! Assumes fprop and bprop have been called before.
+    void clearProppathGradient();
+
+    //! Initialize the parameters. If 'set_seed' is set to false, the seed
+    //! will not be set in this method (it will be assumed to be already
+    //! initialized according to the 'seed' option).
+    //! The index of the extra task (-1 if main task) also needs to be
+    //! provided.
+    virtual void initializeParams(bool set_seed = true);
+
+    //! Computes the result of the application of the
+    //! given transfer function on the input vector
+    void add_transfer_func(const Vec& input, 
+                          string transfer_func = "default") const;
+
+    //! Computes the gradient through the given activation function,
+    //! the output value and the initial gradient on that output
+    //! (i.e. before the activation function).
+    //! After calling this function, gradient_act_output corresponds
+    //! the gradient after the activation function.
+    //! nll_softmax_speed_up_target is to speed up the gradient computation
+    //! for the output layer using the softmax transfert function
+    //! and the NLL cost function is applied.
+    void gradient_transfer_func(Vec& output, Vec& gradient_input, 
+                                Vec& gradient_output,                   
+                                string transfer_func = "default",
+                                int nll_softmax_speed_up_target=-1);
+
+    //! Applies affine transform on input using provided weights
+    //! and bias. Information about the nature of the input and output
+    //! need to be provided.
+    //! If bias.length() == 0, then output initial value is used as the bias.
+    void add_affine_transform(Vec input, Mat weights, Vec bias, Vec output, 
+                              bool input_is_sparse, bool output_is_sparse,
+                              Vec output_indices = Vec(0)) const;
+
+    //! Propagate gradient through affine transform on input using provided weights
+    //! and bias. Information about the nature of the input and output
+    //! need to be provided. 
+    //! If bias.length() == 0, then no backprop is made to bias.
+    void gradient_affine_transform(Vec input, Mat weights, Vec bias, 
+                                   Vec ginput, Mat gweights, Vec gbias, Vec goutput, 
+                                   bool input_is_sparse, bool output_is_sparse,
+                                   real learning_rate,
+                                   real weight_decay, real bias_decay,
+                                   Vec output_indices = Vec(0));
+
+    //! Propagate penalty gradient through weights and bias, 
+    //! scaled by -learning rate.
+    void gradient_penalty(Vec input, Mat weights, Vec bias, 
+                          Mat gweights, Vec gbias,  
+                          bool input_is_sparse, bool output_is_sparse,
+                          real learning_rate,
+                          real weight_decay, real bias_decay,
+                          Vec output_indices = Vec(0));
+    
+    //! Update the neural network parameters using the importance sampling
+    //! estimate of the gradient, based on n_samples of the proposal distribution.
+    void importance_sampling_gradient_update(Vec& inputv, Vec& targetv, 
+                                             real learning_rate, int n_samples, 
+                                             real train_sample_weight=1);
+
+    //! Gives scalar negative energy values for some samples (words).
+    //! Assumes fpropBeforeOutputWeights has been called previously.
+    void getNegativeEnergyValues(Vec samples, Vec neg_energies);
+
+    //! Fill a matrix of weights according to the 'initialization_method' 
+    //! specified. 
+    void fillWeights(const Mat& weights);
+
+    //! Verify gradient of propagation path
+    void verify_gradient(Vec& input, Vec target, real step);
+
+    //! Verify gradient of affine_transform parameters
+    void verify_gradient_affine_transform(
+        Vec global_input, Vec& global_output, Vec& global_targetv, 
+        Vec& global_costs, real sampleweight,
+        Vec input, Mat weights, Vec bias,
+        Mat est_gweights, Vec est_gbias, 
+        bool input_is_sparse, bool output_is_sparse,
+        real step,
+        Vec output_indices = Vec(0)) const;
+    
+    void output_gradient_verification(Vec grad, Vec est_grad);
+
+    //! Changes the reference_set and then calls the parent's class method
+    void batchComputeOutputAndConfidence(VMat inputs, real probability,
+                                         VMat outputs_and_confidence) const;
+    //! Changes the reference_set and then calls the parent's class method
+    virtual void use(VMat testset, VMat outputs) const;
+    //! Changes the reference_set and then calls the parent's class method
+    virtual void test(VMat testset, PP<VecStatsCollector> test_stats, 
+                      VMat testoutputs=0, VMat testcosts=0) const;
+    //! Changes the reference_set and then calls the parent's class method
+    virtual VMat processDataSet(VMat dataset) const;
+        
+};
+
+DECLARE_OBJECT_PTR(NeuralProbabilisticLanguageModel);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From chrish at mail.berlios.de  Tue Jun 17 22:19:29 2008
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Tue, 17 Jun 2008 22:19:29 +0200
Subject: [Plearn-commits] r9139 - trunk/commands
Message-ID: <200806172019.m5HKJTGu024939@sheep.berlios.de>

Author: chrish
Date: 2008-06-17 22:19:29 +0200 (Tue, 17 Jun 2008)
New Revision: 9139

Modified:
   trunk/commands/plearn_desjardins.cc
Log:
Remove stray executable bit from plearn_desjardins.cc


Property changes on: trunk/commands/plearn_desjardins.cc
___________________________________________________________________
Name: svn:executable
   - *



From tihocan at mail.berlios.de  Wed Jun 18 16:37:01 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 18 Jun 2008 16:37:01 +0200
Subject: [Plearn-commits] r9140 - trunk/plearn_learners/generic
Message-ID: <200806181437.m5IEb1T1005910@sheep.berlios.de>

Author: tihocan
Date: 2008-06-18 16:37:01 +0200 (Wed, 18 Jun 2008)
New Revision: 9140

Modified:
   trunk/plearn_learners/generic/PLearner.cc
Log:
Remote computeOutput does not crash anymore when outputsize is undefined

Modified: trunk/plearn_learners/generic/PLearner.cc
===================================================================
--- trunk/plearn_learners/generic/PLearner.cc	2008-06-17 20:19:29 UTC (rev 9139)
+++ trunk/plearn_learners/generic/PLearner.cc	2008-06-18 14:37:01 UTC (rev 9140)
@@ -1318,7 +1318,8 @@
 //! Version of computeOutput that returns a result by value
 Vec PLearner::remote_computeOutput(const Vec& input) const
 {
-    tmp_output.resize(outputsize());
+    int os = outputsize();
+    tmp_output.resize(os >= 0 ? os : 0);
     computeOutput(input, tmp_output);
     return tmp_output;
 }



From chapados at mail.berlios.de  Wed Jun 18 16:58:51 2008
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Wed, 18 Jun 2008 16:58:51 +0200
Subject: [Plearn-commits] r9141 - trunk/python_modules/plearn/gui_tools
Message-ID: <200806181458.m5IEwp98008219@sheep.berlios.de>

Author: chapados
Date: 2008-06-18 16:58:51 +0200 (Wed, 18 Jun 2008)
New Revision: 9141

Added:
   trunk/python_modules/plearn/gui_tools/__init__.py
Modified:
   trunk/python_modules/plearn/gui_tools/console_logger.py
   trunk/python_modules/plearn/gui_tools/xp_workbench.py
Log:
More robust parsing of command-line options

Added: trunk/python_modules/plearn/gui_tools/__init__.py
===================================================================

Modified: trunk/python_modules/plearn/gui_tools/console_logger.py
===================================================================
--- trunk/python_modules/plearn/gui_tools/console_logger.py	2008-06-18 14:37:01 UTC (rev 9140)
+++ trunk/python_modules/plearn/gui_tools/console_logger.py	2008-06-18 14:58:51 UTC (rev 9141)
@@ -1,4 +1,4 @@
-# xp_workbench.py
+# console_logger.py
 # Copyright (C) 2008 by Nicolas Chapados
 #
 #  Redistribution and use in source and binary forms, with or without

Modified: trunk/python_modules/plearn/gui_tools/xp_workbench.py
===================================================================
--- trunk/python_modules/plearn/gui_tools/xp_workbench.py	2008-06-18 14:37:01 UTC (rev 9140)
+++ trunk/python_modules/plearn/gui_tools/xp_workbench.py	2008-06-18 14:58:51 UTC (rev 9141)
@@ -154,7 +154,8 @@
         experiment is currently running.  If so, pop a dialog to ask
         whether it's really OK to close the thing.
         """
-        workbench = info.ui.context['object']
+        #workbench = info.ui.context['object']
+        workbench = info.object
         if workbench.curworker is not None:
             msg = "An experiment is currently running.\n" \
                   "Do you really want to interrupt it and\n" \
@@ -187,14 +188,14 @@
 
     ## The main view
     traits_view  = View(
-        HSplit(Group(Item("script_params@", show_label=False, springy=True),
-                     spring, spring,
+        HSplit(Group(Group(Item("script_params@", show_label=False, springy=False)),
+                     #spring,
                      Group(Item("launch", springy=True, enabled_when="curworker is None"),
                            Item("cancel", springy=True, enabled_when="curworker is not None"),
                            show_labels=False, orientation="horizontal")),
                Group(Item("experiments@", dock="fixed", show_label=False, width=300),
                      springy=True)),
-        resizable=True,
+        resizable=True, #scrollable=True,
         height=0.75, width=0.75,
         buttons=NoButtons,
         handler=WorkbenchHandler() )
@@ -285,9 +286,29 @@
             if arg.startswith('-'):
                 continue
             if '=' in arg:
-                (k,v) = arg.split('=')
-                exec("params.%s = %s" % (k,v))
+                (k,v) = arg.split('=', 1)
+                v = v.replace("'", "\\'")
 
+                ## For compatibility between mixed plnamespace and traits,
+                ## if the first argument to a compound option does not
+                ## exist, skip it without complaining -- it is destined for
+                ## plnamespace, which have been processed before
+                if '.' in k:
+                    k_parts = k.split('.')
+                    try:
+                        getattr(params, k_parts[0])
+                    except AttributeError:
+                        continue
+                
+                if isinstance(eval("params.%s" % k), str):
+                    exec("params.%s = '%s'" % (k,v))
+                else:
+                    ## Potentially dangerous use of 'eval' with
+                    ## command-line arguments; expeditive solution for now,
+                    ## but may opt for more restricted/lenient parsing in
+                    ## the future, like we had for plargs.
+                    exec("params.%s = eval('%s')" % (k,v))
+
         return params
 
 
@@ -297,6 +318,59 @@
                             datetime.now().strftime("expdir_%Y%m%d_%H%m%S"))
 
 
+#####  Top-Level Options  ###################################################
+
+class SingletonOptions(HasStrictTraits):
+    """Subclasses of this class are singletons, designed to provide an
+    approximate drop-in traits-based replacement to plnamespace.  If you
+    don't want or need the singleton behavior, simply have your options
+    classes inherit from HasTraits or HasStrictTraits.
+
+    Note: this class inherits from HasStrictTraits since it is intended to
+    mimic plnamespace as closely as possible to ease migration.  It should
+    be avoided for writing new code.
+    """
+    class __metaclass__(HasStrictTraits.__metaclass__):
+        __instances = {}
+        
+        def __call__(cls, *args, **kwargs):
+            klass_key = str(cls)
+            if klass_key not in SingletonOptions.__instances:
+                instance = type.__call__(cls, *args, **kwargs)
+                SingletonOptions.__instances[klass_key] = instance
+                return instance
+            else:
+                return SingletonOptions.__instances[klass_key]
+                
+
+class TopLevelOptions(HasStrictTraits):
+    """Utility class from which one can inherit for top-level options.
+    It provides a reasonable default traits_view implementation.
+    """
+    def trait_view(self, name=None, view_element=None):
+        if (name or view_element) != None:
+            return super(TopLevelOptions, self).trait_view( name=name,
+                                                            view_element=view_element )
+
+        items  = [ Item(t+"@") for t in self.trait_names()
+                   if t not in [ "expdir_root", "trait_added", "trait_modified" ] ]
+        kwargs = { "show_labels":False, "layout":"tabbed" }
+        return View(Group(*items, **kwargs))
+
+    @staticmethod
+    def print_all_traits(root, out = sys.stdout, prefix=""):
+        """Recursively print out all traits in a key=value format.
+        Useful for generating metainfo files for experiments.
+        """
+        for (trait_name, trait_value) in root.get():
+            if trait_name in ["trait_added", "trait_modified"]:
+                continue
+            elif isinstance(trait_value, "HasTraits"):
+                print_all_traits(trait_value, out, trait_name+".")
+            else:
+                print prefix+trait_name+" = "+str(trait_value)
+    
+
 #####  Utilities  ###########################################################
 
 def _async_raise(tid, exctype):
@@ -325,9 +399,9 @@
         weight_decay      = Trait(1e-8,  desc="weight decay to use for neural-net training")
 
     class MinorOpt(HasStrictTraits):
-        earlystop_fraction  = Trait(  0.0, Desc="fraction of training set to use for early stopping")
-        earlystop_check     = Trait(    1, Desc="check early-stopping criterion every N epochs")
-        earlystop_minstage  = Trait(    1, Desc="minimum optimization stage after which early-stopping can kick in")
+        earlystop_fraction  = Trait(  0.0, desc="fraction of training set to use for early stopping")
+        earlystop_check     = Trait(    1, desc="check early-stopping criterion every N epochs")
+        earlystop_minstage  = Trait(    1, desc="minimum optimization stage after which early-stopping can kick in")
 
     class AllOpt(HasStrictTraits):
         expdir_root = Delegate("GlobalOpt")



From nouiz at mail.berlios.de  Wed Jun 18 20:29:34 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 18 Jun 2008 20:29:34 +0200
Subject: [Plearn-commits] r9142 - in trunk/plearn: base io
Message-ID: <200806181829.m5IITYox021717@sheep.berlios.de>

Author: nouiz
Date: 2008-06-18 20:29:34 +0200 (Wed, 18 Jun 2008)
New Revision: 9142

Modified:
   trunk/plearn/base/pl_hash_fun.h
   trunk/plearn/io/PStream.h
   trunk/plearn/io/pl_io.cc
Log:
bugfix to compile under gcc 4.3.0


Modified: trunk/plearn/base/pl_hash_fun.h
===================================================================
--- trunk/plearn/base/pl_hash_fun.h	2008-06-18 14:58:51 UTC (rev 9141)
+++ trunk/plearn/base/pl_hash_fun.h	2008-06-18 18:29:34 UTC (rev 9142)
@@ -36,7 +36,7 @@
 #define pl_hash_fun_H
 
 #include "ms_hash_wrapper.h"
-#include <string>
+#include <cstring>
 
 namespace PLearn {
 

Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2008-06-18 14:58:51 UTC (rev 9141)
+++ trunk/plearn/io/PStream.h	2008-06-18 18:29:34 UTC (rev 9142)
@@ -42,6 +42,7 @@
 #include <set>
 #include <sstream>
 #include <fstream>
+#include <limits>
 #include <plearn/base/byte_order.h>
 #include <plearn/base/pl_hash_fun.h>
 #include <plearn/base/plerror.h>
@@ -342,8 +343,8 @@
         // Check if there was a conversion problem (sign or overflow)
 
         if (static_cast<I>(x) != y
-            || !(numeric_limits<J>::is_signed) 
-            && chkUnsigned<numeric_limits<I>::is_signed>::notOk(y))
+            || (!(numeric_limits<J>::is_signed) 
+                && chkUnsigned<numeric_limits<I>::is_signed>::notOk(y)))
         {
             std::stringstream error;
             error << "In PStream::readBinaryNumAs, overflow error: " << std::endl

Modified: trunk/plearn/io/pl_io.cc
===================================================================
--- trunk/plearn/io/pl_io.cc	2008-06-18 14:58:51 UTC (rev 9141)
+++ trunk/plearn/io/pl_io.cc	2008-06-18 18:29:34 UTC (rev 9142)
@@ -51,7 +51,7 @@
 #include <plearn/base/plerror.h>
 //#include "pl_math.h"
 #include <plearn/base/byte_order.h> //!< For endianswap.
-
+#include <cstring>
 namespace PLearn {
 using namespace std;
 



From nouiz at mail.berlios.de  Wed Jun 18 21:43:40 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 18 Jun 2008 21:43:40 +0200
Subject: [Plearn-commits] r9143 - trunk/plearn/math
Message-ID: <200806181943.m5IJhesb028000@sheep.berlios.de>

Author: nouiz
Date: 2008-06-18 21:43:40 +0200 (Wed, 18 Jun 2008)
New Revision: 9143

Modified:
   trunk/plearn/math/plapack.cc
Log:
removed warning


Modified: trunk/plearn/math/plapack.cc
===================================================================
--- trunk/plearn/math/plapack.cc	2008-06-18 18:29:34 UTC (rev 9142)
+++ trunk/plearn/math/plapack.cc	2008-06-18 19:43:40 UTC (rev 9143)
@@ -88,12 +88,12 @@
     int INFO = 1;
     if (compute_all)
     {
-        char* JOBZ;
+        char JOBZ;
         if (compute_vectors)
-            JOBZ = "V";
+            JOBZ = 'V';
         else
-            JOBZ = "N";
-        char* UPLO = "U";
+            JOBZ = 'N';
+        char UPLO = 'U';
         int N = in.length();
         real* A = in.data();
         int LDA = N;
@@ -103,10 +103,10 @@
 
         // we now call the LAPACK Fortran function <ssyev>
 #ifdef USEFLOAT
-        ssyev_(JOBZ, UPLO, &N, A, &LDA, W, WORK, &LWORK, &INFO);
+        ssyev_(&JOBZ, &UPLO, &N, A, &LDA, W, WORK, &LWORK, &INFO);
 #endif
 #ifdef USEDOUBLE
-        dsyev_(JOBZ, UPLO, &N, A, &LDA, W, WORK, &LWORK, &INFO);
+        dsyev_(&JOBZ, &UPLO, &N, A, &LDA, W, WORK, &LWORK, &INFO);
 #endif
 
         if (INFO != 0)
@@ -134,13 +134,13 @@
     }
     else
     {
-        char* JOBZ;
+        char JOBZ;
         if (compute_vectors)
-            JOBZ = "V";
+            JOBZ = 'V';
         else
-            JOBZ = "N";
-        char* RANGE = "I";
-        char* UPLO = "U";
+            JOBZ = 'N';
+        char RANGE = 'I';
+        char UPLO = 'U';
         int N = in.length();
         real* A = in.data();
         int LDA = N;
@@ -167,7 +167,7 @@
         int* IFAIL = new int[N];
 
         // we now call the LAPACK Fortran function <ssyevx>
-        lapack_Xsyevx_(JOBZ, RANGE, UPLO, &N, A, &LDA, &VL, &VU, &IL, &IU, &ABSTOL, &M, W, Z, &LDZ, WORK, &LWORK, IWORK, IFAIL, &INFO);
+        lapack_Xsyevx_(&JOBZ, &RANGE, &UPLO, &N, A, &LDA, &VL, &VU, &IL, &IU, &ABSTOL, &M, W, Z, &LDZ, WORK, &LWORK, IWORK, IFAIL, &INFO);
 
         n_evalues_found = M;
         if (M != nb_eigen)



From saintmlx at mail.berlios.de  Wed Jun 18 21:53:42 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Wed, 18 Jun 2008 21:53:42 +0200
Subject: [Plearn-commits] r9144 - trunk/plearn/base
Message-ID: <200806181953.m5IJrg9Z028655@sheep.berlios.de>

Author: saintmlx
Date: 2008-06-18 21:53:41 +0200 (Wed, 18 Jun 2008)
New Revision: 9144

Modified:
   trunk/plearn/base/HelpSystem.cc
Log:
- don't attempt to list unregistered parent classes



Modified: trunk/plearn/base/HelpSystem.cc
===================================================================
--- trunk/plearn/base/HelpSystem.cc	2008-06-18 19:43:40 UTC (rev 9143)
+++ trunk/plearn/base/HelpSystem.cc	2008-06-18 19:53:41 UTC (rev 9144)
@@ -448,11 +448,18 @@
     vector<string> parents;
     const TypeMapEntry* e= 
         &TypeFactory::instance().getTypeMapEntry(cl);
-    while(e->parent_class != cl)
+    bool known_parent= true;
+    while(known_parent && e->parent_class != cl)
     {
         cl= e->parent_class;
         parents.push_back(cl);
-        e= &TypeFactory::instance().getTypeMapEntry(cl);
+        if(TypeFactory::instance().isRegistered(cl))
+            e= &TypeFactory::instance().getTypeMapEntry(cl);
+        else
+        {
+            known_parent= false;
+            parents.push_back("!?!UNKNOWN_PARENT_CLASS!?!");
+        }
     }
     return parents;
 }



From nouiz at mail.berlios.de  Wed Jun 18 22:00:49 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 18 Jun 2008 22:00:49 +0200
Subject: [Plearn-commits] r9145 - trunk/plearn/python
Message-ID: <200806182000.m5IK0nvB029452@sheep.berlios.de>

Author: nouiz
Date: 2008-06-18 22:00:47 +0200 (Wed, 18 Jun 2008)
New Revision: 9145

Modified:
   trunk/plearn/python/PythonObjectWrapper.h
Log:
remove warning in gcc 4.3.0


Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2008-06-18 19:53:41 UTC (rev 9144)
+++ trunk/plearn/python/PythonObjectWrapper.h	2008-06-18 20:00:47 UTC (rev 9145)
@@ -116,7 +116,7 @@
 // explicit here.
 #endif
         if (static_cast<long>(result) != x
-            || !(numeric_limits<I>::is_signed) && x<0)
+            || (!(numeric_limits<I>::is_signed) && x<0))
         {
             PLPythonConversionError("integerFromPyObject<I>", pyobj,
                                     print_traceback);



From nouiz at mail.berlios.de  Wed Jun 18 22:46:01 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 18 Jun 2008 22:46:01 +0200
Subject: [Plearn-commits] r9146 - trunk/plearn/base
Message-ID: <200806182046.m5IKk1ox002594@sheep.berlios.de>

Author: nouiz
Date: 2008-06-18 22:46:01 +0200 (Wed, 18 Jun 2008)
New Revision: 9146

Modified:
   trunk/plearn/base/RealMapping.h
Log:
bugfix for gcc 4.3.0


Modified: trunk/plearn/base/RealMapping.h
===================================================================
--- trunk/plearn/base/RealMapping.h	2008-06-18 20:00:47 UTC (rev 9145)
+++ trunk/plearn/base/RealMapping.h	2008-06-18 20:46:01 UTC (rev 9146)
@@ -140,7 +140,7 @@
 public:
     typedef pair<RealRange, real> single_mapping_t;
     typedef TVec< single_mapping_t > ordered_mapping_t;
-    typedef map<RealRange, real> mapping_t;
+    typedef std::map<RealRange, real> mapping_t;
     typedef mapping_t::iterator iterator;
     typedef mapping_t::const_iterator const_iterator;
 



From larocheh at mail.berlios.de  Wed Jun 18 22:54:01 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 18 Jun 2008 22:54:01 +0200
Subject: [Plearn-commits] r9147 - trunk/plearn_learners_experimental
Message-ID: <200806182054.m5IKs1Mi003278@sheep.berlios.de>

Author: larocheh
Date: 2008-06-18 22:54:00 +0200 (Wed, 18 Jun 2008)
New Revision: 9147

Modified:
   trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc
   trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.h
Log:
Added the Test-Centric NLMP variant


Modified: trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc
===================================================================
--- trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc	2008-06-18 20:46:01 UTC (rev 9146)
+++ trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc	2008-06-18 20:54:00 UTC (rev 9147)
@@ -197,6 +197,13 @@
                   OptionBase::buildoption,
                   "Indication that the value of sigma noise should not be learned.\n");
 
+    declareOption(ol, "use_test_centric_nlmp", 
+                  &DeepNonLocalManifoldParzen::use_test_centric_nlmp,
+                  OptionBase::buildoption,
+                  "Indication that the Test-Centric NLMP variant should "
+		  "be used.\n"
+		  "In this case, train_one_network_per_class must be true.\n");
+
     declareOption(ol, "greedy_stages", 
                   &DeepNonLocalManifoldParzen::greedy_stages,
                   OptionBase::learntoption,
@@ -275,6 +282,17 @@
             PLERROR("DeepNonLocalManifoldParzen::build_() - \n"
                     "min_sigma_noise should be > or = to 0.\n");
 
+	if( use_test_centric_nlmp && !train_one_network_per_class )
+	    PLERROR("DeepNonLocalManifoldParzen::build_() - \n"
+                    "train_one_network_per_class must be true for "
+		    "Test-Centric NLMP variant.\n");
+	  
+	if( use_test_centric_nlmp && n_classes > 1)
+	    PLERROR("DeepNonLocalManifoldParzen::build_() - \n"
+                    "n_classes must be > 1 for "
+		    "Test-Centric NLMP variant.\n");
+	  
+
         if(greedy_stages.length() == 0)
         {
             greedy_stages.resize(n_layers-1);
@@ -1137,115 +1155,145 @@
     Vec target(targetsize());
     real weight;
 
-    if( save_manifold_parzen_parameters )
+    if( use_test_centric_nlmp )
     {
-        updateManifoldParzenParameters();
-
-        int input_j_index;
         for( int i=0; i<n_classes; i++ )
+	{
+	    computeManifoldParzenParameters( input, F, mu, 
+					     pre_sigma_noise, U, sm_svd,
+					     i);
+                    
+	    sigma_noise = pre_sigma_noise[0]*pre_sigma_noise[0] 
+	        + min_sigma_noise;
+                    
+	    mahal = -0.5*pownorm(mu)/sigma_noise;      
+	    norm_term = - n/2.0 * Log2Pi - 0.5*(n-n_components)*
+	        pl_log(sigma_noise);
+        
+	    for(int k=0; k<n_components; k++)
+	    { 
+		uk = U(k);
+		dotp = dot(mu,uk);
+		coef = (1.0/(sm_svd[k]+sigma_noise) - 1.0/sigma_noise);
+		mahal -= dotp*dotp*0.5*coef;
+		norm_term -= 0.5*pl_log(sm_svd[k]+sigma_noise);
+	    }
+	    
+	    log_p_x_g_y = norm_term + mahal;
+	    test_votes[i] = log_p_x_g_y ;
+	}        
+    }
+    else
+    {
+        if( save_manifold_parzen_parameters )
         {
-            for( int j=0; 
-                 j<(n_classes > 1 ? 
-                    class_datasets[i]->length() 
-                    : train_set->length()); 
-                 j++ )
+            updateManifoldParzenParameters();
+        
+            int input_j_index;
+            for( int i=0; i<n_classes; i++ )
             {
-                if( n_classes > 1 )
+                for( int j=0; 
+                     j<(n_classes > 1 ? 
+                        class_datasets[i]->length() 
+                        : train_set->length()); 
+                     j++ )
                 {
-                    class_datasets[i]->getExample(j,input_j,target,weight);
-                    input_j_index = class_datasets[i]->indices[j];
-                }
-                else
-                {
-                    train_set->getExample(j,input_j,target,weight);
-                    input_j_index = j;
-                }
-
-                U << eigenvectors[input_j_index];
-                sm_svd << eigenvalues(input_j_index);
-                sigma_noise = sigma_noises[input_j_index];
-                mu << mus(input_j_index);
-
-                substract(input,input_j,diff_neighbor_input); 
-                substract(diff_neighbor_input,mu,diff); 
+                    if( n_classes > 1 )
+                    {
+                        class_datasets[i]->getExample(j,input_j,target,weight);
+                        input_j_index = class_datasets[i]->indices[j];
+                    }
+                    else
+                    {
+                        train_set->getExample(j,input_j,target,weight);
+                        input_j_index = j;
+                    }
+        
+                    U << eigenvectors[input_j_index];
+                    sm_svd << eigenvalues(input_j_index);
+                    sigma_noise = sigma_noises[input_j_index];
+                    mu << mus(input_j_index);
+        
+                    substract(input,input_j,diff_neighbor_input); 
+                    substract(diff_neighbor_input,mu,diff); 
+                        
+                    mahal = -0.5*pownorm(diff)/sigma_noise;      
+                    norm_term = - n/2.0 * Log2Pi - 0.5*(n-n_components)*
+                        pl_log(sigma_noise);
+        
+                    for(int k=0; k<n_components; k++)
+                    { 
+                        uk = U(k);
+                        dotp = dot(diff,uk);
+                        coef = (1.0/(sm_svd[k]+sigma_noise) - 1.0/sigma_noise);
+                        mahal -= dotp*dotp*0.5*coef;
+                        norm_term -= 0.5*pl_log(sm_svd[k]+sigma_noise);
+                    }
                     
-                mahal = -0.5*pownorm(diff)/sigma_noise;      
-                norm_term = - n/2.0 * Log2Pi - 0.5*(n-n_components)*
-                    pl_log(sigma_noise);
-
-                for(int k=0; k<n_components; k++)
-                { 
-                    uk = U(k);
-                    dotp = dot(diff,uk);
-                    coef = (1.0/(sm_svd[k]+sigma_noise) - 1.0/sigma_noise);
-                    mahal -= dotp*dotp*0.5*coef;
-                    norm_term -= 0.5*pl_log(sm_svd[k]+sigma_noise);
+                    if( j==0 )
+                        log_p_x_g_y = norm_term + mahal;
+                    else
+                        log_p_x_g_y = logadd(norm_term + mahal, log_p_x_g_y);
                 }
-                
-                if( j==0 )
-                    log_p_x_g_y = norm_term + mahal;
-                else
-                    log_p_x_g_y = logadd(norm_term + mahal, log_p_x_g_y);
+        
+                test_votes[i] = log_p_x_g_y;
             }
-
-            test_votes[i] = log_p_x_g_y;
         }
-    }
-    else
-    {
-
-        for( int i=0; i<n_classes; i++ )
+        else
         {
-            for( int j=0; 
-                 j<(n_classes > 1 ? 
-                    class_datasets[i]->length() 
-                    : train_set->length()); 
-                 j++ )
+        
+            for( int i=0; i<n_classes; i++ )
             {
-                if( n_classes > 1 )
+                for( int j=0; 
+                     j<(n_classes > 1 ? 
+                        class_datasets[i]->length() 
+                        : train_set->length()); 
+                     j++ )
                 {
-                    class_datasets[i]->getExample(j,input_j,target,weight);
-                    computeManifoldParzenParameters( input_j, F, mu, 
-                                                     pre_sigma_noise, U, sm_svd,
-                                                     (int) target[0]);
-                }
-                else
-                {
-                    train_set->getExample(j,input_j,target,weight);
-                    computeManifoldParzenParameters( input_j, F, mu, 
-                                                     pre_sigma_noise, U, sm_svd );
-                }
-
-                
-                sigma_noise = pre_sigma_noise[0]*pre_sigma_noise[0] 
-                    + min_sigma_noise;
-                
-                substract(input,input_j,diff_neighbor_input); 
-                substract(diff_neighbor_input,mu,diff); 
+                    if( n_classes > 1 )
+                    {
+                        class_datasets[i]->getExample(j,input_j,target,weight);
+                        computeManifoldParzenParameters( input_j, F, mu, 
+                                                         pre_sigma_noise, U, sm_svd,
+                                                         (int) target[0]);
+                    }
+                    else
+                    {
+                        train_set->getExample(j,input_j,target,weight);
+                        computeManifoldParzenParameters( input_j, F, mu, 
+                                                         pre_sigma_noise, U, sm_svd );
+                    }
+        
                     
-                mahal = -0.5*pownorm(diff)/sigma_noise;      
-                norm_term = - n/2.0 * Log2Pi - 0.5*(n-n_components)*
-                    pl_log(sigma_noise);
-
-                for(int k=0; k<n_components; k++)
-                { 
-                    uk = U(k);
-                    dotp = dot(diff,uk);
-                    coef = (1.0/(sm_svd[k]+sigma_noise) - 1.0/sigma_noise);
-                    mahal -= dotp*dotp*0.5*coef;
-                    norm_term -= 0.5*pl_log(sm_svd[k]+sigma_noise);
+                    sigma_noise = pre_sigma_noise[0]*pre_sigma_noise[0] 
+                        + min_sigma_noise;
+                    
+                    substract(input,input_j,diff_neighbor_input); 
+                    substract(diff_neighbor_input,mu,diff); 
+                        
+                    mahal = -0.5*pownorm(diff)/sigma_noise;      
+                    norm_term = - n/2.0 * Log2Pi - 0.5*(n-n_components)*
+                        pl_log(sigma_noise);
+        
+                    for(int k=0; k<n_components; k++)
+                    { 
+                        uk = U(k);
+                        dotp = dot(diff,uk);
+                        coef = (1.0/(sm_svd[k]+sigma_noise) - 1.0/sigma_noise);
+                        mahal -= dotp*dotp*0.5*coef;
+                        norm_term -= 0.5*pl_log(sm_svd[k]+sigma_noise);
+                    }
+                    
+                    if( j==0 )
+                        log_p_x_g_y = norm_term + mahal;
+                    else
+                        log_p_x_g_y = logadd(norm_term + mahal, log_p_x_g_y);
                 }
-                
-                if( j==0 )
-                    log_p_x_g_y = norm_term + mahal;
-                else
-                    log_p_x_g_y = logadd(norm_term + mahal, log_p_x_g_y);
+        
+                test_votes[i] = log_p_x_g_y;
             }
-
-            test_votes[i] = log_p_x_g_y;
         }
     }
-
     if( n_classes > 1 )
         output[0] = argmax(test_votes);
     else
@@ -1297,8 +1345,9 @@
                 costs[n_layers-1] = 0;
             else
                 costs[n_layers-1] = 1;
-            costs[n_layers] = - test_votes[target_class]
-                +pl_log(class_datasets[target_class]->length()); // Must take into account the 1/n normalization
+	    if( !use_test_centric_nlmp )
+	        costs[n_layers] = - test_votes[target_class]
+                    +pl_log(class_datasets[target_class]->length()); // Must take into account the 1/n normalization
         }
         else
         {

Modified: trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.h
===================================================================
--- trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.h	2008-06-18 20:46:01 UTC (rev 9146)
+++ trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.h	2008-06-18 20:54:00 UTC (rev 9147)
@@ -127,6 +127,10 @@
     //! Indication that the value of sigma noise should not be learned.
     bool do_not_learn_sigma_noise;
 
+    //! Indication that the Test-Centric NLMP variant should be used.
+    //! In this case, train_one_network_per_class must be true.
+    bool use_test_centric_nlmp;
+
     //#####  Public Learnt Options  ###########################################
 
     //! Number of layers



From chapados at mail.berlios.de  Thu Jun 19 01:13:15 2008
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Thu, 19 Jun 2008 01:13:15 +0200
Subject: [Plearn-commits] r9148 - trunk/python_modules/plearn/gui_tools
Message-ID: <200806182313.m5INDFcQ003806@sheep.berlios.de>

Author: chapados
Date: 2008-06-19 01:13:14 +0200 (Thu, 19 Jun 2008)
New Revision: 9148

Modified:
   trunk/python_modules/plearn/gui_tools/console_logger.py
   trunk/python_modules/plearn/gui_tools/xp_workbench.py
Log:
More robust logger

Modified: trunk/python_modules/plearn/gui_tools/console_logger.py
===================================================================
--- trunk/python_modules/plearn/gui_tools/console_logger.py	2008-06-18 20:54:00 UTC (rev 9147)
+++ trunk/python_modules/plearn/gui_tools/console_logger.py	2008-06-18 23:13:14 UTC (rev 9148)
@@ -43,7 +43,7 @@
 from   select import select
 
 from enthought.traits.api       import *
-from enthought.traits.ui.api    import CodeEditor, Group, Item, View
+from enthought.traits.ui.api    import CodeEditor, TextEditor, Group, Handler, Item, View
 from enthought.traits.ui.menu   import NoButtons
 
 
@@ -51,7 +51,24 @@
 _raw_stdout = None
 _raw_stderr = None
 
+#####  Handler  #############################################################
 
+class LoggerHandler(Handler):
+
+    def object_data_changed(self, info):
+        """The purpose of this bit of trickery is to ensure that
+        the display does not scroll back to the first line when
+        data is added to the logger...
+        """
+        numlines = len(info.object.data.split("\n"))
+        assert len(info.ui._editors) == 1
+        editor = info.ui._editors[0]
+        editor.selected_line = numlines # Select the last line of text
+        editor.control.Refresh()
+
+
+#####  Logger Per Se  #######################################################
+
 class ConsoleLogger(HasTraits):
 
     ## Title of logger object
@@ -69,8 +86,20 @@
     active_logger = None
 
     ## Default view
-    traits_view = View(Item('data~', editor=CodeEditor(foldable=False), show_label=False))
+    traits_view = View(Item('data~',
+                            editor=CodeEditor(foldable=False, search='top', auto_scroll=False,
+                                              mark_color=0xFFFFFF),
+                            #editor=TextEditor(multi_line=True),
+                            show_label=False),
+                       handler = LoggerHandler())
 
+    ## Unwrapped (non-autoflush) python stdout and stderr
+    orig_stdout = None
+    orig_stderr = None
+
+    def _data_changed(self):
+        editor = self.traits()['data'].get_editor()
+
     def activate_stdouterr_redirect(self, streams_to_watch={}):
         """Redirect standard output and error to be sent to the log pane
         instead of the console.
@@ -81,6 +110,11 @@
                   "while it is already active for a different one"
         ConsoleLogger.active_logger = self
 
+        ## If redirection is already active, don't insist
+        global _raw_stdout, _raw_stderr
+        if _raw_stdout is not None or _raw_stderr is not None:
+            return
+
         ## Ensure that any pending writes are expelled before redirection
         ## (Python only)
         sys.stdout.flush()
@@ -91,7 +125,6 @@
         ## display debugging messages.  They are called, respectively,
         ## raw_stdout and raw_stderr (Python file objects); make them
         ## global for ease of debugging.
-        global _raw_stdout, _raw_stderr
         if _raw_stdout is None:
             old_stdout_fd = os.dup(sys.stdout.fileno())
             _raw_stdout   = os.fdopen(old_stdout_fd, 'w')
@@ -140,6 +173,27 @@
         listener.setDaemon(True)        # Allow quitting Python even if thread running
         self.is_active = True
         listener.start()
+
+        ## Ensure that all Python-level prints are auto-flushing
+        class AutoFlush(object):
+            def __init__(self, stream):
+                self.stream = stream
+
+            def write(self, text):
+                self.stream.write(text)
+                self.stream.flush()
+
+            def writelines(self, lines):
+                self.stream.writelines(lines)
+                self.stream.flush()
+
+            def flush(self):
+                self.stream.flush()
+
+        self.orig_stdout = sys.stdout
+        self.orig_stderr = sys.stderr
+        sys.stdout = AutoFlush(sys.stdout)
+        sys.stderr = AutoFlush(sys.stderr)
         
 
     def desactivate_stdout_err_redirect(self):
@@ -155,13 +209,17 @@
             sys.stderr.flush()
             time.sleep(0.1)
             
-            if _raw_stdout is not None:
-                os.dup2(_raw_stdout.fileno(), sys.stdout.fileno())
-                _raw_stdout = None
+            if _raw_stdout is not None and self.orig_stdout is not None:
+                os.dup2(_raw_stdout.fileno(), self.orig_stdout.fileno())
+                sys.stdout       = self.orig_stdout
+                _raw_stdout      = None
+                self.orig_stdout = None
 
-            if _raw_stderr is not None:
-                os.dup2(_raw_stderr.fileno(), sys.stderr.fileno())
-                _raw_stderr = None
+            if _raw_stderr is not None and self.orig_stderr is not None:
+                os.dup2(_raw_stderr.fileno(), self.orig_stderr.fileno())
+                sys.stderr       = self.orig_stderr
+                _raw_stderr      = None
+                self.orig_stderr = None
 
             ConsoleLogger.active_logger = None
             self.is_active = False

Modified: trunk/python_modules/plearn/gui_tools/xp_workbench.py
===================================================================
--- trunk/python_modules/plearn/gui_tools/xp_workbench.py	2008-06-18 20:54:00 UTC (rev 9147)
+++ trunk/python_modules/plearn/gui_tools/xp_workbench.py	2008-06-18 23:13:14 UTC (rev 9148)
@@ -314,10 +314,25 @@
 
     @staticmethod
     def expdir_name(expdir_root):
+        """Return an experiment directory from a root location."""
         return os.path.join(expdir_root,
                             datetime.now().strftime("expdir_%Y%m%d_%H%m%S"))
 
 
+    @staticmethod
+    def print_all_traits(root, out = sys.stdout, prefix=""):
+        """Recursively print out all traits in a key=value format.
+        Useful for generating metainfo files for experiments.
+        """
+        for (trait_name, trait_value) in root.get():
+            if trait_name in ["trait_added", "trait_modified"]:
+                continue
+            elif isinstance(trait_value, "HasTraits"):
+                print_all_traits(trait_value, out, trait_name+".")
+            else:
+                print prefix+trait_name+" = "+str(trait_value)
+
+
 #####  Top-Level Options  ###################################################
 
 class SingletonOptions(HasStrictTraits):
@@ -356,19 +371,6 @@
                    if t not in [ "expdir_root", "trait_added", "trait_modified" ] ]
         kwargs = { "show_labels":False, "layout":"tabbed" }
         return View(Group(*items, **kwargs))
-
-    @staticmethod
-    def print_all_traits(root, out = sys.stdout, prefix=""):
-        """Recursively print out all traits in a key=value format.
-        Useful for generating metainfo files for experiments.
-        """
-        for (trait_name, trait_value) in root.get():
-            if trait_name in ["trait_added", "trait_modified"]:
-                continue
-            elif isinstance(trait_value, "HasTraits"):
-                print_all_traits(trait_value, out, trait_name+".")
-            else:
-                print prefix+trait_name+" = "+str(trait_value)
     
 
 #####  Utilities  ###########################################################



From chapados at mail.berlios.de  Thu Jun 19 04:59:07 2008
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Thu, 19 Jun 2008 04:59:07 +0200
Subject: [Plearn-commits] r9149 - trunk/python_modules/plearn/gui_tools
Message-ID: <200806190259.m5J2x7LZ005625@sheep.berlios.de>

Author: chapados
Date: 2008-06-19 04:59:06 +0200 (Thu, 19 Jun 2008)
New Revision: 9149

Modified:
   trunk/python_modules/plearn/gui_tools/xp_workbench.py
Log:
Some fixes when printing metainfos

Modified: trunk/python_modules/plearn/gui_tools/xp_workbench.py
===================================================================
--- trunk/python_modules/plearn/gui_tools/xp_workbench.py	2008-06-18 23:13:14 UTC (rev 9148)
+++ trunk/python_modules/plearn/gui_tools/xp_workbench.py	2008-06-19 02:59:06 UTC (rev 9149)
@@ -324,13 +324,15 @@
         """Recursively print out all traits in a key=value format.
         Useful for generating metainfo files for experiments.
         """
-        for (trait_name, trait_value) in root.get():
-            if trait_name in ["trait_added", "trait_modified"]:
+        traits = root.get()
+        for trait_name in sorted(traits.keys()):
+            trait_value = traits[trait_name]
+            if trait_name in ["trait_added", "trait_modified"] or trait_name.startswith('_'):
                 continue
-            elif isinstance(trait_value, "HasTraits"):
-                print_all_traits(trait_value, out, trait_name+".")
+            elif isinstance(trait_value, HasTraits):
+                ExperimentWorkbench.print_all_traits(trait_value, out, trait_name+".")
             else:
-                print prefix+trait_name+" = "+str(trait_value)
+                print >>out, ("%-40s" % (prefix+trait_name)) + " = " + str(trait_value)
 
 
 #####  Top-Level Options  ###################################################



From tihocan at mail.berlios.de  Thu Jun 19 05:45:11 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 19 Jun 2008 05:45:11 +0200
Subject: [Plearn-commits] r9150 - trunk/plearn_learners/online
Message-ID: <200806190345.m5J3jBGB015931@sheep.berlios.de>

Author: tihocan
Date: 2008-06-19 05:45:09 +0200 (Thu, 19 Jun 2008)
New Revision: 9150

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
Continued work-in-progress experimental code to compare NLL gradient with CD, do not worry it is all disabled by default ;)

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2008-06-19 02:59:06 UTC (rev 9149)
+++ trunk/plearn_learners/online/RBMModule.cc	2008-06-19 03:45:09 UTC (rev 9150)
@@ -1425,7 +1425,23 @@
         int n_hidden_conf = hidden_layer->getConfigurationCount();
         p_xt_given_x.resize(n_visible_conf, visible->length());
         p_ht_given_x.resize(n_hidden_conf, visible->length());
+        Vec input(visible_layer->size);
+        Mat input_mat = input.toMat(1, input.length());
+        Mat grad_nll(hidden_layer->size, visible_layer->size);
+        Mat grad_cd(hidden_layer->size, visible_layer->size);
+        Mat grad_first_term(hidden_layer->size, visible_layer->size);
+        grad_nll.fill(0);
+        grad_cd.fill(0);
         for (int i = 0; i < visible->length(); i++) {
+            // Compute dF(visible)/dWij.
+            PLASSERT_MSG( visible->length() == 1, "The comparison can "
+                    "currently be made only with one input example at a "
+                    "time" );
+            computeHiddenActivations(*visible);
+            hidden_layer->computeExpectations();
+            transposeProduct(grad_first_term,
+                    hidden_layer->getExpectations(),
+                    *visible);
             // First compute P(h|x) for inputs x.
             computeAllHiddenProbabilities(*visible, p_ht_given_x);
             for (int t = 0; t < n_steps_compare; t++) {
@@ -1442,10 +1458,52 @@
                 pout << "Best (P = " << p_xt_given_x.column(0)(best_idx, 0) <<
                     ") for x = " << (*visible)(0) << ":" <<
                     endl << tmp << endl;
+                // Compute E_{X_t}[dF(X_t)/dWij | x].
+                for (int k = 0; k < n_visible_conf; k++) {
+                    visible_layer->getConfiguration(k, input);
+                    computeHiddenActivations(input_mat);
+                    hidden_layer->computeExpectations();
+                    transposeProductScaleAcc(grad_cd,
+                                             hidden_layer->getExpectations(),
+                                             input_mat,
+                                             -p_xt_given_x(k, 0),
+                                             real(1));
+                    if (t == 0) {
+                        // Also compute the gradient for the NLL.
+                        transposeProductScaleAcc(
+                                grad_nll,
+                                hidden_layer->getExpectations(),
+                                input_mat,
+                                -all_p_visible[k],
+                                real(1));
+                    }
+                }
+                // Compute difference between CD and NLL updates.
+                Mat diff = grad_nll.copy();
+                diff -= grad_cd;
+                grad_cd += grad_first_term;
+                if (t == 0)
+                    grad_nll += grad_first_term;
+                pout << "Grad_CD_" << t+1 << "=" << endl << grad_cd << endl;
+                // Compute average relative difference.
+                Vec all_relative_diffs;
+                for (int p = 0; p < diff.length(); p++)
+                    for (int q = 0; q < diff.width(); q++) {
+                        if (!fast_exact_is_equal(grad_nll(p, q), 0))
+                            all_relative_diffs.append(abs(diff(p, q) / grad_nll(p, q)));
+                    }
+                pout << "Median relative difference: "
+                    << median(all_relative_diffs) << endl;
+                pout << "Mean relative difference: "
+                    << mean(all_relative_diffs) << endl;
+                grad_cd.fill(0);
                 // If it is not the last step, update P(h_t|x).
                 if (t < n_steps_compare - 1)
                     product(p_ht_given_x, all_hidden_cond_prob, p_xt_given_x);
             }
+            pout << "P(x)=" << endl << all_p_visible << endl;
+            pout << "Grad_NLL=" << endl << grad_nll << endl;
+            pout << "Grad first term=" << endl << grad_first_term << endl;
         }
     }
 



From larocheh at mail.berlios.de  Fri Jun 20 16:32:34 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 20 Jun 2008 16:32:34 +0200
Subject: [Plearn-commits] r9151 - trunk/plearn_learners_experimental
Message-ID: <200806201432.m5KEWY6L023882@sheep.berlios.de>

Author: larocheh
Date: 2008-06-20 16:32:34 +0200 (Fri, 20 Jun 2008)
New Revision: 9151

Modified:
   trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc
Log:
Some debugging...


Modified: trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc
===================================================================
--- trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc	2008-06-19 03:45:09 UTC (rev 9150)
+++ trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc	2008-06-20 14:32:34 UTC (rev 9151)
@@ -77,7 +77,6 @@
 {
     // random_gen will be initialized in PLearner::build_()
     random_gen = new PRandom();
-    nstages = 0;
 }
 
 void DeepNonLocalManifoldParzen::declareOptions(OptionList& ol)
@@ -150,8 +149,8 @@
     declareOption(ol, "k_neighbors", 
                   &DeepNonLocalManifoldParzen::k_neighbors,
                   OptionBase::buildoption,
-                  "Number of good nearest neighbors to attract and bad nearest "
-                  "neighbors to repel.\n");
+                  "Number of nearest neighbors to use to learn "
+                  "the manifold structure..\n");
 
     declareOption(ol, "n_components", 
                   &DeepNonLocalManifoldParzen::n_components,
@@ -512,6 +511,7 @@
     deepCopyField(temp_ncomp, copies);
     deepCopyField(diff_neighbor_input, copies);
     deepCopyField(sm_svd, copies);
+    deepCopyField(S, copies);
     deepCopyField(uk, copies);
     deepCopyField(fk, copies);
     deepCopyField(uk2, copies);



From larocheh at mail.berlios.de  Fri Jun 20 16:35:23 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 20 Jun 2008 16:35:23 +0200
Subject: [Plearn-commits] r9152 - trunk/plearn_learners_experimental
Message-ID: <200806201435.m5KEZN4t024269@sheep.berlios.de>

Author: larocheh
Date: 2008-06-20 16:35:22 +0200 (Fri, 20 Jun 2008)
New Revision: 9152

Added:
   trunk/plearn_learners_experimental/ManifoldParzen.cc
   trunk/plearn_learners_experimental/ManifoldParzen.h
Log:
Another, less memory intensive, version of Manifold Parzen Windows...


Added: trunk/plearn_learners_experimental/ManifoldParzen.cc
===================================================================
--- trunk/plearn_learners_experimental/ManifoldParzen.cc	2008-06-20 14:32:34 UTC (rev 9151)
+++ trunk/plearn_learners_experimental/ManifoldParzen.cc	2008-06-20 14:35:22 UTC (rev 9152)
@@ -0,0 +1,502 @@
+// -*- C ++ -*-
+
+// ManifoldParzen.cc
+//
+// Copyright (C) 2007 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file ManifoldParzen.cc */
+
+
+#define PL_LOG_MODULE_NAME "ManifoldParzen"
+#include <plearn/io/pl_log.h>
+
+#include "ManifoldParzen.h"
+#include <plearn/vmat/VMat_computeNearestNeighbors.h>
+#include <plearn/vmat/GetInputVMatrix.h>
+#include <plearn_learners/online/GradNNetLayerModule.h>
+#include <plearn/math/plapack.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    ManifoldParzen,
+    "Manifold Parzen Windows classifier and distribution",
+    "");
+  
+ManifoldParzen::ManifoldParzen() :
+    nneighbors( 1 ),
+    ncomponents( 1 ),
+    global_lambda0( 0 ),
+    learn_mu( false ),
+    n_classes( -1 )
+{
+}
+
+void ManifoldParzen::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "nneighbors", 
+                  &ManifoldParzen::nneighbors,
+                  OptionBase::buildoption,
+                  "Number of nearest neighbors to use to learn "
+                  "the manifold structure..\n");
+
+    declareOption(ol, "ncomponents", 
+                  &ManifoldParzen::ncomponents,
+                  OptionBase::buildoption,
+                  "Dimensionality of the manifold.\n");
+
+    declareOption(ol, "global_lambda0", 
+                  &ManifoldParzen::global_lambda0,
+                  OptionBase::buildoption,
+                  "Additive minimum value for the variance in all directions.\n");
+
+    declareOption(ol, "learn_mu", 
+                  &ManifoldParzen::learn_mu,
+                  OptionBase::buildoption,
+                  "Additive minimum value for the variance in all directions.\n");
+
+    declareOption(ol, "n_classes", 
+                  &ManifoldParzen::n_classes,
+                  OptionBase::buildoption,
+                  "Number of classes. If n_classes = 1, learner will output\n"
+                  "log likelihood of a given input. If n_classes > 1,\n"
+                  "classification will be performed.\n");
+
+    declareOption(ol, "train_set", 
+                  &ManifoldParzen::train_set,
+                  OptionBase::learntoption,
+                  "Training set.\n"
+        );
+
+    declareOption(ol, "eigenvalues", 
+                  &ManifoldParzen::eigenvalues,
+                  OptionBase::learntoption,
+                  ""
+        );
+
+    declareOption(ol, "sigma_noises", 
+                  &ManifoldParzen::sigma_noises,
+                  OptionBase::learntoption,
+                  ""
+        );
+
+    declareOption(ol, "mus", 
+                  &ManifoldParzen::mus,
+                  OptionBase::learntoption,
+                  ""
+        );
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void ManifoldParzen::build_()
+{
+    // ### This method should do the real building of the object,
+    // ### according to set 'options', in *any* situation.
+    // ### Typical situations include:
+    // ###  - Initial building of an object from a few user-specified options
+    // ###  - Building of a "reloaded" object: i.e. from the complete set of
+    // ###    all serialised options.
+    // ###  - Updating or "re-building" of an object after a few "tuning"
+    // ###    options have been modified.
+    // ### You should assume that the parent class' build_() has already been
+    // ### called.
+
+    MODULE_LOG << "build_() called" << endl;
+
+    if(inputsize_ > 0 )
+    {
+        // Builds some variables using the training set
+        setTrainingSet(train_set, false);
+
+        if( n_classes <= 0 )
+            PLERROR("ManifoldParzen::build_() - \n"
+                    "n_classes should be > 0.\n");
+        test_votes.resize(n_classes);
+
+        if( nneighbors <= 0 )
+            PLERROR("ManifoldParzen::build_() - \n"
+                    "nneighbors should be > 0.\n");
+
+        if( weightsize_ > 0 )
+            PLERROR("ManifoldParzen::build_() - \n"
+                    "usage of weighted samples (weight size > 0) is not\n"
+                    "implemented yet.\n");
+
+        if( ncomponents < 1 || ncomponents > inputsize_)
+            PLERROR("ManifoldParzen::build_() - \n"
+                    "ncomponents should be > 0 and < or = to inputsize.\n");
+
+        if( global_lambda0 < 0)
+            PLERROR("ManifoldParzen::build_() - \n"
+                    "global_lambda0 should be > or = to 0.\n");
+
+    }
+}
+
+// ### Nothing to add here, simply calls build_
+void ManifoldParzen::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void ManifoldParzen::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+    // deepCopyField(, copies);
+
+    // Protected options
+    deepCopyField(mu, copies);
+    deepCopyField(Ut, copies);
+    deepCopyField(U, copies);
+    deepCopyField(V, copies);
+    deepCopyField(diff_neighbor_input, copies);
+    deepCopyField(uk, copies);
+    deepCopyField(sm_svd, copies);
+    deepCopyField(S, copies);
+    deepCopyField(diff, copies);
+    deepCopyField(eigenvectors, copies);
+    deepCopyField(eigenvalues, copies);
+    deepCopyField(sigma_noises, copies);
+    deepCopyField(mus, copies);
+    deepCopyField(class_datasets, copies);
+    deepCopyField(nearest_neighbors_indices, copies);
+    deepCopyField(test_votes, copies);
+}
+
+
+int ManifoldParzen::outputsize() const
+{
+    return 1;
+}
+
+void ManifoldParzen::forget()
+{
+    //! (Re-)initialize the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!)
+    /*!
+      A typical forget() method should do the following:
+      - call inherited::forget() to initialize its random number generator
+        with the 'seed' option
+      - initialize the learner's parameters, using this random generator
+      - stage = 0
+    */
+    inherited::forget();
+
+    for(int i=0; i < eigenvectors.length(); i++)
+      eigenvectors[i].clear();
+    eigenvalues.clear();
+    sigma_noises.clear();
+    mus.clear();
+    stage = 0;
+}
+
+void ManifoldParzen::train()
+{
+    MODULE_LOG << "train() called " << endl;
+
+    Vec input( inputsize() );
+    Vec nearest_neighbor( inputsize() );
+    Mat nearest_neighbors( nneighbors, inputsize() );
+    Vec target( targetsize() );
+    Vec target2( targetsize() );
+    real weight; // unused
+    real weight2; // unused
+
+    TVec<string> train_cost_names = getTrainCostNames() ;
+    Vec train_costs( train_cost_names.length() );
+    train_costs.fill(MISSING_VALUE) ;
+
+    int sample;
+    PP<ProgressBar> pb;
+
+    // clear stats of previous epoch
+    train_stats->forget();
+
+    if( stage < 1 && nstages > 0 )
+      {
+	stage = 1;
+	MODULE_LOG << "Finding the nearest neighbors" << endl;
+	// Find training nearest neighbors
+	TVec<int> nearest_neighbors_indices_row;
+	nearest_neighbors_indices.resize(train_set->length(), nneighbors);
+	if( n_classes > 1 )
+	  for(int k=0; k<n_classes; k++)
+	    {
+	      for(int i=0; i<class_datasets[k]->length(); i++)
+		{
+		  class_datasets[k]->getExample(i,input,target,weight);
+		  nearest_neighbors_indices_row = nearest_neighbors_indices(
+									    class_datasets[k]->indices[i]);
+		
+		  computeNearestNeighbors(
+					  new GetInputVMatrix((VMatrix *)class_datasets[k]),input,
+					  nearest_neighbors_indices_row,
+					  i);
+		}
+	    }
+	else
+	  for(int i=0; i<train_set->length(); i++)
+	    {
+	      train_set->getExample(i,input,target,weight);
+	      nearest_neighbors_indices_row = nearest_neighbors_indices(i);
+	      computeNearestNeighbors(
+				      train_set,input,
+				      nearest_neighbors_indices_row,
+				      i);
+	    }
+      
+        train_costs.fill(MISSING_VALUE);
+
+        if( report_progress )
+	  pb = new ProgressBar( "Training ManifoldParzen",
+				train_set->length() );
+
+	eigenvectors.resize( train_set->length() );
+	eigenvalues.resize( train_set->length(), ncomponents );
+	sigma_noises.resize( train_set->length(), 1 );
+	mus.resize( train_set->length(), inputsize() );
+	mus.clear();
+        for( sample = 0; sample < train_set->length() ; sample++ )
+	  { 
+            train_set->getExample( sample, input, target, weight );
+	    
+            // Find nearest neighbors
+            if( n_classes > 1 )
+	      for( int k=0; k<nneighbors; k++ )
+                {
+		  class_datasets[(int)round(target[0])]->getExample(
+								    nearest_neighbors_indices(sample,k),
+								    nearest_neighbor, target2, weight2);
+		  
+		  if(round(target[0]) != round(target2[0]))
+		    PLERROR("ManifoldParzen::train(): similar"
+			    " example is not from same class!");
+		  nearest_neighbors(k) << nearest_neighbor;
+                }
+            else
+	      for( int k=0; k<nneighbors; k++ )
+                {
+		  train_set->getExample(
+					nearest_neighbors_indices(sample,k),
+					nearest_neighbor, target2, weight2);
+		  nearest_neighbors(k) << nearest_neighbor;
+                }
+	    
+	    if( learn_mu )
+	      {
+		mu.resize(inputsize());
+		columnMean( nearest_neighbors, mu );
+		mus(sample) << mu;
+		mus(sample) -= input;
+	      }
+	    substractFromRows(nearest_neighbors, input, false); // Boolean is somehow unused???
+	    lapackSVD(nearest_neighbors, Ut, S, V,'A',1.5);
+	    eigenvectors[sample].resize(ncomponents,inputsize());
+	    for (int k=0;k<ncomponents;k++)
+	      {
+		eigenvalues(sample,k) = mypow(S[k],2);
+		eigenvectors[sample](k) << Ut(k);
+	      }
+	    sigma_noises[sample] = 0; // HUGO: Seems stupid for now, but I keep 
+	                              //       this variable in case I want to use
+	                              //       the last eigen value or something...
+
+            if( pb )
+	      pb->update( sample + 1 );
+	  }
+      }
+    
+    train_stats->finalize();
+    MODULE_LOG << "  train costs = " << train_stats->getMean() << endl;
+}
+
+void ManifoldParzen::computeOutput(const Vec& input, Vec& output) const
+{
+
+    test_votes.resize(n_classes);
+    test_votes.clear();
+
+    // Variables for probability computations
+    real log_p_x_g_y = 0;
+    real mahal = 0;
+    real norm_term = 0;
+    real n = inputsize();
+    real dotp = 0;
+    real coef = 0;
+    real sigma_noise = 0;
+    
+    Vec input_j(inputsize());
+    Vec target(targetsize());
+    real weight;
+
+    int input_j_index;
+    for( int i=0; i<n_classes; i++ )
+      {
+	for( int j=0; 
+	     j<(n_classes > 1 ? 
+		class_datasets[i]->length() 
+		: train_set->length()); 
+	     j++ )
+	  {
+	    if( n_classes > 1 )
+	      {
+		class_datasets[i]->getExample(j,input_j,target,weight);
+		input_j_index = class_datasets[i]->indices[j];
+	      }
+	    else
+	      {
+		train_set->getExample(j,input_j,target,weight);
+		input_j_index = j;
+	      }
+	    
+	    U << eigenvectors[input_j_index];
+	    sm_svd << eigenvalues(input_j_index);
+	    sigma_noise = sigma_noises[input_j_index] + global_lambda0;
+	    mu << mus(input_j_index);
+	    
+	    substract(input,input_j,diff_neighbor_input); 
+	    substract(diff_neighbor_input,mu,diff); 
+            
+	    mahal = -0.5*pownorm(diff)/sigma_noise;      
+	    norm_term = - n/2.0 * Log2Pi - 0.5*(n-ncomponents)*
+	      pl_log(sigma_noise);
+	    
+	    for(int k=0; k<ncomponents; k++)
+	      { 
+		uk = U(k);
+		dotp = dot(diff,uk);
+		coef = (1.0/(sm_svd[k]+global_lambda0) - 1.0/sigma_noise);
+		mahal -= dotp*dotp*0.5*coef;
+		norm_term -= 0.5*pl_log(sm_svd[k]+global_lambda0);
+	      }
+	    
+	    if( j==0 )
+	      log_p_x_g_y = norm_term + mahal;
+	    else
+	      log_p_x_g_y = logadd(norm_term + mahal, log_p_x_g_y);
+	  }
+        
+	test_votes[i] = log_p_x_g_y;
+      }
+
+    if( n_classes > 1 )
+        output[0] = argmax(test_votes);
+    else
+        output[0] = test_votes[0]-pl_log(train_set->length());
+}
+
+void ManifoldParzen::computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                           const Vec& target, Vec& costs) const
+{
+
+    //Assumes that computeOutput has been called
+
+    costs.resize( getTestCostNames().length() );
+    costs.fill( MISSING_VALUE );
+
+    if( n_classes > 1 )
+      {
+	int target_class = ((int)round(target[0]));
+	if( ((int)round(output[0])) == target_class )
+	  costs[0] = 0;
+	else
+	  costs[0] = 1;
+	costs[1] = - test_votes[target_class]
+	  +pl_log(class_datasets[target_class]->length()); // Must take into account the 1/n normalization
+      }
+    else
+      {
+	costs[1] = - output[0]; // 1/n normalization already accounted for
+      }
+}
+
+TVec<string> ManifoldParzen::getTestCostNames() const
+{
+    // Return the names of the costs computed by computeCostsFromOutputs
+    // (these may or may not be exactly the same as what's returned by
+    // getTrainCostNames).
+
+    TVec<string> cost_names(0);
+
+    cost_names.append( "class_error" );
+    cost_names.append( "NLL" );
+
+    return cost_names;
+}
+
+TVec<string> ManifoldParzen::getTrainCostNames() const
+{
+    TVec<string> cost_names(0);
+    return cost_names ;    
+}
+
+void ManifoldParzen::setTrainingSet(VMat training_set, bool call_forget)
+{
+    inherited::setTrainingSet(training_set,call_forget);
+    
+    // Separate classes
+    if( n_classes > 1 )
+    {
+        class_datasets.resize(n_classes);
+        for(int k=0; k<n_classes; k++)
+        {
+            class_datasets[k] = new ClassSubsetVMatrix();
+            class_datasets[k]->classes.resize(1);
+            class_datasets[k]->classes[0] = k;
+            class_datasets[k]->source = training_set;
+            class_datasets[k]->build();
+        }
+    }
+
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners_experimental/ManifoldParzen.h
===================================================================
--- trunk/plearn_learners_experimental/ManifoldParzen.h	2008-06-20 14:32:34 UTC (rev 9151)
+++ trunk/plearn_learners_experimental/ManifoldParzen.h	2008-06-20 14:35:22 UTC (rev 9152)
@@ -0,0 +1,203 @@
+// -*- C++ -*-
+
+// ManifoldParzen.h
+//
+// Copyright (C) 2007 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file ManifoldParzen.h */
+
+
+#ifndef ManifoldParzen_INC
+#define ManifoldParzen_INC
+
+#include <plearn/vmat/ClassSubsetVMatrix.h>
+#include <plearn_learners/generic/PLearner.h>
+
+namespace PLearn {
+
+/**
+ * Manifold Parzen Windows classifier and distribution.
+ */
+class ManifoldParzen : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! Number of nearest neighbors to use to learn
+    //! the manifold structure.
+    int nneighbors;
+
+    //! Dimensionality of the manifold
+    int ncomponents;
+
+    //! Additive minimum value for the variance in all directions.
+    real global_lambda0;
+
+    //! Indication that the meam of the gaussians should also be learned
+    bool learn_mu;
+
+    //! Number of classes
+    int n_classes;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    ManifoldParzen();
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Returns the size of this learner's output, (which typically
+    //! may depend on its inputsize(), targetsize() and set options).
+    virtual int outputsize() const;
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training costs
+    //! measured on-line in the process.
+    virtual void train();
+
+    //! Computes the output from the input.
+    virtual void computeOutput(const Vec& input, Vec& output) const;
+
+    //! Computes the costs from already computed output.
+    virtual void computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                         const Vec& target, Vec& costs) const;
+
+    //! Returns the names of the costs computed by computeCostsFromOutpus (and
+    //! thus the test method).
+    virtual TVec<std::string> getTestCostNames() const;
+
+    //! Returns the names of the objective costs that the train method computes
+    //! and  for which it updates the VecStatsCollector train_stats.
+    virtual TVec<std::string> getTrainCostNames() const;
+
+    /**
+     *  Declares the training set.  Then calls build() and forget() if
+     *  necessary.  Also sets this learner's inputsize_ targetsize_ weightsize_
+     *  from those of the training_set.  Note: You shouldn't have to override
+     *  this in subclasses, except in maybe to forward the call to an
+     *  underlying learner.
+     */
+    virtual void setTrainingSet(VMat training_set, bool call_forget=true);
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(ManifoldParzen);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Not Options  #####################################################
+
+    //! Variables for density of a Gaussian
+    mutable Vec mu;
+
+    //! Variables for the SVD and gradient computation
+    mutable Mat Ut, U, V;
+    mutable Vec diff_neighbor_input, uk, sm_svd, S, diff;
+
+    // Saved components of manifold parzen windows
+    //! Eigenvectors
+    mutable TVec<Mat> eigenvectors;
+    //! Eigenvalues
+    mutable Mat eigenvalues;
+    //! Sigma noises
+    mutable Vec sigma_noises;
+    //! Mus
+    mutable Mat mus;
+
+    //! Datasets for each class
+    TVec< PP<ClassSubsetVMatrix> > class_datasets;
+
+    //! Proportions of examples from the other classes (columns), for each
+    //! class (rows)
+    //Vec class_proportions;
+
+    //! Nearest neighbors for each training example
+    TMat<int> nearest_neighbors_indices;
+
+    //! Nearest neighbor votes for test example
+    mutable Vec test_votes;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here    
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(ManifoldParzen);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From larocheh at mail.berlios.de  Fri Jun 20 17:49:58 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 20 Jun 2008 17:49:58 +0200
Subject: [Plearn-commits] r9153 - trunk/plearn_learners_experimental
Message-ID: <200806201549.m5KFnwup030479@sheep.berlios.de>

Author: larocheh
Date: 2008-06-20 17:49:58 +0200 (Fri, 20 Jun 2008)
New Revision: 9153

Modified:
   trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc
Log:
Corrected indentation...



Modified: trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc
===================================================================
--- trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc	2008-06-20 14:35:22 UTC (rev 9152)
+++ trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc	2008-06-20 15:49:58 UTC (rev 9153)
@@ -1,4 +1,4 @@
-// -*- C ++ -*-
+// -*- C++ -*-
 
 // DeepNonLocalManifoldParzen.cc
 //
@@ -286,7 +286,7 @@
                     "train_one_network_per_class must be true for "
 		    "Test-Centric NLMP variant.\n");
 	  
-	if( use_test_centric_nlmp && n_classes > 1)
+	if( use_test_centric_nlmp && n_classes <= 1)
 	    PLERROR("DeepNonLocalManifoldParzen::build_() - \n"
                     "n_classes must be > 1 for "
 		    "Test-Centric NLMP variant.\n");



From larocheh at mail.berlios.de  Fri Jun 20 17:50:37 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 20 Jun 2008 17:50:37 +0200
Subject: [Plearn-commits] r9154 - trunk/plearn_learners_experimental
Message-ID: <200806201550.m5KFobHS030534@sheep.berlios.de>

Author: larocheh
Date: 2008-06-20 17:50:36 +0200 (Fri, 20 Jun 2008)
New Revision: 9154

Modified:
   trunk/plearn_learners_experimental/ManifoldParzen.cc
   trunk/plearn_learners_experimental/ManifoldParzen.h
Log:
Corrected indentation and some other stuff...


Modified: trunk/plearn_learners_experimental/ManifoldParzen.cc
===================================================================
--- trunk/plearn_learners_experimental/ManifoldParzen.cc	2008-06-20 15:49:58 UTC (rev 9153)
+++ trunk/plearn_learners_experimental/ManifoldParzen.cc	2008-06-20 15:50:36 UTC (rev 9154)
@@ -1,4 +1,4 @@
-// -*- C ++ -*-
+// -*- C++ -*-
 
 // ManifoldParzen.cc
 //
@@ -59,7 +59,7 @@
     ncomponents( 1 ),
     global_lambda0( 0 ),
     learn_mu( false ),
-    n_classes( -1 )
+    nclasses( -1 )
 {
 }
 
@@ -86,11 +86,11 @@
                   OptionBase::buildoption,
                   "Additive minimum value for the variance in all directions.\n");
 
-    declareOption(ol, "n_classes", 
-                  &ManifoldParzen::n_classes,
+    declareOption(ol, "nclasses", 
+                  &ManifoldParzen::nclasses,
                   OptionBase::buildoption,
-                  "Number of classes. If n_classes = 1, learner will output\n"
-                  "log likelihood of a given input. If n_classes > 1,\n"
+                  "Number of classes. If nclasses = 1, learner will output\n"
+                  "log likelihood of a given input. If nclasses > 1,\n"
                   "classification will be performed.\n");
 
     declareOption(ol, "train_set", 
@@ -140,11 +140,11 @@
     {
         // Builds some variables using the training set
         setTrainingSet(train_set, false);
-
-        if( n_classes <= 0 )
+        
+        if( nclasses <= 0 )
             PLERROR("ManifoldParzen::build_() - \n"
-                    "n_classes should be > 0.\n");
-        test_votes.resize(n_classes);
+                    "nclasses should be > 0.\n");
+        test_votes.resize(nclasses);
 
         if( nneighbors <= 0 )
             PLERROR("ManifoldParzen::build_() - \n"
@@ -255,8 +255,8 @@
 	// Find training nearest neighbors
 	TVec<int> nearest_neighbors_indices_row;
 	nearest_neighbors_indices.resize(train_set->length(), nneighbors);
-	if( n_classes > 1 )
-	  for(int k=0; k<n_classes; k++)
+	if( nclasses > 1 )
+	  for(int k=0; k<nclasses; k++)
 	    {
 	      for(int i=0; i<class_datasets[k]->length(); i++)
 		{
@@ -297,7 +297,7 @@
             train_set->getExample( sample, input, target, weight );
 	    
             // Find nearest neighbors
-            if( n_classes > 1 )
+            if( nclasses > 1 )
 	      for( int k=0; k<nneighbors; k++ )
                 {
 		  class_datasets[(int)round(target[0])]->getExample(
@@ -349,7 +349,7 @@
 void ManifoldParzen::computeOutput(const Vec& input, Vec& output) const
 {
 
-    test_votes.resize(n_classes);
+    test_votes.resize(nclasses);
     test_votes.clear();
 
     // Variables for probability computations
@@ -365,16 +365,21 @@
     Vec target(targetsize());
     real weight;
 
+    U.resize( ncomponents, inputsize() );
+    sm_svd.resize( ncomponents );
+    mu.resize( inputsize() );
+
+
     int input_j_index;
-    for( int i=0; i<n_classes; i++ )
+    for( int i=0; i<nclasses; i++ )
       {
 	for( int j=0; 
-	     j<(n_classes > 1 ? 
+	     j<(nclasses > 1 ? 
 		class_datasets[i]->length() 
 		: train_set->length()); 
 	     j++ )
 	  {
-	    if( n_classes > 1 )
+	    if( nclasses > 1 )
 	      {
 		class_datasets[i]->getExample(j,input_j,target,weight);
 		input_j_index = class_datasets[i]->indices[j];
@@ -415,7 +420,7 @@
 	test_votes[i] = log_p_x_g_y;
       }
 
-    if( n_classes > 1 )
+    if( nclasses > 1 )
         output[0] = argmax(test_votes);
     else
         output[0] = test_votes[0]-pl_log(train_set->length());
@@ -430,7 +435,7 @@
     costs.resize( getTestCostNames().length() );
     costs.fill( MISSING_VALUE );
 
-    if( n_classes > 1 )
+    if( nclasses > 1 )
       {
 	int target_class = ((int)round(target[0]));
 	if( ((int)round(output[0])) == target_class )
@@ -471,10 +476,10 @@
     inherited::setTrainingSet(training_set,call_forget);
     
     // Separate classes
-    if( n_classes > 1 )
+    if( nclasses > 1 )
     {
-        class_datasets.resize(n_classes);
-        for(int k=0; k<n_classes; k++)
+        class_datasets.resize(nclasses);
+        for(int k=0; k<nclasses; k++)
         {
             class_datasets[k] = new ClassSubsetVMatrix();
             class_datasets[k]->classes.resize(1);

Modified: trunk/plearn_learners_experimental/ManifoldParzen.h
===================================================================
--- trunk/plearn_learners_experimental/ManifoldParzen.h	2008-06-20 15:49:58 UTC (rev 9153)
+++ trunk/plearn_learners_experimental/ManifoldParzen.h	2008-06-20 15:50:36 UTC (rev 9154)
@@ -69,7 +69,7 @@
     bool learn_mu;
 
     //! Number of classes
-    int n_classes;
+    int nclasses;
 
 public:
     //#####  Public Member Functions  #########################################



From tihocan at mail.berlios.de  Fri Jun 20 18:11:43 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 20 Jun 2008 18:11:43 +0200
Subject: [Plearn-commits] r9155 - trunk/plearn_learners/online
Message-ID: <200806201611.m5KGBhqs000454@sheep.berlios.de>

Author: tihocan
Date: 2008-06-20 18:11:42 +0200 (Fri, 20 Jun 2008)
New Revision: 9155

Modified:
   trunk/plearn_learners/online/SplitModule.cc
Log:
Minor cosmetic changes

Modified: trunk/plearn_learners/online/SplitModule.cc
===================================================================
--- trunk/plearn_learners/online/SplitModule.cc	2008-06-20 15:50:36 UTC (rev 9154)
+++ trunk/plearn_learners/online/SplitModule.cc	2008-06-20 16:11:42 UTC (rev 9155)
@@ -59,12 +59,17 @@
     "can be seen as performed on each row of these matrices.\n"
     );
 
+/////////////////
+// SplitModule //
+/////////////////
 SplitModule::SplitModule()
     : down_port_name("down_port")
-/* ### Initialize all fields to their default value here */
 {
 }
 
+////////////////////
+// declareOptions //
+////////////////////
 void SplitModule::declareOptions(OptionList& ol)
 {
     declareOption(ol, "down_port_name", &SplitModule::down_port_name,
@@ -91,6 +96,9 @@
                     "output_size = sum(up_ports_size) but is set automatically.");
 }
 
+////////////
+// build_ //
+////////////
 void SplitModule::build_()
 {
     int n_up_ports = up_port_sizes.length();



From tihocan at mail.berlios.de  Fri Jun 20 18:12:43 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 20 Jun 2008 18:12:43 +0200
Subject: [Plearn-commits] r9156 - trunk/plearn_learners/online
Message-ID: <200806201612.m5KGChDh000522@sheep.berlios.de>

Author: tihocan
Date: 2008-06-20 18:12:42 +0200 (Fri, 20 Jun 2008)
New Revision: 9156

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
The median relative difference between the CD update and the NLL gradient is now a port

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2008-06-20 16:11:42 UTC (rev 9155)
+++ trunk/plearn_learners/online/RBMModule.cc	2008-06-20 16:12:42 UTC (rev 9156)
@@ -78,6 +78,11 @@
     "    a column matrix with entries = -log( sum_h P(x|h) w_h ) for each row x in the input\n"
     "    'visible' port. This quantity would be a valid - log P(x) if sum_h w_h = 1, under the\n"
     "    joint model P(x,h) = P(x|h) P(h), with P(h)=w_h.\n"
+    "   - 'median_reldiff_cd_nll': median relative difference between the CD\n"
+    "     update and the true NLL gradient. Here, the CD update is not\n"
+    "     stochastic, but is computed exactly as the truncation of the log-\n"
+    "     likelihood expansion. This port has size 'n_steps_compare': there\n"
+    "     is one value for each step of the CD.\n"
     "    \n"
     "An RBM also has other ports that exist only if some options are set.\n"
     "If reconstruction_connection is given, then it has\n"
@@ -376,6 +381,7 @@
     // each row x of "visible", and where {P(h)}_h is provided
     // in "neg_log_phidden" for the set of h's in "hidden".
     addPortName("neg_log_pvisible_given_phidden");
+    addPortName("median_reldiff_cd_nll");
     if(reconstruction_connection)
     {
         addPortName("visible_reconstruction.state");
@@ -712,6 +718,9 @@
     PLERROR("In RBMModule::fprop - Not implemented");
 }
 
+//////////////////////////////
+// computePartitionFunction //
+//////////////////////////////
 void RBMModule::computePartitionFunction()
 {
     int hidden_configurations = hidden_layer->getConfigurationCount();
@@ -810,6 +819,9 @@
         pout << "Log Z(" << name << ") = " << log_partition_function << endl;
 }
 
+///////////
+// fprop //
+///////////
 void RBMModule::fprop(const TVec<Mat*>& ports_value)
 {
 
@@ -841,6 +853,8 @@
     bool neg_log_phidden_is_output = neg_log_phidden && neg_log_phidden->isEmpty();
     Mat* neg_log_pvisible_given_phidden = ports_value[getPortIndex("neg_log_pvisible_given_phidden")];
     bool neg_log_pvisible_given_phidden_is_output = neg_log_pvisible_given_phidden && neg_log_pvisible_given_phidden->isEmpty();
+    Mat* median_reldiff_cd_nll = ports_value[getPortIndex("median_reldiff_cd_nll")];
+    bool median_reldiff_cd_nll_is_output = median_reldiff_cd_nll && median_reldiff_cd_nll->isEmpty();
     hidden_bias = ports_value[getPortIndex("hidden_bias")];
     //bool hidden_bias_is_output = hidden_bias && hidden_bias->isEmpty();
     weights = ports_value[getPortIndex("weights")];
@@ -1431,7 +1445,8 @@
         Mat grad_cd(hidden_layer->size, visible_layer->size);
         Mat grad_first_term(hidden_layer->size, visible_layer->size);
         grad_nll.fill(0);
-        grad_cd.fill(0);
+        if (median_reldiff_cd_nll_is_output)
+            median_reldiff_cd_nll->resize(visible->length(), n_steps_compare);
         for (int i = 0; i < visible->length(); i++) {
             // Compute dF(visible)/dWij.
             PLASSERT_MSG( visible->length() == 1, "The comparison can "
@@ -1447,18 +1462,26 @@
             for (int t = 0; t < n_steps_compare; t++) {
                 // Compute P(x_t|x).
                 product(p_xt_given_x, all_visible_cond_prob, p_ht_given_x);
+                /*
                 pout << "P(x_" << (t + 1) << "|x) = " << endl << p_xt_given_x
                      << endl;
+                     */
                 Vec colsum(p_xt_given_x.width());
                 columnSum(p_xt_given_x, colsum);
-                pout << "Sum = " << endl << colsum << endl;
+                for (int j = 0; j < colsum.length(); j++) {
+                    PLASSERT( is_equal(colsum[j], 1) );
+                }
+                //pout << "Sum = " << endl << colsum << endl;
                 int best_idx = argmax(p_xt_given_x.column(0).toVecCopy());
                 Vec tmp(visible_layer->size);
                 visible_layer->getConfiguration(best_idx, tmp);
+                /*
                 pout << "Best (P = " << p_xt_given_x.column(0)(best_idx, 0) <<
                     ") for x = " << (*visible)(0) << ":" <<
                     endl << tmp << endl;
+                */
                 // Compute E_{X_t}[dF(X_t)/dWij | x].
+                grad_cd.fill(0);
                 for (int k = 0; k < n_visible_conf; k++) {
                     visible_layer->getConfiguration(k, input);
                     computeHiddenActivations(input_mat);
@@ -1482,9 +1505,8 @@
                 Mat diff = grad_nll.copy();
                 diff -= grad_cd;
                 grad_cd += grad_first_term;
-                if (t == 0)
-                    grad_nll += grad_first_term;
                 pout << "Grad_CD_" << t+1 << "=" << endl << grad_cd << endl;
+                pout << "Diff =" << endl << diff << endl;
                 // Compute average relative difference.
                 Vec all_relative_diffs;
                 for (int p = 0; p < diff.length(); p++)
@@ -1492,21 +1514,31 @@
                         if (!fast_exact_is_equal(grad_nll(p, q), 0))
                             all_relative_diffs.append(abs(diff(p, q) / grad_nll(p, q)));
                     }
+                pout << "All relative diffs: " << all_relative_diffs << endl;
+                (*median_reldiff_cd_nll)(i, t) = median(all_relative_diffs);
                 pout << "Median relative difference: "
                     << median(all_relative_diffs) << endl;
                 pout << "Mean relative difference: "
                     << mean(all_relative_diffs) << endl;
-                grad_cd.fill(0);
                 // If it is not the last step, update P(h_t|x).
                 if (t < n_steps_compare - 1)
                     product(p_ht_given_x, all_hidden_cond_prob, p_xt_given_x);
             }
-            pout << "P(x)=" << endl << all_p_visible << endl;
+            //pout << "P(x)=" << endl << all_p_visible << endl;
+            grad_nll += grad_first_term;
             pout << "Grad_NLL=" << endl << grad_nll << endl;
             pout << "Grad first term=" << endl << grad_first_term << endl;
         }
     }
 
+    if (median_reldiff_cd_nll_is_output && median_reldiff_cd_nll->isEmpty()) {
+        // We still need to compute 'median_reldiff_cd_nll'. This is not done
+        // during training, in order to save time.
+        PLASSERT( during_training );
+        median_reldiff_cd_nll->resize(visible->length(), n_steps_compare);
+        median_reldiff_cd_nll->fill(MISSING_VALUE);
+    }
+
     // UGLY HACK TO DEAL WITH THE PROBLEM THAT XXX.state MAY NOT BE NEEDED
     // BUT IS ALWAYS EXPECTED BECAUSE IT IS A STATE (!@#$%!!!)
     if (hidden_act && hidden_act->isEmpty())



From larocheh at mail.berlios.de  Fri Jun 20 21:27:24 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 20 Jun 2008 21:27:24 +0200
Subject: [Plearn-commits] r9157 - trunk/plearn_learners_experimental
Message-ID: <200806201927.m5KJRONo007284@sheep.berlios.de>

Author: larocheh
Date: 2008-06-20 21:27:24 +0200 (Fri, 20 Jun 2008)
New Revision: 9157

Modified:
   trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc
   trunk/plearn_learners_experimental/ManifoldParzen.cc
Log:
Corrected indentation...



Modified: trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc
===================================================================
--- trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc	2008-06-20 16:12:42 UTC (rev 9156)
+++ trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc	2008-06-20 19:27:24 UTC (rev 9157)
@@ -550,7 +550,7 @@
     /*!
       A typical forget() method should do the following:
       - call inherited::forget() to initialize its random number generator
-        with the 'seed' option
+      with the 'seed' option
       - initialize the learner's parameters, using this random generator
       - stage = 0
     */
@@ -626,7 +626,7 @@
     for( int i=0 ; i<n_layers-1 ; i++ )
     {
         MODULE_LOG << "Training connection weights between layers " << i
-            << " and " << i+1 << endl;
+                   << " and " << i+1 << endl;
 
         int end_stage = training_schedule[i];
         int* this_stage = greedy_stages.subVec(i,1).data();
@@ -804,7 +804,7 @@
     computeRepresentation(input, previous_input_representation, 
                           index);
     connections[index]->fprop(previous_input_representation,
-                                     activations[index+1]);
+                              activations[index+1]);
     layers[index+1]->fprop(activations[index+1],
                            expectations[index+1]);
 
@@ -896,7 +896,7 @@
     {
         if( !fast_exact_is_equal( cd_decrease_ct , 0 ) )
             lr = cd_learning_rate/(1 + cd_decrease_ct 
-                                       * this_stage); 
+                                   * this_stage); 
         else
             lr = cd_learning_rate;
 
@@ -1005,7 +1005,7 @@
             coef = (1.0/(sm_svd[k]+sigma_noise) - 1.0/sigma_noise);
             multiplyAcc(inv_sigma_zj,uk,dotp*coef);
             mahal -= dotp*dotp*0.5*coef;
-            norm_term -= 0.5*pl_log(sm_svd[k]);
+            norm_term -= 0.5*pl_log(sm_svd[k]+sigma_noise);
             if(j==0)
                 tr_inv_Sigma += coef;
         }
@@ -1107,8 +1107,8 @@
 
 
 void DeepNonLocalManifoldParzen::computeRepresentation(const Vec& input,
-                                                             Vec& representation,
-                                                             int layer) const
+                                                       Vec& representation,
+                                                       int layer) const
 {
     if(layer == 0)
     {
@@ -1301,7 +1301,7 @@
 }
 
 void DeepNonLocalManifoldParzen::computeCostsFromOutputs(const Vec& input, const Vec& output,
-                                           const Vec& target, Vec& costs) const
+                                                         const Vec& target, Vec& costs) const
 {
 
     //Assumes that computeOutput has been called

Modified: trunk/plearn_learners_experimental/ManifoldParzen.cc
===================================================================
--- trunk/plearn_learners_experimental/ManifoldParzen.cc	2008-06-20 16:12:42 UTC (rev 9156)
+++ trunk/plearn_learners_experimental/ManifoldParzen.cc	2008-06-20 19:27:24 UTC (rev 9157)
@@ -212,14 +212,14 @@
     /*!
       A typical forget() method should do the following:
       - call inherited::forget() to initialize its random number generator
-        with the 'seed' option
+      with the 'seed' option
       - initialize the learner's parameters, using this random generator
       - stage = 0
     */
     inherited::forget();
 
     for(int i=0; i < eigenvectors.length(); i++)
-      eigenvectors[i].clear();
+        eigenvectors[i].clear();
     eigenvalues.clear();
     sigma_noises.clear();
     mus.clear();
@@ -249,43 +249,43 @@
     train_stats->forget();
 
     if( stage < 1 && nstages > 0 )
-      {
+    {
 	stage = 1;
 	MODULE_LOG << "Finding the nearest neighbors" << endl;
 	// Find training nearest neighbors
 	TVec<int> nearest_neighbors_indices_row;
 	nearest_neighbors_indices.resize(train_set->length(), nneighbors);
 	if( nclasses > 1 )
-	  for(int k=0; k<nclasses; k++)
+            for(int k=0; k<nclasses; k++)
 	    {
-	      for(int i=0; i<class_datasets[k]->length(); i++)
+                for(int i=0; i<class_datasets[k]->length(); i++)
 		{
-		  class_datasets[k]->getExample(i,input,target,weight);
-		  nearest_neighbors_indices_row = nearest_neighbors_indices(
-									    class_datasets[k]->indices[i]);
+                    class_datasets[k]->getExample(i,input,target,weight);
+                    nearest_neighbors_indices_row = nearest_neighbors_indices(
+                        class_datasets[k]->indices[i]);
 		
-		  computeNearestNeighbors(
-					  new GetInputVMatrix((VMatrix *)class_datasets[k]),input,
-					  nearest_neighbors_indices_row,
-					  i);
+                    computeNearestNeighbors(
+                        new GetInputVMatrix((VMatrix *)class_datasets[k]),input,
+                        nearest_neighbors_indices_row,
+                        i);
 		}
 	    }
 	else
-	  for(int i=0; i<train_set->length(); i++)
+            for(int i=0; i<train_set->length(); i++)
 	    {
-	      train_set->getExample(i,input,target,weight);
-	      nearest_neighbors_indices_row = nearest_neighbors_indices(i);
-	      computeNearestNeighbors(
-				      train_set,input,
-				      nearest_neighbors_indices_row,
-				      i);
+                train_set->getExample(i,input,target,weight);
+                nearest_neighbors_indices_row = nearest_neighbors_indices(i);
+                computeNearestNeighbors(
+                    train_set,input,
+                    nearest_neighbors_indices_row,
+                    i);
 	    }
       
         train_costs.fill(MISSING_VALUE);
 
         if( report_progress )
-	  pb = new ProgressBar( "Training ManifoldParzen",
-				train_set->length() );
+            pb = new ProgressBar( "Training ManifoldParzen",
+                                  train_set->length() );
 
 	eigenvectors.resize( train_set->length() );
 	eigenvalues.resize( train_set->length(), ncomponents );
@@ -293,54 +293,54 @@
 	mus.resize( train_set->length(), inputsize() );
 	mus.clear();
         for( sample = 0; sample < train_set->length() ; sample++ )
-	  { 
+        { 
             train_set->getExample( sample, input, target, weight );
 	    
             // Find nearest neighbors
             if( nclasses > 1 )
-	      for( int k=0; k<nneighbors; k++ )
+                for( int k=0; k<nneighbors; k++ )
                 {
-		  class_datasets[(int)round(target[0])]->getExample(
-								    nearest_neighbors_indices(sample,k),
-								    nearest_neighbor, target2, weight2);
+                    class_datasets[(int)round(target[0])]->getExample(
+                        nearest_neighbors_indices(sample,k),
+                        nearest_neighbor, target2, weight2);
 		  
-		  if(round(target[0]) != round(target2[0]))
-		    PLERROR("ManifoldParzen::train(): similar"
-			    " example is not from same class!");
-		  nearest_neighbors(k) << nearest_neighbor;
+                    if(round(target[0]) != round(target2[0]))
+                        PLERROR("ManifoldParzen::train(): similar"
+                                " example is not from same class!");
+                    nearest_neighbors(k) << nearest_neighbor;
                 }
             else
-	      for( int k=0; k<nneighbors; k++ )
+                for( int k=0; k<nneighbors; k++ )
                 {
-		  train_set->getExample(
-					nearest_neighbors_indices(sample,k),
-					nearest_neighbor, target2, weight2);
-		  nearest_neighbors(k) << nearest_neighbor;
+                    train_set->getExample(
+                        nearest_neighbors_indices(sample,k),
+                        nearest_neighbor, target2, weight2);
+                    nearest_neighbors(k) << nearest_neighbor;
                 }
 	    
 	    if( learn_mu )
-	      {
+            {
 		mu.resize(inputsize());
 		columnMean( nearest_neighbors, mu );
 		mus(sample) << mu;
 		mus(sample) -= input;
-	      }
+            }
 	    substractFromRows(nearest_neighbors, input, false); // Boolean is somehow unused???
 	    lapackSVD(nearest_neighbors, Ut, S, V,'A',1.5);
 	    eigenvectors[sample].resize(ncomponents,inputsize());
 	    for (int k=0;k<ncomponents;k++)
-	      {
-		eigenvalues(sample,k) = mypow(S[k],2);
+            {
+		eigenvalues(sample,k) = mypow(S[k],2)/nneighbors;
 		eigenvectors[sample](k) << Ut(k);
-	      }
+            }
 	    sigma_noises[sample] = 0; // HUGO: Seems stupid for now, but I keep 
 	                              //       this variable in case I want to use
 	                              //       the last eigen value or something...
 
             if( pb )
-	      pb->update( sample + 1 );
-	  }
-      }
+                pb->update( sample + 1 );
+        }
+    }
     
     train_stats->finalize();
     MODULE_LOG << "  train costs = " << train_stats->getMean() << endl;
@@ -372,23 +372,23 @@
 
     int input_j_index;
     for( int i=0; i<nclasses; i++ )
-      {
+    {
 	for( int j=0; 
 	     j<(nclasses > 1 ? 
 		class_datasets[i]->length() 
 		: train_set->length()); 
 	     j++ )
-	  {
+        {
 	    if( nclasses > 1 )
-	      {
+            {
 		class_datasets[i]->getExample(j,input_j,target,weight);
 		input_j_index = class_datasets[i]->indices[j];
-	      }
+            }
 	    else
-	      {
+            {
 		train_set->getExample(j,input_j,target,weight);
 		input_j_index = j;
-	      }
+            }
 	    
 	    U << eigenvectors[input_j_index];
 	    sm_svd << eigenvalues(input_j_index);
@@ -400,25 +400,25 @@
             
 	    mahal = -0.5*pownorm(diff)/sigma_noise;      
 	    norm_term = - n/2.0 * Log2Pi - 0.5*(n-ncomponents)*
-	      pl_log(sigma_noise);
+                pl_log(sigma_noise);
 	    
 	    for(int k=0; k<ncomponents; k++)
-	      { 
+            { 
 		uk = U(k);
 		dotp = dot(diff,uk);
 		coef = (1.0/(sm_svd[k]+global_lambda0) - 1.0/sigma_noise);
 		mahal -= dotp*dotp*0.5*coef;
 		norm_term -= 0.5*pl_log(sm_svd[k]+global_lambda0);
-	      }
+            }
 	    
 	    if( j==0 )
-	      log_p_x_g_y = norm_term + mahal;
+                log_p_x_g_y = norm_term + mahal;
 	    else
-	      log_p_x_g_y = logadd(norm_term + mahal, log_p_x_g_y);
-	  }
+                log_p_x_g_y = logadd(norm_term + mahal, log_p_x_g_y);
+        }
         
 	test_votes[i] = log_p_x_g_y;
-      }
+    }
 
     if( nclasses > 1 )
         output[0] = argmax(test_votes);
@@ -427,7 +427,7 @@
 }
 
 void ManifoldParzen::computeCostsFromOutputs(const Vec& input, const Vec& output,
-                                           const Vec& target, Vec& costs) const
+                                             const Vec& target, Vec& costs) const
 {
 
     //Assumes that computeOutput has been called
@@ -436,19 +436,19 @@
     costs.fill( MISSING_VALUE );
 
     if( nclasses > 1 )
-      {
+    {
 	int target_class = ((int)round(target[0]));
 	if( ((int)round(output[0])) == target_class )
-	  costs[0] = 0;
+            costs[0] = 0;
 	else
-	  costs[0] = 1;
+            costs[0] = 1;
 	costs[1] = - test_votes[target_class]
-	  +pl_log(class_datasets[target_class]->length()); // Must take into account the 1/n normalization
-      }
+            +pl_log(class_datasets[target_class]->length()); // Must take into account the 1/n normalization
+    }
     else
-      {
+    {
 	costs[1] = - output[0]; // 1/n normalization already accounted for
-      }
+    }
 }
 
 TVec<string> ManifoldParzen::getTestCostNames() const



From tihocan at mail.berlios.de  Fri Jun 20 21:29:46 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 20 Jun 2008 21:29:46 +0200
Subject: [Plearn-commits] r9158 - trunk/plearn_learners/online
Message-ID: <200806201929.m5KJTk3H007608@sheep.berlios.de>

Author: tihocan
Date: 2008-06-20 21:29:46 +0200 (Fri, 20 Jun 2008)
New Revision: 9158

Modified:
   trunk/plearn_learners/online/RBMMatrixConnection.h
Log:
Fixed mistake in comments

Modified: trunk/plearn_learners/online/RBMMatrixConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.h	2008-06-20 19:27:24 UTC (rev 9157)
+++ trunk/plearn_learners/online/RBMMatrixConnection.h	2008-06-20 19:29:46 UTC (rev 9158)
@@ -79,7 +79,7 @@
     string L2_decrease_type;
     int L2_n_updates;
 
-    //! Matrix containing unit-to-unit weights (output_size ? input_size)
+    //! Matrix containing unit-to-unit weights (input_size ? output_size)
     Mat weights;
 
     //! used for Gibbs chain methods only



From tihocan at mail.berlios.de  Fri Jun 20 21:30:13 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 20 Jun 2008 21:30:13 +0200
Subject: [Plearn-commits] r9159 - trunk/plearn_learners/online
Message-ID: <200806201930.m5KJUDXv007770@sheep.berlios.de>

Author: tihocan
Date: 2008-06-20 21:30:12 +0200 (Fri, 20 Jun 2008)
New Revision: 9159

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
Added more ports to experiment with

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2008-06-20 19:29:46 UTC (rev 9158)
+++ trunk/plearn_learners/online/RBMModule.cc	2008-06-20 19:30:12 UTC (rev 9159)
@@ -78,12 +78,7 @@
     "    a column matrix with entries = -log( sum_h P(x|h) w_h ) for each row x in the input\n"
     "    'visible' port. This quantity would be a valid - log P(x) if sum_h w_h = 1, under the\n"
     "    joint model P(x,h) = P(x|h) P(h), with P(h)=w_h.\n"
-    "   - 'median_reldiff_cd_nll': median relative difference between the CD\n"
-    "     update and the true NLL gradient. Here, the CD update is not\n"
-    "     stochastic, but is computed exactly as the truncation of the log-\n"
-    "     likelihood expansion. This port has size 'n_steps_compare': there\n"
-    "     is one value for each step of the CD.\n"
-    "    \n"
+    "\n"
     "An RBM also has other ports that exist only if some options are set.\n"
     "If reconstruction_connection is given, then it has\n"
     "  - 'visible_reconstruction_activations.state' : the deterministic reconstruction of the\n"
@@ -100,6 +95,20 @@
     "    of the visible units, only provided to avoid recomputing them in bpropUpdate.\n"
     "  - 'negative_phase_hidden_expectations.state' : the negative phase hidden units\n"
     "    expected values, only provided to avoid recomputing them in bpropUpdate.\n"
+    "The following ports are filled only in test mode when the option\n"
+    "'compare_true_gradient_with_cd' is true:\n"
+    "   - 'median_reldiff_cd_nll': median relative difference between the CD\n"
+    "     update and the true NLL gradient. Here, the CD update is not\n"
+    "     stochastic, but is computed exactly as the truncation of the log-\n"
+    "     likelihood expansion. This port has size 'n_steps_compare': there\n"
+    "     is one value for each step of the CD.\n"
+    "   - 'mean_diff_cd_nll': mean of the absolute difference between the CD\n"
+    "     and NLL gradient updates.\n"
+    "   - 'agreement_cd_nll': fraction of weights for which the CD and NLL\n"
+    "     gradient updates agree on the sign.\n"
+    "   - 'bound_cd_nll': bound on the difference between the CD and NLL\n"
+    "     gradient updates, as computed in (Bengio & Delalleau, 2008)\n"
+    "    \n"
     "\n"
     "The RBM can be trained by gradient descent (wrt to gradients provided on\n"
     "the 'hidden.state' port or on the 'reconstruction_error.state' port)\n"
@@ -382,6 +391,9 @@
     // in "neg_log_phidden" for the set of h's in "hidden".
     addPortName("neg_log_pvisible_given_phidden");
     addPortName("median_reldiff_cd_nll");
+    addPortName("mean_diff_cd_nll");
+    addPortName("agreement_cd_nll");
+    addPortName("bound_cd_nll");
     if(reconstruction_connection)
     {
         addPortName("visible_reconstruction.state");
@@ -788,8 +800,9 @@
             for (int i = 0; i < all_p_visible.length(); i++)
                 all_p_visible[i] =
                     exp(all_p_visible[i] - log_partition_function);
-            pout << "All P(x): " << all_p_visible << endl;
-            pout << "Sum_x P(x) = " << sum(all_p_visible) << endl;
+            //pout << "All P(x): " << all_p_visible << endl;
+            //pout << "Sum_x P(x) = " << sum(all_p_visible) << endl;
+            PLASSERT( is_equal(sum(all_p_visible), 1) );
         }
     }
     else
@@ -855,6 +868,12 @@
     bool neg_log_pvisible_given_phidden_is_output = neg_log_pvisible_given_phidden && neg_log_pvisible_given_phidden->isEmpty();
     Mat* median_reldiff_cd_nll = ports_value[getPortIndex("median_reldiff_cd_nll")];
     bool median_reldiff_cd_nll_is_output = median_reldiff_cd_nll && median_reldiff_cd_nll->isEmpty();
+    Mat* mean_diff_cd_nll = ports_value[getPortIndex("mean_diff_cd_nll")];
+    bool mean_diff_cd_nll_is_output = mean_diff_cd_nll && mean_diff_cd_nll->isEmpty();
+    Mat* agreement_cd_nll = ports_value[getPortIndex("agreement_cd_nll")];
+    bool agreement_cd_nll_is_output = agreement_cd_nll && agreement_cd_nll->isEmpty();
+    Mat* bound_cd_nll = ports_value[getPortIndex("bound_cd_nll")];
+    bool bound_cd_nll_is_output = bound_cd_nll && bound_cd_nll->isEmpty();
     hidden_bias = ports_value[getPortIndex("hidden_bias")];
     //bool hidden_bias_is_output = hidden_bias && hidden_bias->isEmpty();
     weights = ports_value[getPortIndex("weights")];
@@ -1447,6 +1466,43 @@
         grad_nll.fill(0);
         if (median_reldiff_cd_nll_is_output)
             median_reldiff_cd_nll->resize(visible->length(), n_steps_compare);
+        if (mean_diff_cd_nll_is_output)
+            mean_diff_cd_nll->resize(visible->length(), n_steps_compare);
+        if (agreement_cd_nll_is_output)
+            agreement_cd_nll->resize(visible->length(), n_steps_compare);
+        real bound_coeff = MISSING_VALUE;
+        if (bound_cd_nll_is_output) {
+            bound_cd_nll->resize(visible->length(), n_steps_compare);
+            // Compute main bound coefficient:
+            // (1 - N_x N_h sigm(-alpha)^d_x sigm(-beta)^d_h).
+            PP<RBMMatrixConnection> matrix_conn =
+                (RBMMatrixConnection*) get_pointer(connection);
+            PLCHECK(matrix_conn);
+            // Compute alpha.
+            real alpha = 0;
+            for (int j = 0; j < hidden_layer->size; j++) {
+                real alpha_j = abs(hidden_layer->bias[j]);
+                for (int i = 0; i < visible_layer->size; i++)
+                    alpha_j += abs(matrix_conn->weights(j, i));
+                if (alpha_j > alpha)
+                    alpha = alpha_j;
+            }
+            // Compute beta.
+            real beta = 0;
+            for (int i = 0; i < visible_layer->size; i++) {
+                real beta_i = abs(visible_layer->bias[i]);
+                for (int j = 0; j < hidden_layer->size; j++)
+                    beta_i += abs(matrix_conn->weights(j, i));
+                if (beta_i > beta)
+                    beta = beta_i;
+            }
+            bound_coeff = 1 -
+                (visible_layer->getConfigurationCount() *
+                    ipow(sigmoid(-alpha), visible_layer->size)) *
+                (hidden_layer->getConfigurationCount() *
+                    ipow(sigmoid(-beta), hidden_layer->size));
+            pout << "bound_coeff = " << bound_coeff << endl;
+        }
         for (int i = 0; i < visible->length(); i++) {
             // Compute dF(visible)/dWij.
             PLASSERT_MSG( visible->length() == 1, "The comparison can "
@@ -1505,39 +1561,75 @@
                 Mat diff = grad_nll.copy();
                 diff -= grad_cd;
                 grad_cd += grad_first_term;
-                pout << "Grad_CD_" << t+1 << "=" << endl << grad_cd << endl;
-                pout << "Diff =" << endl << diff << endl;
+                //pout << "Grad_CD_" << t+1 << "=" << endl << grad_cd << endl;
+                //pout << "Diff =" << endl << diff << endl;
                 // Compute average relative difference.
                 Vec all_relative_diffs;
+                Vec all_abs_diffs;
                 for (int p = 0; p < diff.length(); p++)
                     for (int q = 0; q < diff.width(); q++) {
+                        all_abs_diffs.append(abs(diff(p, q)));
                         if (!fast_exact_is_equal(grad_nll(p, q), 0))
                             all_relative_diffs.append(abs(diff(p, q) / grad_nll(p, q)));
                     }
-                pout << "All relative diffs: " << all_relative_diffs << endl;
+                //pout << "All relative diffs: " << all_relative_diffs << endl;
                 (*median_reldiff_cd_nll)(i, t) = median(all_relative_diffs);
+                (*mean_diff_cd_nll)(i, t) = mean(all_abs_diffs);
+                // Compute the fraction of parameters for which both updates
+                // agree.
+                int agree = 0;
+                for (int p = 0; p < grad_cd.length(); p++)
+                    for (int q = 0; q < grad_cd.width(); q++) {
+                        if (grad_cd(p, q) *
+                                (grad_first_term(p, q) + grad_nll(p, q)) >= 0)
+                        {
+                            agree++;
+                        }
+                    }
+                if (agreement_cd_nll_is_output)
+                    (*agreement_cd_nll)(i, t) = agree / real(grad_cd.size());
+                if (bound_cd_nll_is_output)
+                    (*bound_cd_nll)(i, t) =
+                        visible_layer->getConfigurationCount() *
+                        ipow(bound_coeff, t + 1);
+                /*
                 pout << "Median relative difference: "
                     << median(all_relative_diffs) << endl;
                 pout << "Mean relative difference: "
                     << mean(all_relative_diffs) << endl;
+                    */
                 // If it is not the last step, update P(h_t|x).
                 if (t < n_steps_compare - 1)
                     product(p_ht_given_x, all_hidden_cond_prob, p_xt_given_x);
             }
             //pout << "P(x)=" << endl << all_p_visible << endl;
             grad_nll += grad_first_term;
-            pout << "Grad_NLL=" << endl << grad_nll << endl;
-            pout << "Grad first term=" << endl << grad_first_term << endl;
+            //pout << "Grad_NLL=" << endl << grad_nll << endl;
+            //pout << "Grad first term=" << endl << grad_first_term << endl;
         }
     }
 
+    // Fill ports that are skipped during training with missing values.
     if (median_reldiff_cd_nll_is_output && median_reldiff_cd_nll->isEmpty()) {
-        // We still need to compute 'median_reldiff_cd_nll'. This is not done
-        // during training, in order to save time.
         PLASSERT( during_training );
         median_reldiff_cd_nll->resize(visible->length(), n_steps_compare);
         median_reldiff_cd_nll->fill(MISSING_VALUE);
     }
+    if (mean_diff_cd_nll_is_output && mean_diff_cd_nll->isEmpty()) {
+        PLASSERT( during_training );
+        mean_diff_cd_nll->resize(visible->length(), n_steps_compare);
+        mean_diff_cd_nll->fill(MISSING_VALUE);
+    }
+    if (agreement_cd_nll_is_output && agreement_cd_nll->isEmpty()) {
+        PLASSERT( during_training );
+        agreement_cd_nll->resize(visible->length(), n_steps_compare);
+        agreement_cd_nll->fill(MISSING_VALUE);
+    }
+    if (bound_cd_nll_is_output && bound_cd_nll->isEmpty()) {
+        PLASSERT( during_training );
+        bound_cd_nll->resize(visible->length(), n_steps_compare);
+        bound_cd_nll->fill(MISSING_VALUE);
+    }
 
     // UGLY HACK TO DEAL WITH THE PROBLEM THAT XXX.state MAY NOT BE NEEDED
     // BUT IS ALWAYS EXPECTED BECAUSE IT IS A STATE (!@#$%!!!)



From chrish at mail.berlios.de  Fri Jun 20 21:39:00 2008
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Fri, 20 Jun 2008 21:39:00 +0200
Subject: [Plearn-commits] r9160 - trunk/python_modules/plearn/pymake
Message-ID: <200806201939.m5KJd0E3008357@sheep.berlios.de>

Author: chrish
Date: 2008-06-20 21:38:59 +0200 (Fri, 20 Jun 2008)
New Revision: 9160

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
Convert string exceptions to raise instance of Exception class (or a subclass)
instead. String exceptions have been deprecated in python since version 1.5!


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-06-20 19:30:12 UTC (rev 9159)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-06-20 19:38:59 UTC (rev 9160)
@@ -1025,7 +1025,7 @@
                 hostname = wait_for_some_completion(outs, errs, ofiles_to_copy, files_to_check)
                 # This should not happen unless localhost is unable to compile
                 if not list_of_hosts:
-                    raise "Couldn't access ANY of the listed hosts for compilation"
+                    raise Exception("Couldn't access ANY of the listed hosts for compilation")
             else:
                 nice_value = nice_values.get(hostname, default_nice_value)
                 ccfile.launch_compilation(hostname, nice_value)
@@ -1594,7 +1594,7 @@
 
     def __init__(self,filepath):
         if not os.path.exists(filepath):
-            raise "Couldn't find file " + filepath
+            raise IOError("Couldn't find file " + filepath)
         if platform == 'win32':
             self.filepath = filepath
             if self.filepath[1] == ":":
@@ -1760,7 +1760,7 @@
         if not hasattr(self,"ccfiles_to_link"):
             #if not self.hasmain or not self.is_ccfile:
             if (not self.hasmain and not create_dll and not create_so and not create_pyso) or not self.is_ccfile:
-                raise "called get_ccfiles_to_link on a file that is not a .cc file or that does not contain a main()"
+                raise Exception("called get_ccfiles_to_link on a file that is not a .cc file or that does not contain a main()")
             self.ccfiles_to_link = []
             visited_hfiles = []
             self.collect_ccfiles_to_link(self.ccfiles_to_link,visited_hfiles)



From laulysta at mail.berlios.de  Fri Jun 20 21:59:07 2008
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Fri, 20 Jun 2008 21:59:07 +0200
Subject: [Plearn-commits] r9161 - trunk/plearn_learners_experimental
Message-ID: <200806201959.m5KJx7HV011343@sheep.berlios.de>

Author: laulysta
Date: 2008-06-20 21:59:07 +0200 (Fri, 20 Jun 2008)
New Revision: 9161

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
Log:


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-06-20 19:38:59 UTC (rev 9160)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-06-20 19:59:07 UTC (rev 9161)
@@ -457,6 +457,10 @@
 {
     MODULE_LOG << "train() called " << endl;
 
+    // reserve memory for sequences
+    Mat seq(5000,2); // contains the current sequence
+    Mat encoded_seq(5000, 4);
+
     Vec input( inputsize() );
     Vec target( targetsize() );
     real weight = 0; // Unused
@@ -503,26 +507,497 @@
             pb = new ProgressBar( "Recurrent training phase of "+classname(),
                                   end_stage - init_stage );
 
+        // TO DO: check this line
         setLearningRate( recurrent_net_learning_rate );
 
         int ith_sample_in_sequence = 0;
         int inputsize_without_masks = inputsize() 
             - ( use_target_layers_masks ? targetsize() : 0 );
         int sum_target_elements = 0;
+
         while(stage < end_stage)
         {
+            train_costs.clear();
+            train_n_items.clear();
+
+            int nseq = nSequences();
+            for(int i=0; i<nseq; i++)
+            {
+                getSequence(i, seq);
+                if(encoding=="raw_masked_supervised")
+                {
+                    splitMaskedSupervisedSequence(seq);
+                }
+                else
+                {
+                    encodeSequence(seq, encoded_seq);
+                    createSupervisedSequence(encoded_seq);
+                }
+
+                fbpropupdate();
+            }
+
+            if( pb )
+                pb->update( stage + 1 - init_stage);
+            
+            for(int i=0; i<train_costs.length(); i++)
+            {
+                if( !fast_exact_is_equal(target_layers_weights[i],0) )
+                    train_costs[i] /= train_n_items[i];
+                else
+                    train_costs[i] = MISSING_VALUE;
+            }
+
+            if(verbosity>0)
+                cout << "mean costs at stage " << stage << 
+                    " = " << train_costs << endl;
+            stage++;
+            train_stats->update(train_costs);
+        }
+
+        if( pb )
+        {
+            delete pb;
+            pb = 0;
+        }
+    }
+
+    train_stats->finalize();        
+}
+
+// TO DO: penser a gestion des prepended dans ce cas
+// Populates: inputslist, targets_list, masks_list
+void DenoisingRecurrentNet::createSupervisedSequence(Mat encoded_seq)
+{
+    PLERROR("Not implemented yet");
+}
+
+
+
+// TO DO: penser a prepend dans ce cas
+
+// Populates: input_list, targets_list, masks_list
+void DenoisingRecurrentNet::splitMaskedSupervisedSequence(Mat seq)
+{
+    int inputsize_without_masks = inputsize()-targetsize();
+    Mat input_part = seq.subMatColumns(0,inputsize_without_masks);
+    Mat mask_part = seq.subMatColumns(inputsize_without_masks, targetsize());
+    Mat target_part = seq.subMatColumns(inputsize_without_masks+targetsize(), targetsize());
+
+    int l = input_part.length();
+    input_list.resize(l);
+    for(int i=0; i<l; i++)
+        input_list[i] = input_part(i);
+
+    int ntargets = target_layers.length();
+    targets_list.resize(ntagets);
+    masks_list.resize(ntargets);
+    int startcol = 0; // starting column of next target in target_part and mask_part
+    for(int k=0; k<ntargets; k++)
+    {
+        int targsize = target_layers[k]->size;
+        targets_list[k] = target_part.subMatColumns(startcol, targsize);
+        masks_list[k] = mask_part.subMatColumns(startcol, targsize);
+        startcol += targsize;
+    }
+}
+
+
+void DenoisingRecurrentNet::fbpropupdate()
+{
+    resize_lists();
+    fprop();
+    recurrent_update();
+}
+
+
+void DenoisingRecurrentNet::resize_lists()
+{
+    int l = input_list.length();
+
+    hidden_list.resize(l, hidden_layer->size);
+    hidden_act_no_bias_list.resize(l, hidden_layer->size);
+
+    if( hidden_layer2 )
+    {
+        hidden2_list.resize(l, hidden_layer2->size);
+        hidden2_act_no_bias_list.resize(l, hidden_layer2->size);
+    }
+
+    int ntargets = target_layers.length();
+    target_prediction_list.resize( ntargets );
+    target_prediction_act_no_bias_list.resize( ntargets );
+
+    for( int tar=0; tar < ntargets; tar++ )
+    {
+        int targsize = target_layers[k]->size;
+        target_prediction_list[tar].resize(l, targsize);
+        target_prediction_act_no_bias_list[tar].resize(l, targsize);
+    }
+
+    nll_list.resize(l,ntargets);
+}
+
+// TODO: think properly about prepended stuff
+
+void DenoisingRecurrentNet::fprop()
+{
+    int l = input_list.length();
+    int ntargets = target_layers.length();
+
+    for(int i=0; i<input_list.length(); i++ )
+    {
+        Vec hidden_act_no_bias_i = hidden_act_no_bias_list(i);
+        input_connections->fprop( input_list[i], hidden_act_no_bias_i);
+
+        if( i > 0 && dynamic_connections )
+        {
+            Vec hidden_i_prev = hidden_list(i-1);
+            dynamic_connections->fprop(hidden_i_prev,dynamic_act_no_bias_contribution );
+            hidden_act_no_bias_i += dynamic_act_no_bias_contribution;
+        }
+        
+        Vec hidden_i = hidden_list(i);
+        hidden_layer->fprop( hidden_act_no_bias_i, 
+                             hidden_i);
+        
+        Vec last_hidden = hidden_i;
+                 
+        if( hidden_layer2 )
+        {
+            Vec hidden2_i = hidden2_list(i); 
+            Vec hidden2_act_no_bias_i = hidden2_act_no_bias_list(i);
+
+            hidden_connections->fprop(hidden_i, hidden2_act_no_bias_i);            
+            hidden_layer2->fprop(hidden2_act_no_bias_i, hidden2_i);
+
+            last_hidden = hidden2_i; // last hidden layer vec 
+        }
+
+        for( int tar=0; tar < ntargets; tar++ )
+        {
+            if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+            {
+                Vec target_prediction_i = target_prediction_list[tar](i);
+                Vec target_prediction_act_no_bias_i = target_prediction_act_no_bias_i;
+                target_connections[tar]->fprop(last_hidden, target_prediction_act_no_bias_list_i);
+                target_layers[tar]->fprop(target_prediction_act_no_bias_i, target_prediction_list_i);
+                if( use_target_layers_masks )
+                    target_prediction_i *= masks_list[tar](i);
+            }
+        }
+
+        sum_target_elements = 0;
+        for( int tar=0; tar < ntargets; tar++ )
+        {
+            if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+            {
+                target_layers[tar]->activation << target_prediction_act_no_bias_list[tar](i);
+                target_layers[tar]->activation += target_layers[tar]->bias;
+                target_layers[tar]->setExpectation(target_prediction_list[tar](i));
+                Vec target_vec = targets_list[tar](i);
+                nll_list(i,tar) = target_layers[tar]->fpropNLL(target_vec); 
+                train_costs[tar] += nll_list(i,tar);
+                        
+                // Normalize by the number of things to predict
+                if( use_target_layers_masks )
+                {
+                    train_n_items[tar] += sum(
+                        input.subVec( inputsize_without_masks 
+                                      + sum_target_elements, 
+                                      target_layers_n_of_target_elements[tar]) );
+                }
+                else
+                    train_n_items[tar]++;
+            }
+            if( use_target_layers_masks )
+                sum_target_elements += target_layers_n_of_target_elements[tar];
+                    
+        }
+    }
+}
+
 /*
-                TMat<real> U,V;//////////crap James
-                TVec<real> S;
-                U.resize(hidden_layer->size,hidden_layer->size);
-                V.resize(hidden_layer->size,hidden_layer->size);
-                S.resize(hidden_layer->size);
-                U << dynamic_connections->weights;
+input_list
+targets_list
+masks_list
+hidden_list
+hidden_act_no_bias_list
+hidden2_list
+hidden2_act_no_bias_list
+target_prediction_list
+target_prediction_act_no_bias_list
+nll_list
+*/
+
+void DenoisingRecurrentNet::recurrent_update()
+{
+    hidden_temporal_gradient.resize(hidden_layer->size);
+    hidden_temporal_gradient.clear();
+    for(int i=hidden_list.length()-1; i>=0; i--){   
+
+        if( hidden_layer2 )
+            hidden_gradient.resize(hidden_layer2->size);
+        else
+            hidden_gradient.resize(hidden_layer->size);
+        hidden_gradient.clear();
+        if(use_target_layers_masks)
+        {
+            for( int tar=0; tar<target_layers.length(); tar++)
+            {
+                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                {
+                    target_layers[tar]->activation << target_prediction_act_no_bias_list[tar](i);
+                    target_layers[tar]->activation += target_layers[tar]->bias;
+                    target_layers[tar]->setExpectation(target_prediction_list[tar](i));
+                    target_layers[tar]->bpropNLL(targets_list[tar](i),nll_list(i,tar),bias_gradient);
+                    bias_gradient *= target_layers_weights[tar];
+                    bias_gradient *= masks_list[tar](i);
+                    target_layers[tar]->update(bias_gradient);
+                    if( hidden_layer2 )
+                        target_connections[tar]->bpropUpdate(hidden2_list(i),target_prediction_act_no_bias_list[tar](i),
+                                                             hidden_gradient, bias_gradient,true);
+                    else
+                        target_connections[tar]->bpropUpdate(hidden_list(i),target_prediction_act_no_bias_list[tar](i),
+                                                             hidden_gradient, bias_gradient,true);
+                }
+            }
+        }
+        else
+        {
+            for( int tar=0; tar<target_layers.length(); tar++)
+            {
+                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                {
+                    target_layers[tar]->activation << target_prediction_act_no_bias_list[tar](i);
+                    target_layers[tar]->activation += target_layers[tar]->bias;
+                    target_layers[tar]->setExpectation(target_prediction_list[tar](i));
+                    target_layers[tar]->bpropNLL(targets_list[tar](i),nll_list(i,tar),bias_gradient);
+                    bias_gradient *= target_layers_weights[tar];
+                    target_layers[tar]->update(bias_gradient);
+                    if( hidden_layer2 )
+                        target_connections[tar]->bpropUpdate(hidden2_list(i),target_prediction_act_no_bias_list[tar](i),
+                                                             hidden_gradient, bias_gradient,true); 
+                    else
+                        target_connections[tar]->bpropUpdate(hidden_list(i),target_prediction_act_no_bias_list[tar](i),
+                                                             hidden_gradient, bias_gradient,true); 
+                        
+                }
+            }
+        }
+
+        if (hidden_layer2)
+        {
+            hidden_layer2->bpropUpdate(
+                hidden2_act_no_bias_list(i), hidden2_list(i),
+                bias_gradient, hidden_gradient);
                 
-                SVD(U,dynamic_connections->weights,S,V);
-                S.fill(-0.5);
-                productScaleAcc(dynamic_connections->bias,dynamic_connections->weights,S,1,0);
+            hidden_connections->bpropUpdate(
+                hidden_list(i),
+                hidden2_act_no_bias_list(i), 
+                hidden_gradient, bias_gradient);
+        }
+            
+        if(i!=0 && dynamic_connections )
+        {   
+            hidden_gradient += hidden_temporal_gradient;
+                
+            hidden_layer->bpropUpdate(
+                hidden_act_no_bias_list(i), hidden_list(i),
+                hidden_temporal_gradient, hidden_gradient);
+                
+            dynamic_connections->bpropUpdate(
+                hidden_list[i-1],
+                hidden_act_no_bias_list(i), // Here, it should be cond_bias, but doesn't matter
+                hidden_gradient, hidden_temporal_gradient);
+                
+            hidden_temporal_gradient << hidden_gradient;
+                
+            input_connections->bpropUpdate(
+                input_list(i),
+                hidden_act_no_bias_list(i), 
+                visi_bias_gradient, hidden_temporal_gradient);// Here, it should be activations - cond_bias, but doesn't matter
+                
+        }
+        else
+        {
+            hidden_layer->bpropUpdate(
+                hidden_act_no_bias_list(i), hidden_list(i),
+                hidden_temporal_gradient, hidden_gradient); // Not really temporal gradient, but this is the final iteration...
+            input_connections->bpropUpdate(
+                input_list(i),
+                hidden_act_no_bias_list(i), 
+                visi_bias_gradient, hidden_temporal_gradient);// Here, it should be activations - cond_bias, but doesn't matter
+
+        }
+    }
+    
+}
+
+/*
+void DenoisingRecurrentNet::old_recurrent_update()
+{
+    hidden_temporal_gradient.resize(hidden_layer->size);
+    hidden_temporal_gradient.clear();
+    for(int i=hidden_list.length()-1; i>=0; i--){   
+
+        if( hidden_layer2 )
+            hidden_gradient.resize(hidden_layer2->size);
+        else
+            hidden_gradient.resize(hidden_layer->size);
+        hidden_gradient.clear();
+        if(use_target_layers_masks)
+        {
+            for( int tar=0; tar<target_layers.length(); tar++)
+            {
+                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                {
+                    target_layers[tar]->activation << target_prediction_act_no_bias_list[tar][i];
+                    target_layers[tar]->activation += target_layers[tar]->bias;
+                    target_layers[tar]->setExpectation(target_prediction_list[tar][i]);
+                    target_layers[tar]->bpropNLL(targets_list[tar][i],nll_list(i,tar),bias_gradient);
+                    bias_gradient *= target_layers_weights[tar];
+                    bias_gradient *= masks_list[tar][i];
+                    target_layers[tar]->update(bias_gradient);
+                    if( hidden_layer2 )
+                        target_connections[tar]->bpropUpdate(hidden2_list[i],target_prediction_act_no_bias_list[tar][i],
+                                                             hidden_gradient, bias_gradient,true);
+                    else
+                        target_connections[tar]->bpropUpdate(hidden_list[i],target_prediction_act_no_bias_list[tar][i],
+                                                             hidden_gradient, bias_gradient,true);
+                }
+            }
+        }
+        else
+        {
+            for( int tar=0; tar<target_layers.length(); tar++)
+            {
+                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                {
+                    target_layers[tar]->activation << target_prediction_act_no_bias_list[tar][i];
+                    target_layers[tar]->activation += target_layers[tar]->bias;
+                    target_layers[tar]->setExpectation(target_prediction_list[tar][i]);
+                    target_layers[tar]->bpropNLL(targets_list[tar][i],nll_list(i,tar),bias_gradient);
+                    bias_gradient *= target_layers_weights[tar];
+                    target_layers[tar]->update(bias_gradient);
+                    if( hidden_layer2 )
+                        target_connections[tar]->bpropUpdate(hidden2_list[i],target_prediction_act_no_bias_list[tar][i],
+                                                             hidden_gradient, bias_gradient,true); 
+                    else
+                        target_connections[tar]->bpropUpdate(hidden_list[i],target_prediction_act_no_bias_list[tar][i],
+                                                             hidden_gradient, bias_gradient,true); 
+                        
+                }
+            }
+        }
+
+        if (hidden_layer2)
+        {
+            hidden_layer2->bpropUpdate(
+                hidden2_act_no_bias_list[i], hidden2_list[i],
+                bias_gradient, hidden_gradient);
+                
+            hidden_connections->bpropUpdate(
+                hidden_list[i],
+                hidden2_act_no_bias_list[i], 
+                hidden_gradient, bias_gradient);
+        }
+            
+        if(i!=0 && dynamic_connections )
+        {   
+            hidden_gradient += hidden_temporal_gradient;
+                
+            hidden_layer->bpropUpdate(
+                hidden_act_no_bias_list[i], hidden_list[i],
+                hidden_temporal_gradient, hidden_gradient);
+                
+            dynamic_connections->bpropUpdate(
+                hidden_list[i-1],
+                hidden_act_no_bias_list[i], // Here, it should be cond_bias, but doesn't matter
+                hidden_gradient, hidden_temporal_gradient);
+                
+            hidden_temporal_gradient << hidden_gradient;
+                
+            input_connections->bpropUpdate(
+                input_list[i],
+                hidden_act_no_bias_list[i], 
+                visi_bias_gradient, hidden_temporal_gradient);// Here, it should be activations - cond_bias, but doesn't matter
+                
+        }
+        else
+        {
+            hidden_layer->bpropUpdate(
+                hidden_act_no_bias_list[i], hidden_list[i],
+                hidden_temporal_gradient, hidden_gradient); // Not really temporal gradient, but this is the final iteration...
+            input_connections->bpropUpdate(
+                input_list[i],
+                hidden_act_no_bias_list[i], 
+                visi_bias_gradient, hidden_temporal_gradient);// Here, it should be activations - cond_bias, but doesn't matter
+
+        }
+    }
+    
+}
 */
+
+
+/*
+void DenoisingRecurrentNet::oldtrain()
+{
+    MODULE_LOG << "train() called " << endl;
+
+    Vec input( inputsize() );
+    Vec target( targetsize() );
+    real weight = 0; // Unused
+    Vec train_costs( getTrainCostNames().length() );
+    train_costs.clear();
+    Vec train_n_items( getTrainCostNames().length() );
+
+    if( !initTrain() )
+    {
+        MODULE_LOG << "train() aborted" << endl;
+        return;
+    }
+
+    ProgressBar* pb = 0;
+
+    // clear stats of previous epoch
+    train_stats->forget();
+
+
+//    if(rbm_stage < rbm_nstages)
+//    {
+//    }
+
+
+    if( stage >= nstages )
+        return;
+
+    if( stage < nstages )
+    {        
+
+        MODULE_LOG << "Training the whole model" << endl;
+
+        int init_stage = stage;
+        //int end_stage = max(0,nstages-(rbm_nstages + dynamic_nstages));
+        int end_stage = nstages;
+
+        MODULE_LOG << "  stage = " << stage << endl;
+        MODULE_LOG << "  end_stage = " << end_stage << endl;
+        MODULE_LOG << "  recurrent_net_learning_rate = " << recurrent_net_learning_rate << endl;
+
+        if( report_progress && stage < end_stage )
+            pb = new ProgressBar( "Recurrent training phase of "+classname(),
+                                  end_stage - init_stage );
+
+        setLearningRate( recurrent_net_learning_rate );
+
+        int ith_sample_in_sequence = 0;
+        int inputsize_without_masks = inputsize() 
+            - ( use_target_layers_masks ? targetsize() : 0 );
+        int sum_target_elements = 0;
+        while(stage < end_stage)
+        {
             train_costs.clear();
             train_n_items.clear();
             for(int sample=0 ; sample<train_set->length() ; sample++ )
@@ -761,9 +1236,220 @@
 
     train_stats->finalize();
 }
+*/
 
+/* TO DO:
+verifier nombre de temps
+implementer correctmeent duration_to_number_of_timeframes(duration)
+declare nouvelles options et valeurs par defaut correctes
+*/
 
 
+/*
+Format de donnees:
+
+matrice de 2 colonnes:
+note, duree
+
+note: midi_number (21..108 numero de touche sur piano)
+      ou 0  (silence)
+      ou -1 (missing)
+      ou -999 (fin de sequence)
+
+duree: 1 double-croche
+       2 
+..16   exprimee en 1/16 de mesure (resultat du quantize de Stan)
+
+
+ */
+
+void DenoisingRecurrentNet::encodeSequence(Mat sequence, Mat& encoded_sequence)
+{
+    //! Possibilities: "timeframe", "note_duration", "note_octav_duration", "generic"
+    int prepend_zero_rows = input_window_size;
+
+    if(encoding=="timeframe")
+        encode_onehot_timeframe(sequence, encoded_sequence, prepend_zero_rows);
+    else if(encoding=="note_duration")
+        encode_onehot_note_octav_duration(sequence, encoded_sequence, prepend_zero_rows);
+    else if(encoding=="note_octav_duration")
+        encode_onehot_note_octav_duration(sequence, encoded_sequence, prepend_zero_rows, true, 4);    
+    else if(encoding=="raw_masked_supervised")
+        PLERROR("raw_masked_supervised encoding not yet implemented");
+    else if(encoding=="generic")
+        PLERROR("generic encoding not yet implemented");
+    else
+        PLERROR("unsupported encoding: %s",encoding.c_str());
+}
+
+
+void DenoisingRecurrentNet::getSequence(int i, Mat& seq) const
+{ 
+    int start = 0;
+    if(i>0)
+        start = boundaries[i-1]+1;
+    int end = boundaries[i];
+    int w = train_set->width();
+    seq.resize(end-start, w);
+    train_set->getMat(start,0,seq);
+}
+
+
+void DenoisingRecurrentNet::setTrainingSet(VMat training_set, bool call_forget)
+{
+    inherited::setTrainingSet(training_set, call_forget);
+    locateSequenceBoundaries(training_set, boundaries, end_of_sequence_symbol);
+}
+
+
+void DenoisingRecurrentNet::locateSequenceBoundaries(VMat dataset, TVec<int>& boundaries, real end_of_sequence_symbol)
+{
+    boundaries.resize(0);
+    int l = dataset->length();
+    for(int i=0; i<l; i++)
+    {
+        if(dataset(i,0)==end_of_sequence_symbol)
+            boundaries.append(i);
+    }
+}
+
+
+
+
+// encodings
+
+
+/*
+  use note_nbits=13 bits for note + octav_nbits bits for octav + duration_nbits bits for duration
+  bit positions are numbered starting at 0.
+
+  if note is a silence (midi_number==0) then bit at position 12 is on
+  otherwise bit at position midi_number%12 is on
+
+  To compute octav bit position, we first compute the min and max of midi_number/12
+  this gives us the octav_min.
+  Then bit at position note_nbits+(midi_number/12)-octav_min is switched to on.
+
+  bit at position note_nbits+octav_nbits+duration is on
+ */
+
+void DenoisingRecurrentNet::encode_onehot_note_octav_duration(Mat sequence, Mat& encoded_sequence, int prepend_zero_rows,
+                                                              bool use_silence, in octav_nbits, int duration_nbits)
+{
+    int l = sequence.length();
+    encoded_sequence.resize(prepend_zero_rows+l,note_nbits+octav_nbits+duration_nbits);
+    encoded_sequence.clear();
+    int octav_min = 10000;
+    int octav_max = -10000;
+
+    int note_nbits = use_silence ?13 :12;
+
+    if(octav_nbits>0)
+    {
+        for(int i=0; i<l; i++)
+        {
+            int midi_number = int(sequence(i,0));
+            int octav = midi_number/12;
+            if(octav<octav_min)
+                octav_min = octav;
+            if(octav>octav_max)
+                octav_max = octav;
+        }
+        if(octav_max-octav_min > octav_nbits)
+            PLERROR("Octav range too big. Does not fit in octav_nbits");
+    }
+
+    
+    for(int i=0; i<l; i++)
+    {
+        int midi_number = int(sequence(i,0));
+        if(midi_number==0) // silence
+        {
+            if(use_silence)
+                encoded_sequence(prepend_zero_rows+i,12) = 1;
+        }
+        else
+            encoded_sequence(prepend_zero_rows+i,midi_number%12) = 1;
+
+        if(octav_nbits>0)
+        {
+            int octavpos = midi_number/12-octav_min;
+            encoded_sequence(prepend_zero_rows+i,note_nbits+octavpos) = 1;
+        }
+
+        int duration = int(sequence(i,1));
+        if(duration<0 || duration>=duration_nbits)
+            PLERROR("duration out of valid range");
+        encoded_sequence(prepend_zero_rows+i,note_nbits+octav_nbits+duration) = 1;
+    }
+}
+
+
+int DenoisingRecurrentNet::duration_to_number_of_timeframes(int duration)
+{
+    return duration+1;
+}
+
+/*
+  use note_nbits+1 bits for note at every timeframe
+  last bit indicates continuation of the preceeding note.
+ */
+
+void DenoisingRecurrentNet::encode_onehot_timeframe(Mat sequence, Mat& encoded_sequence, 
+                                                    int prepend_zero_rows, bool use_silence)
+{
+    int l = sequence.length();
+    int newl = 0;
+
+    // First compute length of timeframe sequence
+    for(int i=0; i<l; i++)
+    {
+        int duration = int(sequence(i,1));
+        newl += duration_to_number_of_timeframes(duration);
+    }
+
+    int nnotes = use_silence ?13 :12;
+
+    // reserve one extra bit to mean repetition
+    encoded_sequence.resize(prepend_zero_rows+newl, nnotes+1);
+    encoded_sequence.clear();
+
+    int k=prepend_zero_rows;
+    for(int i=0; i<l; i++)
+    {
+        int midi_number = int(sequence(i,0));
+        if(midi_number==0) // silence
+        {
+            if(use_silence)
+                encoded_sequence(k++,12) = 1;
+        }
+        else
+            encoded_sequence(k++,midi_number%12) = 1;
+
+        int duration = int(sequence(i,1));
+        int nframes = duration_to_number_of_timeframes(duration);
+        while(--nframes>0) // setb repetition bit
+            encoded_sequence(k++,nnotes) = 1;            
+    }    
+}
+    
+
+// input noise injection
+void inject_zero_forcing_noise(Mat sequence, double noise_prob)
+{
+    if(!sequence.isCompact())
+        PLEERROR("Expected a compact sequence");
+    real* p = sequence.data();
+    int n = sequence.size();
+    while(n--)
+    {
+        if(*p!=real(0.) && random_gen->uniform_sample()<noise_prob)
+            *p = real(0.);
+        ++p;
+    }
+}
+
+
 void DenoisingRecurrentNet::clamp_units(const Vec layer_vector,
                                              PP<RBMLayer> layer,
                                              TVec<int> symbol_sizes) const
@@ -840,110 +1526,7 @@
     }
 }
 
-void DenoisingRecurrentNet::recurrent_update()
-{
-        hidden_temporal_gradient.resize(hidden_layer->size);
-        hidden_temporal_gradient.clear();
-        for(int i=hidden_list.length()-1; i>=0; i--){   
 
-            if( hidden_layer2 )
-                hidden_gradient.resize(hidden_layer2->size);
-            else
-                hidden_gradient.resize(hidden_layer->size);
-            hidden_gradient.clear();
-            if(use_target_layers_masks)
-            {
-                for( int tar=0; tar<target_layers.length(); tar++)
-                {
-                    if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                    {
-                        target_layers[tar]->activation << target_prediction_act_no_bias_list[tar][i];
-                        target_layers[tar]->activation += target_layers[tar]->bias;
-                        target_layers[tar]->setExpectation(target_prediction_list[tar][i]);
-                        target_layers[tar]->bpropNLL(targets_list[tar][i],nll_list(i,tar),bias_gradient);
-                        bias_gradient *= target_layers_weights[tar];
-                        bias_gradient *= masks_list[tar][i];
-                        target_layers[tar]->update(bias_gradient);
-                        if( hidden_layer2 )
-                            target_connections[tar]->bpropUpdate(hidden2_list[i],target_prediction_act_no_bias_list[tar][i],
-                                                                 hidden_gradient, bias_gradient,true);
-                        else
-                            target_connections[tar]->bpropUpdate(hidden_list[i],target_prediction_act_no_bias_list[tar][i],
-                                                                 hidden_gradient, bias_gradient,true);
-                    }
-                }
-            }
-            else
-            {
-                for( int tar=0; tar<target_layers.length(); tar++)
-                {
-                    if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                    {
-                        target_layers[tar]->activation << target_prediction_act_no_bias_list[tar][i];
-                        target_layers[tar]->activation += target_layers[tar]->bias;
-                        target_layers[tar]->setExpectation(target_prediction_list[tar][i]);
-                        target_layers[tar]->bpropNLL(targets_list[tar][i],nll_list(i,tar),bias_gradient);
-                        bias_gradient *= target_layers_weights[tar];
-                        target_layers[tar]->update(bias_gradient);
-                        if( hidden_layer2 )
-                            target_connections[tar]->bpropUpdate(hidden2_list[i],target_prediction_act_no_bias_list[tar][i],
-                                                                 hidden_gradient, bias_gradient,true); 
-                        else
-                            target_connections[tar]->bpropUpdate(hidden_list[i],target_prediction_act_no_bias_list[tar][i],
-                                                                 hidden_gradient, bias_gradient,true); 
-                        
-                    }
-                }
-            }
-
-            if (hidden_layer2)
-            {
-                hidden_layer2->bpropUpdate(
-                    hidden2_act_no_bias_list[i], hidden2_list[i],
-                    bias_gradient, hidden_gradient);
-                
-                hidden_connections->bpropUpdate(
-                    hidden_list[i],
-                    hidden2_act_no_bias_list[i], 
-                    hidden_gradient, bias_gradient);
-            }
-            
-            if(i!=0 && dynamic_connections )
-            {   
-                hidden_gradient += hidden_temporal_gradient;
-                
-                hidden_layer->bpropUpdate(
-                    hidden_act_no_bias_list[i], hidden_list[i],
-                    hidden_temporal_gradient, hidden_gradient);
-                
-                dynamic_connections->bpropUpdate(
-                    hidden_list[i-1],
-                    hidden_act_no_bias_list[i], // Here, it should be cond_bias, but doesn't matter
-                    hidden_gradient, hidden_temporal_gradient);
-                
-                hidden_temporal_gradient << hidden_gradient;
-                
-                input_connections->bpropUpdate(
-                    input_list[i],
-                    hidden_act_no_bias_list[i], 
-                    visi_bias_gradient, hidden_temporal_gradient);// Here, it should be activations - cond_bias, but doesn't matter
-                
-            }
-            else
-            {
-                hidden_layer->bpropUpdate(
-                    hidden_act_no_bias_list[i], hidden_list[i],
-                    hidden_temporal_gradient, hidden_gradient); // Not really temporal gradient, but this is the final iteration...
-                input_connections->bpropUpdate(
-                    input_list[i],
-                    hidden_act_no_bias_list[i], 
-                    visi_bias_gradient, hidden_temporal_gradient);// Here, it should be activations - cond_bias, but doesn't matter
-
-            }
-        }
-    
-}
-
 void DenoisingRecurrentNet::computeOutput(const Vec& input, Vec& output) const
 {
     PLERROR("DenoisingRecurrentNet::computeOutput(): this is a dynamic, "

Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-06-20 19:38:59 UTC (rev 9160)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-06-20 19:59:07 UTC (rev 9161)
@@ -120,12 +120,61 @@
     //! Number of symbols for each symbolic field of train_set
     TVec< TVec<int> > target_symbol_sizes;
 
+    //! Chooses what type of encoding to apply to an input sequence
+    //! Possibilities: "timeframe", "note_duration", "note_octav_duration", "raw_masked_supervised"
+    string encoding;
     
+    //! Input window size
+    int input_window_size;
+
+    // Phase greedy (unsupervised)
+    double input_noise_prob;
+    double input_reconstruction_lr;
+    double hidden_noise_prob;
+    double hidden_reconstruciton_lr;
+
+    // Phase noisy recurrent (supervised): uses input_noise_prob
+    double noisy_recurrent_lr;
+    double dynamic_gradient_scale_factor;
     
+    // Phase recurrent no noise (supervised fine tuning)
+    double recurrent_lr;
+
+    
     //#####  Not Options  #####################################################
 
 
 public:
+    //#####  Public static Functions  #########################################
+        
+    // Finding sequence end indexes
+    static TVec<int> locateSequenceBoundaries(VMat dataset, real end_of_sequence_symbol);
+
+    // encodings
+
+    static void encode_onehot_note_octav_duration(Mat sequence, Mat& encoded_sequence, int prepend_zero_rows,
+                                                  bool use_silence=true, int octav_nbits=0, int duration_nbits=8);
+    
+    static void encode_onehot_timeframe(Mat sequence, Mat& encoded_sequence, int prepend_zero_rows, 
+                                        bool use_silence=true);
+    
+
+    // input noise injection
+    void inject_zero_forcing_noise(Mat sequence, double noise_prob);
+
+    inline static Vec getInputWindow(Mat sequence, int startpos, int winsize)
+    { return sequence.subMatRows(startpos, winsize).toVec(); }
+          
+    // 
+    inline static void getNoteAndOctave(int midi_number, int& note, int& octave)
+    {
+        note = midi_number%12;
+        octave = midi_number/12;
+    }
+    
+
+
+public:
     //#####  Public Member Functions  #########################################
 
     //! Default constructor
@@ -138,6 +187,8 @@
     //! may depend on its inputsize(), targetsize() and set options).
     virtual int outputsize() const;
 
+    void setTrainingSet(VMat training_set, bool call_forget=true);
+
     //! (Re-)initializes the PLearner in its fresh state (that state may depend
     //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
     //! a fresh learner!).
@@ -162,11 +213,16 @@
     //! thus the test method).
     virtual TVec<std::string> getTestCostNames() const;
 
-    
+    //! Returns the number of sequences in the training_set
+    int nSequences() const
+    { return boundaries.length(); }
 
-//    //! Generate music in a folder
+    //! Returns the ith sequence
+    void getSequence(int i, Mat& seq) const;
+
+    //! Generate music in a folder
     void generate(int t, int n);
-//
+
 //    //! Generate a part of the data in a folder
 //    void gen();
 
@@ -243,32 +299,43 @@
     mutable Vec hidden_temporal_gradient;
         
     //! List of hidden layers values
-    mutable TVec< Vec > hidden_list;
-    mutable TVec< Vec > hidden_act_no_bias_list;
+    // mutable TVec< Vec > hidden_list;
+    mutable Mat hidden_list;
+    // mutable TVec< Vec > hidden_act_no_bias_list;
+    mutable Mat hidden_act_no_bias_list;
 
     //! List of second hidden layers values
-    mutable TVec< Vec > hidden2_list;
-    mutable TVec< Vec > hidden2_act_no_bias_list;
+    // mutable TVec< Vec > hidden2_list;
+    mutable Mat hidden2_list;
+    // mutable TVec< Vec > hidden2_act_no_bias_list;
+    mutable Mat hidden2_act_no_bias_list;
 
     //! List of target prediction values
-    mutable TVec< TVec< Vec > > target_prediction_list;
-    mutable TVec< TVec< Vec > > target_prediction_act_no_bias_list;
+    // mutable TVec< TVec< Vec > > target_prediction_list;
+    mutable TVec<Mat> target_prediction_list;
+    // mutable TVec< TVec< Vec > > target_prediction_act_no_bias_list;
+    mutable TVec<Mat> target_prediction_act_no_bias_list;
 
     //! List of inputs values
     mutable TVec< Vec > input_list;
 
     //! List of inputs values
-    mutable TVec< TVec< Vec > > targets_list;
+    // mutable TVec< TVec< Vec > > targets_list;
+    mutable TVec<Mat> targets_list;
 
     //! List of the nll of the input samples in a sequence
     mutable Mat nll_list;
 
     //! List of all targets' masks
-    mutable TVec< TVec< Vec > > masks_list;
+    // mutable TVec< TVec< Vec > > masks_list;
+    mutable TVec< Mat > masks_list;
 
     //! Contribution of dynamic weights to hidden layer activation
     mutable Vec dynamic_act_no_bias_contribution;
 
+    TVec<int> boundaries;
+
+
 protected:
     //#####  Protected Member Functions  ######################################
 



From saintmlx at mail.berlios.de  Mon Jun 23 20:56:30 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Mon, 23 Jun 2008 20:56:30 +0200
Subject: [Plearn-commits] r9162 - trunk/python_modules/plearn/learners
Message-ID: <200806231856.m5NIuU8e027292@sheep.berlios.de>

Author: saintmlx
Date: 2008-06-23 20:56:29 +0200 (Mon, 23 Jun 2008)
New Revision: 9162

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:
- avoid building a string then passing it to eval...



Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2008-06-20 19:59:07 UTC (rev 9161)
+++ trunk/python_modules/plearn/learners/SVM.py	2008-06-23 18:56:29 UTC (rev 9162)
@@ -997,17 +997,19 @@
             elif pn == 'kernel_type':
                 # libsvm wants upper case for kernel_type
                 param[pn] = param[pn].upper()
-        s= ', '.join( pn+' = '+str(param[pn])
-                      for pn in param )
-        if len(s)>0:s=', '+s
+        #s= ', '.join( pn+' = '+str(param[pn])
+        #              for pn in param )
+        #if len(s)>0:s=', '+s
 
         
         if 'proba' in self.outputs_type:
             # if this function is defined (see 
-            return eval('svm_parameter( svm_type = C_SVC, probability = 1 '+s+')' )
+            #return eval('svm_parameter( svm_type = C_SVC, probability = 1 '+s+')' )
+            return svm_parameter( svm_type = C_SVC, probability = 1, **param )
         else:
             # Note: 'svm_type = C_SVC' stands for classification
-            return eval('svm_parameter( svm_type = C_SVC '+s+')' )
+            #return eval('svm_parameter( svm_type = C_SVC '+s+')' )
+            return svm_parameter( svm_type = C_SVC, **param )
     
 
     """ Write given results with corresponding parameters



From tihocan at mail.berlios.de  Mon Jun 23 21:36:09 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 23 Jun 2008 21:36:09 +0200
Subject: [Plearn-commits] r9163 - trunk/plearn_learners/online
Message-ID: <200806231936.m5NJa9om032207@sheep.berlios.de>

Author: tihocan
Date: 2008-06-23 21:36:09 +0200 (Mon, 23 Jun 2008)
New Revision: 9163

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
Added another experimental port: weights_stats

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2008-06-23 18:56:29 UTC (rev 9162)
+++ trunk/plearn_learners/online/RBMModule.cc	2008-06-23 19:36:09 UTC (rev 9163)
@@ -108,6 +108,11 @@
     "     gradient updates agree on the sign.\n"
     "   - 'bound_cd_nll': bound on the difference between the CD and NLL\n"
     "     gradient updates, as computed in (Bengio & Delalleau, 2008)\n"
+    "   - 'weights_stats': first element is the median of the absolute value\n"
+    "     of all weights and biases, second element is the mean, third\n"
+    "     element is the maximum sum of weights and biases (in absolute\n"
+    "     values) over columns of the weight matrix, and third element is\n"
+    "     the same over rows.\n"
     "    \n"
     "\n"
     "The RBM can be trained by gradient descent (wrt to gradients provided on\n"
@@ -394,6 +399,7 @@
     addPortName("mean_diff_cd_nll");
     addPortName("agreement_cd_nll");
     addPortName("bound_cd_nll");
+    addPortName("weights_stats");
     if(reconstruction_connection)
     {
         addPortName("visible_reconstruction.state");
@@ -874,6 +880,8 @@
     bool agreement_cd_nll_is_output = agreement_cd_nll && agreement_cd_nll->isEmpty();
     Mat* bound_cd_nll = ports_value[getPortIndex("bound_cd_nll")];
     bool bound_cd_nll_is_output = bound_cd_nll && bound_cd_nll->isEmpty();
+    Mat* weights_stats = ports_value[getPortIndex("weights_stats")];
+    bool weights_stats_is_output = weights_stats && weights_stats->isEmpty();
     hidden_bias = ports_value[getPortIndex("hidden_bias")];
     //bool hidden_bias_is_output = hidden_bias && hidden_bias->isEmpty();
     weights = ports_value[getPortIndex("weights")];
@@ -1471,19 +1479,27 @@
         if (agreement_cd_nll_is_output)
             agreement_cd_nll->resize(visible->length(), n_steps_compare);
         real bound_coeff = MISSING_VALUE;
-        if (bound_cd_nll_is_output) {
-            bound_cd_nll->resize(visible->length(), n_steps_compare);
+        if (bound_cd_nll_is_output || weights_stats_is_output) {
+            if (bound_cd_nll_is_output)
+                bound_cd_nll->resize(visible->length(), n_steps_compare);
+            if (weights_stats_is_output)
+                weights_stats->resize(visible->length(), 4);
             // Compute main bound coefficient:
             // (1 - N_x N_h sigm(-alpha)^d_x sigm(-beta)^d_h).
             PP<RBMMatrixConnection> matrix_conn =
                 (RBMMatrixConnection*) get_pointer(connection);
             PLCHECK(matrix_conn);
+            Vec all_abs_weights_and_biases;
             // Compute alpha.
             real alpha = 0;
             for (int j = 0; j < hidden_layer->size; j++) {
                 real alpha_j = abs(hidden_layer->bias[j]);
-                for (int i = 0; i < visible_layer->size; i++)
-                    alpha_j += abs(matrix_conn->weights(j, i));
+                all_abs_weights_and_biases.append(alpha_j);
+                for (int i = 0; i < visible_layer->size; i++) {
+                    real abs_w_ij = abs(matrix_conn->weights(j, i));
+                    alpha_j += abs_w_ij;
+                    all_abs_weights_and_biases.append(abs_w_ij);
+                }
                 if (alpha_j > alpha)
                     alpha = alpha_j;
             }
@@ -1491,6 +1507,7 @@
             real beta = 0;
             for (int i = 0; i < visible_layer->size; i++) {
                 real beta_i = abs(visible_layer->bias[i]);
+                all_abs_weights_and_biases.append(beta_i);
                 for (int j = 0; j < hidden_layer->size; j++)
                     beta_i += abs(matrix_conn->weights(j, i));
                 if (beta_i > beta)
@@ -1501,7 +1518,17 @@
                     ipow(sigmoid(-alpha), visible_layer->size)) *
                 (hidden_layer->getConfigurationCount() *
                     ipow(sigmoid(-beta), hidden_layer->size));
-            pout << "bound_coeff = " << bound_coeff << endl;
+            //pout << "bound_coeff = " << bound_coeff << endl;
+            if (weights_stats_is_output) {
+                real med_weight = median(all_abs_weights_and_biases);
+                real mean_weight = mean(all_abs_weights_and_biases);
+                for (int i = 0; i < visible->length(); i++) {
+                    (*weights_stats)(i, 0) = med_weight;
+                    (*weights_stats)(i, 1) = mean_weight;
+                    (*weights_stats)(i, 2) = alpha;
+                    (*weights_stats)(i, 3) = beta;
+                }
+            }
         }
         for (int i = 0; i < visible->length(); i++) {
             // Compute dF(visible)/dWij.
@@ -1630,6 +1657,11 @@
         bound_cd_nll->resize(visible->length(), n_steps_compare);
         bound_cd_nll->fill(MISSING_VALUE);
     }
+    if (weights_stats_is_output && weights_stats->isEmpty()) {
+        PLASSERT( during_training );
+        weights_stats->resize(visible->length(), 4);
+        weights_stats->fill(MISSING_VALUE);
+    }
 
     // UGLY HACK TO DEAL WITH THE PROBLEM THAT XXX.state MAY NOT BE NEEDED
     // BUT IS ALWAYS EXPECTED BECAUSE IT IS A STATE (!@#$%!!!)



From lamblin at mail.berlios.de  Wed Jun 25 16:49:52 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 25 Jun 2008 16:49:52 +0200
Subject: [Plearn-commits] r9164 - trunk/plearn_learners/online
Message-ID: <200806251449.m5PEnqGp024637@sheep.berlios.de>

Author: lamblin
Date: 2008-06-25 16:49:51 +0200 (Wed, 25 Jun 2008)
New Revision: 9164

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
Log:
Fix bug when reloading from a previously saved DBN.


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2008-06-23 19:36:09 UTC (rev 9163)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2008-06-25 14:49:51 UTC (rev 9164)
@@ -861,20 +861,19 @@
         ? train_stats_window
         : train_set->length();
 
-    if (stage == 0) {
-        // Training set-dependent initialization.
-        minibatch_size = batch_size > 0 ? batch_size : train_set->length();
-        for (int i = 0 ; i < n_layers; i++) {
-            activations_gradients[i].resize(minibatch_size, layers[i]->size);
-            expectations_gradients[i].resize(minibatch_size, layers[i]->size);
+    // Training set-dependent initialization.
+    minibatch_size = batch_size > 0 ? batch_size : train_set->length();
+    for (int i = 0 ; i < n_layers; i++)
+    {
+        activations_gradients[i].resize(minibatch_size, layers[i]->size);
+        expectations_gradients[i].resize(minibatch_size, layers[i]->size);
 
-            if (background_gibbs_update_ratio>0 && i<n_layers-1)
-                gibbs_down_state[i].resize(minibatch_size, layers[i]->size);
-        }
-        if (final_cost)
-            final_cost_gradients.resize(minibatch_size, final_cost->input_size);
-        optimized_costs.resize(minibatch_size);
+        if (background_gibbs_update_ratio>0 && i<n_layers-1)
+            gibbs_down_state[i].resize(minibatch_size, layers[i]->size);
     }
+    if (final_cost)
+        final_cost_gradients.resize(minibatch_size, final_cost->input_size);
+    optimized_costs.resize(minibatch_size);
 
     Vec input( inputsize() );
     Vec target( targetsize() );



From chrish at mail.berlios.de  Wed Jun 25 17:15:35 2008
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Wed, 25 Jun 2008 17:15:35 +0200
Subject: [Plearn-commits] r9165 - trunk
Message-ID: <200806251515.m5PFFZhA027717@sheep.berlios.de>

Author: chrish
Date: 2008-06-25 17:15:35 +0200 (Wed, 25 Jun 2008)
New Revision: 9165

Modified:
   trunk/pymake.config.model
Log:
Add explicit settings for python version used at ApSTAT instead of relying
on -autopython. Set to use python 2.4 for now.


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2008-06-25 14:49:51 UTC (rev 9164)
+++ trunk/pymake.config.model	2008-06-25 15:15:35 UTC (rev 9165)
@@ -284,14 +284,17 @@
 
 pyver = sys.version.split()[0][0:3]
 pyoption = 'python%s' % pyver.replace('.', '')
-#verify/set optionargs for python and numpy
+
+# Verify/set optionargs for python and numpy
 python_choices= [x for x in options_choices if 'autopython' in x][0]
-if [x for x in python_choices if x in optionargs]==[]:
+if [x for x in python_choices if x in optionargs]==[] and not domain_name.endswith('.apstat.com'):
     optionargs.append('autopython')
+
 numpy_choices= [x for x in options_choices if 'numpy' in x][0]
 if [x for x in numpy_choices if x in optionargs]==[]:
     optionargs.append('numpy')
-#auto-detected python version, if needed
+
+# Auto-detected python version, if needed
 if 'autopython' in optionargs:
     optionargs.remove('autopython')
     optionargs += [ pyoption ]
@@ -300,7 +303,15 @@
     # First find which version of python is installed.
     python_includedirs=[]
     numpy_includedirs = []
-    if domain_name.endswith('iro.umontreal.ca'):
+
+    if domain_name.endswith('apstat.com'):
+        python_version = '2.4'
+        optionargs += [ 'python%s' % python_version.replace('.', '') ]
+        python_lib_root = '/usr/lib'
+        numpy_site_packages = '-L/usr/lib/python' + python_version + '/site-packages/numarray'
+        python_includedirs = [ '/usr/include/python' + python_version]
+        numpy_includedirs = python_includedirs
+    elif domain_name.endswith('iro.umontreal.ca'):
         optionargs += [ pyoption ]
         python_version = pyver
         python_lib_root = '/usr/lib'



From chrish at mail.berlios.de  Wed Jun 25 17:41:16 2008
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Wed, 25 Jun 2008 17:41:16 +0200
Subject: [Plearn-commits] r9166 - trunk
Message-ID: <200806251541.m5PFfGkK030676@sheep.berlios.de>

Author: chrish
Date: 2008-06-25 17:41:15 +0200 (Wed, 25 Jun 2008)
New Revision: 9166

Modified:
   trunk/pymake.config.model
Log:
Silence pymake python version mismatch warning for ApSTAT.

Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2008-06-25 15:15:35 UTC (rev 9165)
+++ trunk/pymake.config.model	2008-06-25 15:41:15 UTC (rev 9166)
@@ -365,7 +365,7 @@
         
     python_includedirs= numpy_includedirs
 
-    if python_version != pyver:
+    if python_version != pyver and not domain_name.endswith('apstat.com'):
         print '*** WARNING: python version mismatch:'
         print '\tcompiling for python'+python_version+' using python'+pyver
 



From tihocan at mail.berlios.de  Wed Jun 25 18:46:10 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 25 Jun 2008 18:46:10 +0200
Subject: [Plearn-commits] r9167 - trunk/plearn_learners/online
Message-ID: <200806251646.m5PGkAK3023612@sheep.berlios.de>

Author: tihocan
Date: 2008-06-25 18:46:07 +0200 (Wed, 25 Jun 2008)
New Revision: 9167

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
Added more agreement data

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2008-06-25 15:41:15 UTC (rev 9166)
+++ trunk/plearn_learners/online/RBMModule.cc	2008-06-25 16:46:07 UTC (rev 9167)
@@ -105,7 +105,9 @@
     "   - 'mean_diff_cd_nll': mean of the absolute difference between the CD\n"
     "     and NLL gradient updates.\n"
     "   - 'agreement_cd_nll': fraction of weights for which the CD and NLL\n"
-    "     gradient updates agree on the sign.\n"
+    "     gradient updates agree on the sign, followed by the fraction of\n"
+    "     weights for which the CD update has same sign as the difference\n"
+    "     between the NLL gradient and the CD update.\n"
     "   - 'bound_cd_nll': bound on the difference between the CD and NLL\n"
     "     gradient updates, as computed in (Bengio & Delalleau, 2008)\n"
     "   - 'weights_stats': first element is the median of the absolute value\n"
@@ -1477,7 +1479,7 @@
         if (mean_diff_cd_nll_is_output)
             mean_diff_cd_nll->resize(visible->length(), n_steps_compare);
         if (agreement_cd_nll_is_output)
-            agreement_cd_nll->resize(visible->length(), n_steps_compare);
+            agreement_cd_nll->resize(visible->length(), 2 * n_steps_compare);
         real bound_coeff = MISSING_VALUE;
         if (bound_cd_nll_is_output || weights_stats_is_output) {
             if (bound_cd_nll_is_output)
@@ -1605,16 +1607,22 @@
                 // Compute the fraction of parameters for which both updates
                 // agree.
                 int agree = 0;
+                int agree2 = 0;
                 for (int p = 0; p < grad_cd.length(); p++)
                     for (int q = 0; q < grad_cd.width(); q++) {
                         if (grad_cd(p, q) *
-                                (grad_first_term(p, q) + grad_nll(p, q)) >= 0)
+                                (grad_first_term(p, q) + grad_nll(p, q)) > 0)
                         {
                             agree++;
                         }
+                        if (grad_cd(p, q) * diff(p, q) > 0)
+                            agree2++;
                     }
-                if (agreement_cd_nll_is_output)
+                if (agreement_cd_nll_is_output) {
                     (*agreement_cd_nll)(i, t) = agree / real(grad_cd.size());
+                    (*agreement_cd_nll)(i, t + n_steps_compare) =
+                        agree2 / real(grad_cd.size());
+                }
                 if (bound_cd_nll_is_output)
                     (*bound_cd_nll)(i, t) =
                         visible_layer->getConfigurationCount() *
@@ -1649,7 +1657,7 @@
     }
     if (agreement_cd_nll_is_output && agreement_cd_nll->isEmpty()) {
         PLASSERT( during_training );
-        agreement_cd_nll->resize(visible->length(), n_steps_compare);
+        agreement_cd_nll->resize(visible->length(), 2 * n_steps_compare);
         agreement_cd_nll->fill(MISSING_VALUE);
     }
     if (bound_cd_nll_is_output && bound_cd_nll->isEmpty()) {



From chrish at mail.berlios.de  Wed Jun 25 22:27:15 2008
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Wed, 25 Jun 2008 22:27:15 +0200
Subject: [Plearn-commits] r9168 - trunk
Message-ID: <200806252027.m5PKRFRP012325@sheep.berlios.de>

Author: chrish
Date: 2008-06-25 22:27:15 +0200 (Wed, 25 Jun 2008)
New Revision: 9168

Modified:
   trunk/pymake.config.model
Log:
Disable strict aliasing when compiling with g++ 4. The code isn't ready
for it: too many uninvestigated warnings.


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2008-06-25 16:46:07 UTC (rev 9167)
+++ trunk/pymake.config.model	2008-06-25 20:27:15 UTC (rev 9168)
@@ -534,7 +534,7 @@
               description = 'compiling with g++, with no MPI support',
               compiler = 'g++',
               compileroptions = '-Wno-deprecated '+pedantic_mode+'-Wno-long-long -ftemplate-depth-100 ' \
-                      + gcc_opt_options+ " -Wno-unknown-pragmas",
+                      + gcc_opt_options+ " -fno-strict-aliasing -Wno-unknown-pragmas",
               cpp_definitions = ['USING_MPI=0'],
               linker = 'g++' )
 



From chapados at mail.berlios.de  Wed Jun 25 23:53:43 2008
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Wed, 25 Jun 2008 23:53:43 +0200
Subject: [Plearn-commits] r9169 - trunk/python_modules/plearn/gui_tools
Message-ID: <200806252153.m5PLrhJq020222@sheep.berlios.de>

Author: chapados
Date: 2008-06-25 23:53:43 +0200 (Wed, 25 Jun 2008)
New Revision: 9169

Modified:
   trunk/python_modules/plearn/gui_tools/xp_workbench.py
Log:
Better integration for non-gui mode

Modified: trunk/python_modules/plearn/gui_tools/xp_workbench.py
===================================================================
--- trunk/python_modules/plearn/gui_tools/xp_workbench.py	2008-06-25 20:27:15 UTC (rev 9168)
+++ trunk/python_modules/plearn/gui_tools/xp_workbench.py	2008-06-25 21:53:43 UTC (rev 9169)
@@ -46,7 +46,6 @@
 from enthought.pyface.api          import confirm, YES
 
 from console_logger    import ConsoleLogger
-from mpl_figure_editor import TraitedFigure
 
 
 #####  ExperimentContext  ###################################################
@@ -58,34 +57,74 @@
     ## Complete path to experiment directory
     expdir = Directory
 
-    ## The console logger
-    console_logger = ConsoleLogger
+    ## Whether we are using a GUI interface or not
+    gui = false
 
-    
+    ## Copy of experiment parameters
+    script_params = Instance(HasTraits, ())
 
+    ## Function to call to run experiment
+    expfunc = Function
+
+    ## The console logger (default value is None)
+    console_logger = Instance(ConsoleLogger)
+
     ## All tabs
-    _all_tabs       = List(HasTraits, desc="Set of tabs associated with experiment",
-                           editor=ListEditor(use_notebook=True,
-                                             deletable=False,
-                                             dock_style="tab",
-                                             page_name=".title"))
-    
-    @property
-    def _display_expdir(self):
-        """Shortened version of expdir suitable for display."""
-        return os.path.basename(self.expdir)
+    _all_tabs = List(HasTraits, desc="Set of tabs associated with experiment",
+                     editor=ListEditor(use_notebook=True,
+                                       deletable=False,
+                                       dock_style="tab",
+                                       page_name=".title"))
 
-    def Figure(self, title="Figure"):
+    def run_experiment(self):
+        """Run the current expfunc with current script_params."""
+        self.expfunc(self.script_params, self)
+
+
+    def figure(self, title="Figure"):
         """Return a new Matplotlib figure and add it to the notebook.
 
         Note that you must paint on this figure using the Matplotlib OO
         API, not pylab.
         """
-        f = TraitedFigure()
-        f.title = title
-        self._all_tabs.append(f)
-        return f.figure
+        if self.gui:
+            ## Late import to allow backend to be chosen
+            from mpl_figure_editor import TraitedFigure
+            f = TraitedFigure()
+            f.title = title
+            f.figure.__traited_figure = f
+            return f.figure
+        else:
+            ## Late import to allow backend to be chosen
+            import pylab
+            return pylab.figure()
+
+
+    def show(self, fig):
+        """Take a figure returned by Figure() and show it on screen.
+
+        Depending on whether we are under a GUI or not, the behavior
+        changes.
+        """
+        if self.gui:
+            self._all_tabs.append(fig.__traited_figure)
+        else:
+            ## Late import to allow backend to be chosen
+            import pylab
+            pylab.show()
+
+    @property
+    def _display_expdir(self):
+        """Shortened version of expdir suitable for display."""
+        return os.path.basename(self.expdir)
+
+    def _console_logger_changed(self, old, new):
+        """Keep the logger in the list of tabs."""
+        if old is not None and old in self._all_tabs:
+            self._all_tabs.remove(old)
+        self._all_tabs.append(new)
     
+    
     ## Default view
     traits_view = View(Item('_all_tabs@', show_label=False))
 
@@ -106,16 +145,16 @@
     http://sebulba.wikispaces.com/recipe+thread2 for details of how this is
     done.  However it does not seem to work very well for now...
     """
-    def __init__(self, func, params, context, wkbench):
+    def __init__(self, xp_context, wkbench):
         def work():
             ## Call user-defined work function, and before quitting
             ## notify that we are done
-            func(params, context)
+            xp_context.run_experiment()
             wkbench.curworker = None
 
         super(_WorkerThread,self).__init__(target=work)
         self.setDaemon(True)     # Allow quitting Python even if thread still running
-        self.context = context
+        self.xp_context = xp_context
 
     def _get_my_tid(self):
         """determines this (self's) thread id"""
@@ -182,6 +221,9 @@
     expfunc   = Function(desc="Function to run when experiment is running")
     curworker = Instance(threading.Thread, desc="Worker thread, if any is running")
 
+    ## Whether we are running under a gui
+    gui = false
+
     ## Active traits
     launch = Button("Launch Experiment")
     cancel = Button("Cancel")
@@ -202,14 +244,22 @@
 
     ## Experiment management
     def _launch_fired(self):
+        """Called when the 'launch' button is clicked."""
+        context = self._new_xp_context()
+        context.console_logger = ConsoleLogger()
+        self.curworker = _WorkerThread(context, self)
+        self.curworker.start()
+
+    def _new_xp_context(self):
+        """Initialize an experiment context
+        """
         expdir  = self.expdir_name(self.script_params.expdir_root)
-        logger  = ConsoleLogger()
         context = ExperimentContext(expdir = expdir,
-                                    console_logger = logger,
-                                    _all_tabs = [ logger ])
+                                    gui = self.gui,
+                                    script_params = self.script_params,
+                                    expfunc = self.expfunc)
         self.experiments.append(context)
-        self.curworker = _WorkerThread(self.expfunc, self.script_params, context, self)
-        self.curworker.start()
+        return context
 
     def _cancel_fired(self):
         if self.curworker is not None:
@@ -220,14 +270,14 @@
     def _curworker_changed(self, old, new):
         ## If curworker had an active console logger, disable it
         if old is not None:
-            context = old.context
+            context = old.xp_context
             logger  = context.console_logger
             if logger is not None:
                 logger.desactivate_stdout_err_redirect()
 
         ## And redirect output to the new logger...
         if new is not None:
-            context = new.context
+            context = new.xp_context
             logger = context.console_logger
             if logger is not None:
                 logger.activate_stdouterr_redirect()
@@ -265,13 +315,18 @@
                 gui = False
             else:
                 gui = True
+        self.gui = gui
 
         ## Bind the command-line arguments to the parameters
         self.script_params = self.bind(params, sys.argv)
         self.expfunc = func
 
         ## Run the thing
-        self.configure_traits()
+        if self.gui:
+            self.configure_traits()
+        else:
+            context = self._new_xp_context()
+            context.run_experiment()
 
 
     @staticmethod
@@ -424,10 +479,11 @@
 
         from numpy import sin, cos, linspace, pi
         print "Drawing a figure..."
-        f = context.Figure()
+        f = context.figure()
         axes = f.add_subplot(111)
         t = linspace(0, 2*pi, 200)
         axes.plot(sin(t)*(1+0.5*cos(11*t)), cos(t)*(1+0.5*cos(11*t)))
+        context.show(f)
 
         print "Sleeping during a long computation..."
         sys.stdout.flush()



From tihocan at mail.berlios.de  Thu Jun 26 17:15:58 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 26 Jun 2008 17:15:58 +0200
Subject: [Plearn-commits] r9170 - trunk/plearn_learners/online
Message-ID: <200806261515.m5QFFwr2028656@sheep.berlios.de>

Author: tihocan
Date: 2008-06-26 17:15:57 +0200 (Thu, 26 Jun 2008)
New Revision: 9170

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
Added a couple more ports for my own selfish pleasure

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2008-06-25 21:53:43 UTC (rev 9169)
+++ trunk/plearn_learners/online/RBMModule.cc	2008-06-26 15:15:57 UTC (rev 9170)
@@ -115,6 +115,10 @@
     "     element is the maximum sum of weights and biases (in absolute\n"
     "     values) over columns of the weight matrix, and third element is\n"
     "     the same over rows.\n"
+    "   - 'ratio_cd_leftout': median ratio between the absolute value of the\n"
+    "     CD update and the absolute value of the term left out in CD (i.e.\n"
+    "     the difference between NLL gradient and CD).\n"
+    "   - 'abs_cd': average absolute value of the CD update\n"
     "    \n"
     "\n"
     "The RBM can be trained by gradient descent (wrt to gradients provided on\n"
@@ -402,6 +406,8 @@
     addPortName("agreement_cd_nll");
     addPortName("bound_cd_nll");
     addPortName("weights_stats");
+    addPortName("ratio_cd_leftout");
+    addPortName("abs_cd");
     if(reconstruction_connection)
     {
         addPortName("visible_reconstruction.state");
@@ -810,7 +816,7 @@
                     exp(all_p_visible[i] - log_partition_function);
             //pout << "All P(x): " << all_p_visible << endl;
             //pout << "Sum_x P(x) = " << sum(all_p_visible) << endl;
-            PLASSERT( is_equal(sum(all_p_visible), 1) );
+            PLCHECK( is_equal(sum(all_p_visible), 1) );
         }
     }
     else
@@ -884,6 +890,10 @@
     bool bound_cd_nll_is_output = bound_cd_nll && bound_cd_nll->isEmpty();
     Mat* weights_stats = ports_value[getPortIndex("weights_stats")];
     bool weights_stats_is_output = weights_stats && weights_stats->isEmpty();
+    Mat* ratio_cd_leftout = ports_value[getPortIndex("ratio_cd_leftout")];
+    bool ratio_cd_leftout_is_output = ratio_cd_leftout && ratio_cd_leftout->isEmpty();
+    Mat* abs_cd = ports_value[getPortIndex("abs_cd")];
+    bool abs_cd_is_output = abs_cd && abs_cd->isEmpty();
     hidden_bias = ports_value[getPortIndex("hidden_bias")];
     //bool hidden_bias_is_output = hidden_bias && hidden_bias->isEmpty();
     weights = ports_value[getPortIndex("weights")];
@@ -1486,6 +1496,10 @@
                 bound_cd_nll->resize(visible->length(), n_steps_compare);
             if (weights_stats_is_output)
                 weights_stats->resize(visible->length(), 4);
+            if (ratio_cd_leftout_is_output)
+                ratio_cd_leftout->resize(visible->length(), n_steps_compare);
+            if (abs_cd_is_output)
+                abs_cd->resize(visible->length(), n_steps_compare);
             // Compute main bound coefficient:
             // (1 - N_x N_h sigm(-alpha)^d_x sigm(-beta)^d_h).
             PP<RBMMatrixConnection> matrix_conn =
@@ -1554,7 +1568,7 @@
                 Vec colsum(p_xt_given_x.width());
                 columnSum(p_xt_given_x, colsum);
                 for (int j = 0; j < colsum.length(); j++) {
-                    PLASSERT( is_equal(colsum[j], 1) );
+                    PLCHECK( is_equal(colsum[j], 1) );
                 }
                 //pout << "Sum = " << endl << colsum << endl;
                 int best_idx = argmax(p_xt_given_x.column(0).toVecCopy());
@@ -1595,11 +1609,14 @@
                 // Compute average relative difference.
                 Vec all_relative_diffs;
                 Vec all_abs_diffs;
+                Vec all_ratios;
                 for (int p = 0; p < diff.length(); p++)
                     for (int q = 0; q < diff.width(); q++) {
                         all_abs_diffs.append(abs(diff(p, q)));
                         if (!fast_exact_is_equal(grad_nll(p, q), 0))
                             all_relative_diffs.append(abs(diff(p, q) / grad_nll(p, q)));
+                        if (!fast_exact_is_equal(diff(p, q), 0))
+                            all_ratios.append(abs(grad_cd(p, q) / diff(p, q)));
                     }
                 //pout << "All relative diffs: " << all_relative_diffs << endl;
                 (*median_reldiff_cd_nll)(i, t) = median(all_relative_diffs);
@@ -1608,6 +1625,7 @@
                 // agree.
                 int agree = 0;
                 int agree2 = 0;
+                real mean_abs_updates = 0;
                 for (int p = 0; p < grad_cd.length(); p++)
                     for (int q = 0; q < grad_cd.width(); q++) {
                         if (grad_cd(p, q) *
@@ -1617,7 +1635,9 @@
                         }
                         if (grad_cd(p, q) * diff(p, q) > 0)
                             agree2++;
+                        mean_abs_updates += abs(grad_cd(p, q));
                     }
+                mean_abs_updates /= real(grad_cd.size());
                 if (agreement_cd_nll_is_output) {
                     (*agreement_cd_nll)(i, t) = agree / real(grad_cd.size());
                     (*agreement_cd_nll)(i, t + n_steps_compare) =
@@ -1627,6 +1647,10 @@
                     (*bound_cd_nll)(i, t) =
                         visible_layer->getConfigurationCount() *
                         ipow(bound_coeff, t + 1);
+                if (ratio_cd_leftout_is_output)
+                    (*ratio_cd_leftout)(i, t) = median(all_ratios);
+                if (abs_cd_is_output)
+                    (*abs_cd)(i, t) = mean_abs_updates;
                 /*
                 pout << "Median relative difference: "
                     << median(all_relative_diffs) << endl;
@@ -1670,6 +1694,16 @@
         weights_stats->resize(visible->length(), 4);
         weights_stats->fill(MISSING_VALUE);
     }
+    if (ratio_cd_leftout_is_output && ratio_cd_leftout->isEmpty()) {
+        PLASSERT( during_training );
+        ratio_cd_leftout->resize(visible->length(), n_steps_compare);
+        ratio_cd_leftout->fill(MISSING_VALUE);
+    }
+    if (abs_cd_is_output && abs_cd->isEmpty()) {
+        PLASSERT( during_training );
+        abs_cd->resize(visible->length(), n_steps_compare);
+        abs_cd->fill(MISSING_VALUE);
+    }
 
     // UGLY HACK TO DEAL WITH THE PROBLEM THAT XXX.state MAY NOT BE NEEDED
     // BUT IS ALWAYS EXPECTED BECAUSE IT IS A STATE (!@#$%!!!)



From chapados at mail.berlios.de  Thu Jun 26 17:27:23 2008
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Thu, 26 Jun 2008 17:27:23 +0200
Subject: [Plearn-commits] r9171 - trunk/plearn_learners/hyper
Message-ID: <200806261527.m5QFRNXL029519@sheep.berlios.de>

Author: chapados
Date: 2008-06-26 17:27:22 +0200 (Thu, 26 Jun 2008)
New Revision: 9171

Modified:
   trunk/plearn_learners/hyper/HyperCommand.cc
   trunk/plearn_learners/hyper/HyperCommand.h
   trunk/plearn_learners/hyper/HyperLearner.cc
   trunk/plearn_learners/hyper/HyperOptimize.cc
Log:
Made HyperLearner::forget call HyperComment::forget, same for HyperOptimize with its substrategy, and made functions in HyperCommand remote-callable from Python

Modified: trunk/plearn_learners/hyper/HyperCommand.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperCommand.cc	2008-06-26 15:15:57 UTC (rev 9170)
+++ trunk/plearn_learners/hyper/HyperCommand.cc	2008-06-26 15:27:22 UTC (rev 9171)
@@ -79,6 +79,28 @@
         " The verbosity level. Default to 0.");
 }
 
+void HyperCommand::declareMethods(RemoteMethodMap& rmm)
+{
+    // Insert a backpointer to remote methods; note that this is different from
+    // declareOptions().
+    rmm.inherited(inherited::_getRemoteMethodMap_());
+
+    declareMethod(
+        rmm, "forget", &HyperCommand::forget,
+        (BodyDoc("Resets the command's internal state as if freshly constructed")));
+
+    declareMethod(
+        rmm, "optimize", &HyperCommand::optimize,
+        (BodyDoc("Executes the command, returning the resulting costvec of its optimization\n"
+                 "(or an empty vec if it didn't do any testng)."),
+         RetDoc ("Cost vectors arising from optimization")));
+
+    declareMethod(
+        rmm, "getResultNames", &HyperCommand::getResultNames,
+        (BodyDoc("Returns the names of the results returned by the optimize() method"),
+         RetDoc ("Vector of strings containing the result names")));
+}
+
 void HyperCommand::build_()
 {
     // ### This method should do the real building of the object,

Modified: trunk/plearn_learners/hyper/HyperCommand.h
===================================================================
--- trunk/plearn_learners/hyper/HyperCommand.h	2008-06-26 15:15:57 UTC (rev 9170)
+++ trunk/plearn_learners/hyper/HyperCommand.h	2008-06-26 15:27:22 UTC (rev 9171)
@@ -97,6 +97,9 @@
     //! Declares this class' options
     static void declareOptions(OptionList& ol);
 
+    //! Declare the methods that are remote-callable
+    static void declareMethods(RemoteMethodMap& rmm);
+
 public:
     // simply calls inherited::build() then build_()
     virtual void build();

Modified: trunk/plearn_learners/hyper/HyperLearner.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.cc	2008-06-26 15:15:57 UTC (rev 9170)
+++ trunk/plearn_learners/hyper/HyperLearner.cc	2008-06-26 15:27:22 UTC (rev 9171)
@@ -251,6 +251,10 @@
 {
     learner_->forget();
     stage = 0;
+
+    // Forward the forget to each command of the strategy.
+    for (int i=0, n=strategy.size() ; i<n ; ++i)
+        strategy[i]->forget();
 }
 
 ////////////

Modified: trunk/plearn_learners/hyper/HyperOptimize.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperOptimize.cc	2008-06-26 15:15:57 UTC (rev 9170)
+++ trunk/plearn_learners/hyper/HyperOptimize.cc	2008-06-26 15:27:22 UTC (rev 9171)
@@ -343,6 +343,9 @@
     best_objective = REAL_MAX;
     best_results = Vec();
     best_learner = 0;
+
+    for (int i=0, n=sub_strategy.size() ; i<n ; ++i)
+        sub_strategy[i]->forget();
 }
 
 Vec HyperOptimize::optimize()



From louradou at mail.berlios.de  Thu Jun 26 17:30:38 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Thu, 26 Jun 2008 17:30:38 +0200
Subject: [Plearn-commits] r9172 - trunk/python_modules/plearn/learners
Message-ID: <200806261530.m5QFUcuK029999@sheep.berlios.de>

Author: louradou
Date: 2008-06-26 17:30:38 +0200 (Thu, 26 Jun 2008)
New Revision: 9172

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:
fixed: passing svm kernel type to libsvm



Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2008-06-26 15:27:22 UTC (rev 9171)
+++ trunk/python_modules/plearn/learners/SVM.py	2008-06-26 15:30:38 UTC (rev 9172)
@@ -996,20 +996,12 @@
                 param.pop(pn)
             elif pn == 'kernel_type':
                 # libsvm wants upper case for kernel_type
-                param[pn] = param[pn].upper()
-        #s= ', '.join( pn+' = '+str(param[pn])
-        #              for pn in param )
-        #if len(s)>0:s=', '+s
+                param[pn] = eval(param[pn].upper())
 
-        
         if 'proba' in self.outputs_type:
-            # if this function is defined (see 
-            #return eval('svm_parameter( svm_type = C_SVC, probability = 1 '+s+')' )
-            return svm_parameter( svm_type = C_SVC, probability = 1, **param )
-        else:
-            # Note: 'svm_type = C_SVC' stands for classification
-            #return eval('svm_parameter( svm_type = C_SVC '+s+')' )
-            return svm_parameter( svm_type = C_SVC, **param )
+            param.update({'probability':1})
+        
+        return svm_parameter( svm_type = C_SVC, **param )
     
 
     """ Write given results with corresponding parameters



From laulysta at mail.berlios.de  Thu Jun 26 17:55:03 2008
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Thu, 26 Jun 2008 17:55:03 +0200
Subject: [Plearn-commits] r9173 - trunk/plearn_learners_experimental
Message-ID: <200806261555.m5QFt3a2032199@sheep.berlios.de>

Author: laulysta
Date: 2008-06-26 17:55:02 +0200 (Thu, 26 Jun 2008)
New Revision: 9173

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
Log:
train and test functions fused


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-06-26 15:30:38 UTC (rev 9172)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-06-26 15:55:02 UTC (rev 9173)
@@ -458,8 +458,8 @@
     MODULE_LOG << "train() called " << endl;
 
     // reserve memory for sequences
-    Mat seq(5000,2); // contains the current sequence
-    Mat encoded_seq(5000, 4);
+    seq.resize(5000,2); // contains the current sequence
+    encoded_seq.resize(5000, 4);
 
     Vec input( inputsize() );
     Vec target( targetsize() );
@@ -534,7 +534,9 @@
                     createSupervisedSequence(encoded_seq);
                 }
 
-                fbpropupdate();
+                resize_lists();
+                fprop(train_costs, train_n_items);
+                recurrent_update();
             }
 
             if( pb )
@@ -603,14 +605,6 @@
 }
 
 
-void DenoisingRecurrentNet::fbpropupdate()
-{
-    resize_lists();
-    fprop();
-    recurrent_update();
-}
-
-
 void DenoisingRecurrentNet::resize_lists()
 {
     int l = input_list.length();
@@ -640,7 +634,8 @@
 
 // TODO: think properly about prepended stuff
 
-void DenoisingRecurrentNet::fprop()
+// fprop accumulates costs in costs and n_items in n_items
+void DenoisingRecurrentNet::fprop(Vec train_costs, Vec train_n_items)
 {
     int l = input_list.length();
     int ntargets = target_layers.length();
@@ -1287,8 +1282,8 @@
 { 
     int start = 0;
     if(i>0)
-        start = boundaries[i-1]+1;
-    int end = boundaries[i];
+        start = trainset_boundaries[i-1]+1;
+    int end = trainset_boundaries[i];
     int w = train_set->width();
     seq.resize(end-start, w);
     train_set->getMat(start,0,seq);
@@ -1298,7 +1293,7 @@
 void DenoisingRecurrentNet::setTrainingSet(VMat training_set, bool call_forget)
 {
     inherited::setTrainingSet(training_set, call_forget);
-    locateSequenceBoundaries(training_set, boundaries, end_of_sequence_symbol);
+    locateSequenceBoundaries(training_set, trainset_boundaries, end_of_sequence_symbol);
 }
 
 
@@ -1542,8 +1537,109 @@
             "log-likelihooh costs for a whole VMat");
 }
 
+
+
 void DenoisingRecurrentNet::test(VMat testset, PP<VecStatsCollector> test_stats,
                   VMat testoutputs, VMat testcosts)const
+{
+    int len = testset.length();
+    Vec input;
+    Vec target;
+    real weight;
+
+
+    Vec output(outputsize());
+    output.clear();
+    Vec costs(nTestCosts());
+    costs.clear();
+    Vec n_items(nTestCosts());
+    n_items.clear();
+
+    PP<ProgressBar> pb;
+    if (report_progress) 
+        pb = new ProgressBar("Testing learner", len);
+
+    if (len == 0) {
+        // Empty test set: we give -1 cost arbitrarily.
+        costs.fill(-1);
+        test_stats->update(costs);
+    }
+
+    int w = testset->width();
+    locateSequenceBoundaries(testset, testset_boundaries, end_of_sequence_symbol);
+    int nseq = testset_boundaries.length();
+
+    seq.resize(5000,2); // contains the current sequence
+    seq.resize(5000, 4);
+
+    for(int i=0; i<nseq; i++)
+    {
+        int start = 0;
+        if(i>0)
+            start = testset_boundaries[i-1]+1;
+        int end = testset_boundaries[i];
+        seq.resize(end-start, w);
+        testset->getMat(start,0,seq);
+
+        if(encoding=="raw_masked_supervised")
+        {
+            splitMaskedSupervisedSequence(seq);
+        }
+        else
+        {
+            encodeSequence(seq, encoded_seq);
+            createSupervisedSequence(encoded_seq);
+        }
+
+        resize_lists();
+        fprop(test_costs, test_n_items);
+
+        /*
+        if (testoutputs)
+        {
+            int sum_target_layers_size = 0;
+            for( int tar=0; tar < target_layers.length(); tar++ )
+            {
+                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                {
+                    output.subVec(sum_target_layers_size,target_layers[tar]->size)
+                        << target_prediction_list[tar][ ith_sample_in_sequence ];
+                }
+                sum_target_layers_size += target_layers[tar]->size;
+            }
+            testoutputs->putOrAppendRow(i, output);
+        }
+        */
+
+    }
+
+        if (report_progress)
+            pb->update(i);
+
+    }
+
+    for(int i=0; i<costs.length(); i++)
+    {
+        if( !fast_exact_is_equal(target_layers_weights[i],0) )
+            costs[i] /= n_items[i];
+        else
+            costs[i] = MISSING_VALUE;
+    }
+    if (testcosts)
+        testcosts->putOrAppendRow(0, costs);
+    
+    if (test_stats)
+        test_stats->update(costs, weight);
+
+
+    if( pb )
+        pb->update( stage + 1 - init_stage);
+
+}
+
+/*
+void DenoisingRecurrentNet::oldtest(VMat testset, PP<VecStatsCollector> test_stats,
+                  VMat testoutputs, VMat testcosts)const
 { 
 
     int len = testset.length();
@@ -1828,8 +1924,8 @@
     nll_list.resize(0,0);
     masks_list.resize(0);   
 }
+*/
 
-
 TVec<string> DenoisingRecurrentNet::getTestCostNames() const
 {
     TVec<string> cost_names(0);

Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-06-26 15:30:38 UTC (rev 9172)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-06-26 15:55:02 UTC (rev 9173)
@@ -148,7 +148,7 @@
     //#####  Public static Functions  #########################################
         
     // Finding sequence end indexes
-    static TVec<int> locateSequenceBoundaries(VMat dataset, real end_of_sequence_symbol);
+    static void locateSequenceBoundaries(VMat dataset, TVec<int>& boundaries, real end_of_sequence_symbol);
 
     // encodings
 
@@ -333,9 +333,9 @@
     //! Contribution of dynamic weights to hidden layer activation
     mutable Vec dynamic_act_no_bias_contribution;
 
-    TVec<int> boundaries;
+    TVec<int> trainset_boundaries;
+    TVec<int> testset_boundaries;
 
-
 protected:
     //#####  Protected Member Functions  ######################################
 



From laulysta at mail.berlios.de  Thu Jun 26 17:55:57 2008
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Thu, 26 Jun 2008 17:55:57 +0200
Subject: [Plearn-commits] r9174 - trunk/plearn_learners_experimental
Message-ID: <200806261555.m5QFtvgQ032263@sheep.berlios.de>

Author: laulysta
Date: 2008-06-26 17:55:56 +0200 (Thu, 26 Jun 2008)
New Revision: 9174

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
Log:


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-06-26 15:55:02 UTC (rev 9173)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-06-26 15:55:56 UTC (rev 9174)
@@ -336,6 +336,9 @@
     TVec<int> trainset_boundaries;
     TVec<int> testset_boundaries;
 
+    Mat seq; // contains the current train or test sequence
+    Mat encoded_seq; // contains encoded version of current train or test sequence
+
 protected:
     //#####  Protected Member Functions  ######################################
 



From nouiz at mail.berlios.de  Thu Jun 26 18:51:00 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 26 Jun 2008 18:51:00 +0200
Subject: [Plearn-commits] r9175 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200806261651.m5QGp0ga027624@sheep.berlios.de>

Author: nouiz
Date: 2008-06-26 18:50:52 +0200 (Thu, 26 Jun 2008)
New Revision: 9175

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
can submit to multiple os type on condor


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-06-26 15:55:56 UTC (rev 9174)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-06-26 16:50:52 UTC (rev 9175)
@@ -834,8 +834,11 @@
             req+="&&((Arch == \"INTEL\")||(Arch == \"X86_64\"))"
         else :
             req+="&&(Arch == \"%s\")"%(self.targetcondorplatform)
+
         if self.os:
-            req+='&&(OpSyS == "'+self.os+'")'
+            req=reduce(lambda x,y:x+' || (OpSys == "'+str(y)+'")',
+                       self.os.split(','),
+                       req+'&&(False ')+")"
 
         source_file=os.getenv("CONDOR_LOCAL_SOURCE")
         condor_home = os.getenv('CONDOR_HOME')

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-06-26 15:55:56 UTC (rev 9174)
+++ trunk/scripts/dbidispatch	2008-06-26 16:50:52 UTC (rev 9175)
@@ -56,7 +56,7 @@
 cluster and condor options:
   The '--3264', '--32' or '--64' specify which type of cpu the node must have to execute the commands.
   The '--mem=X' speficify the number of meg the program need to execute.
-  The '--os=X' speficify the os of the server: fc4 or fc7. Default: fc4
+  The '--os=X' speficify the os of the server. Cluster default: fc4. Condor default to the same as the submit host. On condor, --os=FC7,FC9 tell to use FC7 or FC9 hosts.
 
 cluster only options:
   The '--[no_]cwait' is transfered to cluster. This must be enabled if there is not nb_proc available nodes. Otherwise when there are no nodes available, the launch of that command fails.



From tihocan at mail.berlios.de  Thu Jun 26 19:49:35 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 26 Jun 2008 19:49:35 +0200
Subject: [Plearn-commits] r9176 - trunk/plearn_learners/generic
Message-ID: <200806261749.m5QHnZ3r030227@sheep.berlios.de>

Author: tihocan
Date: 2008-06-26 19:49:35 +0200 (Thu, 26 Jun 2008)
New Revision: 9176

Modified:
   trunk/plearn_learners/generic/NNet.cc
   trunk/plearn_learners/generic/NNet.h
Log:
Moved some code in a new virtual method 'getCost' that can be overridden in subclasses

Modified: trunk/plearn_learners/generic/NNet.cc
===================================================================
--- trunk/plearn_learners/generic/NNet.cc	2008-06-26 16:50:52 UTC (rev 9175)
+++ trunk/plearn_learners/generic/NNet.cc	2008-06-26 17:49:35 UTC (rev 9176)
@@ -579,87 +579,8 @@
     costs.resize(ncosts);
 
     for (int k=0; k<ncosts; k++)
-    {
-        // create costfuncs and apply individual weights if weightpart > 1
-        if (cost_funcs[k]=="mse")
-            costs[k]= sumsquare(the_output-the_target);
-        else if (cost_funcs[k]=="mse_onehot")
-            costs[k] = onehot_squared_loss(the_output, the_target);
-        else if (cost_funcs[k]=="NLL") 
-        {
-            if (the_output->width() == 1) {
-                // Assume sigmoid output here!
-                costs[k] = stable_cross_entropy(before_transfer_func, the_target);
-            } else {
-                if (output_transfer_func == "log_softmax")
-                    costs[k] = -the_output[the_target];
-                else
-                    costs[k] = neg_log_pi(the_output, the_target);
-            }
-        } 
-        else if (cost_funcs[k]=="class_error")
-        {
-            if (the_output->width()==1)
-                costs[k] = binary_classification_loss(the_output, the_target);
-            else
-                costs[k] = classification_loss(the_output, the_target);
-        }
-        else if (cost_funcs[k]=="binary_class_error")
-            costs[k] = binary_classification_loss(the_output, the_target);
-        else if (cost_funcs[k]=="multiclass_error")
-            costs[k] = multiclass_loss(the_output, the_target);
-        else if (cost_funcs[k]=="cross_entropy")
-            costs[k] = cross_entropy(the_output, the_target);
-        else if(cost_funcs[k]=="conf_rated_adaboost_cost")
-        {
-            if(output_transfer_func != "sigmoid")
-                PLWARNING("In NNet:buildCosts(): conf_rated_adaboost_cost expects an output in (0,1)");
-            alpha_adaboost = Var(1,1); alpha_adaboost->value[0] = 1.0;
-            params.append(alpha_adaboost);
-            costs[k] = conf_rated_adaboost_cost(the_output, the_target, alpha_adaboost);
-        }
-        else if (cost_funcs[k]=="gradient_adaboost_cost")
-        {
-            if(output_transfer_func != "sigmoid")
-                PLWARNING("In NNet:buildCosts(): gradient_adaboost_cost expects an output in (0,1)");
-            costs[k] = gradient_adaboost_cost(the_output, the_target);
-        }
-        else if (cost_funcs[k]=="stable_cross_entropy") {
-            Var c = stable_cross_entropy(before_transfer_func, the_target);
-            costs[k] = c;
-            PLASSERT( classification_regularizer >= 0 );
-            if (classification_regularizer > 0) {
-                // There is a regularizer to add to the cost function.
-                dynamic_cast<NegCrossEntropySigmoidVariable*>((Variable*) c)->
-                    setRegularizer(classification_regularizer);
-            }
-        }
-        else if (cost_funcs[k]=="margin_perceptron_cost")
-            costs[k] = margin_perceptron_cost(the_output,the_target,margin);
-        else if (cost_funcs[k]=="lift_output")
-            costs[k] = lift_output(the_output, the_target);
-        else if (cost_funcs[k]=="poisson_nll") {
-            VarArray the_varray(the_output, the_target);
-            if (weightsize()>0)
-                the_varray.push_back(sampleweight);
-            costs[k] = neglogpoissonvariable(the_varray);
-        }
-        else if (cost_funcs[k] == "L1")
-            costs[k] = sumabs(the_output - the_target);
-        else {
-            // Assume we got a Variable name and its options                
-            costs[k]= dynamic_cast<Variable*>(newObject(cost_funcs[k]));
-            if(costs[k].isNull())
-                PLERROR("In NNet::build_()  unknown cost_func option: %s",cost_funcs[k].c_str());
-            costs[k]->setParents(the_output & the_target);
-            costs[k]->build();
-        }
+        costs[k] = getCost(cost_funcs[k], the_output, the_target, before_transfer_func);
 
-        // We don't need to take into account the sampleweight, because it is
-        // taken care of in stats->update.
-    }
-
-
     /*
      * weight and bias decay penalty
      */
@@ -1049,6 +970,95 @@
     n_training_bags = -1;
 }
 
+/////////////
+// getCost //
+/////////////
+Var NNet::getCost(const string& costname, const Var& the_output,
+                  const Var& the_target, const Var& before_transfer_func)
+{
+    // We don't need to take into account the sampleweight, because it is
+    // taken care of in stats->update.
+    if (costname=="mse")
+        return sumsquare(the_output-the_target);
+    else if (costname=="mse_onehot")
+        return onehot_squared_loss(the_output, the_target);
+    else if (costname=="NLL") 
+    {
+        if (the_output->width() == 1) {
+            // Assume sigmoid output here!
+            return stable_cross_entropy(before_transfer_func, the_target);
+        } else {
+            if (output_transfer_func == "log_softmax")
+                return -the_output[the_target];
+            else
+                return neg_log_pi(the_output, the_target);
+        }
+    } 
+    else if (costname=="class_error")
+    {
+        if (the_output->width()==1)
+            return binary_classification_loss(the_output, the_target);
+        else
+            return classification_loss(the_output, the_target);
+    }
+    else if (costname=="binary_class_error")
+        return binary_classification_loss(the_output, the_target);
+    else if (costname=="multiclass_error")
+        return multiclass_loss(the_output, the_target);
+    else if (costname=="cross_entropy")
+        return cross_entropy(the_output, the_target);
+    else if(costname=="conf_rated_adaboost_cost")
+    {
+        if(output_transfer_func != "sigmoid")
+            PLWARNING("In NNet:buildCosts(): conf_rated_adaboost_cost expects an output in (0,1)");
+        alpha_adaboost = Var(1,1); alpha_adaboost->value[0] = 1.0;
+        params.append(alpha_adaboost);
+        return conf_rated_adaboost_cost(the_output, the_target, alpha_adaboost);
+    }
+    else if (costname=="gradient_adaboost_cost")
+    {
+        if(output_transfer_func != "sigmoid")
+            PLWARNING("In NNet:buildCosts(): gradient_adaboost_cost expects an output in (0,1)");
+        return gradient_adaboost_cost(the_output, the_target);
+    }
+    else if (costname=="stable_cross_entropy") {
+        Var c = stable_cross_entropy(before_transfer_func, the_target);
+        PLASSERT( classification_regularizer >= 0 );
+        if (classification_regularizer > 0) {
+            // There is a regularizer to add to the cost function.
+            dynamic_cast<NegCrossEntropySigmoidVariable*>((Variable*) c)->
+                setRegularizer(classification_regularizer);
+        }
+        return c;
+    }
+    else if (costname=="margin_perceptron_cost")
+        return margin_perceptron_cost(the_output,the_target,margin);
+    else if (costname=="lift_output")
+        return lift_output(the_output, the_target);
+    else if (costname=="poisson_nll") {
+        VarArray the_varray(the_output, the_target);
+        if (weightsize()>0) {
+            PLERROR("In NNet::getCost - The weight is used, is this really "
+                    "intended? (see comment in code at the top of this "
+                    "method");
+            the_varray.push_back(sampleweight);
+        }
+        return neglogpoissonvariable(the_varray);
+    }
+    else if (costname == "L1")
+        return sumabs(the_output - the_target);
+    else {
+        // Assume we got a Variable name and its options                
+        Var cost = dynamic_cast<Variable*>(newObject(costname));
+        if(cost.isNull())
+            PLERROR("In NNet::build_() - unknown cost name: %s",
+                    costname.c_str());
+        cost->setParents(the_output & the_target);
+        cost->build();
+        return cost;
+    }
+}
+
 ///////////////////////
 // getTrainCostNames //
 ///////////////////////

Modified: trunk/plearn_learners/generic/NNet.h
===================================================================
--- trunk/plearn_learners/generic/NNet.h	2008-06-26 16:50:52 UTC (rev 9175)
+++ trunk/plearn_learners/generic/NNet.h	2008-06-26 17:49:35 UTC (rev 9176)
@@ -267,6 +267,11 @@
     //! Build the costs variable from other variables.
     void buildCosts(const Var& output, const Var& target, const Var& hidden_layer, const Var& before_transfer_func);
 
+    //! Return the cost corresponding to the given cost name. This method is
+    //! virtual so that subclasses can implement their own custom costs.
+    virtual Var getCost(const string& costname, const Var& output,
+                        const Var& target, const Var& before_transfer_func);
+
     //! Build the various functions used in the network.
     void buildFuncs(const Var& the_input, const Var& the_output, const Var& the_target, const Var& the_sampleweight, const Var& the_bag_size);
 



From tihocan at mail.berlios.de  Thu Jun 26 19:50:27 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 26 Jun 2008 19:50:27 +0200
Subject: [Plearn-commits] r9177 - trunk/plearn/var
Message-ID: <200806261750.m5QHoRs6030370@sheep.berlios.de>

Author: tihocan
Date: 2008-06-26 19:50:25 +0200 (Thu, 26 Jun 2008)
New Revision: 9177

Modified:
   trunk/plearn/var/Variable.cc
   trunk/plearn/var/Variable.h
Log:
Added new remote method to be able to set the value of a Variable

Modified: trunk/plearn/var/Variable.cc
===================================================================
--- trunk/plearn/var/Variable.cc	2008-06-26 17:49:35 UTC (rev 9176)
+++ trunk/plearn/var/Variable.cc	2008-06-26 17:50:25 UTC (rev 9177)
@@ -285,6 +285,13 @@
              ArgDoc ("val", "Value to fill with")));
 
     declareMethod(
+            rmm, "setValueSubMat", &Variable::setValueSubMat,
+            (BodyDoc("Replace a sub-matrix of the value with the given data"),
+             ArgDoc ("submat", "Data to set (as a matrix)"),
+             ArgDoc ("istart", "Row where 'submat' is inserted"),
+             ArgDoc ("jstart", "Column where 'submat' is inserted")));
+
+    declareMethod(
             rmm, "fprop", &Variable::fprop,
             (BodyDoc("Update value of this Var")));
 
@@ -1101,11 +1108,17 @@
     matRValue.mod_ = matRValue.width();
 }
 
+//////////////////////
+// makeSharedRValue //
+//////////////////////
 void Variable::makeSharedRValue(Vec& v, int offset_)
 {
     makeSharedRValue(v.storage,v.offset_+offset_);
 }
     
+///////////////////////
+// resizeDiagHessian //
+///////////////////////
 void Variable::resizeDiagHessian()
 {
     matDiagHessian.resize(length(),width());
@@ -1113,6 +1126,9 @@
     diaghessiandata = diaghessian.data();
 }
 
+//////////////////
+// resizeRValue //
+//////////////////
 void Variable::resizeRValue()
 {
     if (!rvaluedata)
@@ -1123,6 +1139,13 @@
     }
 }
 
+////////////////////
+// setValueSubMat //
+////////////////////
+void Variable::setValueSubMat(const Mat& submat, int istart, int jstart)
+{
+    matValue.subMat(istart, jstart, submat.length(), submat.width()) << submat;
+}
 
 
 } // end of namespace PLearn

Modified: trunk/plearn/var/Variable.h
===================================================================
--- trunk/plearn/var/Variable.h	2008-06-26 17:49:35 UTC (rev 9176)
+++ trunk/plearn/var/Variable.h	2008-06-26 17:50:25 UTC (rev 9177)
@@ -277,6 +277,11 @@
 
     void fillGradient(real value) { gradient.fill(value); }
     void fillValue(real val) { value.fill(val); }
+
+    //! Replace with 'submat' the sub-matrix of the value starting at row
+    //! 'istart' and column 'jstart'.
+    void setValueSubMat(const Mat& submat, int istart, int jstart);
+    
     void clearRowsToUpdate()
     {
         rows_to_update.resize(0);



From tihocan at mail.berlios.de  Thu Jun 26 19:51:01 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 26 Jun 2008 19:51:01 +0200
Subject: [Plearn-commits] r9178 - trunk/scripts/Skeletons
Message-ID: <200806261751.m5QHp1Bb030454@sheep.berlios.de>

Author: tihocan
Date: 2008-06-26 19:51:00 +0200 (Thu, 26 Jun 2008)
New Revision: 9178

Modified:
   trunk/scripts/Skeletons/BinaryVariable.cc
   trunk/scripts/Skeletons/BinaryVariable.h
Log:
Improved skeleton to have a proper build mechanism

Modified: trunk/scripts/Skeletons/BinaryVariable.cc
===================================================================
--- trunk/scripts/Skeletons/BinaryVariable.cc	2008-06-26 17:50:25 UTC (rev 9177)
+++ trunk/scripts/Skeletons/BinaryVariable.cc	2008-06-26 17:51:00 UTC (rev 9178)
@@ -19,26 +19,30 @@
     // ### in the parent classes' constructors, something that you must ensure)
 }
 
-// constructor from input variables.
-DERIVEDCLASS::DERIVEDCLASS(Variable* input1, Variable* input2)
+DERIVEDCLASS::DERIVEDCLASS(Variable* input1, Variable* input2,
+                           bool call_build_)
 // ### replace with actual parameters
-//  : inherited(input1, input2, this_variable_length, this_variable_width),
+//  : inherited(input1, input2, this_variable_length, this_variable_width,
+//              call_build_),
 //    parameter(default_value),
 //    ...
 {
-    // ### You may (or not) want to call build_() to finish building the object
+    if (call_build_)
+        build_();
 }
 
 // constructor from input variable and parameters
 // DERIVEDCLASS::DERIVEDCLASS(Variable* input1, Variable* input2,
-//                            param_type the_parameter,...)
+//                            param_type the_parameter, ...,
+//                            bool call_build_)
 // ### replace with actual parameters
-//  : inherited(input1, input2, this_variable_length, this_variable_width),
+//  : inherited(input1, input2, this_variable_length, this_variable_width,
+//              call_build_),
 //    parameter(the_parameter),
 //    ...
 //{
-//    // ### You may (or not) want to call build_() to finish building the
-//    // ### object
+//    if (call_build_)
+//        build_();
 //}
 
 void DERIVEDCLASS::recomputeSize(int& l, int& w) const

Modified: trunk/scripts/Skeletons/BinaryVariable.h
===================================================================
--- trunk/scripts/Skeletons/BinaryVariable.h	2008-06-26 17:50:25 UTC (rev 9177)
+++ trunk/scripts/Skeletons/BinaryVariable.h	2008-06-26 17:51:00 UTC (rev 9178)
@@ -31,18 +31,18 @@
 public:
     //#####  Public Member Functions  #########################################
 
-    //! Default constructor, usually does nothing
+    //! Default constructor.
     DERIVEDCLASS();
 
-    //! Constructor initializing from two input variables
+    //! Constructor initializing from two input variables.
     // ### Make sure the implementation in the .cc calls inherited constructor
     // ### and initializes all fields with reasonable default values.
-    DERIVEDCLASS(Variable* input1, Variable* input2);
+    DERIVEDCLASS(Variable* input1, Variable* input2, bool call_build_ = true);
 
     // ### If your class has parameters, you probably want a constructor that
     // ### initializes them
     // DERIVEDCLASS(Variable* input1, Variable* input2,
-    //              param_type the_parameter, ...);
+    //              param_type the_parameter, ..., bool call_build_ = true);
 
     // Your other public member functions go here
 



From tihocan at mail.berlios.de  Thu Jun 26 19:51:45 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 26 Jun 2008 19:51:45 +0200
Subject: [Plearn-commits] r9179 - trunk/plearn/math
Message-ID: <200806261751.m5QHpj8B030521@sheep.berlios.de>

Author: tihocan
Date: 2008-06-26 19:51:44 +0200 (Thu, 26 Jun 2008)
New Revision: 9179

Modified:
   trunk/plearn/math/TVec_decl.h
Log:
Removed operator= for TVec, use the more explicit method 'fill' instead

Modified: trunk/plearn/math/TVec_decl.h
===================================================================
--- trunk/plearn/math/TVec_decl.h	2008-06-26 17:51:00 UTC (rev 9178)
+++ trunk/plearn/math/TVec_decl.h	2008-06-26 17:51:44 UTC (rev 9179)
@@ -504,10 +504,6 @@
             *it = val;
     }
 
-    //! same as fill(f)
-    inline void operator=(const T& f) const
-    { fill(f); }
-
     inline void clear() const
     { if(isNotEmpty()) clear_n(data(),length()); }
 



From tihocan at mail.berlios.de  Thu Jun 26 19:52:19 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 26 Jun 2008 19:52:19 +0200
Subject: [Plearn-commits] r9180 - trunk/plearn/misc
Message-ID: <200806261752.m5QHqJpX030566@sheep.berlios.de>

Author: tihocan
Date: 2008-06-26 19:52:19 +0200 (Thu, 26 Jun 2008)
New Revision: 9180

Modified:
   trunk/plearn/misc/vmatmain.cc
Log:
When using 'vmat diff', will now notice differences in string mappings

Modified: trunk/plearn/misc/vmatmain.cc
===================================================================
--- trunk/plearn/misc/vmatmain.cc	2008-06-26 17:51:44 UTC (rev 9179)
+++ trunk/plearn/misc/vmatmain.cc	2008-06-26 17:52:19 UTC (rev 9180)
@@ -165,6 +165,12 @@
                     out << "Elements at " << i << ',' << j << " differ by "
                         << v1[j] - v2[j] << endl;
                 ++ndiff;
+            } else if (m1->getValString(j, v1[j]) != m2->getValString(j, v2[j])) {
+                if (verbose)
+                    out << "Elements at " << i << ',' << j << " differ: "
+                        << "'" << m1->getValString(j, v1[j]) << "' != "
+                        << "'" << m2->getValString(j, v2[j]) << "'" << endl;
+                ++ndiff;
             }
         }
     }



From ducharme at mail.berlios.de  Thu Jun 26 19:54:12 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Thu, 26 Jun 2008 19:54:12 +0200
Subject: [Plearn-commits] r9181 - trunk/plearn_learners/regressors
Message-ID: <200806261754.m5QHsCjJ030820@sheep.berlios.de>

Author: ducharme
Date: 2008-06-26 19:54:11 +0200 (Thu, 26 Jun 2008)
New Revision: 9181

Modified:
   trunk/plearn_learners/regressors/BasisSelectionRegressor.cc
   trunk/plearn_learners/regressors/BasisSelectionRegressor.h
Log:
Ajout de quelques options :
    - Termes d'interactions avec les meilleurs candidats seulement,
    - Ordre maximum pour les termes d'interactions,
    - Range qui debutent tous a -infini.

Refactorisation d'une partie du code.
- 


Modified: trunk/plearn_learners/regressors/BasisSelectionRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/BasisSelectionRegressor.cc	2008-06-26 17:52:19 UTC (rev 9180)
+++ trunk/plearn_learners/regressors/BasisSelectionRegressor.cc	2008-06-26 17:54:11 UTC (rev 9181)
@@ -69,11 +69,14 @@
       consider_raw_inputs(true),
       consider_normalized_inputs(false),
       consider_input_range_indicators(false),
+      fixed_min_range(false),
       indicator_desired_prob(0.05),
       indicator_min_prob(0.01),
       n_kernel_centers_to_pick(-1),
       consider_interaction_terms(false),
       max_interaction_terms(-1),
+      consider_n_best_for_interaction(-1),
+      interaction_max_order(-1),
       consider_sorted_encodings(false),
       max_n_vals_for_sorted_encodings(-1),
       normalize_features(false),
@@ -125,7 +128,7 @@
 
     declareOption(ol, "consider_input_range_indicators", &BasisSelectionRegressor::consider_input_range_indicators,
                   OptionBase::buildoption,
-                  "If true, then we'll include in the dictionary indicatr functions\n"
+                  "If true, then we'll include in the dictionary indicator functions\n"
                   "triggered by input ranges and input special values\n"
                   "Special values will include all symbolic values\n"
                   "(detected by the existance of a corresponding string mapping)\n"
@@ -135,9 +138,14 @@
                   "and indicator_min_prob options. The necessary statistics are obtained\n"
                   "from the counts in the StatsCollector of the train_set VMatrix.\n");
 
+    declareOption(ol, "fixed_min_range", &BasisSelectionRegressor::fixed_min_range,
+                  OptionBase::buildoption,
+                  "If true, the min value of all range functions will be set to -FLT_MAX.\n"
+                  "This correspond to a 'thermometer' type of mapping.");
+
     declareOption(ol, "indicator_desired_prob", &BasisSelectionRegressor::indicator_desired_prob,
                   OptionBase::buildoption,
-                  "The algo will try to build input ranges that have at least that probability of occurencein the training set.");
+                  "The algo will try to build input ranges that have at least that probability of occurence in the training set.");
 
     declareOption(ol, "indicator_min_prob", &BasisSelectionRegressor::indicator_min_prob,
                   OptionBase::buildoption,
@@ -170,6 +178,14 @@
                   "Maximum number of interaction terms to consider.  -1 means no max.\n"
                   "If more terms are possible, some are chosen randomly at each stage.\n");
 
+    declareOption(ol, "consider_n_best_for_interaction", &BasisSelectionRegressor::consider_n_best_for_interaction,
+                  OptionBase::buildoption,
+                  "Only the top best functions of single variables are considered when building interaction terms.  -1 means no max.\n");
+
+    declareOption(ol, "interaction_max_order", &BasisSelectionRegressor::interaction_max_order,
+                  OptionBase::buildoption,
+                  "Maximum order of a feature in an interaction function.  -1 means no max.\n");
+
     declareOption(ol, "consider_sorted_encodings", &BasisSelectionRegressor::consider_sorted_encodings,
                   OptionBase::buildoption,
                   "If true, the dictionary will be enriched with encodings sorted in target order.\n"
@@ -210,6 +226,11 @@
                   OptionBase::learntoption,
                   "CURRENTLY UNUSED");
 
+    declareOption(ol, "scores", &BasisSelectionRegressor::scores,
+                  OptionBase::learntoption,
+                  "Matrice of the scores for each candidate function.\n"
+                  "Used only when 'consider_n_best_for_interaction' > 0.");
+
     declareOption(ol, "candidate_functions", &BasisSelectionRegressor::candidate_functions,
                   OptionBase::learntoption,
                   "The list of current candidate functions.");
@@ -259,6 +280,7 @@
     deepCopyField(learner, copies);
     deepCopyField(selected_functions, copies);
     deepCopyField(alphas, copies);
+    deepCopyField(scores, copies);
 
     deepCopyField(simple_candidate_functions, copies);
     deepCopyField(candidate_functions, copies);
@@ -308,110 +330,167 @@
 
 void BasisSelectionRegressor::appendCandidateFunctionsOfSingleField(int fieldnum, TVec<RealFunc>& functions) const
 {
-    string fieldname = train_set->fieldName(fieldnum);
-    StatsCollector& st = train_set->getStats(fieldnum);
-    if(consider_raw_inputs)
+    VMField field_info = train_set->getFieldInfos(fieldnum);
+    string fieldname = field_info.name;
+    VMField::FieldType fieldtype = field_info.fieldtype;
+    StatsCollector& stats_collector = train_set->getStats(fieldnum);
+
+    real n           = stats_collector.n();
+    real nmissing    = stats_collector.nmissing();
+    real nnonmissing = stats_collector.nnonmissing();
+    real min_count     = indicator_min_prob * n;
+    real desired_count = indicator_desired_prob * n;
+
+    // Raw inputs for non-discrete variables
+    if (consider_raw_inputs  &&  (fieldtype != VMField::DiscrGeneral))
     {
         RealFunc f = new RealFunctionOfInputFeature(fieldnum);
         f->setInfo(fieldname);
         functions.append(f);
     }
-    if(consider_normalized_inputs)
+
+    // Normalized inputs for non-discrete variables
+    if (consider_normalized_inputs  &&  (fieldtype != VMField::DiscrGeneral))
     {
-        if(st.nnonmissing()>0)
+        if (nnonmissing > 0)
         {
-            real m = st.mean();
-            real stddev = st.stddev();
-            if(stddev>1e-9)
+            real mean   = stats_collector.mean();
+            real stddev = stats_collector.stddev();
+            if (stddev > 1e-9)
             {
-                RealFunc f = new ShiftAndRescaleFeatureRealFunction(fieldnum, -m, 1./stddev, 0.);
-                f->setInfo(fieldname+"-"+tostring(m)+"/"+tostring(stddev));
+                RealFunc f = new ShiftAndRescaleFeatureRealFunction(fieldnum, -mean, 1./stddev, 0.);
+                string info = fieldname + "-" + tostring(mean) + "/" + tostring(stddev);
+                f->setInfo(info);
                 functions.append(f);
             }
         }
 
     }
-    if(consider_input_range_indicators)
+
+    if (consider_input_range_indicators)
     {
         const map<real,string>& smap = train_set->getRealToStringMapping(fieldnum);
-        map<real,string>::const_iterator sit = smap.begin();
-        map<real,string>::const_iterator smapend = smap.end();
+        map<real,string>::const_iterator smap_it  = smap.begin();
+        map<real,string>::const_iterator smap_end = smap.end();
 
-        while(sit!=smapend)
+        map<real, StatsCollectorCounts>* counts = stats_collector.getApproximateCounts();
+        map<real,StatsCollectorCounts>::const_iterator count_it = counts->begin();
+        map<real,StatsCollectorCounts>::const_iterator count_end = counts->end();
+
+        // Indicator function for mapped variables
+        while (smap_it != smap_end)
         {
-            RealFunc f = new RealValueIndicatorFunction(fieldnum, sit->first);
-            f->setInfo(fieldname+"="+sit->second);
+            RealFunc f = new RealValueIndicatorFunction(fieldnum, smap_it->first);
+            string info = fieldname + "=" + smap_it->second;
+            f->setInfo(info);
             functions.append(f);
-            ++sit;
+            ++smap_it;
         }
 
-        real n = st.n();
-        real min_count = indicator_min_prob*n;
-        real desired_count = indicator_desired_prob*n;
-        if(st.nmissing() >= min_count && n-st.nmissing() >= min_count)
+        // Indicator function for discrete variables not using mapping
+        if (fieldtype == VMField::DiscrGeneral  ||  fieldtype == VMField::DiscrMonotonic)
         {
+            while (count_it != count_end)
+            {
+                real val = count_it->first;
+                // Make sure the variable don't use mapping for this particular value
+                bool mapped_value = false;
+                smap_it = smap.begin();
+                while (smap_it != smap_end)
+                {
+                    if (smap_it->first == val)
+                    {
+                        mapped_value = true;
+                        break;
+                    }
+                    ++smap_it;
+                }
+
+                if (!mapped_value)
+                {
+                    RealFunc f = new RealValueIndicatorFunction(fieldnum, val);
+                    string info = fieldname + "=" + tostring(val);
+                    f->setInfo(info);
+                    functions.append(f);
+                }
+                ++count_it;
+            }
+        }
+
+        // If enough missing values, add an indicator function for it
+        if (nmissing >= min_count && nnonmissing >= min_count)
+        {
             RealFunc f = new RealValueIndicatorFunction(fieldnum, MISSING_VALUE);
-            f->setInfo(fieldname+"=MISSING");
+            string info = fieldname + "=MISSING";
+            f->setInfo(info);
             functions.append(f);
         }
+
+        // For fieldtype DiscrGeneral, it stops here.
+        // A RealRangeIndicatorFunction makes no sense for DiscrGeneral
+        if (fieldtype == VMField::DiscrGeneral) return;
         
-        map<real, StatsCollectorCounts>* counts = st.getApproximateCounts();
-        map<real,StatsCollectorCounts>::const_iterator cit = counts->begin();
-        map<real,StatsCollectorCounts>::const_iterator cend = counts->end();
-        real cumcount = 0;
+        real cum_count = 0;
         real low = -FLT_MAX;
         real val = FLT_MAX;
-        while(cit!=cend)
+        count_it = counts->begin();
+        while (count_it != count_end)
         {
-            val = cit->first;
-            cumcount += cit->second.nbelow;
-            bool in_smap = (smap.find(val)!=smapend);
-            if((cumcount>=desired_count || in_smap&&cumcount>=min_count) &&
-               (n-cumcount>=desired_count || in_smap&&n-cumcount>=min_count))
+            val = count_it->first;
+            cum_count += count_it->second.nbelow;
+            bool in_smap = (smap.find(val) != smap_end);
+            if((cum_count>=desired_count || in_smap&&cum_count>=min_count) && (n-cum_count>=desired_count || in_smap&&n-cum_count>=min_count))
             {
                 RealRange range(']',low,val,'[');
+                if (fixed_min_range) range.low = -FLT_MAX;
                 RealFunc f = new RealRangeIndicatorFunction(fieldnum, range);
-                f->setInfo(fieldname+"__"+tostring(range));
+                string info = fieldname + "__" + tostring(range);
+                f->setInfo(info);
                 functions.append(f);
-                cumcount = 0;
+                cum_count = 0;
                 low = val;
             }
 
-            cumcount += cit->second.n;
-            if(in_smap)
+            cum_count += count_it->second.n;
+            if (in_smap)
             {
-                cumcount = 0;
+                cum_count = 0;
                 low = val;
             }
-            else if(cumcount>=desired_count && n-cumcount>=desired_count)
+            else if (cum_count>=desired_count && n-cum_count>=desired_count)
             {
                 RealRange range(']',low,val,']');
+                if (fixed_min_range) range.low = -FLT_MAX;
                 RealFunc f = new RealRangeIndicatorFunction(fieldnum, range);
-                f->setInfo(fieldname+"__"+tostring(range));
+                string info = fieldname + "__" + tostring(range);
+                f->setInfo(info);
                 functions.append(f);
-                cumcount = 0;
+                cum_count = 0;
                 low = val;
             }            
-            ++cit;
+            ++count_it;
         }
         // last chunk
-        if(cumcount>0)
+        if (cum_count > 0)
         {
-            if(cumcount>=min_count && n-cumcount>=min_count)
+            if (cum_count>=min_count && n-cum_count>=min_count)
             {
                 RealRange range(']',low,val,']');
+                if (fixed_min_range) range.low = -FLT_MAX;
                 RealFunc f = new RealRangeIndicatorFunction(fieldnum, range);
-                f->setInfo(fieldname+"__"+tostring(range));
+                string info = fieldname + "__" + tostring(range);
+                f->setInfo(info);
                 functions.append(f);
             }
-            else if(functions.length()>0) // possibly lump it together with last range
+            else if (functions.length()>0) // possibly lump it together with last range
             {
                 RealRangeIndicatorFunction* f = (RealRangeIndicatorFunction*)(RealFunction*)functions.lastElement();
                 RealRange& range = f->range;
-                if(smap.find(range.high)!=smapend) // last element does not appear to be symbolic
+                if(smap.find(range.high) != smap_end) // last element does not appear to be symbolic
                 {                    
                     range.high = val; // OK, change the last range to include val
-                    f->setInfo(fieldname+"__"+tostring(range));
+                    string info = fieldname + "__" + tostring(range);
+                    f->setInfo(info);
                 }
             }
         }
@@ -424,7 +503,7 @@
     {
         int nc = n_kernel_centers_to_pick;
         kernel_centers.resize(nc, inputsize());
-        real weight;        
+        real weight;
         int l = train_set->length();
         if(random_gen.isNull())
             random_gen = new PRandom();
@@ -467,13 +546,11 @@
 
 void BasisSelectionRegressor::buildAllCandidateFunctions()
 {
-    bool mandatory_fns_just_added= false;
     if(selected_functions.length()==0)
     {
         candidate_functions= mandatory_functions.copy();
         while(candidate_functions.length() > 0)
             appendFunctionToSelection(0);
-        mandatory_fns_just_added= true;
     }
 
     if(simple_candidate_functions.length()==0)
@@ -482,50 +559,97 @@
     candidate_functions = simple_candidate_functions.copy();
     TVec<RealFunc> interaction_candidate_functions;
 
-    int candidate_start = consider_constant_function?1:0; // skip bias
-    int ncandidates = simple_candidate_functions.length();
-    if(consider_interaction_terms)
+    int candidate_start = consider_constant_function ? 1 : 0; // skip bias
+    int ncandidates = candidate_functions.length();
+    int nselected = selected_functions.length();
+    if (nselected > 0  &&  consider_interaction_terms)
     {
-        int nselected = selected_functions.length();
-        for(int k=0; k<nselected; k++)
+        TVec<RealFunc> top_candidate_functions = simple_candidate_functions.copy();
+        int start = candidate_start;
+        if (consider_n_best_for_interaction > 0  &&  ncandidates > consider_n_best_for_interaction)
         {
-            for(int j=candidate_start; j<ncandidates; j++)
+            top_candidate_functions = buildTopCandidateFunctions();
+            start = 0;
+        }
+
+        for (int k=0; k<nselected; k++)
+        {
+            for (int j=start; j<top_candidate_functions.length(); j++)
             {
-                RealFunc f = new RealFunctionProduct(selected_functions[k],simple_candidate_functions[j]);
-                f->setInfo("("+selected_functions[k]->getInfo()+"*"+simple_candidate_functions[j]->getInfo()+")");
-                interaction_candidate_functions.append(f);
+                addInteractionFunction(selected_functions[k], top_candidate_functions[j], interaction_candidate_functions);
             }
         }
     }
 
-    // explicit interaction variables / functions
+    // Build explicit_interaction_functions from explicit_interaction_variables
     explicit_interaction_functions.resize(0);
-    for(int k= 0; k < explicit_interaction_variables.length(); ++k)
-        appendCandidateFunctionsOfSingleField(train_set->getFieldIndex(explicit_interaction_variables[k]), 
-                                              explicit_interaction_functions);
+    for(int k=0; k<explicit_interaction_variables.length(); ++k)
+        appendCandidateFunctionsOfSingleField(train_set->getFieldIndex(explicit_interaction_variables[k]), explicit_interaction_functions);
 
+    // Add interaction_candidate_functions from explicit_interaction_functions
     for(int k= 0; k < explicit_interaction_functions.length(); ++k)
     {
-        for(int j= candidate_start; j < ncandidates; ++j)
+        for(int j=candidate_start; j<ncandidates; ++j)
         {
-            RealFunc f = new RealFunctionProduct(explicit_interaction_functions[k],simple_candidate_functions[j]);
-            f->setInfo("("+explicit_interaction_functions[k]->getInfo()+"*"+simple_candidate_functions[j]->getInfo()+")");
-            interaction_candidate_functions.append(f);
+            addInteractionFunction(explicit_interaction_functions[k], simple_candidate_functions[j], interaction_candidate_functions);
         }
     }
 
-    
-    if(max_interaction_terms < 0)
-        candidate_functions.append(interaction_candidate_functions);
-    else
+    // If too many interaction_candidate_functions, we choose them at random
+    if(max_interaction_terms > 0  &&  interaction_candidate_functions.length() > max_interaction_terms)
     {
         shuffleElements(interaction_candidate_functions);
-        for(int i= 0; i < max_interaction_terms && i < interaction_candidate_functions.length(); ++i)
-            candidate_functions.append(interaction_candidate_functions[i]);
+        interaction_candidate_functions.resize(max_interaction_terms);
     }
+    candidate_functions.append(interaction_candidate_functions);
+}
 
+void BasisSelectionRegressor::addInteractionFunction(RealFunc& f1, RealFunc& f2, TVec<RealFunc>& all_functions)
+{
+    // Check that feature in f2 don't exceed "interaction_max_order" of that feature in f1
+    // Note that f2 should be a new function to be added (and thus an instance of RealFunctionOfInputFeature)
+    if (interaction_max_order > 0)
+    {
+        int order = 0;
+        computeOrder(f1, order);
+        computeOrder(f2, order);
+        if (order > interaction_max_order)
+            return;
+    }
+
+    RealFunc f = new RealFunctionProduct(f1, f2);
+    f->setInfo("(" + f1->getInfo() + "*" + f2->getInfo() + ")");
+    all_functions.append(f);
 }
 
+void BasisSelectionRegressor::computeOrder(RealFunc& func, int& order)
+{
+    if (dynamic_cast<RealFunctionOfInputFeature*>((RealFunction*) func))
+    {
+        ++order;
+    }
+    else if (RealFunctionProduct* f = dynamic_cast<RealFunctionProduct*>((RealFunction*) func))
+    {
+        computeOrder(f->f1, order);
+        computeOrder(f->f2, order);
+    }
+    else
+        PLERROR("In BasisSelectionRegressor::computeOrder: bad function type.");
+}
+
+TVec<RealFunc> BasisSelectionRegressor::buildTopCandidateFunctions()
+{
+    // The scores matrix should match (in size) the candidate_functions matrix
+    assert(scores.length() == candidate_functions.length());
+
+    sortRows(scores, 1, false);
+    TVec<RealFunc> top_best_functions;
+    for (int i=0; i<consider_n_best_for_interaction; i++)
+        top_best_functions.append(simple_candidate_functions[(int)scores(i,0)]);
+
+    return top_best_functions;
+}
+
 /* Returns the index of the most correlated (or anti-correlated) feature 
 among the full candidate features. 
 */
@@ -541,7 +665,7 @@
 
     computeWeightedAveragesWithResidue(candidate_functions, wsum, E_x, E_xx, E_y, E_yy, E_xy);
     
-    Vec scores = (E_xy-E_y*E_x)/sqrt(E_xx-square(E_x));
+    scores.resize(simple_candidate_functions.length(), 2);
 
     if(verbosity>=5)
         perr << "n_candidates = " << n_candidates << endl;
@@ -551,8 +675,6 @@
     best_candidate_index = -1;
     best_score = 0;
 
-    if(verbosity>=10)
-        perr << "scores: ";
     for(int j=0; j<n_candidates; j++)
     {
         real score = 0;
@@ -561,12 +683,18 @@
         else
             score = fabs(E_xy[j]);
         if(verbosity>=10)
-            perr << score << ' '; 
+            perr << score << ' ';
         if(score>best_score)
         {
             best_candidate_index = j;
             best_score = score;
         }
+        // we keep the score only for the simple_candidate_functions
+        if (j < simple_candidate_functions.length())
+        {
+            scores(j, 0) = j;
+            scores(j, 1) = score;
+        }
     }
 
     if(verbosity>=10)
@@ -587,7 +715,7 @@
     Vec& E_xy;
     const Vec& Y;
     boost::mutex& ts_mx;
-    const VMat& train_set;  
+    const VMat& train_set;
     boost::mutex& pb_mx;
     PP<ProgressBar> pb;
     int thread_subtrain_length;
@@ -800,6 +928,7 @@
 }
 
 
+/*
 void BasisSelectionRegressor::computeWeightedCorrelationsWithY(const TVec<RealFunc>& functions, const Vec& Y,  
                                                                real& wsum,
                                                                Vec& E_x, Vec& V_x,
@@ -917,14 +1046,15 @@
     covar = E_xy - E_x*E_y;
     correl = covar/sqrt(V_x*V_y);
 }
+*/
 
 void BasisSelectionRegressor::appendFunctionToSelection(int candidate_index)
 {
     RealFunc f = candidate_functions[candidate_index];
-    int l = train_set->length();
-    int nf = selected_functions.length();    
     if(precompute_features)
     {
+        int l = train_set->length();
+        int nf = selected_functions.length();
         features.resize(l,nf+1, max(1,static_cast<int>(0.25*l*nf)),true);  // enlarge width while preserving content
         real weight;
         for(int i=0; i<l; i++)
@@ -973,7 +1103,7 @@
         newtrainset= new RealFunctionsProcessedVMatrix(train_set, selected_functions, false, true, true);
     newtrainset->defineSizes(nf,1,weighted?1:0);
     // perr.clearOutMap();
-    // perr << "new train set:\n" << newtrainset << endl; 
+    // perr << "new train set:\n" << newtrainset << endl;
     learner->setTrainingSet(newtrainset);
     learner->forget();
     learner->train();
@@ -1016,7 +1146,7 @@
 
         if(candidate_functions.length()>0)
         {
-            int best_candidate_index = -1;  
+            int best_candidate_index = -1;
             real best_score = 0;
             findBestCandidateFunction(best_candidate_index, best_score);
             if(verbosity>=2)
@@ -1026,7 +1156,7 @@
             if(best_candidate_index>=0)
             {
                 if(verbosity>=2)
-                    perr << "  function info = " << candidate_functions[best_candidate_index]->getInfo() << endl;                
+                    perr << "  function info = " << candidate_functions[best_candidate_index]->getInfo() << endl;
                 if(verbosity>=3)
                     perr << "  function= " << candidate_functions[best_candidate_index] << endl;
                 appendFunctionToSelection(best_candidate_index);
@@ -1066,7 +1196,7 @@
         targets[i] = t;
         residue[i] = t;
         residue_sum += w*t;
-        residue_sum_sq += w*square(t);        
+        residue_sum_sq += w*square(t);
         weights[i] = w;
         weights_sum += w;
     }
@@ -1084,7 +1214,7 @@
     {
         train_set->getExample(i, input, targ, weight);
         Vec v = features(i);
-        evaluate_functions(selected_functions, input, v);        
+        evaluate_functions(selected_functions, input, v);
     }    
 }
 
@@ -1162,7 +1292,7 @@
 
 void BasisSelectionRegressor::setTrainStatsCollector(PP<VecStatsCollector> statscol)
 { 
-    train_stats = statscol; 
+    train_stats = statscol;
     learner->setTrainStatsCollector(statscol);
 }
 

Modified: trunk/plearn_learners/regressors/BasisSelectionRegressor.h
===================================================================
--- trunk/plearn_learners/regressors/BasisSelectionRegressor.h	2008-06-26 17:52:19 UTC (rev 9180)
+++ trunk/plearn_learners/regressors/BasisSelectionRegressor.h	2008-06-26 17:54:11 UTC (rev 9181)
@@ -71,6 +71,7 @@
     bool consider_raw_inputs;
     bool consider_normalized_inputs;
     bool consider_input_range_indicators;
+    bool fixed_min_range;
     real indicator_desired_prob;
     real indicator_min_prob;
     TVec<Ker> kernels;
@@ -78,6 +79,8 @@
     int n_kernel_centers_to_pick;
     bool consider_interaction_terms;
     int max_interaction_terms;
+    int consider_n_best_for_interaction;
+    int interaction_max_order;
     bool consider_sorted_encodings;
     int max_n_vals_for_sorted_encodings;
     bool normalize_features;
@@ -89,6 +92,7 @@
     //#####  Public Learnt Options  ############################################
     TVec<RealFunc> selected_functions;
     Vec alphas;
+    mutable Mat scores;
 
 
     struct thread_wawr;
@@ -190,20 +194,30 @@
     void buildSimpleCandidateFunctions();
 
     //! Builds candidate_functions.
-    //! If consider_interactionis false, candidate_functions is the same as simple_candidate_functions
+    //! If consider_interactions is false, candidate_functions is the same as simple_candidate_functions
     //! If consider_interactions is true,  candidate_functions will in addidion include all products 
     //! between simple_candidate_functions and selected_functions 
     void buildAllCandidateFunctions();
 
+    //! Builds top best candidate functions (from simple_candidate_functions only)
+    TVec<RealFunc> buildTopCandidateFunctions();
+
     //! Returns the index of the best candidate function (most colinear with the residue)
     void findBestCandidateFunction(int& best_candidate_index, real& best_score) const;
 
+    //! Adds an interaction term to all_functions as RealFunctionProduct(f1, f2)
+    void addInteractionFunction(RealFunc& f1, RealFunc& f2, TVec<RealFunc>& all_functions);
+
+    //! Computes the order of the given function (number of single function embedded)
+    void computeOrder(RealFunc& func, int& order);
+
     void computeWeightedAveragesWithResidue(const TVec<RealFunc>& functions,   
                                             real& wsum,
                                             Vec& E_x, Vec& E_xx,
                                             real& E_y, real& E_yy,
                                             Vec& E_xy) const;
 
+    /*
     void computeWeightedCorrelationsWithY(const TVec<RealFunc>& functions, const Vec& Y,  
                                           real& wsum,
                                           Vec& E_x, Vec& V_x,
@@ -211,6 +225,7 @@
                                           Vec& E_xy, Vec& V_xy,
                                           Vec& covar, Vec& correl,
                                           real min_variance = 1e-6) const;
+    */
     void appendFunctionToSelection(int candidate_index);
     void retrainLearner();
     void initTargetsResidueWeight();



From chrish at mail.berlios.de  Thu Jun 26 21:42:04 2008
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Thu, 26 Jun 2008 21:42:04 +0200
Subject: [Plearn-commits] r9182 - trunk/python_modules/plearn/pymake
Message-ID: <200806261942.m5QJg4au015841@sheep.berlios.de>

Author: chrish
Date: 2008-06-26 21:42:00 +0200 (Thu, 26 Jun 2008)
New Revision: 9182

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
Flush stdout after printing Waiting for NFS... message, so it shows up right away.

Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-06-26 17:54:11 UTC (rev 9181)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-06-26 19:42:00 UTC (rev 9182)
@@ -1095,6 +1095,11 @@
         else:
             if not local_compilation:
                 print 'Waiting for NFS to catch up...',
+                # Flush stdhout to make sure the previous message shows up,
+                # so if wait_for_all_...() doesn't work properly, then at
+                # least we learn we are waiting for NFS now and not when we
+                # press Ctrl+C.
+                sys.stdout.flush()
                 ccfile.nfs_wait_for_all_linkable_ofiles()
                 print 'done'
             if local_ofiles:



From chrish at mail.berlios.de  Thu Jun 26 22:13:44 2008
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Thu, 26 Jun 2008 22:13:44 +0200
Subject: [Plearn-commits] r9183 - trunk/plearn/base
Message-ID: <200806262013.m5QKDi9M019467@sheep.berlios.de>

Author: chrish
Date: 2008-06-26 22:13:44 +0200 (Thu, 26 Jun 2008)
New Revision: 9183

Modified:
   trunk/plearn/base/plerror.h
Log:
* When compiling with g++, annotate errormsg and friends with noreturn and 
  format(printf, ...) gcc __attribute(())'s for better warning messages.


Modified: trunk/plearn/base/plerror.h
===================================================================
--- trunk/plearn/base/plerror.h	2008-06-26 19:42:00 UTC (rev 9182)
+++ trunk/plearn/base/plerror.h	2008-06-26 20:13:44 UTC (rev 9183)
@@ -51,6 +51,12 @@
 #include <string>
 #include "plexceptions.h"
 
+#ifndef __GNUC__
+// Suppress __attribute__(()) GCC extension on other compilers.
+#define __attribute__(x)
+#endif
+
+
 namespace PLearn {
 
 #ifndef USE_EXCEPTIONS
@@ -62,15 +68,25 @@
 #define PLWARNING warningmsg
 #define PLDEPRECATED deprecationmsg
 
-void errormsg2(const char* filename,const int linenumber,const char* msg, ...);
-void errormsg(const char* msg, ...);
-void warningmsg(const char* msg, ...);
-void deprecationmsg(const char* msg, ...);
-void exitmsg(const char* msg, ...);
+void errormsg2(const char* filename, const int linenumber, const char* msg, ...)
+    __attribute__((noreturn))
+    __attribute__((format(printf, 3, 4)));
+void errormsg(const char* msg, ...)
+    __attribute__((noreturn))
+    __attribute__((format(printf, 1, 2)));
+void warningmsg(const char* msg, ...)
+    __attribute__((format(printf, 1, 2)));
+void deprecationmsg(const char* msg, ...)
+    __attribute__((format(printf, 1, 2)));
+void exitmsg(const char* msg, ...)
+    __attribute__((noreturn))
+    __attribute__((format(printf, 1, 2)));
 void pl_assert_fail(const char* expr, const char* file, unsigned line,
-                    const char* function, const std::string& message);
+                    const char* function, const std::string& message)
+    __attribute__((noreturn));
 void pl_check_fail(const char* expr, const char* file, unsigned line,
-                   const char* function, const std::string& message);
+                   const char* function, const std::string& message)
+    __attribute__((noreturn));
 
 // Redefine the assert mechanism to throw an exception through PLERROR.
 // The following macros are defined:



From chrish at mail.berlios.de  Thu Jun 26 23:07:49 2008
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Thu, 26 Jun 2008 23:07:49 +0200
Subject: [Plearn-commits] r9184 - in trunk: plearn/io plearn/ker
	plearn/python plearn/var plearn/vmat plearn_learners/misc
	plearn_learners/regressors
Message-ID: <200806262107.m5QL7nKo025110@sheep.berlios.de>

Author: chrish
Date: 2008-06-26 23:07:47 +0200 (Thu, 26 Jun 2008)
New Revision: 9184

Modified:
   trunk/plearn/io/pl_io_deprecated.cc
   trunk/plearn/ker/SummationKernel.cc
   trunk/plearn/python/PythonObjectWrapper.h
   trunk/plearn/var/ColumnIndexVariable.cc
   trunk/plearn/var/Variable.cc
   trunk/plearn/vmat/FileVMatrix.cc
   trunk/plearn/vmat/VMatLanguage.cc
   trunk/plearn_learners/misc/VariableSelectionWithDirectedGradientDescent.cc
   trunk/plearn_learners/regressors/LinearRegressor.cc
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
Log:
Fix some of the (probably long-standing) format string bugs uncovered by
GCC's __attribute__((format)). Like shooting fish in a barrel.


Modified: trunk/plearn/io/pl_io_deprecated.cc
===================================================================
--- trunk/plearn/io/pl_io_deprecated.cc	2008-06-26 20:13:44 UTC (rev 9183)
+++ trunk/plearn/io/pl_io_deprecated.cc	2008-06-26 21:07:47 UTC (rev 9184)
@@ -72,7 +72,7 @@
     if (   headerlen<classnamelen+2 
            || header[0]!='<' || header.substr(1,classnamelen)!=classname
            || (header[1+classnamelen]!='>' && header[1+classnamelen]!=':') )
-        PLERROR("In Object::readHeader WRONG HEADER: %s (SHOULD BE {s:version>)",header.c_str(),classname.c_str());
+        PLERROR("In Object::readHeader WRONG HEADER: %s (SHOULD BE {%s:version>)",header.c_str(),classname.c_str());
     if (header[1+classnamelen]==':')
         return toint(header.substr(2+classnamelen, headerlen-classnamelen-2));
     else return 0;

Modified: trunk/plearn/ker/SummationKernel.cc
===================================================================
--- trunk/plearn/ker/SummationKernel.cc	2008-06-26 20:13:44 UTC (rev 9183)
+++ trunk/plearn/ker/SummationKernel.cc	2008-06-26 21:07:47 UTC (rev 9184)
@@ -251,7 +251,7 @@
         int i = lexical_cast<int>(term_index);
         if (i < 0 || i >= m_terms.size())
             PLERROR("%s: out of bounds access to term %d when computing derivative\n"
-                    "for kernel parameter '%d'; only %d terms (0..%d) are available\n"
+                    "for kernel parameter '%s'; only %d terms (0..%d) are available\n"
                     "in the SummationKernel", __FUNCTION__, i, kernel_param.c_str(),
                     m_terms.size(), m_terms.size()-1);
         

Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2008-06-26 20:13:44 UTC (rev 9183)
+++ trunk/plearn/python/PythonObjectWrapper.h	2008-06-26 21:07:47 UTC (rev 9184)
@@ -471,7 +471,7 @@
 template <typename T>
 int numpyType()
 {
-    PLERROR("in numpyType: no numpy equivalent to C++ type ",
+    PLERROR("in numpyType: no numpy equivalent to C++ type %s",
             TypeTraits<T*>::name().c_str());
     return -1; //shut up compiler
 }

Modified: trunk/plearn/var/ColumnIndexVariable.cc
===================================================================
--- trunk/plearn/var/ColumnIndexVariable.cc	2008-06-26 20:13:44 UTC (rev 9183)
+++ trunk/plearn/var/ColumnIndexVariable.cc	2008-06-26 21:07:47 UTC (rev 9184)
@@ -83,7 +83,7 @@
                 "variable representing the indices of input1");
     if (input1 && input2 && input1->width() != input2->size())
         PLERROR("In ColumnIndexVariable::build_ - input1's width (%d) "
-                "should be equal to input2's size (%s)",
+                "should be equal to input2's size (%d)",
                 input1->width(), input2->size());
 }
 

Modified: trunk/plearn/var/Variable.cc
===================================================================
--- trunk/plearn/var/Variable.cc	2008-06-26 20:13:44 UTC (rev 9183)
+++ trunk/plearn/var/Variable.cc	2008-06-26 21:07:47 UTC (rev 9184)
@@ -1012,7 +1012,7 @@
 {
     int n=nelems();
     if (storage->length()<offset_+n) 
-        PLERROR("Variable::makeSharedValue, storage(%d) too small({d+%d)",
+        PLERROR("Variable::makeSharedValue, storage(%d) too small(%d+%d)",
                 storage->length(),offset_,nelems());
     real* v=value.data();
     real* x=valuedata=storage->data+offset_;
@@ -1035,7 +1035,7 @@
 {
     int n=nelems();
     if (storage->length()<offset_+n) 
-        PLERROR("Variable::makeSharedGradient, storage(%d) too small({d+%d)",
+        PLERROR("Variable::makeSharedGradient, storage(%d) too small(%d+%d)",
                 storage->length(),offset_,nelems());
     real* v=gradient.data();
     real* x=gradientdata=storage->data+offset_;
@@ -1076,7 +1076,7 @@
     resizeRValue();
     int n=nelems();
     if (storage->length()<offset_+n) 
-        PLERROR("Variable::makeSharedRValue, storage(%d) too small({d+%d)",
+        PLERROR("Variable::makeSharedRValue, storage(%d) too small(%d+%d)",
                 storage->length(),offset_,nelems());
     real* v=rValue.data();
     real* x=rvaluedata=storage->data+offset_;

Modified: trunk/plearn/vmat/FileVMatrix.cc
===================================================================
--- trunk/plearn/vmat/FileVMatrix.cc	2008-06-26 20:13:44 UTC (rev 9183)
+++ trunk/plearn/vmat/FileVMatrix.cc	2008-06-26 21:07:47 UTC (rev 9184)
@@ -282,7 +282,7 @@
         PRInt64 expectedsize=DATAFILE_HEADERLENGTH+length_*width_*elemsize;
         if(info.size!=expectedsize)
             PLWARNING("In FileVMatrix::build_() - The file '%s' have a size"
-                      " of %d, expected %d",
+                      " of %ld, expected %ld",
                       filename_.c_str(), info.size, expectedsize);
 #endif
         

Modified: trunk/plearn/vmat/VMatLanguage.cc
===================================================================
--- trunk/plearn/vmat/VMatLanguage.cc	2008-06-26 20:13:44 UTC (rev 9183)
+++ trunk/plearn/vmat/VMatLanguage.cc	2008-06-26 21:07:47 UTC (rev 9184)
@@ -385,7 +385,7 @@
                     // Keyword indicating we go till the end.
                     a = srcfieldnames.length() - 1;
                 else
-                    PLERROR("fieldcopy macro syntax is : [start:end] EG: [@year:%6]. 'end' must be after 'start'.. OR [field] to copy a single field");
+                    PLERROR("fieldcopy macro syntax is : [start:end] EG: [@year:%%6]. 'end' must be after 'start'.. OR [field] to copy a single field");
 
                 if (parts[1][0] == '@')
                 {
@@ -402,12 +402,12 @@
                     // Keyword indicating we go till the end.
                     b = srcfieldnames.length() - 1;
                 else
-                    PLERROR("fieldcopy macro syntax is : [start:end] EG: [@year:%6]. 'end' must be after 'start'.. OR [field] to copy a single field");
+                    PLERROR("fieldcopy macro syntax is : [start:end] EG: [@year:%%6]. 'end' must be after 'start'.. OR [field] to copy a single field");
 
                 if (a > b)
                     PLERROR("In copyfield macro '%s', you have specified a "
                             "start field (%d) that is after the end field "
-                            "(%d). E.g.: [%10:%5]", token.c_str(), a, b);
+                            "(%d). E.g.: [%%10:%%5]", token.c_str(), a, b);
                 if (a == -1)
                     PLERROR("In copyfield macro, unknown field : '%s'", astr.c_str());
                 if (b == -1)
@@ -448,11 +448,11 @@
                 else if (parts[0][0] == '%')
                     a = toint(parts[0].substr(1));
                 else
-                    PLERROR("fieldcopy macro syntax is : [start:end] EG: [@year:%6]. 'end' must be after 'start'.. OR [field] to copy a single field");
+                    PLERROR("fieldcopy macro syntax is : [start:end] EG: [@year:%%6]. 'end' must be after 'start'.. OR [field] to copy a single field");
 
                 if (a == -1) {
                     if (!ignore_if_missing)
-                        PLERROR("In copyfield macro, unknown field :%s",astr.c_str());
+                        PLERROR("In copyfield macro, unknown field: '%s'", astr.c_str());
                 }
                 else {
                     processed_sourcecode += string("%") + tostring(a) + " ";
@@ -462,7 +462,7 @@
                     fieldnames.push_back(srcfieldnames[a]);
                 }
             }
-            else PLERROR("Strange fieldcopy format. e.g : [%0:%5]. Found parts '%s'",join(parts," ").c_str());
+            else PLERROR("Strange fieldcopy format. e.g : [%%0:%%5]. Found parts '%s'",join(parts," ").c_str());
         }
 
         // did we find a comment?

Modified: trunk/plearn_learners/misc/VariableSelectionWithDirectedGradientDescent.cc
===================================================================
--- trunk/plearn_learners/misc/VariableSelectionWithDirectedGradientDescent.cc	2008-06-26 20:13:44 UTC (rev 9183)
+++ trunk/plearn_learners/misc/VariableSelectionWithDirectedGradientDescent.cc	2008-06-26 21:07:47 UTC (rev 9184)
@@ -130,7 +130,7 @@
         if (inputsize < 1)
             PLERROR("VariableSelectionWithDirectedGradientDescent: expected  inputsize greater than 0, got %d", inputsize);
         if (targetsize <= 0)
-            PLERROR("In VariableSelectionWithDirectedGradientDescent::train - The targetsize must be >= 1", targetsize);
+            PLERROR("In VariableSelectionWithDirectedGradientDescent::train - The targetsize (%d) must be >= 1", targetsize);
         if (weightsize != 0)
             PLERROR("VariableSelectionWithDirectedGradientDescent: expected weightsize to be 1, got %d", weightsize_);
         input_weights.resize(targetsize, inputsize + 1);

Modified: trunk/plearn_learners/regressors/LinearRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/LinearRegressor.cc	2008-06-26 20:13:44 UTC (rev 9183)
+++ trunk/plearn_learners/regressors/LinearRegressor.cc	2008-06-26 21:07:47 UTC (rev 9184)
@@ -225,7 +225,7 @@
 void LinearRegressor::train()
 {
     if(targetsize()<=0)
-        PLERROR("In LinearRegressor::train() -  Targetsize (%s) must be "
+        PLERROR("In LinearRegressor::train() -  Targetsize (%d) must be "
                 "positive", targetsize());
     // Preparatory buffer allocation
     bool recompute_XXXY = (XtX.length()==0);

Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2008-06-26 20:13:44 UTC (rev 9183)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2008-06-26 21:07:47 UTC (rev 9184)
@@ -134,7 +134,7 @@
         }
     }
     if (multiclass_found < 1) 
-        PLERROR("RegressionTreeMultilassLeave: Unknown target: %d row: %d\n", target,row);
+        PLERROR("RegressionTreeMultilassLeave: Unknown target: %g row: %d\n", target, row);
 }
 
 void RegressionTreeMulticlassLeave::addRow(int row, Vec outputv, Vec errorv)



From plearner at mail.berlios.de  Fri Jun 27 00:23:34 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Fri, 27 Jun 2008 00:23:34 +0200
Subject: [Plearn-commits] r9185 - trunk/plearn_learners_experimental
Message-ID: <200806262223.m5QMNYVr031194@sheep.berlios.de>

Author: plearner
Date: 2008-06-27 00:23:34 +0200 (Fri, 27 Jun 2008)
New Revision: 9185

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
Log:


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-06-26 21:07:47 UTC (rev 9184)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-06-26 22:23:34 UTC (rev 9185)
@@ -459,7 +459,6 @@
 
     // reserve memory for sequences
     seq.resize(5000,2); // contains the current sequence
-    encoded_seq.resize(5000, 4);
 
     Vec input( inputsize() );
     Vec target( targetsize() );
@@ -480,12 +479,6 @@
     train_stats->forget();
 
 
-    /***** RBM training phase *****/
-//    if(rbm_stage < rbm_nstages)
-//    {
-//    }
-
-
     /***** Recurrent phase *****/
     if( stage >= nstages )
         return;
@@ -510,11 +503,6 @@
         // TO DO: check this line
         setLearningRate( recurrent_net_learning_rate );
 
-        int ith_sample_in_sequence = 0;
-        int inputsize_without_masks = inputsize() 
-            - ( use_target_layers_masks ? targetsize() : 0 );
-        int sum_target_elements = 0;
-
         while(stage < end_stage)
         {
             train_costs.clear();
@@ -524,17 +512,7 @@
             for(int i=0; i<nseq; i++)
             {
                 getSequence(i, seq);
-                if(encoding=="raw_masked_supervised")
-                {
-                    splitMaskedSupervisedSequence(seq);
-                }
-                else
-                {
-                    encodeSequence(seq, encoded_seq);
-                    createSupervisedSequence(encoded_seq);
-                }
-
-                resize_lists();
+                encodeSequenceAndPopulateLists(seq);
                 fprop(train_costs, train_n_items);
                 recurrent_update();
             }
@@ -567,10 +545,29 @@
     train_stats->finalize();        
 }
 
+
+//! does encoding if needed and populates the list.
+void DenoisingRecurrentNet::encodeSequenceAndPopulateLists(Mat seq)
+{
+    if(encoding=="raw_masked_supervised") // old already encoded format (for backward testing)
+        splitRawMaskedSupervisedSequence(seq);
+    else
+        encodeAndCreateSupervisedSequence(seq);
+    resize_lists();
+}
+
 // TO DO: penser a gestion des prepended dans ce cas
-// Populates: inputslist, targets_list, masks_list
-void DenoisingRecurrentNet::createSupervisedSequence(Mat encoded_seq)
+// encodes sequ, then populates: inputslist, targets_list, masks_list
+void DenoisingRecurrentNet::encodeAndCreateSupervisedSequence(Mat seq)
 {
+    encodeSequence(seq, encoded_seq);
+    // now work with encoded_seq
+    int l = encoded_seq;
+    resize_lists(l);
+
+    // TO DO: populate lists
+    // ....
+
     PLERROR("Not implemented yet");
 }
 
@@ -578,16 +575,16 @@
 
 // TO DO: penser a prepend dans ce cas
 
-// Populates: input_list, targets_list, masks_list
-void DenoisingRecurrentNet::splitMaskedSupervisedSequence(Mat seq)
+// For the (backward testing) raw_masked_supervised case. Populates: input_list, targets_list, masks_list
+void DenoisingRecurrentNet::splitRawMaskedSupervisedSequence(Mat seq)
 {
+    int l = seq.length();
+    resize_lists(l);
     int inputsize_without_masks = inputsize()-targetsize();
     Mat input_part = seq.subMatColumns(0,inputsize_without_masks);
     Mat mask_part = seq.subMatColumns(inputsize_without_masks, targetsize());
     Mat target_part = seq.subMatColumns(inputsize_without_masks+targetsize(), targetsize());
 
-    int l = input_part.length();
-    input_list.resize(l);
     for(int i=0; i<l; i++)
         input_list[i] = input_part(i);
 
@@ -605,10 +602,9 @@
 }
 
 
-void DenoisingRecurrentNet::resize_lists()
+void DenoisingRecurrentNet::resize_lists(int l)
 {
-    int l = input_list.length();
-
+    input_list.resize(l);
     hidden_list.resize(l, hidden_layer->size);
     hidden_act_no_bias_list.resize(l, hidden_layer->size);
 
@@ -634,7 +630,7 @@
 
 // TODO: think properly about prepended stuff
 
-// fprop accumulates costs in costs and n_items in n_items
+// fprop accumulates costs in costs and counts in n_items
 void DenoisingRecurrentNet::fprop(Vec train_costs, Vec train_n_items)
 {
     int l = input_list.length();
@@ -682,7 +678,7 @@
             }
         }
 
-        sum_target_elements = 0;
+        int sum_target_elements = 0;
         for( int tar=0; tar < ntargets; tar++ )
         {
             if( !fast_exact_is_equal(target_layers_weights[tar],0) )
@@ -1258,11 +1254,14 @@
 
  */
 
-void DenoisingRecurrentNet::encodeSequence(Mat sequence, Mat& encoded_sequence)
+void DenoisingRecurrentNet::encodeSequence(Mat sequence, Mat& encoded_seq)
 {
     //! Possibilities: "timeframe", "note_duration", "note_octav_duration", "generic"
     int prepend_zero_rows = input_window_size;
 
+    // reserve some minimum space for encoded_seq
+    encoded_seq.resize(5000, 4);
+
     if(encoding=="timeframe")
         encode_onehot_timeframe(sequence, encoded_sequence, prepend_zero_rows);
     else if(encoding=="note_duration")
@@ -1570,51 +1569,46 @@
     int nseq = testset_boundaries.length();
 
     seq.resize(5000,2); // contains the current sequence
-    seq.resize(5000, 4);
+    encoded_seq.resize(5000, 4);
 
+
+    int pos = 0; // position in testoutputs
     for(int i=0; i<nseq; i++)
     {
         int start = 0;
         if(i>0)
             start = testset_boundaries[i-1]+1;
         int end = testset_boundaries[i];
-        seq.resize(end-start, w);
+        int seqlen = end-start; // target_prediction_list[0].length();
+        seq.resize(seqlen, w);
         testset->getMat(start,0,seq);
-
-        if(encoding=="raw_masked_supervised")
-        {
-            splitMaskedSupervisedSequence(seq);
-        }
-        else
-        {
-            encodeSequence(seq, encoded_seq);
-            createSupervisedSequence(encoded_seq);
-        }
-
-        resize_lists();
+        encodeSequenceAndPopulateLists(seq);
         fprop(test_costs, test_n_items);
 
-        /*
         if (testoutputs)
         {
-            int sum_target_layers_size = 0;
-            for( int tar=0; tar < target_layers.length(); tar++ )
+            for(int t=0; t<seqlen; t++)
             {
-                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                int sum_target_layers_size = 0;
+                for( int tar=0; tar < target_layers.length(); tar++ )
                 {
-                    output.subVec(sum_target_layers_size,target_layers[tar]->size)
-                        << target_prediction_list[tar][ ith_sample_in_sequence ];
+                    if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                    {
+                        output.subVec(sum_target_layers_size,target_layers[tar]->size)
+                            << target_prediction_list[tar](t);
+                    }
+                    sum_target_layers_size += target_layers[tar]->size;
                 }
-                sum_target_layers_size += target_layers[tar]->size;
+                testoutputs->putOrAppendRow(pos++, output);
             }
-            testoutputs->putOrAppendRow(i, output);
         }
-        */
+        else
+            pos += seqlen;
 
+        if (report_progress)
+            pb->update(pos);
     }
 
-        if (report_progress)
-            pb->update(i);
 
     }
 
@@ -1630,11 +1624,6 @@
     
     if (test_stats)
         test_stats->update(costs, weight);
-
-
-    if( pb )
-        pb->update( stage + 1 - init_stage);
-
 }
 
 /*

Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-06-26 21:07:47 UTC (rev 9184)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-06-26 22:23:34 UTC (rev 9185)
@@ -152,13 +152,19 @@
 
     // encodings
 
+    //! encodes sequence according to specified encoding option
+    void encodeSequence(Mat sequence, Mat& encoded_seq);
+
     static void encode_onehot_note_octav_duration(Mat sequence, Mat& encoded_sequence, int prepend_zero_rows,
                                                   bool use_silence=true, int octav_nbits=0, int duration_nbits=8);
     
-    static void encode_onehot_timeframe(Mat sequence, Mat& encoded_sequence, int prepend_zero_rows, 
-                                        bool use_silence=true);
-    
+    static void encode_onehot_timeframe(Mat sequence, Mat& encoded_sequence, 
+                                        int prepend_zero_rows, bool use_silence=true);    
 
+    static int duration_to_number_of_timeframes(int duration);
+
+
+
     // input noise injection
     void inject_zero_forcing_noise(Mat sequence, double noise_prob);
 
@@ -351,6 +357,16 @@
     //! This does the actual building.
     void build_();
 
+    //! does encoding if needed and populates the list.
+    void encodeSequenceAndPopulateLists(Mat seq);
+
+    //! encodes seq, then populates: inputslist, targets_list, masks_list
+    void encodeAndCreateSupervisedSequence(Mat seq);
+
+    //! For the (backward testing) raw_masked_supervised case. Populates: input_list, targets_list, masks_list
+    void splitRawMaskedSupervisedSequence(Mat seq);
+
+
 private:
     //#####  Private Data Members  ############################################
 



From plearner at mail.berlios.de  Fri Jun 27 17:12:48 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Fri, 27 Jun 2008 17:12:48 +0200
Subject: [Plearn-commits] r9186 - trunk/plearn_learners_experimental
Message-ID: <200806271512.m5RFCmiM015198@sheep.berlios.de>

Author: plearner
Date: 2008-06-27 17:12:48 +0200 (Fri, 27 Jun 2008)
New Revision: 9186

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
Log:
Development in progress on DenoisingRecurrentNet


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-06-26 22:23:34 UTC (rev 9185)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-06-27 15:12:48 UTC (rev 9186)
@@ -460,9 +460,7 @@
     // reserve memory for sequences
     seq.resize(5000,2); // contains the current sequence
 
-    Vec input( inputsize() );
-    Vec target( targetsize() );
-    real weight = 0; // Unused
+    // real weight = 0; // Unused
     Vec train_costs( getTrainCostNames().length() );
     train_costs.clear();
     Vec train_n_items( getTrainCostNames().length() );
@@ -547,22 +545,21 @@
 
 
 //! does encoding if needed and populates the list.
-void DenoisingRecurrentNet::encodeSequenceAndPopulateLists(Mat seq)
+void DenoisingRecurrentNet::encodeSequenceAndPopulateLists(Mat seq) const
 {
     if(encoding=="raw_masked_supervised") // old already encoded format (for backward testing)
         splitRawMaskedSupervisedSequence(seq);
     else
         encodeAndCreateSupervisedSequence(seq);
-    resize_lists();
 }
 
 // TO DO: penser a gestion des prepended dans ce cas
 // encodes sequ, then populates: inputslist, targets_list, masks_list
-void DenoisingRecurrentNet::encodeAndCreateSupervisedSequence(Mat seq)
+void DenoisingRecurrentNet::encodeAndCreateSupervisedSequence(Mat seq) const
 {
     encodeSequence(seq, encoded_seq);
     // now work with encoded_seq
-    int l = encoded_seq;
+    int l = encoded_seq.length();
     resize_lists(l);
 
     // TO DO: populate lists
@@ -576,7 +573,7 @@
 // TO DO: penser a prepend dans ce cas
 
 // For the (backward testing) raw_masked_supervised case. Populates: input_list, targets_list, masks_list
-void DenoisingRecurrentNet::splitRawMaskedSupervisedSequence(Mat seq)
+void DenoisingRecurrentNet::splitRawMaskedSupervisedSequence(Mat seq) const
 {
     int l = seq.length();
     resize_lists(l);
@@ -589,7 +586,7 @@
         input_list[i] = input_part(i);
 
     int ntargets = target_layers.length();
-    targets_list.resize(ntagets);
+    targets_list.resize(ntargets);
     masks_list.resize(ntargets);
     int startcol = 0; // starting column of next target in target_part and mask_part
     for(int k=0; k<ntargets; k++)
@@ -602,7 +599,7 @@
 }
 
 
-void DenoisingRecurrentNet::resize_lists(int l)
+void DenoisingRecurrentNet::resize_lists(int l) const
 {
     input_list.resize(l);
     hidden_list.resize(l, hidden_layer->size);
@@ -620,7 +617,7 @@
 
     for( int tar=0; tar < ntargets; tar++ )
     {
-        int targsize = target_layers[k]->size;
+        int targsize = target_layers[tar]->size;
         target_prediction_list[tar].resize(l, targsize);
         target_prediction_act_no_bias_list[tar].resize(l, targsize);
     }
@@ -631,12 +628,12 @@
 // TODO: think properly about prepended stuff
 
 // fprop accumulates costs in costs and counts in n_items
-void DenoisingRecurrentNet::fprop(Vec train_costs, Vec train_n_items)
+void DenoisingRecurrentNet::fprop(Vec train_costs, Vec train_n_items) const
 {
     int l = input_list.length();
     int ntargets = target_layers.length();
 
-    for(int i=0; i<input_list.length(); i++ )
+    for(int i=0; i<l; i++ )
     {
         Vec hidden_act_no_bias_i = hidden_act_no_bias_list(i);
         input_connections->fprop( input_list[i], hidden_act_no_bias_i);
@@ -671,8 +668,8 @@
             {
                 Vec target_prediction_i = target_prediction_list[tar](i);
                 Vec target_prediction_act_no_bias_i = target_prediction_act_no_bias_i;
-                target_connections[tar]->fprop(last_hidden, target_prediction_act_no_bias_list_i);
-                target_layers[tar]->fprop(target_prediction_act_no_bias_i, target_prediction_list_i);
+                target_connections[tar]->fprop(last_hidden, target_prediction_act_no_bias_i);
+                target_layers[tar]->fprop(target_prediction_act_no_bias_i, target_prediction_i);
                 if( use_target_layers_masks )
                     target_prediction_i *= masks_list[tar](i);
             }
@@ -798,14 +795,14 @@
                 hidden_temporal_gradient, hidden_gradient);
                 
             dynamic_connections->bpropUpdate(
-                hidden_list[i-1],
+                hidden_list(i-1),
                 hidden_act_no_bias_list(i), // Here, it should be cond_bias, but doesn't matter
                 hidden_gradient, hidden_temporal_gradient);
                 
             hidden_temporal_gradient << hidden_gradient;
                 
             input_connections->bpropUpdate(
-                input_list(i),
+                input_list[i],
                 hidden_act_no_bias_list(i), 
                 visi_bias_gradient, hidden_temporal_gradient);// Here, it should be activations - cond_bias, but doesn't matter
                 
@@ -816,7 +813,7 @@
                 hidden_act_no_bias_list(i), hidden_list(i),
                 hidden_temporal_gradient, hidden_gradient); // Not really temporal gradient, but this is the final iteration...
             input_connections->bpropUpdate(
-                input_list(i),
+                input_list[i],
                 hidden_act_no_bias_list(i), 
                 visi_bias_gradient, hidden_temporal_gradient);// Here, it should be activations - cond_bias, but doesn't matter
 
@@ -1254,7 +1251,7 @@
 
  */
 
-void DenoisingRecurrentNet::encodeSequence(Mat sequence, Mat& encoded_seq)
+void DenoisingRecurrentNet::encodeSequence(Mat sequence, Mat& encoded_seq) const
 {
     //! Possibilities: "timeframe", "note_duration", "note_octav_duration", "generic"
     int prepend_zero_rows = input_window_size;
@@ -1263,11 +1260,11 @@
     encoded_seq.resize(5000, 4);
 
     if(encoding=="timeframe")
-        encode_onehot_timeframe(sequence, encoded_sequence, prepend_zero_rows);
+        encode_onehot_timeframe(sequence, encoded_seq, prepend_zero_rows);
     else if(encoding=="note_duration")
-        encode_onehot_note_octav_duration(sequence, encoded_sequence, prepend_zero_rows);
+        encode_onehot_note_octav_duration(sequence, encoded_seq, prepend_zero_rows);
     else if(encoding=="note_octav_duration")
-        encode_onehot_note_octav_duration(sequence, encoded_sequence, prepend_zero_rows, true, 4);    
+        encode_onehot_note_octav_duration(sequence, encoded_seq, prepend_zero_rows, true, 4);    
     else if(encoding=="raw_masked_supervised")
         PLERROR("raw_masked_supervised encoding not yet implemented");
     else if(encoding=="generic")
@@ -1298,6 +1295,7 @@
 
 void DenoisingRecurrentNet::locateSequenceBoundaries(VMat dataset, TVec<int>& boundaries, real end_of_sequence_symbol)
 {
+    boundaries.resize(10000);
     boundaries.resize(0);
     int l = dataset->length();
     for(int i=0; i<l; i++)
@@ -1328,16 +1326,16 @@
  */
 
 void DenoisingRecurrentNet::encode_onehot_note_octav_duration(Mat sequence, Mat& encoded_sequence, int prepend_zero_rows,
-                                                              bool use_silence, in octav_nbits, int duration_nbits)
+                                                              bool use_silence, int octav_nbits, int duration_nbits)
 {
     int l = sequence.length();
+    int note_nbits = use_silence ?13 :12;
+
     encoded_sequence.resize(prepend_zero_rows+l,note_nbits+octav_nbits+duration_nbits);
     encoded_sequence.clear();
     int octav_min = 10000;
     int octav_max = -10000;
 
-    int note_nbits = use_silence ?13 :12;
-
     if(octav_nbits>0)
     {
         for(int i=0; i<l; i++)
@@ -1429,10 +1427,10 @@
     
 
 // input noise injection
-void inject_zero_forcing_noise(Mat sequence, double noise_prob)
+void DenoisingRecurrentNet::inject_zero_forcing_noise(Mat sequence, double noise_prob)
 {
     if(!sequence.isCompact())
-        PLEERROR("Expected a compact sequence");
+        PLERROR("Expected a compact sequence");
     real* p = sequence.data();
     int n = sequence.size();
     while(n--)
@@ -1542,13 +1540,10 @@
                   VMat testoutputs, VMat testcosts)const
 {
     int len = testset.length();
-    Vec input;
-    Vec target;
-    real weight;
 
+    //Vec output(outputsize());
+    //output.clear();
 
-    Vec output(outputsize());
-    output.clear();
     Vec costs(nTestCosts());
     costs.clear();
     Vec n_items(nTestCosts());
@@ -1583,7 +1578,7 @@
         seq.resize(seqlen, w);
         testset->getMat(start,0,seq);
         encodeSequenceAndPopulateLists(seq);
-        fprop(test_costs, test_n_items);
+        fprop(costs, n_items);
 
         if (testoutputs)
         {
@@ -1609,9 +1604,6 @@
             pb->update(pos);
     }
 
-
-    }
-
     for(int i=0; i<costs.length(); i++)
     {
         if( !fast_exact_is_equal(target_layers_weights[i],0) )
@@ -1928,8 +1920,16 @@
     return getTestCostNames();
 }
 
+
 void DenoisingRecurrentNet::generate(int t, int n)
 {
+    PLERROR("generate not yet implemented");
+}
+
+
+/*
+void DenoisingRecurrentNet::oldgenerate(int t, int n)
+{
     //PPath* the_filename = "/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/generate/scoreGen.amat";
     data = new AutoVMatrix();
     //data->filename = "/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/listData/target_tm12_input_t_tm12_tp12/scoreGen_tar_tm12__in_tm12_tp12.amat";
@@ -1955,10 +1955,10 @@
 
     Vec output(outputsize());
     output.clear();
-    /*Vec costs(nTestCosts());
-    costs.clear();
-    Vec n_items(nTestCosts());
-    n_items.clear();*/
+//     Vec costs(nTestCosts());
+//     costs.clear();
+//     Vec n_items(nTestCosts());
+//     n_items.clear();
 
     int r,r2;
     
@@ -1987,32 +1987,32 @@
             }       
         }
     
-/*
-        for (int k = 1; k <= t; k++)
-        {
-            partTarSize = outputsize();
-            for( int tar=0; tar < target_layers.length(); tar++ )
-            {
-                if(i>=t){
-                    input.subVec(inputsize_without_masks-(tarSize*(t-k))-partTarSize-1,target_layers[tar]->size) << target_prediction_list[tar][ith_sample_in_sequence-k];
-                    partTarSize -= target_layers[tar]->size;
-                }
-            }
-        }
-*/
+
+//         for (int k = 1; k <= t; k++)
+//         {
+//             partTarSize = outputsize();
+//             for( int tar=0; tar < target_layers.length(); tar++ )
+//             {
+//                 if(i>=t){
+//                     input.subVec(inputsize_without_masks-(tarSize*(t-k))-partTarSize-1,target_layers[tar]->size) << target_prediction_list[tar][ith_sample_in_sequence-k];
+//                     partTarSize -= target_layers[tar]->size;
+//                 }
+//             }
+//         }
+
         if( fast_exact_is_equal(input[0],end_of_sequence_symbol) )
         {
-            /*  ith_sample_in_sequence = 0;
-            hidden_list.resize(0);
-            hidden_act_no_bias_list.resize(0);
-            hidden2_list.resize(0);
-            hidden2_act_no_bias_list.resize(0);
-            target_prediction_list.resize(0);
-            target_prediction_act_no_bias_list.resize(0);
-            input_list.resize(0);
-            targets_list.resize(0);
-            nll_list.resize(0,0);
-            masks_list.resize(0);*/
+//             ith_sample_in_sequence = 0;
+//             hidden_list.resize(0);
+//             hidden_act_no_bias_list.resize(0);
+//             hidden2_list.resize(0);
+//             hidden2_act_no_bias_list.resize(0);
+//             target_prediction_list.resize(0);
+//             target_prediction_act_no_bias_list.resize(0);
+//             input_list.resize(0);
+//             targets_list.resize(0);
+//             nll_list.resize(0,0);
+//             masks_list.resize(0);
 
             
 
@@ -2186,18 +2186,18 @@
                 nll_list(ith_sample_in_sequence,tar) = 
                     target_layers[tar]->fpropNLL( 
                         targets_list[tar][ith_sample_in_sequence] ); 
-                /*costs[tar] += nll_list(ith_sample_in_sequence,tar);
+//                 costs[tar] += nll_list(ith_sample_in_sequence,tar);
                 
-                // Normalize by the number of things to predict
-                if( use_target_layers_masks )
-                {
-                    n_items[tar] += sum(
-                        input.subVec( inputsize_without_masks 
-                                      + sum_target_elements, 
-                                      target_layers_n_of_target_elements[tar]) );
-                }
-                else
-                n_items[tar]++;*/
+//                 // Normalize by the number of things to predict
+//                 if( use_target_layers_masks )
+//                 {
+//                     n_items[tar] += sum(
+//                         input.subVec( inputsize_without_masks 
+//                                       + sum_target_elements, 
+//                                       target_layers_n_of_target_elements[tar]) );
+//                 }
+//                 else
+//                 n_items[tar]++;
             }
             if( use_target_layers_masks )
                 sum_target_elements += 
@@ -2209,30 +2209,18 @@
 
     }
 
-    /*  
-    ith_sample_in_sequence = 0;
-    hidden_list.resize(0);
-    hidden_act_no_bias_list.resize(0);
-    hidden2_list.resize(0);
-    hidden2_act_no_bias_list.resize(0);
-    target_prediction_list.resize(0);
-    target_prediction_act_no_bias_list.resize(0);
-    input_list.resize(0);
-    targets_list.resize(0);
-    nll_list.resize(0,0);
-    masks_list.resize(0);   
+//     ith_sample_in_sequence = 0;
+//     hidden_list.resize(0);
+//     hidden_act_no_bias_list.resize(0);
+//     hidden2_list.resize(0);
+//     hidden2_act_no_bias_list.resize(0);
+//     target_prediction_list.resize(0);
+//     target_prediction_act_no_bias_list.resize(0);
+//     input_list.resize(0);
+//     targets_list.resize(0);
+//     nll_list.resize(0,0);
+//     masks_list.resize(0);   
 
-
-    */
-
-
-
-
-
-
-
-
-
     
     //Vec tempo;
     //TVec<real> tempo;
@@ -2264,6 +2252,9 @@
      myfile.close();
 
 }
+
+*/
+
 /*
 void DenoisingRecurrentNet::gen()
 {

Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-06-26 22:23:34 UTC (rev 9185)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-06-27 15:12:48 UTC (rev 9186)
@@ -153,7 +153,8 @@
     // encodings
 
     //! encodes sequence according to specified encoding option
-    void encodeSequence(Mat sequence, Mat& encoded_seq);
+    //! (declared const because it needs to be called in test)
+    void encodeSequence(Mat sequence, Mat& encoded_seq) const;
 
     static void encode_onehot_note_octav_duration(Mat sequence, Mat& encoded_sequence, int prepend_zero_rows,
                                                   bool use_silence=true, int octav_nbits=0, int duration_nbits=8);
@@ -221,7 +222,7 @@
 
     //! Returns the number of sequences in the training_set
     int nSequences() const
-    { return boundaries.length(); }
+    { return trainset_boundaries.length(); }
 
     //! Returns the ith sequence
     void getSequence(int i, Mat& seq) const;
@@ -340,10 +341,10 @@
     mutable Vec dynamic_act_no_bias_contribution;
 
     TVec<int> trainset_boundaries;
-    TVec<int> testset_boundaries;
+    mutable TVec<int> testset_boundaries;
 
-    Mat seq; // contains the current train or test sequence
-    Mat encoded_seq; // contains encoded version of current train or test sequence
+    mutable Mat seq; // contains the current train or test sequence
+    mutable Mat encoded_seq; // contains encoded version of current train or test sequence
 
 protected:
     //#####  Protected Member Functions  ######################################
@@ -357,15 +358,21 @@
     //! This does the actual building.
     void build_();
 
+    // note: the following functions are declared const because they have
+    // to be called by test (which is const). Similarly, the members they 
+    // manipulate are all declared mutable.
+    void fprop(Vec train_costs, Vec train_n_items) const;
+
     //! does encoding if needed and populates the list.
-    void encodeSequenceAndPopulateLists(Mat seq);
+    void encodeSequenceAndPopulateLists(Mat seq) const;
 
     //! encodes seq, then populates: inputslist, targets_list, masks_list
-    void encodeAndCreateSupervisedSequence(Mat seq);
+    void encodeAndCreateSupervisedSequence(Mat seq) const;
 
     //! For the (backward testing) raw_masked_supervised case. Populates: input_list, targets_list, masks_list
-    void splitRawMaskedSupervisedSequence(Mat seq);
+    void splitRawMaskedSupervisedSequence(Mat seq) const;
 
+    void resize_lists(int l) const;
 
 private:
     //#####  Private Data Members  ############################################



From chrish at mail.berlios.de  Fri Jun 27 17:33:38 2008
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Fri, 27 Jun 2008 17:33:38 +0200
Subject: [Plearn-commits] r9187 - in trunk: plearn/var plearn/vmat
	plearn_learners/generic
Message-ID: <200806271533.m5RFXcaS017903@sheep.berlios.de>

Author: chrish
Date: 2008-06-27 17:33:36 +0200 (Fri, 27 Jun 2008)
New Revision: 9187

Modified:
   trunk/plearn/var/VarArray.cc
   trunk/plearn/vmat/ProcessDatasetVMatrix.cc
   trunk/plearn_learners/generic/AddCostToLearner.cc
Log:
Some more format string bugfixes.

Modified: trunk/plearn/var/VarArray.cc
===================================================================
--- trunk/plearn/var/VarArray.cc	2008-06-27 15:12:48 UTC (rev 9186)
+++ trunk/plearn/var/VarArray.cc	2008-06-27 15:33:36 UTC (rev 9187)
@@ -313,7 +313,7 @@
             real* value = v->gradientdata;
             int vlength = v->nelems();
             if(vlength!=n)
-                PLERROR("IN VarArray::copyGradientFrom length of -th Var in the array differs from length of %d-th argument",i,i);
+                PLERROR("IN VarArray::copyGradientFrom length of -th Var in the array differs from length of %d-th argument",i);
             for(int j=0; j<vlength; j++)
                 value[j] = *data++;
         }
@@ -333,7 +333,7 @@
             real* value = v->gradientdata;
             int vlength = v->nelems();
             if(vlength!=n)
-                PLERROR("IN VarArray::copyGradientFrom length of -th Var in the array differs from length of %d-th argument",i,i);
+                PLERROR("IN VarArray::copyGradientFrom length of -th Var in the array differs from length of %d-th argument",i);
             for(int j=0; j<vlength; j++)
                 *data++ = value[j];
         }

Modified: trunk/plearn/vmat/ProcessDatasetVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ProcessDatasetVMatrix.cc	2008-06-27 15:12:48 UTC (rev 9186)
+++ trunk/plearn/vmat/ProcessDatasetVMatrix.cc	2008-06-27 15:33:36 UTC (rev 9187)
@@ -219,7 +219,7 @@
             vm = old_vm;
     } else
         PLERROR("In ProcessDatasetVMatrix::build_ - Unknown value for the "
-                "'precompute' option: ", precompute.c_str());
+                "'precompute' option: '%s'", precompute.c_str());
 
     inherited::build();
 }

Modified: trunk/plearn_learners/generic/AddCostToLearner.cc
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.cc	2008-06-27 15:12:48 UTC (rev 9186)
+++ trunk/plearn_learners/generic/AddCostToLearner.cc	2008-06-27 15:33:36 UTC (rev 9187)
@@ -572,7 +572,7 @@
 #ifdef BOUNDCHECK
             if (sub_learner_out >= n_classes
                 || is_missing(sub_learner_out))
-                PLERROR("In AddCostToLearner::computeCostsFromOutputs - bad output value of sub_learner: sub_learner_out=%f,  "
+                PLERROR("In AddCostToLearner::computeCostsFromOutputs - bad output value of sub_learner: sub_learner_out=%d,  "
                         " missing or higher or egual to n_classes (%d)",
                         sub_learner_out,n_classes);
             if (the_target >= n_classes



From plearner at mail.berlios.de  Fri Jun 27 20:34:02 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Fri, 27 Jun 2008 20:34:02 +0200
Subject: [Plearn-commits] r9188 - trunk/plearn_learners_experimental
Message-ID: <200806271834.m5RIY2ts021602@sheep.berlios.de>

Author: plearner
Date: 2008-06-27 20:34:01 +0200 (Fri, 27 Jun 2008)
New Revision: 9188

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
Log:


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-06-27 15:33:36 UTC (rev 9187)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-06-27 18:34:01 UTC (rev 9188)
@@ -72,11 +72,10 @@
 
 
 DenoisingRecurrentNet::DenoisingRecurrentNet() :
-    //rbm_learning_rate( 0.01 ),
     recurrent_net_learning_rate( 0.01),
     use_target_layers_masks( false ),
-    end_of_sequence_symbol( -1000 )
-    //rbm_nstages( 0 ),
+    end_of_sequence_symbol( -1000 ),
+    encoding("raw_masked_supervised")
 {
     random_gen = new PRandom();
 }
@@ -153,12 +152,11 @@
                   OptionBase::buildoption,
                   "The RBMConnection from input_layer to hidden_layer.\n");
 
-    /*
-    declareOption(ol, "", 
-                  &DenoisingRecurrentNet::,
+    declareOption(ol, "encoding", 
+                  &DenoisingRecurrentNet::encoding,
                   OptionBase::buildoption,
-                  "");
-    */
+                  "Chooses what type of encoding to apply to an input sequence\n"
+                  "Possibilities: timeframe, note_duration, note_octav_duration, raw_masked_supervised");
 
 
     declareOption(ol, "target_layers_n_of_target_elements", 
@@ -667,40 +665,26 @@
             if( !fast_exact_is_equal(target_layers_weights[tar],0) )
             {
                 Vec target_prediction_i = target_prediction_list[tar](i);
-                Vec target_prediction_act_no_bias_i = target_prediction_act_no_bias_i;
+                Vec target_prediction_act_no_bias_i = target_prediction_act_no_bias_list[tar](i);
                 target_connections[tar]->fprop(last_hidden, target_prediction_act_no_bias_i);
                 target_layers[tar]->fprop(target_prediction_act_no_bias_i, target_prediction_i);
                 if( use_target_layers_masks )
                     target_prediction_i *= masks_list[tar](i);
-            }
-        }
 
-        int sum_target_elements = 0;
-        for( int tar=0; tar < ntargets; tar++ )
-        {
-            if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-            {
-                target_layers[tar]->activation << target_prediction_act_no_bias_list[tar](i);
+                target_layers[tar]->activation << target_prediction_act_no_bias_i;
                 target_layers[tar]->activation += target_layers[tar]->bias;
-                target_layers[tar]->setExpectation(target_prediction_list[tar](i));
+                target_layers[tar]->setExpectation(target_prediction_i);
+
                 Vec target_vec = targets_list[tar](i);
                 nll_list(i,tar) = target_layers[tar]->fpropNLL(target_vec); 
                 train_costs[tar] += nll_list(i,tar);
-                        
+
                 // Normalize by the number of things to predict
                 if( use_target_layers_masks )
-                {
-                    train_n_items[tar] += sum(
-                        input.subVec( inputsize_without_masks 
-                                      + sum_target_elements, 
-                                      target_layers_n_of_target_elements[tar]) );
-                }
+                    train_n_items[tar] += sum(masks_list[tar](i));
                 else
                     train_n_items[tar]++;
             }
-            if( use_target_layers_masks )
-                sum_target_elements += target_layers_n_of_target_elements[tar];
-                    
         }
     }
 }
@@ -1541,8 +1525,8 @@
 {
     int len = testset.length();
 
-    //Vec output(outputsize());
-    //output.clear();
+    Vec output(outputsize());
+    output.clear();
 
     Vec costs(nTestCosts());
     costs.clear();
@@ -1596,6 +1580,8 @@
                 }
                 testoutputs->putOrAppendRow(pos++, output);
             }
+            output.fill(end_of_sequence_symbol);
+            testoutputs->putOrAppendRow(pos++, output);
         }
         else
             pos += seqlen;
@@ -1615,7 +1601,7 @@
         testcosts->putOrAppendRow(0, costs);
     
     if (test_stats)
-        test_stats->update(costs, weight);
+        test_stats->update(costs, 1.);
 }
 
 /*



From plearner at mail.berlios.de  Fri Jun 27 21:07:14 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Fri, 27 Jun 2008 21:07:14 +0200
Subject: [Plearn-commits] r9189 - trunk/plearn_learners_experimental
Message-ID: <200806271907.m5RJ7E2U024211@sheep.berlios.de>

Author: plearner
Date: 2008-06-27 21:07:14 +0200 (Fri, 27 Jun 2008)
New Revision: 9189

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
Log:


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-06-27 18:34:01 UTC (rev 9188)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-06-27 19:07:14 UTC (rev 9189)
@@ -75,7 +75,8 @@
     recurrent_net_learning_rate( 0.01),
     use_target_layers_masks( false ),
     end_of_sequence_symbol( -1000 ),
-    encoding("raw_masked_supervised")
+    encoding("raw_masked_supervised"),
+    input_window_size(1)
 {
     random_gen = new PRandom();
 }
@@ -158,6 +159,12 @@
                   "Chooses what type of encoding to apply to an input sequence\n"
                   "Possibilities: timeframe, note_duration, note_octav_duration, raw_masked_supervised");
 
+    declareOption(ol, "input_window_size", 
+                  &DenoisingRecurrentNet::input_window_size,
+                  OptionBase::buildoption,
+                  "How many time steps to present as input\n"
+                  "This option is ignored when mode is raw_masked_supervised,"
+                  "since in this mode the full expanded and preprocessed input and target are given explicitly.");
 
     declareOption(ol, "target_layers_n_of_target_elements", 
                   &DenoisingRecurrentNet::target_layers_n_of_target_elements,



