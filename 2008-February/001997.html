<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r8549 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2008-February/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8549%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200802202010.m1KKAYAp002959%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001996.html">
   <LINK REL="Next"  HREF="001998.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r8549 - trunk/plearn_learners/online</H1>
    <B>larocheh at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8549%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200802202010.m1KKAYAp002959%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r8549 - trunk/plearn_learners/online">larocheh at mail.berlios.de
       </A><BR>
    <I>Wed Feb 20 21:10:34 CET 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="001996.html">[Plearn-commits] r8548 - trunk/plearn_learners/online
</A></li>
        <LI>Next message: <A HREF="001998.html">[Plearn-commits] r8550 - trunk/plearn_learners/distributions
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1997">[ date ]</a>
              <a href="thread.html#1997">[ thread ]</a>
              <a href="subject.html#1997">[ subject ]</a>
              <a href="author.html#1997">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: larocheh
Date: 2008-02-20 21:10:33 +0100 (Wed, 20 Feb 2008)
New Revision: 8549

Added:
   trunk/plearn_learners/online/RBMWoodsLayer.cc
   trunk/plearn_learners/online/RBMWoodsLayer.h
Log:
RBM layer with tree structured groups of neurons.


Added: trunk/plearn_learners/online/RBMWoodsLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMWoodsLayer.cc	2008-02-20 20:09:16 UTC (rev 8548)
+++ trunk/plearn_learners/online/RBMWoodsLayer.cc	2008-02-20 20:10:33 UTC (rev 8549)
@@ -0,0 +1,1193 @@
+// -*- C++ -*-
+
+// RBMWoodsLayer.cc
+//
+// Copyright (C) 2008 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file RBMWoodsLayer.cc */
+
+
+
+#include &quot;RBMWoodsLayer.h&quot;
+#include &lt;plearn/math/TMat_maths.h&gt;
+#include &quot;RBMConnection.h&quot;
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    RBMWoodsLayer,
+    &quot;RBM layer with tree-structured groups of units.&quot;,
+    &quot;&quot;);
+
+RBMWoodsLayer::RBMWoodsLayer( real the_learning_rate ) :
+    inherited( the_learning_rate ),
+    n_trees( 10 ),
+    tree_depth( 3 )
+{
+}
+
+////////////////////
+// generateSample //
+////////////////////
+void RBMWoodsLayer::generateSample()
+{
+    PLASSERT_MSG(random_gen,
+                 &quot;random_gen should be initialized before generating samples&quot;);
+
+    PLCHECK_MSG(expectation_is_up_to_date, &quot;Expectation should be computed &quot;
+            &quot;before calling generateSample()&quot;);
+
+    sample.clear();
+
+    int n_nodes_per_tree = size / n_trees;    
+    int node, depth, node_sample, sub_tree_size;
+    int offset = 0;
+
+    for( int t=0; t&lt;n_trees; t++ )
+    {
+        depth = 0;
+        node = n_nodes_per_tree / 2;
+        sub_tree_size = node;
+        while( depth &lt; tree_depth )
+        {
+            node_sample = random_gen-&gt;binomial_sample( 
+                local_node_expectation[ node + offset ] );
+            sample[node + offset] = node_sample;
+
+            // Descending in the tree
+            sub_tree_size /= 2;
+            if ( node_sample &gt; 0.5 )
+                node -= sub_tree_size+1;
+            else
+                node += sub_tree_size+1;
+            depth++;
+        }
+        offset += n_nodes_per_tree;
+    }
+}
+
+/////////////////////
+// generateSamples //
+/////////////////////
+void RBMWoodsLayer::generateSamples()
+{
+    PLASSERT_MSG(random_gen,
+                 &quot;random_gen should be initialized before generating samples&quot;);
+
+    PLCHECK_MSG(expectations_are_up_to_date, &quot;Expectations should be computed &quot;
+            &quot;before calling generateSamples()&quot;);
+
+    PLASSERT( samples.width() == size &amp;&amp; samples.length() == batch_size );
+
+    PLERROR( &quot;RBMWoodsLayer::generateSamples(): not implemented yet&quot; );
+}
+
+void RBMWoodsLayer::computeProbabilisticClustering(Vec&amp; prob_clusters)
+{
+    int n_nodes_per_tree = size / n_trees;    
+    int n_leaves = n_nodes_per_tree+1;
+    prob_clusters.resize( n_trees * n_leaves );
+    int node, depth, sub_tree_size, grand_parent;
+    int offset = 0;
+    bool left_of_grand_parent;
+    real grand_parent_prob;
+
+    // Get local expectations at every node
+    
+    // Divide and conquer computation of local (conditional) free energies
+    for( int t=0; t&lt;n_trees; t++ )
+    {
+        depth = tree_depth-1;
+        sub_tree_size = 0;
+
+        // Initialize last level
+        for( int n=sub_tree_size; n&lt;n_nodes_per_tree; n += 2*sub_tree_size + 2 )
+        {
+            //on_free_energy[ n + offset ] = safeexp(activation[n+offset]);
+            //off_free_energy[ n + offset ] = 1;
+            // Now working in log-domain
+            on_free_energy[ n + offset ] = activation[n+offset];
+            off_free_energy[ n + offset ] = 0;
+            
+        }
+
+        depth = tree_depth-2;
+        sub_tree_size = 1;
+
+        while( depth &gt;= 0 )
+        {
+            for( int n=sub_tree_size; n&lt;n_nodes_per_tree; n += 2*sub_tree_size + 2 )
+            {
+                //on_free_energy[ n + offset ] = safeexp(activation[n+offset]) * 
+                //    ( on_free_energy[n + offset - sub_tree_size] + off_free_energy[n + offset - sub_tree_size] ) ;
+                //off_free_energy[ n + offset ] = 
+                //    ( on_free_energy[n + offset + sub_tree_size] + off_free_energy[n + offset + sub_tree_size] ) ;
+                // Now working in log-domain
+                on_free_energy[ n + offset ] = activation[n+offset] + 
+                    logadd( on_free_energy[n + offset - sub_tree_size],
+                            off_free_energy[n + offset - sub_tree_size] ) ;
+                off_free_energy[ n + offset ] = 
+                    logadd( on_free_energy[n + offset + sub_tree_size],
+                            off_free_energy[n + offset + sub_tree_size] ) ;
+
+            }
+            sub_tree_size = 2 * ( sub_tree_size + 1 ) - 1;
+            depth--;
+        }
+        offset += n_nodes_per_tree;
+    }    
+    
+    for( int i=0 ; i&lt;size ; i++ )
+        //local_node_expectation[i] = on_free_energy[i] / ( on_free_energy[i] + off_free_energy[i] );
+        // Now working in log-domain
+        local_node_expectation[i] = safeexp(on_free_energy[i] 
+                                            - logadd(on_free_energy[i], off_free_energy[i]));
+
+    // Compute marginal expectations over clustering
+    offset = 0;
+    for( int t=0; t&lt;n_trees; t++ )
+    {
+        // Initialize root        
+        node = n_nodes_per_tree / 2;
+        expectation[ node + offset ] = local_node_expectation[ node + offset ];
+        off_expectation[ node + offset ] = (1 - local_node_expectation[ node + offset ]);
+        sub_tree_size = node;
+
+        // First level nodes
+        depth = 1;
+        sub_tree_size /= 2;
+
+        // Left child
+        node = sub_tree_size;
+        expectation[ node + offset ] = local_node_expectation[ node + offset ]
+            * local_node_expectation[ node + offset + sub_tree_size + 1 ];
+        off_expectation[ node + offset ] = (1 - local_node_expectation[ node + offset ])
+            * local_node_expectation[ node + offset + sub_tree_size + 1 ];
+        
+        // Right child
+        node = 3*sub_tree_size+2;
+        expectation[ node + offset ] = local_node_expectation[ node + offset ]
+            * (1 - local_node_expectation[ node + offset - sub_tree_size - 1 ]);
+        off_expectation[ node + offset ] = (1 - local_node_expectation[ node + offset ])
+            * (1 - local_node_expectation[ node + offset - sub_tree_size - 1 ]);
+
+        // Set other nodes, level-wise
+        depth = 2;
+        sub_tree_size /= 2;
+        while( depth &lt; tree_depth )
+        {
+            // Left child
+            left_of_grand_parent = true;
+            for( int n=sub_tree_size; n&lt;n_nodes_per_tree; n += 4*sub_tree_size + 4 )
+            {
+                if( left_of_grand_parent )
+                {
+                    grand_parent = n + offset + 3*sub_tree_size + 3;
+                    grand_parent_prob = expectation[ grand_parent ];
+                    left_of_grand_parent = false;
+                }
+                else
+                {
+                    grand_parent = n + offset - sub_tree_size - 1;
+                    grand_parent_prob = off_expectation[ grand_parent ];
+                    left_of_grand_parent = true;
+                }
+
+                expectation[ n + offset ] = local_node_expectation[ n + offset ]
+                    * local_node_expectation[ n + offset + sub_tree_size + 1 ]
+                    * grand_parent_prob;
+                off_expectation[ n + offset ] = (1 - local_node_expectation[ n + offset ])
+                    * local_node_expectation[ n + offset + sub_tree_size + 1 ]
+                    * grand_parent_prob;
+            }
+
+            // Right child
+            left_of_grand_parent = true;
+            for( int n=3*sub_tree_size+2; n&lt;n_nodes_per_tree; n += 4*sub_tree_size + 4 )
+            {
+                if( left_of_grand_parent )
+                {
+                    grand_parent = n + offset + sub_tree_size + 1;
+                    grand_parent_prob = expectation[ grand_parent ];
+                    left_of_grand_parent = false;
+                }
+                else
+                {
+                    grand_parent = n + offset - 3*sub_tree_size - 3;
+                    grand_parent_prob = off_expectation[ grand_parent ];
+                    left_of_grand_parent = true;
+                }
+
+                expectation[ n + offset ] = local_node_expectation[ n + offset ]
+                    * (1 - local_node_expectation[ n + offset - sub_tree_size - 1 ])
+                    * grand_parent_prob;
+                off_expectation[ n + offset ] = (1 - local_node_expectation[ n + offset ])
+                    * (1 - local_node_expectation[ n + offset - sub_tree_size - 1 ])
+                    * grand_parent_prob;
+            }
+            sub_tree_size /= 2;
+            depth++;
+        }
+        offset += n_nodes_per_tree;
+    }
+
+    offset = 0;
+    for( int t=0; t&lt;n_trees; t++ )
+    {
+        for( int i=0; i&lt;n_nodes_per_tree; i = i+2)
+            prob_clusters[i+offset+t] = expectation[i+offset];
+        for( int i=0; i&lt;n_nodes_per_tree; i = i+2)
+            prob_clusters[i+1+offset+t] = off_expectation[i+offset];
+        offset += n_nodes_per_tree;
+    }
+}
+
+////////////////////////
+// computeExpectation //
+////////////////////////
+void RBMWoodsLayer::computeExpectation()
+{
+    if( expectation_is_up_to_date )
+        return;
+
+    int n_nodes_per_tree = size / n_trees;    
+    int node, depth, sub_tree_size, grand_parent;
+    int offset = 0;
+    bool left_of_grand_parent;
+    real grand_parent_prob;
+
+    // Get local expectations at every node
+    
+    // Divide and conquer computation of local (conditional) free energies
+    for( int t=0; t&lt;n_trees; t++ )
+    {
+        depth = tree_depth-1;
+        sub_tree_size = 0;
+
+        // Initialize last level
+        for( int n=sub_tree_size; n&lt;n_nodes_per_tree; n += 2*sub_tree_size + 2 )
+        {
+            //on_free_energy[ n + offset ] = safeexp(activation[n+offset]);
+            //off_free_energy[ n + offset ] = 1;
+            // Now working in log-domain
+            on_free_energy[ n + offset ] = activation[n+offset];
+            off_free_energy[ n + offset ] = 0;
+            
+        }
+
+        depth = tree_depth-2;
+        sub_tree_size = 1;
+
+        while( depth &gt;= 0 )
+        {
+            for( int n=sub_tree_size; n&lt;n_nodes_per_tree; n += 2*sub_tree_size + 2 )
+            {
+                //on_free_energy[ n + offset ] = safeexp(activation[n+offset]) * 
+                //    ( on_free_energy[n + offset - sub_tree_size] + off_free_energy[n + offset - sub_tree_size] ) ;
+                //off_free_energy[ n + offset ] = 
+                //    ( on_free_energy[n + offset + sub_tree_size] + off_free_energy[n + offset + sub_tree_size] ) ;
+                // Now working in log-domain
+                on_free_energy[ n + offset ] = activation[n+offset] + 
+                    logadd( on_free_energy[n + offset - sub_tree_size],
+                            off_free_energy[n + offset - sub_tree_size] ) ;
+                off_free_energy[ n + offset ] = 
+                    logadd( on_free_energy[n + offset + sub_tree_size],
+                            off_free_energy[n + offset + sub_tree_size] ) ;
+
+            }
+            sub_tree_size = 2 * ( sub_tree_size + 1 ) - 1;
+            depth--;
+        }
+        offset += n_nodes_per_tree;
+    }    
+    
+    for( int i=0 ; i&lt;size ; i++ )
+        //local_node_expectation[i] = on_free_energy[i] / ( on_free_energy[i] + off_free_energy[i] );
+        // Now working in log-domain
+        local_node_expectation[i] = safeexp(on_free_energy[i] 
+                                            - logadd(on_free_energy[i], off_free_energy[i]));
+
+    // Compute marginal expectations
+    offset = 0;
+    for( int t=0; t&lt;n_trees; t++ )
+    {
+        // Initialize root        
+        node = n_nodes_per_tree / 2;
+        expectation[ node + offset ] = local_node_expectation[ node + offset ];
+        off_expectation[ node + offset ] = (1 - local_node_expectation[ node + offset ]);
+        sub_tree_size = node;
+
+        // First level nodes
+        depth = 1;
+        sub_tree_size /= 2;
+
+        // Left child
+        node = sub_tree_size;
+        expectation[ node + offset ] = local_node_expectation[ node + offset ]
+            * local_node_expectation[ node + offset + sub_tree_size + 1 ];
+        off_expectation[ node + offset ] = (1 - local_node_expectation[ node + offset ])
+            * local_node_expectation[ node + offset + sub_tree_size + 1 ];
+
+        // Right child
+        node = 3*sub_tree_size+2;
+        expectation[ node + offset ] = local_node_expectation[ node + offset ]
+            * (1 - local_node_expectation[ node + offset - sub_tree_size - 1 ]);
+        off_expectation[ node + offset ] = (1 - local_node_expectation[ node + offset ])
+            * (1 - local_node_expectation[ node + offset - sub_tree_size - 1 ]);
+
+        // Set other nodes, level-wise
+        depth = 2;
+        sub_tree_size /= 2;
+        while( depth &lt; tree_depth )
+        {
+            // Left child
+            left_of_grand_parent = true;
+            for( int n=sub_tree_size; n&lt;n_nodes_per_tree; n += 4*sub_tree_size + 4 )
+            {
+                if( left_of_grand_parent )
+                {
+                    grand_parent = n + offset + 3*sub_tree_size + 3;
+                    grand_parent_prob = expectation[ grand_parent ];
+                    left_of_grand_parent = false;
+                }
+                else
+                {
+                    grand_parent = n + offset - sub_tree_size - 1;
+                    grand_parent_prob = off_expectation[ grand_parent ];
+                    left_of_grand_parent = true;
+                }
+
+                expectation[ n + offset ] = local_node_expectation[ n + offset ]
+                    * local_node_expectation[ n + offset + sub_tree_size + 1 ]
+                    * grand_parent_prob;
+                off_expectation[ n + offset ] = (1 - local_node_expectation[ n + offset ])
+                    * local_node_expectation[ n + offset + sub_tree_size + 1 ]
+                    * grand_parent_prob;
+
+            }
+
+            // Right child
+            left_of_grand_parent = true;
+            for( int n=3*sub_tree_size+2; n&lt;n_nodes_per_tree; n += 4*sub_tree_size + 4 )
+            {
+                if( left_of_grand_parent )
+                {
+                    grand_parent = n + offset + sub_tree_size + 1;
+                    grand_parent_prob = expectation[ grand_parent ];
+                    left_of_grand_parent = false;
+                }
+                else
+                {
+                    grand_parent = n + offset - 3*sub_tree_size - 3;
+                    grand_parent_prob = off_expectation[ grand_parent ];
+                    left_of_grand_parent = true;
+                }
+
+                expectation[ n + offset ] = local_node_expectation[ n + offset ]
+                    * (1 - local_node_expectation[ n + offset - sub_tree_size - 1 ])
+                    * grand_parent_prob;
+                off_expectation[ n + offset ] = (1 - local_node_expectation[ n + offset ])
+                    * (1 - local_node_expectation[ n + offset - sub_tree_size - 1 ])
+                    * grand_parent_prob;
+            }
+            sub_tree_size /= 2;
+            depth++;
+        }
+        offset += n_nodes_per_tree;
+    }
+
+    expectation_is_up_to_date = true;
+}
+
+/////////////////////////
+// computeExpectations //
+/////////////////////////
+void RBMWoodsLayer::computeExpectations()
+{
+    if( expectations_are_up_to_date )
+        return;
+
+    PLERROR( &quot;RBMWoodsLayer::computeExpectations(): not implemented yet&quot; );
+
+    expectations_are_up_to_date = true;
+}
+
+///////////
+// fprop //
+///////////
+void RBMWoodsLayer::fprop( const Vec&amp; input, Vec&amp; output ) const
+{
+    PLASSERT( input.size() == input_size );
+    output.resize( output_size );
+
+    int n_nodes_per_tree = size / n_trees;    
+    int node, depth, sub_tree_size, grand_parent;
+    int offset = 0;    
+    bool left_of_grand_parent;
+    real grand_parent_prob;
+
+    // Get local expectations at every node
+    
+    // Divide and conquer computation of local (conditional) free energies
+    for( int t=0; t&lt;n_trees; t++ )
+    {
+        depth = tree_depth-1;
+        sub_tree_size = 0;
+
+        // Initialize last level
+        for( int n=sub_tree_size; n&lt;n_nodes_per_tree; n += 2*sub_tree_size + 2 )
+        {
+            //on_free_energy[ n + offset ] = safeexp(input[n+offset] + bias[n+offset]);
+            //off_free_energy[ n + offset ] = 1;
+            // Now working in log-domain
+            on_free_energy[ n + offset ] = input[n+offset] + bias[n+offset];
+            off_free_energy[ n + offset ] = 0;            
+        }
+
+        depth = tree_depth-2;
+        sub_tree_size = 1;
+
+        while( depth &gt;= 0 )
+        {
+            for( int n=sub_tree_size; n&lt;n_nodes_per_tree; n += 2*sub_tree_size + 2 )
+            {
+                //on_free_energy[ n + offset ] = safeexp(input[n+offset] + bias[n+offset]) * 
+                //    ( on_free_energy[n + offset - sub_tree_size] + off_free_energy[n + offset - sub_tree_size] ) ;
+                //off_free_energy[ n + offset ] = 
+                //    ( on_free_energy[n + offset + sub_tree_size] + off_free_energy[n + offset + sub_tree_size] ) ;
+                // Now working in the log-domain
+                on_free_energy[ n + offset ] = input[n+offset] + bias[n+offset] +
+                    logadd( on_free_energy[n + offset - sub_tree_size], 
+                            off_free_energy[n + offset - sub_tree_size] ) ;
+                off_free_energy[ n + offset ] = 
+                    logadd( on_free_energy[n + offset + sub_tree_size], 
+                            off_free_energy[n + offset + sub_tree_size] ) ;
+            }
+            sub_tree_size = 2 * ( sub_tree_size + 1 ) - 1;
+            depth--;
+        }
+        offset += n_nodes_per_tree;
+    }    
+    
+    for( int i=0 ; i&lt;size ; i++ )
+        //local_node_expectation[i] = on_free_energy[i] / ( on_free_energy[i] + off_free_energy[i] );
+        // Now working in log-domain
+        local_node_expectation[i] = safeexp(on_free_energy[i] 
+                                            - logadd(on_free_energy[i], off_free_energy[i]));
+
+    // Compute marginal expectations
+    offset = 0;    
+    for( int t=0; t&lt;n_trees; t++ )
+    {
+        // Initialize root        
+        node = n_nodes_per_tree / 2;
+        output[ node + offset ] = local_node_expectation[ node + offset ];
+        off_expectation[ node + offset ] = (1 - local_node_expectation[ node + offset ]);
+        sub_tree_size = node;
+
+        // First level nodes
+        depth = 1;
+        sub_tree_size /= 2;
+
+        // Left child
+        node = sub_tree_size;
+        output[ node + offset ] = local_node_expectation[ node + offset ]
+            * local_node_expectation[ node + offset + sub_tree_size + 1 ];
+        off_expectation[ node + offset ] = (1 - local_node_expectation[ node + offset ])
+            * local_node_expectation[ node + offset + sub_tree_size + 1 ];
+
+        // Right child
+        node = 3*sub_tree_size+2;
+        output[ node + offset ] = local_node_expectation[ node + offset ]
+            * (1 - local_node_expectation[ node + offset - sub_tree_size - 1 ]);
+        off_expectation[ node + offset ] = (1 - local_node_expectation[ node + offset ])
+            * (1 - local_node_expectation[ node + offset - sub_tree_size - 1 ]);
+
+        // Set other nodes, level-wise
+        depth = 2;
+        sub_tree_size /= 2;
+        while( depth &lt; tree_depth )
+        {
+            // Left child
+            left_of_grand_parent = true;
+            for( int n=sub_tree_size; n&lt;n_nodes_per_tree; n += 4*sub_tree_size + 4 )
+            {
+                if( left_of_grand_parent )
+                {
+                    grand_parent = n + offset + 3*sub_tree_size + 3;
+                    grand_parent_prob = output[ grand_parent ];
+                    left_of_grand_parent = false;
+                }
+                else
+                {
+                    grand_parent = n + offset - sub_tree_size - 1;
+                    grand_parent_prob = off_expectation[ grand_parent ];
+                    left_of_grand_parent = true;
+                }
+
+                output[ n + offset ] = local_node_expectation[ n + offset ]
+                    * local_node_expectation[ n + offset + sub_tree_size + 1 ]
+                    * grand_parent_prob;
+                off_expectation[ n + offset ] = (1 - local_node_expectation[ n + offset ])
+                    * local_node_expectation[ n + offset + sub_tree_size + 1 ]
+                    * grand_parent_prob;
+            }
+
+            // Right child
+            left_of_grand_parent = true;
+            for( int n=3*sub_tree_size+2; n&lt;n_nodes_per_tree; n += 4*sub_tree_size + 4 )
+            {
+                if( left_of_grand_parent )
+                {
+                    grand_parent = n + offset + sub_tree_size + 1;
+                    grand_parent_prob = output[ grand_parent ];
+                    left_of_grand_parent = false;
+                }
+                else
+                {
+                    grand_parent = n + offset - 3*sub_tree_size - 3;
+                    grand_parent_prob = off_expectation[ grand_parent ];
+                    left_of_grand_parent = true;
+                }
+
+                output[ n + offset ] = local_node_expectation[ n + offset ]
+                    * (1 - local_node_expectation[ n + offset - sub_tree_size - 1 ])
+                    * grand_parent_prob;
+                off_expectation[ n + offset ] = (1 - local_node_expectation[ n + offset ])
+                    * (1 - local_node_expectation[ n + offset - sub_tree_size - 1 ])
+                    * grand_parent_prob;
+            }
+            sub_tree_size /= 2;
+            depth++;
+        }
+        offset += n_nodes_per_tree;
+    }
+}
+
+void RBMWoodsLayer::fprop( const Mat&amp; inputs, Mat&amp; outputs ) const
+{
+    int mbatch_size = inputs.length();
+    PLASSERT( inputs.width() == size );
+    outputs.resize( mbatch_size, size );
+
+    PLERROR( &quot;RBMWoodsLayer::fprop(): not implemented yet&quot; );
+}
+
+void RBMWoodsLayer::fprop( const Vec&amp; input, const Vec&amp; rbm_bias,
+                              Vec&amp; output ) const
+{
+    PLASSERT( input.size() == input_size );
+    PLASSERT( rbm_bias.size() == input_size );
+    output.resize( output_size );
+
+    PLERROR( &quot;RBMWoodsLayer::fprop(): not implemented yet&quot; );
+}
+
+/////////////////
+// bpropUpdate //
+/////////////////
+void RBMWoodsLayer::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                                   Vec&amp; input_gradient,
+                                   const Vec&amp; output_gradient,
+                                   bool accumulate)
+{
+    PLASSERT( input.size() == size );
+    PLASSERT( output.size() == size );
+    PLASSERT( output_gradient.size() == size );
+
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+    }
+    else
+    {
+        input_gradient.resize( size );
+        input_gradient.clear();
+    }
+
+    // Compute gradient on marginal expectations
+    int n_nodes_per_tree = size / n_trees;    
+    int node, depth, sub_tree_size, grand_parent;
+    int offset = 0;
+    bool left_of_grand_parent;
+    real grand_parent_prob;
+    real node_exp, parent_exp, out_grad, off_grad;
+    local_node_expectation_gradient.clear();
+    on_tree_gradient.clear();
+    off_tree_gradient.clear();
+
+    for( int t=0; t&lt;n_trees; t++ )
+    {
+        // Set other nodes, level-wise
+        depth = tree_depth-1;
+        sub_tree_size = 0;
+        while( depth &gt; 1 )
+        {
+            // Left child
+            left_of_grand_parent = true;
+            for( int n=sub_tree_size; n&lt;n_nodes_per_tree; n += 4*sub_tree_size + 4 )
+            {
+                out_grad = output_gradient[ n + offset ] + 
+                    on_tree_gradient[ n + offset ] ;
+                off_grad = off_tree_gradient[ n + offset ] ;
+                node_exp = local_node_expectation[ n + offset ];
+                parent_exp = local_node_expectation[ n + offset + sub_tree_size + 1 ];
+
+                if( left_of_grand_parent )
+                {
+                    grand_parent = n + offset + 3*sub_tree_size + 3;
+                    grand_parent_prob = output[ grand_parent ];
+                    // Gradient for rest of the tree
+                    on_tree_gradient[ grand_parent ] += 
+                        ( out_grad * node_exp 
+                          + off_grad * (1 - node_exp) ) 
+                        * parent_exp;
+                    left_of_grand_parent = false;
+                }
+                else
+                {
+                    grand_parent = n + offset - sub_tree_size - 1;
+                    grand_parent_prob = off_expectation[ grand_parent ];
+                    // Gradient for rest of the tree
+                    off_tree_gradient[ grand_parent ] += 
+                        ( out_grad * node_exp 
+                          + off_grad * (1 - node_exp) )
+                        * parent_exp;
+                    left_of_grand_parent = true;
+                }
+
+                // Gradient w/r current node
+                local_node_expectation_gradient[ n + offset ] += 
+                    ( out_grad - off_grad ) * parent_exp * grand_parent_prob
+                    * node_exp * ( 1 - node_exp );
+
+                // Gradient w/r parent node
+                local_node_expectation_gradient[ n + offset + sub_tree_size + 1 ] += 
+                        ( out_grad * node_exp + off_grad * (1 - node_exp) )  * grand_parent_prob
+                        * parent_exp * (1-parent_exp) ;
+
+            }
+
+            // Right child
+            left_of_grand_parent = true;
+            for( int n=3*sub_tree_size+2; n&lt;n_nodes_per_tree; n += 4*sub_tree_size + 4 )
+            {
+                out_grad = output_gradient[ n + offset ] + 
+                    on_tree_gradient[ n + offset ] ;
+                off_grad = off_tree_gradient[ n + offset ] ;
+                node_exp = local_node_expectation[ n + offset ];
+                parent_exp = local_node_expectation[ n + offset - sub_tree_size - 1 ];
+
+                if( left_of_grand_parent )
+                {
+                    grand_parent = n + offset + sub_tree_size + 1;
+                    grand_parent_prob = output[ grand_parent ];
+                    // Gradient for rest of the tree
+                    on_tree_gradient[ grand_parent ] += 
+                        ( out_grad * node_exp 
+                          + off_grad * (1 - node_exp) ) 
+                        * ( 1 - parent_exp );
+                    left_of_grand_parent = false;
+                }
+                else
+                {
+                    grand_parent = n + offset - 3*sub_tree_size - 3;
+                    grand_parent_prob = off_expectation[ grand_parent ];
+                    // Gradient for rest of the tree
+                    off_tree_gradient[ grand_parent ] += 
+                        ( out_grad * node_exp 
+                          + off_grad * (1 - node_exp) ) 
+                        * ( 1 - parent_exp );
+                    left_of_grand_parent = true;
+                }
+
+                // Gradient w/r current node
+                local_node_expectation_gradient[ n + offset ] += 
+                    ( out_grad - off_grad ) * ( 1 - parent_exp ) * grand_parent_prob
+                    * node_exp * ( 1 - node_exp );
+
+                // Gradient w/r parent node
+                local_node_expectation_gradient[ n + offset - sub_tree_size - 1 ] -= 
+                    ( out_grad * node_exp + off_grad * (1 - node_exp) )  * grand_parent_prob
+                    * parent_exp * (1-parent_exp) ;
+            }
+            sub_tree_size = 2 * ( sub_tree_size + 1 ) - 1;
+            depth--;
+        }
+
+        ////// First level nodes
+        depth = 1;
+
+        //// Left child
+        node = sub_tree_size;
+        out_grad = output_gradient[ node + offset ] + 
+            on_tree_gradient[ node + offset ] ;
+        off_grad = off_tree_gradient[ node + offset ] ;
+        node_exp = local_node_expectation[ node + offset ];
+        parent_exp = local_node_expectation[ node + offset + sub_tree_size + 1 ];
+        
+        // Gradient w/r current node
+        local_node_expectation_gradient[ node + offset ] += 
+            ( out_grad - off_grad ) * parent_exp
+            * node_exp * ( 1 - node_exp );
+        
+        // Gradient w/r parent node
+        local_node_expectation_gradient[ node + offset + sub_tree_size + 1 ] += 
+            ( out_grad * node_exp  + off_grad * (1 - node_exp) )
+            * parent_exp * (1-parent_exp) ;
+
+        //// Right child
+        node = 3*sub_tree_size+2;
+        out_grad = output_gradient[ node + offset ] + 
+            on_tree_gradient[ node + offset ] ;
+        off_grad = off_tree_gradient[ node + offset ] ;
+        node_exp = local_node_expectation[ node + offset ];
+        parent_exp = local_node_expectation[ node + offset - sub_tree_size - 1 ];
+
+        // Gradient w/r current node
+        local_node_expectation_gradient[ node + offset ] += 
+            ( out_grad - off_grad ) * ( 1 - parent_exp ) 
+            * node_exp * ( 1 - node_exp );
+        
+        // Gradient w/r parent node
+        local_node_expectation_gradient[ node + offset - sub_tree_size - 1 ] -= 
+            ( out_grad * node_exp + off_grad * (1 - node_exp) ) 
+            * parent_exp * (1-parent_exp) ;
+        
+        ////// Root
+        node = n_nodes_per_tree / 2;
+        sub_tree_size = 2 * ( sub_tree_size + 1 ) - 1;
+
+        out_grad = output_gradient[ node + offset ] + 
+            on_tree_gradient[ node + offset ] ;
+        off_grad = off_tree_gradient[ node + offset ] ;
+        node_exp = local_node_expectation[ node + offset ];
+        local_node_expectation_gradient[ node + offset ] += 
+            ( out_grad - off_grad ) * node_exp * ( 1 - node_exp );
+
+        offset += n_nodes_per_tree;
+    }
+
+    for( int i=0 ; i&lt;size ; i++ )
+    {
+        node_exp = local_node_expectation[i];
+        out_grad = local_node_expectation_gradient[i];
+        on_free_energy_gradient[i] = out_grad * node_exp * ( 1 - node_exp );
+        off_free_energy_gradient[i] = -out_grad * node_exp * ( 1 - node_exp );
+    }
+
+    offset = 0;
+    for( int t=0; t&lt;n_trees; t++ )
+    {
+        depth = 0;
+        sub_tree_size = n_nodes_per_tree / 2;
+
+        while( depth &lt; tree_depth-1 )
+        {
+            for( int n=sub_tree_size; n&lt;n_nodes_per_tree; n += 2*sub_tree_size + 2 )
+            {
+                out_grad = on_free_energy_gradient[ n + offset ];
+                node_exp = local_node_expectation[n + offset - sub_tree_size];
+                input_gradient[n+offset] += out_grad;
+                on_free_energy[n + offset - sub_tree_size] += out_grad * node_exp; 
+                off_free_energy[n + offset - sub_tree_size] += out_grad * (1 - node_exp); 
+
+                out_grad = off_free_energy_gradient[ n + offset ];
+                node_exp = local_node_expectation[n + offset + sub_tree_size];
+                on_free_energy[n + offset + sub_tree_size] += out_grad * node_exp; 
+                off_free_energy[n + offset + sub_tree_size] += out_grad * (1 - node_exp); 
+            }
+            sub_tree_size /= 2;
+            depth++;
+        }
+
+        depth = tree_depth-1;
+        sub_tree_size = 0;
+
+        for( int n=sub_tree_size; n&lt;n_nodes_per_tree; n += 2*sub_tree_size + 2 )
+            input_gradient[n+offset] += on_free_energy_gradient[ n + offset ];
+
+        offset += n_nodes_per_tree;
+    }
+
+    if( momentum != 0. )
+        bias_inc.resize( size );
+
+    for( int i=0 ; i&lt;size ; i++ )
+    {
+        real in_grad_i = input_gradient[i];
+        input_gradient[i] += in_grad_i;
+
+        if( momentum == 0. )
+        {
+            // update the bias: bias -= learning_rate * input_gradient
+            bias[i] -= learning_rate * in_grad_i;
+        }
+        else
+        {
+            // The update rule becomes:
+            // bias_inc = momentum * bias_inc - learning_rate * input_gradient
+            // bias += bias_inc
+            bias_inc[i] = momentum * bias_inc[i] - learning_rate * in_grad_i;
+            bias[i] += bias_inc[i];
+        }
+    }
+
+    applyBiasDecay();
+}
+
+void RBMWoodsLayer::bpropUpdate(const Mat&amp; inputs, const Mat&amp; outputs,
+                                   Mat&amp; input_gradients,
+                                   const Mat&amp; output_gradients,
+                                   bool accumulate)
+{
+    PLASSERT( inputs.width() == size );
+    PLASSERT( outputs.width() == size );
+    PLASSERT( output_gradients.width() == size );
+
+    int mbatch_size = inputs.length();
+    PLASSERT( outputs.length() == mbatch_size );
+    PLASSERT( output_gradients.length() == mbatch_size );
+
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradients.width() == size &amp;&amp;
+                input_gradients.length() == mbatch_size,
+                &quot;Cannot resize input_gradients and accumulate into it&quot; );
+    }
+    else
+    {
+        input_gradients.resize(mbatch_size, size);
+        input_gradients.clear();
+    }
+
+    PLERROR( &quot;RBMWoodsLayer::bpropUpdate(): not implemeted yet&quot; );
+
+    if( momentum != 0. )
+        bias_inc.resize( size );
+
+    // TODO Can we do this more efficiently? (using BLAS)
+
+    // We use the average gradient over the mini-batch.
+    real avg_lr = learning_rate / inputs.length();
+
+    for (int j = 0; j &lt; mbatch_size; j++)
+    {
+        for( int i=0 ; i&lt;size ; i++ )
+        {
+            real output_i = outputs(j, i);
+            real in_grad_i = output_i * (1-output_i) * output_gradients(j, i);
+            input_gradients(j, i) += in_grad_i;
+
+            if( momentum == 0. )
+            {
+                // update the bias: bias -= learning_rate * input_gradient
+                bias[i] -= avg_lr * in_grad_i;
+            }
+            else
+            {
+                PLERROR(&quot;In RBMWoodsLayer:bpropUpdate - Not implemented for &quot;
+                        &quot;momentum with mini-batches&quot;);
+                // The update rule becomes:
+                // bias_inc = momentum * bias_inc - learning_rate * input_gradient
+                // bias += bias_inc
+                bias_inc[i] = momentum * bias_inc[i] - learning_rate * in_grad_i;
+                bias[i] += bias_inc[i];
+            }
+        }
+    }
+
+    applyBiasDecay();
+}
+
+
+//! TODO: add &quot;accumulate&quot; here
+void RBMWoodsLayer::bpropUpdate(const Vec&amp; input, const Vec&amp; rbm_bias,
+                                   const Vec&amp; output,
+                                   Vec&amp; input_gradient, Vec&amp; rbm_bias_gradient,
+                                   const Vec&amp; output_gradient)
+{
+    PLASSERT( input.size() == size );
+    PLASSERT( rbm_bias.size() == size );
+    PLASSERT( output.size() == size );
+    PLASSERT( output_gradient.size() == size );
+    input_gradient.resize( size );
+    rbm_bias_gradient.resize( size );
+
+    PLERROR( &quot;RBMWoodsLayer::bpropUpdate(): not implemeted yet&quot; );
+
+    for( int i=0 ; i&lt;size ; i++ )
+    {
+        real output_i = output[i];
+        input_gradient[i] = output_i * (1-output_i) * output_gradient[i];
+    }
+
+    rbm_bias_gradient &lt;&lt; input_gradient;
+    addBiasDecay(rbm_bias_gradient);
+}
+
+real RBMWoodsLayer::fpropNLL(const Vec&amp; target)
+{
+    PLASSERT( target.size() == input_size );
+
+    PLERROR( &quot;RBMWoodsLayer::fpropNLL(): not implemeted yet&quot; );
+
+    real ret = 0;
+    real target_i, activation_i;
+    if(use_fast_approximations){
+        for( int i=0 ; i&lt;size ; i++ )
+        {
+            target_i = target[i];
+            activation_i = activation[i];
+            ret += tabulated_softplus(activation_i) - target_i * activation_i;
+            // nll = - target*log(sigmoid(act)) -(1-target)*log(1-sigmoid(act))
+            // but it is numerically unstable, so use instead the following identity:
+            //     = target*softplus(-act) +(1-target)*(act+softplus(-act))
+            //     = act + softplus(-act) - target*act 
+            //     = softplus(act) - target*act
+        }
+    } else {
+        for( int i=0 ; i&lt;size ; i++ )
+        {
+            target_i = target[i];
+            activation_i = activation[i];
+            ret += softplus(activation_i) - target_i * activation_i;
+        }
+    }
+    return ret;
+}
+
+void RBMWoodsLayer::fpropNLL(const Mat&amp; targets, const Mat&amp; costs_column)
+{
+    // computeExpectations(); // why?
+
+    PLERROR( &quot;RBMWoodsLayer::fpropNLL(): not implemeted yet&quot; );
+
+    PLASSERT( targets.width() == input_size );
+    PLASSERT( targets.length() == batch_size );
+    PLASSERT( costs_column.width() == 1 );
+    PLASSERT( costs_column.length() == batch_size );
+
+    for (int k=0;k&lt;batch_size;k++) // loop over minibatch
+    {
+        real nll = 0;
+        real* activation = activations[k];
+        real* target = targets[k];
+        if(use_fast_approximations){
+            for( int i=0 ; i&lt;size ; i++ ) // loop over outputs
+            {
+                if(!fast_exact_is_equal(target[i],0.0))
+                    // nll -= target[i] * pl_log(expectations[i]); 
+                    // but it is numerically unstable, so use instead
+                    // log (1/(1+exp(-x))) = -log(1+exp(-x)) = -softplus(-x)
+                    nll += target[i] * tabulated_softplus(-activation[i]);
+                if(!fast_exact_is_equal(target[i],1.0))
+                    // nll -= (1-target[i]) * pl_log(1-output[i]);
+                    // log (1 - 1/(1+exp(-x))) = log(exp(-x)/(1+exp(-x)))
+                    //                         = log(1/(1+exp(x)))
+                    //                         = -log(1+exp(x))
+                    //                         = -softplus(x)
+                    nll += (1-target[i]) * tabulated_softplus(activation[i]);
+            }
+        } else {
+            for( int i=0 ; i&lt;size ; i++ ) // loop over outputs
+            {
+                if(!fast_exact_is_equal(target[i],0.0))
+                    // nll -= target[i] * pl_log(expectations[i]); 
+                    // but it is numerically unstable, so use instead
+                    // log (1/(1+exp(-x))) = -log(1+exp(-x)) = -softplus(-x)
+                    nll += target[i] * softplus(-activation[i]);
+                if(!fast_exact_is_equal(target[i],1.0))
+                    // nll -= (1-target[i]) * pl_log(1-output[i]);
+                    // log (1 - 1/(1+exp(-x))) = log(exp(-x)/(1+exp(-x)))
+                    //                         = log(1/(1+exp(x)))
+                    //                         = -log(1+exp(x))
+                    //                         = -softplus(x)
+                    nll += (1-target[i]) * softplus(activation[i]);
+            }
+        }
+        costs_column(k,0) = nll;
+    }
+}
+
+void RBMWoodsLayer::bpropNLL(const Vec&amp; target, real nll, Vec&amp; bias_gradient)
+{
+    PLERROR( &quot;RBMWoodsLayer::bpropNLL(): not implemeted yet&quot; );
+    computeExpectation();
+
+    PLASSERT( target.size() == input_size );
+    bias_gradient.resize( size );
+
+    // bias_gradient = expectation - target
+    substract(expectation, target, bias_gradient);
+    addBiasDecay(bias_gradient);
+}
+
+void RBMWoodsLayer::bpropNLL(const Mat&amp; targets, const Mat&amp; costs_column,
+                                Mat&amp; bias_gradients)
+{
+    PLERROR( &quot;RBMWoodsLayer::bpropNLL(): not implemeted yet&quot; );
+    computeExpectations();
+
+    PLASSERT( targets.width() == input_size );
+    PLASSERT( targets.length() == batch_size );
+    PLASSERT( costs_column.width() == 1 );
+    PLASSERT( costs_column.length() == batch_size );
+    bias_gradients.resize( batch_size, size );
+
+    // bias_gradients = expectations - targets
+    substract(expectations, targets, bias_gradients);
+
+    addBiasDecay(bias_gradients);
+}
+
+void RBMWoodsLayer::declareOptions(OptionList&amp; ol)
+{
+    declareOption(ol, &quot;n_trees&quot;, &amp;RBMWoodsLayer::n_trees,
+                  OptionBase::buildoption,
+                  &quot;Number of trees in the woods.&quot;);
+
+    declareOption(ol, &quot;tree_depth&quot;, &amp;RBMWoodsLayer::tree_depth,
+                  OptionBase::buildoption,
+                  &quot;Depth of the trees in the woods (1 gives the ordinary &quot;
+                  &quot;RBMBinomialLayer).&quot;);
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void RBMWoodsLayer::build_()
+{
+    PLASSERT( n_trees &gt; 0 );
+    PLASSERT( tree_depth &gt; 0 );
+
+    if ( tree_depth &lt; 2 )
+        PLERROR(&quot;RBMWoodsLayer::build_(): tree_depth &lt; 2 not supported, use &quot;
+                &quot;RBMBinomialLayer instead.&quot;);
+
+    size = n_trees * ( ipow( 2, tree_depth ) - 1 );
+    local_node_expectation.resize( size );
+    on_free_energy.resize( size );
+    off_free_energy.resize( size );
+    off_expectation.resize( size );
+    local_node_expectation_gradient.resize( size );
+    on_tree_gradient.resize( size );
+    off_tree_gradient.resize( size );
+    on_free_energy_gradient.resize( size );
+    off_free_energy_gradient.resize( size );
+
+    // Must call parent's build, since size was just set
+    inherited::build();
+}
+
+void RBMWoodsLayer::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void RBMWoodsLayer::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField( off_expectation, copies );
+    deepCopyField( local_node_expectation, copies );
+    deepCopyField( on_free_energy, copies );
+    deepCopyField( off_free_energy, copies );
+    deepCopyField( local_node_expectation_gradient, copies );
+    deepCopyField( on_tree_gradient, copies );
+    deepCopyField( off_tree_gradient, copies );
+    deepCopyField( on_free_energy_gradient, copies );
+    deepCopyField( off_free_energy_gradient, copies );
+}
+
+real RBMWoodsLayer::energy(const Vec&amp; unit_values) const
+{
+    PLERROR( &quot;RBMWoodsLayer::energy(): not implemeted yet&quot; );
+    return -dot(unit_values, bias);
+}
+
+real RBMWoodsLayer::freeEnergyContribution(const Vec&amp; unit_activations)
+    const
+{
+    PLERROR( &quot;RBMWoodsLayer::freeEnergyContribution(): not implemeted yet&quot; );
+    PLASSERT( unit_activations.size() == size );
+
+    // result = -\sum_{i=0}^{size-1} softplus(a_i)
+    real result = 0;
+    real* a = unit_activations.data();
+    for (int i=0; i&lt;size; i++)
+    {
+        if (use_fast_approximations)
+            result -= tabulated_softplus(a[i]);
+        else
+            result -= softplus(a[i]);
+    }
+    return result;
+}
+
+int RBMWoodsLayer::getConfigurationCount()
+{
+    PLERROR( &quot;RBMWoodsLayer::getConfigurationCount(): not implemeted yet&quot; );
+    return size &lt; 31 ? 1&lt;&lt;size : INFINITE_CONFIGURATIONS;
+}
+
+void RBMWoodsLayer::getConfiguration(int conf_index, Vec&amp; output)
+{
+    PLERROR( &quot;RBMWoodsLayer::getConfigurationCount(): not implemeted yet&quot; );
+    PLASSERT( output.length() == size );
+    PLASSERT( conf_index &gt;= 0 &amp;&amp; conf_index &lt; getConfigurationCount() );
+
+    for ( int i = 0; i &lt; size; ++i ) {
+        output[i] = conf_index &amp; 1;
+        conf_index &gt;&gt;= 1;
+    }
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/RBMWoodsLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMWoodsLayer.h	2008-02-20 20:09:16 UTC (rev 8548)
+++ trunk/plearn_learners/online/RBMWoodsLayer.h	2008-02-20 20:10:33 UTC (rev 8549)
@@ -0,0 +1,203 @@
+// -*- C++ -*-
+
+// RBMWoodsLayer.h
+//
+// Copyright (C) 2008 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file RBMWoodsLayer.h */
+
+
+#ifndef RBMWoodsLayer_INC
+#define RBMWoodsLayer_INC
+
+#include &quot;RBMLayer.h&quot;
+
+namespace PLearn {
+using namespace std;
+
+/**
+ * RBM layer with tree-structured groups of units
+ *
+ */
+class RBMWoodsLayer: public RBMLayer
+{
+    typedef RBMLayer inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    // Number of trees in the woods
+    int n_trees;
+
+    // Depth of the trees in the woods (1 gives the ordinary RBMBinomialLayer)
+    int tree_depth;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    RBMWoodsLayer( real the_learning_rate=0. );
+
+    //! generate a sample, and update the sample field
+    virtual void generateSample() ;
+
+    //! Inherited.
+    virtual void generateSamples();
+
+    virtual void computeProbabilisticClustering(Vec&amp; prob_clusters);
+
+    //! Compute marginal expectations of all units
+    virtual void computeExpectation() ;
+
+    //! Compute marginal mini-batch expectations of all units.
+    virtual void computeExpectations();
+
+    //! forward propagation
+    virtual void fprop( const Vec&amp; input, Vec&amp; output ) const;
+
+    //! Batch forward propagation
+    virtual void fprop( const Mat&amp; inputs, Mat&amp; outputs ) const;
+
+    //! forward propagation with provided bias
+    virtual void fprop( const Vec&amp; input, const Vec&amp; rbm_bias,
+                        Vec&amp; output ) const;
+
+    //! back-propagates the output gradient to the input
+    virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                             Vec&amp; input_gradient, const Vec&amp; output_gradient,
+                             bool accumulate=false);
+
+    //! back-propagates the output gradient to the input and the bias
+    virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; rbm_bias,
+                             const Vec&amp; output,
+                             Vec&amp; input_gradient, Vec&amp; rbm_bias_gradient,
+                             const Vec&amp; output_gradient) ;
+
+    //! Back-propagate the output gradient to the input, and update parameters.
+    virtual void bpropUpdate(const Mat&amp; inputs, const Mat&amp; outputs,
+                             Mat&amp; input_gradients,
+                             const Mat&amp; output_gradients,
+                             bool accumulate = false);
+
+    //! Computes the negative log-likelihood of target given the
+    //! internal activations of the layer
+    virtual real fpropNLL(const Vec&amp; target);
+    virtual void fpropNLL(const Mat&amp; targets, const Mat&amp; costs_column);
+
+    //! Computes the gradient of the negative log-likelihood of target
+    //! with respect to the layer's bias, given the internal activations
+    virtual void bpropNLL(const Vec&amp; target, real nll, Vec&amp; bias_gradient);
+    virtual void bpropNLL(const Mat&amp; targets, const Mat&amp; costs_column,
+                          Mat&amp; bias_gradients);
+
+    //! compute -bias' unit_values
+    virtual real energy(const Vec&amp; unit_values) const;
+
+    //! Computes -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! This quantity is used for computing the free energy of a sample x in
+    //! the OTHER layer of an RBM, from which unit_activations was computed.
+    virtual real freeEnergyContribution(const Vec&amp; unit_activations) const;
+
+    virtual int getConfigurationCount();
+
+    virtual void getConfiguration(int conf_index, Vec&amp; output);
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(RBMWoodsLayer);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
+
+protected:
+    //#####  Not Options  #####################################################
+
+    // Probability that sampling reaches a node but samples h=0 (expectation is for h=1)
+    Vec off_expectation;
+    // Ordinary RBMBinomialLayer expectation
+    Vec local_node_expectation;
+
+    // Computations of the local_node_expectation free energies for h = 1
+    Vec on_free_energy;
+    // Computations of the local_node_expectation free energies for h = 0
+    Vec off_free_energy;
+
+    // Gradient through the local_node_expectations (after sigmoid)
+    Vec local_node_expectation_gradient;
+    // Gradient through the tree structure
+    Vec on_tree_gradient;
+    Vec off_tree_gradient;
+    Vec on_free_energy_gradient;
+    Vec off_free_energy_gradient;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList&amp; ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(RBMWoodsLayer);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001996.html">[Plearn-commits] r8548 - trunk/plearn_learners/online
</A></li>
	<LI>Next message: <A HREF="001998.html">[Plearn-commits] r8550 - trunk/plearn_learners/distributions
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1997">[ date ]</a>
              <a href="thread.html#1997">[ thread ]</a>
              <a href="subject.html#1997">[ subject ]</a>
              <a href="author.html#1997">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
