<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r9740 - trunk/plearn_learners_experimental
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2008-December/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9740%20-%20trunk/plearn_learners_experimental&In-Reply-To=%3C200812042300.mB4N0963001588%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="003179.html">
   <LINK REL="Next"  HREF="003181.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r9740 - trunk/plearn_learners_experimental</H1>
    <B>laulysta at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9740%20-%20trunk/plearn_learners_experimental&In-Reply-To=%3C200812042300.mB4N0963001588%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r9740 - trunk/plearn_learners_experimental">laulysta at mail.berlios.de
       </A><BR>
    <I>Fri Dec  5 00:00:09 CET 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="003179.html">[Plearn-commits] r9739 - trunk/python_modules/plearn/pymake
</A></li>
        <LI>Next message: <A HREF="003181.html">[Plearn-commits] r9741 - trunk/python_modules/plearn/pymake
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#3180">[ date ]</a>
              <a href="thread.html#3180">[ thread ]</a>
              <a href="subject.html#3180">[ subject ]</a>
              <a href="author.html#3180">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: laulysta
Date: 2008-12-05 00:00:08 +0100 (Fri, 05 Dec 2008)
New Revision: 9740

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
Log:
reconstruction working


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-12-04 17:51:27 UTC (rev 9739)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-12-04 23:00:08 UTC (rev 9740)
@@ -71,9 +71,6 @@
     );
 
 DenoisingRecurrentNet::DenoisingRecurrentNet() :
-    prediction_cost_weight(1),
-    input_reconstruction_cost_weight(0),
-    hidden_reconstruction_cost_weight(0),
     use_target_layers_masks( false ),
     end_of_sequence_symbol( -1000 ),
     encoding(&quot;note_octav_duration&quot;),
@@ -87,6 +84,9 @@
     noisy_recurrent_lr( 0.000001),
     dynamic_gradient_scale_factor( 1.0 ),
     recurrent_lr( 0.00001 ),
+    prediction_cost_weight(1),
+    input_reconstruction_cost_weight(0),
+    hidden_reconstruction_cost_weight(0),
     current_learning_rate(0)
 {
     random_gen = new PRandom();
@@ -672,30 +672,29 @@
                 if(corrupt_input)  // WARNING: encoded_sequence will be dirty!!!!
                     inject_zero_forcing_noise(encoded_seq, input_noise_prob);
                 
-                recurrentFprop(train_costs, train_n_items);
-
                 // greedy phase
                 if(input_reconstruction_lr!=0 || hidden_reconstruction_lr!=0){
                     setLearningRate( input_reconstruction_lr );
-                    performGreedyDenoisingPhase(train_costs, train_n_items);
+                    recurrentFprop(train_costs, train_n_items);
+                    recurrentUpdate(input_reconstruction_cost_weight, 0);
                 }
 
                 // recurrent noisy phase
                 if(noisy_recurrent_lr!=0)
                 {
                     setLearningRate( noisy_recurrent_lr );
-                    //recurrentFprop(train_costs, train_n_items);
-                    recurrentUpdate(true);
+                    recurrentFprop(train_costs, train_n_items);
+                    recurrentUpdate(input_reconstruction_cost_weight, 1);
                 }
 
                 // recurrent no noise phase
                 if(recurrent_lr!=0)
                 {
-                    if(corrupt_input) // need to recover the clean sequence
-                        encoded_seq &lt;&lt; clean_encoded_seq;                    
-                    setLearningRate( recurrent_lr );
+                    if(corrupt_input) // need to recover the clean sequence                        
+                        encoded_seq &lt;&lt; clean_encoded_seq;                  
+                    setLearningRate( recurrent_lr );                    
                     recurrentFprop(train_costs, train_n_items);
-                    recurrentUpdate(false);
+                    recurrentUpdate(0,1);
                 }
             }
 
@@ -741,49 +740,6 @@
 }
 
 
-void DenoisingRecurrentNet::performGreedyDenoisingPhase(Vec train_costs, Vec train_n_items)
-{
-    hidden_temporal_gradient.resize(hidden_layer-&gt;size);
-    hidden_gradient.resize(hidden_layer-&gt;size);
-    
-    double cost = 0;
-    Mat reconstruction_weights = getInputConnectionsWeightMatrix();
-    for(int i=hidden_list.length()-1; i&gt;=0; i--){ 
-        // Add contribution of input reconstruction cost in hidden_gradient
-        if(input_reconstruction_cost_weight!=0)
-        {
-            hidden_temporal_gradient.clear();
-            hidden_gradient.clear();
-
-            Vec clean_input = clean_encoded_seq.subMatRows(i, input_window_size).toVec();
-            
-            train_costs[train_costs.length()-2] += fpropUpdateInputReconstructionFromHidden(hidden_list(i), reconstruction_weights, input_reconstruction_bias, input_reconstruction_prob, 
-                                                                    clean_input, hidden_gradient, input_reconstruction_cost_weight, input_reconstruction_lr);
-            train_n_items[train_n_items.length()-2]++;
-
-            if(i!=0 &amp;&amp; dynamic_connections )
-            {
-                hidden_layer-&gt;bpropUpdate(
-                    hidden_act_no_bias_list(i), hidden_list(i),
-                    hidden_temporal_gradient, hidden_gradient);
-                
-                dynamic_connections-&gt;bpropUpdate(
-                    hidden_list(i-1),
-                    hidden_act_no_bias_list(i), // Here, it should be dynamic_act_no_bias_contribution, but doesn't matter because a RBMMatrixConnection::bpropUpdate doesn't use its second argument
-                    hidden_gradient, hidden_temporal_gradient);
-            }
-            else{
-                hidden_layer-&gt;bpropUpdate(
-                    hidden_act_no_bias_list(i), hidden_list(i),
-                    hidden_temporal_gradient, hidden_gradient); // Not really temporal gradient, but this is the final iteration...
-            }
-        }
-    }
-
-    
-}
-
-
 //! does encoding if needed and populates the list.
 void DenoisingRecurrentNet::encodeSequenceAndPopulateLists(Mat seq) const
 {
@@ -852,8 +808,9 @@
         masks_list[k] = mask_part.subMatColumns(startcol, targsize);
         startcol += targsize;
     }
-    encoded_seq.resize(seq.length(), seq.width());
-    encoded_seq &lt;&lt; seq;
+
+    encoded_seq.resize(input_part.length(), input_part.width());
+    encoded_seq &lt;&lt; input_part;
 }
 
 
@@ -1051,14 +1008,27 @@
     // predict (denoised) input_reconstruction 
     transposeProduct(input_reconstruction_activation, reconstruction_weights, hidden); 
     input_reconstruction_activation += input_reconstruction_bias;
-    applyMultipleSoftmaxToInputWindow(input_reconstruction_activation, input_reconstruction_prob);
-    
-    double neg_log_cost = 0; // neg log softmax
-    for(int k=0; k&lt;input_reconstruction_prob.length(); k++)
-        if(clean_input[k]!=0)
-            neg_log_cost -= clean_input[k]*safelog(input_reconstruction_prob[k]);
 
-    return neg_log_cost;
+    double result_cost = 0;
+    if(encoding==&quot;raw_masked_supervised&quot;) // complicated input format... consider it's squared error
+    {
+        real r;
+        input_reconstruction_prob &lt;&lt; input_reconstruction_activation;
+        for(int i=0; i&lt;input_reconstruction_activation.length(); i++)
+            r += input_reconstruction_activation[i] - clean_input[i];
+        result_cost = r*r;
+    }
+    else // suppose it's a multiple softmax
+    {
+        applyMultipleSoftmaxToInputWindow(input_reconstruction_activation, input_reconstruction_prob);
+    
+        double neg_log_cost = 0; // neg log softmax
+        for(int k=0; k&lt;input_reconstruction_prob.length(); k++)
+            if(clean_input[k]!=0)
+                neg_log_cost -= clean_input[k]*safelog(input_reconstruction_prob[k]);
+        result_cost = neg_log_cost;
+    }
+    return result_cost;
 }
 
 //! Backpropagates reconstruction cost (after comparison with clean_input) with learning rate input_reconstruction_lr
@@ -1076,8 +1046,10 @@
     multiplyAcc(input_reconstruction_bias, input_reconstruction_activation_grad, -lr);
 
     // update weight
-    // productTransposeAcc(reconstruction_weights, hidden, input_reconstruction_activation_grad);
-    externalProductScaleAcc(reconstruction_weights, hidden, input_reconstruction_activation_grad, -lr);
+    // THIS IS COMMENTED OUT BECAUSE THE reconstruction_weights ARE tied (same) TO THE input_connection weights, 
+    // WHICH GET UPDATED LATER IN recurrentUpdate SO IF WE UPDATE THEM HERE THEY WOULD GET UPDATED TWICE.
+    // WARNING: THIS WOULD NO LONGER BE THE CASE IF THEY WERE NOT TIED!
+    // externalProductScaleAcc(reconstruction_weights, hidden, input_reconstruction_activation_grad, -lr);
 
     // accumulate in hidden_gradient
     productAcc(hidden_gradient, reconstruction_weights, input_reconstruction_activation_grad);
@@ -1099,7 +1071,8 @@
 nll_list
 */
 
-void DenoisingRecurrentNet::recurrentUpdate(bool input_is_corrupted)
+void DenoisingRecurrentNet::recurrentUpdate(real input_reconstruction_weight,
+                                            real temporal_gradient_contribution)
 {
     hidden_temporal_gradient.resize(hidden_layer-&gt;size);
     hidden_temporal_gradient.clear();
@@ -1110,7 +1083,6 @@
         else
             hidden_gradient.resize(hidden_layer-&gt;size);
         hidden_gradient.clear();
-        int tar = 0;
         if( prediction_cost_weight!=0 )
         {
             for( int tar=0; tar&lt;target_layers.length(); tar++)
@@ -1133,34 +1105,38 @@
                                                              hidden_gradient, bias_gradient,true);
                 }
             }
-        }
-        if (hidden_layer2)
-        {
-            hidden_layer2-&gt;bpropUpdate(
-                hidden2_act_no_bias_list(i), hidden2_list(i),
-                bias_gradient, hidden_gradient);
+
+            if (hidden_layer2)
+            {
+                hidden_layer2-&gt;bpropUpdate(
+                    hidden2_act_no_bias_list(i), hidden2_list(i),
+                    bias_gradient, hidden_gradient);
                 
-            hidden_connections-&gt;bpropUpdate(
-                hidden_list(i),
-                hidden2_act_no_bias_list(i), 
-                hidden_gradient, bias_gradient);
+                hidden_connections-&gt;bpropUpdate(
+                    hidden_list(i),
+                    hidden2_act_no_bias_list(i), 
+                    hidden_gradient, bias_gradient);
+            }
         }
             
         // Add contribution of input reconstruction cost in hidden_gradient
-        /*   if(input_is_corrupted &amp;&amp; input_reconstruction_cost_weight!=0)
+        if(input_reconstruction_weight!=0)
         {
             Mat reconstruction_weights = getInputConnectionsWeightMatrix();
             Vec clean_input = clean_encoded_seq.subMatRows(i, input_window_size).toVec();
 
             fpropUpdateInputReconstructionFromHidden(hidden_list(i), reconstruction_weights, input_reconstruction_bias, input_reconstruction_prob, 
-                                                     clean_input, hidden_gradient, input_reconstruction_cost_weight, current_learning_rate);
-                                                     }*/
+                                                     clean_input, hidden_gradient, input_reconstruction_weight, current_learning_rate);
+        }
 
         if(i!=0 &amp;&amp; dynamic_connections )
         {   
             // add contribution to gradient of next time step hidden layer
-            hidden_gradient += hidden_temporal_gradient;
-                
+            if(temporal_gradient_contribution&gt;0)
+            { // add weighted contribution of hidden_temporal gradient to hidden_gradient
+                // It does this: hidden_gradient += temporal_gradient_contribution*hidden_temporal_gradient;
+                multiplyAcc(hidden_gradient, hidden_temporal_gradient, temporal_gradient_contribution);
+            }
             hidden_layer-&gt;bpropUpdate(
                 hidden_act_no_bias_list(i), hidden_list(i),
                 hidden_temporal_gradient, hidden_gradient);

Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-12-04 17:51:27 UTC (rev 9739)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-12-04 23:00:08 UTC (rev 9740)
@@ -266,7 +266,8 @@
     //! Updates both the RBM parameters and the 
     //! dynamic connections in the recurrent tuning phase,
     //! after the visible units have been clamped
-    void recurrentUpdate(bool input_is_corrupted);
+    void recurrentUpdate(real input_reconstruction_weight,
+                         real temporal_gradient_contribution = 1);
 
     virtual void test(VMat testset, PP&lt;VecStatsCollector&gt; test_stats,
                       VMat testoutputs=0, VMat testcosts=0) const;
@@ -377,8 +378,6 @@
     void build_();
 
 
-    void performGreedyDenoisingPhase(Vec train_costs, Vec train_n_items);
-
     void applyMultipleSoftmaxToInputWindow(Vec input_reconstruction_activation, Vec input_reconstruction_prob);
 
     // note: the following functions are declared const because they have


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="003179.html">[Plearn-commits] r9739 - trunk/python_modules/plearn/pymake
</A></li>
	<LI>Next message: <A HREF="003181.html">[Plearn-commits] r9741 - trunk/python_modules/plearn/pymake
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#3180">[ date ]</a>
              <a href="thread.html#3180">[ thread ]</a>
              <a href="subject.html#3180">[ subject ]</a>
              <a href="author.html#3180">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
