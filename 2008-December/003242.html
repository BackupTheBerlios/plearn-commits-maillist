<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r9802 - trunk/plearn_learners_experimental
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2008-December/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9802%20-%20trunk/plearn_learners_experimental&In-Reply-To=%3C200812200043.mBK0hCMw019784%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="003241.html">
   <LINK REL="Next"  HREF="003243.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r9802 - trunk/plearn_learners_experimental</H1>
    <B>laulysta at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9802%20-%20trunk/plearn_learners_experimental&In-Reply-To=%3C200812200043.mBK0hCMw019784%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r9802 - trunk/plearn_learners_experimental">laulysta at mail.berlios.de
       </A><BR>
    <I>Sat Dec 20 01:43:12 CET 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="003241.html">[Plearn-commits] r9801 - trunk/plearn_learners/regressors
</A></li>
        <LI>Next message: <A HREF="003243.html">[Plearn-commits] r9803 - trunk/plearn_learners_experimental
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#3242">[ date ]</a>
              <a href="thread.html#3242">[ thread ]</a>
              <a href="subject.html#3242">[ subject ]</a>
              <a href="author.html#3242">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: laulysta
Date: 2008-12-20 01:43:12 +0100 (Sat, 20 Dec 2008)
New Revision: 9802

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
Log:
auto encodeur de la couche cach?\195?\169e



Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-12-19 21:44:56 UTC (rev 9801)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-12-20 00:43:12 UTC (rev 9802)
@@ -683,7 +683,7 @@
                 if(hidden_reconstruction_lr!=0){
                     setLearningRate( hidden_reconstruction_lr );
                     recurrentFprop(train_costs, train_n_items);
-                    recurrentUpdate(0, hidden_reconstruction_cost_weight, 0);
+                    recurrentUpdate(0, hidden_reconstruction_cost_weight, 1);
                 }
 
                 // recurrent noisy phase
@@ -1070,31 +1070,68 @@
 }
 
 
-double DenoisingRecurrentNet::fpropHiddenReconstructionFromLastHidden(Vec hidden, Mat reconstruction_weights, Vec&amp; reconstruction_prob, 
-                                                                 Vec clean_input, Vec hidden_gradient, double input_reconstruction_cost_weight, double lr)
+double DenoisingRecurrentNet::fpropHiddenReconstructionFromLastHidden(Vec hidden, Mat reconstruction_weights, Vec hidden_reconstruction_activation_grad, Vec&amp; reconstruction_prob, 
+                                                                 Vec hidden_target, Vec hidden_gradient, double hidden_reconstruction_cost_weight, double lr)
 {
     // set appropriate sizes
-    int fullinputlength = clean_input.length();
+    int fullinputlength = hidden_target.length();
     Vec reconstruction_activation;
-    /*if(reconstruction_bias.length()==0)
-    {
-        reconstruction_bias.resize(fullinputlength);
-        reconstruction_bias.clear();
-        }*/
+   
     reconstruction_activation.resize(fullinputlength);
     reconstruction_prob.resize(fullinputlength);
 
     // predict (denoised) input_reconstruction 
     transposeProduct(reconstruction_activation, reconstruction_weights, hidden); 
+    //product(reconstruction_activation, reconstruction_weights, hidden); 
     //reconstruction_activation += hidden_layer-&gt;bias;
     
     hidden_layer-&gt;fprop(reconstruction_activation, reconstruction_prob);
 
     /********************************************************************************/
+    // Vec hidden_reconstruction_activation_grad;
+    hidden_reconstruction_activation_grad.resize(reconstruction_prob.size());
+    hidden_reconstruction_activation_grad &lt;&lt; reconstruction_prob;
+    hidden_reconstruction_activation_grad -= hidden_target;
+    hidden_reconstruction_activation_grad *= hidden_reconstruction_cost_weight;
+
+    productAcc(hidden_gradient, reconstruction_weights, hidden_reconstruction_activation_grad);
+
+    // update weight
+    //externalProductScaleAcc(reconstruction_weights, hidden, hidden_reconstruction_activation_grad, -lr);
+    /********************************************************************************/
+
+    double result_cost = 0;
+    double neg_log_cost = 0; // neg log softmax
+    for(int k=0; k&lt;reconstruction_prob.length(); k++)
+        if(hidden_target[k]!=0)
+            neg_log_cost -= hidden_target[k]*safelog(reconstruction_prob[k]);
+    result_cost = neg_log_cost;
+    
+    return result_cost;
+}
+
+double DenoisingRecurrentNet::fpropHiddenSymmetricDynamicMatrix(Vec hidden, Mat reconstruction_weights, Vec&amp; reconstruction_prob, 
+                                                                 Vec hidden_target, Vec hidden_gradient, double hidden_reconstruction_cost_weight, double lr)
+{
+    // set appropriate sizes
+    int fullinputlength = hidden_target.length();
+    Vec reconstruction_activation;
+   
+    reconstruction_activation.resize(fullinputlength);
+    reconstruction_prob.resize(fullinputlength);
+
+    // predict (denoised) input_reconstruction 
+    transposeProduct(reconstruction_activation, reconstruction_weights, hidden); //truc de stan
+    //product(reconstruction_activation, reconstruction_weights, hidden); 
+    //reconstruction_activation += hidden_layer-&gt;bias;
+    
+    hidden_layer-&gt;fprop(reconstruction_activation, reconstruction_prob);
+
+    /********************************************************************************/
     Vec hidden_reconstruction_activation_grad;
     hidden_reconstruction_activation_grad.resize(reconstruction_prob.size());
     hidden_reconstruction_activation_grad &lt;&lt; reconstruction_prob;
-    hidden_reconstruction_activation_grad -= clean_input;
+    hidden_reconstruction_activation_grad -= hidden_target;
     hidden_reconstruction_activation_grad *= hidden_reconstruction_cost_weight;
 
     productAcc(hidden_gradient, reconstruction_weights, hidden_reconstruction_activation_grad);
@@ -1103,8 +1140,8 @@
     double result_cost = 0;
     double neg_log_cost = 0; // neg log softmax
     for(int k=0; k&lt;reconstruction_prob.length(); k++)
-        if(clean_input[k]!=0)
-            neg_log_cost -= clean_input[k]*safelog(reconstruction_prob[k]);
+        if(hidden_target[k]!=0)
+            neg_log_cost -= hidden_target[k]*safelog(reconstruction_prob[k]);
     result_cost = neg_log_cost;
     
     return result_cost;
@@ -1122,7 +1159,7 @@
 target_prediction_act_no_bias_list
 nll_list
 */
-
+/*
 void DenoisingRecurrentNet::recurrentUpdate(real input_reconstruction_weight,
                                             real hidden_reconstruction_weight,
                                             real temporal_gradient_contribution)
@@ -1186,17 +1223,141 @@
         if(i!=0 &amp;&amp; dynamic_connections )
         {   
 
+
+            hidden_layer-&gt;bpropUpdate(
+                hidden_act_no_bias_list(i), hidden_list(i),
+                hidden_temporal_gradient, hidden_gradient);
+            input_connections-&gt;bpropUpdate(
+                input_list[i],
+                hidden_act_no_bias_list(i), 
+                visi_bias_gradient, hidden_temporal_gradient);// Here, it should be activations - cond_bias, but doesn't matter
+                
+
+
             // Add contribution of hidden reconstruction cost in hidden_gradient
             if(hidden_reconstruction_weight!=0)
             {
                 Mat reconstruction_weights = getDynamicConnectionsWeightMatrix();
-                //Vec clean_input = clean_encoded_seq.subMatRows(i, input_window_size).toVec();
+                //truc stan
+                fpropHiddenReconstructionFromLastHidden(hidden_list(i-1), reconstruction_weights, hidden_reconstruction_prob, hidden_list(i), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
+                //fpropHiddenReconstructionFromLastHidden(hidden_list(i), reconstruction_weights, hidden_reconstruction_prob, hidden_list(i-1), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
+            
+            }
+            // add contribution to gradient of next time step hidden layer
+            if(temporal_gradient_contribution&gt;0)
+            { // add weighted contribution of hidden_temporal gradient to hidden_gradient
+                // It does this: hidden_gradient += temporal_gradient_contribution*hidden_temporal_gradient;
+                multiplyAcc(hidden_gradient, hidden_temporal_gradient, temporal_gradient_contribution);
+            }
+  
+
+            hidden_layer-&gt;bpropUpdate(
+                hidden_act_no_bias_list(i), hidden_list(i),
+                hidden_temporal_gradient, hidden_gradient);
                 
-                fpropHiddenReconstructionFromLastHidden(hidden_list(i-1), reconstruction_weights, hidden_reconstruction_prob, 
-                                                        hidden_list(i), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
+            dynamic_connections-&gt;bpropUpdate(
+                hidden_list(i-1),
+                hidden_act_no_bias_list(i), // Here, it should be dynamic_act_no_bias_contribution, but doesn't matter because a RBMMatrixConnection::bpropUpdate doesn't use its second argument
+                hidden_gradient, hidden_temporal_gradient);
+                
+            
+            hidden_temporal_gradient &lt;&lt; hidden_gradient;                
+        }
+        else
+        {
+            hidden_layer-&gt;bpropUpdate(
+                hidden_act_no_bias_list(i), hidden_list(i),
+                hidden_temporal_gradient, hidden_gradient); // Not really temporal gradient, but this is the final iteration...
+            input_connections-&gt;bpropUpdate(
+                input_list[i],
+                hidden_act_no_bias_list(i), 
+                visi_bias_gradient, hidden_temporal_gradient);// Here, it should be activations - cond_bias, but doesn't matter
+
+        }
+    }
+    
+}
+
+*/
+void DenoisingRecurrentNet::recurrentUpdate(real input_reconstruction_weight,
+                                            real hidden_reconstruction_weight,
+                                            real temporal_gradient_contribution)
+{
+    hidden_temporal_gradient.resize(hidden_layer-&gt;size);
+    hidden_temporal_gradient.clear();
+    for(int i=hidden_list.length()-1; i&gt;=0; i--){   
+
+        if( hidden_layer2 )
+            hidden_gradient.resize(hidden_layer2-&gt;size);
+        else
+            hidden_gradient.resize(hidden_layer-&gt;size);
+        hidden_gradient.clear();
+        if( prediction_cost_weight!=0 )
+        {
+            for( int tar=0; tar&lt;target_layers.length(); tar++)
+            {
+                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                {
+                    target_layers[tar]-&gt;activation &lt;&lt; target_prediction_act_no_bias_list[tar](i);
+                    target_layers[tar]-&gt;activation += target_layers[tar]-&gt;bias;
+                    target_layers[tar]-&gt;setExpectation(target_prediction_list[tar](i));
+                    target_layers[tar]-&gt;bpropNLL(targets_list[tar](i),nll_list(i,tar),bias_gradient);
+                    bias_gradient *= prediction_cost_weight;
+                    if(use_target_layers_masks)
+                        bias_gradient *= masks_list[tar](i);
+                    target_layers[tar]-&gt;update(bias_gradient);
+                    if( hidden_layer2 )
+                        target_connections[tar]-&gt;bpropUpdate(hidden2_list(i),target_prediction_act_no_bias_list[tar](i),
+                                                             hidden_gradient, bias_gradient,true);
+                    else
+                        target_connections[tar]-&gt;bpropUpdate(hidden_list(i),target_prediction_act_no_bias_list[tar](i),
+                                                             hidden_gradient, bias_gradient,true);
+                }
             }
 
+            if (hidden_layer2)
+            {
+                hidden_layer2-&gt;bpropUpdate(
+                    hidden2_act_no_bias_list(i), hidden2_list(i),
+                    bias_gradient, hidden_gradient);
+                
+                hidden_connections-&gt;bpropUpdate(
+                    hidden_list(i),
+                    hidden2_act_no_bias_list(i), 
+                    hidden_gradient, bias_gradient);
+            }
+        }
+            
+        // Add contribution of input reconstruction cost in hidden_gradient
+        if(input_reconstruction_weight!=0)
+        {
+            Mat reconstruction_weights = getInputConnectionsWeightMatrix();
+            Vec clean_input = clean_encoded_seq.subMatRows(i, input_window_size).toVec();
 
+            fpropUpdateInputReconstructionFromHidden(hidden_list(i), reconstruction_weights, input_reconstruction_bias, input_reconstruction_prob, 
+                                                     clean_input, hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
+        }
+
+
+        if(i!=0 &amp;&amp; dynamic_connections )
+        {   
+
+            // Add contribution of hidden reconstruction cost in hidden_gradient
+            Vec hidden_reconstruction_activation_grad;
+            hidden_reconstruction_activation_grad.resize(hidden_layer-&gt;size);
+            Mat reconstruction_weights = getDynamicConnectionsWeightMatrix();
+            if(hidden_reconstruction_weight!=0)
+            {
+                //Vec hidden_reconstruction_activation_grad;
+                //Mat reconstruction_weights = getDynamicConnectionsWeightMatrix();
+
+                //truc stan
+                //fpropHiddenSymmetricDynamicMatrix(hidden_list(i-1), reconstruction_weights, hidden_reconstruction_prob, hidden_list(i), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
+                fpropHiddenReconstructionFromLastHidden(hidden_list(i), reconstruction_weights, hidden_reconstruction_activation_grad, hidden_reconstruction_prob, hidden_list(i-1), hidden_gradient, hidden_reconstruction_weight, dynamic_gradient_scale_factor*current_learning_rate);
+            
+            }
+
+
             // add contribution to gradient of next time step hidden layer
             if(temporal_gradient_contribution&gt;0)
             { // add weighted contribution of hidden_temporal gradient to hidden_gradient
@@ -1211,7 +1372,13 @@
                 hidden_list(i-1),
                 hidden_act_no_bias_list(i), // Here, it should be dynamic_act_no_bias_contribution, but doesn't matter because a RBMMatrixConnection::bpropUpdate doesn't use its second argument
                 hidden_gradient, hidden_temporal_gradient);
-                
+
+            if(hidden_reconstruction_weight!=0)
+            {
+                // update weight
+                externalProductScaleAcc(reconstruction_weights, hidden_list(i), hidden_reconstruction_activation_grad, -dynamic_gradient_scale_factor*current_learning_rate);
+            }
+
             input_connections-&gt;bpropUpdate(
                 input_list[i],
                 hidden_act_no_bias_list(i), 

Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-12-19 21:44:56 UTC (rev 9801)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-12-20 00:43:12 UTC (rev 9802)
@@ -319,7 +319,7 @@
     
     //! Stores hidden gradient of dynamic connections coming from time t+1
     mutable Vec hidden_temporal_gradient;
-        
+
     //! List of hidden layers values
     // mutable TVec&lt; Vec &gt; hidden_list;
     mutable Mat hidden_list;
@@ -423,9 +423,11 @@
     void updateInputReconstructionFromHidden(Vec hidden, Mat&amp; reconstruction_weights, Vec&amp; input_reconstruction_bias, Vec input_reconstruction_prob, 
                                              Vec clean_input, Vec hidden_gradient, double input_reconstruction_cost_weight, double lr);
 
-    double fpropHiddenReconstructionFromLastHidden(Vec hidden, Mat reconstruction_weights, Vec&amp; reconstruction_prob, 
-                                                                          Vec clean_input, Vec hidden_gradient, double input_reconstruction_cost_weight, double lr);
-
+    double fpropHiddenReconstructionFromLastHidden(Vec hidden, Mat reconstruction_weights, Vec hidden_reconstruction_activation_grad, Vec&amp; reconstruction_prob, 
+                                                                          Vec clean_input, Vec hidden_gradient, double hidden_reconstruction_cost_weight, double lr);
+    
+    double fpropHiddenSymmetricDynamicMatrix(Vec hidden, Mat reconstruction_weights, Vec&amp; reconstruction_prob, 
+                                                                          Vec clean_input, Vec hidden_gradient, double hidden_reconstruction_cost_weight, double lr);
 private:
     //#####  Private Data Members  ############################################
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="003241.html">[Plearn-commits] r9801 - trunk/plearn_learners/regressors
</A></li>
	<LI>Next message: <A HREF="003243.html">[Plearn-commits] r9803 - trunk/plearn_learners_experimental
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#3242">[ date ]</a>
              <a href="thread.html#3242">[ thread ]</a>
              <a href="subject.html#3242">[ subject ]</a>
              <a href="author.html#3242">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
