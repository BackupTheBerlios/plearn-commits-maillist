From nouiz at mail.berlios.de  Mon Apr  2 18:45:50 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 2 Apr 2007 18:45:50 +0200
Subject: [Plearn-commits] r6811 - in trunk: . plearn/base
Message-ID: <200704021645.l32GjotS022180@sheep.berlios.de>

Author: nouiz
Date: 2007-04-02 18:45:48 +0200 (Mon, 02 Apr 2007)
New Revision: 6811

Modified:
   trunk/plearn/base/plerror.cc
   trunk/plearn/base/plerror.h
   trunk/pymake.config.model
Log:
PLERROR() now print the file and the line number before the error


Modified: trunk/plearn/base/plerror.cc
===================================================================
--- trunk/plearn/base/plerror.cc	2007-03-31 19:36:57 UTC (rev 6810)
+++ trunk/plearn/base/plerror.cc	2007-04-02 16:45:48 UTC (rev 6811)
@@ -56,8 +56,18 @@
 ostream* error_stream = &cerr;
 
 #define ERROR_MSG_SIZE 1024
+#ifndef USER_SUPPLIED_ERROR
+void errormsg2(const char* filename,const int linenumber,const char* msg, ...){
+    va_list args;
+    va_start(args,msg);
+    char message[ERROR_MSG_SIZE];
+    
+    snprintf(message, ERROR_MSG_SIZE, "In file: \"%s\" at line %d\n", filename, linenumber);
+    PLASSERT(ERROR_MSG_SIZE>=strlen(message)+strlen(msg));
+    strcat(message,msg);
+    errormsg(message, args);
 
-#ifndef USER_SUPPLIED_ERROR
+}
 void errormsg(const char* msg, ...)
 {
     va_list args;

Modified: trunk/plearn/base/plerror.h
===================================================================
--- trunk/plearn/base/plerror.h	2007-03-31 19:36:57 UTC (rev 6810)
+++ trunk/plearn/base/plerror.h	2007-04-02 16:45:48 UTC (rev 6811)
@@ -57,10 +57,12 @@
 extern ostream* error_stream;
 #endif
 
-#define PLERROR   errormsg
+#define PLERROR(...)   errormsg2(__FILE__,__LINE__,__VA_ARGS__)
+//#define PLERROR   errormsg //Use if the compiler don't like variadic macros
 #define PLWARNING warningmsg
 #define PLDEPRECATED deprecationmsg
 
+void errormsg2(const char* filename,const int linenumber,const char* msg, ...);
 void errormsg(const char* msg, ...);
 void warningmsg(const char* msg, ...);
 void deprecationmsg(const char* msg, ...);

Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2007-03-31 19:36:57 UTC (rev 6810)
+++ trunk/pymake.config.model	2007-04-02 16:45:48 UTC (rev 6811)
@@ -432,14 +432,14 @@
               description = 'compiling with g++, with no MPI support',
               compiler = 'g++',
               compileroptions = '-Wno-deprecated '+pedantic_mode+'-Wno-long-long -ftemplate-depth-100 ' \
-                      + gcc_opt_options,
+                      + gcc_opt_options + "-Wno-variadic-macros",
               cpp_definitions = ['USING_MPI=0'],
               linker = 'g++' )
 
 pymakeOption( name = 'g++3',
               description = 'compiling with g++3 (version 3.0), with no MPI support',
               compiler = 'g++3',
-              compileroptions = '-Wno-deprecated '+pedantic_mode+'-ftemplate-depth-100',
+              compileroptions = '-Wno-deprecated '+pedantic_mode+'-ftemplate-depth-100' + "-Wno-variadic-macros",
               cpp_definitions = ['USING_MPI=0'],
               linker = 'g++3' )
 
@@ -449,7 +449,7 @@
               compiler = 'g++',
               compileroptions = '-Wno-deprecated '+pedantic_mode \
                               + '-Wno-long-long -ftemplate-depth-100 '\
-                              + '-mno-cygwin',
+                              + '-mno-cygwin' + "-Wno-variadic-macros",
               cpp_definitions = ['USING_MPI=0', '_MINGW_'],
               linker = 'g++ -mno-cygwin' )
 



From nouiz at mail.berlios.de  Mon Apr  2 19:09:21 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 2 Apr 2007 19:09:21 +0200
Subject: [Plearn-commits] r6812 - trunk
Message-ID: <200704021709.l32H9LZB016808@sheep.berlios.de>

Author: nouiz
Date: 2007-04-02 19:09:21 +0200 (Mon, 02 Apr 2007)
New Revision: 6812

Modified:
   trunk/pymake.config.model
Log:
Forgot spaces


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2007-04-02 16:45:48 UTC (rev 6811)
+++ trunk/pymake.config.model	2007-04-02 17:09:21 UTC (rev 6812)
@@ -432,14 +432,14 @@
               description = 'compiling with g++, with no MPI support',
               compiler = 'g++',
               compileroptions = '-Wno-deprecated '+pedantic_mode+'-Wno-long-long -ftemplate-depth-100 ' \
-                      + gcc_opt_options + "-Wno-variadic-macros",
+                      + gcc_opt_options + ' -Wno-variadic-macros',
               cpp_definitions = ['USING_MPI=0'],
               linker = 'g++' )
 
 pymakeOption( name = 'g++3',
               description = 'compiling with g++3 (version 3.0), with no MPI support',
               compiler = 'g++3',
-              compileroptions = '-Wno-deprecated '+pedantic_mode+'-ftemplate-depth-100' + "-Wno-variadic-macros",
+              compileroptions = '-Wno-deprecated '+pedantic_mode+'-ftemplate-depth-100' + ' -Wno-variadic-macros',
               cpp_definitions = ['USING_MPI=0'],
               linker = 'g++3' )
 
@@ -449,7 +449,7 @@
               compiler = 'g++',
               compileroptions = '-Wno-deprecated '+pedantic_mode \
                               + '-Wno-long-long -ftemplate-depth-100 '\
-                              + '-mno-cygwin' + "-Wno-variadic-macros",
+                              + '-mno-cygwin' + ' -Wno-variadic-macros',
               cpp_definitions = ['USING_MPI=0', '_MINGW_'],
               linker = 'g++ -mno-cygwin' )
 



From nouiz at mail.berlios.de  Mon Apr  2 20:01:06 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 2 Apr 2007 20:01:06 +0200
Subject: [Plearn-commits] r6813 - in trunk: . plearn/base
Message-ID: <200704021801.l32I16Iq011479@sheep.berlios.de>

Author: nouiz
Date: 2007-04-02 20:01:05 +0200 (Mon, 02 Apr 2007)
New Revision: 6813

Modified:
   trunk/plearn/base/plerror.h
   trunk/pymake.config.model
Log:
Put the printing of more information of PLERROR in comment as it break the build on gcc version under 4


Modified: trunk/plearn/base/plerror.h
===================================================================
--- trunk/plearn/base/plerror.h	2007-04-02 17:09:21 UTC (rev 6812)
+++ trunk/plearn/base/plerror.h	2007-04-02 18:01:05 UTC (rev 6813)
@@ -57,8 +57,8 @@
 extern ostream* error_stream;
 #endif
 
-#define PLERROR(...)   errormsg2(__FILE__,__LINE__,__VA_ARGS__)
-//#define PLERROR   errormsg //Use if the compiler don't like variadic macros
+//#define PLERROR(...)   errormsg2(__FILE__,__LINE__,__VA_ARGS__)
+#define PLERROR   errormsg //Use if the compiler don't like variadic macros
 #define PLWARNING warningmsg
 #define PLDEPRECATED deprecationmsg
 

Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2007-04-02 17:09:21 UTC (rev 6812)
+++ trunk/pymake.config.model	2007-04-02 18:01:05 UTC (rev 6813)
@@ -432,14 +432,14 @@
               description = 'compiling with g++, with no MPI support',
               compiler = 'g++',
               compileroptions = '-Wno-deprecated '+pedantic_mode+'-Wno-long-long -ftemplate-depth-100 ' \
-                      + gcc_opt_options + ' -Wno-variadic-macros',
+                      + gcc_opt_options,
               cpp_definitions = ['USING_MPI=0'],
               linker = 'g++' )
 
 pymakeOption( name = 'g++3',
               description = 'compiling with g++3 (version 3.0), with no MPI support',
               compiler = 'g++3',
-              compileroptions = '-Wno-deprecated '+pedantic_mode+'-ftemplate-depth-100' + ' -Wno-variadic-macros',
+              compileroptions = '-Wno-deprecated '+pedantic_mode+'-ftemplate-depth-100',
               cpp_definitions = ['USING_MPI=0'],
               linker = 'g++3' )
 
@@ -449,7 +449,7 @@
               compiler = 'g++',
               compileroptions = '-Wno-deprecated '+pedantic_mode \
                               + '-Wno-long-long -ftemplate-depth-100 '\
-                              + '-mno-cygwin' + ' -Wno-variadic-macros',
+                              + '-mno-cygwin',
               cpp_definitions = ['USING_MPI=0', '_MINGW_'],
               linker = 'g++ -mno-cygwin' )
 



From chapados at mail.berlios.de  Mon Apr  2 20:29:19 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Mon, 2 Apr 2007 20:29:19 +0200
Subject: [Plearn-commits] r6814 - trunk/plearn/var
Message-ID: <200704021829.l32ITJ0Q013216@sheep.berlios.de>

Author: chapados
Date: 2007-04-02 20:29:19 +0200 (Mon, 02 Apr 2007)
New Revision: 6814

Modified:
   trunk/plearn/var/ObjectOptionVariable.cc
   trunk/plearn/var/ObjectOptionVariable.h
Log:
- Bugfix in fprop in the case of sliced variables -- major impact
  on optimisation results when using Automatic Relevance Determination
  with Gaussian Processes

- Removed the option to exponentiate a variable, since this was not
  used and complicated the code unnecessarily.


Modified: trunk/plearn/var/ObjectOptionVariable.cc
===================================================================
--- trunk/plearn/var/ObjectOptionVariable.cc	2007-04-02 18:01:05 UTC (rev 6813)
+++ trunk/plearn/var/ObjectOptionVariable.cc	2007-04-02 18:29:19 UTC (rev 6814)
@@ -60,19 +60,17 @@
     "a changeOption on the object.\n");
 
 ObjectOptionVariable::ObjectOptionVariable()
-    : m_log_variable(false),
-      m_final_object(0),
+    : m_final_object(0),
       m_option_type(OptionTypeUnknown),
       m_option_int(0),
       m_index(-1)
 { }
 
 ObjectOptionVariable::ObjectOptionVariable(PP<Object> root, const string& option_name,
-                                           const string& initial_value, bool log_variable)
+                                           const string& initial_value)
     : m_root(root),
       m_option_name(option_name),
       m_initial_value(initial_value),
-      m_log_variable(log_variable),
       m_final_object(0),
       m_option_type(OptionTypeUnknown),
       m_option_int(0),
@@ -159,22 +157,32 @@
     case OptionTypeVec:
         // Assume row vector
         PLASSERT( m_option_vec );
-        inherited::resize(1, m_option_vec->size());
-        value << *m_option_vec;
+        if (m_index < 0 || m_option_vec->size() == 0) {
+            inherited::resize(1, m_option_vec->size());
+            value << *m_option_vec;
+        }
+        else {
+            inherited::resize(1,1);
+            value[0] = (*m_option_vec)[m_index];
+        }
         break;
         
     case OptionTypeMat:
+        // Indexing works for ROWS in the case of matrices
         PLASSERT( m_option_mat );
-        inherited::resize(m_option_mat->length(), m_option_mat->width());
-        matValue << *m_option_mat;
+        if (m_index < 0 || m_option_mat->length() == 0) {
+            inherited::resize(m_option_mat->length(), m_option_mat->width());
+            matValue << *m_option_mat;
+        }
+        else {
+            inherited::resize(1, m_option_mat->width());
+            value << (*m_option_mat)(m_index);
+        }
         break;
 
     default:
         PLASSERT( false );
     }
-
-    if (m_log_variable)
-        compute_log(value, value);
 }
 
 void ObjectOptionVariable::build()
@@ -197,37 +205,38 @@
     // Depending on the type of the option, set the proper contents of the option.
     switch (m_option_type) {
     case OptionTypeInt:
+    {
         PLASSERT( m_option_int );
-        if (m_log_variable)
-            *m_option_int = int(exp(value[0]));
-        else
-            *m_option_int = int(value[0]);
+        *m_option_int = int(value[0]);
         break;
+    }
 
     case OptionTypeReal:
+    {
         PLASSERT( m_option_real );
-        if (m_log_variable)
-            *m_option_real = exp(value[0]);
-        else
-            *m_option_real = value[0];
+        *m_option_real = value[0];
         break;
+    }
 
     case OptionTypeVec:
+    {
         PLASSERT( m_option_vec );
-        if (m_log_variable)
-            exp(value, *m_option_vec);
+        if (m_index < 0)
+            *m_option_vec << value;
         else
-            *m_option_vec << value;
+            (*m_option_vec)[m_index] = value[0];
         break;
+    }
 
     case OptionTypeMat:
+    {
         PLASSERT( m_option_mat );
-        *m_option_mat << matValue;
-        if (m_log_variable) {
-            Vec option_vec = m_option_mat->toVec();
-            exp(option_vec, option_vec);
-        }
+        if (m_index < 0)
+            *m_option_mat << matValue;
+        else
+            (*m_option_mat)(m_index)<< value;
         break;
+    }
 
     default:
         PLASSERT( false );

Modified: trunk/plearn/var/ObjectOptionVariable.h
===================================================================
--- trunk/plearn/var/ObjectOptionVariable.h	2007-04-02 18:01:05 UTC (rev 6813)
+++ trunk/plearn/var/ObjectOptionVariable.h	2007-04-02 18:29:19 UTC (rev 6814)
@@ -77,19 +77,11 @@
      */
     string m_initial_value;
 
-    /**
-     *  If true, the contents ('value') of the variable is assumed to be the
-     *  log of the contents of the mirrored object option.  This can be useful
-     *  when performing unconstrained optimization on the variable, but the
-     *  option values should remain positive.
-     */
-    bool m_log_variable;
-    
 public:
     //!  Default constructor for persistence
     ObjectOptionVariable();
     ObjectOptionVariable(PP<Object> root, const string& option_name,
-                         const string& initial_value="", bool log_variable=false);
+                         const string& initial_value="");
 
     
     //#####  PLearn::Object Protocol  #########################################



From chapados at mail.berlios.de  Tue Apr  3 01:02:32 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Tue, 3 Apr 2007 01:02:32 +0200
Subject: [Plearn-commits] r6815 - trunk/python_modules/plearn/math
Message-ID: <200704022302.l32N2WTH019604@sheep.berlios.de>

Author: chapados
Date: 2007-04-03 01:02:31 +0200 (Tue, 03 Apr 2007)
New Revision: 6815

Modified:
   trunk/python_modules/plearn/math/arrays.py
Log:
Minor conveniences to improve numarray compatibility under python 2.3

Modified: trunk/python_modules/plearn/math/arrays.py
===================================================================
--- trunk/python_modules/plearn/math/arrays.py	2007-04-02 18:29:19 UTC (rev 6814)
+++ trunk/python_modules/plearn/math/arrays.py	2007-04-02 23:02:31 UTC (rev 6815)
@@ -212,6 +212,12 @@
     arrx = array(x)
     return sum(arrx,axis) / arrx.shape[axis]
 
+def any(x):
+    return logical_or.reduce(ravel(x))
+
+def all(x):
+    return logical_and.reduce(ravel(x))
+
 if __name__ == '__main__':
     a = array(range(10))
     print "lag(%s): %s"%(a, lag(a))



From dorionc at mail.berlios.de  Tue Apr  3 14:58:48 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Tue, 3 Apr 2007 14:58:48 +0200
Subject: [Plearn-commits] r6816 - trunk/python_modules/plearn/parallel
Message-ID: <200704031258.l33CwmKo029322@sheep.berlios.de>

Author: dorionc
Date: 2007-04-03 14:58:47 +0200 (Tue, 03 Apr 2007)
New Revision: 6816

Modified:
   trunk/python_modules/plearn/parallel/dispatch.py
Log:
An attempt to avoid blocking read() calls

Modified: trunk/python_modules/plearn/parallel/dispatch.py
===================================================================
--- trunk/python_modules/plearn/parallel/dispatch.py	2007-04-02 23:02:31 UTC (rev 6815)
+++ trunk/python_modules/plearn/parallel/dispatch.py	2007-04-03 12:58:47 UTC (rev 6816)
@@ -217,7 +217,8 @@
             logging.debug("* select.select() returned list of len=%d"%len(iwtd))
             for fromchild in iwtd:
                 ready = cls._child_processes[fromchild]
-                read_str = ready.process.fromchild.read(BUFSIZE)
+                read_str = ready.process.fromchild.read()
+                #read_str = ready.process.fromchild.read(BUFSIZE)
                 if hasattr(ready, 'logfile'):
                     ready.logfile.write(read_str)
 
@@ -261,6 +262,9 @@
 
             # Always using no wait to get the process id...
             self.process = Popen4(command)
+            flags = fcntl.fcntl(self.process.fromchild, fcntl.F_GETFL)
+            fcntl.fcntl(self.process.fromchild, fcntl.F_SETFL, flags | os.O_NONBLOCK)
+
             task_signature = "[%s]"%command
             if LOGDIR and os.path.isdir(LOGDIR):
                 filepath = os.path.join(LOGDIR, self.getLogFileBaseName())
@@ -329,7 +333,11 @@
     
     def listAvailableMachines(cls):
         for m in cls._machines:
-            loadavg = cls.getLoadAvg(m)
+            try:
+                loadavg = cls.getLoadAvg(m)
+            except Exception:
+                continue # Machine is rebooting, shut down, ...
+
             max_loadavg = MAX_LOADAVG.get(m, cls._max_load)
             #print "Load %f / %f"%(loadavg, max_loadavg)
             if loadavg < max_loadavg:



From nouiz at mail.berlios.de  Tue Apr  3 17:04:17 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 3 Apr 2007 17:04:17 +0200
Subject: [Plearn-commits] r6817 - trunk
Message-ID: <200704031504.l33F4H6t005233@sheep.berlios.de>

Author: nouiz
Date: 2007-04-03 17:04:16 +0200 (Tue, 03 Apr 2007)
New Revision: 6817

Modified:
   trunk/pymake.config.model
Log:
Modified the config file for compilation to allow the correct 
configuration of python on system that don't have predefined value.

I put the activation of those modif in comment as it will require the 
recompilation for everybody


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2007-04-03 12:58:47 UTC (rev 6816)
+++ trunk/pymake.config.model	2007-04-03 15:04:16 UTC (rev 6817)
@@ -246,8 +246,8 @@
 ### find a better way to know where to find the libraries.
 ### TODO Should find a way to automatically find python version ?
 if domain_name.endswith('iro.umontreal.ca'):
+    optionargs += [ 'python24' ]
     python_version = '2.4'
-    cpp_definitions += [ 'PL_PYTHON_VERSION=240' ]
     python_lib_root = '/usr/lib'
     if platform == 'cygwin':
         numpy_includedirs = [ ]
@@ -266,30 +266,36 @@
 elif domain_name.endswith('.ms'):
     numpy_includedirs = [ '/opt/python2.4/include' ]
     numpy_site_packages = join(homedir, '../delallea/local/lib/python2.4/site-packages/numarray -lutil')
+    optionargs += [ 'python24' ]
     python_version = '2.4'
-    cpp_definitions += [ 'PL_PYTHON_VERSION=240' ]
     python_lib_root = '/opt/python2.4/lib'
     linkeroptions_tail += '-lgcc_eh -lunwind -lcprts'
 
 elif domain_name.endswith('.rqchp.qc.ca'):
     numpy_includedirs   = [ '/usr/network.ALTIX/python-2.4.1/include' ]
     numpy_site_packages = join(homedir, '../delallea/local/lib/python2.4/site-packages/numarray -lutil')
+    optionargs += [ 'python24' ]
     python_version = '2.4'
-    cpp_definitions += [ 'PL_PYTHON_VERSION=240' ]
     python_lib_root = '/usr/network.ALTIX/python-2.4.1/lib'
     linkeroptions_tail += '-lcprts -lifcore -lcxa' # -lunwind -lguide' # -lguide_stats -lifcoremt -lifport -limf -lipr'
 elif platform=='darwin':
+    optionargs += [ 'python'+python_version[0]+python_version[2] ]
     python_version = sys.version[0:3]
     # numpy_includedirs   = [ '/usr/network.ALTIX/python-2.4.1/include' ]
-    cpp_definitions += [ 'PL_PYTHON_VERSION='+python_version[0]+python_version[2]+'0' ]
     python_lib_root = '/sw/lib'
     numpy_site_packages = '/sw/lib/python'+python_version+'/site-packages/numarray' 
     numpy_includedirs   = [ '/sw/include/python'+python_version]
 else:
-    numpy_includedirs   = []
-    numpy_site_packages = '/usr/lib/python2.3/site-packages/numarray' 
-    python_version = '2.3'
+    if 'python24' in optionargs:
+        python_version = '2.4'
+    elif 'python25' in optionargs:
+        python_version = '2.5'
+    else:
+        python_version = '2.3'
+
     python_lib_root = '/usr/lib'
+    numpy_site_packages = '/usr/lib/python'+python_version+'/site-packages/numarray'
+    numpy_includedirs   = [ '/usr/include/python'+python_version]
 
 if platform!='darwin':
     optionalLibrary( name = 'python',
@@ -409,6 +415,7 @@
   [ 'double', 'float' ],
   
   # [ 'throwerrors', 'exiterrors' ],
+  #[ 'python23', 'python24', 'python25' ],
   
   [ 'blas', 'p3blas','p4blas','athlonblas','pentiumblas', 'mammouthblas',
     'noblas', 'veclib', 'scs'],
@@ -593,6 +600,23 @@
 cpp_variables += ['USEFLOAT', 'USEDOUBLE']
 
 
+#####  Python version  ######################################################
+
+pymakeOption( name = 'python23',
+              description = 'the installed version of python is 2.3.X',
+              cpp_definitions = ['PL_PYTHON_VERSION=230'])
+
+pymakeOption( name = 'python24',
+              description = 'the installed version of python is 2.4.X',
+              cpp_definitions = ['PL_PYTHON_VERSION=240'] )
+pymakeOption( name = 'python25',
+              description = 'the installed version of python is 2.5.X',
+              cpp_definitions = ['PL_PYTHON_VERSION=250'] )
+
+cpp_variables += ['PL_PYTHON_VERSION=250','PL_PYTHON_VERSION=240','PL_PYTHON_VERSION=230']
+
+
+
 #####  Error Behavior  ######################################################
 
 # Not used any more, 'USE_EXCEPTIONS' is defined earlier in this file.



From nouiz at mail.berlios.de  Tue Apr  3 21:48:06 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 3 Apr 2007 21:48:06 +0200
Subject: [Plearn-commits] r6818 - trunk
Message-ID: <200704031948.l33Jm61g010235@sheep.berlios.de>

Author: nouiz
Date: 2007-04-03 21:48:06 +0200 (Tue, 03 Apr 2007)
New Revision: 6818

Modified:
   trunk/pymake.config.model
Log:
Activeted the change about the python version, as my last commit broke the build. Sorry about that.


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2007-04-03 15:04:16 UTC (rev 6817)
+++ trunk/pymake.config.model	2007-04-03 19:48:06 UTC (rev 6818)
@@ -415,7 +415,7 @@
   [ 'double', 'float' ],
   
   # [ 'throwerrors', 'exiterrors' ],
-  #[ 'python23', 'python24', 'python25' ],
+  [ 'python23', 'python24', 'python25' ],
   
   [ 'blas', 'p3blas','p4blas','athlonblas','pentiumblas', 'mammouthblas',
     'noblas', 'veclib', 'scs'],



From chrish at mail.berlios.de  Tue Apr  3 23:09:09 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Tue, 3 Apr 2007 23:09:09 +0200
Subject: [Plearn-commits] r6819 - in tags: .
	SAFIR-Warehouse-3.1beta/plearn_learners/testers
Message-ID: <200704032109.l33L992D018409@sheep.berlios.de>

Author: chrish
Date: 2007-04-03 23:09:08 +0200 (Tue, 03 Apr 2007)
New Revision: 6819

Added:
   tags/SAFIR-Warehouse-3.1beta/
Modified:
   tags/SAFIR-Warehouse-3.1beta/plearn_learners/testers/PTester.cc
Log:
Tag pour release SAFIR-Warehouse 3.1beta

Copied: tags/SAFIR-Warehouse-3.1beta (from rev 6818, trunk)

Modified: tags/SAFIR-Warehouse-3.1beta/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2007-04-03 19:48:06 UTC (rev 6818)
+++ tags/SAFIR-Warehouse-3.1beta/plearn_learners/testers/PTester.cc	2007-04-03 21:09:08 UTC (rev 6819)
@@ -434,13 +434,39 @@
 
     if (expdir != "")
     {
+#if 0
+        // Make sure the directories above the expdir already exist.
+        force_mkdir(expdir.up());
+
+        // There are still race conditions here. However fixing them would
+        // involve a complete rework of PLearn's way of managing expdirs. At
+        // least, when using pyplearn's, expdir creation is now race-free.
+        if (!mkdir_lowlevel(expdir)) {
+            if (!isdir(expdir))
+                PLERROR("In PTester Could not create experiment directory %s",expdir.c_str());
+            else {
+                // When using a pyplearn, the pyplearn script is responsible for
+                // creating the expdir (so it can do it in a race-free way). To
+                // make this work with enforce_clean_expdir, we consider an expdir
+                // that exists but does not contain anything as "clean".
+                if (enforce_clean_expdir && lsdir(expdir).size() > 0)
+                    PLERROR("Directory (or file) %s already exists.\n"
+                            "First move it out of the way.", expdir.c_str());
+            }
+        }
+#else
+        // XXX Given that force_mkdir() returns true if the directory already
+        // exists, this is a textbook example of a race condition.
         if (pathexists(expdir) && enforce_clean_expdir)
             PLERROR("Directory (or file) %s already exists.\n"
                     "First move it out of the way.", expdir.c_str());
         if (!force_mkdir(expdir))
             PLERROR("In PTester Could not create experiment directory %s",expdir.c_str());
+#endif
         expdir = expdir.absolute() / "";
 
+        
+
         // Save this tester description in the expdir
         if (save_initial_tester)
             PLearn::save(expdir / "tester.psave", *this);



From tihocan at mail.berlios.de  Wed Apr  4 00:07:27 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 4 Apr 2007 00:07:27 +0200
Subject: [Plearn-commits] r6820 - trunk/plearn/vmat
Message-ID: <200704032207.l33M7RLS021439@sheep.berlios.de>

Author: tihocan
Date: 2007-04-04 00:07:27 +0200 (Wed, 04 Apr 2007)
New Revision: 6820

Modified:
   trunk/plearn/vmat/BootstrapVMatrix.cc
Log:
Can now also sort elements when allowing repetitions

Modified: trunk/plearn/vmat/BootstrapVMatrix.cc
===================================================================
--- trunk/plearn/vmat/BootstrapVMatrix.cc	2007-04-03 21:09:08 UTC (rev 6819)
+++ trunk/plearn/vmat/BootstrapVMatrix.cc	2007-04-03 22:07:27 UTC (rev 6820)
@@ -175,8 +175,9 @@
             indices = TVec<int>(0, l-1, 1); // Range-vector
             rgen->shuffleElements(indices);
             indices = indices.subVec(0, nsamp);
-            if (!shuffle) sortElements(indices);
         }
+        if (!shuffle)
+            sortElements(indices);
 
         // Because we changed the indices, a rebuild may be needed.
         inherited::build();



From tihocan at mail.berlios.de  Wed Apr  4 00:08:08 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 4 Apr 2007 00:08:08 +0200
Subject: [Plearn-commits] r6821 - trunk/plearn/vmat
Message-ID: <200704032208.l33M88Sg021474@sheep.berlios.de>

Author: tihocan
Date: 2007-04-04 00:08:08 +0200 (Wed, 04 Apr 2007)
New Revision: 6821

Modified:
   trunk/plearn/vmat/BootstrapSplitter.cc
Log:
Fixed bug where allow_repetitions was not working properly

Modified: trunk/plearn/vmat/BootstrapSplitter.cc
===================================================================
--- trunk/plearn/vmat/BootstrapSplitter.cc	2007-04-03 22:07:27 UTC (rev 6820)
+++ trunk/plearn/vmat/BootstrapSplitter.cc	2007-04-03 22:08:08 UTC (rev 6821)
@@ -101,8 +101,11 @@
         for (int i = 0; i < n_splits; i++) {
             // Construct a new bootstrap sample from the dataset.
             PP<PRandom> vmat_rgen= rgen->split();
+            // Note: indices in the bootstrapped sets are sorted, so that
+            // access may be faster (e.g. when reading large data from disk).
             bootstrapped_sets(i,0) = 
-                new BootstrapVMatrix(dataset,frac,vmat_rgen,allow_repetitions);
+                new BootstrapVMatrix(dataset, frac, vmat_rgen, false,
+                                     allow_repetitions);
         }
     } else {
         bootstrapped_sets.resize(0,0);



From tihocan at mail.berlios.de  Wed Apr  4 00:08:28 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 4 Apr 2007 00:08:28 +0200
Subject: [Plearn-commits] r6822 - trunk/commands
Message-ID: <200704032208.l33M8S3I021547@sheep.berlios.de>

Author: tihocan
Date: 2007-04-04 00:08:28 +0200 (Wed, 04 Apr 2007)
New Revision: 6822

Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
Including BaggingLearner and BootstrapSplitter

Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2007-04-03 22:08:08 UTC (rev 6821)
+++ trunk/commands/plearn_noblas_inc.h	2007-04-03 22:08:28 UTC (rev 6822)
@@ -135,7 +135,6 @@
  ************/
 
 // Classifiers
-#include <plearn_learners/meta/AdaBoost.h>
 #include <plearn_learners/classifiers/BinaryStump.h>
 #include <plearn_learners/classifiers/ClassifierFromConditionalPDistribution.h>
 #include <plearn_learners/classifiers/ClassifierFromDensity.h>
@@ -161,6 +160,9 @@
 // Hyper
 #include <plearn_learners/hyper/HyperLearner.h>
 
+// Meta
+#include <plearn_learners/meta/BaggingLearner.h>
+
 // Regressors
 #include <plearn_learners/regressors/ConstantRegressor.h>
 #include <plearn_learners/regressors/GaussianProcessRegressor.h>
@@ -217,6 +219,7 @@
  * Splitter *
  ************/
 #include <plearn/vmat/BinSplitter.h>
+#include <plearn/vmat/BootstrapSplitter.h>
 #include <plearn/vmat/ClassSeparationSplitter.h>
 #include <plearn/vmat/ConcatSetsSplitter.h>
 #include <plearn/vmat/DBSplitter.h>



From tihocan at mail.berlios.de  Wed Apr  4 00:10:01 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 4 Apr 2007 00:10:01 +0200
Subject: [Plearn-commits] r6823 - trunk/plearn_learners/meta
Message-ID: <200704032210.l33MA1hw021656@sheep.berlios.de>

Author: tihocan
Date: 2007-04-04 00:10:01 +0200 (Wed, 04 Apr 2007)
New Revision: 6823

Modified:
   trunk/plearn_learners/meta/BaggingLearner.cc
Log:
- Removed build statements in build_(), which are not needed since objects should already be built
- Setting stage=1 after training
- Fixed bug in resetInternalState: forget() was called on the sub-learners, which should not be the case


Modified: trunk/plearn_learners/meta/BaggingLearner.cc
===================================================================
--- trunk/plearn_learners/meta/BaggingLearner.cc	2007-04-03 22:08:28 UTC (rev 6822)
+++ trunk/plearn_learners/meta/BaggingLearner.cc	2007-04-03 22:10:01 UTC (rev 6823)
@@ -83,12 +83,15 @@
     inherited::declareOptions(ol);
 }
 
+////////////
+// build_ //
+////////////
 void BaggingLearner::build_()
-{
-    if(splitter) splitter->build();
-    if(template_learner) template_learner->build();
-}
+{}
 
+///////////
+// build //
+///////////
 void BaggingLearner::build()
 {
     inherited::build();
@@ -148,11 +151,13 @@
     // sequential train
     for(int i= 0; i < nbags; ++i)
     {
-        PLearner& l= *learners[i];
-        l.setTrainingSet(splitter->getSplit(i)[0]);
-        l.train();
+        PP<PLearner> l = learners[i];
+        l->setTrainingSet(splitter->getSplit(i)[0]);
+        l->train();
     }
 
+    stage++;
+    PLASSERT( stage == 1 );
 }
 
 void BaggingLearner::computeOutput(const Vec& input, Vec& output) const
@@ -215,21 +220,36 @@
     return template_learner->getTrainCostNames();
 }
 
+////////////////
+// nTestCosts //
+////////////////
 int BaggingLearner::nTestCosts() const
 {
     PLASSERT(template_learner);
     return template_learner->nTestCosts();
 }
+
+/////////////////
+// nTrainCosts //
+/////////////////
 int BaggingLearner::nTrainCosts() const
 {
     PLASSERT(template_learner);
     return template_learner->nTrainCosts();
 }
+
+////////////////////////
+// resetInternalState //
+////////////////////////
 void BaggingLearner::resetInternalState()
 {
     for(int i= 0; i < learners.length(); ++i)
-        learners[i]->forget();
+        learners[i]->resetInternalState();
 }
+
+///////////////////////
+// isStatefulLearner //
+///////////////////////
 bool BaggingLearner::isStatefulLearner() const
 {
     PLASSERT(template_learner);



From tihocan at mail.berlios.de  Wed Apr  4 00:10:24 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 4 Apr 2007 00:10:24 +0200
Subject: [Plearn-commits] r6824 - trunk/plearn_learners/generic
Message-ID: <200704032210.l33MAO1K021692@sheep.berlios.de>

Author: tihocan
Date: 2007-04-04 00:10:23 +0200 (Wed, 04 Apr 2007)
New Revision: 6824

Modified:
   trunk/plearn_learners/generic/PLearner.cc
Log:
Stage is now reset to 0 in forget()

Modified: trunk/plearn_learners/generic/PLearner.cc
===================================================================
--- trunk/plearn_learners/generic/PLearner.cc	2007-04-03 22:10:01 UTC (rev 6823)
+++ trunk/plearn_learners/generic/PLearner.cc	2007-04-03 22:10:23 UTC (rev 6824)
@@ -547,8 +547,12 @@
 {
     if (random_gen && seed_ != 0)
         random_gen->manual_seed(seed_);
+    stage = 0;
 }
 
+////////////////
+// nTestCosts //
+////////////////
 int PLearner::nTestCosts() const 
 { 
     if(n_test_costs_<0)
@@ -556,6 +560,9 @@
     return n_test_costs_;
 }
 
+/////////////////
+// nTrainCosts //
+/////////////////
 int PLearner::nTrainCosts() const 
 { 
     if(n_train_costs_<0)



From tihocan at mail.berlios.de  Wed Apr  4 00:10:46 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 4 Apr 2007 00:10:46 +0200
Subject: [Plearn-commits] r6825 - trunk/plearn_learners/generic
Message-ID: <200704032210.l33MAkRl021724@sheep.berlios.de>

Author: tihocan
Date: 2007-04-04 00:10:45 +0200 (Wed, 04 Apr 2007)
New Revision: 6825

Modified:
   trunk/plearn_learners/generic/AddCostToLearner.cc
Log:
Bug fix: costs was not properly resized

Modified: trunk/plearn_learners/generic/AddCostToLearner.cc
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.cc	2007-04-03 22:10:23 UTC (rev 6824)
+++ trunk/plearn_learners/generic/AddCostToLearner.cc	2007-04-03 22:10:45 UTC (rev 6825)
@@ -236,6 +236,7 @@
 {
     int n_original_costs = learner_->nTestCosts();
     // We give only costs.subVec to the sub-learner because it may want to resize it.
+    costs.resize(nTestCosts());
     Vec sub_costs = costs.subVec(0, n_original_costs);
     int target_length = target.length();
     if (compute_costs_on_bags) {



From yoshua at mail.berlios.de  Wed Apr  4 01:02:47 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Wed, 4 Apr 2007 01:02:47 +0200
Subject: [Plearn-commits] r6826 - in trunk: . commands/PLearnCommands
	plearn/misc plearn_learners/generic/EXPERIMENTAL
Message-ID: <200704032302.l33N2lwe002289@sheep.berlios.de>

Author: yoshua
Date: 2007-04-04 01:02:46 +0200 (Wed, 04 Apr 2007)
New Revision: 6826

Modified:
   trunk/commands/PLearnCommands/plearn_main.cc
   trunk/plearn/misc/PLearnService.cc
   trunk/plearn/misc/RemotePLearnServer.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
   trunk/pymake.config.model
Log:


Modified: trunk/commands/PLearnCommands/plearn_main.cc
===================================================================
--- trunk/commands/PLearnCommands/plearn_main.cc	2007-04-03 22:10:45 UTC (rev 6825)
+++ trunk/commands/PLearnCommands/plearn_main.cc	2007-04-03 23:02:46 UTC (rev 6826)
@@ -50,7 +50,9 @@
 #include <plearn/io/pl_log.h>
 #include <plearn/math/random.h>
 #include <plearn/misc/Calendar.h>
+#ifndef BUGGED_SERVER
 #include <plearn/misc/PLearnService.h>
+#endif
 #include <plearn/vmat/VMat.h>
 
 #if USING_MPI
@@ -202,6 +204,7 @@
     // Option for parallel processing through PLearnService
     int servers_pos = findpos(command_line, "--servers");
     int serversfile_pos = -1;
+#ifndef BUGGED_SERVER
     if (servers_pos != -1)
     {
         serversfile_pos = servers_pos+1;
@@ -210,6 +213,7 @@
         string serversfile = command_line[serversfile_pos];
         PLearnService::instance().connectToServers(serversfile);
     }
+#endif
   
     // The following removes the options from the command line. It also
     // parses the plearn command as being the first non-optional argument on

Modified: trunk/plearn/misc/PLearnService.cc
===================================================================
--- trunk/plearn/misc/PLearnService.cc	2007-04-03 22:10:45 UTC (rev 6825)
+++ trunk/plearn/misc/PLearnService.cc	2007-04-03 23:02:46 UTC (rev 6826)
@@ -49,6 +49,7 @@
 #include <plearn/io/pl_log.h>
 #include <plearn/io/Poll.h>
 
+#ifndef BUGGED_SERVER
 namespace PLearn {
 using namespace std;
 
@@ -441,6 +442,7 @@
 
 
 } // end of namespace PLearn
+#endif
 
 
 /*

Modified: trunk/plearn/misc/RemotePLearnServer.cc
===================================================================
--- trunk/plearn/misc/RemotePLearnServer.cc	2007-04-03 22:10:45 UTC (rev 6825)
+++ trunk/plearn/misc/RemotePLearnServer.cc	2007-04-03 23:02:46 UTC (rev 6826)
@@ -42,6 +42,7 @@
 /*! \file RemotePLearnServer.cc */
 
 
+#ifndef BUGGED_SERVER
 #include "RemotePLearnServer.h"
 #include "PLearnService.h"
 #include <plearn/io/pl_log.h>
@@ -268,6 +269,7 @@
 
 
 } // end of namespace PLearn
+#endif
 
 
 /*

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-04-03 22:10:45 UTC (rev 6825)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-04-03 23:02:46 UTC (rev 6826)
@@ -58,6 +58,7 @@
     : noutputs(-1),
       init_lrate(0.01),
       lrate_decay(0),
+      output_layer_lrate_scale(1),
       minibatch_size(1),
       output_type("NLL"),
       verbosity(0),
@@ -109,6 +110,11 @@
                   OptionBase::buildoption,
                   "Learning rate decay factor\n");
 
+    declareOption(ol, "output_layer_lrate_scale", &NatGradNNet::output_layer_lrate_scale,
+                  OptionBase::buildoption,
+                  "Scaling factor of the learning rate for the output layer. Values less than 1"
+                  "mean that the output layer parameters have a smaller learning rate than the others.\n");
+
     declareOption(ol, "minibatch_size", &NatGradNNet::minibatch_size,
                   OptionBase::buildoption,
                   "Update the parameters only so often (number of examples).\n");
@@ -394,6 +400,7 @@
 
         Mat previous_neurons_gradient = neuron_gradients_per_layer[i-1];
         Mat previous_neurons_output = neuron_outputs_per_layer[i-1];
+        real layer_lrate_factor = (i==n_layers-1)?output_layer_lrate_scale:1;
         // optionally correct the gradient on neurons using their covariance
         if (neurons_natgrad_template && neurons_natgrad_per_layer[i])
         {
@@ -430,11 +437,13 @@
             // compute gradient on weights and update them in one go (more efficient)
             productScaleAcc(layer_params[i-1],neuron_gradients_per_layer[i],true,
                             neuron_extended_outputs_per_layer[i-1],false,
-                            -lrate/minibatch_size,1); // mean gradient, has less variance, can afford larger learning rate
+                            -layer_lrate_factor*lrate/minibatch_size,1); // mean gradient, has less variance, can afford larger learning rate
     }
-    if (full_natgrad) 
+    if (full_natgrad)
     {
         (*full_natgrad)(t/minibatch_size,all_params_gradient,all_params_delta); // compute update direction by natural gradient
+        if (output_layer_lrate_scale!=1.0)
+            layer_params_delta[n_layers-2] *= output_layer_lrate_scale; // scale output layer's learning rate 
         multiplyAcc(all_params,all_params_delta,-lrate); // update
     } else if (params_natgrad_template)
     {
@@ -442,8 +451,10 @@
         {
             NatGradEstimator& neuron_natgrad = *(params_natgrad_per_neuron[i]);
             neuron_natgrad(t/minibatch_size,neuron_params_gradient[i],neuron_params_delta[i]); // compute update direction by natural gradient
-            multiplyAcc(neuron_params[i],neuron_params_delta[i],-lrate); // update
         }
+        if (output_layer_lrate_scale!=1.0)
+            layer_params_delta[n_layers-2] *= output_layer_lrate_scale; // scale output layer's learning rate 
+        multiplyAcc(all_params,all_params_delta,-lrate); // update
     }
 }
 

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-04-03 22:10:45 UTC (rev 6825)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-04-03 23:02:46 UTC (rev 6826)
@@ -71,6 +71,9 @@
     //! learning rate decay factor
     real lrate_decay;
 
+    //! scaling factor of the learning rate for the output layer
+    real output_layer_lrate_scale;
+
     //! update the parameters only so often
     int minibatch_size;
 

Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2007-04-03 22:10:45 UTC (rev 6825)
+++ trunk/pymake.config.model	2007-04-03 23:02:46 UTC (rev 6826)
@@ -269,7 +269,8 @@
     optionargs += [ 'python24' ]
     python_version = '2.4'
     python_lib_root = '/opt/python2.4/lib'
-    linkeroptions_tail += '-lgcc_eh -lunwind -lcprts'
+    #linkeroptions_tail += '-lgcc_eh -lunwind -lcprts'
+    linkeroptions_tail += '-lunwind -lcprts'
 
 elif domain_name.endswith('.rqchp.qc.ca'):
     numpy_includedirs   = [ '/usr/network.ALTIX/python-2.4.1/include' ]
@@ -473,6 +474,7 @@
               # Disable some warnings:
               compiler = 'icpc -w1 -wd981 -wd383 -wd869 -wd444 -wd810 -wd1418 -wd654 -wd279',
               cpp_definitions = ['USING_MPI=0'],
+              compileroptions = '-DBUGGED_SERVER ',
               linker = 'icpc  '
               )
 



From larocheh at mail.berlios.de  Wed Apr  4 01:08:37 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 4 Apr 2007 01:08:37 +0200
Subject: [Plearn-commits] r6827 - trunk/plearn/var
Message-ID: <200704032308.l33N8bV2010718@sheep.berlios.de>

Author: larocheh
Date: 2007-04-04 01:08:36 +0200 (Wed, 04 Apr 2007)
New Revision: 6827

Modified:
   trunk/plearn/var/ConfRatedAdaboostCostVariable.cc
   trunk/plearn/var/ConfRatedAdaboostCostVariable.h
   trunk/plearn/var/GradientAdaboostCostVariable.cc
   trunk/plearn/var/GradientAdaboostCostVariable.h
Log:
Changed the comments...


Modified: trunk/plearn/var/ConfRatedAdaboostCostVariable.cc
===================================================================
--- trunk/plearn/var/ConfRatedAdaboostCostVariable.cc	2007-04-03 23:02:46 UTC (rev 6826)
+++ trunk/plearn/var/ConfRatedAdaboostCostVariable.cc	2007-04-03 23:08:36 UTC (rev 6827)
@@ -50,8 +50,9 @@
 
 PLEARN_IMPLEMENT_OBJECT(
     ConfRatedAdaboostCostVariable,
-    "Cost used for confidence-rated Adaboost ",
-    "NO HELP");
+    "Cost used for Confidence-rated Adaboost ",
+    "See \"Improved Boosting Algorithms Using Confidence-rated Predictions\" by\n"
+    "Schapire and Singer.");
 
 ////////////////////////////////////
 // ConfRatedAdaboostCostVariable //

Modified: trunk/plearn/var/ConfRatedAdaboostCostVariable.h
===================================================================
--- trunk/plearn/var/ConfRatedAdaboostCostVariable.h	2007-04-03 23:02:46 UTC (rev 6826)
+++ trunk/plearn/var/ConfRatedAdaboostCostVariable.h	2007-04-03 23:08:36 UTC (rev 6827)
@@ -48,12 +48,6 @@
 namespace PLearn {
 using namespace std;
 
-// cost[i] = exp(- alpha*signed_target[i]*signed_output[i])
-// where cost is a column vector and
-// where signed_target and signed_output is inferred from target as follows.
-// target must be in (0,1).
-// signed_target[i] = target[i]*2-1 and 
-// signed_output[i] = 2*output[i]-1
 class ConfRatedAdaboostCostVariable: public NaryVariable
 {
     typedef NaryVariable inherited;

Modified: trunk/plearn/var/GradientAdaboostCostVariable.cc
===================================================================
--- trunk/plearn/var/GradientAdaboostCostVariable.cc	2007-04-03 23:02:46 UTC (rev 6826)
+++ trunk/plearn/var/GradientAdaboostCostVariable.cc	2007-04-03 23:08:36 UTC (rev 6827)
@@ -50,9 +50,10 @@
 
 PLEARN_IMPLEMENT_OBJECT(
     GradientAdaboostCostVariable,
-    "Compute sigmoid of its first input, and then computes the negative "
-    "cross-entropy cost",
-    "NO HELP");
+    "Cost for weak learner in MarginBoost version of AdaBoost",
+    "Cost for a weak learner used in the functional gradient descent view of\n"
+    "boosting on a margin-based loss function. See \"Functional Gradient \n"
+    "Techniques for Combining Hypotheses\" by Mason et al.");
 
 ////////////////////////////////////
 // GradientAdaboostCostVariable //

Modified: trunk/plearn/var/GradientAdaboostCostVariable.h
===================================================================
--- trunk/plearn/var/GradientAdaboostCostVariable.h	2007-04-03 23:02:46 UTC (rev 6826)
+++ trunk/plearn/var/GradientAdaboostCostVariable.h	2007-04-03 23:08:36 UTC (rev 6827)
@@ -48,12 +48,10 @@
 namespace PLearn {
 using namespace std;
 
-// cost[i] = -1*signed_target[i]*signe_output[i]
-// where cost is a column vector and
-// where signed_target and signed_output is inferred from target as follows.
-// target must be in (0,1).
-// signed_target[i] = target[i]*2-1 and
-// signed_output[i] = output*[i]2-1
+//! Cost for weak learner in MarginBoost version of AdaBoost
+//! Cost for a weak learner used in the functional gradient descent view of
+//! boosting on a margin-based loss function. See "Functional Gradient 
+//! Techniques for Combining Hypotheses" by Mason et al.
 class GradientAdaboostCostVariable: public BinaryVariable
 {
     typedef BinaryVariable inherited;



From larocheh at mail.berlios.de  Wed Apr  4 01:09:25 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 4 Apr 2007 01:09:25 +0200
Subject: [Plearn-commits] r6828 - trunk/plearn_learners/generic
Message-ID: <200704032309.l33N9PqQ011413@sheep.berlios.de>

Author: larocheh
Date: 2007-04-04 01:09:24 +0200 (Wed, 04 Apr 2007)
New Revision: 6828

Modified:
   trunk/plearn_learners/generic/NNet.cc
Log:
Changed the comments for some of the costs functions...


Modified: trunk/plearn_learners/generic/NNet.cc
===================================================================
--- trunk/plearn_learners/generic/NNet.cc	2007-04-03 23:08:36 UTC (rev 6827)
+++ trunk/plearn_learners/generic/NNet.cc	2007-04-03 23:09:24 UTC (rev 6828)
@@ -273,8 +273,10 @@
         "  - \"stable_cross_entropy\" (more accurate backprop and possible regularization, for binary classification)\n"
         "  - \"margin_perceptron_cost\" (a hard version of the cross_entropy, uses the 'margin' option)\n"
         "  - \"lift_output\" (not a real cost function, just the output for lift computation)\n"
-        "  - \"conf_rated_adaboost_cost\" (for confidence rated Adaboost)\n"
-        "  - \"gradient_adaboost_cost\" (also for confidence rated Adaboost)\n"
+        "  - \"conf_rated_adaboost_cost\" (for Confidence-rated Adaboost)\n"
+        "  - \"gradient_adaboost_cost\" (for MarginBoost, see \"Functional \n"
+        "                                Gradient Techniques for Combining \n"
+        "                                Hypotheses\" by Mason et al.)\n"
         "  - \"poisson_nll\"\n"
         "  - \"L1\"\n"
         "The FIRST function of the list will be used as \n"



From larocheh at mail.berlios.de  Wed Apr  4 01:49:59 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 4 Apr 2007 01:49:59 +0200
Subject: [Plearn-commits] r6829 - trunk/plearn_learners/classifiers
Message-ID: <200704032349.l33Nnxc4006686@sheep.berlios.de>

Author: larocheh
Date: 2007-04-04 01:49:59 +0200 (Wed, 04 Apr 2007)
New Revision: 6829

Modified:
   trunk/plearn_learners/classifiers/BinaryStump.cc
Log:
Changed the comments...


Modified: trunk/plearn_learners/classifiers/BinaryStump.cc
===================================================================
--- trunk/plearn_learners/classifiers/BinaryStump.cc	2007-04-03 23:09:24 UTC (rev 6828)
+++ trunk/plearn_learners/classifiers/BinaryStump.cc	2007-04-03 23:49:59 UTC (rev 6829)
@@ -131,16 +131,21 @@
 }
 
 PLEARN_IMPLEMENT_OBJECT(BinaryStump, "Binary stump classifier", 
-                        "This algorithm finds the most accurate binary stumps that classifies to a certain tag (0 or 1)\n"
-                        "every points that have a certain feature (coordinate) higher than a learned threshold.\n"
-                        "The tag, feature and threshold are chosen to minimize the weighted error or classification.\n"
-                        "Only the first target is considered, the others are ignored.\n");
+                        "This algorithm finds the most accurate binary stump\n"
+                        "that classifies to a certain tag (0 or 1)\n"
+                        "every points that have a certain feature (coordinate)\n"
+                        "higher than a learned threshold.\n"
+                        "The tag, feature and threshold are chosen to minimize\n"
+                        "the weighted classification error.\n"
+                        "Only the first target is considered, the others are \n"
+                        "ignored.\n");
 
 void BinaryStump::declareOptions(OptionList& ol)
 {
     declareOption(ol, "feature", &BinaryStump::feature, OptionBase::learntoption,
                   "Feature tested by the stump");
-    declareOption(ol, "threshold", &BinaryStump::threshold, OptionBase::learntoption,
+    declareOption(ol, "threshold", &BinaryStump::threshold, 
+                  OptionBase::learntoption,
                   "Threshold for decision");
     declareOption(ol, "tag", &BinaryStump::tag, OptionBase::learntoption,
                   "Tag assigned when feature is lower than the threshold");
@@ -225,7 +230,6 @@
     real best_error = 0;
 
     {
-
         real w_sum_1 = 0;
         real w_sum_error = 0;
         real w_sum = 0;
@@ -360,7 +364,8 @@
 
     if(!fast_exact_is_equal(target[0], 0) &&
        !fast_exact_is_equal(target[0], 1))
-        PLERROR("In BinaryStump:computeCostsFromOutputs() : target should be either 1 or 0");
+        PLERROR("In BinaryStump:computeCostsFromOutputs() : "
+                "target should be either 1 or 0");
 
     costs[0] = !fast_exact_is_equal(output[0], target[0]); 
 }                                



From larocheh at mail.berlios.de  Wed Apr  4 02:02:31 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 4 Apr 2007 02:02:31 +0200
Subject: [Plearn-commits] r6830 - trunk/plearn_learners/meta
Message-ID: <200704040002.l3402Vs7007238@sheep.berlios.de>

Author: larocheh
Date: 2007-04-04 02:02:22 +0200 (Wed, 04 Apr 2007)
New Revision: 6830

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
   trunk/plearn_learners/meta/AdaBoost.h
Log:
Debugged et recommented...


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2007-04-03 23:49:59 UTC (rev 6829)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2007-04-04 00:02:22 UTC (rev 6830)
@@ -53,9 +53,16 @@
 using namespace std;
 
 AdaBoost::AdaBoost()
-    : sum_voting_weights(0.0), initial_sum_weights(0.0),
-      target_error(0.5), output_threshold(0.5), compute_training_error(1), 
-      pseudo_loss_adaboost(1), conf_rated_adaboost(0), weight_by_resampling(1), early_stopping(1),
+    : found_zero_error_weak_learner(0),
+      sum_voting_weights(0.0), 
+      initial_sum_weights(0.0),
+      target_error(0.5), 
+      output_threshold(0.5), 
+      compute_training_error(1), 
+      pseudo_loss_adaboost(1), 
+      conf_rated_adaboost(0), 
+      weight_by_resampling(1), 
+      early_stopping(1),
       save_often(0)
 { }
 
@@ -64,13 +71,32 @@
     "AdaBoost boosting algorithm for TWO-CLASS classification",
     "Given a classification weak-learner, this algorithm \"boosts\" it in\n"
     "order to obtain a much more powerful classification algorithm.\n"
-    "The classifier is two-class, returning 0 or 1, or a number between 0 and 1\n"
-    "(in that case the user can set the 'pseudo_loss_adaboost' option, which\n"
-    "computes a more precise notion of error taking into account the precise\n"
-    "value outputted by the soft classifier).\n"
+    "The classifier is two-class, returning 0 or 1, or a number between 0 and 1.\n"
+    "In the latter case, the user can use two different versions of AdaBoost:\n"
+    " - \"Pseudo-loss\" AdaBoost:    see \"Experiments with a New Boosting \n"
+    "                                  Algorithm\" by Freund and Schapire.\n"
+    "                                  Set the 'pseudo_loss_adaboost' option\n"
+    "                                  to select this version\n"
+    "\n"
+    " - \"Confidence-rated\" AdaBoost: see \"Improved Boosting Algorithms Using\n"
+    "                                Confidence-rated Predictions\" by\n"
+    "                                Schapire and Singer.\n"
+    "                                Set the 'conf_rated_adaboost' option\n"
+    "                                to select this version.\n"
+    "These versions compute a more precise notion of error, taking into \n"
+    "account the precise value outputted by the soft classifier.\n"
+    "Also, \"Confidence-rated\" AdaBoost uses a line search at each stage to\n"
+    "compute the weight of the trained weak learner.\n\n"
+    "It should be noted that, except for the optimization of the weak learners,\n"
+    "\"Confidence-rated\" AdaBoost is equivalent to MarginBoost (see \n"
+    "\"Functional Gradient Techniques for Combining Hypotheses\" by \n"
+    "Mason et al.) when using the exponential loss on the margin. Hence, the\n"
+    "'conf_rated_adaboost' option can be used in that case too, and all that\n"
+    "needs to be adjusted is the choice of weak learners.\n\n"
     "The nstages option from PLearner is used to specify the desired\n"
     "number of boosting rounds (but the algorithm can stop earlier if\n"
-    "the next weak learner is unable to unable to make significant progress.\n");
+    "the next weak learner is unable to make significant progress or if\n"
+    "the weak learner has 0 error on the training set).\n");
 
 void AdaBoost::declareOptions(OptionList& ol)
 {
@@ -80,7 +106,8 @@
 
     declareOption(ol, "voting_weights", &AdaBoost::voting_weights,
                   OptionBase::learntoption,
-                  "Weights given to the weak learners (their output is linearly combined with these weights\n"
+                  "Weights given to the weak learners (their output is\n"
+                  "linearly combined with these weights\n"
                   "to form the output of the AdaBoost learner).\n");
 
     declareOption(ol, "sum_voting_weights", &AdaBoost::sum_voting_weights,
@@ -93,46 +120,77 @@
 
     declareOption(ol, "weak_learner_template", &AdaBoost::weak_learner_template,
                   OptionBase::buildoption,
-                  "Template for the regression weak learner to be boosted into a classifier");
+                  "Template for the regression weak learner to be"
+                  "boosted into a classifier");
 
     declareOption(ol, "target_error", &AdaBoost::target_error,
                   OptionBase::buildoption,
-                  "This is the target average weighted error below which each weak learner"
-                  "must reach after its training (ordinary adaboost: target_error=0.5).");
+                  "This is the target average weighted error below"
+                  "which each weak learner\n"
+                  "must reach after its training (ordinary adaboost:"
+                  "target_error=0.5).");
 
     declareOption(ol, "pseudo_loss_adaboost", &AdaBoost::pseudo_loss_adaboost,
                   OptionBase::buildoption,
-                  "Whether to use a variant of AdaBoost which is appropriate for soft classifiers\n"
-                  "whose output is between 0 and 1 rather than being either 0 or 1.\n");
+                  "Whether to use Pseudo-loss Adaboost (see \"Experiments with\n"
+                  "a New Boosting Algorithm\" by Freund and Schapire), which\n"
+                  "takes into account the precise value outputted by\n"
+                  "the soft classifier.");
 
     declareOption(ol, "conf_rated_adaboost", &AdaBoost::conf_rated_adaboost,
                   OptionBase::buildoption,
-                  "Whether to use a version of Adaboost appropriate for weak learners that can\n"
-                  "give a confidence rate to their predictions.\n");
+                  "Whether to use Confidence-rated AdaBoost (see \"Improved\n"
+                  "Boosting Algorithms Using Confidence-rated Predictions\" by\n"
+                  "Schapire and Singer) which takes into account the precise\n"
+                  "value outputted by the soft classifier. It also searchs\n"
+                  "the weight of a weak learner using a line search according\n"
+                  "to a criteria which is more appropriate for soft classifiers.\n"
+                  "This option can also be used to obtain MarginBoost with the\n"
+                  "exponential loss, provided that an appropriate choice of\n"
+                  "weak learner is made by the user (see \"Functional Gradient\n"
+                  "Techniques for Combining Hypotheses\" by Mason et al.).\n");
 
     declareOption(ol, "weight_by_resampling", &AdaBoost::weight_by_resampling,
                   OptionBase::buildoption,
-                  "Whether to train the weak learner using resampling to represent the weighting\n"
-                  "given to examples. If false then give these weights explicitly in the training set\n"
-                  "of the weak learner (note that some learners can accomodate weights well, others not).\n");
+                  "Whether to train the weak learner using resampling"
+                  "to represent the weighting\n"
+                  "given to examples. If false then give these weights "
+                  "explicitly in the training set\n"
+                  "of the weak learner (note that some learners can accomodate "
+                  "weights well, others not).\n");
 
     declareOption(ol, "output_threshold", &AdaBoost::output_threshold,
                   OptionBase::buildoption,
-                  "To interpret the output of the learner as a class, it is compared to this\n"
-                  "threshold: class 1 if greather than output_threshold, class 0 otherwise.\n");
+                  "To interpret the output of the learner as a class, it is "
+                  "compared to this\n"
+                  "threshold: class 1 if greather than output_threshold, class "
+                  "0 otherwise.\n");
 
-    declareOption(ol, "provide_learner_expdir", &AdaBoost::provide_learner_expdir, OptionBase::buildoption,
-                  "If true, each weak learner to be trained will have its experiment directory set to WeakLearner#kExpdir/");
+    declareOption(ol, "provide_learner_expdir", &AdaBoost::provide_learner_expdir,
+                  OptionBase::buildoption,
+                  "If true, each weak learner to be trained will have its\n"
+                  "experiment directory set to WeakLearner#kExpdir/");
 
-    declareOption(ol, "early_stopping", &AdaBoost::early_stopping, OptionBase::buildoption,
-                  "If true, then boosting stops when the next weak learner is too weak (avg error > target_error - .01)\n");
+    declareOption(ol, "early_stopping", &AdaBoost::early_stopping, 
+                  OptionBase::buildoption,
+                  "If true, then boosting stops when the next weak learner\n"
+                  "is too weak (avg error > target_error - .01)\n");
 
-    declareOption(ol, "save_often", &AdaBoost::save_often, OptionBase::buildoption,
-                  "If true, then save the model after training each weak learner, under <expdir>/model.psave\n");
+    declareOption(ol, "save_often", &AdaBoost::save_often, 
+                  OptionBase::buildoption,
+                  "If true, then save the model after training each weak\n"
+                  "learner, under <expdir>/model.psave\n");
 
-    declareOption(ol, "compute_training_error", &AdaBoost::compute_training_error, OptionBase::buildoption,
+    declareOption(ol, "compute_training_error", 
+                  &AdaBoost::compute_training_error, OptionBase::buildoption,
                   "Whether to compute training error at each stage.\n");
 
+    declareOption(ol, "found_zero_error_weak_learner", 
+                  &AdaBoost::found_zero_error_weak_learner, 
+                  OptionBase::learntoption,
+                  "Indication that a weak learner with 0 training error"
+                  "has been found.\n");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -191,11 +249,15 @@
         PLERROR("In AdaBoost::train, you did not setTrainStatsCollector");
 
     if (train_set->targetsize()!=1)
-        PLERROR("In AdaBoost::train, targetsize should be 1, found %d", train_set->targetsize());
+        PLERROR("In AdaBoost::train, targetsize should be 1, found %d", 
+                train_set->targetsize());
 
     if (nstages < stage)        //!< Asking to revert to previous stage
         forget();
 
+    if(found_zero_error_weak_learner) // Training is over...
+        return;
+
     static Vec input;
     static Vec output;
     static Vec target;
@@ -218,8 +280,9 @@
         if (train_set->weightsize()>0)
         {
             PP<ProgressBar> pb;
-            if(report_progress) pb = new ProgressBar("AdaBoost round " + tostring(stage) +
-                                                     ": extracting initial weights", n);
+            if(report_progress) pb = new ProgressBar(
+                "AdaBoost round " + tostring(stage) +
+                ": extracting initial weights", n);
             initial_sum_weights=0;
             for (int i=0; i<n; ++i) {
                 if(report_progress) pb->update(i);
@@ -246,26 +309,33 @@
         VMat weak_learner_training_set;
         { 
             PP<ProgressBar> pb;
-            if(report_progress) pb = new ProgressBar("AdaBoost round " + tostring(stage) +
-                                                     ": making training set for weak learner", n);
+            if(report_progress) pb = new ProgressBar(
+                "AdaBoost round " + tostring(stage) +
+                ": making training set for weak learner", n);
+
             // We shall now construct a training set for the new weak learner:
             if (weight_by_resampling)
             {
-                // use a "smart" resampling that approximated sampling with replacement
-                // with the probabilities given by example_weights.
+                // use a "smart" resampling that approximated sampling 
+                // with replacement with the probabilities given by 
+                // example_weights.
                 map<real,int> indices;
                 for (int i=0; i<n; ++i) {
                     if(report_progress) pb->update(i);
                     real p_i = example_weights[i];
-                    int n_samples_of_row_i = int(rint(gaussian_mu_sigma(n*p_i,sqrt(n*p_i*(1-p_i))))); // randomly choose how many repeats of example i
+                    // randomly choose how many repeats of example i
+                    int n_samples_of_row_i = 
+                        int(rint(gaussian_mu_sigma(n*p_i,sqrt(n*p_i*(1-p_i))))); 
                     for (int j=0;j<n_samples_of_row_i;j++)
                     {
                         if (j==0)
                             indices[i]=i;
                         else
                         {
-                            real k=n*uniform_sample(); // put the others in random places
-                            indices[k]=i; // while avoiding collisions
+                            // put the others in random places
+                            real k=n*uniform_sample(); 
+                            // while avoiding collisions
+                            indices[k]=i; 
                         }
                     }
                 }
@@ -274,15 +344,19 @@
                 map<real,int>::iterator last = indices.end();
                 for (;it!=last;++it)
                     train_indices.push_back(it->second);
-                weak_learner_training_set = new SelectRowsVMatrix(unweighted_data, train_indices);
+                weak_learner_training_set = 
+                    new SelectRowsVMatrix(unweighted_data, train_indices);
                 weak_learner_training_set->defineSizes(inputsize(), 1, 0);
             }
             else
             {
                 Mat data_weights_column = example_weights.toMat(n,1).copy();
-                data_weights_column *= initial_sum_weights; // to bring the weights to the same average level as the original ones
+                // to bring the weights to the same average level as 
+                // the original ones
+                data_weights_column *= initial_sum_weights; 
                 VMat data_weights = VMat(data_weights_column);
-                weak_learner_training_set = new ConcatColumnsVMatrix(unweighted_data,data_weights);
+                weak_learner_training_set = 
+                    new ConcatColumnsVMatrix(unweighted_data,data_weights);
                 weak_learner_training_set->defineSizes(inputsize(), 1, 1);
             }
         }
@@ -291,11 +365,6 @@
         PP<PLearner> new_weak_learner = ::PLearn::deepCopy(weak_learner_template);
         new_weak_learner->setTrainingSet(weak_learner_training_set);
         new_weak_learner->setTrainStatsCollector(new VecStatsCollector);
-        /*
-          string file = "train_" + tostring(stage);
-          MemoryVMatrix *temp_train_set = new MemoryVMatrix(weak_learner_training_set);
-          PLearn::save(file,temp_train_set->data);
-        */
         if(expdir!="" && provide_learner_expdir)
             new_weak_learner->setExperimentDirectory( expdir / ("WeakLearner"+tostring(stage)+"Expdir") );
 
@@ -313,16 +382,20 @@
                 real y_i=target[0];
                 real f_i=output[0];
                 if(conf_rated_adaboost)
-                {          
+                {
+                    // an error between 0 and 1 (before weighting)
                     examples_error[i] = 2*(f_i+y_i-2*f_i*y_i);
-                    learners_error[stage] += example_weights[i]*examples_error[i]/2;
+                    learners_error[stage] += example_weights[i]*
+                        examples_error[i]/2;
                 }
                 else
                 {
-                    if (pseudo_loss_adaboost) // an error between 0 and 1 (before weighting)
+                    // an error between 0 and 1 (before weighting)
+                    if (pseudo_loss_adaboost) 
                     {
-                        examples_error[i] = 0.5*(f_i+y_i-2*f_i*y_i);  
-                        learners_error[stage] += example_weights[i]*examples_error[i];
+                        examples_error[i] = 2*(f_i+y_i-2*f_i*y_i);
+                        learners_error[stage] += example_weights[i]*
+                            examples_error[i]/2;
                     }
                     else
                     {
@@ -331,7 +404,7 @@
                             if (f_i<output_threshold)
                             {
                                 learners_error[stage] += example_weights[i];
-                                examples_error[i]=1;
+                                examples_error[i]=2;
                             }
                             else examples_error[i] = 0;
                         }
@@ -339,7 +412,7 @@
                         {
                             if (f_i>=output_threshold) {
                                 learners_error[stage] += example_weights[i];
-                                examples_error[i]=1;
+                                examples_error[i]=2;
                             }
                             else examples_error[i]=0;
                         }
@@ -349,26 +422,45 @@
         }
 
         if (verbosity>1)
-            cout << "weak learner at stage " << stage << " has average loss = " << learners_error[stage] << endl;
+            cout << "weak learner at stage " << stage 
+                 << " has average loss = " << learners_error[stage] << endl;
 
         // stopping criterion (in addition to n_stages)
-        if (early_stopping && (fast_exact_is_equal(learners_error[stage], 0) || learners_error[stage] > target_error - 0.01))
+        if (early_stopping && learners_error[stage] >= target_error)
         {
             nstages = stage;
-            cout << "AdaBoost::train early stopping because learner's loss at stage " << stage << " is " << learners_error[stage] << endl;
+            cout << 
+                "AdaBoost::train early stopping because learner's loss at stage " 
+                 << stage << " is " << learners_error[stage] << endl;       
             break;
         }
 
+        if(fast_exact_is_equal(learners_error[stage], 0))
+        {
+            cout << "AdaBoost::train found weak learner with 0 training "
+                 << "error at stage " 
+                 << stage << " is " << learners_error[stage] << endl;  
+
+            // Simulate infinite weight on new_weak_learner
+            weak_learners.resize(0);
+            weak_learners.push_back(new_weak_learner);
+            voting_weights.resize(0);
+            voting_weights.push_back(1);
+            sum_voting_weights = 1;
+            found_zero_error_weak_learner = true;
+            break;
+        }
+
+
         weak_learners.push_back(new_weak_learner);
 
         if (save_often && expdir!="")
             PLearn::save(append_slash(expdir)+"model.psave", *this);
       
         // compute the new learner's weight
-
         if(conf_rated_adaboost)
         {
-            // Find optimal weight with line search, blame Norman if this doesn't work ;) 
+            // Find optimal weight with line search
       
             real ax = -10;
             real bx = 1;
@@ -475,14 +567,17 @@
         }
         else
         {
-            voting_weights.push_back(0.5*safeflog(((1-learners_error[stage])*target_error)/(learners_error[stage]*(1-target_error))));
+            voting_weights.push_back(
+                0.5*safeflog(((1-learners_error[stage])*target_error)
+                             /(learners_error[stage]*(1-target_error))));
             sum_voting_weights += abs(voting_weights[stage]);
         }
 
         real sum_w=0;
         for (int i=0;i<n;i++)
         {
-            example_weights[i] *= exp(-voting_weights[stage]*(1-examples_error[i]));
+            example_weights[i] *= exp(-voting_weights[stage]*
+                                      (1-examples_error[i]));
             sum_w += example_weights[i];
         }
         example_weights *= real(1.0)/sum_w;
@@ -504,7 +599,9 @@
                 train_stats->finalize();
             }
             if (verbosity>2)
-                cout << "At stage " << stage << " boosted (weighted) classification error on training set = " << train_stats->getMean() << endl;
+                cout << "At stage " << stage << 
+                    " boosted (weighted) classification error on training set = " 
+                     << train_stats->getMean() << endl;
      
         }
     }
@@ -520,7 +617,8 @@
     {
         weak_learners[i]->computeOutput(input,weak_learner_output);
         if(!pseudo_loss_adaboost && !conf_rated_adaboost)
-            sum_out += (weak_learner_output[0] < output_threshold ? 0 : 1) *voting_weights[i];
+            sum_out += (weak_learner_output[0] < output_threshold ? 0 : 1) 
+                *voting_weights[i];
         else
             sum_out += weak_learner_output[0]*voting_weights[i];
     }

Modified: trunk/plearn_learners/meta/AdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.h	2007-04-03 23:49:59 UTC (rev 6829)
+++ trunk/plearn_learners/meta/AdaBoost.h	2007-04-04 00:02:22 UTC (rev 6830)
@@ -71,6 +71,9 @@
 
     //! Vector of weak learners learned from boosting
     TVec< PP<PLearner> > weak_learners;
+
+    //! Indication that a weak learner with 0 training error has been found
+    bool found_zero_error_weak_learner;
   
 public:
 
@@ -96,10 +99,10 @@
     // whether to compute training error during training
     bool compute_training_error;
 
-    // use more refined training criterion when weak classifier is soft
+    // use Pseudo-loss Adaboost
     bool pseudo_loss_adaboost;
 
-    // use confidence-rated adaboost
+    // use Confidence-rated adaboost
     bool conf_rated_adaboost;
 
     // use resampling (vs weighting) to train the underlying classifier



From nouiz at mail.berlios.de  Wed Apr  4 18:37:09 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 4 Apr 2007 18:37:09 +0200
Subject: [Plearn-commits] r6831 - trunk/plearn/base
Message-ID: <200704041637.l34Gb9nJ007782@sheep.berlios.de>

Author: nouiz
Date: 2007-04-04 18:37:08 +0200 (Wed, 04 Apr 2007)
New Revision: 6831

Modified:
   trunk/plearn/base/TypeFactory.cc
Log:
Put the multi-line description of PLearn object in comment when we do plearn help object
Put additional information in comment in the help


Modified: trunk/plearn/base/TypeFactory.cc
===================================================================
--- trunk/plearn/base/TypeFactory.cc	2007-04-04 00:02:22 UTC (rev 6830)
+++ trunk/plearn/base/TypeFactory.cc	2007-04-04 16:37:08 UTC (rev 6831)
@@ -134,14 +134,24 @@
 
     // Display basic help
     out << "## " << entry.one_line_descr << endl << endl;
-    out << "## " << entry.multi_line_help << endl << endl;
 
+    string tmpstr;
+    for(uint i=0;i<entry.multi_line_help.size();i++){
+        if(entry.multi_line_help.at(i)!='\n'){
+            tmpstr.append(1,entry.multi_line_help.at(i));
+        }
+        else
+            tmpstr.append("\n## ");
+    }
+
+    out << "## " << tmpstr << endl << endl;
+
     if(entry.constructor) // it's an instantiable class
         obj = (*entry.constructor)();
     else
-        out << "Note: " << classname << " is a base-class with pure virtual methods that cannot be instantiated directly.\n" 
-            << "(default values for build options can only be displayed for instantiable classes, \n"
-            << " so you'll only see question marks here.)\n" << endl;
+        out << "## Note: " << classname << " is a base-class with pure virtual methods that cannot be instantiated directly.\n" 
+            << "## (default values for build options can only be displayed for instantiable classes, \n"
+            << "## so you'll only see question marks here.)\n" << endl;
       
     out << "################################################################## \n"
         << "##                         Build Options                        ## \n"



From tihocan at mail.berlios.de  Wed Apr  4 20:58:12 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 4 Apr 2007 20:58:12 +0200
Subject: [Plearn-commits] r6832 - trunk/plearn_learners/classifiers
Message-ID: <200704041858.l34IwCbk018938@sheep.berlios.de>

Author: tihocan
Date: 2007-04-04 20:58:11 +0200 (Wed, 04 Apr 2007)
New Revision: 6832

Modified:
   trunk/plearn_learners/classifiers/KNNClassifier.cc
Log:
There is no reason to assume the costs vector has the correct size

Modified: trunk/plearn_learners/classifiers/KNNClassifier.cc
===================================================================
--- trunk/plearn_learners/classifiers/KNNClassifier.cc	2007-04-04 16:37:08 UTC (rev 6831)
+++ trunk/plearn_learners/classifiers/KNNClassifier.cc	2007-04-04 18:58:11 UTC (rev 6832)
@@ -270,7 +270,7 @@
 void KNNClassifier::computeCostsFromOutputs(const Vec& input, const Vec& output, 
                                             const Vec& target, Vec& costs) const
 {
-    PLASSERT( costs.size() == 2 );
+    costs.resize(nTestCosts());
     int sel_class = argmax(output);
     costs[0] = sel_class != int(target[0]);
     costs[1] = -pl_log(output[int(target[0])]);



From tihocan at mail.berlios.de  Wed Apr  4 21:00:14 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 4 Apr 2007 21:00:14 +0200
Subject: [Plearn-commits] r6833 - trunk/plearn_learners/classifiers
Message-ID: <200704041900.l34J0EqO019189@sheep.berlios.de>

Author: tihocan
Date: 2007-04-04 21:00:14 +0200 (Wed, 04 Apr 2007)
New Revision: 6833

Modified:
   trunk/plearn_learners/classifiers/BinaryStump.cc
   trunk/plearn_learners/classifiers/BinaryStump.h
Log:
Added a new option to output a two-dimensional one-hot vector instead of a single scalar class number

Modified: trunk/plearn_learners/classifiers/BinaryStump.cc
===================================================================
--- trunk/plearn_learners/classifiers/BinaryStump.cc	2007-04-04 18:58:11 UTC (rev 6832)
+++ trunk/plearn_learners/classifiers/BinaryStump.cc	2007-04-04 19:00:14 UTC (rev 6833)
@@ -121,14 +121,15 @@
 
 }
 
-BinaryStump::BinaryStump() 
-/* ### Initialize all fields to their default value here */
-{
-    feature = 0;
-    tag = 0;
-    threshold = 0;
-    // build_();
-}
+/////////////////
+// BinaryStump //
+/////////////////
+BinaryStump::BinaryStump():
+    feature(0),
+    tag(0),
+    threshold(0),
+    one_hot_output(false)
+{}
 
 PLEARN_IMPLEMENT_OBJECT(BinaryStump, "Binary stump classifier", 
                         "This algorithm finds the most accurate binary stump\n"
@@ -140,13 +141,23 @@
                         "Only the first target is considered, the others are \n"
                         "ignored.\n");
 
+////////////////////
+// declareOptions //
+////////////////////
 void BinaryStump::declareOptions(OptionList& ol)
 {
+    declareOption(ol, "one_hot_output", &BinaryStump::one_hot_output,
+                  OptionBase::buildoption,
+        "If set to 1, the output will be a two-dimensional one-hot vector\n"
+        "instead of just a single number.");
+
     declareOption(ol, "feature", &BinaryStump::feature, OptionBase::learntoption,
                   "Feature tested by the stump");
+
     declareOption(ol, "threshold", &BinaryStump::threshold, 
                   OptionBase::learntoption,
                   "Threshold for decision");
+
     declareOption(ol, "tag", &BinaryStump::tag, OptionBase::learntoption,
                   "Tag assigned when feature is lower than the threshold");
 
@@ -162,17 +173,29 @@
 }
 
 
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
 void BinaryStump::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 }
 
 
+////////////////
+// outputsize //
+////////////////
 int BinaryStump::outputsize() const
 {
-    return 1;
+    if (one_hot_output)
+        return 2;
+    else
+        return 1;
 }
 
+////////////
+// forget //
+////////////
 void BinaryStump::forget()
 {
     stage = 0;
@@ -181,6 +204,9 @@
     threshold = 0;
 }
     
+///////////
+// train //
+///////////
 void BinaryStump::train()
 {
 
@@ -350,39 +376,56 @@
 }
 
 
+///////////////////
+// computeOutput //
+///////////////////
 void BinaryStump::computeOutput(const Vec& input, Vec& output) const
 {
     output.resize(outputsize());
-    if(input[feature] < threshold) output[0] = tag;
-    else output[0] = 1-tag;
+    int predict = input[feature] < threshold ? tag : 1 - tag;
+    if (one_hot_output) {
+        output[predict] = 1;
+        output[1 - predict] = 0;
+    } else
+        output[0] = predict;
 }    
 
+/////////////////////////////
+// computeCostsFromOutputs //
+/////////////////////////////
 void BinaryStump::computeCostsFromOutputs(const Vec& input, const Vec& output, 
                                           const Vec& target, Vec& costs) const
 {
-    costs.resize(outputsize());
+    costs.resize(1);
 
     if(!fast_exact_is_equal(target[0], 0) &&
        !fast_exact_is_equal(target[0], 1))
         PLERROR("In BinaryStump:computeCostsFromOutputs() : "
                 "target should be either 1 or 0");
 
-    costs[0] = !fast_exact_is_equal(output[0], target[0]); 
+    real predict = one_hot_output ? argmin(output) : output[0];
+    costs[0] = !is_equal(predict, target[0]); 
 }                                
 
+//////////////////////
+// getTestCostNames //
+//////////////////////
 TVec<string> BinaryStump::getTestCostNames() const
 {
     return getTrainCostNames();
 }
 
+///////////////////////
+// getTrainCostNames //
+///////////////////////
 TVec<string> BinaryStump::getTrainCostNames() const
 {
-    TVec<string> costs(1);
-    costs[0] = "binary_class_error";
+    static TVec<string> costs;
+    if (costs.isEmpty())
+        costs.append("binary_class_error");
     return costs;
 }
 
-
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/classifiers/BinaryStump.h
===================================================================
--- trunk/plearn_learners/classifiers/BinaryStump.h	2007-04-04 18:58:11 UTC (rev 6832)
+++ trunk/plearn_learners/classifiers/BinaryStump.h	2007-04-04 19:00:14 UTC (rev 6833)
@@ -73,7 +73,9 @@
     // * public build options *
     // ************************
 
+    bool one_hot_output;
 
+
     // ****************
     // * Constructors *
     // ****************



From saintmlx at mail.berlios.de  Thu Apr  5 03:35:44 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 5 Apr 2007 03:35:44 +0200
Subject: [Plearn-commits] r6834 - in trunk: commands/PLearnCommands
 plearn/base plearn/base/test plearn/base/test/ObjectGraphIterator
 plearn/base/test/PP plearn/io plearn/io/test plearn/math
 plearn/math/test/PentadiagonalSolveInPlace plearn/math/test/TMat
 plearn/math/test/VecStatsCollector plearn/math/test/pl_math plearn/misc
 plearn/misc/test plearn/opt/test plearn/python plearn/python/test
 plearn/python/test/.pytest/EmbeddedPython_InterfunctionXchg/expected_results
 plearn/var/test plearn/vmat plearn/vmat/test
 plearn_learners/classifiers/test/KNNClassifier
 plearn_learners/distributions/test plearn_learners/generic
 plearn_learners/generic/test/SimpleNet plearn_learners/meta
 plearn_learners/regressors
 plearn_learners/regressors/test/GaussianProcessRegressor
 plearn_learners/regressors/test/KernelRidgeRegressor
 plearn_learners/regressors/test/LinearRegressor
 plearn_learners/regressors/test/PLS
 plearn_learners/regressors/test/StackedPCARegression
 plearn_learners/unsupervised/test python_modules/p! learn/math/test
 python_modules/plearn/parallel python_modules/plearn/plide
 python_modules/plearn/plide/resources python_modules/plearn/pyplearn
 python_modules/plearn/utilities scripts
Message-ID: <200704050135.l351ZiG1026678@sheep.berlios.de>

Author: saintmlx
Date: 2007-04-05 03:35:37 +0200 (Thu, 05 Apr 2007)
New Revision: 6834

Added:
   trunk/python_modules/plearn/utilities/pltraceback.py
Modified:
   trunk/commands/PLearnCommands/HelpCommand.cc
   trunk/commands/PLearnCommands/PLearnCommandRegistry.cc
   trunk/commands/PLearnCommands/PLearnCommandRegistry.h
   trunk/commands/PLearnCommands/Plide.cc
   trunk/commands/PLearnCommands/ServerCommand.cc
   trunk/commands/PLearnCommands/plearn_main.cc
   trunk/plearn/base/HelpSystem.cc
   trunk/plearn/base/HelpSystem.h
   trunk/plearn/base/Object.h
   trunk/plearn/base/TypeFactory.cc
   trunk/plearn/base/TypeFactory.h
   trunk/plearn/base/test/ObjectGraphIterator/pytest.config
   trunk/plearn/base/test/PP/pytest.config
   trunk/plearn/base/test/pytest.config
   trunk/plearn/io/PStream.cc
   trunk/plearn/io/PStream.h
   trunk/plearn/io/fileutils.cc
   trunk/plearn/io/fileutils.h
   trunk/plearn/io/pl_log.cc
   trunk/plearn/io/pl_log.h
   trunk/plearn/io/test/pytest.config
   trunk/plearn/math/TMat_decl.h
   trunk/plearn/math/TMat_maths_impl.h
   trunk/plearn/math/test/PentadiagonalSolveInPlace/pytest.config
   trunk/plearn/math/test/TMat/pytest.config
   trunk/plearn/math/test/VecStatsCollector/pytest.config
   trunk/plearn/math/test/pl_math/pytest.config
   trunk/plearn/misc/PLearnServer.cc
   trunk/plearn/misc/PLearnServer.h
   trunk/plearn/misc/PLearnService.cc
   trunk/plearn/misc/PLearnService.h
   trunk/plearn/misc/RemotePLearnServer.cc
   trunk/plearn/misc/test/pytest.config
   trunk/plearn/misc/vmatmain.cc
   trunk/plearn/opt/test/pytest.config
   trunk/plearn/python/PythonCodeSnippet.cc
   trunk/plearn/python/test/.pytest/EmbeddedPython_InterfunctionXchg/expected_results/RUN.log
   trunk/plearn/python/test/pytest.config
   trunk/plearn/var/test/pytest.config
   trunk/plearn/vmat/BootstrapSplitter.cc
   trunk/plearn/vmat/EncodedVMatrix.cc
   trunk/plearn/vmat/VMatLanguage.cc
   trunk/plearn/vmat/test/pytest.config
   trunk/plearn_learners/classifiers/test/KNNClassifier/pytest.config
   trunk/plearn_learners/distributions/test/pytest.config
   trunk/plearn_learners/generic/PLearner.cc
   trunk/plearn_learners/generic/VPLCombinedLearner.cc
   trunk/plearn_learners/generic/VPLPreprocessedLearner2.cc
   trunk/plearn_learners/generic/test/SimpleNet/pytest.config
   trunk/plearn_learners/meta/BaggingLearner.cc
   trunk/plearn_learners/meta/BaggingLearner.h
   trunk/plearn_learners/regressors/LinearRegressor.cc
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/pytest.config
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/pytest.config
   trunk/plearn_learners/regressors/test/LinearRegressor/pytest.config
   trunk/plearn_learners/regressors/test/PLS/pytest.config
   trunk/plearn_learners/regressors/test/StackedPCARegression/pytest.config
   trunk/plearn_learners/unsupervised/test/pytest.config
   trunk/python_modules/plearn/math/test/pytest.config
   trunk/python_modules/plearn/parallel/dispatch.py
   trunk/python_modules/plearn/plide/plide.py
   trunk/python_modules/plearn/plide/plide_help.py
   trunk/python_modules/plearn/plide/plide_options.py
   trunk/python_modules/plearn/plide/resources/help_prolog.html
   trunk/python_modules/plearn/pyplearn/plargs.py
   trunk/python_modules/plearn/pyplearn/pytest.config
   trunk/scripts/xdispatch
Log:
- migrated most help to HelpSystem
- ran pytest update --all
- made plide run stand-alone
- parallelized BaggingLearner::train
- more details for python errors (PythonCodeSnippet)
- added erf to VMatLanguage
- minor fixes here and there



Modified: trunk/commands/PLearnCommands/HelpCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/HelpCommand.cc	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/commands/PLearnCommands/HelpCommand.cc	2007-04-05 01:35:37 UTC (rev 6834)
@@ -38,12 +38,13 @@
 
 /*! \file HelpCommand.cc */
 #include "HelpCommand.h"
+
 #include <iostream>
 #include <plearn/db/getDataSet.h>
 #include <plearn/base/general.h>        //!< For prgname().
 #include <plearn/base/stringutils.h>
-#include <plearn/base/TypeFactory.h>    //!< For displayObjectHelp().
 #include <plearn/io/fileutils.h>        //!< For isfile().
+
 #include <plearn/base/HelpSystem.h>
 
 namespace PLearn {
@@ -104,8 +105,7 @@
          << "  % " + prgname()  + " command_name command_arguments \n" << endl;
 
     pout << "Available commands are: " << endl;
-    PLearnCommandRegistry::print_command_summary(cout); // TODO Change to pout.
-    pout << endl;
+    pout << HelpSystem::helpCommands() << endl;
 
     pout << "For more details on a specific command, type: \n" 
          << "  % " << prgname() << " help <command_name> \n"
@@ -130,22 +130,22 @@
 void HelpCommand::run(const vector<string>& args)
 {
     if(args.size()==0)
-        helpOverview();
+        helpOverview();//TODO: move to HelpSystem
     else
     {
         string about = args[0];
         if(extract_extension(about)==".plearn") // help is asked about a plearn script
-            helpAboutScript(about);
+            helpAboutScript(about);//TODO: move to HelpSystem
         if(about=="scripts")
-            helpScripts();
+            helpScripts();//TODO: move to HelpSystem
         else if(about=="commands")
             helpCommands();
         else if(about=="datasets")
-            helpDatasets();
+            helpDatasets();//TODO: move to HelpSystem
         else if(PLearnCommandRegistry::is_registered(about))
-            PLearnCommandRegistry::help(about, cout); // TODO Change to pout.
+            pout << HelpSystem::helpOnCommand(about) << flush;
         else 
-            displayObjectHelp(cout, about);   // TODO Change to pout.
+            pout << HelpSystem::helpOnClass(about) << flush;
     }
 }
 

Modified: trunk/commands/PLearnCommands/PLearnCommandRegistry.cc
===================================================================
--- trunk/commands/PLearnCommands/PLearnCommandRegistry.cc	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/commands/PLearnCommands/PLearnCommandRegistry.cc	2007-04-05 01:35:37 UTC (rev 6834)
@@ -40,6 +40,8 @@
 #include "PLearnCommandRegistry.h"
 #include <iostream>
 #include <vector>
+#include <plearn/base/HelpSystem.h>
+#include <plearn/io/PStream.h>
 
 namespace PLearn {
 using namespace std;
@@ -50,13 +52,13 @@
     return commands_;
 }
 
-
 void PLearnCommandRegistry::do_register(PLearnCommand* command)
 { commands()[command->name] = command; }
 
 bool PLearnCommandRegistry::is_registered(const string& commandname)
 { return commands().find(commandname)!=commands().end(); }
-  
+
+/*  
 void PLearnCommandRegistry::print_command_summary(ostream& out)
 {
     command_map::iterator it = commands().begin();
@@ -68,14 +70,16 @@
     }
     out << endl;
 }
+*/
 
 //! Issues a "bad command" message
 void PLearnCommandRegistry::badcommand(const string& commandname)
 {
-    cerr << "No '" << commandname << "' command available." << endl;
-    cerr << "Available commands are: " << endl;
-    print_command_summary(cerr);
-    cerr << "You can get more help for any of these commands by invoking the help command" << endl;
+    perr << "No '" << commandname << "' command available." << endl;
+    perr << "Available commands are: " << endl;
+    //print_command_summary(cerr);
+    perr << HelpSystem::helpCommands() << flush;
+    perr << "You can get more help for any of these commands by invoking the help command" << endl;
 }
 
 void PLearnCommandRegistry::run(const string& commandname, const vector<string>& args)
@@ -86,7 +90,16 @@
     else
         it->second->run(args);
 }
-  
+
+PLearnCommand* PLearnCommandRegistry::getCommand(const string& commandname)
+{
+    command_map::iterator it = commands().find(commandname);
+    if(it==commands().end()) badcommand(commandname);
+    return it->second;
+}
+
+
+/*  
 void PLearnCommandRegistry::help(const string& commandname, ostream& out)
 { 
     command_map::iterator it = commands().find(commandname);
@@ -99,6 +112,7 @@
         out << it->second->helpmsg << endl;        
     }
 }
+*/
 
 } // end of namespace PLearn
 

Modified: trunk/commands/PLearnCommands/PLearnCommandRegistry.h
===================================================================
--- trunk/commands/PLearnCommands/PLearnCommandRegistry.h	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/commands/PLearnCommands/PLearnCommandRegistry.h	2007-04-05 01:35:37 UTC (rev 6834)
@@ -56,15 +56,26 @@
     // Minor help for the HTML helper here...
     friend class HTMLHelpCommand;
 
-protected:
+public:// public so that the command map can be read
     typedef map<string, PLearnCommand*> command_map;
 
+protected:
+
     //! Issues a "bad command" message
     static void badcommand(const string& commandname);
 
+    //! Returns a reference to the unique command map
+    static command_map& commands();
  
 public:
 
+    //! Returns a const reference to the unique command map
+    inline static const command_map& allCommands() 
+    { return commands(); }
+
+    //! Returns the PLearnCommand with the given name, or error
+    static PLearnCommand* getCommand(const string& commandname);
+
     //! registers a command
     static void do_register(PLearnCommand* command);
 
@@ -72,20 +83,18 @@
     static bool is_registered(const string& commandname);
   
     //! prints a list of all commands with their one-line summary
-    static void print_command_summary(ostream& out);
+    //static void print_command_summary(ostream& out);
 
     //! run the given (registered) command with the given arguments
     static void run(const string& commandname, const vector<string>& args);
   
     //! prints out detailed help for the given command on the given stream
-    static void help(const string& commandname, ostream& out);
+    //static void help(const string& commandname, ostream& out);
 
     //! This constructor will simply register the given PLearnCommand
     inline PLearnCommandRegistry(PLearnCommand* plcommand)
     { do_register(plcommand); }
   
-    //! Returns a reference to the unique command map
-    static command_map& commands();
 };
   
 

Modified: trunk/commands/PLearnCommands/Plide.cc
===================================================================
--- trunk/commands/PLearnCommands/Plide.cc	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/commands/PLearnCommands/Plide.cc	2007-04-05 01:35:37 UTC (rev 6834)
@@ -233,7 +233,6 @@
 
 const char* plide_code = "from plearn.plide.plide import *\n" ;
 
-
 //! The actual implementation of the 'Plide' command
 void Plide::run(const vector<string>& args)
 {
@@ -355,6 +354,8 @@
 
 PythonObjectWrapper Plide::helpResourcesPath(const TVec<PythonObjectWrapper>& args)
 {
+// DEPRECATED: see HelpSystem
+/*
     if (args.size() != 1)
         PLERROR("%sExpecting 1 argument; got %d", __FUNCTION__, args.size());
 
@@ -369,6 +370,7 @@
     if (! m_help_command)
         PLERROR("%sThe PLearn command 'HTMLHelpCommand' must be linked into "
                 "the executable in order to use the Plide help system.", __FUNCTION__);
+*/
     return PythonObjectWrapper();
 }
 

Modified: trunk/commands/PLearnCommands/ServerCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/ServerCommand.cc	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/commands/PLearnCommands/ServerCommand.cc	2007-04-05 01:35:37 UTC (rev 6834)
@@ -49,6 +49,7 @@
 #include <plearn/io/pl_log.h>
 #include <plearn/io/PrPStreamBuf.h>
 #include <plearn/base/tostring.h>
+#include <plearn/base/PrUtils.h>
 #include <nspr/prio.h>
 #include <nspr/prerror.h>
 #include <nspr/prnetdb.h>
@@ -102,7 +103,8 @@
 
         // Allow reuse of the socket immediately after the server shuts down
         PRSocketOptionData socket_option_data;
-        socket_option_data.value.reuse_addr = PR_SockOpt_Reuseaddr;
+        socket_option_data.option = PR_SockOpt_Reuseaddr;
+        socket_option_data.value.reuse_addr = PR_TRUE;
         PR_SetSocketOption(sock, &socket_option_data);
 
         string myhostname = "UNKNOWN_HOSTNAME";
@@ -135,7 +137,8 @@
             PRNetAddr addr;
             PRFileDesc* fd = PR_Accept(sock, &addr, PR_INTERVAL_NO_TIMEOUT);
             if(fd==0)
-                PLERROR("ServerCommand: accept returned 0, error code is: %d",PR_GetError());
+                PLERROR("ServerCommand: accept returned 0, error code is: %d : %s",
+                        PR_GetError(), getPrErrorString().c_str());
             st = PR_NetAddrToString(&addr, buf, sizeof(buf));
             NORMAL_LOG << "PLEARN_SERVER CONNECTION_FROM "  << buf << endl;
             PStream io(new PrPStreamBuf(fd,fd,true,true));

Modified: trunk/commands/PLearnCommands/plearn_main.cc
===================================================================
--- trunk/commands/PLearnCommands/plearn_main.cc	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/commands/PLearnCommands/plearn_main.cc	2007-04-05 01:35:37 UTC (rev 6834)
@@ -113,6 +113,16 @@
     return s;
 }
 
+
+BEGIN_DECLARE_REMOTE_FUNCTIONS
+
+    declareFunction("versionString", &version_string,
+                    (BodyDoc("Returns PLearn version as a string.\n"),
+                     RetDoc ("version string")));
+
+END_DECLARE_REMOTE_FUNCTIONS
+
+
 static void set_global_calendars(string command_line_option)
 {
     // Assume command-line-option of the form

Modified: trunk/plearn/base/HelpSystem.cc
===================================================================
--- trunk/plearn/base/HelpSystem.cc	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/base/HelpSystem.cc	2007-04-05 01:35:37 UTC (rev 6834)
@@ -2,6 +2,7 @@
 
 // HelpSystem.cc
 // Copyright (c) 2006 Pascal Vincent
+// Copyright (C) 2007 Xavier Saint-Mleux, ApSTAT Technologies, inc.
 
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
@@ -31,54 +32,245 @@
 // This file is part of the PLearn library. For more information on the PLearn
 // library, go to the PLearn Web site at www.plearn.org
 
+#include "HelpSystem.h"
 
-#include "HelpSystem.h"
 #include "stringutils.h"    //!< For addprefix.
+#include "tostring.h"
+#include <plearn/misc/HTMLUtils.h>
+
+#include <plearn/io/openFile.h>
+#include <plearn/io/fileutils.h>
+#include <plearn/io/openString.h>
+
 #include <plearn/base/TypeFactory.h>
 #include <plearn/base/RemoteTrampoline.h>
 #include <plearn/base/RemoteDeclareMethod.h>
 #include <plearn/base/RemoteMethodDoc.h>
 #include <plearn/base/RemoteMethodMap.h>
+#include <commands/PLearnCommands/PLearnCommandRegistry.h>
+#include <plearn/base/Object.h>
 
+
 namespace PLearn {
 using namespace std;
 
 
-vector< pair<string, int> > listFunctions()
+/*****
+ * Commands
+ */
+
+vector<string> HelpSystem::listCommands()
+{
+    vector<string> commands;
+    for(PLearnCommandRegistry::command_map::const_iterator
+            it  = PLearnCommandRegistry::allCommands().begin();
+        it != PLearnCommandRegistry::allCommands().end(); ++it)
+        commands.push_back(it->first);
+    return commands;
+}
+
+string HelpSystem::helpCommands()
+{
+    vector<string> commands= listCommands();
+    string s= "";
+    for(vector<string>::iterator it= commands.begin();
+        it != commands.end(); ++it)
+    {
+        PLearnCommand* cmd= PLearnCommandRegistry::getCommand(*it);
+        s+= *it + "\t:  " + cmd->description + '\n';
+    }
+    return s+'\n';
+}
+
+string HelpSystem::helpOnCommand(const string& commandname)
+{
+    PLearnCommand* cmd= PLearnCommandRegistry::getCommand(commandname);
+    string help= "*** Help for command '" + commandname + "' ***\n";
+    help+= cmd->description + '\n';
+    help+= cmd->helpmsg + '\n';        
+    return help;
+}
+
+string HelpSystem::helpCommandsHTML()
+{
+    string s;
+    s.reserve(16384);
+    s+= "<div class=\"generaltable\">\n"
+        "<h2>Index of Available Commands</h2>\n"
+        "<table cellspacing=\"0\" cellpadding=\"0\">\n";
+   
+    int i=0;
+    vector<string> commands= listCommands();
+    for(vector<string>::iterator it= commands.begin();
+        it != commands.end(); ++it, ++i)
+    {
+        PLearnCommand* cmd= PLearnCommandRegistry::getCommand(*it);
+        string helpurl= "command_" + *it + ".html";
+        string helphtml= "<a href=\"" + helpurl + "\">" + *it + "</a>";
+        s+= string("  <tr class=\"") + (i%2 == 0? "even" : "odd") + "\">\n"
+            "    <td>" + helphtml + "</td>"
+            "<td>" + cmd->description + "</td></tr>\n";
+    }
+    s+="</table>\n</div>\n";
+    return helpPrologueHTML("Command Index") + s + helpEpilogueHTML();
+}
+
+string HelpSystem::helpOnCommandHTML(const string& commandname)
+{
+    PLearnCommand* cmd= PLearnCommandRegistry::getCommand(commandname);
+    string s= string("<div class=\"cmdname\">")
+        + HTMLUtils::quote(commandname) + "</div>\n"
+        "<div class=\"cmddescr\">"
+        + HTMLUtils::quote_format_and_highlight(cmd->description) + "</div>\n"
+        "<div class=\"cmdhelp\">"
+        + HTMLUtils::quote_format_and_highlight(cmd->helpmsg) 
+        + "</div>\n";
+    return helpPrologueHTML(commandname) + s + helpEpilogueHTML();
+}
+
+
+/*****
+ * Functions
+ */
+
+vector<pair<string, int> > HelpSystem::listFunctions()
 { 
     return getGlobalFunctionMap().getMethodList(); 
 }
 
-vector<string> listFunctionPrototypes()
+vector<string> HelpSystem::listFunctionPrototypes()
 { 
     return getGlobalFunctionMap().getMethodPrototypes(); 
 }
 
-string helpFunction(const string& functionname)
+string HelpSystem::helpFunctions()
 {
-    return getGlobalFunctionMap().getMethodHelpText(functionname);
+    vector<pair<string, int> > funcs= listFunctions();
+    string s;
+    for(vector<pair<string, int> >::iterator it= funcs.begin();
+        it != funcs.end(); ++it)
+        s+= it->first +'('+tostring(it->second)+")\t:"
+            + helpOnFunction(it->first, it->second) +'\n';
+    return s;
 }
 
-vector< pair<string, int> > listMethods(const string& classname)
+string HelpSystem::helpOnFunction(const string& functionname, int arity)
 {
-    const RemoteMethodMap& rmm = TypeFactory::instance().getTypeMapEntry(classname).get_remote_methods();
-    return rmm.getMethodList(); 
+    return getGlobalFunctionMap().getMethodHelpText(functionname, arity);
 }
 
-vector<string> listMethodPrototypes(const string& classname)
+string HelpSystem::helpFunctionsHTML()
 {
-    const RemoteMethodMap& rmm = TypeFactory::instance().getTypeMapEntry(classname).get_remote_methods();
-    return rmm.getMethodPrototypes(); 
+    string s= "<div class=\"rmitable\">\n" 
+        "<h2>List of Functions</h2>\n"
+        "<table cellspacing=\"0\" cellpadding=\"0\">\n";
+    s.reserve(32768);
+
+    vector<pair<string, int> > functions= listFunctions();
+
+    int i= 0;
+    for(vector<pair<string, int> >::iterator it= functions.begin(); 
+        it != functions.end(); ++it)
+        s+= string("<tr class=\"") + (i++ == 0? "first" : "others") + "\">\n"
+            + helpOnFunctionHTML(it->first, it->second)
+            + "</tr>\n";
+
+    if(index == 0)
+        s+= "<tr><td>No Remote-Callable Functions.</td></tr>\n";
+           
+    s+= "</table></div>\n";
+
+    return helpPrologueHTML("Function Index") + s + helpEpilogueHTML();
 }
 
-string helpMethod(const string& classname, const string& methodname)
+namespace
 {
-    const RemoteMethodMap& rmm = TypeFactory::instance().getTypeMapEntry(classname).get_remote_methods();
-    return rmm.getMethodHelpText(methodname);
+string formatMethodDocHTML(const RemoteMethodDoc& doc, const string& classname= "")
+{
+    // Generate the function signature and argument-list table in HTML form
+    string return_type = HTMLUtils::quote_format_and_highlight(doc.returnType());
+    string args = "";
+    string arg_table = "";
+    list<ArgDoc>::const_iterator adit= doc.argListDoc().begin(), 
+        adend= doc.argListDoc().end();
+    list<string>::const_iterator tyit= doc.argListType().begin(), 
+        tyend= doc.argListType().end();
+    int j=0;
+    for( ; tyit != tyend ; ++tyit) 
+    {
+        string arg_type= HTMLUtils::quote_format_and_highlight(*tyit);
+        string arg_name= "";
+        string arg_doc= "(no documentation)";
+
+        if(adit != adend)
+        {
+            arg_name= HTMLUtils::quote(adit->m_argument_name);
+            arg_doc= HTMLUtils::quote_format_and_highlight(adit->m_doc);
+            ++adit;
+        }
+
+        if(!args.empty())  args += ", ";
+
+        args+= arg_type + ' ' + arg_name;
+
+        if(!arg_table.empty()) arg_table += "</tr><tr><td></td>";
+
+        string td1_class = (++j % 2 == 0? "argnameeven" : "argnameodd");
+        string td2_class = (  j % 2 == 0? "argdoceven"  : "argdocodd");
+                
+        arg_table+= 
+            "  <td class=\"" + td1_class + "\">" + arg_type + ' ' + arg_name + "</td>"
+            + "  <td class=\"" + td2_class + "\">" + arg_doc  + "</td>";
+    } 
+
+    // Header
+    string s= "<td colspan=\"3\"><div class=\"rmiprototype\">";
+    s+= return_type
+        + " <span class=\"rmifuncname\">" + doc.name() + "</span>"
+        + '(' + args + ')' + "</div>\n" 
+        "</tr><tr><td></td><td colspan=\"2\">" 
+        + HTMLUtils::quote_format_and_highlight(doc.bodyDoc())
+        + (classname == "" ? "" 
+           : " (defined by " + HTMLUtils::quote_format_and_highlight(classname) + ")")
+        + "</td>";
+
+    // Args table
+    if(!arg_table.empty())
+        s+= "</tr>\n<tr>\n"
+            "<td class=\"rmititle\">Arguments</td>\n" + arg_table;
+
+    // Ret. val
+    string td1_class = (++j % 2 == 0? "argnameeven" : "argnameodd");
+    string td2_class = (  j % 2 == 0? "argdoceven"  : "argdocodd");
+            
+    if(!doc.returnType().empty() || !doc.returnDoc().empty()) 
+        s+= "</tr>\n<tr>\n"
+            "<td class=\"rmititle\">Returns</td>\n"
+            "<td class=\"" + td1_class + "\">" + return_type + "</td>"
+            "<td class=\"" + td2_class + "\">" 
+            + HTMLUtils::quote_format_and_highlight(doc.returnDoc()) + "</td>\n";
+
+    //last <tr> s/b closed by calling fn.
+    return s;
 }
+}
 
-vector<string> listClasses()
+string HelpSystem::helpOnFunctionHTML(const string& functionname, int arity)
 {
+    // the result is a list of table fields (<td>...</td>)
+    // should be enclosed in a table row (<table><tr>...</tr></table>)
+    const RemoteTrampoline* t= 
+        getGlobalFunctionMap().lookup(functionname, arity);
+    PLASSERT(t);
+    return formatMethodDocHTML(t->documentation());
+}
+
+/*****
+ * Classes
+ */
+
+vector<string> HelpSystem::listClasses()
+{
     const TypeMap& type_map = TypeFactory::instance().getTypeMap();
     int nclasses = type_map.size();
     vector<string> class_list(type_map.size());
@@ -88,7 +280,7 @@
     return class_list;
 }
 
-map<string, string> getClassTree()
+map<string, string> HelpSystem::getClassTree()
 {
     const TypeMap& type_map = TypeFactory::instance().getTypeMap();
     map<string, string> class_tree;
@@ -99,186 +291,819 @@
     return class_tree;
 }
 
+string HelpSystem::helpClasses()
+{
+    vector<string> classes= listClasses();
+    string s= "";
+    for(vector<string>::iterator it= classes.begin();
+        it != classes.end(); ++it)
+    {
+        const TypeMapEntry& e= 
+            TypeFactory::instance().getTypeMapEntry(*it);
+        s+= "- " + *it + "\t("+ e.parent_class +")\t:  " 
+            + e.one_line_descr + '\n';
+    }
+    return s+'\n';
+}
 
+string HelpSystem::helpOnClass(const string& classname)
+{
 
-BEGIN_DECLARE_REMOTE_FUNCTIONS
-    declareFunction("listFunctions", &listFunctions,
-                    (BodyDoc("Returns a list of all registered global functions as pairs of (funtionname, nargs)")));
+    const TypeMap& type_map = TypeFactory::instance().getTypeMap();
+    TypeMap::const_iterator it = type_map.find(classname);
+    TypeMap::const_iterator itend = type_map.end();
 
-    declareFunction("listFunctionPrototypes", &listFunctionPrototypes,
-                    (BodyDoc("Returns a list of the prototypes of all registered global functions")));
+    if(it==itend)
+        PLERROR("Object type %s unknown.\n"
+                "Did you #include it, does it call the IMPLEMENT_NAME_AND_DEEPCOPY macro?\n"
+                "and has it indeed been linked with your program?", classname.c_str());
 
-    declareFunction("helpFunction", &helpFunction,
-                    (BodyDoc("Will return full help on all registered global functions with the given name "),
-                     ArgDoc ("functionname", "The name of the function on which to get help")));
+    const TypeMapEntry& entry = it->second;
+    Object* obj = 0;
 
-    declareFunction("listMethods", &listMethods,
-                    (BodyDoc("Returns a list of all registered methods for the given class as pairs of (methodname, nargs)"),
-                     ArgDoc ("classname", "The name of the class whose methods you want to list.")));
+    string s= "################################################################## \n";
+    s+= "## " + classname + "\n";
+    s+= "## " + helpClassParents(classname) + '\n';
+    s+= "################################################################## \n\n";
 
-    declareFunction("listMethodPrototypes", &listMethodPrototypes,
-                    (BodyDoc("Returns a list of the prototypes of all registered methods for the given class"),
-                     ArgDoc ("classname", "The name of the class whose method prototypes you want to list.")));
+    // Display basic help
+    s+= "## " + entry.one_line_descr + "\n\n";
+    s+= "## " + entry.multi_line_help + "\n\n";
 
-    declareFunction("helpMethod", &helpMethod,
-                    (BodyDoc("Will return full help on all registered methods of the class with the given name"),
-                     ArgDoc ("classname", "The name of the class"),
-                     ArgDoc ("methodname", "The name of the method")));
-                     
-    declareFunction("listClasses", &listClasses,
-                    (BodyDoc("Returns a list of all registered Object classes")));
+    if(entry.constructor) // it's an instantiable class
+        obj = (*entry.constructor)();
+    else
+        s+= "Note: " + classname 
+            + " is a base-class with pure virtual methods that cannot be instantiated directly.\n" 
+            "(default values for build options can only be displayed for instantiable classes, \n"
+            " so you'll only see question marks here.)\n\n";
+      
+    s+= "################################################################## \n"
+        "##                         Build Options                        ## \n"
+        "## (including those inherited from parent and ancestor classes) ## \n"
+        "################################################################## \n\n";
+    s+= classname + "( \n";
+    s+= helpClassOptions(classname);
+    s+= ");\n\n";
 
-    declareFunction("getClassTree", &getClassTree,
-                    (BodyDoc("Returns a map, mapping all registered Object classnames to their parentclassname")));
+    if(obj) delete obj;
 
-END_DECLARE_REMOTE_FUNCTIONS
+    s+= "################################################################## \n";
+    s+= "## Subclasses of " + classname + " \n"
+        "# (only those that can be instantiated) \n"
+        "################################################################## \n\n";
+    s+= addprefix("# ", helpDerivedClasses(classname));
 
+    s+= "\n\n################################################################## \n";
+    s+= "## Remote-callable methods of " + classname + " \n"
+        "################################################################## \n\n";
+    s+= addprefix("# ", helpMethods(classname));
 
-/*
+    s+= "\n\n################################################################## \n\n";
 
+    return s;
+}
 
-TVec<string> listOptions(const string& classname)
+string HelpSystem::helpClassesHTML()
 {
-    const TypeMapEntry& entry = TypeFactory::instance().getTypeMapEntry(classname);
-    OptionList& options = (*entry.getoptionlist_method)();    
-    TVec<string> optionnames;
-    for( OptionList::iterator it = options.begin(); it!=options.end(); ++it )
-        optionnames.append((*it)->optionname());
-    return optionnames;
+    string s= "<div class=\"generaltable\">\n"
+        "<h2>Index of Available Classes</h2>\n"
+        "<table cellspacing=\"0\" cellpadding=\"0\">\n";
+    s.reserve(131072);
+    int i=0;
+    vector<string> classes= listClasses();
+    for(vector<string>::iterator it= classes.begin();
+        it != classes.end(); ++it)
+    {
+        const TypeMapEntry& e= TypeFactory::instance().getTypeMapEntry(*it);
+        s+= string("  <tr class=\"") + (i++%2 == 0? "even" : "odd") + "\">\n"
+            "    <td>" 
+            + HTMLUtils::quote_format_and_highlight(*it)
+            + "</td>\n"
+            "    <td>" 
+            + HTMLUtils::quote_format_and_highlight(e.one_line_descr)
+            + '(' + HTMLUtils::quote_format_and_highlight(e.parent_class) + ')'
+            + "</td></tr>\n";
+    }
+    s+= "</table></div>\n";
+    return helpPrologueHTML("Class Index") + s + helpEpilogueHTML();
 }
 
-string helpOption(const string& classname, const string& optionname)
+string HelpSystem::helpOnClassHTML(const string& classname)
 {
-    const TypeMapEntry& entry = TypeFactory::instance().getTypeMapEntry(classname);
-    OptionList& options = (*entry.getoptionlist_method)();    
-    OptionList::iterator op = options.find(optionname);
-    if(op==options.end())
-        PLERROR("Class %s has no option named %s", classname.c_str(), optionname.c_str());
+    string s= "";
+    s.reserve(65536);
 
-    OptionBase::flag_t flags = (*op)->flags();
+    // Output parents heading
+    s+= helpClassParentsHTML(classname);
 
-    string descr = (*op)->description();
-    string optname = (*op)->optionname();
-    string opttype = (*op)->optiontype();
-    string defaultval = "?";
-    if(obj) // it's an instantiable class
+    // Output general information
+    const TypeMapEntry& entry= TypeFactory::instance().getTypeMapEntry(classname);
+    s+= "<div class=\"classname\">" + HTMLUtils::quote(classname) + "</div>\n"
+        "<div class=\"classdescr\">" 
+        + HTMLUtils::quote(entry.one_line_descr) + "</div>\n"
+        "<div class=\"classhelp\">" 
+        + HTMLUtils::quote_format_and_highlight(entry.multi_line_help) 
+        + "</div>\n";
+  
+    if(!entry.constructor) // it's not an instantiable class
+        s+= "<div class=\"classhelp\"><b>Note:</b>" + HTMLUtils::quote(classname)
+            + " is a base-class with pure virtual methods "
+            "that cannot be instantiated directly.\n" 
+            "(default values for build options can only be "
+            "displayed for instantiable classes, \n"
+            " so you'll only see question marks here.)</div>\n";
+
+    // Output list of options
+    s+= helpClassOptionsHTML(classname);
+
+    // Output instantiable derived classes
+    s+= helpDerivedClassesHTML(classname);
+
+    // Output remote-callable methods
+    s+= helpMethodsHTML(classname);
+
+    return helpPrologueHTML(classname) + s + helpEpilogueHTML();
+}
+
+
+vector<string> HelpSystem::listClassParents(const string& classname)
+{
+    string cl= classname;
+    vector<string> parents;
+    const TypeMapEntry* e= 
+        &TypeFactory::instance().getTypeMapEntry(cl);
+    while(e->parent_class != cl)
     {
-        defaultval = (*op)->defaultval(); 
-        if(defaultval=="")
-            defaultval = (*op)->writeIntoString(obj);
+        cl= e->parent_class;
+        parents.push_back(cl);
+        e= &TypeFactory::instance().getTypeMapEntry(cl);
     }
+    return parents;
+}
 
-    string optflags = "";
-    if(flags & OptionBase::buildoption)
-        optflags += " buildoption |";
-    if(flags & OptionBase::learntoption)
-        optflags += " learntoption |";
-    if(flags & OptionBase::tuningoption)
-        optflags += " tuningoption |";
-    if(flags & OptionBase::nosave)
-        optflags += " nosave |";
-    if(flags & OptionBase::nonparentable)
-        optflags += " nonparentable |";
-    if(flags & OptionBase::nontraversable)
-        optflags += " nontraversable |";
+string HelpSystem::helpClassParents(const string& classname)
+{
+    vector<string> parents= listClassParents(classname);
+    string s= "";
+    for(vector<string>::reverse_iterator it= parents.rbegin();
+        it != parents.rend(); ++it)
+        s+= *it + " > ";
+    return s + classname;
+}
 
-    string helpstring = string("OPTION ")+classname+"::"+optionname+"\n"
-        +"Flags: "+optflags+"\n"
-        +"Type: "+opttype+"\n"
-        +"Description: "+ descr +"\n";
-    return helpstring;
+string HelpSystem::helpClassParentsHTML(const string& classname)
+{
+    return string("<div class=\"crumbtrail\">")
+        + HTMLUtils::quote_format_and_highlight(helpClassParents(classname))
+        + "</div>";
 }
 
+vector<string> HelpSystem::listDerivedClasses(const string& classname)
+{
+    const TypeMapEntry& entry= 
+        TypeFactory::instance().getTypeMapEntry(classname);
+    const TypeMap& the_map= TypeFactory::instance().getTypeMap();
+    vector<string> classes;
+    for(TypeMap::const_iterator it= the_map.begin();
+        it != the_map.end(); ++it)
+    {
+        const TypeMapEntry& e= it->second;
+        if(e.constructor && it->first!=classname)
+        {
+            Object* o= (*e.constructor)();
+            if((*entry.isa_method)(o))
+                classes.push_back(it->first);
+            if(o) delete o;
+        }
+    }
+    return classes;
+}
 
+string HelpSystem::helpDerivedClasses(const string& classname)
+{
+    vector<string> classes= listDerivedClasses(classname);
+    string s= "";
+    for(vector<string>::iterator it= classes.begin();
+        it != classes.end(); ++it)
+        s+= right(*it, 30) + " - " 
+            + TypeFactory::instance().getTypeMapEntry(*it).one_line_descr 
+            + '\n';
+    return s;
+}
 
-void printObjectHelp(PStream out, const string& classname)
+string HelpSystem::helpDerivedClassesHTML(const string& classname)
 {
-    const TypeMap& type_map = TypeFactory::instance().getTypeMap();
-    TypeMap::const_iterator it = type_map.find(classname);
-    TypeMap::const_iterator itend = type_map.end();
+    // Output instantiable derived classes
+    vector<string> classes= listDerivedClasses(classname);
 
-    if(it==itend)
-        PLERROR("Object type %s unknown.\n"
-                "Did you #include it, does it call the IMPLEMENT_NAME_AND_DEEPCOPY macro?\n"
-                "and has it indeed been linked with your program?", classname.c_str());
+    string s= "<div class=\"generaltable\">\n"
+        "<h2>List of Instantiable Derived Classes</h2>\n"
+        "<table cellspacing=\"0\" cellpadding=\"0\">\n";
 
-    const TypeMapEntry& entry = it->second;
-    Object* obj = 0;
+    int i= 0;
+    for(vector<string>::iterator it= classes.begin();
+        it != classes.end(); ++it)
+        s+= string("  <tr class=\"") + (i++%2 == 0? "even" : "odd") + "\">\n"
+            "    <td>"
+            + HTMLUtils::quote_format_and_highlight(*it) + "</td><td>" 
+            + HTMLUtils::quote_format_and_highlight(
+                TypeFactory::instance().getTypeMapEntry(*it).one_line_descr)
+            + "    </td>  </tr>\n";
 
-    out << "****************************************************************** \n"
-        << "** " << classname << "\n"
-        << "****************************************************************** \n" << endl;
+    if(i==0)
+        s+= "<tr><td>This class does not have instantiable "
+            "derived classes.</td></tr>\n";
 
-    // Display basic help
-    out << entry.one_line_descr << endl << endl;
-    out << entry.multi_line_help << endl << endl;
+    return s+"</table></div>\n";
+}
 
-    if(entry.constructor) // it's an instantiable class
-        obj = (*entry.constructor)();
-    else
-        out << "Note: " << classname << " is a base-class with pure virtual methods that cannot be instantiated directly.\n" 
-            << "(default values for build options can only be displayed for instantiable classes, \n"
-            << " so you'll only see question marks here.)\n" << endl;
-      
-    out << "****************************************************************** \n"
-        << "**                         Build Options                        ** \n"
-        << "** (including those inherited from parent and ancestor classes) ** \n"
-        << "****************************************************************** \n" << endl;
+pair<string, vector<string> > HelpSystem::precisOnClass(const string& classname)
+{
+    const TypeMapEntry& tme = TypeFactory::instance().getTypeMapEntry(classname);
+    return make_pair(tme.one_line_descr, listBuildOptions(classname));
+}
 
-    out << classname + "( \n";
-    OptionList& options = (*entry.getoptionlist_method)();    
 
-    for( OptionList::iterator olIt = options.begin(); olIt!=options.end(); ++olIt )
+/*****
+ * Options
+ */
+
+vector<string> HelpSystem::listClassOptions(const string& classname)
+{
+    const TypeMapEntry& e= TypeFactory::instance().getTypeMapEntry(classname);
+    OptionList& optlist= (*e.getoptionlist_method)();
+    int nopts= optlist.size();
+    vector<string> options(nopts);
+    for(int i= 0; i < nopts; ++i) options[i]= optlist[i]->optionname();
+    return options;
+}
+
+vector<string> HelpSystem::listBuildOptions(const string& classname)
+{
+    const TypeMapEntry& e= TypeFactory::instance().getTypeMapEntry(classname);
+    OptionList& optlist= (*e.getoptionlist_method)();
+    vector<string> options;
+    for(OptionList::iterator it= optlist.begin();
+        it != optlist.end(); ++it)
+        if((*it)->flags() & OptionBase::buildoption)
+            options.push_back((*it)->optionname());
+    return options;
+}
+
+string HelpSystem::helpClassOptions(const string& classname)
+{
+    string s= "";
+    vector<string> options= listClassOptions(classname);
+    for(vector<string>::iterator it= options.begin();
+        it != options.end(); ++it)
+        s+= helpOnOption(classname, *it) + '\n';
+    return s;
+}
+
+string HelpSystem::helpOnOption(const string& classname, const string& optionname)
+{
+    string s= "";
+    PP<OptionBase> opt= getOptionByName(classname, optionname);
+    OptionBase::flag_t flags = opt->flags();
+    if(flags & OptionBase::buildoption 
+       && opt->level() <= OptionBase::getCurrentOptionLevel())
+        s+= addprefix("# ", opt->optiontype() + ": " + opt->description())
+            + addprefix("# ", "*OptionLevel: " + opt->levelString())
+            + opt->optionname() + " = " 
+            + getOptionDefaultVal(classname, optionname) + " ;\n\n";
+
+    return s;
+}
+
+string HelpSystem::helpClassOptionsHTML(const string& classname)
+{
+    string s= "<div class=\"generaltable\">\n" 
+        "<h2>List of All Options</h2>\n" 
+        "<table cellspacing=\"0\" cellpadding=\"0\">\n";
+    s.reserve(32768);
+
+    vector<string> options= listClassOptions(classname);
+    int i= 0;
+    for(vector<string>::iterator it= options.begin();
+        it != options.end(); ++it)
     {
-        OptionBase::flag_t flags = (*olIt)->flags();
-        if(flags & OptionBase::buildoption)
+        PP<OptionBase> opt= getOptionByName(classname, *it);
+        if(opt->flags() & OptionBase::buildoption 
+           && opt->level() <= OptionBase::getCurrentOptionLevel())
         {
-            string descr = (*olIt)->description();
-            string optname = (*olIt)->optionname();
-            string opttype = (*olIt)->optiontype();
-            string defaultval = "?";
-            if(obj) // it's an instantiable class
-            {
-                defaultval = (*olIt)->defaultval(); 
-                if(defaultval=="")
-                    defaultval = (*olIt)->writeIntoString(obj);
-            }
-            // string holderclass = (*olIt)->optionHolderClassName(this);
-            out << addprefix("# ", opttype + ": " + descr);
-            out << optname + " = " + defaultval + " ;\n\n";
+            s+= string("  <tr class=\"") + (i % 2 == 0? "even" : "odd") + "\">\n"
+                + helpOnOptionHTML(classname, *it) + "</tr>\n";
+            ++i;
         }
     }
-    out << ");\n\n";
+    
+    if (i==0)
+        s+= "<tr><td>This class does not specify any build options.</td></tr>\n";
+    
+    s+= "</table></div>\n";
+    return s;
+}
 
-    if(obj)
+string HelpSystem::helpOnOptionHTML(const string& classname, const string& optionname)
+{
+    // the result is a list of table fields (<td>...</td>)
+    // should be enclosed in a table row (<table><tr>...</tr></table>)
+    string s= "";
+    PP<OptionBase> opt= getOptionByName(classname, optionname);
+    s+= "    <td><div class=\"opttype\">" 
+        + HTMLUtils::quote_format_and_highlight(opt->optiontype()) + "</div>\n"
+        "    <div class=\"optname\">" 
+        + HTMLUtils::quote(opt->optionname()) + "</div>\n";
+
+    string defaultval= getOptionDefaultVal(classname, optionname);
+    if (defaultval != "?")
+        s+= "    <div class=\"optvalue\">= " 
+            + HTMLUtils::quote(defaultval) + "</div>\n";
+
+    string flag_string = join(opt->flagStrings(), " | ");
+    s+= string("    <div class=\"opttype\"><i>(")
+        + join(opt->flagStrings(), " | ") + ")</i></div>\n"
+        + "    <div class=\"optlevel\"><i>(OptionLevel: " 
+        + opt->levelString() + ")</i></div>\n"
+        "    </td>\n";
+
+    string descr= opt->description();
+    if (removeblanks(descr) == "") descr = "(no description)";
+    s+= string("    <td>")+HTMLUtils::quote_format_and_highlight(descr);
+
+    string defclass= getOptionDefiningClass(classname, optionname);
+    if (defclass != "") 
+        s+= string("    <span class=\"fromclass\">")
+            + "(defined&nbsp;by&nbsp;" 
+            + HTMLUtils::highlight_known_classes(defclass) + ")"
+            "</span>\n";
+    s+= "    </td>\n";
+    return s;
+}
+
+string HelpSystem::getOptionDefaultVal(const string& classname, 
+                                       const string& optionname)
+{
+    PP<OptionBase> opt= getOptionByName(classname, optionname);
+    const TypeMapEntry& entry= TypeFactory::instance().getTypeMapEntry(classname);
+    Object* obj= 0;
+    if(entry.constructor) obj= (*entry.constructor)();
+
+    string defaultval= "?";
+    if(obj) // it's an instantiable class
+    {
+        defaultval= opt->defaultval(); 
+        if(defaultval=="") defaultval= opt->writeIntoString(obj);
         delete obj;
+    }
+    return defaultval;
+}
 
-    out << "****************************************************************** \n"
-        << "** Subclasses of " << classname << " \n"
-        << "** (only those that can be instantiated) \n"
-        << "****************************************************************** \n" << endl;
-    for(it = type_map.begin(); it!=itend; ++it)
+string HelpSystem::getOptionDefiningClass(const string& classname, 
+                                          const string& optionname)
+{
+    PP<OptionBase> opt= getOptionByName(classname, optionname);
+    const TypeMapEntry& entry= TypeFactory::instance().getTypeMapEntry(classname);
+    string defclass= "";
+    if(entry.constructor)
     {
-        // cerr << "Attempting to instantiate: " << it->first << endl;
-        const TypeMapEntry& e = it->second;
-        if(e.constructor && it->first!=classname)
+        Object* obj= (*entry.constructor)();
+        defclass= opt->optionHolderClassName(obj);
+        delete obj;
+    }
+    return defclass;
+}
+
+/*****
+ * Methods
+ */
+
+vector<pair<string, int> > HelpSystem::listMethods(const string& classname)
+{
+    return TypeFactory::instance().getTypeMapEntry(classname).
+        get_remote_methods().getMethodList();
+}
+
+vector<string> HelpSystem::listMethodPrototypes(const string& classname)
+{
+    return TypeFactory::instance().getTypeMapEntry(classname).
+        get_remote_methods().getMethodPrototypes();
+}
+
+string HelpSystem::helpMethods(const string& classname)
+{
+    vector<pair<string, int> > methods= listMethods(classname);
+    string s= "";
+    for(vector<pair<string, int> >::iterator it= methods.begin();
+        it != methods.end(); ++it)
+        s+= string("\n##########\n") 
+            + helpOnMethod(classname, it->first, it->second) + '\n';
+    return s;
+}
+
+string HelpSystem::helpOnMethod(const string& classname, 
+                                const string& methodname, int arity)
+{
+    return TypeFactory::instance().getTypeMapEntry(classname)
+        .get_remote_methods().getMethodHelpText(methodname, arity);
+}
+
+string HelpSystem::helpMethodsHTML(const string& classname)
+{
+    string s= "<div class=\"rmitable\">\n" 
+        "<h2>List of Remote-Callable Methods</h2>\n"
+        "<table cellspacing=\"0\" cellpadding=\"0\">\n";
+    
+    vector<string> parents= listClassParents(classname);
+    parents.insert(parents.begin(), classname);
+    vector<pair<string, int> > methods;
+    map<pair<string, int>, string> definingclass;
+    for(vector<string>::iterator it= parents.begin();
+        it != parents.end(); ++it)
+    {
+        vector<pair<string, int> > ms= listMethods(*it);
+        for(vector<pair<string, int> >::iterator jt= ms.begin();
+            jt != ms.end(); ++jt)
         {
-            Object* o = (*e.constructor)();
-            if( (*entry.isa_method)(o) ) {
-                out.width(30);
-                out << it->first << " - " << e.one_line_descr << endl;
-            }
-            if(o)
-                delete o;
+            if(definingclass.find(*jt) == definingclass.end())
+                methods.push_back(*jt);
+            definingclass[*jt]= *it;
         }
     }
 
-    out << "\n\n------------------------------------------------------------------ \n" << endl;
+    int i= 0;
+    for(vector<pair<string, int> >::iterator it= methods.begin(); 
+        it != methods.end(); ++it)
+        s+= string("<tr class=\"") + (i++ == 0? "first" : "others") + "\">\n"
+            + helpOnMethodHTML(definingclass[*it], it->first, it->second)
+            + "</tr>\n";
 
+    if(index == 0)
+        s+= "<tr><td>This class does not define any remote-callable methods.</td></tr>\n";
+           
+    s+= "</table></div>\n";
+
+    return s;
 }
 
-*/
+string HelpSystem::helpOnMethodHTML(const string& classname, 
+                                    const string& methodname, int arity)
+{
+    // the result is a list of table fields (<td>...</td>)
+    // should be enclosed in a table row (<table><tr>...</tr></table>)
+    const RemoteTrampoline* t= TypeFactory::instance().getTypeMapEntry(classname)
+        .get_remote_methods().lookup(methodname, arity);
+    PLASSERT(t);
+    return formatMethodDocHTML(t->documentation(), classname);
+}
 
+
+/***
+ * General Stuff
+ */
+
+string HelpSystem::html_resources_path; // init.
+
+string HelpSystem::helpIndexHTML()
+{
+    static PPath from_dir= "";
+    static string the_index= string("<div class=\"cmdname\">\n")
+        + "Welcome to PLearn User-Level Class and Commands Help\n" 
+        "</div>"
+        "<div class=\"cmdhelp\">\n"
+        "<ul>\n"
+        "  <li> <a href=\"classes_index.html\">Class Index</a>\n"
+        "  <li> <a href=\"commands_index.html\">Command Index</a>\n"
+        "  <li> <a href=\"functions_index.html\">Function Index</a>\n"
+        "</ul></div>\n";
+
+    if(from_dir != html_resources_path)
+    {
+        from_dir= html_resources_path;
+        PPath indexfile= from_dir/"index.html";
+        if(pathexists(indexfile))
+        {
+            PStream f= openFile(indexfile,
+                                PStream::raw_ascii);
+            the_index= f.readAll();
+        }
+    }
+
+    return helpPrologueHTML() + the_index + helpEpilogueHTML();
+}
+
+string HelpSystem::helpPrologueHTML(const string& title)
+{
+    static PPath from_dir= "";
+    static string the_prologue= "<html><body>";
+
+    if(from_dir != html_resources_path)
+    {
+        from_dir= html_resources_path;
+        PStream f= openFile(from_dir/"help_prolog.html",
+                            PStream::raw_ascii);
+        the_prologue= f.readAll();
+    }
+
+    map<string, string> dic;
+    dic["PAGE_TITLE"]= title;
+    PStream stm= openString(the_prologue, PStream::raw_ascii);
+    return readAndMacroProcess(stm, dic, false);
+}
+
+string HelpSystem::helpEpilogueHTML()
+{
+    static PPath from_dir= "";
+    static string the_epilogue= "</body></html>";
+
+    if(from_dir != html_resources_path)
+    {
+        from_dir= html_resources_path;
+        PStream f= openFile(from_dir/"help_epilog.html",
+                            PStream::raw_ascii);
+        the_epilogue= f.readAll();
+    }
+
+    return HTMLUtils::generated_by() + the_epilogue;
+}
+
+
+
+BEGIN_DECLARE_REMOTE_FUNCTIONS
+
+// Commands
+
+    declareFunction("listCommands", &HelpSystem::listCommands,
+                    (BodyDoc("Returns a list of all registered "
+                             "commands as strings."),
+                     RetDoc ("vector of command names")));
+
+    declareFunction("helpCommands", &HelpSystem::helpCommands,
+                    (BodyDoc("Returns a plain text list of all "
+                             "registered commands."),
+                     RetDoc ("plain text list of commands")));
+
+    declareFunction("helpOnCommand", &HelpSystem::helpOnCommand,
+                    (BodyDoc("Will return full help for the "
+                             "command with the given name "),
+                     ArgDoc ("commandname", 
+                             "The name of the command on which to get help"),
+                     RetDoc ("help text for the command")));
+
+    // HTML
+    declareFunction("helpCommandsHTML", &HelpSystem::helpCommandsHTML,
+                    (BodyDoc("Returns an HTML list of all "
+                             "registered commands."),
+                     RetDoc ("HTML list of commands")));
+
+    declareFunction("helpOnCommandHTML", &HelpSystem::helpOnCommandHTML,
+                    (BodyDoc("Will return full help for the "
+                             "command with the given name, in HTML"),
+                     ArgDoc ("commandname", 
+                             "The name of the command on which to get help"),
+                     RetDoc ("help text for the command, in HTML")));
+
+// Functions
+
+    declareFunction("listFunctions", &HelpSystem::listFunctions,
+                    (BodyDoc("Returns a list of all registered global "
+                             "functions as pairs of (funtionname, nargs)"),
+                     RetDoc ("vector of function names, arity")));
+
+    declareFunction("listFunctionPrototypes", 
+                    &HelpSystem::listFunctionPrototypes,
+                    (BodyDoc("Returns a list of the prototypes "
+                             "of all registered global functions"),
+                     RetDoc ("vector of function prototypes as strings")));
+
+    declareFunction("helpFunctions", &HelpSystem::helpFunctions,
+                    (BodyDoc("Returns a list of all registered global "
+                             "functions as plain text"),
+                     RetDoc ("plain text list of functions")));
+
+    declareFunction("helpOnFunction", &HelpSystem::helpOnFunction,
+                    (BodyDoc("Will return full help on all registered "
+                             "global functions with the given name "),
+                     ArgDoc ("functionname", 
+                             "The name of the function on which to get help"),
+                     ArgDoc ("arity", "The number of params"),
+                     RetDoc ("help text for the function")));
+
+    // HTML
+    declareFunction("helpFunctionsHTML", &HelpSystem::helpFunctionsHTML,
+                    (BodyDoc("Returns a list of all registered global "
+                             "functions as an HTML page."),
+                     RetDoc ("HTML list of functions")));
+
+    declareFunction("helpOnFunctionHTML", &HelpSystem::helpOnFunctionHTML,
+                    (BodyDoc("Will return full HTML help on all registered "
+                             "global functions with the given name "),
+                     ArgDoc ("functionname", 
+                             "The name of the function on which to get help"),
+                     ArgDoc ("arity", "The number of params"),
+                     RetDoc ("HTML help text for the function")));
+
+// Classes
+
+    declareFunction("listClasses", &HelpSystem::listClasses,
+                    (BodyDoc("Returns a list of all registered Object classes"),
+                     RetDoc ("vector of class names")));
+
+    declareFunction("getClassTree", &HelpSystem::getClassTree,
+                    (BodyDoc("Returns a map, mapping all registered "
+                             "Object classnames to their parentclassname"),
+                     RetDoc ("map of class names to class names")));
+
+    declareFunction("helpClasses", &HelpSystem::helpClasses,
+                    (BodyDoc("Returns a plain text list of all registered Object classes"),
+                     RetDoc ("plain text list of class names")));
+
+    declareFunction("helpOnClass", &HelpSystem::helpOnClass,
+                    (BodyDoc("Will return full help for "
+                             "the class with the given name"),
+                     ArgDoc ("classname", 
+                             "The name of the class on which to get help"),
+                     RetDoc ("help text for the class")));
+
+    declareFunction("precisOnClass", &HelpSystem::precisOnClass,
+                    (BodyDoc("Will return short class descr. and list of build options"),
+                     ArgDoc ("classname", 
+                             "The name of the class on which to get help"),
+                     RetDoc ("pair of classname and list of options")));
+
+    // HTML
+    declareFunction("helpClassesHTML", &HelpSystem::helpClassesHTML,
+                    (BodyDoc("Returns a list of all registered Object "
+                             "classes as an HTML page."),
+                     RetDoc ("HTML list of class names")));
+
+    declareFunction("helpOnClassHTML", &HelpSystem::helpOnClassHTML,
+                    (BodyDoc("Will return full HTML help for "
+                             "the class with the given name"),
+                     ArgDoc ("classname", 
+                             "The name of the class on which to get help"),
+                     RetDoc ("HTML help text for the class")));
+    // Parents
+    declareFunction("listClassParents", &HelpSystem::listClassParents,
+                    (BodyDoc("List of parent classes."),
+                     ArgDoc ("classname", 
+                             "The name of the class on which to get parents"),
+                     RetDoc ("vector of parent class names")));
+
+    declareFunction("helpClassParents", &HelpSystem::helpClassParents,
+                    (BodyDoc("Text list of parent classes."),
+                     ArgDoc ("classname", 
+                             "The name of the class on which to get parents"),
+                     RetDoc ("text list of parent class names")));
+
+    declareFunction("helpClassParentsHTML", &HelpSystem::helpClassParentsHTML,
+                    (BodyDoc("HTML list of parent classes."),
+                     ArgDoc ("classname", 
+                             "The name of the class on which to get parents"),
+                     RetDoc ("HTML list of parent class names")));
+    // Children
+    declareFunction("listDerivedClasses", &HelpSystem::listDerivedClasses,
+                    (BodyDoc("List of derived classes."),
+                     ArgDoc ("classname", 
+                             "The name of the class on which to get children"),
+                     RetDoc ("List of derived class names")));
+
+    declareFunction("helpDerivedClasses", &HelpSystem::helpDerivedClasses,
+                    (BodyDoc("Text list of derived classes."),
+                     ArgDoc ("classname", 
+                             "The name of the class on which to get children"),
+                     RetDoc ("Text list of derived class names")));
+
+    declareFunction("helpDerivedClassesHTML", &HelpSystem::helpDerivedClassesHTML,
+                    (BodyDoc("HTML list of derived classes."),
+                     ArgDoc ("classname", 
+                             "The name of the class on which to get children"),
+                     RetDoc ("HTML list of derived class names")));
+
+// Options
+
+    declareFunction("listClassOptions", &HelpSystem::listClassOptions,
+                    (BodyDoc("Returns a list of all options "
+                             "for the given class."),
+                     ArgDoc ("classname", "The name of the class "
+                             "on which to get option help"),
+                     RetDoc ("vector of option names")));
+
+    declareFunction("listBuildOptions", &HelpSystem::listBuildOptions,
+                    (BodyDoc("Returns a list of build options "
+                             "for the given class."),
+                     ArgDoc ("classname", "The name of the class "
+                             "on which to get option help"),
+                     RetDoc ("vector of option names")));
+
+    declareFunction("helpOnOption", &HelpSystem::helpOnOption,
+                    (BodyDoc("Will return full help for the option with "
+                             "the given name within the given class"),
+                     ArgDoc ("classname", "The name of the class "
+                             "on which to get option help"),
+                     ArgDoc ("optionname", 
+                             "The name of the option on which to get help"),
+                     RetDoc ("help text for the option")));
+    // HTML
+    declareFunction("helpClassOptionsHTML", &HelpSystem::helpClassOptionsHTML,
+                    (BodyDoc("Returns a list of all options "
+                             "for the given class as an HTML page."),
+                     ArgDoc ("classname", "The name of the class "
+                             "on which to get option help"),
+                     RetDoc ("HTML list of option names")));
+
+    declareFunction("helpOnOptionHTML", &HelpSystem::helpOnOptionHTML,
+                    (BodyDoc("Will return full HTML help for the option "
+                             "with the given name within the given class"),
+                     ArgDoc ("classname", "The name of the class "
+                             "on which to get option help"),
+                     ArgDoc ("optionname", 
+                             "The name of the option on which to get help"),
+                     RetDoc ("HTML help text for the option")));
+
+// Methods
+
+    declareFunction("listMethods", &HelpSystem::listMethods,
+                    (BodyDoc("Returns a list of all registered methods "
+                             "for the given class as pairs of (methodname, nargs)"),
+                     ArgDoc ("classname", 
+                             "The name of the class whose methods you want to list."),
+                     RetDoc ("vector of method names")));
+
+    declareFunction("listMethodPrototypes", 
+                    &HelpSystem::listMethodPrototypes,
+                    (BodyDoc("Returns a list of the prototypes of "
+                             "all registered methods for the given class"),
+                     ArgDoc ("classname", "The name of the class "
+                             "whose method prototypes you want to list."),
+                     RetDoc ("vector of prototypes as strings")));
+
+    declareFunction("helpMethods", &HelpSystem::helpMethods,
+                    (BodyDoc("Returns a list of all registered methods "
+                             "for the given class as text."),
+                     ArgDoc ("classname", 
+                             "The name of the class whose methods you want to list."),
+                     RetDoc ("Text list of method names")));
+
+    declareFunction("helpOnMethod", &HelpSystem::helpOnMethod,
+                    (BodyDoc("Will return full help on all registered "
+                             "methods of the class with the given name"),
+                     ArgDoc ("classname", "The name of the class"),
+                     ArgDoc ("methodname", "The name of the method"),
+                     ArgDoc ("arity", "The number of params"),
+                     RetDoc ("help text")));
+
+    // HTML
+    declareFunction("helpMethodsHTML", &HelpSystem::helpMethodsHTML,
+                    (BodyDoc("Returns a list of all registered methods "
+                             "for the given class as an HTML page."),
+                     ArgDoc ("classname", 
+                             "The name of the class whose methods you want to list."),
+                     RetDoc ("HTML list of method names")));
+
+    declareFunction("helpOnMethodHTML", &HelpSystem::helpOnMethodHTML,
+                    (BodyDoc("Will return full help on all registered "
+                             "methods of the class with the given name"),
+                     ArgDoc ("classname", "The name of the class"),
+                     ArgDoc ("methodname", "The name of the method"),
+                     ArgDoc ("arity", "The number of params"),
+                     RetDoc ("help text in HTML")));
+
+// HTML-only
+
+    declareFunction("helpIndexHTML", &HelpSystem::helpIndexHTML,
+                    (BodyDoc("Returns the global help index in HTML."),
+                     RetDoc ("HTML global help index")));
+
+    declareFunction("setResourcesPathHTML", 
+                    &HelpSystem::setResourcesPathHTML,
+                    (BodyDoc("Sets the help resource path "
+                             "for HTML resources."),
+                     ArgDoc ("path","HTML help resource path")));
+
+    declareFunction("getResourcesPathHTML", 
+                    &HelpSystem::getResourcesPathHTML,
+                    (BodyDoc("Gets the help resource path "
+                             "for HTML resources."),
+                     RetDoc ("path of HTML resources")));
+
+END_DECLARE_REMOTE_FUNCTIONS
+
+
+PP<OptionBase> HelpSystem::getOptionByName(const string& classname, 
+                                           const string& optionname)
+{
+    const TypeMapEntry& e= TypeFactory::instance().getTypeMapEntry(classname);
+    OptionList& optlist= (*e.getoptionlist_method)();
+    for(OptionList::iterator it= optlist.begin();
+        it != optlist.end(); ++it)
+        if((*it)->optionname() == optionname)
+            return *it;
+    return 0;//not found...
+}
+
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/base/HelpSystem.h
===================================================================
--- trunk/plearn/base/HelpSystem.h	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/base/HelpSystem.h	2007-04-05 01:35:37 UTC (rev 6834)
@@ -2,6 +2,7 @@
 
 // HelpSystem.h
 // Copyright (c) 2006 Pascal Vincent
+// Copyright (C) 2007 Xavier Saint-Mleux, ApSTAT Technologies, inc.
 
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
@@ -40,6 +41,7 @@
 #include <vector>
 #include <utility>
 #include <map>
+#include <plearn/base/OptionBase.h>
 
 namespace PLearn {
 using std::string;
@@ -47,66 +49,209 @@
 using std::pair;
 using std::map;
 
-//! Returns a list of all registered global functions as pairs of (funtionname, nargs)
-vector< pair<string, int> > listFunctions();
 
-//! Returns a list of the prototypes of all registered global functions
-vector<string> listFunctionPrototypes();
+struct HelpSystem // more or less a namespace
+{
 
-//! Will return full help on all registered global functions with the given name 
-string helpFunction(const string& functionname);
+    /**
+     * Help on Commands
+     */
 
-//! Returns a list of all registered methods for the given class as pairs of (methodname, nargs)
-vector< pair<string, int> > listMethods(const string& classname);
+    //! Returns a list of all plearn command names
+    static vector<string> listCommands();
 
-//! Returns a list of the prototypes of all registered methods for the given class
-vector<string> listMethodPrototypes(const string& classname);
+    //! Returns a text list of all plearn command names
+    static string helpCommands();
 
-//! Will return full help on all registered methods of the class with the given name 
-string helpMethod(const string& classname, const string& methodname);
+    //! Will return full help on the given command
+    static string helpOnCommand(const string& commandname);
 
-//! Returns a list of all registered Object classes
-vector<string> listClasses();
+    //! Returns a list of all plearn commands as an HTML page
+    static string helpCommandsHTML();
 
-//! Returns a map, mapping all registered Object classnames to their parentclassname
-map<string, string> getClassTree();
+    //! Will return full HTML help on the given command
+    static string helpOnCommandHTML(const string& commandname);
 
-/*
+    /**
+     * Help on Global Functions
+     */
 
-//! Returns a list of all direct subclasses of classname
-//! Throws an exception if classname is not registered.
-vector<string> childrenOf(const string& classname);
+    //! Returns a list of all registered global functions as pairs of (funtionname, nargs)
+    static vector<pair<string, int> > listFunctions();
 
-//! Returns a list of all descendents of the given class
-//! Throws an exception if classname is not registered.
-vector<string> descendantsOf(const string& classname);
+    //! Returns a list of the prototypes of all registered global functions
+    static vector<string> listFunctionPrototypes();
 
-//! Returns the parent class of classname (empty string if no parent)
-//! Throws an exception if classname is not registered.
-string parentOf(const string& classname);
+    //! Returns a list of all registered global functions in plain text
+    static string helpFunctions();
 
-//! Returns the structured FunctionHelp object describing the specified function
-//! Throws an exception if no function is registered with that name and number of arguments
-//FunctionHelp getFunctionDoc(const string& functionname, int nargs);
+    //! Will return full help on all registered global functions with the given name 
+    static string helpOnFunction(const string& functionname, int arity);
 
-//! Returns the structured FunctionHelp object describing the specified method
-//! Throws an exception if classname is not registered or if it has no
-//! registered method with that name and number of arguments.
-//FunctionHelp getMethodDoc(const string& classname, const string& methodname, int nargs);
+    //! Returns a list of all registered global functions as an HTML page
+    static string helpFunctionsHTML();
 
+    //! Will return full help on all registered global functions with
+    //! the given name, as an HTML string.
+    static string helpOnFunctionHTML(const string& functionname, int arity);
 
-//! Returns the list of options
-// vector<string> listClassOptions(const string& classname);
 
-//! Will returns detailed help on registered class with the given name
-//! listing its parent class, and detailed help on all options including inherited ones,
-//! as well as listing all its registered methods.
-string helpClass(const string& classname);
+    /**
+     * Help on Registered Classes
+     */
 
+    //! Returns a list of all registered Object classes
+    static vector<string> listClasses();
 
-*/
+    //! Returns a map, mapping all registered Object classnames to their parentclassname
+    static map<string, string> getClassTree();
 
+    //! Returns a list of all registered Object classes as plain text
+    static string helpClasses();
 
+    //! Will returns detailed help on registered class with the given name
+    //! listing its parent class, and detailed help on all options including inherited ones,
+    //! as well as listing all its registered methods.
+    static string helpOnClass(const string& classname);
+
+    //! Returns a list of all registered Object classes as an HTML page
+    static string helpClassesHTML();
+
+    //! same as helpOnClass, but in HTML
+    static string helpOnClassHTML(const string& classname);
+
+    /* Class Parents */
+
+    //! Returns a list of all parent classes of this class
+    //! list goes from classname::inherited up to Object
+    static vector<string> listClassParents(const string& classname);
+
+    //! Returns a text list of all parent classes of this class
+    //! list goes from Object down to classname
+    static string helpClassParents(const string& classname);
+
+    //! Returns an HTML list of all parent classes of this class
+    //! list goes from Object down to classname
+    static string helpClassParentsHTML(const string& classname);
+
+    /* Derived Classes */
+
+    //! Returns a list of all instantiable classes derived from 'classname'
+    static vector<string> listDerivedClasses(const string& classname);
+
+    //! Returns a text list of all instantiable classes derived from 'classname'
+    static string helpDerivedClasses(const string& classname);
+
+    //! Returns an HTML list of all instantiable classes derived from 'classname'
+    static string helpDerivedClassesHTML(const string& classname);
+
+    //! Returns a pair of class descr. and list of build options
+    static pair<string, vector<string> > precisOnClass(const string& classname);
+
+    /**
+     * Help on Class Options
+     */
+
+     //! Returns the list of all options for the class with the given name
+    static vector<string> listClassOptions(const string& classname);
+
+     //! Returns the list of build options for the class with the given name
+    static vector<string> listBuildOptions(const string& classname);
+
+     //! Returns the list of options for the class with the given name, as text
+    static string helpClassOptions(const string& classname);
+
+    //! Will return full help on the declared option of the class with the given name 
+    static string helpOnOption(const string& classname, const string& optionname);
+
+     //! Returns the list of options for the class with the given name, in HTML
+    static string helpClassOptionsHTML(const string& classname);
+
+    //! Will return full help on the declared option of the class
+    //! with the given name, as an HTML string.
+    static string helpOnOptionHTML(const string& classname, 
+                                   const string& optionname);
+
+    //! Returns the default value for this option, or "?" if 
+    //! it is from an abstract class
+    static string getOptionDefaultVal(const string& classname, 
+                                      const string& optionname);
+
+    //! Returns the class that defines this option, or "" if not known
+    static string getOptionDefiningClass(const string& classname, 
+                                         const string& optionname);
+
+    /**
+     * Help on Class Methods
+     */
+
+    //! Returns a list of all registered methods for the 
+    //! given class as pairs of (methodname, nargs)
+    static vector<pair<string, int> > listMethods(const string& classname);
+
+    //! Returns a list of the prototypes of all registered methods for the given class
+    static vector<string> listMethodPrototypes(const string& classname);
+
+    //! Returns a list of all registered methods for the 
+    //! given class as text
+    static string helpMethods(const string& classname);
+
+    //! Will return full help on the registered method of the class with the 
+    //! given name and arity
+    static string helpOnMethod(const string& classname, 
+                               const string& methodname, int arity= -1);
+
+    //! Returns a list of all registered methods for the 
+    //! given class as an HTML page
+    static string helpMethodsHTML(const string& classname);
+
+    //! Will return full help on the registered method of the class 
+    //! with the given name and arity, as an HTML string.
+    static string helpOnMethodHTML(const string& classname, 
+                                   const string& methodname, int arity= -1);
+
+    /**
+     * HTML Help
+     */
+
+private:
+    //! Directory that holds HTML resources.
+    //! These include: 
+    //! - index.html (optional)
+    //! - help_prolog.html
+    //! - help_epilog.html
+    //! e.g. {PLEARNDIR}/python_modules/plearn/plide/resources
+    static string html_resources_path;
+    
+public:
+    //! Sets the path for resources for HTML help
+    static void setResourcesPathHTML(const string& path)
+    { html_resources_path= path; }
+
+    //! Returns the path for resources for HTML help
+    static string getResourcesPathHTML()
+    { return html_resources_path; }
+
+    //! Returns the global help index as an HTML page
+    static string helpIndexHTML();
+
+    //! Returns the standard heading for HTML help
+    static string helpPrologueHTML(const string& title= 
+                                   "PLearn User-Level Documentation");
+
+    //! Returns the standard ending for HTML help
+    static string helpEpilogueHTML();
+
+
+
+private:
+    static PP<OptionBase> getOptionByName(const string& classname, 
+                                          const string& optionname);
+
+
+};
+
+
 } // end of namespace PLearn
 
 #endif //!<  HelpSystem_INC_

Modified: trunk/plearn/base/Object.h
===================================================================
--- trunk/plearn/base/Object.h	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/base/Object.h	2007-04-05 01:35:37 UTC (rev 6834)
@@ -174,7 +174,7 @@
         static bool _isa_(const Object* o);                     \
         virtual CLASSTYPE* deepCopy(CopiesMap &copies) const;   \
         static void _static_initialize_();                      \
-        static StaticInitializer _static_initializer_
+        static StaticInitializer _static_initializer_           
 
 #define PLEARN_IMPLEMENT_ABSTRACT_OBJECT(CLASSTYPE, ONELINEDESCR, MULTILINEHELP)                \
         string CLASSTYPE::_classname_()                                                         \

Modified: trunk/plearn/base/TypeFactory.cc
===================================================================
--- trunk/plearn/base/TypeFactory.cc	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/base/TypeFactory.cc	2007-04-05 01:35:37 UTC (rev 6834)
@@ -114,6 +114,8 @@
 
 //#####  displayObjectHelp  ###################################################
 
+// DEPRECATED: use HelpSystem instead
+/*
 void displayObjectHelp(ostream& out, const string& classname)
 {
     const TypeMap& type_map = TypeFactory::instance().getTypeMap();
@@ -212,9 +214,9 @@
     out << "\n\n################################################################## \n" << endl;
 
 }
+*/
 
 
-
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/base/TypeFactory.h
===================================================================
--- trunk/plearn/base/TypeFactory.h	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/base/TypeFactory.h	2007-04-05 01:35:37 UTC (rev 6834)
@@ -180,7 +180,7 @@
 
 
 //! Will display the help message for an object of the given classname
-void displayObjectHelp(ostream& out, const string& classname);
+//void displayObjectHelp(ostream& out, const string& classname); //!< DEPRECATED: use HelpSystem
 
 
 } // end of namespace PLearn

Modified: trunk/plearn/base/test/ObjectGraphIterator/pytest.config
===================================================================
--- trunk/plearn/base/test/ObjectGraphIterator/pytest.config	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/base/test/ObjectGraphIterator/pytest.config	2007-04-05 01:35:37 UTC (rev 6834)
@@ -41,11 +41,12 @@
     3) Call 'which PRGNAME'
     4) Fail
     
-    Compilable program should provided the keyword argument 'compiler'
+    Compilable program should provide the keyword argument 'compiler'
     mapping to a string interpreted as the compiler name (e.g.
-    "compiler = 'pymake'"). Arguments to be forwarded to the compiler can be
-    provided as a string through the 'compile_options' keyword argument.
-      @type(program):
+    "compiler = 'pymake'"). If no compiler is provided while the program is
+    believed to be compilable, 'pymake' will be assigned by
+    default. Arguments to be forwarded to the compiler can be provided as a
+    string through the 'compile_options' keyword argument. @type program:
     Program
     
       @ivar(arguments):
@@ -78,6 +79,13 @@
       - A Program (see 'program' option) instance
       - None: if you are sure no files are to be compared.
     
+      @ivar(ignored_files_re):
+    Default behaviour of a test is to compare all
+    files created by running the test. In some case, one may prefer some of
+    these files to be ignored.
+      @type(ignored_files_re):
+    list of regular expressions
+    
       @ivar(disabled):
     If true, the test will not be ran.
       @type(disabled):
@@ -90,10 +98,9 @@
     category = "General",
     program = Program(
         name = "plearn_tests",
-        compiler = "pymake",
-        compile_options = ""
+        compiler = "pymake"
         ),
-    arguments = 'PLEARNDIR:scripts/command_line_object.plearn "object=ObjectGraphIteratorTest(save = 0)"',
+    arguments = "PLEARNDIR:scripts/command_line_object.plearn \"object=ObjectGraphIteratorTest(save = 0)\"",
     resources = [ ],
     precision = 1e-06,
     pfileprg = "__program__",

Modified: trunk/plearn/base/test/PP/pytest.config
===================================================================
--- trunk/plearn/base/test/PP/pytest.config	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/base/test/PP/pytest.config	2007-04-05 01:35:37 UTC (rev 6834)
@@ -41,11 +41,12 @@
     3) Call 'which PRGNAME'
     4) Fail
     
-    Compilable program should provided the keyword argument 'compiler'
+    Compilable program should provide the keyword argument 'compiler'
     mapping to a string interpreted as the compiler name (e.g.
-    "compiler = 'pymake'"). Arguments to be forwarded to the compiler can be
-    provided as a string through the 'compile_options' keyword argument.
-      @type(program):
+    "compiler = 'pymake'"). If no compiler is provided while the program is
+    believed to be compilable, 'pymake' will be assigned by
+    default. Arguments to be forwarded to the compiler can be provided as a
+    string through the 'compile_options' keyword argument. @type program:
     Program
     
       @ivar(arguments):
@@ -78,6 +79,13 @@
       - A Program (see 'program' option) instance
       - None: if you are sure no files are to be compared.
     
+      @ivar(ignored_files_re):
+    Default behaviour of a test is to compare all
+    files created by running the test. In some case, one may prefer some of
+    these files to be ignored.
+      @type(ignored_files_re):
+    list of regular expressions
+    
       @ivar(disabled):
     If true, the test will not be ran.
       @type(disabled):
@@ -90,10 +98,9 @@
     category = "General",
     program = Program(
         name = "plearn_tests",
-        compiler = "pymake",
-        compile_options = ""
+        compiler = "pymake"
         ),
-    arguments = 'PLEARNDIR:scripts/command_line_object.plearn object="PPTest(save = 0)"',
+    arguments = "PLEARNDIR:scripts/command_line_object.plearn object=\"PPTest(save = 0)\"",
     resources = [ ],
     precision = 1e-06,
     pfileprg = "__program__",

Modified: trunk/plearn/base/test/pytest.config
===================================================================
--- trunk/plearn/base/test/pytest.config	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/base/test/pytest.config	2007-04-05 01:35:37 UTC (rev 6834)
@@ -93,16 +93,16 @@
     
 """
 Test(
-  name = "PL_assert",
-  description = "Exercises the PLearn assertion-checking mechanism",
-  category = "General",
-  program = Program(
-    name = "assertions",
-    compiler = "pymake"
-    ),
-  arguments = "",
-  resources = [ ],
-  precision = 1e-06,
-  pfileprg = None,
-  disabled = False
-  )
+    name = "PL_assert",
+    description = "Exercises the PLearn assertion-checking mechanism",
+    category = "General",
+    program = Program(
+        name = "assertions",
+        compiler = "pymake"
+        ),
+    arguments = "",
+    resources = [ ],
+    precision = 1e-06,
+    pfileprg = None,
+    disabled = False
+    )

Modified: trunk/plearn/io/PStream.cc
===================================================================
--- trunk/plearn/io/PStream.cc	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/io/PStream.cc	2007-04-05 01:35:37 UTC (rev 6834)
@@ -265,6 +265,25 @@
     return oldmode;
 }
 
+
+/////////////
+// readAll //
+/////////////
+
+string PStream::readAll()
+{
+    const int bufsz= 4096;
+    char buf[bufsz];
+    string s= "";
+    int nread= ptr->read(buf, bufsz);
+    while(nread > 0)
+    {
+        s.append(buf, nread);
+        nread= ptr->read(buf, bufsz);
+    }
+    return s;
+}
+
 //////////////////
 // readExpected //
 //////////////////

Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/io/PStream.h	2007-04-05 01:35:37 UTC (rev 6834)
@@ -339,6 +339,9 @@
         return *this;
     }
 
+    //! Reads ultil eof, returns whole contents as a string
+    string readAll();
+
     //! Reads the next character and launches a PLERROR if it is different from
     //! expect.
     void readExpected(char expect);

Modified: trunk/plearn/io/fileutils.cc
===================================================================
--- trunk/plearn/io/fileutils.cc	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/io/fileutils.cc	2007-04-05 01:35:37 UTC (rev 6834)
@@ -690,7 +690,7 @@
 /////////////////////////
 // readAndMacroProcess //
 /////////////////////////
-string readAndMacroProcess(PStream& in, map<string, string>& variables)
+string readAndMacroProcess(PStream& in, map<string, string>& variables, bool skip_comments)
 {
     string text; // the processed text to return
     bool inside_a_quoted_string=false; // inside a quoted string we don't skip characters following a #
@@ -702,11 +702,10 @@
         if (last_c!='\\' && c=='"') // we find either the beginning or end of a quoted string
             inside_a_quoted_string = !inside_a_quoted_string; // flip status
 
-        if(!inside_a_quoted_string && c=='#')  // It's a comment: skip rest of line
-        {
+        if(!inside_a_quoted_string && c=='#' && skip_comments)
+            // It's a comment: skip rest of line
             while(c!=EOF && c!='\n' && c!='\r')
                 c = in.get();
-        }
 
         if(c==EOF)
             break;

Modified: trunk/plearn/io/fileutils.h
===================================================================
--- trunk/plearn/io/fileutils.h	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/io/fileutils.h	2007-04-05 01:35:37 UTC (rev 6834)
@@ -204,7 +204,8 @@
 //! entry in the map (the DEFINE macro will be discarded).
 //! Also every $INCLUDE{filepath} will be replaced in place by the text of
 //! the file it includes
-string readAndMacroProcess(PStream& in, map<string, string>& variables);
+string readAndMacroProcess(PStream& in, map<string, string>& variables, 
+                           bool skip_comments= true);
 
 /*! Given a filename, generates the standard PLearn variables FILEPATH,
   DIRPATH, FILENAME, FILEBASE, FILEEXT, DATE, TIME and DATETIME and

Modified: trunk/plearn/io/pl_log.cc
===================================================================
--- trunk/plearn/io/pl_log.cc	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/io/pl_log.cc	2007-04-05 01:35:37 UTC (rev 6834)
@@ -50,6 +50,7 @@
 #include <plearn/base/stringutils.h>
 #include <plearn/io/StringPStreamBuf.h>
 #include "pl_log.h"
+#include "ServerLogStreamBuf.h"
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn/io/pl_log.h
===================================================================
--- trunk/plearn/io/pl_log.h	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/io/pl_log.h	2007-04-05 01:35:37 UTC (rev 6834)
@@ -55,7 +55,6 @@
 
 // From Plearn
 #include "PStream.h"
-#include "ServerLogStreamBuf.h"
 
 namespace PLearn {
 

Modified: trunk/plearn/io/test/pytest.config
===================================================================
--- trunk/plearn/io/test/pytest.config	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/io/test/pytest.config	2007-04-05 01:35:37 UTC (rev 6834)
@@ -41,11 +41,12 @@
     3) Call 'which PRGNAME'
     4) Fail
     
-    Compilable program should provided the keyword argument 'compiler'
+    Compilable program should provide the keyword argument 'compiler'
     mapping to a string interpreted as the compiler name (e.g.
-    "compiler = 'pymake'"). Arguments to be forwarded to the compiler can be
-    provided as a string through the 'compile_options' keyword argument.
-      @type(program):
+    "compiler = 'pymake'"). If no compiler is provided while the program is
+    believed to be compilable, 'pymake' will be assigned by
+    default. Arguments to be forwarded to the compiler can be provided as a
+    string through the 'compile_options' keyword argument. @type program:
     Program
     
       @ivar(arguments):
@@ -78,6 +79,13 @@
       - A Program (see 'program' option) instance
       - None: if you are sure no files are to be compared.
     
+      @ivar(ignored_files_re):
+    Default behaviour of a test is to compare all
+    files created by running the test. In some case, one may prefer some of
+    these files to be ignored.
+      @type(ignored_files_re):
+    list of regular expressions
+    
       @ivar(disabled):
     If true, the test will not be ran.
       @type(disabled):
@@ -91,10 +99,9 @@
     category = "General",
     program = Program(
         name = "plearn_tests",
-        compiler = "pymake",
-        compile_options = ""
+        compiler = "pymake"
         ),
-    arguments = 'PLEARNDIR:scripts/command_line_object.plearn "object=PLLogTest(save = 0)"',
+    arguments = "PLEARNDIR:scripts/command_line_object.plearn \"object=PLLogTest(save = 0)\"",
     resources = [ ],
     precision = 1e-06,
     pfileprg = "__program__",
@@ -107,10 +114,9 @@
     category = "General",
     program = Program(
         name = "plearn_tests",
-        compiler = "pymake",
-        compile_options = ""
+        compiler = "pymake"
         ),
-    arguments = 'PLEARNDIR:scripts/command_line_object.plearn "object=PStreamBufTest(save = 0)"',
+    arguments = "PLEARNDIR:scripts/command_line_object.plearn \"object=PStreamBufTest(save = 0)\"",
     resources = [ ],
     precision = 1e-06,
     pfileprg = "__program__",
@@ -123,10 +129,9 @@
     category = "General",
     program = Program(
         name = "plearn_tests",
-        compiler = "pymake",
-        compile_options = ""
+        compiler = "pymake"
         ),
-    arguments = 'PLEARNDIR:scripts/command_line_object.plearn "object=PPathTest(save = 0)"',
+    arguments = "PLEARNDIR:scripts/command_line_object.plearn \"object=PPathTest(save = 0)\"",
     resources = [ ],
     precision = 1e-06,
     pfileprg = "__program__",
@@ -139,10 +144,9 @@
     category = "General",
     program = Program(
         name = "plearn_tests",
-        compiler = "pymake",
-        compile_options = ""
+        compiler = "pymake"
         ),
-    arguments = 'PLEARNDIR:scripts/command_line_object.plearn "object=TupleTest(save = 0)"',
+    arguments = "PLEARNDIR:scripts/command_line_object.plearn \"object=TupleTest(save = 0)\"",
     resources = [ ],
     precision = 1e-06,
     pfileprg = "__program__",

Modified: trunk/plearn/math/TMat_decl.h
===================================================================
--- trunk/plearn/math/TMat_decl.h	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/math/TMat_decl.h	2007-04-05 01:35:37 UTC (rev 6834)
@@ -633,7 +633,8 @@
     {
 #ifdef BOUNDCHECK
         if(rowstart<0 || newlength<0 || rowstart+newlength>length())
-            PLERROR("TMat::subMatRows(int rowstart, int newlength) OUT OF BOUNDS");
+            PLERROR("TMat::subMatRows(int rowstart, int newlength) OUT OF BOUNDS"
+                    "length=%d, rowstart=%d, newlength=%d", length(), rowstart, newlength);
 #endif
         TMat<T> subm = *this;
         subm.length_ = newlength;

Modified: trunk/plearn/math/TMat_maths_impl.h
===================================================================
--- trunk/plearn/math/TMat_maths_impl.h	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/math/TMat_maths_impl.h	2007-04-05 01:35:37 UTC (rev 6834)
@@ -1126,6 +1126,7 @@
             o[i] = - v[i];
         return opposite;
     }
+    return TVec<T>();
 }
 
 template<class T>

Modified: trunk/plearn/math/test/PentadiagonalSolveInPlace/pytest.config
===================================================================
--- trunk/plearn/math/test/PentadiagonalSolveInPlace/pytest.config	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/math/test/PentadiagonalSolveInPlace/pytest.config	2007-04-05 01:35:37 UTC (rev 6834)
@@ -41,11 +41,12 @@
     3) Call 'which PRGNAME'
     4) Fail
     
-    Compilable program should provided the keyword argument 'compiler'
+    Compilable program should provide the keyword argument 'compiler'
     mapping to a string interpreted as the compiler name (e.g.
-    "compiler = 'pymake'"). Arguments to be forwarded to the compiler can be
-    provided as a string through the 'compile_options' keyword argument.
-      @type(program):
+    "compiler = 'pymake'"). If no compiler is provided while the program is
+    believed to be compilable, 'pymake' will be assigned by
+    default. Arguments to be forwarded to the compiler can be provided as a
+    string through the 'compile_options' keyword argument. @type program:
     Program
     
       @ivar(arguments):
@@ -78,6 +79,13 @@
       - A Program (see 'program' option) instance
       - None: if you are sure no files are to be compared.
     
+      @ivar(ignored_files_re):
+    Default behaviour of a test is to compare all
+    files created by running the test. In some case, one may prefer some of
+    these files to be ignored.
+      @type(ignored_files_re):
+    list of regular expressions
+    
       @ivar(disabled):
     If true, the test will not be ran.
       @type(disabled):
@@ -90,10 +98,9 @@
     category = "General",
     program = Program(
         name = "plearn_tests",
-        compiler = "pymake",
-        compile_options = ""
+        compiler = "pymake"
         ),
-    arguments = 'PLEARNDIR:scripts/command_line_object.plearn "object=PentaTest(save = 0)"',
+    arguments = "PLEARNDIR:scripts/command_line_object.plearn \"object=PentaTest(save = 0)\"",
     resources = [ ],
     precision = 1e-06,
     pfileprg = "__program__",

Modified: trunk/plearn/math/test/TMat/pytest.config
===================================================================
--- trunk/plearn/math/test/TMat/pytest.config	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/math/test/TMat/pytest.config	2007-04-05 01:35:37 UTC (rev 6834)
@@ -41,11 +41,12 @@
     3) Call 'which PRGNAME'
     4) Fail
     
-    Compilable program should provided the keyword argument 'compiler'
+    Compilable program should provide the keyword argument 'compiler'
     mapping to a string interpreted as the compiler name (e.g.
-    "compiler = 'pymake'"). Arguments to be forwarded to the compiler can be
-    provided as a string through the 'compile_options' keyword argument.
-      @type(program):
+    "compiler = 'pymake'"). If no compiler is provided while the program is
+    believed to be compilable, 'pymake' will be assigned by
+    default. Arguments to be forwarded to the compiler can be provided as a
+    string through the 'compile_options' keyword argument. @type program:
     Program
     
       @ivar(arguments):
@@ -78,6 +79,13 @@
       - A Program (see 'program' option) instance
       - None: if you are sure no files are to be compared.
     
+      @ivar(ignored_files_re):
+    Default behaviour of a test is to compare all
+    files created by running the test. In some case, one may prefer some of
+    these files to be ignored.
+      @type(ignored_files_re):
+    list of regular expressions
+    
       @ivar(disabled):
     If true, the test will not be ran.
       @type(disabled):
@@ -90,8 +98,7 @@
     category = "General",
     program = Program(
         name = "plearn_tests",
-        compiler = "pymake",
-        compile_options = ""
+        compiler = "pymake"
         ),
     arguments = "tmat_test.pyplearn",
     resources = [ "tmat_test.pyplearn" ],

Modified: trunk/plearn/math/test/VecStatsCollector/pytest.config
===================================================================
--- trunk/plearn/math/test/VecStatsCollector/pytest.config	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/math/test/VecStatsCollector/pytest.config	2007-04-05 01:35:37 UTC (rev 6834)
@@ -100,7 +100,7 @@
         name = "plearn_tests",
         compiler = "pymake"
         ),
-    arguments = 'PLEARNDIR:scripts/command_line_object.plearn object="RemoveObservationTest(save=0)"',
+    arguments = "PLEARNDIR:scripts/command_line_object.plearn object=\"RemoveObservationTest(save=0)\"",
     resources = [ ],
     precision = 1e-06,
     pfileprg = "__program__",
@@ -115,8 +115,7 @@
     category = "General",
     program = Program(
         name = "plearn_tests",
-        compiler = "pymake",
-        compile_options = ""
+        compiler = "pymake"
         ),
     arguments = "constant_regressor.pyplearn",
     resources = [ "constant_regressor.pyplearn" ],

Modified: trunk/plearn/math/test/pl_math/pytest.config
===================================================================
--- trunk/plearn/math/test/pl_math/pytest.config	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/math/test/pl_math/pytest.config	2007-04-05 01:35:37 UTC (rev 6834)
@@ -41,11 +41,12 @@
     3) Call 'which PRGNAME'
     4) Fail
     
-    Compilable program should provided the keyword argument 'compiler'
+    Compilable program should provide the keyword argument 'compiler'
     mapping to a string interpreted as the compiler name (e.g.
-    "compiler = 'pymake'"). Arguments to be forwarded to the compiler can be
-    provided as a string through the 'compile_options' keyword argument.
-      @type(program):
+    "compiler = 'pymake'"). If no compiler is provided while the program is
+    believed to be compilable, 'pymake' will be assigned by
+    default. Arguments to be forwarded to the compiler can be provided as a
+    string through the 'compile_options' keyword argument. @type program:
     Program
     
       @ivar(arguments):
@@ -78,6 +79,13 @@
       - A Program (see 'program' option) instance
       - None: if you are sure no files are to be compared.
     
+      @ivar(ignored_files_re):
+    Default behaviour of a test is to compare all
+    files created by running the test. In some case, one may prefer some of
+    these files to be ignored.
+      @type(ignored_files_re):
+    list of regular expressions
+    
       @ivar(disabled):
     If true, the test will not be ran.
       @type(disabled):
@@ -90,10 +98,9 @@
     category = "General",
     program = Program(
         name = "plearn_tests",
-        compiler = "pymake",
-        compile_options = ""
+        compiler = "pymake"
         ),
-    arguments = 'PLEARNDIR:scripts/command_line_object.plearn "object=PLMathTest()"',
+    arguments = "PLEARNDIR:scripts/command_line_object.plearn \"object=PLMathTest()\"",
     resources = [ ],
     precision = 1e-06,
     pfileprg = "__program__",

Modified: trunk/plearn/misc/PLearnServer.cc
===================================================================
--- trunk/plearn/misc/PLearnServer.cc	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/misc/PLearnServer.cc	2007-04-05 01:35:37 UTC (rev 6834)
@@ -51,8 +51,8 @@
 #include <plearn/io/load_and_save.h>
 #include <plearn/io/pl_log.h>
 #include <plearn/math/random.h>
-
 #include <plearn/io/PyPLearnScript.h> // For smartLoadObject
+#include <plearn/io/ServerLogStreamBuf.h> 
 
 namespace PLearn {
 using namespace std;
@@ -102,11 +102,16 @@
     getInstance()->io.implicit_storage = impl_stor;
 }
 
-void PLearnServer::setVerbosity(int verbosity)
+void PLearnServer::loggingControl(int vlevel, TVec<string> modules)
 {
-    PL_Log::instance().verbosity(verbosity);
+    PL_Log::instance().verbosity(vlevel);
+    PL_Log::instance().enableNamedLogging(modules);
 }
 
+void PLearnServer::setOptionLevel(const OptionBase::OptionLevel& level)
+{
+    OptionBase::setCurrentOptionLevel(level);
+}
 
 BEGIN_DECLARE_REMOTE_FUNCTIONS
 
@@ -124,10 +129,15 @@
                 (BodyDoc("change the implicit_storage mode of the io of the PLearnServer instance.\n"),
                  ArgDoc ("impl_stor", "Whether or not to use implicit_storage")));
 
-declareFunction("setVerbosity", &PLearnServer::setVerbosity,
-                (BodyDoc("change the verbosity for logs of the PLearnServer instance.\n"),
-                 ArgDoc ("verbosity", "verbosity level")));
+declareFunction("loggingControl", &PLearnServer::loggingControl,
+                (BodyDoc("Set current logging level and modules.\n"),
+                 ArgDoc ("vlevel","the verbosity level"),
+                 ArgDoc ("modules","list of modules names to log")));
 
+declareFunction("setOptionLevel", &PLearnServer::setOptionLevel,
+                (BodyDoc("Set current option level.\n"),
+                 ArgDoc ("level","option level")));
+
 END_DECLARE_REMOTE_FUNCTIONS
 
 
@@ -200,6 +210,7 @@
     io << endl;
 }
 
+
 bool PLearnServer::run()
 {
     int obj_id;
@@ -219,8 +230,8 @@
     // forward pout&perr to client
     PStream orig_pout= pout;
     PStream orig_perr= perr;
-    pout= new ServerLogStreamBuf(io, "pout");
-    perr= new ServerLogStreamBuf(io, "perr");
+    pout= new ServerLogStreamBuf(io, "pout", VLEVEL_NORMAL);
+    perr= new ServerLogStreamBuf(io, "perr", VLEVEL_NORMAL);
 
     DBG_LOG << "ENTERING PLearnServer::run()" << endl;
 
@@ -315,6 +326,7 @@
                 DBG_LOG << "  ojbj_id = " << obj_id << endl;
                 found = objmap.find(obj_id);
                 DBG_LOG << "objmap= " << objmap << endl;
+                DBG_LOG << "cmo= " << io.copies_map_out << endl;
                 if(found == objmap.end()) // unexistant obj_id
                     PLERROR("Calling a method on a non-existing object");
                 else 

Modified: trunk/plearn/misc/PLearnServer.h
===================================================================
--- trunk/plearn/misc/PLearnServer.h	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/misc/PLearnServer.h	2007-04-05 01:35:37 UTC (rev 6834)
@@ -98,8 +98,12 @@
     //! change the implicit_storage mode of the io of the PLearnServer instance.
     static void implicit_storage(bool impl_stor);
 
-    static void setVerbosity(int verbosity);
+    //! Set current logging level and modules to log
+    static void loggingControl(int vlevel, TVec<string> modules);
 
+    //! Set current option level
+    static void setOptionLevel(const OptionBase::OptionLevel& level);
+
 };
 
 

Modified: trunk/plearn/misc/PLearnService.cc
===================================================================
--- trunk/plearn/misc/PLearnService.cc	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/misc/PLearnService.cc	2007-04-05 01:35:37 UTC (rev 6834)
@@ -127,7 +127,9 @@
 
         reserved_servers.insert(serv);
         serv->getResults();
-        serv->callFunction("setVerbosity", PL_Log::instance().verbosity());
+        serv->callFunction("loggingControl", 
+                           PL_Log::instance().verbosity(),
+                           PL_Log::instance().namedLogging());
         serv->getResults();
         reserved_servers.erase(serv);
         available_servers.push(serv);
@@ -375,7 +377,7 @@
 PLearnService::~PLearnService()
 {
     if(reserved_servers.size() != 0)
-        PLERROR("PLearnService::~PLearnService : some servers are still reserved; free them first.");
+        PLWARNING("PLearnService::~PLearnService : some servers are still reserved; free them first.");
 
     TVec<PP<RemotePLearnServer> > servers(available_servers.length());
     servers << available_servers;

Modified: trunk/plearn/misc/PLearnService.h
===================================================================
--- trunk/plearn/misc/PLearnService.h	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/misc/PLearnService.h	2007-04-05 01:35:37 UTC (rev 6834)
@@ -133,6 +133,8 @@
                            log_callback_t the_log_callback = log_callback,
                            progress_callback_t the_progress_callback = progress_callback);
     
+    static pair<string, int> getId(RemotePLearnServer* server) { return servers_ids[server]; }
+
     ~PLearnService();
 };
 

Modified: trunk/plearn/misc/RemotePLearnServer.cc
===================================================================
--- trunk/plearn/misc/RemotePLearnServer.cc	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/misc/RemotePLearnServer.cc	2007-04-05 01:35:37 UTC (rev 6834)
@@ -204,16 +204,6 @@
 {
     deleteAllObjectsAsync();
     getResults();
-/*
-    if(io)
-    {
-        io.write("!Z "); 
-        io << endl;
-        expectResults(0);
-    }
-    else
-        DBG_LOG << "in RemotePLearnServer::deleteAllObjects() : stream not good." << endl;
-*/
 }
 
 void RemotePLearnServer::deleteAllObjectsAsync()
@@ -252,7 +242,11 @@
         break;
     case 'E':
         io >> msg;
-        PLERROR(msg.c_str());
+        {
+            pair<string, int> id= PLearnService::getId(this);
+            PLERROR("From server %s %d : %s", id.first.c_str(), id.second, 
+                    msg.c_str());
+        }
         break;
     default:
         PLERROR("RemotePLearnServer: expected R (return command), but read %c ????",command);

Modified: trunk/plearn/misc/test/pytest.config
===================================================================
--- trunk/plearn/misc/test/pytest.config	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/misc/test/pytest.config	2007-04-05 01:35:37 UTC (rev 6834)
@@ -41,11 +41,12 @@
     3) Call 'which PRGNAME'
     4) Fail
     
-    Compilable program should provided the keyword argument 'compiler'
+    Compilable program should provide the keyword argument 'compiler'
     mapping to a string interpreted as the compiler name (e.g.
-    "compiler = 'pymake'"). Arguments to be forwarded to the compiler can be
-    provided as a string through the 'compile_options' keyword argument.
-      @type(program):
+    "compiler = 'pymake'"). If no compiler is provided while the program is
+    believed to be compilable, 'pymake' will be assigned by
+    default. Arguments to be forwarded to the compiler can be provided as a
+    string through the 'compile_options' keyword argument. @type program:
     Program
     
       @ivar(arguments):
@@ -78,6 +79,13 @@
       - A Program (see 'program' option) instance
       - None: if you are sure no files are to be compared.
     
+      @ivar(ignored_files_re):
+    Default behaviour of a test is to compare all
+    files created by running the test. In some case, one may prefer some of
+    these files to be ignored.
+      @type(ignored_files_re):
+    list of regular expressions
+    
       @ivar(disabled):
     If true, the test will not be ran.
       @type(disabled):
@@ -90,10 +98,9 @@
     category = "General",
     program = Program(
         name = "plearn_tests",
-        compiler = "pymake",
-        compile_options = ""
+        compiler = "pymake"
         ),
-    arguments = 'PLEARNDIR:scripts/command_line_object.plearn "object=HeapTest(save = 0)"',
+    arguments = "PLEARNDIR:scripts/command_line_object.plearn \"object=HeapTest(save = 0)\"",
     resources = [ ],
     precision = 1e-06,
     pfileprg = "__program__",

Modified: trunk/plearn/misc/vmatmain.cc
===================================================================
--- trunk/plearn/misc/vmatmain.cc	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/misc/vmatmain.cc	2007-04-05 01:35:37 UTC (rev 6834)
@@ -56,6 +56,7 @@
 #include <plearn/db/getDataSet.h>
 #include <plearn/display/Gnuplot.h>
 #include <plearn/io/openFile.h>
+#include <plearn/base/HelpSystem.h>
 
 namespace PLearn {
 using namespace std;
@@ -490,8 +491,9 @@
             "all matrix file formats. Type 'vmat help dataset' to see what other\n"
             "<dataset> strings are available." << endl;
 #endif
-
-        PLearnCommandRegistry::help("vmat", cout);
+        
+        //PLearnCommandRegistry::help("vmat", cout);
+        pout << HelpSystem::helpOnCommand("vmat") << flush;
         exit(0);
     }
 

Modified: trunk/plearn/opt/test/pytest.config
===================================================================
--- trunk/plearn/opt/test/pytest.config	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/opt/test/pytest.config	2007-04-05 01:35:37 UTC (rev 6834)
@@ -41,11 +41,12 @@
     3) Call 'which PRGNAME'
     4) Fail
     
-    Compilable program should provided the keyword argument 'compiler'
+    Compilable program should provide the keyword argument 'compiler'
     mapping to a string interpreted as the compiler name (e.g.
-    "compiler = 'pymake'"). Arguments to be forwarded to the compiler can be
-    provided as a string through the 'compile_options' keyword argument.
-      @type(program):
+    "compiler = 'pymake'"). If no compiler is provided while the program is
+    believed to be compilable, 'pymake' will be assigned by
+    default. Arguments to be forwarded to the compiler can be provided as a
+    string through the 'compile_options' keyword argument. @type program:
     Program
     
       @ivar(arguments):
@@ -78,6 +79,13 @@
       - A Program (see 'program' option) instance
       - None: if you are sure no files are to be compared.
     
+      @ivar(ignored_files_re):
+    Default behaviour of a test is to compare all
+    files created by running the test. In some case, one may prefer some of
+    these files to be ignored.
+      @type(ignored_files_re):
+    list of regular expressions
+    
       @ivar(disabled):
     If true, the test will not be ran.
       @type(disabled):
@@ -90,8 +98,7 @@
     category = "General",
     program = Program(
         name = "plearn_tests",
-        compiler = "pymake",
-        compile_options = ""
+        compiler = "pymake"
         ),
     arguments = "--enable-logging ConjGradientOptimizer conjgradientoptimizer_test.plearn",
     resources = [ "conjgradientoptimizer_test.plearn" ],
@@ -106,17 +113,9 @@
     category = "General",
     program = Program(
         name = "plearn_tests",
-        compiler = "pymake",
-        compile_options = ""
+        compiler = "pymake"
         ),
-    arguments = 'PLEARNDIR:scripts/command_line_object.plearn ' +\
-                '"object=ConjRosenbrock('                       +\
-                '    D=2,opt='                                  +\
-                '    ConjGradientOptimizer(nstages     = 12,'   +\
-                '                          sigma       = 0.1,'  +\
-                '                          rho         = 0.05,' +\
-                '                          slope_ratio = 10,'   +\
-                '                          verbosity   = 1))"'  ,
+    arguments = "PLEARNDIR:scripts/command_line_object.plearn \"object=ConjRosenbrock(    D=2,opt=    ConjGradientOptimizer(nstages     = 12,                          sigma       = 0.1,                          rho         = 0.05,                          slope_ratio = 10,                          verbosity   = 1))\"",
     resources = [ ],
     precision = 1e-06,
     pfileprg = "__program__",
@@ -129,20 +128,11 @@
     category = "General",
     program = Program(
         name = "plearn_tests",
-        compiler = "pymake",
-        compile_options = ""
+        compiler = "pymake"
         ),
-    arguments = 'PLEARNDIR:scripts/command_line_object.plearn ' +\
-                '"object=ConjRosenbrock('                       +\
-                '    D=100,opt='                                +\
-                '    ConjGradientOptimizer(nstages     = 12,'   +\
-                '                          sigma       = 0.1,'  +\
-                '                          rho         = 0.05,' +\
-                '                          slope_ratio = 10,'   +\
-                '                          verbosity   = 1))"'  ,
+    arguments = "PLEARNDIR:scripts/command_line_object.plearn \"object=ConjRosenbrock(    D=100,opt=    ConjGradientOptimizer(nstages     = 12,                          sigma       = 0.1,                          rho         = 0.05,                          slope_ratio = 10,                          verbosity   = 1))\"",
     resources = [ ],
     precision = 1e-06,
     pfileprg = "__program__",
     disabled = False
     )
-

Modified: trunk/plearn/python/PythonCodeSnippet.cc
===================================================================
--- trunk/plearn/python/PythonCodeSnippet.cc	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/python/PythonCodeSnippet.cc	2007-04-05 01:35:37 UTC (rev 6834)
@@ -473,6 +473,40 @@
     PythonGlobalInterpreterLock gil;         // For thread-safety
     if (PyErr_Occurred()) {
         if (m_remap_python_exceptions) {
+
+            // format using cgitb, throw as PythonError (PLearnError)
+            PyObject *exception, *v, *traceback;
+            PyErr_Fetch(&exception, &v, &traceback);
+            PyErr_NormalizeException(&exception, &v, &traceback);
+            
+            //PyObject* tbstr= PyString_FromString("cgitb");
+            PyObject* tbstr= PyString_FromString("plearn.utilities.pltraceback");
+            PyObject* tbmod= PyImport_Import(tbstr);
+            Py_XDECREF(tbstr);
+            if(!tbmod)
+                throw PythonException("PythonCodeSnippet::handlePythonErrors : "
+                                      "Unable to import cgitb module.");
+            PyObject* tbdict= PyModule_GetDict(tbmod);
+            Py_XDECREF(tbmod);
+            PyObject* formatFunc= PyDict_GetItemString(tbdict, "text");
+            if(!formatFunc)
+                throw PythonException("PythonCodeSnippet::handlePythonErrors : "
+                                      "Can't find cgitb.text");
+            PyObject* args= Py_BuildValue("((OOO))", exception, v, traceback);
+            if(!args)
+                throw PythonException("PythonCodeSnippet::handlePythonErrors : "
+                                      "Can't build args for cgitb.text");
+            PyObject* pystr= PyObject_CallObject(formatFunc, args);
+            Py_XDECREF(args);
+            if(!pystr)
+                throw PythonException("PythonCodeSnippet::handlePythonErrors : "
+                                      "call to cgitb.text failed");
+            string str= PyString_AsString(pystr);
+            Py_XDECREF(pystr);
+            
+            throw PythonException(str);
+
+/*
             PyObject *ptype = 0, *pvalue = 0, *ptraceback = 0;
             PyErr_Fetch(&ptype, &pvalue, &ptraceback);
 
@@ -491,8 +525,14 @@
                 msg += string("\nException Type: ") + PyString_AsString(ptype_str);
             if (pvalue_str)
                 msg += string("\nException Value: ") + PyString_AsString(pvalue_str);
+            char* ptraceback_as_str= 0;
             if (ptraceback_str)
-                msg += string("\nTraceback: ") + PyString_AsString(ptraceback_str);
+            {
+                ptraceback_as_str= PyTraceback_AsString(ptraceback_str);
+                //msg += string("\nTraceback: ") + PyString_AsString(ptraceback_str);
+                msg += string("\nTraceback: ") + ptraceback_as_str;
+                PyMem_Free(ptraceback_as_str);
+            }
 
             Py_XDECREF(ptype);
             Py_XDECREF(pvalue);
@@ -502,6 +542,7 @@
             Py_XDECREF(ptraceback_str);
 
             throw PythonException(msg);
+*/
         }
         else {
             PyErr_Print();

Modified: trunk/plearn/python/test/.pytest/EmbeddedPython_InterfunctionXchg/expected_results/RUN.log
===================================================================
--- trunk/plearn/python/test/.pytest/EmbeddedPython_InterfunctionXchg/expected_results/RUN.log	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/python/test/.pytest/EmbeddedPython_InterfunctionXchg/expected_results/RUN.log	2007-04-05 01:35:37 UTC (rev 6834)
@@ -16,10 +16,31 @@
 Setting the string:   'This string should survive within the Python environment'
 Read back the string: 'This string should survive within the Python environment'
 Trying to read back from second snippet:
-Caught Python Exception: 'Encountered Python Exception
-Exception Type: exceptions.NameError
-Exception Value: global name 'buf' is not defined
-Traceback: <traceback object at 0x[memory_address]>'
+Caught Python Exception: 'NameError
+Python 2.3.5: /usr/bin/python
+
+
+A problem occurred in a Python script.  Here is the sequence of
+function calls leading up to the error, in the order they occurred.
+
+ <string> in get_value()
+
+NameError: global name 'buf' is not defined
+    __doc__ = 'Name not found globally.'
+    __getitem__ = <bound method NameError.__getitem__ of <exceptions.NameError instance>>
+    __init__ = <bound method NameError.__init__ of <exceptions.NameError instance>>
+    __module__ = 'exceptions'
+    __str__ = <bound method NameError.__str__ of <exceptions.NameError instance>>
+    args = ("global name 'buf' is not defined",)
+
+The above is a description of an error in a Python program.  Here is
+the original traceback:
+
+Traceback (most recent call last):
+  File "<string>", line 12, in get_value
+NameError: global name 'buf' is not defined
+
+'
 Associated 'some_global_map' with: {Oui: 16, bon: 512, est: 64, et: 256, il: 32, juste: 128}
 Read back from Python environment: {Oui: 16, bon: 512, est: 64, et: 256, il: 32, juste: 128}
 Printing some_global_map within Python: {'Oui': 16L, 'est': 64L, 'juste': 128L, 'il': 32L, 'bon': 512L, 'et': 256L}

Modified: trunk/plearn/python/test/pytest.config
===================================================================
--- trunk/plearn/python/test/pytest.config	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/python/test/pytest.config	2007-04-05 01:35:37 UTC (rev 6834)
@@ -41,11 +41,12 @@
     3) Call 'which PRGNAME'
     4) Fail
     
-    Compilable program should provided the keyword argument 'compiler'
+    Compilable program should provide the keyword argument 'compiler'
     mapping to a string interpreted as the compiler name (e.g.
-    "compiler = 'pymake'"). Arguments to be forwarded to the compiler can be
-    provided as a string through the 'compile_options' keyword argument.
-      @type(program):
+    "compiler = 'pymake'"). If no compiler is provided while the program is
+    believed to be compilable, 'pymake' will be assigned by
+    default. Arguments to be forwarded to the compiler can be provided as a
+    string through the 'compile_options' keyword argument. @type program:
     Program
     
       @ivar(arguments):
@@ -78,6 +79,13 @@
       - A Program (see 'program' option) instance
       - None: if you are sure no files are to be compared.
     
+      @ivar(ignored_files_re):
+    Default behaviour of a test is to compare all
+    files created by running the test. In some case, one may prefer some of
+    these files to be ignored.
+      @type(ignored_files_re):
+    list of regular expressions
+    
       @ivar(disabled):
     If true, the test will not be ran.
       @type(disabled):
@@ -90,10 +98,9 @@
     category = "General",
     program = Program(
         name = "plearn_tests",
-        compiler = "pymake",
-        compile_options = ""
+        compiler = "pymake"
         ),
-    arguments = 'PLEARNDIR:scripts/command_line_object.plearn "object=BasicIdentityCallsTest(save = 0)"',
+    arguments = "PLEARNDIR:scripts/command_line_object.plearn \"object=BasicIdentityCallsTest(save = 0)\"",
     resources = [ ],
     precision = 1e-06,
     pfileprg = "__program__",
@@ -106,10 +113,9 @@
     category = "General",
     program = Program(
         name = "plearn_tests",
-        compiler = "pymake",
-        compile_options = ""
+        compiler = "pymake"
         ),
-    arguments = 'PLEARNDIR:scripts/command_line_object.plearn "object=InterfunctionXchgTest(save = 0)"',
+    arguments = "PLEARNDIR:scripts/command_line_object.plearn \"object=InterfunctionXchgTest(save = 0)\"",
     resources = [ ],
     precision = 1e-06,
     pfileprg = "__program__",
@@ -122,10 +128,9 @@
     category = "General",
     program = Program(
         name = "plearn_tests",
-        compiler = "pymake",
-        compile_options = ""
+        compiler = "pymake"
         ),
-    arguments = 'PLEARNDIR:scripts/command_line_object.plearn "object=MemoryStressTest(save = 0)"',
+    arguments = "PLEARNDIR:scripts/command_line_object.plearn \"object=MemoryStressTest(save = 0)\"",
     resources = [ ],
     precision = 1e-06,
     pfileprg = "__program__",
@@ -153,10 +158,9 @@
     category = "General",
     program = Program(
         name = "plearn_tests",
-        compiler = "pymake",
-        compile_options = ""
+        compiler = "pymake"
         ),
-    arguments = 'PLEARNDIR:scripts/command_line_object.plearn "object=InjectionTest(save = 0)"',
+    arguments = "PLEARNDIR:scripts/command_line_object.plearn \"object=InjectionTest(save = 0)\"",
     resources = [ ],
     precision = 1e-06,
     pfileprg = "__program__",
@@ -169,11 +173,10 @@
     category = "General",
     program = Program(
         name = "plearn_tests",
-        compiler = "pymake",
-        compile_options = ""
+        compiler = "pymake"
         ),
-    arguments = 'vmat cat test_python_vmatrix.pymat',
-    resources = [ 'test_python_vmatrix.pymat' ],
+    arguments = "vmat cat test_python_vmatrix.pymat",
+    resources = [ "test_python_vmatrix.pymat" ],
     precision = 1e-06,
     pfileprg = "__program__",
     disabled = False

Modified: trunk/plearn/var/test/pytest.config
===================================================================
--- trunk/plearn/var/test/pytest.config	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/var/test/pytest.config	2007-04-05 01:35:37 UTC (rev 6834)
@@ -41,11 +41,12 @@
     3) Call 'which PRGNAME'
     4) Fail
     
-    Compilable program should provided the keyword argument 'compiler'
+    Compilable program should provide the keyword argument 'compiler'
     mapping to a string interpreted as the compiler name (e.g.
-    "compiler = 'pymake'"). Arguments to be forwarded to the compiler can be
-    provided as a string through the 'compile_options' keyword argument.
-      @type(program):
+    "compiler = 'pymake'"). If no compiler is provided while the program is
+    believed to be compilable, 'pymake' will be assigned by
+    default. Arguments to be forwarded to the compiler can be provided as a
+    string through the 'compile_options' keyword argument. @type program:
     Program
     
       @ivar(arguments):
@@ -78,6 +79,13 @@
       - A Program (see 'program' option) instance
       - None: if you are sure no files are to be compared.
     
+      @ivar(ignored_files_re):
+    Default behaviour of a test is to compare all
+    files created by running the test. In some case, one may prefer some of
+    these files to be ignored.
+      @type(ignored_files_re):
+    list of regular expressions
+    
       @ivar(disabled):
     If true, the test will not be ran.
       @type(disabled):
@@ -90,10 +98,9 @@
     category = "General",
     program = Program(
         name = "plearn_tests",
-        compiler = "pymake",
-        compile_options = ""
+        compiler = "pymake"
         ),
-    arguments = 'PLEARNDIR:scripts/command_line_object.plearn "object=VarUtilsTest()"',
+    arguments = "PLEARNDIR:scripts/command_line_object.plearn \"object=VarUtilsTest()\"",
     resources = [ ],
     precision = 1e-06,
     pfileprg = "__program__",

Modified: trunk/plearn/vmat/BootstrapSplitter.cc
===================================================================
--- trunk/plearn/vmat/BootstrapSplitter.cc	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/vmat/BootstrapSplitter.cc	2007-04-05 01:35:37 UTC (rev 6834)
@@ -104,8 +104,8 @@
             // Note: indices in the bootstrapped sets are sorted, so that
             // access may be faster (e.g. when reading large data from disk).
             bootstrapped_sets(i,0) = 
-                new BootstrapVMatrix(dataset, frac, vmat_rgen, false,
-                                     allow_repetitions);
+                new BootstrapVMatrix(dataset,frac,vmat_rgen, 
+                                     false, allow_repetitions);
         }
     } else {
         bootstrapped_sets.resize(0,0);

Modified: trunk/plearn/vmat/EncodedVMatrix.cc
===================================================================
--- trunk/plearn/vmat/EncodedVMatrix.cc	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/vmat/EncodedVMatrix.cc	2007-04-05 01:35:37 UTC (rev 6834)
@@ -152,7 +152,6 @@
 /////////////////////////////////
 void EncodedVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    PLERROR("EncodedVMatrix::makeDeepCopyFromShallowCopy fully implemented?");
     inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(encodings, copies);
     deepCopyField(defaults, copies);

Modified: trunk/plearn/vmat/VMatLanguage.cc
===================================================================
--- trunk/plearn/vmat/VMatLanguage.cc	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/vmat/VMatLanguage.cc	2007-04-05 01:35:37 UTC (rev 6834)
@@ -48,6 +48,7 @@
 #include <plearn/io/fileutils.h>
 #include <plearn/io/openFile.h>
 #include <plearn/misc/Calendar.h>
+#include <plearn/math/pl_erf.h>
 
 namespace PLearn {
 using namespace std;
@@ -186,7 +187,8 @@
                         " _ sumabs         : v0 v1 v2 ... vn  -->  sum_i |vi|\n"
                         "                    (no pop, and starts from the beginning of the stack)\n"
                         " _ varproduct     : a0 a1 ... an n+1 b0 b1 ... bm m+1 ... num_vars -> res0 ..."
-                        "                    (product of encoded variables)"
+                        "                    (product of encoded variables)\n"
+                        " _ erf            : a     -->  erf(a)    ; the error function erf\n"
     );
 
 //////////////////
@@ -890,6 +892,7 @@
         opcodes["cos"]  = 63;   // a -> cos(a)
         opcodes["varproduct"] = 64; // a0 a1 ... an n+1 b0 b1 ... bm m+1 ... num_vars -> res0 ... (product of encoded variables)
         opcodes["thermometer"] = 65; // index nclasses -> thermometer encoding
+        opcodes["erf"] = 66;
     }
 }
 
@@ -1347,6 +1350,9 @@
             
             break;
         }        
+        case 66: // erf
+            pstack.push(pl_erf(pstack.pop()));
+            break;
         default:
             PLASSERT_MSG(false, "BUG IN VMatLanguage::run while running program: unexpected opcode: " +
                          tostring(op));

Modified: trunk/plearn/vmat/test/pytest.config
===================================================================
--- trunk/plearn/vmat/test/pytest.config	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn/vmat/test/pytest.config	2007-04-05 01:35:37 UTC (rev 6834)
@@ -197,9 +197,6 @@
     disabled = False
     )
 
-# This test will fail if the VMat specialization of the diff function
-# (currently found in <plearn/base/diff.h>) does not function properly.
-# In particular, this is currently the case under Visual C++.
 Test(
     name = "PL_diff_VMat",
     description = "Tests the diff function to compare VMat options.",

Modified: trunk/plearn_learners/classifiers/test/KNNClassifier/pytest.config
===================================================================
--- trunk/plearn_learners/classifiers/test/KNNClassifier/pytest.config	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn_learners/classifiers/test/KNNClassifier/pytest.config	2007-04-05 01:35:37 UTC (rev 6834)
@@ -41,11 +41,12 @@
     3) Call 'which PRGNAME'
     4) Fail
     
-    Compilable program should provided the keyword argument 'compiler'
+    Compilable program should provide the keyword argument 'compiler'
     mapping to a string interpreted as the compiler name (e.g.
-    "compiler = 'pymake'"). Arguments to be forwarded to the compiler can be
-    provided as a string through the 'compile_options' keyword argument.
-      @type(program):
+    "compiler = 'pymake'"). If no compiler is provided while the program is
+    believed to be compilable, 'pymake' will be assigned by
+    default. Arguments to be forwarded to the compiler can be provided as a
+    string through the 'compile_options' keyword argument. @type program:
     Program
     
       @ivar(arguments):
@@ -78,6 +79,13 @@
       - A Program (see 'program' option) instance
       - None: if you are sure no files are to be compared.
     
+      @ivar(ignored_files_re):
+    Default behaviour of a test is to compare all
+    files created by running the test. In some case, one may prefer some of
+    these files to be ignored.
+      @type(ignored_files_re):
+    list of regular expressions
+    
       @ivar(disabled):
     If true, the test will not be ran.
       @type(disabled):
@@ -90,8 +98,7 @@
     category = "General",
     program = Program(
         name = "plearn_tests",
-        compiler = "pymake",
-        compile_options = ""
+        compiler = "pymake"
         ),
     arguments = "knn_classifier.pyplearn kernel=GaussianKernel sigma=1 output=tester",
     resources = [ "knn_classifier.pyplearn" ],
@@ -106,8 +113,7 @@
     category = "General",
     program = Program(
         name = "plearn_tests",
-        compiler = "pymake",
-        compile_options = ""
+        compiler = "pymake"
         ),
     arguments = "knn_classifier.pyplearn kernel=GaussianKernel sigma=3 output=tester",
     resources = [ "knn_classifier.pyplearn" ],
@@ -122,8 +128,7 @@
     category = "General",
     program = Program(
         name = "plearn_tests",
-        compiler = "pymake",
-        compile_options = ""
+        compiler = "pymake"
         ),
     arguments = "knn_classifier.pyplearn kernel=Constant output=grapher",
     resources = [ "knn_classifier.pyplearn" ],

Modified: trunk/plearn_learners/distributions/test/pytest.config
===================================================================
--- trunk/plearn_learners/distributions/test/pytest.config	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn_learners/distributions/test/pytest.config	2007-04-05 01:35:37 UTC (rev 6834)
@@ -41,11 +41,12 @@
     3) Call 'which PRGNAME'
     4) Fail
     
-    Compilable program should provided the keyword argument 'compiler'
+    Compilable program should provide the keyword argument 'compiler'
     mapping to a string interpreted as the compiler name (e.g.
-    "compiler = 'pymake'"). Arguments to be forwarded to the compiler can be
-    provided as a string through the 'compile_options' keyword argument.
-      @type(program):
+    "compiler = 'pymake'"). If no compiler is provided while the program is
+    believed to be compilable, 'pymake' will be assigned by
+    default. Arguments to be forwarded to the compiler can be provided as a
+    string through the 'compile_options' keyword argument. @type program:
     Program
     
       @ivar(arguments):
@@ -78,6 +79,13 @@
       - A Program (see 'program' option) instance
       - None: if you are sure no files are to be compared.
     
+      @ivar(ignored_files_re):
+    Default behaviour of a test is to compare all
+    files created by running the test. In some case, one may prefer some of
+    these files to be ignored.
+      @type(ignored_files_re):
+    list of regular expressions
+    
       @ivar(disabled):
     If true, the test will not be ran.
       @type(disabled):
@@ -189,14 +197,6 @@
     disabled = False
     )
 
-
-# The following test fails at ApSTAT with a crash, even with Boost 1.33.1. Maybe a gcc-3.3 compiler bug being tickled?
-# Here is the backtrace:
-#
-# #0  0x0874cc4f in std::list<boost::detail::sep_<unsigned int, boost::property<boost::edge_bundle_t, PLearn::MissingFlag, boost::no_property> >, std::allocator<boost::detail::sep_<unsigned int, boost::property<boost::edge_bundle_t, PLearn::MissingFlag, boost::no_property> > > >::begin (this=0x9415c68) at stl_list.h:571
-# #1  0x08748199 in out_edges<boost::detail::adj_list_gen<boost::adjacency_list<boost::listS, boost::vecS, boost::directedS, PLearn::NoProperty, PLearn::MissingFlag, boost::no_property, boost::listS>, boost::vecS, boost::listS, boost::directedS, boost::property<boost::vertex_bundle_t, PLearn::NoProperty, boost::no_property>, boost::property<boost::edge_bundle_t, PLearn::MissingFlag, boost::no_property>, boost::no_property, boost::listS>::config, boost::directed_graph_helper<boost::detail::adj_list_gen<boost::adjacency_list<boost::listS, boost::vecS, boost::directedS, PLearn::NoProperty, PLearn::MissingFlag, boost::no_property, boost::listS>, boost::vecS, boost::listS, boost::directedS, boost::property<boost::vertex_bundle_t, PLearn::NoProperty, boost::no_property>, boost::property<boost::edge_bundle_t, PLearn::MissingFlag, boost::no_property>, boost::no_property, boost::listS>::config> > (u=4, g_=@0xbfbf1f40) at adjacency_list.hpp:1510
-# #2  0x08741ec9 in PLearn::GaussMix::train (this=0x942dd18)
-#     at /home/chrish/PLearn/plearn_learners/distributions/GaussMix.cc:2859
 Test(
     name = "PL_GaussMix_General_Missing",
     description = "Test training of mixtures of Gaussians of general type with missing values",

Modified: trunk/plearn_learners/generic/PLearner.cc
===================================================================
--- trunk/plearn_learners/generic/PLearner.cc	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn_learners/generic/PLearner.cc	2007-04-05 01:35:37 UTC (rev 6834)
@@ -810,14 +810,20 @@
         costs.fill(-1);
         test_stats->update(costs);
     }
+
+
+    PP<ProgressBar> pb;
+    if (report_progress) 
+        pb = new ProgressBar("Testing learner", len);
+
 #ifndef BUGGED_SERVER
     PLearnService& service(PLearnService::instance());
 
     //DUMMY: need to find a better way to calc. nservers -xsm
-    const int chunksize= 10000;//nb. rows in each chunk sent to a remote server
-    const int chunks_per_server= 10;//ideal nb. chunks per server
+    const int chunksize= 2500;//nb. rows in each chunk sent to a remote server
+    const int chunks_per_server= 3;//ideal nb. chunks per server
     int nservers= min(len/(chunks_per_server*chunksize), service.availableServers());
-
+    
     if(nservers > 1 && parallelize_here && !isStatefulLearner())
     {// parallel test
         CopiesMap copies;
@@ -830,9 +836,15 @@
         map<PP<RemotePLearnServer>, int> learners_ids;
         map<PP<RemotePLearnServer>, int> chunknums;
         map<int, PP<VecStatsCollector> > vscs;
+        map<PP<RemotePLearnServer>, int> chunkszs;
+        int rowsdone= 0;
 
+        bool rep_prog= report_progress;
+        const_cast<bool&>(report_progress)= false;//servers dont report progress
         for(int i= 0; i < nservers; ++i)
             servers[i]->newObjectAsync(*this);
+        const_cast<bool&>(report_progress)= rep_prog;
+
         while(nservers > 0)
         {
             PP<RemotePLearnServer> s= service.waitForResult();
@@ -845,6 +857,7 @@
                     s->getResults(id);
                     learners_ids[s]= id;
                     int clen= min(chunksize, testset.length()-curpos);
+                    chunkszs[s]= clen;
                     VMat sts= new RowsSubVMatrix(testset, curpos, clen);
                     if(master_sends_testset_rows)
                         sts= new MemoryVMatrix(sts.toMat());
@@ -864,6 +877,7 @@
                 {
                     /* step 4 (once per slave) */
                     s->getResults(); // learner deleted
+                    s->unlink(testset);
                     service.freeServer(s);
                     --nservers;
                 }
@@ -875,11 +889,15 @@
 
                 s->getResults(vsc, chunkout, chunkcosts);
 
+                rowsdone+= chunkszs[s];
+                if(report_progress) pb->update(rowsdone);
+
                 int chunknum= chunknums[s];
                 if(curpos < len) // more chunks to do, assign one to this server
                 {
                     /* step 2 (repeat as needed) */
                     int clen= min(chunksize, testset.length()-curpos);
+                    chunkszs[s]= clen;
                     VMat sts= new RowsSubVMatrix(testset, curpos, clen);
                     if(master_sends_testset_rows)
                         sts= new MemoryVMatrix(sts.toMat());
@@ -922,9 +940,7 @@
     else // Sequential test 
     {
 #endif
-        PP<ProgressBar> pb;
-        if (report_progress) 
-            pb = new ProgressBar("Testing learner", len);
+
         for (int i = 0; i < len; i++)
         {
             testset.getExample(i, input, target, weight);

Modified: trunk/plearn_learners/generic/VPLCombinedLearner.cc
===================================================================
--- trunk/plearn_learners/generic/VPLCombinedLearner.cc	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn_learners/generic/VPLCombinedLearner.cc	2007-04-05 01:35:37 UTC (rev 6834)
@@ -341,10 +341,8 @@
     output.resize(outputsize());
     costs.resize(nTestCosts());
 
-    int ilen = input.length();
-    int tlen = target.length();
-    PLASSERT(ilen==inputsize());
-    PLASSERT(tlen==targetsize());
+    PLASSERT(input.length()==inputsize());
+    PLASSERT(target.length()==targetsize());
 
     output.resize(outputsize());
     int nlearners = sublearners_.length();    

Modified: trunk/plearn_learners/generic/VPLPreprocessedLearner2.cc
===================================================================
--- trunk/plearn_learners/generic/VPLPreprocessedLearner2.cc	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn_learners/generic/VPLPreprocessedLearner2.cc	2007-04-05 01:35:37 UTC (rev 6834)
@@ -482,10 +482,8 @@
     }
 
     PLASSERT( learner_ );
-    int ilen = input.length();
-    int tlen = target.length();
-    PLASSERT(ilen==inputsize());
-    PLASSERT(tlen==targetsize());
+    PLASSERT(input.length()==inputsize());
+    PLASSERT(target.length()==targetsize());
 
     Vec newinput = input;
     if(!input_prg.empty())//input_prg_)

Modified: trunk/plearn_learners/generic/test/SimpleNet/pytest.config
===================================================================
--- trunk/plearn_learners/generic/test/SimpleNet/pytest.config	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn_learners/generic/test/SimpleNet/pytest.config	2007-04-05 01:35:37 UTC (rev 6834)
@@ -41,11 +41,12 @@
     3) Call 'which PRGNAME'
     4) Fail
     
-    Compilable program should provided the keyword argument 'compiler'
+    Compilable program should provide the keyword argument 'compiler'
     mapping to a string interpreted as the compiler name (e.g.
-    "compiler = 'pymake'"). Arguments to be forwarded to the compiler can be
-    provided as a string through the 'compile_options' keyword argument.
-      @type(program):
+    "compiler = 'pymake'"). If no compiler is provided while the program is
+    believed to be compilable, 'pymake' will be assigned by
+    default. Arguments to be forwarded to the compiler can be provided as a
+    string through the 'compile_options' keyword argument. @type program:
     Program
     
       @ivar(arguments):
@@ -78,6 +79,13 @@
       - A Program (see 'program' option) instance
       - None: if you are sure no files are to be compared.
     
+      @ivar(ignored_files_re):
+    Default behaviour of a test is to compare all
+    files created by running the test. In some case, one may prefer some of
+    these files to be ignored.
+      @type(ignored_files_re):
+    list of regular expressions
+    
       @ivar(disabled):
     If true, the test will not be ran.
       @type(disabled):

Modified: trunk/plearn_learners/meta/BaggingLearner.cc
===================================================================
--- trunk/plearn_learners/meta/BaggingLearner.cc	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn_learners/meta/BaggingLearner.cc	2007-04-05 01:35:37 UTC (rev 6834)
@@ -38,6 +38,11 @@
 
 
 #include "BaggingLearner.h"
+#include <plearn/base/tostring.h>
+#include <plearn/base/ProgressBar.h>
+#include <plearn/misc/PLearnService.h>
+#include <plearn/misc/RemotePLearnServer.h>
+#include <plearn/vmat/MemoryVMatrix.h>
 
 namespace PLearn {
 using namespace std;
@@ -49,10 +54,14 @@
 
 BaggingLearner::BaggingLearner(PP<Splitter> splitter_, 
                                PP<PLearner> template_learner_,
-                               char reduce_func_)
+                               TVec<string> stats_,
+                               int exclude_extremes_,
+                               bool output_sub_outputs_)
     :splitter(splitter_),
      template_learner(template_learner_),
-     reduce_func(reduce_func_)
+     stats(stats_),
+     exclude_extremes(exclude_extremes_),
+     output_sub_outputs(output_sub_outputs_)
 {
 }
 
@@ -68,16 +77,24 @@
                   "Template for all sub-learners; deep-copied once for each bag",
                   "", OptionBase::basic_level);
 
-    declareOption(ol, "reduce_func", &BaggingLearner::reduce_func,
+    declareOption(ol, "stats", &BaggingLearner::stats,
                   OptionBase::buildoption,
-                  "Function used to combine outputs from all learners.\n"
+                  "Functions used to combine outputs from all learners.\n"
                   "\t- 'A' = Average\n",
                   "", OptionBase::basic_level);
 
+    declareOption(ol, "exclude_extremes", &BaggingLearner::exclude_extremes,
+                  OptionBase::buildoption,
+                  "If >0, sub-learners outputs are sorted and the exclude_extremes "
+                  "highest and lowest are excluded");
+                  
+    declareOption(ol, "output_sub_outputs", &BaggingLearner::output_sub_outputs,
+                  OptionBase::buildoption,
+                  "Wether computeOutput should append sub-learners outputs to output.");
+                  
     declareOption(ol, "learners", &BaggingLearner::learners,
                   OptionBase::learntoption,
-                  "Trained sub-learners",
-                  "", OptionBase::basic_level);
+                  "Trained sub-learners");
 
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
@@ -107,8 +124,13 @@
 }
 
 int BaggingLearner::outputsize() const
-{
-    return template_learner->outputsize();
+{ 
+    PLASSERT(template_learner);
+    PLASSERT(splitter);
+    int sz= template_learner->outputsize() * stats.length(); 
+    if(output_sub_outputs)
+        sz+= template_learner->outputsize() * splitter->nsplits();
+    return sz;
 }
 
 void BaggingLearner::forget()
@@ -145,15 +167,94 @@
         {
             CopiesMap c;
             learners[i]= template_learner->deepCopy(c);
+            learners[i]->report_progress= false;
         }
     }
 
+    PP<ProgressBar> pb= 0;
+    if(report_progress)
+        pb= new ProgressBar("BaggingLearner::train",nbags);
+
+    PLearnService& service(PLearnService::instance());
+    int nservers= min(nbags, service.availableServers());
+
+    if(nservers > 1 && parallelize_here)//parallel train
+    {
+        TVec<PP<RemotePLearnServer> > servers= service.reserveServers(nservers);
+        nservers= servers.length();
+
+        map<PP<RemotePLearnServer>, int> learners_ids;
+        map<PP<RemotePLearnServer>, int> bagnums;
+        map<PP<RemotePLearnServer>, int> step;
+
+        for(int i= 0; i < nservers; ++i)
+        {
+            RemotePLearnServer* s= servers[i];
+            int id= s->newObject(*learners[i]);
+            VMat sts= splitter->getSplit(i)[0];
+            if(master_sends_testset_rows)
+                sts= new MemoryVMatrix(sts.toMat());
+            s->callMethod(id, "setTrainingSet", sts, true);
+            learners_ids[s]= id;
+            bagnums[s]= i;
+            step[s]= 1;
+        }
+
+        int lastbag= nservers-1;
+        int ndone= 0;
+
+        while(nservers > 0)
+        {
+            PP<RemotePLearnServer> s= service.waitForResult();
+            switch(step[s])
+            {
+            case 1: 
+                DBG_LOG << "** get setTrainingSet result" << endl;
+                s->getResults();//from setTrainingSet
+                s->callMethod(learners_ids[s], "train");
+                step[s]= 2;
+                break;
+            case 2:
+                DBG_LOG << "** get train result" << endl;
+                s->getResults();//from train
+                if(pb) pb->update(++ndone);
+                s->callMethod(learners_ids[s], "getObject");
+                step[s]= 3;
+                break;
+            case 3:
+                DBG_LOG << "** get getObject result" << endl;
+                s->getResults(learners[bagnums[s]]);//from getObject
+                s->deleteObject(learners_ids[s]);
+                if(++lastbag < nbags)
+                {
+                    int id= s->newObject(*learners[lastbag]);
+                    VMat sts= splitter->getSplit(lastbag)[0];
+                    if(master_sends_testset_rows)
+                        sts= new MemoryVMatrix(sts.toMat());
+                    s->callMethod(id, "setTrainingSet", sts, true);
+                    learners_ids[s]= id;
+                    bagnums[s]= lastbag;
+                    step[s]= 1;
+                }
+                else
+                {
+                    service.freeServer(s);
+                    --nservers;
+                }
+                break;
+            }
+        }
+
+        return; // avoid extra indentation
+    }
+
     // sequential train
     for(int i= 0; i < nbags; ++i)
     {
         PP<PLearner> l = learners[i];
         l->setTrainingSet(splitter->getSplit(i)[0]);
         l->train();
+        if(pb) pb->update(i);
     }
 
     stage++;
@@ -164,60 +265,94 @@
 {
     int nout = outputsize();
     output.resize(nout);
-    output.fill(0.);
     int nlearners= learners.size();
-    static TVec<Vec> learners_outputs(nlearners);//don't realloc every time
+    PLASSERT(template_learner);
+    int sub_nout = template_learner->outputsize();
+    learners_outputs.resize(nlearners, sub_nout);
 
+    last_test_input.resize(input.size());
+    last_test_input << input;//save it, to test in computeCostsFromOutputs
+
     for(int i= 0; i < nlearners; ++i)
-        learners[i]->computeOutput(input, learners_outputs[i]);
+    {
+        Vec outp= learners_outputs(i);
+        learners[i]->computeOutput(input, outp);
+    }
 
-    switch(reduce_func)
+    if(exclude_extremes > 0)
     {
-    case 'A':
-        for(int i= 0; i < nlearners; ++i)
-            for(int j= 0; j < nout; ++j)
-                output[j]+= learners_outputs[i][j];
-        for(int j= 0; j < nout; ++j)
-            output[j]/= nlearners;
-        break;
-    default:
-        PLERROR("BaggingLearner::computeOutput : reduce_func '%c' unknown.",
-                reduce_func);
+        outputs.resize(nlearners, sub_nout);
+        outputs << learners_outputs;
+        //exclude highest and lowest n predictions for each output
+        int nexcl= 2*exclude_extremes;
+        if(nlearners <= nexcl)
+            PLERROR("BaggingLearner::computeOutput : Cannot exclude all outputs! "
+                    "nlearners=%d, exclude_extremes=%d",nlearners,exclude_extremes);
+        // sort all in place, one output at a time
+        for(int j= 0; j < sub_nout; ++j)
+            sortElements(outputs.column(j).toVec());
+        // exclude from both ends
+        outputs= outputs.subMatRows(exclude_extremes, outputs.length()-nexcl);
+        nlearners-= nexcl;
     }
+    else 
+        outputs= learners_outputs;
+
+    stcol.forget();
+    for(int i= 0; i < outputs.length(); ++i)
+        stcol.update(outputs(i));
+    
+    int i= 0;
+    for(int j= 0; j < stcol.size(); ++j)
+        for(TVec<string>::iterator it= stats.begin();
+            it != stats.end(); ++it)
+            output[i++]= stcol.getStats(j).getStat(*it);
+
+    if(output_sub_outputs)
+        for(int j= 0; j < nlearners; ++j)
+            for(int k= 0; k < sub_nout; ++k)
+                output[i++]= learners_outputs(j,k);
 }
 
 void BaggingLearner::computeCostsFromOutputs(const Vec& input, const Vec& output,
-                                           const Vec& target, Vec& costs) const
+                                             const Vec& target, Vec& costs) const
 {
-    // FIXME: for now, costs are the average of underlying learners' costs
-    //        BUT the name of those costs is the same... (misleading)
+    if(input != last_test_input)
+        PLERROR("BaggingLearner::computeCostsFromOutputs has to be called "
+                "right after computeOutput, with the same input.");
+    
     int nlearners= learners.size();
-    TVec<Vec> learners_costs(nlearners);
-    int ncosts= nTestCosts();
-    costs.resize(ncosts);
-    costs.fill(0.);
+    costs.resize(nTestCosts());
+    int k= 0;
     for(int i= 0; i < nlearners; ++i)
-        learners[i]->computeCostsFromOutputs(input, output, target, 
-                                             learners_costs[i]);
-    for(int i= 0; i < nlearners; ++i)
-        for(int j= 0; j < ncosts; ++j)
-            costs[j]+= learners_costs[i][j];
-    for(int i= 0; i < ncosts; ++i)
-        costs[i]/= nlearners;
+    {
+        Vec subcosts;
+        learners[i]->computeCostsFromOutputs(input, learners_outputs(i),
+                                             target, subcosts);
+        for(int j= 0; j < subcosts.length(); ++j)
+            costs[k++]= subcosts[j];
+    }
+
 }
 
 TVec<string> BaggingLearner::getTestCostNames() const
 {
+    PLASSERT(splitter);
     PLASSERT(template_learner);
-    PLWARNING("BaggingLearner::getTestCostNames() : the test costs are actually the mean of test costs for all learners (bags).");
-    return template_learner->getTestCostNames();
+    int nbags= splitter->nsplits();
+    TVec<string> subcosts= template_learner->getTestCostNames();
+    TVec<string> costnames(nTestCosts());
+    int nsubcosts= subcosts.length();
+    int k= 0;
+    for(int i= 0; i < nbags; ++i)
+        for(int j= 0; j < nsubcosts; ++j)
+            costnames[k++]= string("learner")+tostring(i)+"."+subcosts[j];
+    return costnames;
 }
 
 TVec<string> BaggingLearner::getTrainCostNames() const
 {
-    PLASSERT(template_learner);
-    PLWARNING("BaggingLearner::getTrainCostNames() : the train costs are actually the mean of train costs for all learners (bags).");
-    return template_learner->getTrainCostNames();
+    return TVec<string>(); // for now
 }
 
 ////////////////
@@ -225,8 +360,9 @@
 ////////////////
 int BaggingLearner::nTestCosts() const
 {
+    PLASSERT(splitter);
     PLASSERT(template_learner);
-    return template_learner->nTestCosts();
+    return splitter->nsplits()*template_learner->nTestCosts();
 }
 
 /////////////////
@@ -234,8 +370,7 @@
 /////////////////
 int BaggingLearner::nTrainCosts() const
 {
-    PLASSERT(template_learner);
-    return template_learner->nTrainCosts();
+    return 0;
 }
 
 ////////////////////////
@@ -265,7 +400,25 @@
     inherited::setTrainingSet(training_set, call_forget);
 }
 
+TVec<string> BaggingLearner::getOutputNames() const
+{
+    PLASSERT(template_learner);
+    PLASSERT(splitter);
+    TVec<string> suboutputnames= template_learner->getOutputNames();
+    TVec<string> outputnames= addStatNames(suboutputnames);
+    if(output_sub_outputs)
+    {
+        int nbags= splitter->nsplits();
+        int nsout= suboutputnames.length();
+        for(int i= 0; i < nbags; ++i)
+            for(int j= 0; j < nsout; ++j)
+                outputnames.append(string("learner")+tostring(i)+"."+suboutputnames[j]);
+    }
+    return outputnames;
+}
 
+
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/meta/BaggingLearner.h
===================================================================
--- trunk/plearn_learners/meta/BaggingLearner.h	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn_learners/meta/BaggingLearner.h	2007-04-05 01:35:37 UTC (rev 6834)
@@ -57,15 +57,23 @@
 
     PP<Splitter> splitter; //!< splitter used to get bags(=splits)
     PP<PLearner> template_learner; //!< to deep-copy once for each bag
-    char reduce_func; //!< function used to combine outputs from all learners
-    
+    //! functions used to combine outputs from all learners
+    TVec<string> stats;
+    //! for computeOutput: remove the highest and lowest 
+    //! outputs before averaging (nb. to exclude at each end)
+    int exclude_extremes; 
+    //! Wether computeOutput should append sub-learners outputs to output.
+    bool output_sub_outputs;
+
 public:
     //#####  Public Member Functions  #########################################
 
     //! Default constructor
     BaggingLearner(PP<Splitter> splitter_= 0, 
                    PP<PLearner> template_learner_= 0,
-                   char reduce_func_= 'A');
+                   TVec<string> stats_= TVec<string>(1,"E"),
+                   int exclude_extremes_= 0,
+                   bool output_sub_outputs_= false);
 
     //#####  PLearner Member Functions  #######################################
     virtual int outputsize() const;
@@ -90,6 +98,8 @@
 
     virtual void setTrainingSet(VMat training_set, bool call_forget=true);
 
+    virtual TVec<string> getOutputNames() const;
+
     //#####  PLearn::Object Protocol  #########################################
     PLEARN_DECLARE_OBJECT(BaggingLearner);
 
@@ -115,7 +125,25 @@
 private:
     //#####  Private Data Members  ############################################
 
-    // The rest of the private stuff goes here
+protected:
+    mutable VecStatsCollector stcol;
+    mutable Mat learners_outputs;
+    mutable Mat outputs;
+    mutable Vec learner_costs;
+    mutable Vec last_test_input;
+
+
+    TVec<string> addStatNames(const TVec<string>& names) const
+    {
+        TVec<string> outputnames;
+        for(TVec<string>::iterator it= names.begin(); it != names.end(); ++it)
+            for(TVec<string>::const_iterator jt= stats.begin();
+                jt != stats.end(); ++jt)
+                outputnames.push_back(*jt + '[' + *it + ']');
+        return outputnames;
+    }
+
+
 };
 
 // Declares a few other classes and functions related to this class

Modified: trunk/plearn_learners/regressors/LinearRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/LinearRegressor.cc	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn_learners/regressors/LinearRegressor.cc	2007-04-05 01:35:37 UTC (rev 6834)
@@ -260,7 +260,8 @@
                              weight_decay, weights, 
                              !recompute_XXXY, XtX, XtY,
                              sum_squared_y, outputwise_sum_squared_Y,
-                             true, verbosity, cholesky, include_bias?1:0);
+                             true, report_progress?verbosity:0, 
+                             cholesky, include_bias?1:0);
     }
     else if (train_set->weightsize()==1)
     {
@@ -269,7 +270,8 @@
                                      train_set.subMatColumns(inputsize()+targetsize(),1),
                                      weight_decay, weights,
                                      !recompute_XXXY, XtX, XtY, sum_squared_y, outputwise_sum_squared_Y,
-                                     sum_gammas, true, verbosity, cholesky, include_bias?1:0);
+                                     sum_gammas, true, report_progress?verbosity:0, 
+                                     cholesky, include_bias?1:0);
     }
     else
         PLERROR("LinearRegressor: expected dataset's weightsize to be either 1 or 0, got %d\n",

Modified: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/pytest.config
===================================================================
--- trunk/plearn_learners/regressors/test/GaussianProcessRegressor/pytest.config	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn_learners/regressors/test/GaussianProcessRegressor/pytest.config	2007-04-05 01:35:37 UTC (rev 6834)
@@ -101,7 +101,10 @@
         compiler = "pymake"
         ),
     arguments = "server < INPUTS_GPR",
-    resources = [ "INPUTS_GPR", "learner_hyperopt.plearn" ],
+    resources = [
+        "INPUTS_GPR",
+        "learner_hyperopt.plearn"
+        ],
     precision = 1e-06,
     pfileprg = "__program__",
     disabled = False

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/pytest.config
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/pytest.config	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/pytest.config	2007-04-05 01:35:37 UTC (rev 6834)
@@ -41,11 +41,12 @@
     3) Call 'which PRGNAME'
     4) Fail
     
-    Compilable program should provided the keyword argument 'compiler'
+    Compilable program should provide the keyword argument 'compiler'
     mapping to a string interpreted as the compiler name (e.g.
-    "compiler = 'pymake'"). Arguments to be forwarded to the compiler can be
-    provided as a string through the 'compile_options' keyword argument.
-      @type(program):
+    "compiler = 'pymake'"). If no compiler is provided while the program is
+    believed to be compilable, 'pymake' will be assigned by
+    default. Arguments to be forwarded to the compiler can be provided as a
+    string through the 'compile_options' keyword argument. @type program:
     Program
     
       @ivar(arguments):
@@ -78,6 +79,13 @@
       - A Program (see 'program' option) instance
       - None: if you are sure no files are to be compared.
     
+      @ivar(ignored_files_re):
+    Default behaviour of a test is to compare all
+    files created by running the test. In some case, one may prefer some of
+    these files to be ignored.
+      @type(ignored_files_re):
+    list of regular expressions
+    
       @ivar(disabled):
     If true, the test will not be ran.
       @type(disabled):
@@ -90,8 +98,7 @@
     category = "General",
     program = Program(
         name = "plearn_tests",
-        compiler = "pymake",
-        compile_options = ""
+        compiler = "pymake"
         ),
     arguments = "kernel_ridge_regressor.pyplearn",
     resources = [ "kernel_ridge_regressor.pyplearn" ],
@@ -106,8 +113,7 @@
     category = "General",
     program = Program(
         name = "plearn_tests",
-        compiler = "pymake",
-        compile_options = ""
+        compiler = "pymake"
         ),
     arguments = "kernel_ridge_hyper_regressor.pyplearn",
     resources = [ "kernel_ridge_hyper_regressor.pyplearn" ],

Modified: trunk/plearn_learners/regressors/test/LinearRegressor/pytest.config
===================================================================
--- trunk/plearn_learners/regressors/test/LinearRegressor/pytest.config	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn_learners/regressors/test/LinearRegressor/pytest.config	2007-04-05 01:35:37 UTC (rev 6834)
@@ -41,11 +41,12 @@
     3) Call 'which PRGNAME'
     4) Fail
     
-    Compilable program should provided the keyword argument 'compiler'
+    Compilable program should provide the keyword argument 'compiler'
     mapping to a string interpreted as the compiler name (e.g.
-    "compiler = 'pymake'"). Arguments to be forwarded to the compiler can be
-    provided as a string through the 'compile_options' keyword argument.
-      @type(program):
+    "compiler = 'pymake'"). If no compiler is provided while the program is
+    believed to be compilable, 'pymake' will be assigned by
+    default. Arguments to be forwarded to the compiler can be provided as a
+    string through the 'compile_options' keyword argument. @type program:
     Program
     
       @ivar(arguments):
@@ -78,6 +79,13 @@
       - A Program (see 'program' option) instance
       - None: if you are sure no files are to be compared.
     
+      @ivar(ignored_files_re):
+    Default behaviour of a test is to compare all
+    files created by running the test. In some case, one may prefer some of
+    these files to be ignored.
+      @type(ignored_files_re):
+    list of regular expressions
+    
       @ivar(disabled):
     If true, the test will not be ran.
       @type(disabled):
@@ -90,8 +98,7 @@
     category = "General",
     program = Program(
         name = "plearn_tests",
-        compiler = "pymake",
-        compile_options = ""
+        compiler = "pymake"
         ),
     arguments = "linear_regressor.pyplearn",
     resources = [ "linear_regressor.pyplearn" ],

Modified: trunk/plearn_learners/regressors/test/PLS/pytest.config
===================================================================
--- trunk/plearn_learners/regressors/test/PLS/pytest.config	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn_learners/regressors/test/PLS/pytest.config	2007-04-05 01:35:37 UTC (rev 6834)
@@ -41,11 +41,12 @@
     3) Call 'which PRGNAME'
     4) Fail
     
-    Compilable program should provided the keyword argument 'compiler'
+    Compilable program should provide the keyword argument 'compiler'
     mapping to a string interpreted as the compiler name (e.g.
-    "compiler = 'pymake'"). Arguments to be forwarded to the compiler can be
-    provided as a string through the 'compile_options' keyword argument.
-      @type(program):
+    "compiler = 'pymake'"). If no compiler is provided while the program is
+    believed to be compilable, 'pymake' will be assigned by
+    default. Arguments to be forwarded to the compiler can be provided as a
+    string through the 'compile_options' keyword argument. @type program:
     Program
     
       @ivar(arguments):
@@ -78,6 +79,13 @@
       - A Program (see 'program' option) instance
       - None: if you are sure no files are to be compared.
     
+      @ivar(ignored_files_re):
+    Default behaviour of a test is to compare all
+    files created by running the test. In some case, one may prefer some of
+    these files to be ignored.
+      @type(ignored_files_re):
+    list of regular expressions
+    
       @ivar(disabled):
     If true, the test will not be ran.
       @type(disabled):
@@ -90,8 +98,7 @@
     category = "General",
     program = Program(
         name = "plearn_tests",
-        compiler = "pymake",
-        compile_options = ""
+        compiler = "pymake"
         ),
     arguments = "PL_pls.plearn",
     resources = [ "PL_pls.plearn" ],

Modified: trunk/plearn_learners/regressors/test/StackedPCARegression/pytest.config
===================================================================
--- trunk/plearn_learners/regressors/test/StackedPCARegression/pytest.config	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn_learners/regressors/test/StackedPCARegression/pytest.config	2007-04-05 01:35:37 UTC (rev 6834)
@@ -41,11 +41,12 @@
     3) Call 'which PRGNAME'
     4) Fail
     
-    Compilable program should provided the keyword argument 'compiler'
+    Compilable program should provide the keyword argument 'compiler'
     mapping to a string interpreted as the compiler name (e.g.
-    "compiler = 'pymake'"). Arguments to be forwarded to the compiler can be
-    provided as a string through the 'compile_options' keyword argument.
-      @type(program):
+    "compiler = 'pymake'"). If no compiler is provided while the program is
+    believed to be compilable, 'pymake' will be assigned by
+    default. Arguments to be forwarded to the compiler can be provided as a
+    string through the 'compile_options' keyword argument. @type program:
     Program
     
       @ivar(arguments):
@@ -78,6 +79,13 @@
       - A Program (see 'program' option) instance
       - None: if you are sure no files are to be compared.
     
+      @ivar(ignored_files_re):
+    Default behaviour of a test is to compare all
+    files created by running the test. In some case, one may prefer some of
+    these files to be ignored.
+      @type(ignored_files_re):
+    list of regular expressions
+    
       @ivar(disabled):
     If true, the test will not be ran.
       @type(disabled):
@@ -90,8 +98,7 @@
     category = "General",
     program = Program(
         name = "plearn_tests",
-        compiler = "pymake",
-        compile_options = ""
+        compiler = "pymake"
         ),
     arguments = "stacked_pca_regression.pyplearn",
     resources = [ "stacked_pca_regression.pyplearn" ],
@@ -106,8 +113,7 @@
     category = "General",
     program = Program(
         name = "plearn_tests",
-        compiler = "pymake",
-        compile_options = ""
+        compiler = "pymake"
         ),
     arguments = "linear_regressor.pyplearn",
     resources = [ "linear_regressor.pyplearn" ],

Modified: trunk/plearn_learners/unsupervised/test/pytest.config
===================================================================
--- trunk/plearn_learners/unsupervised/test/pytest.config	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/plearn_learners/unsupervised/test/pytest.config	2007-04-05 01:35:37 UTC (rev 6834)
@@ -41,11 +41,12 @@
     3) Call 'which PRGNAME'
     4) Fail
     
-    Compilable program should provided the keyword argument 'compiler'
+    Compilable program should provide the keyword argument 'compiler'
     mapping to a string interpreted as the compiler name (e.g.
-    "compiler = 'pymake'"). Arguments to be forwarded to the compiler can be
-    provided as a string through the 'compile_options' keyword argument.
-      @type(program):
+    "compiler = 'pymake'"). If no compiler is provided while the program is
+    believed to be compilable, 'pymake' will be assigned by
+    default. Arguments to be forwarded to the compiler can be provided as a
+    string through the 'compile_options' keyword argument. @type program:
     Program
     
       @ivar(arguments):
@@ -78,6 +79,13 @@
       - A Program (see 'program' option) instance
       - None: if you are sure no files are to be compared.
     
+      @ivar(ignored_files_re):
+    Default behaviour of a test is to compare all
+    files created by running the test. In some case, one may prefer some of
+    these files to be ignored.
+      @type(ignored_files_re):
+    list of regular expressions
+    
       @ivar(disabled):
     If true, the test will not be ran.
       @type(disabled):

Modified: trunk/python_modules/plearn/math/test/pytest.config
===================================================================
--- trunk/python_modules/plearn/math/test/pytest.config	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/python_modules/plearn/math/test/pytest.config	2007-04-05 01:35:37 UTC (rev 6834)
@@ -41,11 +41,12 @@
     3) Call 'which PRGNAME'
     4) Fail
     
-    Compilable program should provided the keyword argument 'compiler'
+    Compilable program should provide the keyword argument 'compiler'
     mapping to a string interpreted as the compiler name (e.g.
-    "compiler = 'pymake'"). Arguments to be forwarded to the compiler can be
-    provided as a string through the 'compile_options' keyword argument.
-      @type(program):
+    "compiler = 'pymake'"). If no compiler is provided while the program is
+    believed to be compilable, 'pymake' will be assigned by
+    default. Arguments to be forwarded to the compiler can be provided as a
+    string through the 'compile_options' keyword argument. @type program:
     Program
     
       @ivar(arguments):
@@ -78,6 +79,13 @@
       - A Program (see 'program' option) instance
       - None: if you are sure no files are to be compared.
     
+      @ivar(ignored_files_re):
+    Default behaviour of a test is to compare all
+    files created by running the test. In some case, one may prefer some of
+    these files to be ignored.
+      @type(ignored_files_re):
+    list of regular expressions
+    
       @ivar(disabled):
     If true, the test will not be ran.
       @type(disabled):

Modified: trunk/python_modules/plearn/parallel/dispatch.py
===================================================================
--- trunk/python_modules/plearn/parallel/dispatch.py	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/python_modules/plearn/parallel/dispatch.py	2007-04-05 01:35:37 UTC (rev 6834)
@@ -43,12 +43,12 @@
 # than once: use the MAX_LOADAVG map to allow for higher maximum load
 # average than the default of 2.
 SSH_MACHINES_MAP = { 'apstat.com': [ 'embla',
+                                     'valhalla',
                                      'inari', 
                                      'kamado',
                                      'loki',
                                      'odin',
                                      'midgard',
-                                     'valhalla',
                                      'vili'
                                      ],
 
@@ -59,8 +59,8 @@
                      }
 
 # To override the default of 1
-MAX_LOADAVG = { 'inari'  : 4 ,
-                'kamado' : 4 }
+MAX_LOADAVG = { 'inari'     : 4 ,
+                'kamado'    : 4 }
 
 # Do not perform a new query for the loadavg until recently launched
 # processes are likely to have started. 
@@ -338,7 +338,7 @@
             except Exception:
                 continue # Machine is rebooting, shut down, ...
 
-            max_loadavg = MAX_LOADAVG.get(m, cls._max_load)
+            max_loadavg = MAX_LOADAVG.get(m, 1.)*cls._max_load
             #print "Load %f / %f"%(loadavg, max_loadavg)
             if loadavg < max_loadavg:
                 # Register the load average *plus* one, taking in account

Modified: trunk/python_modules/plearn/plide/plide.py
===================================================================
--- trunk/python_modules/plearn/plide/plide.py	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/python_modules/plearn/plide/plide.py	2007-04-05 01:35:37 UTC (rev 6834)
@@ -98,7 +98,7 @@
 
     PlideVersion = "0.01"
 
-    def __init__( self, *args, **kwargs ):
+    def __init__( self, streams_to_watch= {}, *args, **kwargs ):
         GladeAppWindow.__init__(self, gladeFile())
 
         ## Forward injected to imported Plide modules
@@ -119,7 +119,7 @@
         welcome_text = kwargs.get("welcome_text",
                                   "<b>Welcome to Plide %s!</b>" % self.PlideVersion)
         self.status_display(welcome_text, has_markup=True)
-        self.setup_stdouterr_redirect()
+        self.setup_stdouterr_redirect(streams_to_watch)
         
         ## Set up help system
         injected.helpResourcesPath(helpResourcesPath())
@@ -164,7 +164,7 @@
             if help_context:
                 self.help_viewer.display_page(help_context)
             
-    def setup_stdouterr_redirect( self ):
+    def setup_stdouterr_redirect( self, streams_to_watch= {} ):
         """Redirect standard output and error to be sent to the log pane
         instead of the console.
         """
@@ -193,18 +193,21 @@
         fcntl.fcntl(self.stdout_read, fcntl.F_SETFL, out_flags | os.O_NONBLOCK)
         fcntl.fcntl(self.stderr_read, fcntl.F_SETFL, err_flags | os.O_NONBLOCK)
 
+        streams_to_watch.update({ self.stdout_read:'stdout',
+                                  self.stderr_read:'stderr' })
+
         def callback( fd, cb_condition ):
             # print >>raw_stderr, "log_updater: got stuff on fd", fd, \
             #       "with condition", cb_condition
             raw_stderr.flush()
             data = os.read(fd,65536)
 
-            kind = { self.stdout_read:'stdout', self.stderr_read:'stderr' }.get(fd,'')
+            kind = streams_to_watch.get(fd,'')
             self.log_display(data, kind)
             return True                 # Ensure it's called again!
         
-        gobject.io_add_watch(self.stdout_read, gobject.IO_IN, callback)
-        gobject.io_add_watch(self.stderr_read, gobject.IO_IN, callback)
+        for s in streams_to_watch:
+            gobject.io_add_watch(s, gobject.IO_IN, callback)
 
     def setup_statusbar( self ):
         """Arrange the status bar area to contain both a status line and a progress bar.
@@ -651,7 +654,7 @@
                 options_holder.pyplearn_actualize()
             else:
                 ## FIXME: Generate a brand-new expdir (minor hack)
-                plargs._parse_(["expdir="+generateExpdir()])
+                plargs.parse(["expdir="+generateExpdir()])
                 
             expdir = plargs.expdir
             plearn_script = eval('str(PyPLearnScript( main() ))', script_env)
@@ -844,9 +847,9 @@
 #global plide_main_window
 plide_main_window = None
 
-def StartPlide(argv = []):
+def StartPlide(argv = [], streams_to_watch= {}):
     global plide_main_window
-    plide_main_window = PlideMain()
+    plide_main_window = PlideMain(streams_to_watch)
 
     ## Consider each file passed as command-line argument and create a tab
     ## to view it.
@@ -909,15 +912,43 @@
 
 #####  Standalone Running  ##################################################
 
+from plearn.io.server import *
+
+class Poubelle(RemotePLearnServer):
+    """
+    Patch to test standalone running while 'slave' mode still exists.
+    """
+    def __init__(self):
+        command= 'plearn server'
+        self.errstm= None
+        try:
+            from subprocess import Popen, PIPE
+            p= Popen([command], shell=True, stdin=PIPE, stdout=PIPE, stderr=PIPE, close_fds=True)
+            (to_server, from_server, child_pid) = (p.stdin, p.stdout, p.pid)
+            self.errstm= p.stderr
+        except:
+            to_server, from_server = os.popen2(command, 'b')
+            child_pid = -1
+        RemotePLearnServer.__init__(self,from_server, to_server, pid=child_pid)
+
+
+    def getAllClassnames(self): return self.listClasses()
+    def helpResourcesPath(self,path): return self.setResourcesPathHTML(path)
+    def helpIndex(self): return self.helpIndexHTML()
+    def helpClasses(self): return self.helpClassesHTML()
+    def helpCommands(self): return self.helpCommandsHTML()
+    def helpOnCommand(self, command): return self.helpOnCommandHTML(command)
+    def helpOnClass(self, classname): return self.helpOnClassHTML(classname)
+
 if __name__ == "__main__":
-    class Poubelle:
-        def __getattr__(self, attr): return None
+    #class Poubelle:
+    #    def __getattr__(self, attr): return None
 
     global plide_main_window
     global injected
     injected = Poubelle()
     
-    StartPlide()
+    StartPlide(streams_to_watch= {injected.errstm.fileno(): 'injected-stderr'})
     # print >>sys.stderr, "Random stuff to stderr"
     # print >>sys.stdout, "Random stuff to stdout"
     # sys.stderr.flush()

Modified: trunk/python_modules/plearn/plide/plide_help.py
===================================================================
--- trunk/python_modules/plearn/plide/plide_help.py	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/python_modules/plearn/plide/plide_help.py	2007-04-05 01:35:37 UTC (rev 6834)
@@ -93,7 +93,9 @@
         """Callback that's invoked when HTML widget reports that a link has
         been clicked by the user.
         """       
+        #self.plide_main.cursor_hourglass()
         self.display_page(uri)
+        #self.plide_main.cursor_normal()
 
     def display_page( self, uri, doc = None ):
         """Take the current page to the back list, clear the forward list,
@@ -108,20 +110,24 @@
         self.update_title(title)
 
     def go_back( self, widget ):
+        #self.plide_main.cursor_hourglass()
         if self.back_pages:
             if self.cur_page:
                 self.fwd_pages.append(self.cur_page)
             self.cur_page = self.back_pages.pop()
             title = self.update_doc_from_uri(self.cur_page)
             self.update_title(title)
+        #self.plide_main.cursor_normal()
 
     def go_forward( self, widget ):
+        #self.plide_main.cursor_hourglass()
         if self.fwd_pages:
             if self.cur_page:
                 self.back_pages.append(self.cur_page)
             self.cur_page = self.fwd_pages.pop()
             title = self.update_doc_from_uri(self.cur_page)
             self.update_title(title)
+        #self.plide_main.cursor_normal()
 
     def update_title( self, new_title ):
         """Update the title label, and shade/unshade back-forward controls
@@ -137,6 +143,7 @@
         back-forward history or updates to the title.  Return the new page
         title.
         """
+      
         if doc is None:
             doc = self.doc
 
@@ -158,14 +165,18 @@
             html  = injected.helpIndex()
             title = "Help Index"
 
-        elif uri == "class_index.html":
+        elif uri == "class_index.html" or  uri == "classes_index.html":
             html  = injected.helpClasses()
             title = "Class Index"
 
-        elif uri == "command_index.html":
+        elif uri == "command_index.html" or uri == "commands_index.html":
             html  = injected.helpCommands()
             title = "Command Index"
 
+        elif uri == "functions_index.html":
+            html  = injected.helpFunctionsHTML()
+            title = "Function Index"
+
         elif uri.startswith("command_") and uri.endswith(".html"):
             command = uri[8:-5]         # Drop prefix and suffix
             title   = command + " (command)"

Modified: trunk/python_modules/plearn/plide/plide_options.py
===================================================================
--- trunk/python_modules/plearn/plide/plide_options.py	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/python_modules/plearn/plide/plide_options.py	2007-04-05 01:35:37 UTC (rev 6834)
@@ -109,7 +109,8 @@
         #     plarg_defaults, 'Global Configuration Options'))
 
         ## Look at all options in plargs_namespace
-        for clsname,cls in plargs_namespace._subclasses.iteritems():
+        #for clsname,cls in plargs_namespace._subclasses.iteritems():
+        for clsname,cls in plnamespace._subclasses.iteritems():
             short_doc = toolkit_doc(cls, 0).rstrip('.')
             full_doc  = toolkit_doc(cls, 1, "\n")
             group = PyPLearnOptionsGroup(clsname, public_members(cls).keys(),
@@ -123,11 +124,11 @@
         handles logging control.
         """
         ## Generate a brand-new expdir
-        plargs._parse_(["expdir="+generateExpdir()])
+        plargs.parse(["expdir="+generateExpdir()])
         
         ## Apply the manual overrides
         if self.option_overrides:
-            plargs._parse_(self.option_overrides)
+            plargs.parse(self.option_overrides)
 
         verbosity_map = { 0:0, 1:1, 2:5, 3:10, 4:500 }
         verbosity = verbosity_map.get(self.log_verbosity, 5)

Modified: trunk/python_modules/plearn/plide/resources/help_prolog.html
===================================================================
--- trunk/python_modules/plearn/plide/resources/help_prolog.html	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/python_modules/plearn/plide/resources/help_prolog.html	2007-04-05 01:35:37 UTC (rev 6834)
@@ -125,10 +125,14 @@
 
 
 </style>
-<title>PLearn User-Level Documentation</title>
+<!-- <title>PLearn User-Level Documentation</title> -->
+<title>${PAGE_TITLE}</title>
 </head>
 <body id="wrapper">
 <div class="topnav">
-  <a href="index.html">Main Index</a><a href="class_index.html">Class Index</a><a href="command_index.html">Command Index</a>
+  <a href="index.html">Main Index</a>
+  <a href="classes_index.html">Class Index</a>
+  <a href="commands_index.html">Command Index</a>
+  <a href="functions_index.html">Function Index</a>
 </div>
 <!-- <div id="banner"></div> -->
\ No newline at end of file

Modified: trunk/python_modules/plearn/pyplearn/plargs.py
===================================================================
--- trunk/python_modules/plearn/pyplearn/plargs.py	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/python_modules/plearn/pyplearn/plargs.py	2007-04-05 01:35:37 UTC (rev 6834)
@@ -676,8 +676,9 @@
         context = actualContext(plargs)
         for statement in args:
             assert isinstance(statement, str), statement
-            
+
             option, value = statement.split('=', 1)
+
             option = option.strip()
             value  = value.strip()
 

Modified: trunk/python_modules/plearn/pyplearn/pytest.config
===================================================================
--- trunk/python_modules/plearn/pyplearn/pytest.config	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/python_modules/plearn/pyplearn/pytest.config	2007-04-05 01:35:37 UTC (rev 6834)
@@ -144,7 +144,7 @@
     name = "PL_misspelled_plargs",
     description = "",
     category = "General",
-    program = Program(name="python"),
+    program = Program( name = "python" ),
     arguments = "-c \"from plearn.pyplearn.plargs import test_misspelled_plargs; test_misspelled_plargs()\" ",
     resources = [ ],
     precision = 1e-06,

Added: trunk/python_modules/plearn/utilities/pltraceback.py
===================================================================
--- trunk/python_modules/plearn/utilities/pltraceback.py	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/python_modules/plearn/utilities/pltraceback.py	2007-04-05 01:35:37 UTC (rev 6834)
@@ -0,0 +1,14 @@
+
+# pltraceback.py
+
+"""
+Traceback text formatting; just use cgitb after replacing some functions...
+"""
+
+import os.path
+import time
+import cgitb
+
+time.ctime= lambda x: ''
+os.path.abspath= os.path.basename
+text= cgitb.text

Modified: trunk/scripts/xdispatch
===================================================================
--- trunk/scripts/xdispatch	2007-04-04 19:00:14 UTC (rev 6833)
+++ trunk/scripts/xdispatch	2007-04-05 01:35:37 UTC (rev 6834)
@@ -52,6 +52,11 @@
                       dest= "nb_servers",
                       help='Number of servers to launch (maximum)',
                       default=3)
+
+    parser.add_option('-t', '--nb-servers-per-server', type="int",
+                      dest= "nb_servers2",
+                      help='Number of servers to launch per server (maximum)',
+                      default=1)
    
     (options, args) = parser.parse_args()
 
@@ -66,8 +71,13 @@
             plearn_command= args[0]
             # launch slaves with the desired plearn executable
             for i in xrange(options.nb_servers):
-                launch_server( ['time',plearn_command, 'server', '0'] )
+                if options.nb_servers2 < 2:
+                    launch_server( ['time',plearn_command, 'server', '0'] )
+                else:
+                    launch_server( ['xdispatch', '-s'+str(options.nb_servers2),
+                                    '--', plearn_command, 'server', '0'] )
 
+
             # temp file for the list of slaves (host, port, pid)
             tf, fname= tempfile.mkstemp(suffix= '.plserv', prefix= '.plserv_', dir= '/tmp/')
             for si in Task.getLaunchedServersInfo():



From saintmlx at mail.berlios.de  Thu Apr  5 03:44:09 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 5 Apr 2007 03:44:09 +0200
Subject: [Plearn-commits] r6835 - trunk/plearn/misc
Message-ID: <200704050144.l351i999027211@sheep.berlios.de>

Author: saintmlx
Date: 2007-04-05 03:44:08 +0200 (Thu, 05 Apr 2007)
New Revision: 6835

Added:
   trunk/plearn/misc/HTMLUtils.cc
   trunk/plearn/misc/HTMLUtils.h
Log:
- added HTMLUtils



Added: trunk/plearn/misc/HTMLUtils.cc
===================================================================
--- trunk/plearn/misc/HTMLUtils.cc	2007-04-05 01:35:37 UTC (rev 6834)
+++ trunk/plearn/misc/HTMLUtils.cc	2007-04-05 01:44:08 UTC (rev 6835)
@@ -0,0 +1,205 @@
+// -*- C++ -*-
+
+// HTMLUtils.cc
+//
+// Copyright (C) 2004-2006 Nicolas Chapados 
+// Copyright (C) 2007 Xavier Saint-Mleux, ApSTAT Technologies, inc.
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Xavier Saint-Mleux
+
+/*! \file HTMLUtils.cc */
+
+
+#include "HTMLUtils.h"
+
+#include <plearn/base/stringutils.h>
+#include <plearn/base/tostring.h>
+#include <boost/regex.hpp>
+#include <commands/PLearnCommands/plearn_main.h>
+#include <plearn/base/TypeFactory.h>
+#include <plearn/base/OptionBase.h>
+
+namespace PLearn {
+using namespace std;
+
+string HTMLUtils::quote(string s)
+{
+    search_replace(s, "&", "&amp;");
+    search_replace(s, "<", "&lt;");
+    search_replace(s, ">", "&gt;");
+    search_replace(s, "\"", "&quot;");
+    return s;
+}
+
+string HTMLUtils::highlight_known_classes(string typestr)
+{
+    vector<string> tokens = split(typestr, " \t\n\r<>,.';:\"");
+    set<string> replaced; // Carry out replacements for a given token only once
+    const TypeMap& type_map = TypeFactory::instance().getTypeMap();
+    vector<string>::size_type n=tokens.size();
+    for (unsigned int i=0; i<n ; ++i) {
+        TypeMap::const_iterator it = type_map.find(tokens[i]);
+        if (it != type_map.end() && replaced.find(tokens[i]) == replaced.end()) {
+            replaced.insert(tokens[i]);
+      
+            // ensure we only match whole words with the regular expression
+            const boost::regex e("\\<" + tokens[i] + "\\>");
+            const string repl_str("<a href=\"class_$&.html\\?level="
+                                  + tostring(OptionBase::getCurrentOptionLevel()) 
+                                  +"\">$&</a>");
+            typestr = regex_replace(typestr, e, repl_str, boost::match_default | boost::format_default);
+        }
+    }
+    return typestr;
+}
+
+string HTMLUtils::format_free_text(string text)
+{
+    // sort of DWIM HTML formatting for free-text; cannot use split since it
+    // eats up consecutive delimiters
+    text = removeblanks(text);
+    size_t curpos = 0, curnl = text.find('\n');
+    bool ul_active = false;
+    bool start_paragraph = false;
+    string finallines;
+    for ( ; curpos != string::npos ;
+          curpos = curnl+(curnl!=string::npos), curnl = text.find('\n', curpos) ) {
+        string curline = text.substr(curpos, curnl-curpos);
+
+        // step 1: check if the line starts with a '-': if so, start a new <UL>
+        // if not in one, or extend one if so
+        if (removeblanks(curline).substr(0,1) == "-" ||
+            removeblanks(curline).substr(0,1) == "*" )
+        {
+            curline = removeblanks(curline).substr(1);
+            if (! ul_active)
+                curline = "<ul><li>" + curline;
+            else
+                curline = "<li>" + curline;
+            start_paragraph = false;
+            ul_active = true;
+        }
+
+        // otherwise, a line that starts with some whitespace within a list
+        // just extends the previous <li> :: don't touch it
+        else if (ul_active && (curline == "" ||
+                               curline.substr(0,1) == " " ||
+                               curline.substr(0,1) == "\t")) {
+            /* do nothing */
+        }
+
+        // otherwise, normal processing
+        else {
+            // any line that is empty or starts with some whitespace gets its own <br>
+            if (removeblanks(curline) == "") {
+                // Don't start new paragraph right away; wait until we
+                // encounter some text that's neither a <ul> or a <pre>
+                start_paragraph = true;
+                curline = "";
+            }
+            else if (curline[0] == ' ' || curline[0] == '\t') {
+                start_paragraph = false;
+                curline = "<pre>" + curline + "</pre>";
+            }
+
+            // if we were processing a list, close it first
+            if (ul_active) {
+                curline = "</ul>" + curline;
+                ul_active = 0;
+            }
+        }
+
+        if (!curline.empty() && start_paragraph) {
+            finallines += "<p>";
+            start_paragraph = false;
+        }
+        
+        finallines += curline + "\n";
+    }
+
+    // Close any pending open blocks
+    if (ul_active)
+        finallines += "</ul>\n";
+  
+    // Finally join the lines
+    return make_http_hyperlinks(finallines);
+}
+string HTMLUtils::make_http_hyperlinks(string text)
+{
+    // Find elements of the form XYZ://x.y.z/a/b/c and make them into
+    // hyperlink. An issue is to determine when
+    static const char* recognized_protocols[] = 
+        { "http://", "https://", "ftp://", "mailto:" };        // for now...
+    static const vector<string> protocols_vector(
+        recognized_protocols,
+        recognized_protocols + sizeof(recognized_protocols) / sizeof(recognized_protocols[0]));
+
+    // Match everything that starts with the recognized protocol up to the
+    // following whitespace, excluding trailing punctuation if any.
+    // Make sure the URL is NOT enclosed in quotes
+    static const boost::regex e( string("(?!\")") + "(" +
+                                 "(?:" + join(protocols_vector, "|") + ")" +
+                                 "\\S+(?:\\w|/)" +
+                                 ")" + "(?!\")" + "([[:punct:]]*\\s|$)");
+
+    const string repl_str("<a href=\"$1\?level=" 
+                          + tostring(OptionBase::getCurrentOptionLevel())
+                          +"\">$1</a>$2");
+    text = regex_replace(text, e, repl_str, boost::match_default | boost::format_default);
+    return text;
+}
+
+string HTMLUtils::generated_by()
+{
+    time_t curtime = time(NULL);
+    struct tm *broken_down_time = localtime(&curtime);
+    const int SIZE = 100;
+    char time_buffer[SIZE];
+    strftime(time_buffer,SIZE,"%Y/%m/%d %H:%M:%S",broken_down_time);
+
+    return string("<p>&nbsp;</p><address>Generated on " ) +
+        time_buffer + " by " + version_string() + "</address>";
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/misc/HTMLUtils.h
===================================================================
--- trunk/plearn/misc/HTMLUtils.h	2007-04-05 01:35:37 UTC (rev 6834)
+++ trunk/plearn/misc/HTMLUtils.h	2007-04-05 01:44:08 UTC (rev 6835)
@@ -0,0 +1,78 @@
+// -*- C++ -*-
+
+// HTMLUtils.h
+//
+// Copyright (C) 2004-2006 Nicolas Chapados 
+// Copyright (C) 2007 Xavier Saint-Mleux, ApSTAT Technologies, inc.
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Xavier Saint-Mleux
+
+/*! \file HTMLUtils.h */
+
+
+#ifndef HTMLUtils_INC
+#define HTMLUtils_INC
+
+#include <string>
+
+// Put includes here
+
+namespace PLearn {
+using namespace std;
+
+struct HTMLUtils //as namespace
+{
+    static string quote(string s);
+    static string highlight_known_classes(string typestr);
+    static string format_free_text(string text);
+    static string make_http_hyperlinks(string text);
+    static string generated_by();
+    inline static string quote_format_and_highlight(string s)
+    { return highlight_known_classes(format_free_text(quote(s))); }
+
+};
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From saintmlx at mail.berlios.de  Thu Apr  5 03:46:44 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 5 Apr 2007 03:46:44 +0200
Subject: [Plearn-commits] r6836 - trunk/plearn/misc
Message-ID: <200704050146.l351kiwU027380@sheep.berlios.de>

Author: saintmlx
Date: 2007-04-05 03:46:43 +0200 (Thu, 05 Apr 2007)
New Revision: 6836

Added:
   trunk/plearn/misc/HTMLHelpGenerator.cc
   trunk/plearn/misc/HTMLHelpGenerator.h
Log:
- added HTMLHelpGenerator



Added: trunk/plearn/misc/HTMLHelpGenerator.cc
===================================================================
--- trunk/plearn/misc/HTMLHelpGenerator.cc	2007-04-05 01:44:08 UTC (rev 6835)
+++ trunk/plearn/misc/HTMLHelpGenerator.cc	2007-04-05 01:46:43 UTC (rev 6836)
@@ -0,0 +1,142 @@
+// -*- C++ -*-
+
+// HTMLHelpGenerator.cc
+//
+// Copyright (C) 2007 Xavier Saint-Mleux, ApSTAT Technologies inc.
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Xavier Saint-Mleux
+
+/*! \file HTMLHelpGenerator.cc */
+
+
+#include "HTMLHelpGenerator.h"
+#include <plearn/io/openFile.h>
+#include <plearn/io/fileutils.h>
+#include <plearn/base/stringutils.h>
+#include <plearn/base/HelpSystem.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    HTMLHelpGenerator,
+    "ONE LINE USER DESCRIPTION",
+    "MULTI LINE\nHELP FOR USERS"
+    );
+
+HTMLHelpGenerator::HTMLHelpGenerator(const PPath& output_dir_, 
+                                     const PPath& resources_dir_)
+    :output_dir(output_dir_), resources_dir(resources_dir_)
+{}
+
+void HTMLHelpGenerator::run()
+{
+    openFile(output_dir/"index.html", PStream::raw_ascii, "w") 
+        << HelpSystem::helpIndexHTML();
+    helpCommands();
+    helpFunctions();
+    helpClasses();
+}
+
+void HTMLHelpGenerator::helpCommands()
+{
+    openFile(output_dir/"commands_index.html", PStream::raw_ascii, "w") 
+        << HelpSystem::helpCommandsHTML();
+    vector<string> commands= HelpSystem::listCommands();
+    for(vector<string>::iterator it= commands.begin();
+        it != commands.end(); ++it)
+        openFile(output_dir/"command_"+(*it)+".html", PStream::raw_ascii, "w") 
+            << HelpSystem::helpOnCommandHTML(*it);
+}
+
+void HTMLHelpGenerator::helpFunctions()
+{
+    openFile(output_dir/"functions_index.html", PStream::raw_ascii, "w") 
+        << HelpSystem::helpFunctionsHTML();
+}
+
+void HTMLHelpGenerator::helpClasses()
+{
+    openFile(output_dir/"classes_index.html", PStream::raw_ascii, "w")
+        << HelpSystem::helpClassesHTML();
+    vector<string> commands= HelpSystem::listClasses();
+    for(vector<string>::iterator it= commands.begin();
+        it != commands.end(); ++it)
+        openFile(output_dir/"class_"+(*it)+".html", PStream::raw_ascii, "w") 
+            << HelpSystem::helpOnClassHTML(*it);
+}
+
+void HTMLHelpGenerator::build()
+{
+    inherited::build();
+    build_();
+}
+
+void HTMLHelpGenerator::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+}
+
+void HTMLHelpGenerator::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "output_dir", &HTMLHelpGenerator::output_dir,
+                  OptionBase::buildoption,
+                  "Directory where the .html files should be generated");
+
+    declareOption(ol, "resources_dir", &HTMLHelpGenerator::resources_dir,
+                  OptionBase::buildoption,
+                  "Directory where the HTML help resources should be found.");
+
+    inherited::declareOptions(ol);
+}
+
+void HTMLHelpGenerator::build_()
+{
+    if(output_dir != "" && !pathexists(output_dir))
+        force_mkdir(output_dir);
+    HelpSystem::setResourcesPathHTML(resources_dir);
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/misc/HTMLHelpGenerator.h
===================================================================
--- trunk/plearn/misc/HTMLHelpGenerator.h	2007-04-05 01:44:08 UTC (rev 6835)
+++ trunk/plearn/misc/HTMLHelpGenerator.h	2007-04-05 01:46:43 UTC (rev 6836)
@@ -0,0 +1,133 @@
+// -*- C++ -*-
+
+// HTMLHelpGenerator.h
+//
+// Copyright (C) 2007 Xavier Saint-Mleux, ApSTAT Technologies inc.
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Xavier Saint-Mleux
+
+/*! \file HTMLHelpGenerator.h */
+
+
+#ifndef HTMLHelpGenerator_INC
+#define HTMLHelpGenerator_INC
+
+#include <plearn/base/Object.h>
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class HTMLHelpGenerator : public Object
+{
+    typedef Object inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+
+    //! Directory where the .html files should be generated
+    PPath output_dir;
+
+    //! Directory for HTML resources (prologue, epilogue, index.html)
+    //! e.g. {PLEARNDIR}/python_modules/plearn/plide/resources
+    PPath resources_dir;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    HTMLHelpGenerator(const PPath& output_dir_= "", 
+                      const PPath& resources_dir_= "");
+
+    virtual void run();
+    
+    void helpCommands();
+    void helpFunctions();
+    void helpClasses();
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(HTMLHelpGenerator);
+
+    virtual void build();
+
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+protected:
+    //#####  Protected Member Functions  ######################################
+    
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+};
+
+DECLARE_OBJECT_PTR(HTMLHelpGenerator);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From tihocan at mail.berlios.de  Thu Apr  5 18:35:36 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 5 Apr 2007 18:35:36 +0200
Subject: [Plearn-commits] r6837 - in trunk/plearn/python/test: .
	.pytest/EmbeddedPython_InterfunctionXchg/expected_results
Message-ID: <200704051635.l35GZahp029205@sheep.berlios.de>

Author: tihocan
Date: 2007-04-05 18:35:35 +0200 (Thu, 05 Apr 2007)
New Revision: 6837

Modified:
   trunk/plearn/python/test/.pytest/EmbeddedPython_InterfunctionXchg/expected_results/RUN.log
   trunk/plearn/python/test/InterfunctionXchgTest.cc
Log:
Test EmbeddedPython_InterfunctionXchg now passes even with a Python version different from 2.3.5

Modified: trunk/plearn/python/test/.pytest/EmbeddedPython_InterfunctionXchg/expected_results/RUN.log
===================================================================
--- trunk/plearn/python/test/.pytest/EmbeddedPython_InterfunctionXchg/expected_results/RUN.log	2007-04-05 01:46:43 UTC (rev 6836)
+++ trunk/plearn/python/test/.pytest/EmbeddedPython_InterfunctionXchg/expected_results/RUN.log	2007-04-05 16:35:35 UTC (rev 6837)
@@ -17,7 +17,7 @@
 Read back the string: 'This string should survive within the Python environment'
 Trying to read back from second snippet:
 Caught Python Exception: 'NameError
-Python 2.3.5: /usr/bin/python
+Python 2.X.Y: /usr/bin/python
 
 
 A problem occurred in a Python script.  Here is the sequence of

Modified: trunk/plearn/python/test/InterfunctionXchgTest.cc
===================================================================
--- trunk/plearn/python/test/InterfunctionXchgTest.cc	2007-04-05 01:46:43 UTC (rev 6836)
+++ trunk/plearn/python/test/InterfunctionXchgTest.cc	2007-04-05 16:35:35 UTC (rev 6837)
@@ -178,9 +178,16 @@
         // differ from one computer to another, causing the test to fail.
         boost::regex memory_adr("0x[abcdef0123456789]{6,}",
                                 boost::regex::perl|boost::regex::icase);
-        string msg_without_adr = regex_replace(exception_msg, memory_adr,
-                                               "0x[memory_address]");
-        cout << "Caught Python Exception: '" << msg_without_adr << "'" << endl;
+        string msg_without_sys_dependent_data =
+            regex_replace(exception_msg, memory_adr,
+                    "0x[memory_address]");
+        boost::regex python_ver("Python 2\\.[0-9]\\.[0-9]",
+                                boost::regex::perl|boost::regex::icase);
+        msg_without_sys_dependent_data =
+            regex_replace(msg_without_sys_dependent_data, python_ver,
+                    "Python 2.X.Y");
+        cout << "Caught Python Exception: '" << msg_without_sys_dependent_data
+             << "'" << endl;
     }
     catch (const PLearnError& e) {
         cout << "Caught PLearn Exception: '" << e.message() << "'" << endl;



From tihocan at mail.berlios.de  Thu Apr  5 18:36:59 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 5 Apr 2007 18:36:59 +0200
Subject: [Plearn-commits] r6838 - trunk/plearn/vmat
Message-ID: <200704051636.l35Gax3Z000523@sheep.berlios.de>

Author: tihocan
Date: 2007-04-05 18:36:58 +0200 (Thu, 05 Apr 2007)
New Revision: 6838

Modified:
   trunk/plearn/vmat/BootstrapSplitter.cc
Log:
- Better help
- Implemented deep copy


Modified: trunk/plearn/vmat/BootstrapSplitter.cc
===================================================================
--- trunk/plearn/vmat/BootstrapSplitter.cc	2007-04-05 16:35:35 UTC (rev 6837)
+++ trunk/plearn/vmat/BootstrapSplitter.cc	2007-04-05 16:36:58 UTC (rev 6838)
@@ -2,7 +2,7 @@
 
 // BootstrapSplitter.cc
 //
-// Copyright (C) 2003 Olivier Delalleau
+// Copyright (C) 2003,2007 Olivier Delalleau
 //
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
@@ -44,34 +44,35 @@
 namespace PLearn {
 using namespace std;
 
-BootstrapSplitter::BootstrapSplitter()
-    :Splitter(),
-     frac(0.6667),
-     n_splits(0),
-     allow_repetitions(false),
-     seed(1827),
-     rgen(new PRandom())
-    /* ### Initialise all fields to their default value */
-{
-}
+///////////////////////
+// BootstrapSplitter //
+///////////////////////
+BootstrapSplitter::BootstrapSplitter():
+    frac(0.6667),
+    n_splits(0),
+    allow_repetitions(false),
+    seed(1827),
+    rgen(new PRandom())
+{}
 
 PLEARN_IMPLEMENT_OBJECT(BootstrapSplitter,
-                        "A splitter whose splits are bootstrap samples of the original dataset",
-                        "BootstrapSplitter implements a ...");
+        "A splitter whose splits are bootstrap samples of the original dataset.",
+        "Each split of this splitter contains a single set, which is a\n"
+        "bootstrap sample of the original dataset. Note that, by default,\n"
+        "samples are taken without repetition: in order to perform a 'real'\n"
+        "bootstrap, the option 'allow_repetitions thus must be set to 1.\n"
+);
 
+////////////////////
+// declareOptions //
+////////////////////
 void BootstrapSplitter::declareOptions(OptionList& ol)
 {
-    // ### Declare all of this object's options here
-    // ### For the "flags" of each option, you should typically specify
-    // ### one of OptionBase::buildoption, OptionBase::learntoption or
-    // ### OptionBase::tuningoption. Another possible flag to be combined with
-    // ### is OptionBase::nosave
-
     declareOption(ol, "n_splits", &BootstrapSplitter::n_splits, OptionBase::buildoption,
-                  "the number of splits wanted");
+                  "Number of splits wanted.");
 
     declareOption(ol, "frac", &BootstrapSplitter::frac, OptionBase::buildoption,
-                  "the fraction of elements to take in each bootstrap");
+                  "Fraction of elements to take in each bootstrap.");
 
     declareOption(ol, "allow_repetitions", &BootstrapSplitter::allow_repetitions, 
                   OptionBase::buildoption,
@@ -79,21 +80,17 @@
 
     declareOption(ol, "seed", &BootstrapSplitter::seed, 
                   OptionBase::buildoption,
-                  "Allows each row to appear more than once per split.");
+                  "Seed for the random number generator.");
 
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
 
+////////////
+// build_ //
+////////////
 void BootstrapSplitter::build_()
 {
-    // ### This method should do the real building of the object,
-    // ### according to set 'options', in *any* situation.
-    // ### Typical situations include:
-    // ###  - Initial building of an object from a few user-specified options
-    // ###  - Building of a "reloaded" object: i.e. from the complete set of all serialised options.
-    // ###  - Updating or "re-building" of an object after a few "tuning" options have been modified.
-    // ### You should assume that the parent class' build_() has already been called.
     if (dataset) {
         rgen->manual_seed(seed);
         bootstrapped_sets.resize(0,0); // First clear the current sets.
@@ -112,36 +109,39 @@
     }
 }
 
-// ### Nothing to add here, simply calls build_
+///////////
+// build //
+///////////
 void BootstrapSplitter::build()
 {
     inherited::build();
     build_();
 }
 
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
 void BootstrapSplitter::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     Splitter::makeDeepCopyFromShallowCopy(copies);
-
-    // ### Call deepCopyField on all "pointer-like" fields
-    // ### that you wish to be deepCopied rather than
-    // ### shallow-copied.
-    // ### ex:
-    // deepCopyField(trainvec, copies);
-
-    // ### Remove this line when you have fully implemented this method.
-    PLERROR("BootstrapSplitter::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
+    deepCopyField(bootstrapped_sets, copies);
+    deepCopyField(rgen,              copies);
 }
 
+/////////////
+// nsplits //
+/////////////
 int BootstrapSplitter::nsplits() const
 {
-    // ### Return the number of available splits
     return n_splits;
 }
 
+///////////////////
+// nSetsPerSplit //
+///////////////////
 int BootstrapSplitter::nSetsPerSplit() const
 {
-    // ### Return the number of sets per split
+    // One single set per split.
     return 1;
 }
 



From tihocan at mail.berlios.de  Thu Apr  5 18:39:27 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 5 Apr 2007 18:39:27 +0200
Subject: [Plearn-commits] r6839 - trunk/plearn/vmat
Message-ID: <200704051639.l35GdRk6006727@sheep.berlios.de>

Author: tihocan
Date: 2007-04-05 18:39:26 +0200 (Thu, 05 Apr 2007)
New Revision: 6839

Modified:
   trunk/plearn/vmat/ExplicitSplitter.cc
Log:
Better help.


Modified: trunk/plearn/vmat/ExplicitSplitter.cc
===================================================================
--- trunk/plearn/vmat/ExplicitSplitter.cc	2007-04-05 16:36:58 UTC (rev 6838)
+++ trunk/plearn/vmat/ExplicitSplitter.cc	2007-04-05 16:39:26 UTC (rev 6839)
@@ -43,28 +43,40 @@
 namespace PLearn {
 using namespace std;
 
+//////////////////////
+// ExplicitSplitter //
+//////////////////////
 ExplicitSplitter::ExplicitSplitter()
-    :Splitter()
 {}
 
+PLEARN_IMPLEMENT_OBJECT(ExplicitSplitter,
+    "Splitter whose sets are explicitely given.",
+    "ExplicitSplitter allows one to define a splitter by providing directly\n"
+    "the datasets for each split, as a matrix of VMatrices.\n"
+    "This splitter thus ignores the 'dataset' received from setDataSet(..).\n"
+);
 
-PLEARN_IMPLEMENT_OBJECT(ExplicitSplitter, "ONE LINE DESCR",
-                        "ExplicitSplitter allows you to define a 'splitter' by giving explicitly the datasets for each split\n"
-                        "as a matrix VMatrices.\n"
-                        "(This splitter in effect ignores the 'dataset' it is given with setDataSet) \n");
-
+////////////////////
+// declareOptions //
+////////////////////
 void ExplicitSplitter::declareOptions(OptionList& ol)
 {
-    declareOption(ol, "splitsets", &ExplicitSplitter::splitsets, OptionBase::buildoption,
-                  "This is a matrix of VMat giving explicitly the datasets for each split.");
+    declareOption(ol, "splitsets", &ExplicitSplitter::splitsets,
+        OptionBase::buildoption,
+       "The i-th row of this matrix contains the sets for the i-th split.");
+
     inherited::declareOptions(ol);
 }
 
+////////////
+// build_ //
+////////////
 void ExplicitSplitter::build_()
-{
-}
+{}
 
-// ### Nothing to add here, simply calls build_
+///////////
+// build //
+///////////
 void ExplicitSplitter::build()
 {
     inherited::build();



From tihocan at mail.berlios.de  Thu Apr  5 18:41:58 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 5 Apr 2007 18:41:58 +0200
Subject: [Plearn-commits] r6840 - trunk/plearn/base
Message-ID: <200704051641.l35GfwgG009297@sheep.berlios.de>

Author: tihocan
Date: 2007-04-05 18:41:49 +0200 (Thu, 05 Apr 2007)
New Revision: 6840

Modified:
   trunk/plearn/base/HelpSystem.cc
Log:
The multi-line help (displayed in plearn help) has now each line properly preceded by '# ', so that a script obtained by redirecting a plearn help command can be used without annoying crashes.


Modified: trunk/plearn/base/HelpSystem.cc
===================================================================
--- trunk/plearn/base/HelpSystem.cc	2007-04-05 16:39:26 UTC (rev 6839)
+++ trunk/plearn/base/HelpSystem.cc	2007-04-05 16:41:49 UTC (rev 6840)
@@ -328,7 +328,9 @@
 
     // Display basic help
     s+= "## " + entry.one_line_descr + "\n\n";
-    s+= "## " + entry.multi_line_help + "\n\n";
+    string ml_help = "# " + entry.multi_line_help;
+    search_replace(ml_help, "\n", "\n# ");
+    s+= ml_help + "\n\n";
 
     if(entry.constructor) // it's an instantiable class
         obj = (*entry.constructor)();



From tihocan at mail.berlios.de  Thu Apr  5 18:43:40 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 5 Apr 2007 18:43:40 +0200
Subject: [Plearn-commits] r6841 - trunk/plearn_learners/meta
Message-ID: <200704051643.l35GhePP012020@sheep.berlios.de>

Author: tihocan
Date: 2007-04-05 18:43:39 +0200 (Thu, 05 Apr 2007)
New Revision: 6841

Modified:
   trunk/plearn_learners/meta/BaggingLearner.cc
Log:
- Better help
- Minor code reformatting to keep lines below the 80 character mark
- Added missing deep copy statements


Modified: trunk/plearn_learners/meta/BaggingLearner.cc
===================================================================
--- trunk/plearn_learners/meta/BaggingLearner.cc	2007-04-05 16:41:49 UTC (rev 6840)
+++ trunk/plearn_learners/meta/BaggingLearner.cc	2007-04-05 16:43:39 UTC (rev 6841)
@@ -49,8 +49,13 @@
 
 PLEARN_IMPLEMENT_OBJECT(
     BaggingLearner,
-    "Learner that trains several sub-learners on 'bags'",
-    "Learner that trains several sub-learners on 'bags'... (TODO: more txt)");
+    "Performs bagging on several sub-learners.",
+    "Bagging consists in training several sub-learners (all obtained by a\n"
+    "copy of the provided 'template_learner') on different subsets of the\n"
+    "training data, then aggregating their outputs in order to make a test\n"
+    "prediction (the way outputs are aggregated is governed by the 'stats'\n"
+    "option).\n"
+);
 
 BaggingLearner::BaggingLearner(PP<Splitter> splitter_, 
                                PP<PLearner> template_learner_,
@@ -68,25 +73,26 @@
 void BaggingLearner::declareOptions(OptionList& ol)
 {
     declareOption(ol, "splitter", &BaggingLearner::splitter,
-                  OptionBase::buildoption,
-                  "Splitter used to get bags(=splits)",
-                  "", OptionBase::basic_level);
+        OptionBase::buildoption,
+        "Splitter used to get bags. In each split, only the first set is\n"
+        "used (as the training set for a bag). A typical splitter used in\n"
+        "bagging is a BootstrapSplitter.", "", OptionBase::basic_level);
 
     declareOption(ol, "template_learner", &BaggingLearner::template_learner,
-                  OptionBase::buildoption,
-                  "Template for all sub-learners; deep-copied once for each bag",
-                  "", OptionBase::basic_level);
+        OptionBase::buildoption,
+        "Template for all sub-learners; deep-copied once for each bag.",
+        "", OptionBase::basic_level);
 
     declareOption(ol, "stats", &BaggingLearner::stats,
-                  OptionBase::buildoption,
-                  "Functions used to combine outputs from all learners.\n"
-                  "\t- 'A' = Average\n",
-                  "", OptionBase::basic_level);
+        OptionBase::buildoption,
+        "Statistics used to combine outputs from all learners. You can use\n"
+        "any statistic that can be computed by a StatsCollector.", "",
+        OptionBase::basic_level);
 
     declareOption(ol, "exclude_extremes", &BaggingLearner::exclude_extremes,
                   OptionBase::buildoption,
                   "If >0, sub-learners outputs are sorted and the exclude_extremes "
-                  "highest and lowest are excluded");
+                  "highest and lowest are excluded.");
                   
     declareOption(ol, "output_sub_outputs", &BaggingLearner::output_sub_outputs,
                   OptionBase::buildoption,
@@ -94,7 +100,7 @@
                   
     declareOption(ol, "learners", &BaggingLearner::learners,
                   OptionBase::learntoption,
-                  "Trained sub-learners");
+                  "Trained sub-learners.");
 
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
@@ -115,14 +121,26 @@
     build_();
 }
 
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
 void BaggingLearner::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
-    deepCopyField(splitter, copies);
+    deepCopyField(splitter,         copies);
     deepCopyField(template_learner, copies);
-    deepCopyField(learners, copies);
+    deepCopyField(stats,            copies);
+    deepCopyField(learners,         copies);
+    deepCopyField(learners_outputs, copies);
+    deepCopyField(outputs,          copies);
+    deepCopyField(learner_costs,    copies);
+    deepCopyField(last_test_input,  copies);
+    // TODO Do we need to deep-copy stcol?
 }
 
+////////////////
+// outputsize //
+////////////////
 int BaggingLearner::outputsize() const
 { 
     PLASSERT(template_learner);
@@ -133,6 +151,9 @@
     return sz;
 }
 
+////////////
+// forget //
+////////////
 void BaggingLearner::forget()
 {
     for(int i= 0; i < learners.length(); ++i)
@@ -140,6 +161,9 @@
     inherited::forget();
 }
 
+///////////
+// train //
+///////////
 void BaggingLearner::train()
 {
     PLASSERT(train_set);
@@ -261,6 +285,9 @@
     PLASSERT( stage == 1 );
 }
 
+///////////////////
+// computeOutput //
+///////////////////
 void BaggingLearner::computeOutput(const Vec& input, Vec& output) const
 {
     int nout = outputsize();
@@ -314,6 +341,9 @@
                 output[i++]= learners_outputs(j,k);
 }
 
+/////////////////////////////
+// computeCostsFromOutputs //
+/////////////////////////////
 void BaggingLearner::computeCostsFromOutputs(const Vec& input, const Vec& output,
                                              const Vec& target, Vec& costs) const
 {
@@ -335,6 +365,9 @@
 
 }
 
+//////////////////////
+// getTestCostNames //
+//////////////////////
 TVec<string> BaggingLearner::getTestCostNames() const
 {
     PLASSERT(splitter);
@@ -350,6 +383,9 @@
     return costnames;
 }
 
+///////////////////////
+// getTrainCostNames //
+///////////////////////
 TVec<string> BaggingLearner::getTrainCostNames() const
 {
     return TVec<string>(); // for now
@@ -391,6 +427,9 @@
     return template_learner->isStatefulLearner();
 }
 
+////////////////////
+// setTrainingSet //
+////////////////////
 void BaggingLearner::setTrainingSet(VMat training_set, bool call_forget)
 {
     PLASSERT(template_learner);
@@ -400,6 +439,9 @@
     inherited::setTrainingSet(training_set, call_forget);
 }
 
+////////////////////
+// getOutputNames //
+////////////////////
 TVec<string> BaggingLearner::getOutputNames() const
 {
     PLASSERT(template_learner);



From tihocan at mail.berlios.de  Thu Apr  5 18:46:51 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 5 Apr 2007 18:46:51 +0200
Subject: [Plearn-commits] r6842 - trunk/plearn_learners/generic
Message-ID: <200704051646.l35GkpSA016337@sheep.berlios.de>

Author: tihocan
Date: 2007-04-05 18:46:50 +0200 (Thu, 05 Apr 2007)
New Revision: 6842

Modified:
   trunk/plearn_learners/generic/AddCostToLearner.cc
Log:
- Better help
- Using is_equal when comparing floating numbers for class error and binary class error, which should be safer
- Added explicit error message when the class error cannot be computed


Modified: trunk/plearn_learners/generic/AddCostToLearner.cc
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.cc	2007-04-05 16:43:39 UTC (rev 6841)
+++ trunk/plearn_learners/generic/AddCostToLearner.cc	2007-04-05 16:46:50 UTC (rev 6842)
@@ -116,15 +116,23 @@
                   "be trained without the bag information (see SumOverBagsVariable for info on bags).");
 
     declareOption(ol, "costs", &AddCostToLearner::costs, OptionBase::buildoption,
-                  "The costs to be added:\n"
-                  " - 'class_error': 1 if (t != o), 0 otherwise\n"
-                  " - 'binary_class_error': same as class error with output = (o > 0.5)\n"
-                  " - 'lift_output': to compute the lift cost (for the positive class)\n"
-                  " - 'opposite_lift_output': to compute the lift cost (for the negative) class\n"
-                  " - 'cross_entropy': t*log(o) + (1-t)*log(1-o)\n"
-                  " - 'mse': the mean squared error (o - t)^2\n"
-                  " - 'squared_norm_reconstruction_error': | ||i||^2 - ||o||^2 |\n"
-        );
+        "The costs to be added:\n"
+        " - 'class_error': classification error. If the sub-learner's output\n"
+        "   has the same length as the target vector, then they are compared\n"
+        "   component-wise. Otherwise, the target must be a one-dimensional\n"
+        "   vector (an integer corresponding to the class), and the output\n"
+        "   from the sub-learner is interpreted as a vector of weights for\n"
+        "   each class."
+        " - 'binary_class_error': classification error for a one-dimensional\n"
+        "   target that must be either 0 or 1. The output must also be one-\n"
+        "   dimensional, and is interpreted as the predicted probability for\n"
+        "   class 1 (thus class 1 is chosen when the output is > 0.5)\n"
+        " - 'lift_output': to compute the lift cost (for the positive class)\n"
+        " - 'opposite_lift_output': to compute the lift cost (for the negative) class\n"
+        " - 'cross_entropy': t*log(o) + (1-t)*log(1-o)\n"
+        " - 'mse': the mean squared error (o - t)^2\n"
+        " - 'squared_norm_reconstruction_error': | ||i||^2 - ||o||^2 |\n"
+    );
 
     declareOption(ol, "force_output_to_target_interval", &AddCostToLearner::force_output_to_target_interval, OptionBase::buildoption,
                   "If set to 1 and 'rescale_output' is also set to 1, then the scaled output\n"
@@ -425,25 +433,29 @@
         } else if (c == "class_error") {
             int output_length = sub_learner_output.length();
             bool good = true;
-            if (fast_exact_is_equal(output_length, target_length)) {
+            if (output_length == target_length) {
                 for (int k = 0; k < desired_target.length(); k++)
-                    if (!fast_exact_is_equal(desired_target[k],
-                                             sub_learner_output[k])) {
+                    if (!is_equal(desired_target[k],
+                                  sub_learner_output[k])) {
                         good = false;
                         break;
                     }
             } else if (target_length == 1) {
                 // We assume the target is a number between 0 and c-1, and the output
                 // is a vector of length c giving the weight for each class.
-                good = (argmax(sub_learner_output) == int(desired_target[0]));
+                good = is_equal(argmax(sub_learner_output), desired_target[0]);
+            } else {
+                PLERROR("In AddCostToLearner::computeCostsFromOutputs - Wrong "
+                        "output and/or target for the 'class_error' cost");
             }
-            costs[ind_cost] = real(!good);
+            costs[ind_cost] = good ? 0 : 1;
         } else if (c == "binary_class_error") {
             PLASSERT( target_length == 1 );
             real t = desired_target[0];
             PLASSERT( fast_exact_is_equal(t, 0) || fast_exact_is_equal(t, 1));
-            costs[ind_cost] = real((sub_learner_output[0] > 0.5) ==
-                                   fast_exact_is_equal(t,0));
+            PLASSERT( sub_learner_output.length() == 1 );
+            real predict = sub_learner_output[0] > 0.5 ? 1 : 0;
+            costs[ind_cost] = is_equal(t, predict) ? 0 : 1;
         } else if (c == "mse") {
             costs[ind_cost] = powdistance(desired_target, sub_learner_output);
         } else if (c == "squared_norm_reconstruction_error") {



From tihocan at mail.berlios.de  Thu Apr  5 20:50:48 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 5 Apr 2007 20:50:48 +0200
Subject: [Plearn-commits] r6843 - trunk/plearn/io
Message-ID: <200704051850.l35IomCA010352@sheep.berlios.de>

Author: tihocan
Date: 2007-04-05 20:50:48 +0200 (Thu, 05 Apr 2007)
New Revision: 6843

Modified:
   trunk/plearn/io/PStream.h
Log:
Typo fix in comment

Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2007-04-05 16:46:50 UTC (rev 6842)
+++ trunk/plearn/io/PStream.h	2007-04-05 18:50:48 UTC (rev 6843)
@@ -339,7 +339,7 @@
         return *this;
     }
 
-    //! Reads ultil eof, returns whole contents as a string
+    //! Reads until an EOF is found, and returns the whole content as a string.
     string readAll();
 
     //! Reads the next character and launches a PLERROR if it is different from



From tihocan at mail.berlios.de  Thu Apr  5 21:17:53 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 5 Apr 2007 21:17:53 +0200
Subject: [Plearn-commits] r6844 - trunk
Message-ID: <200704051917.l35JHr7L011635@sheep.berlios.de>

Author: tihocan
Date: 2007-04-05 21:17:53 +0200 (Thu, 05 Apr 2007)
New Revision: 6844

Modified:
   trunk/pymake.config.model
Log:
cpp_variables is meant to store only names of macro variables that are not modified in the code

Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2007-04-05 18:50:48 UTC (rev 6843)
+++ trunk/pymake.config.model	2007-04-05 19:17:53 UTC (rev 6844)
@@ -615,10 +615,7 @@
               description = 'the installed version of python is 2.5.X',
               cpp_definitions = ['PL_PYTHON_VERSION=250'] )
 
-cpp_variables += ['PL_PYTHON_VERSION=250','PL_PYTHON_VERSION=240','PL_PYTHON_VERSION=230']
 
-
-
 #####  Error Behavior  ######################################################
 
 # Not used any more, 'USE_EXCEPTIONS' is defined earlier in this file.



From saintmlx at mail.berlios.de  Fri Apr  6 03:49:27 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Fri, 6 Apr 2007 03:49:27 +0200
Subject: [Plearn-commits] r6845 - in trunk: commands/PLearnCommands
	plearn/base plearn/vmat plearn_learners/generic plearn_learners/meta
Message-ID: <200704060149.l361nR9k018729@sheep.berlios.de>

Author: saintmlx
Date: 2007-04-06 03:49:24 +0200 (Fri, 06 Apr 2007)
New Revision: 6845

Modified:
   trunk/commands/PLearnCommands/HelpCommand.cc
   trunk/commands/PLearnCommands/plearn_main.cc
   trunk/plearn/base/HelpSystem.cc
   trunk/plearn/base/HelpSystem.h
   trunk/plearn/base/Option.h
   trunk/plearn/base/OptionBase.cc
   trunk/plearn/base/OptionBase.h
   trunk/plearn/vmat/BootstrapVMatrix.cc
   trunk/plearn_learners/generic/NNet.cc
   trunk/plearn_learners/generic/PLearner.cc
   trunk/plearn_learners/meta/AdaBoost.cc
   trunk/plearn_learners/meta/BaggingLearner.cc
Log:
- removed 'normal' option level
- changed param. order for declareOption
- help on options now sorted by option level, w/ headings for each level
- removed --option-level from plearn_main; now in HelpCommand
- removed remote methods from text help



Modified: trunk/commands/PLearnCommands/HelpCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/HelpCommand.cc	2007-04-05 19:17:53 UTC (rev 6844)
+++ trunk/commands/PLearnCommands/HelpCommand.cc	2007-04-06 01:49:24 UTC (rev 6845)
@@ -134,6 +134,11 @@
     else
     {
         string about = args[0];
+        
+        if(args.size() > 1)//is option level present?
+            OptionBase::setCurrentOptionLevel(
+                OptionBase::optionLevelFromString(args[1]));
+
         if(extract_extension(about)==".plearn") // help is asked about a plearn script
             helpAboutScript(about);//TODO: move to HelpSystem
         if(about=="scripts")

Modified: trunk/commands/PLearnCommands/plearn_main.cc
===================================================================
--- trunk/commands/PLearnCommands/plearn_main.cc	2007-04-05 19:17:53 UTC (rev 6844)
+++ trunk/commands/PLearnCommands/plearn_main.cc	2007-04-06 01:49:24 UTC (rev 6845)
@@ -173,6 +173,7 @@
     // set verbosity level now so that it is valid for the rest of global_options
     PL_Log::instance().verbosity( verbosity_value );
 
+/*
     int option_level_pos= findpos(command_line, "--option-level");
     int option_level_value_pos= -1;
     if (option_level_pos != -1)
@@ -182,6 +183,7 @@
             PLERROR("Option --option-level must be followed by an OptionLevel.");
         OptionBase::setCurrentOptionLevel(OptionBase::optionLevelFromString(command_line[option_level_value_pos]));
     }
+*/
 
     // Option to enable logging for the specified modules, specified as
     // --enable-logging module1,module2,module3,... i.e. as a comma-separated
@@ -244,9 +246,9 @@
              c != global_calendar_pos        &&
              c != global_calendar_value_pos  &&
              c != servers_pos                &&
-             c != serversfile_pos            &&
+             c != serversfile_pos            /*&&
              c != option_level_pos           &&
-             c != option_level_value_pos
+             c != option_level_value_pos*/
             )
         {
             if ( the_command == "" )

Modified: trunk/plearn/base/HelpSystem.cc
===================================================================
--- trunk/plearn/base/HelpSystem.cc	2007-04-05 19:17:53 UTC (rev 6844)
+++ trunk/plearn/base/HelpSystem.cc	2007-04-06 01:49:24 UTC (rev 6845)
@@ -34,6 +34,8 @@
 
 #include "HelpSystem.h"
 
+#include <algorithm>
+
 #include "stringutils.h"    //!< For addprefix.
 #include "tostring.h"
 #include <plearn/misc/HTMLUtils.h>
@@ -355,12 +357,12 @@
         "# (only those that can be instantiated) \n"
         "################################################################## \n\n";
     s+= addprefix("# ", helpDerivedClasses(classname));
-
+/*
     s+= "\n\n################################################################## \n";
     s+= "## Remote-callable methods of " + classname + " \n"
         "################################################################## \n\n";
     s+= addprefix("# ", helpMethods(classname));
-
+*/
     s+= "\n\n################################################################## \n\n";
 
     return s;
@@ -533,33 +535,77 @@
 
 vector<string> HelpSystem::listClassOptions(const string& classname)
 {
-    const TypeMapEntry& e= TypeFactory::instance().getTypeMapEntry(classname);
-    OptionList& optlist= (*e.getoptionlist_method)();
-    int nopts= optlist.size();
-    vector<string> options(nopts);
-    for(int i= 0; i < nopts; ++i) options[i]= optlist[i]->optionname();
-    return options;
+    vector<pair<OptionBase::OptionLevel, string> > optswl=
+        listClassOptionsWithLevels(classname);
+    int nopts= optswl.size();
+    vector<string> opts(nopts);
+    for(int i= 0; i < nopts; ++i)
+        opts[i]= optswl[i].second;
+    return opts;
 }
 
 vector<string> HelpSystem::listBuildOptions(const string& classname)
 {
+    vector<pair<OptionBase::OptionLevel, string> > optswl=
+        listBuildOptionsWithLevels(classname);
+    int nopts= optswl.size();
+    vector<string> opts(nopts);
+    for(int i= 0; i < nopts; ++i)
+        opts[i]= optswl[i].second;
+    return opts;
+}
+
+vector<pair<OptionBase::OptionLevel, string> >
+HelpSystem::listClassOptionsWithLevels(const string& classname, 
+                                       const OptionBase::flag_t& flags)
+{
     const TypeMapEntry& e= TypeFactory::instance().getTypeMapEntry(classname);
     OptionList& optlist= (*e.getoptionlist_method)();
-    vector<string> options;
-    for(OptionList::iterator it= optlist.begin();
-        it != optlist.end(); ++it)
-        if((*it)->flags() & OptionBase::buildoption)
-            options.push_back((*it)->optionname());
+    int nopts= optlist.size();
+    vector<pair<OptionBase::OptionLevel, string> > options;
+    for(int i= 0; i < nopts; ++i)
+        if(optlist[i]->flags() & flags)
+            options.push_back(make_pair(optlist[i]->level(), 
+                                        optlist[i]->optionname()));
     return options;
 }
 
+vector<pair<OptionBase::OptionLevel, string> >
+HelpSystem::listBuildOptionsWithLevels(const string& classname)
+{ return listClassOptionsWithLevels(classname, OptionBase::buildoption); }
+
 string HelpSystem::helpClassOptions(const string& classname)
 {
     string s= "";
-    vector<string> options= listClassOptions(classname);
-    for(vector<string>::iterator it= options.begin();
-        it != options.end(); ++it)
-        s+= helpOnOption(classname, *it) + '\n';
+    vector<pair<OptionBase::OptionLevel, string> > options= 
+        listClassOptionsWithLevels(classname);
+    sort(options.begin(), options.end());
+    OptionBase::OptionLevel lvl= 0;
+    OptionBase::OptionLevel curlvl= OptionBase::getCurrentOptionLevel();
+    int nbeyond= 0;
+    for(vector<pair<OptionBase::OptionLevel, string> >::iterator 
+            it= options.begin(); it != options.end(); ++it)
+    {
+        if(lvl != it->first)
+        {
+            lvl= it->first;
+            if(lvl <= curlvl)
+                s+= "##############################\n"
+                    "# Options for level " 
+                    + OptionBase::optionLevelToString(lvl) 
+                    + "\n\n";
+        }
+        if(lvl <= curlvl)
+            s+= helpOnOption(classname, it->second) + '\n';
+        else
+            ++nbeyond;
+    }
+    if(nbeyond > 0)
+        s+= "##############################\n"
+            "# " + tostring(nbeyond)
+            + " hidden options beyond level "
+            + OptionBase::optionLevelToString(curlvl)
+            + "\n\n";
     return s;
 }
 
@@ -571,7 +617,7 @@
     if(flags & OptionBase::buildoption 
        && opt->level() <= OptionBase::getCurrentOptionLevel())
         s+= addprefix("# ", opt->optiontype() + ": " + opt->description())
-            + addprefix("# ", "*OptionLevel: " + opt->levelString())
+            //+ addprefix("# ", "*OptionLevel: " + opt->levelString())
             + opt->optionname() + " = " 
             + getOptionDefaultVal(classname, optionname) + " ;\n\n";
 

Modified: trunk/plearn/base/HelpSystem.h
===================================================================
--- trunk/plearn/base/HelpSystem.h	2007-04-05 19:17:53 UTC (rev 6844)
+++ trunk/plearn/base/HelpSystem.h	2007-04-06 01:49:24 UTC (rev 6845)
@@ -158,6 +158,16 @@
      //! Returns the list of build options for the class with the given name
     static vector<string> listBuildOptions(const string& classname);
 
+     //! Returns the list of all options for the class with the given name
+    static vector<pair<OptionBase::OptionLevel, string> > 
+    listClassOptionsWithLevels(const string& classname,
+                               const OptionBase::flag_t& flags= 
+                               OptionBase::getCurrentFlags());
+
+     //! Returns the list of build options for the class with the given name
+    static vector<pair<OptionBase::OptionLevel, string> > 
+    listBuildOptionsWithLevels(const string& classname);
+
      //! Returns the list of options for the class with the given name, as text
     static string helpClassOptions(const string& classname);
 

Modified: trunk/plearn/base/Option.h
===================================================================
--- trunk/plearn/base/Option.h	2007-04-05 19:17:53 UTC (rev 6844)
+++ trunk/plearn/base/Option.h	2007-04-06 01:49:24 UTC (rev 6845)
@@ -215,8 +215,8 @@
                           OptionType ObjectType::* member_ptr, //!< &YourClass::your_field
                           OptionBase::flag_t flags,            //!< see the flags in OptionBase
                           const string& description,           //!< a description of the option
-                          const string& defaultval="",         //!< default value for this option, as set by the default constructor
-                          const OptionBase::OptionLevel level= OptionBase::default_level) //!< Option level (see OptionBase)
+                          const OptionBase::OptionLevel level= OptionBase::default_level, //!< Option level (see OptionBase)
+                          const string& defaultval="")         //!< default value for this option, as set by the default constructor
 {
     ol.push_back(new Option<ObjectType, OptionType>(optionname, member_ptr, flags, 
                                                     TypeTraits<OptionType>::name(), 
@@ -230,8 +230,8 @@
                           OptionType* ObjectType::* member_ptr,
                           OptionBase::flag_t flags,
                           const string& description,
-                          const string& defaultval="",
-                          const OptionBase::OptionLevel level= OptionBase::default_level)
+                          const OptionBase::OptionLevel level= OptionBase::default_level,
+                          const string& defaultval="")
 {
     ol.push_back(new Option<ObjectType, OptionType *>(optionname, member_ptr, flags,
                                                       TypeTraits<OptionType *>::name(), 
@@ -245,8 +245,8 @@
                           TVec<VecElementType> ObjectType::* member_ptr,
                           OptionBase::flag_t flags,
                           const string& description,
-                          const string& defaultval="",
-                          const OptionBase::OptionLevel level= OptionBase::default_level)
+                          const OptionBase::OptionLevel level= OptionBase::default_level,
+                          const string& defaultval="")
 {
     ol.push_back(new TVecOption<ObjectType, VecElementType>(
                      optionname, member_ptr, flags,
@@ -263,8 +263,8 @@
                             OptionType ObjectType::* member_ptr, //!< &YourClass::your_field
                             OptionBase::flag_t flags,            //! see the flags in OptionBase
                             const string& description,           //!< a description of the option
-                            const string& defaultval="",         //!< default value for this option, as set by the default constructor
-                            const OptionBase::OptionLevel level= OptionBase::default_level) //!< Option level (see OptionBase)
+                            const OptionBase::OptionLevel level= OptionBase::default_level, //!< Option level (see OptionBase)
+                            const string& defaultval="")         //!< default value for this option, as set by the default constructor
 {
     bool found = false;
     for (OptionList::iterator it = ol.begin(); !found && it != ol.end(); it++) {
@@ -289,8 +289,8 @@
                             OptionType* ObjectType::* member_ptr,//!< &YourClass::your_field
                             OptionBase::flag_t flags,            //! see the flags in OptionBase
                             const string& description,           //!< a description of the option
-                            const string& defaultval="",         //!< default value for this option, as set by the default constructor
-                            const OptionBase::OptionLevel level= OptionBase::default_level) //!< Option level (see OptionBase)
+                            const OptionBase::OptionLevel level= OptionBase::default_level, //!< Option level (see OptionBase)
+                            const string& defaultval="")         //!< default value for this option, as set by the default constructor
 {
     bool found = false;
     for (OptionList::iterator it = ol.begin(); !found && it != ol.end(); it++) {
@@ -315,8 +315,8 @@
                             TVec<VecElementType> ObjectType::* member_ptr,
                             OptionBase::flag_t flags,
                             const string& description,
-                            const string& defaultval="",
-                            const OptionBase::OptionLevel level= OptionBase::default_level)
+                            const OptionBase::OptionLevel level= OptionBase::default_level,
+                            const string& defaultval="")
 {
     bool found = false;
     for (OptionList::iterator it = ol.begin(); !found && it != ol.end(); it++) {

Modified: trunk/plearn/base/OptionBase.cc
===================================================================
--- trunk/plearn/base/OptionBase.cc	2007-04-05 19:17:53 UTC (rev 6844)
+++ trunk/plearn/base/OptionBase.cc	2007-04-06 01:49:24 UTC (rev 6845)
@@ -64,13 +64,12 @@
                                                 | OptionBase::nontraversable
                                                 | OptionBase::remotetransmit);
 
-const OptionBase::OptionLevel OptionBase::basic_level= 100;
-const OptionBase::OptionLevel OptionBase::normal_level= 200;
+const OptionBase::OptionLevel OptionBase::basic_level= 200;
 const OptionBase::OptionLevel OptionBase::advanced_level= 400;
 const OptionBase::OptionLevel OptionBase::expert_level= 800;
 const OptionBase::OptionLevel OptionBase::experimental_level= 9999;
 const OptionBase::OptionLevel OptionBase::deprecated_level= 99999999;
-const OptionBase::OptionLevel OptionBase::default_level= OptionBase::normal_level;
+const OptionBase::OptionLevel OptionBase::default_level= OptionBase::basic_level;
 OptionBase::OptionLevel OptionBase::current_option_level_= OptionBase::default_level;
 
 OptionBase::OptionBase(const string& optionname, flag_t flags,
@@ -180,7 +179,6 @@
     if(str_to_level.size() == 0)
     {
         str_to_level["basic"]= basic_level;
-        str_to_level["normal"]= normal_level;
         str_to_level["advanced"]= advanced_level;
         str_to_level["expert"]= expert_level;
         str_to_level["experimental"]= experimental_level;

Modified: trunk/plearn/base/OptionBase.h
===================================================================
--- trunk/plearn/base/OptionBase.h	2007-04-05 19:17:53 UTC (rev 6844)
+++ trunk/plearn/base/OptionBase.h	2007-04-06 01:49:24 UTC (rev 6845)
@@ -133,7 +133,6 @@
      */
     typedef unsigned int OptionLevel;
     static const OptionLevel basic_level; //!< mandatory options
-    static const OptionLevel normal_level; //!< simple, useful options
     static const OptionLevel advanced_level; //!< useful but seldom used
     static const OptionLevel expert_level; //!< more than advanced, not experimental
     static const OptionLevel experimental_level; //!< only a few people understand those

Modified: trunk/plearn/vmat/BootstrapVMatrix.cc
===================================================================
--- trunk/plearn/vmat/BootstrapVMatrix.cc	2007-04-05 19:17:53 UTC (rev 6844)
+++ trunk/plearn/vmat/BootstrapVMatrix.cc	2007-04-06 01:49:24 UTC (rev 6845)
@@ -119,12 +119,12 @@
     declareOption(ol, "allow_repetitions", &BootstrapVMatrix::allow_repetitions, 
                   OptionBase::buildoption,
                   "Wether examples should be allowed to appear each more than once.",
-                  "", OptionBase::advanced_level);
+                  OptionBase::advanced_level);
 
     declareOption(ol, "own_seed", &BootstrapVMatrix::own_seed,
                   (OptionBase::learntoption | OptionBase::nosave),
-        "DEPRECATED: old random generator seed",
-        "", OptionBase::deprecated_level);
+                  "DEPRECATED: old random generator seed",
+                  OptionBase::deprecated_level);
 
     inherited::declareOptions(ol);
 

Modified: trunk/plearn_learners/generic/NNet.cc
===================================================================
--- trunk/plearn_learners/generic/NNet.cc	2007-04-05 19:17:53 UTC (rev 6844)
+++ trunk/plearn_learners/generic/NNet.cc	2007-04-06 01:49:24 UTC (rev 6845)
@@ -292,14 +292,14 @@
         "A user-specified NAry Var that computes the output of the first hidden layer\n"
         "from the network input vector and a set of parameters. Its first argument should\n"
         "be the network input and the remaining arguments the tunable parameters.\n",
-        "", OptionBase::advanced_level);
+        OptionBase::advanced_level);
 
     declareOption(
         ol, "first_hidden_layer_is_output",
         &NNet::first_hidden_layer_is_output, OptionBase::buildoption,
         "If true and a 'first_hidden_layer' Var is provided, then this layer\n"
         "will be considered as the NNet output before transfer function.",
-        "", OptionBase::advanced_level);
+        OptionBase::advanced_level);
 
     declareOption(
         ol, "n_non_params_in_first_hidden_layer",
@@ -308,7 +308,7 @@
         "Number of elements in the 'varray' option of 'first_hidden_layer'\n"
         "that are not updated parameters (assumed to be the last elements in\n"
         "'varray').",
-        "", OptionBase::advanced_level);
+        OptionBase::advanced_level);
 
     declareOption(
         ol, "transpose_first_hidden_layer",
@@ -316,7 +316,7 @@
         OptionBase::buildoption, 
         "If true and the 'first_hidden_layer' option is set, this layer will\n"
         "be transposed, and the input variable given to this layer will also\n"
-        "be transposed.", "", OptionBase::advanced_level);
+        "be transposed.", OptionBase::advanced_level);
 
     declareOption(
         ol, "margin", &NNet::margin, OptionBase::buildoption, 

Modified: trunk/plearn_learners/generic/PLearner.cc
===================================================================
--- trunk/plearn_learners/generic/PLearner.cc	2007-04-05 19:17:53 UTC (rev 6844)
+++ trunk/plearn_learners/generic/PLearner.cc	2007-04-06 01:49:24 UTC (rev 6845)
@@ -198,7 +198,7 @@
         "DEPRECATED: use parallelize_here instead.\n"
         "Max number of computation servers to use in parallel with the main process.\n"
         "If <=0 no parallelization will occur at this level.\n",
-        "", OptionBase::deprecated_level);
+        OptionBase::deprecated_level);
 
     declareOption(
         ol, "save_trainingset_prefix", &PLearner::save_trainingset_prefix,

Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2007-04-05 19:17:53 UTC (rev 6844)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2007-04-06 01:49:24 UTC (rev 6845)
@@ -53,9 +53,9 @@
 using namespace std;
 
 AdaBoost::AdaBoost()
-    : found_zero_error_weak_learner(0),
-      sum_voting_weights(0.0), 
+    : sum_voting_weights(0.0), 
       initial_sum_weights(0.0),
+      found_zero_error_weak_learner(0),
       target_error(0.5), 
       output_threshold(0.5), 
       compute_training_error(1), 

Modified: trunk/plearn_learners/meta/BaggingLearner.cc
===================================================================
--- trunk/plearn_learners/meta/BaggingLearner.cc	2007-04-05 19:17:53 UTC (rev 6844)
+++ trunk/plearn_learners/meta/BaggingLearner.cc	2007-04-06 01:49:24 UTC (rev 6845)
@@ -76,17 +76,17 @@
         OptionBase::buildoption,
         "Splitter used to get bags. In each split, only the first set is\n"
         "used (as the training set for a bag). A typical splitter used in\n"
-        "bagging is a BootstrapSplitter.", "", OptionBase::basic_level);
+        "bagging is a BootstrapSplitter.", OptionBase::basic_level);
 
     declareOption(ol, "template_learner", &BaggingLearner::template_learner,
-        OptionBase::buildoption,
-        "Template for all sub-learners; deep-copied once for each bag.",
-        "", OptionBase::basic_level);
+                  OptionBase::buildoption,
+                  "Template for all sub-learners; deep-copied once for each bag.",
+                  OptionBase::basic_level);
 
     declareOption(ol, "stats", &BaggingLearner::stats,
         OptionBase::buildoption,
         "Statistics used to combine outputs from all learners. You can use\n"
-        "any statistic that can be computed by a StatsCollector.", "",
+        "any statistic that can be computed by a StatsCollector.",
         OptionBase::basic_level);
 
     declareOption(ol, "exclude_extremes", &BaggingLearner::exclude_extremes,



From tihocan at mail.berlios.de  Fri Apr  6 15:38:36 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 6 Apr 2007 15:38:36 +0200
Subject: [Plearn-commits] r6846 - trunk/plearn_learners/classifiers
Message-ID: <200704061338.l36DcaXJ028755@sheep.berlios.de>

Author: tihocan
Date: 2007-04-06 15:38:36 +0200 (Fri, 06 Apr 2007)
New Revision: 6846

Modified:
   trunk/plearn_learners/classifiers/BinaryStump.cc
Log:
Fixed some typos in comments, and added debug code

Modified: trunk/plearn_learners/classifiers/BinaryStump.cc
===================================================================
--- trunk/plearn_learners/classifiers/BinaryStump.cc	2007-04-06 01:49:24 UTC (rev 6845)
+++ trunk/plearn_learners/classifiers/BinaryStump.cc	2007-04-06 13:38:36 UTC (rev 6846)
@@ -43,6 +43,9 @@
 
 #include "BinaryStump.h"
 
+#define PL_LOG_MODULE_NAME "BinaryStump"
+#include <plearn/io/pl_log.h>
+
 namespace PLearn {
 using namespace std;
 
@@ -284,13 +287,13 @@
 
         // We choose as the first stump to consider, the stump that classifies
         // in the most frequent class
-        // every points which have their first coordonate greater than
-        // the smallest value for this coordonate in the training set MINUS ONE.
+        // every points which have their first coordinate greater than
+        // the smallest value for this coordinate in the training set MINUS ONE.
         // This approximatly corresponds to classify any points to the most
         // frequent class.
 
         feature = 0;
-        threshold = sf[0].second-1; 
+        threshold = sf[0].second-1;  // TODO Why? (done below already?)
         PP<ProgressBar> pb;
         if(report_progress)
             pb = new ProgressBar("Finding best stump",inputsize()*sf.length());
@@ -315,8 +318,11 @@
             //for(int i=0; i<sf.length();i++)
             qsort_vec(sf,buffer);
       
-            if(d==0) // initialize threshold
+            if(d==0) { // initialize threshold
                 threshold = sf[0].second-1; 
+                DBG_MODULE_LOG << "Initializing threshold <- " << threshold <<
+                    endl;
+            }
 
             real w_sum_l_1 = 0;
             real w_sum_l = 0;
@@ -351,15 +357,22 @@
                 }
       
                 // We choose the first stump that minimizes the
-                // weighted error
-
-                if(best_error > c_w_sum_error)
+                // weighted error.
+                if (best_error > c_w_sum_error)
                 {
                     best_error = c_w_sum_error;
           
                     tag = w_sum_error_1 > w_sum - w_sum_error_1 ? 0 : 1;
                     threshold = (f1+f2)/2;
+                    DBG_MODULE_LOG << "Updating treshold <- " << threshold <<
+                        " (c_w_sum_error = " << c_w_sum_error <<
+                        ", best_error = " << best_error << ")" << endl;
+
                     feature = d;
+                } else {
+                    DBG_MODULE_LOG << "No update (c_w_sum_error = " <<
+                        c_w_sum_error << ", best_error = " << best_error << ")"
+                        << endl;
                 }
             }
             prog++;



From yoshua at mail.berlios.de  Sat Apr  7 03:34:57 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Sat, 7 Apr 2007 03:34:57 +0200
Subject: [Plearn-commits] r6847 - in trunk: plearn/misc
	plearn_learners/generic/EXPERIMENTAL
Message-ID: <200704070134.l371YvDk010722@sheep.berlios.de>

Author: yoshua
Date: 2007-04-07 03:34:55 +0200 (Sat, 07 Apr 2007)
New Revision: 6847

Modified:
   trunk/plearn/misc/Calendar.cc
   trunk/plearn/misc/Calendar.h
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.cc
Log:

Problemes de compilation en -float corriges.


Modified: trunk/plearn/misc/Calendar.cc
===================================================================
--- trunk/plearn/misc/Calendar.cc	2007-04-06 13:38:36 UTC (rev 6846)
+++ trunk/plearn/misc/Calendar.cc	2007-04-07 01:34:55 UTC (rev 6847)
@@ -435,12 +435,12 @@
 }
 
 
-Vec Calendar::remote_getGlobalCalendar(string calendar_name)
+JTimeVec Calendar::remote_getGlobalCalendar(string calendar_name)
 {
     const Calendar* cal = getGlobalCalendar(calendar_name);
     if (! cal) {
         PLERROR("Global calendar '%s' not found", calendar_name.c_str());
-        return Vec();                        //!< Shut up compiler
+        return JTimeVec();                        //!< Shut up compiler
     }
     else
         return cal->timestamps_;

Modified: trunk/plearn/misc/Calendar.h
===================================================================
--- trunk/plearn/misc/Calendar.h	2007-04-06 13:38:36 UTC (rev 6846)
+++ trunk/plearn/misc/Calendar.h	2007-04-07 01:34:55 UTC (rev 6847)
@@ -305,7 +305,7 @@
     //#####  Remote-Callables  ################################################
     
     void remote_setGlobalCalendar(string calendar_name, Vec calendar_dates);
-    Vec remote_getGlobalCalendar(string calendar_name);
+    JTimeVec remote_getGlobalCalendar(string calendar_name);
 };
 
 // Declares a few other classes and functions related to this class

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.cc	2007-04-06 13:38:36 UTC (rev 6846)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.cc	2007-04-07 01:34:55 UTC (rev 6847)
@@ -203,7 +203,7 @@
     int n = n_eigen+i;
     Xt.resize(n+1,n_dim);
     Vec newX = Xt(n);
-    real rn = pow(gamma,-0.5*(i+1));
+    real rn = pow(gamma,real(-0.5*(i+1)));
     multiply(g,rn,newX);
     G.resize(n+1,n+1);
     Vec newG=G(n);



From laulysta at mail.berlios.de  Sat Apr  7 04:43:42 2007
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Sat, 7 Apr 2007 04:43:42 +0200
Subject: [Plearn-commits] r6848 - trunk/plearn_learners_experimental
Message-ID: <200704070243.l372hgC3013547@sheep.berlios.de>

Author: laulysta
Date: 2007-04-07 04:43:40 +0200 (Sat, 07 Apr 2007)
New Revision: 6848

Modified:
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h
Log:
With a dynamic connection between the visible layers


Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2007-04-07 01:34:55 UTC (rev 6847)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2007-04-07 02:43:40 UTC (rev 6848)
@@ -111,6 +111,11 @@
                   "OnlineLearningModule corresponding to dynamic links "
                   "between RBMs' hidden layers");
 
+    declareOption(ol, "visible_connections", &DynamicallyLinkedRBMsModel::visible_connections,
+                  OptionBase::buildoption,
+                  "OnlineLearningModule corresponding to dynamic links "
+                  "between RBMs' visible layers");
+
     /*
     declareOption(ol, "", &DynamicallyLinkedRBMsModel::,
                   OptionBase::learntoption,
@@ -171,6 +176,9 @@
         pos_up_values.resize(hidden_layer->size);
         hidden_layer_target.resize(hidden_layer->size);
         hidden_layer_sample.resize(hidden_layer->size);
+        visible_layer_sample.resize(visible_size);
+        previous_hidden_layer.resize(hidden_layer->size);
+        previous_visible_layer.resize(visible_size);
 
         visible_layer->size = visible_size;
 
@@ -180,17 +188,22 @@
         dynamic_connections->input_size = hidden_layer->size;
         dynamic_connections->output_size = hidden_layer->size;
 
+        visible_connections->input_size = visible_size;
+        visible_connections->output_size = visible_size;
+
         // Set random generators
         visible_layer->random_gen = random_gen;
         hidden_layer->random_gen = random_gen;
         connections->random_gen = random_gen;
         dynamic_connections->random_gen = random_gen;
+        visible_connections->random_gen = random_gen;
 
         // Build components
         visible_layer->build();
         hidden_layer->build();
         connections->build();
         dynamic_connections->build();
+        visible_connections->build();
     }
 
 }
@@ -211,15 +224,20 @@
     deepCopyField( hidden_layer , copies);
     deepCopyField( dynamic_connections , copies);
     deepCopyField( dynamic_connections_copy , copies);
+    deepCopyField( visible_connections , copies);
     deepCopyField( connections , copies);
     deepCopyField( symbol_sizes , copies);
     deepCopyField(cond_bias , copies);
+    deepCopyField(visi_cond_bias , copies);
     deepCopyField(bias_gradient , copies);
+    deepCopyField(visi_bias_gradient , copies);
     deepCopyField( hidden_layer_target , copies);
     deepCopyField( input_gradient , copies);
     deepCopyField(previous_input , copies);
     deepCopyField(previous_hidden_layer , copies);
+    deepCopyField(previous_visible_layer , copies);
     deepCopyField(hidden_layer_sample , copies);
+    deepCopyField(visible_layer_sample , copies);
     deepCopyField(pos_down_values , copies);
     deepCopyField(pos_up_values , copies);
     deepCopyField(alpha , copies);
@@ -255,6 +273,7 @@
     hidden_layer->forget();
     connections->forget();
     dynamic_connections->forget();
+    visible_connections->forget();
 
     stage = 0;
 }
@@ -351,8 +370,23 @@
                     continue;
 
                 clamp_visible_units(input);
-                
+//                if(sample == 0)
+//                {
+//                    cout << "Pre rbm_update" << endl;
+//                    cout << "connection=" << dynamic_cast<RBMMatrixConnection*>((RBMConnection *)connections)->weights << endl;
+//                    cout << "visible_bias=" << visible_layer->bias << endl;
+//                    cout << "hidden_bias=" << hidden_layer->bias << endl;
+//                }
                 mean_cost += rbm_update();
+//                if(sample == 0)
+//                {
+//                    cout << "Post rbm_update" << endl;
+//                    cout << "hidden->expectation" << hidden_layer->expectation << endl;
+//                    cout << "visible->expectation" << visible_layer->expectation << endl;
+//                    cout << "connection=" << dynamic_cast<RBMMatrixConnection*>((RBMConnection *)connections)->weights << endl;
+//                    cout << "visible_bias=" << visible_layer->bias << endl;
+//                    cout << "hidden_bias=" << hidden_layer->bias << endl;
+//                }
             }
 
             if( pb )
@@ -389,7 +423,9 @@
                                   end_stage - init_stage );
 
         previous_hidden_layer.resize(hidden_layer->size);
+        previous_visible_layer.resize(visible_layer->size);
         dynamic_connections->setLearningRate( dynamic_learning_rate );
+        visible_connections->setLearningRate( dynamic_learning_rate );
         real mean_cost = 0;
         while(stage < end_stage)
         {
@@ -398,18 +434,23 @@
                 if(sample > 0)
                 {
                     previous_hidden_layer << hidden_layer_sample;
+                    previous_visible_layer << visible_layer_sample;
                     // ** or **
                     // hidden_layer->generateSample();
                     // previous_hidden_layer << hidden_layer->sample;
                 }
                 else
+                {
                     previous_hidden_layer.clear();
+                    previous_visible_layer.clear();
+                }
 
                 train_set->getExample(sample, input, target, weight);
 
                 if(train_set->getString(sample,0) == "<oov>")
                 {
                     hidden_layer_sample.clear();
+                    visible_layer_sample.clear();
                     continue;
                 }
             
@@ -433,14 +474,15 @@
             pb = 0;
         }
 
-    }
+        //Make a copy of the dynamique phase
+        CopiesMap map;
+        dynamic_connections_copy = dynamic_connections->deepCopy(map);
 
 
-    //Make a copy of the dynamique phase
-    CopiesMap map;
+    }
 
-    dynamic_connections_copy = dynamic_connections->deepCopy(map);
 
+    //cout << dynamic_connections_copy->weights << endl;
 
 
     //alpha = Vec(hidden_layer->size,1);
@@ -515,6 +557,10 @@
 
     }
 
+    //cout << dynamic_connections->weights - dynamic_connections_copy->weights<< endl;
+
+
+
     train_stats->finalize();
 }
 
@@ -595,7 +641,7 @@
     
     connections->update( 
         pos_down_values, pos_up_values, visible_layer->sample
-        , hidden_layer->sample ); // ... of connections between layers.
+        , hidden_layer->expectation  ); // ... of connections between layers.
     
     // Compute reconstruction error
     
@@ -603,14 +649,18 @@
 
     visible_layer->getAllActivations( connections );
 
+    
     visible_layer->computeExpectation();
     
+    
     return visible_layer->fpropNLL(pos_down_values);
    
 }
 
 real DynamicallyLinkedRBMsModel::dynamic_connections_update()
 {
+    
+
     // Obtain target hidden_layer h_t
     connections->setAsDownInput(visible_layer->expectation);
     hidden_layer->getAllActivations(connections);
@@ -621,27 +671,52 @@
     
     // Use "previous_hidden_layer" field and "dynamic_connections" module 
     // to set bias of "hidden_layer"
-
     dynamic_connections->fprop(previous_hidden_layer,hidden_layer->activation);
-    // hidden_layer->activation *= alpha;
     hidden_layer->expectation_is_up_to_date = false;
     hidden_layer->computeExpectation();
 
+    
+
     // Ask "hidden_layer" for maximum likelyhood gradient on bias
     real nll = hidden_layer->fpropNLL(hidden_layer_target);
     hidden_layer->bpropNLL(hidden_layer_target, nll, bias_gradient);
 
-    // bpropUpdate through dynamic_connections
-    // real delta = 0;
-    // for(int i=0; i<hidden_layer->size; i++)
-    //    delta -= dynamic_learning_rate * bias_gradient[i] * hidden_layer->activation[i]/alpha;
-    // bias_gradient *= alpha;
-    // alpha += delta;
+   
     dynamic_connections->bpropUpdate(previous_hidden_layer,
                                      hidden_layer->activation,
                                      input_gradient, bias_gradient);
 
-    return nll;
+
+
+
+
+
+
+
+    //////////////////// VISIBLE DYNAMIC CONNECTION
+
+    visible_layer_sample << visible_layer->expectation;
+
+    // Use "previous_visible_layer" field and "visible_connections" module 
+    // to set bias of "visible_layer"
+   
+    visible_connections->fprop(previous_visible_layer,visible_layer->activation);
+    visible_layer->expectation_is_not_up_to_date();
+    visible_layer->computeExpectation();
+
+    
+
+    // Ask "visible_layer" for maximum likelyhood gradient on bias
+    real nll_visi = visible_layer->fpropNLL(visible_layer_sample);
+    visible_layer->bpropNLL(visible_layer_sample, nll_visi, visi_bias_gradient);
+
+
+    visible_connections->bpropUpdate(previous_visible_layer,
+                                     visible_layer->activation,
+                                     input_gradient, visi_bias_gradient);
+
+
+    return nll_visi;
 }
 
 real DynamicallyLinkedRBMsModel::fine_tuning_update()
@@ -748,7 +823,7 @@
     
     connections->update( 
         pos_down_values, pos_up_values, visible_layer->sample
-        , hidden_layer->sample ); // ... of connections between layers.
+        , hidden_layer->expectation ); // ... of connections between layers.
 
     //cout << ((PLearn::PP<PLearn::GradNNetLayerModule>)dynamic_connections)->weights << endl;
 
@@ -831,6 +906,8 @@
     int len = testset.length();
     Vec input;
     Vec target;
+    Vec bias_tempo;
+    Vec visi_bias_tempo;
     real weight;
 
     Vec output(outputsize());
@@ -878,10 +955,13 @@
             //h*_{t-1}
             //////////////////////////////////
             dynamic_connections->fprop(previous_hidden_layer, cond_bias);
-           
             hidden_layer->getAllBias(cond_bias); //**************************
 
-
+            //v*_{t-1} VISIBLE DYNAMIC CONNECTION
+            //////////////////////////////////
+            visible_connections->fprop(previous_input, visi_cond_bias);
+            visible_layer->getAllBias(visi_cond_bias); //**************************
+            
             //up phase
             connections->setAsDownInput( previous_input );
             hidden_layer->getAllActivations( connections );
@@ -889,6 +969,7 @@
             //////////////////////////////////
 
             previous_hidden_layer << hidden_layer->expectation;//h_{t-2} au prochain tour//******************************
+            
 
             //h*_{t}
             ////////////
@@ -905,12 +986,31 @@
         {
             previous_hidden_layer.clear();//h_{t-1}
             dynamic_connections->fprop(previous_hidden_layer,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+
             //dynamic_connections_copy->fprop(previous_hidden_layer,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
             hidden_layer->expectation_is_up_to_date = false;
             hidden_layer->computeExpectation();//h_{t}
 
             previous_input.resize(visible_layer->size);
             previous_input << visible_layer->expectation;
+
+            bias_tempo.resize(hidden_layer->bias.length());
+            bias_tempo << hidden_layer->bias;
+
+
+
+
+            /////////VISIBLE DYNAMIC CONNECTION
+            previous_visible_layer.clear();//v_{t-1}
+            visible_connections->fprop(previous_visible_layer,visible_layer->activation);//conection entre v_{t-1} et v_{t}
+
+            visible_layer->expectation_is_not_up_to_date();
+            visible_layer->computeExpectation();//v_{t}
+
+            visi_bias_tempo.resize(visible_layer->bias.length());
+            visi_bias_tempo << visible_layer->bias;
+
+            
             begin++;
         }
 
@@ -936,9 +1036,10 @@
 
         costs[0] = visible_layer->fpropNLL(previous_input) / inputsize() ;
 
+       
+        hidden_layer->getAllBias(bias_tempo); 
+        visible_layer->getAllBias(visi_bias_tempo); 
 
-
-
         // costs[0] = 0; //nll/nb_de_temps_par_mesure
 
         if (testoutputs)

Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h	2007-04-07 01:34:55 UTC (rev 6847)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h	2007-04-07 02:43:40 UTC (rev 6848)
@@ -91,11 +91,15 @@
 
     //! OnlineLearningModule corresponding to dynamic links
     //! between RBMs' hidden layers
-    PP<OnlineLearningModule> dynamic_connections;
+    PP<GradNNetLayerModule> dynamic_connections;
 
+    //! OnlineLearningModule corresponding to dynamic links
+    //! between RBMs' visible layers
+    PP<GradNNetLayerModule> visible_connections;
+
     //! Copy OnlineLearningModule corresponding to dynamic links
     //! between RBMs' hidden layers
-    PP<OnlineLearningModule> dynamic_connections_copy;
+    PP<GradNNetLayerModule> dynamic_connections_copy;
 
     //! The weights of the connections between the RBM visible and hidden layers
     PP<RBMConnection> connections;
@@ -207,9 +211,15 @@
     //! Stores conditional bias
     mutable Vec cond_bias;
 
+    //! Stores visible conditional bias
+    mutable Vec visi_cond_bias;
+
     //! Stores bias gradient
     mutable Vec bias_gradient;
 
+     //! Stores bias gradient
+    mutable Vec visi_bias_gradient;
+
     //! Stores hidden layer target in dynamic learning phase
     mutable Vec hidden_layer_target;
 
@@ -222,9 +232,15 @@
     //! Stores previous hidden layer value
     mutable Vec previous_hidden_layer;
 
+    //! Stores previous visible layer value
+    mutable Vec previous_visible_layer;
+
     //! Stores a sample from the hidden layer
     mutable Vec hidden_layer_sample;
 
+    //! Stores a sample from the visible layer
+    mutable Vec visible_layer_sample;
+
     //! Store a copy of the positive phase values
     mutable Vec pos_down_values;
     mutable Vec pos_up_values;



From yoshua at mail.berlios.de  Sat Apr  7 16:09:16 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Sat, 7 Apr 2007 16:09:16 +0200
Subject: [Plearn-commits] r6849 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200704071409.l37E9Gnf010081@sheep.berlios.de>

Author: yoshua
Date: 2007-04-07 16:09:16 +0200 (Sat, 07 Apr 2007)
New Revision: 6849

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
Log:
Added options to perform moving average of parameters during training
for use only on test prediction.


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-04-07 02:43:40 UTC (rev 6848)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-04-07 14:09:16 UTC (rev 6849)
@@ -56,6 +56,8 @@
 
 NatGradNNet::NatGradNNet()
     : noutputs(-1),
+      params_averaging_coeff(1.0),
+      params_averaging_freq(5),
       init_lrate(0.01),
       lrate_decay(0),
       output_layer_lrate_scale(1),
@@ -98,10 +100,30 @@
 
     declareOption(ol, "layer_params", &NatGradNNet::layer_params,
                   OptionBase::learntoption,
-                  "Parameters for each layer, organized as follows: layer_params[i] \n"
+                  "Training parameters for each layer, organized as follows: layer_params[i] \n"
                   "is a matrix of dimension layer_sizes[i+1] x (layer_sizes[i]+1)\n"
                   "containing the neuron biases in its first column.\n");
 
+    declareOption(ol, "layer_mparams", &NatGradNNet::layer_mparams,
+                  OptionBase::learntoption,
+                  "Test parameters for each layer, organized like layer_params.\n"
+                  "This is a moving average of layer_params, computed with\n"
+                  "coefficient params_averaging_coeff. Thus the mparams are\n"
+                  "a smoothed version of the params, and they are used only\n"
+                  "during testing.\n");
+
+    declareOption(ol, "params_averaging_coeff", &NatGradNNet::params_averaging_coeff,
+                  OptionBase::buildoption,
+                  "Coefficient used to control how fast we forget old parameters\n"
+                  "in the moving average performed as follows:\n"
+                  "mparams <-- (1-params_averaging_coeff)mparams + params_averaging_coeff*params\n");
+
+    declareOption(ol, "params_averaging_freq", &NatGradNNet::params_averaging_freq,
+                  OptionBase::buildoption,
+                  "How often (in terms of number of minibatches, i.e. weight updates)\n"
+                  "do we perform the moving average update calculation\n"
+                  "mparams <-- (1-params_averaging_coeff)mparams + params_averaging_coeff*params\n");
+
     declareOption(ol, "init_lrate", &NatGradNNet::init_lrate,
                   OptionBase::buildoption,
                   "Initial learning rate\n");
@@ -190,6 +212,7 @@
     layer_sizes[0]=inputsize_;
     layer_sizes[n_layers-1]=noutputs;
     layer_params.resize(n_layers-1);
+    layer_mparams.resize(n_layers-1);
     layer_params_delta.resize(n_layers-1);
     layer_params_gradient.resize(n_layers-1);
     biases.resize(n_layers-1);
@@ -202,6 +225,7 @@
         n_params+=layer_sizes[i+1]*(1+layer_sizes[i]);
     }
     all_params.resize(n_params);
+    all_mparams.resize(n_params);
     all_params_gradient.resize(n_params);
     all_params_delta.resize(n_params);
     neuron_params.resize(n_neurons);
@@ -211,6 +235,7 @@
     {
         int np=layer_sizes[i+1]*(1+layer_sizes[i]);
         layer_params[i]=all_params.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
+        layer_mparams[i]=all_mparams.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
         biases[i]=layer_params[i].subMatColumns(0,1);
         weights[i]=layer_params[i].subMatColumns(1,layer_sizes[i]); // weights[0] from layer 0 to layer 1
         layer_params_gradient[i]=all_params_gradient.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
@@ -271,6 +296,7 @@
 
     deepCopyField(hidden_layer_sizes, copies);
     deepCopyField(layer_params, copies);
+    deepCopyField(layer_mparams, copies);
     deepCopyField(neurons_natgrad_template, copies);
     deepCopyField(neurons_natgrad_per_layer, copies);
     deepCopyField(params_natgrad_template, copies);
@@ -283,6 +309,7 @@
     deepCopyField(neuron_outputs_per_layer, copies);
     deepCopyField(neuron_extended_outputs_per_layer, copies);
     deepCopyField(all_params, copies);
+    deepCopyField(all_mparams, copies);
     deepCopyField(all_params_gradient, copies);
     deepCopyField(layer_params_gradient, copies);
     deepCopyField(neuron_gradients, copies);
@@ -317,6 +344,8 @@
     }
     stage = 0;
     cumulative_training_time=0;
+    if (params_averaging_coeff!=1.0)
+        all_mparams << all_params;
 }
 
 void NatGradNNet::train()
@@ -369,6 +398,11 @@
                 
             }
         }
+        if (params_averaging_coeff!=1.0 && 
+            b==minibatch_size-1 && 
+            (stage+1)%(minibatch_size*params_averaging_freq)==0)
+            multiplyScaledAdd(all_params, 1-params_averaging_coeff,
+                              params_averaging_coeff, all_mparams);
         if( pb )
             pb->update( stage + 1 );
     }
@@ -391,7 +425,7 @@
     // mean gradient over minibatch_size examples has less variance, can afford larger learning rate
     real lrate = sqrt(real(minibatch_size))*init_lrate/(1 + t*lrate_decay);
     PLASSERT(targets.length()==minibatch_size && train_costs.length()==minibatch_size && example_weights.length()==minibatch_size);
-    fpropNet(minibatch_size);
+    fpropNet(minibatch_size,true);
     fbpropLoss(neuron_outputs_per_layer[n_layers-1],targets,example_weights,train_costs);
     for (int i=n_layers-1;i>0;i--)
     {
@@ -461,12 +495,12 @@
 void NatGradNNet::computeOutput(const Vec& input, Vec& output) const
 {
     neuron_outputs_per_layer[0](0) << input;
-    fpropNet(1);
+    fpropNet(1,false);
     output << neuron_outputs_per_layer[n_layers-1](0);
 }
 
 //! compute (pre-final-non-linearity) network top-layer output given input
-void NatGradNNet::fpropNet(int n_examples) const
+void NatGradNNet::fpropNet(int n_examples, bool during_training) const
 {
     PLASSERT_MSG(n_examples<=minibatch_size,"NatGradNNet::fpropNet: nb input vectors treated should be <= minibatch_size\n");
     for (int i=0;i<n_layers-1;i++)
@@ -479,7 +513,9 @@
             next_layer = next_layer.subMatRows(0,n_examples);
         }
         // try to use BLAS for the expensive operation
-        productScaleAcc(next_layer, prev_layer, false, layer_params[i], true, 1, 0);
+        productScaleAcc(next_layer, prev_layer, false, 
+                        during_training?layer_params[i]:layer_mparams[i], 
+                        true, 1, 0);
         // compute layer's output non-linearity
         if (i+1<n_layers-1)
             for (int k=0;k<n_examples;k++)

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-04-07 02:43:40 UTC (rev 6848)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-04-07 14:09:16 UTC (rev 6849)
@@ -64,7 +64,14 @@
     //! layer_params[i] is a matrix of dimension layer_sizes[i+1] x (layer_sizes[i]+1)
     //! containing the neuron biases in its first column.
     TVec<Mat> layer_params;
+    //! mean layer_params, averaged over past updates (moving average)
+    TVec<Mat> layer_mparams;
 
+    //! mparams <-- (1-params_averaging_coeff)mparams + params_averaging_coeff*params
+    real params_averaging_coeff;
+    //! how often (in terms of minibatches, i.e. weight updates) do we perform the above?
+    int params_averaging_freq;
+
     //! initial learning rate
     real init_lrate;
 
@@ -202,7 +209,7 @@
 
     //! compute a minibatch of size n_examples network top-layer output given layer 0 output (= network input)
     //! (note that log-probabilities are computed for classification tasks, output_type=NLL)
-    void fpropNet(int n_examples) const;
+    void fpropNet(int n_examples, bool during_training) const;
 
     //! compute train costs given the network top-layer output
     //! and write into neuron_gradients_per_layer[n_layers-2], gradient on pre-non-linearity activation
@@ -223,6 +230,7 @@
     Vec all_params; // all the parameters in one vector
     Vec all_params_delta; // update direction
     Vec all_params_gradient; // all the parameter gradients in one vector
+    Vec all_mparams; // mean parameters (moving-averaged over past values)
     TVec<Mat> layer_params_gradient;
     TVec<Vec> layer_params_delta;
     TVec<Vec> neuron_params; // params of each neuron (pointing in all_params)



From yoshua at mail.berlios.de  Sat Apr  7 17:45:42 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Sat, 7 Apr 2007 17:45:42 +0200
Subject: [Plearn-commits] r6850 - trunk/plearn/math
Message-ID: <200704071545.l37Fjg86016630@sheep.berlios.de>

Author: yoshua
Date: 2007-04-07 17:45:42 +0200 (Sat, 07 Apr 2007)
New Revision: 6850

Modified:
   trunk/plearn/math/TMat_maths_specialisation.h
Log:


Modified: trunk/plearn/math/TMat_maths_specialisation.h
===================================================================
--- trunk/plearn/math/TMat_maths_specialisation.h	2007-04-07 14:09:16 UTC (rev 6849)
+++ trunk/plearn/math/TMat_maths_specialisation.h	2007-04-07 15:45:42 UTC (rev 6850)
@@ -415,26 +415,13 @@
     }
 #endif
 
-/*    int one = 1;
+      int one = 1;
       char trans = transposeA ?'N' :'T';
       int lda = A.mod();
       int m = A.width();
       int n = A.length();
-      static int ndbg=0;
 
-      extern bool debug_;
-      if (debug_ && ndbg<3)
-      {
-      cout << "A=" << A << " * " << x << " ==> " << y << endl;
-      ndbg++;
-      }
       sgemv_(&trans, &m, &n, &alpha, A.data(), &lda, x.data(), &one, &beta, y.data(), &one);
-      if (debug_ && ndbg<3)
-      {
-      cout << "A=" << A << " * " << x << " ==> " << y << endl;
-      ndbg++;
-      }
-*/
 }
 
 //! A <- A + alpha x.y'



From yoshua at mail.berlios.de  Sat Apr  7 20:54:25 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Sat, 7 Apr 2007 20:54:25 +0200
Subject: [Plearn-commits] r6851 - trunk/plearn/python
Message-ID: <200704071854.l37IsPZk012659@sheep.berlios.de>

Author: yoshua
Date: 2007-04-07 20:54:25 +0200 (Sat, 07 Apr 2007)
New Revision: 6851

Modified:
   trunk/plearn/python/PythonObjectWrapper.cc
   trunk/plearn/python/PythonObjectWrapper.h
Log:
Making this code compile under -float. Created tReal macro to 
stand either for tFloat64 or tFloat32 numarray python/C types.
UNTESTED in float (behavior should not change for double, hopefully).


Modified: trunk/plearn/python/PythonObjectWrapper.cc
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.cc	2007-04-07 15:45:42 UTC (rev 6850)
+++ trunk/plearn/python/PythonObjectWrapper.cc	2007-04-07 18:54:25 UTC (rev 6851)
@@ -110,13 +110,13 @@
     return PyLong_AsLong(pyobj);
 }
 
-double ConvertFromPyObject<double>::convert(PyObject* pyobj, bool print_traceback)
+real ConvertFromPyObject<real>::convert(PyObject* pyobj, bool print_traceback)
 {
     PLASSERT( pyobj );
     if (! PyFloat_Check(pyobj))
-        PLPythonConversionError("ConvertFromPyObject<double>", pyobj,
+        PLPythonConversionError("ConvertFromPyObject<real>", pyobj,
                                 print_traceback);
-    return PyFloat_AS_DOUBLE(pyobj);
+    return (real)PyFloat_AS_DOUBLE(pyobj);
 }
 
 string ConvertFromPyObject<string>::convert(PyObject* pyobj, bool print_traceback)
@@ -133,7 +133,7 @@
     // NA_InputArray possibly creates a well-behaved temporary (i.e. not
     // discontinuous is memory)
     PLASSERT( pyobj );
-    PyArrayObject* pyarr = NA_InputArray(pyobj, tFloat64, NUM_C_ARRAY);
+    PyArrayObject* pyarr = NA_InputArray(pyobj, tReal, NUM_C_ARRAY);
     if (! pyarr)
         PLPythonConversionError("ConvertFromPyObject<Vec>", pyobj, print_traceback);
     if (pyarr->nd != 1)
@@ -141,7 +141,7 @@
                 "should be 1; got %d", pyarr->nd);
 
     v.resize(pyarr->dimensions[0]);
-    v.copyFrom((double*)(NA_OFFSETDATA(pyarr)), pyarr->dimensions[0]);
+    v.copyFrom((real*)(NA_OFFSETDATA(pyarr)), pyarr->dimensions[0]);
     Py_XDECREF(pyarr);
 }
 
@@ -157,7 +157,7 @@
     // NA_InputArray possibly creates a well-behaved temporary (i.e. not
     // discontinuous is memory)
     PLASSERT( pyobj );
-    PyArrayObject* pyarr = NA_InputArray(pyobj, tFloat64, NUM_C_ARRAY);
+    PyArrayObject* pyarr = NA_InputArray(pyobj, tReal, NUM_C_ARRAY);
     if (! pyarr)
         PLPythonConversionError("ConvertFromPyObject<Mat>", pyobj, print_traceback);
     if (pyarr->nd != 2)
@@ -165,7 +165,7 @@
                 "should be 2; got %d", pyarr->nd);
 
     m.resize(pyarr->dimensions[0], pyarr->dimensions[1]);
-    m.toVec().copyFrom((double*)(NA_OFFSETDATA(pyarr)),
+    m.toVec().copyFrom((real*)(NA_OFFSETDATA(pyarr)),
                        pyarr->dimensions[0] * pyarr->dimensions[1]);
     Py_XDECREF(pyarr);
 }
@@ -247,9 +247,9 @@
 {
     PyArrayObject* pyarr = 0;
     if (data.isNull() || data.isEmpty())
-        pyarr = NA_NewArray(NULL, tFloat64, 1, 0);
+        pyarr = NA_NewArray(NULL, tReal, 1, 0);
     else
-        pyarr = NA_NewArray(data.data(), tFloat64, 1, data.size());
+        pyarr = NA_NewArray(data.data(), tReal, 1, data.size());
         
     return (PyObject*)pyarr;
 }
@@ -258,9 +258,9 @@
 {
     PyArrayObject* pyarr = 0;
     if (data.isNull() || data.isEmpty())
-        pyarr = NA_NewArray(NULL, tFloat64, 2, data.length(), data.width());
+        pyarr = NA_NewArray(NULL, tReal, 2, data.length(), data.width());
     else if (data.mod() == data.width())
-        pyarr = NA_NewArray(data.data(), tFloat64, 2, data.length(), data.width());
+        pyarr = NA_NewArray(data.data(), tReal, 2, data.length(), data.width());
     else {
         // static PyObject* NA_NewAll( int ndim, maybelong *shape, NumarrayType
         // type, void *buffer, maybelong byteoffset, maybelong bytestride, int
@@ -278,13 +278,13 @@
         // maybelong shape[2];
         // shape[0] = data.length();
         // shape[1] = data.width();
-        // pyarr = NA_NewAll(2, shape, tFloat64, data.data(), 0, data.mod()*sizeof(double),
+        // pyarr = NA_NewAll(2, shape, tReal, data.data(), 0, data.mod()*sizeof(real),
         //                   NA_ByteOrder(), 1, 1);
 
         // NOTE (NC) -- I could not get the above function to work; for now,
         // simply copy the matrix to new storage before converting to Python.
         Mat new_data = data.copy();
-        pyarr = NA_NewArray(new_data.data(), tFloat64, 2,
+        pyarr = NA_NewArray(new_data.data(), tReal, 2,
                             new_data.length(), new_data.width());
     }
 

Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2007-04-07 15:45:42 UTC (rev 6850)
+++ trunk/plearn/python/PythonObjectWrapper.h	2007-04-07 18:54:25 UTC (rev 6851)
@@ -58,6 +58,11 @@
 #include <plearn/vmat/VMat.h>
 #include <plearn/base/tostring.h>
 
+#ifdef USEFLOAT
+#define tReal tFloat32
+#else
+#define tReal tFloat64
+#endif
 
 namespace PLearn {
 
@@ -161,9 +166,9 @@
 };
 
 template <>
-struct ConvertFromPyObject<double>
+struct ConvertFromPyObject<real>
 {
-    static double convert(PyObject*, bool print_traceback);
+    static real convert(PyObject*, bool print_traceback);
 };
 
 template <>



From yoshua at mail.berlios.de  Sat Apr  7 21:19:17 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Sat, 7 Apr 2007 21:19:17 +0200
Subject: [Plearn-commits] r6852 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200704071919.l37JJHaV015600@sheep.berlios.de>

Author: yoshua
Date: 2007-04-07 21:19:16 +0200 (Sat, 07 Apr 2007)
New Revision: 6852

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
Log:


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-04-07 18:54:25 UTC (rev 6851)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-04-07 19:19:16 UTC (rev 6852)
@@ -514,7 +514,8 @@
         }
         // try to use BLAS for the expensive operation
         productScaleAcc(next_layer, prev_layer, false, 
-                        during_training?layer_params[i]:layer_mparams[i], 
+                        (during_training || params_averaging_coeff==1.0)?
+                        layer_params[i]:layer_mparams[i], 
                         true, 1, 0);
         // compute layer's output non-linearity
         if (i+1<n_layers-1)



From saintmlx at mail.berlios.de  Mon Apr  9 16:04:05 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Mon, 9 Apr 2007 16:04:05 +0200
Subject: [Plearn-commits] r6853 - trunk/python_modules/plearn/pyplearn
Message-ID: <200704091404.l39E45rj014311@sheep.berlios.de>

Author: saintmlx
Date: 2007-04-09 16:04:05 +0200 (Mon, 09 Apr 2007)
New Revision: 6853

Modified:
   trunk/python_modules/plearn/pyplearn/plearn_repr.py
Log:
- fixed double quotes within strings for pyplearn scripts: escape backslashes first, then double quotes



Modified: trunk/python_modules/plearn/pyplearn/plearn_repr.py
===================================================================
--- trunk/python_modules/plearn/pyplearn/plearn_repr.py	2007-04-07 19:19:16 UTC (rev 6852)
+++ trunk/python_modules/plearn/pyplearn/plearn_repr.py	2007-04-09 14:04:05 UTC (rev 6853)
@@ -234,7 +234,8 @@
         return str(obj)
 
     elif isinstance(obj, str):
-        return '"%s"' % obj.replace('"', r'\"') # toolkit.quote( obj )
+        # escape backslashes, then double quotes
+        return '"%s"' % obj.replace('\\','\\\\').replace('"', r'\"') 
 
     elif isinstance(obj, list):
         elem_format = lambda elem: inner_repr( elem, indent_level+1 )



From chapados at mail.berlios.de  Mon Apr  9 17:26:57 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Mon, 9 Apr 2007 17:26:57 +0200
Subject: [Plearn-commits] r6854 - trunk/plearn/ker
Message-ID: <200704091526.l39FQvvr019709@sheep.berlios.de>

Author: chapados
Date: 2007-04-09 17:26:56 +0200 (Mon, 09 Apr 2007)
New Revision: 6854

Modified:
   trunk/plearn/ker/RationalQuadraticARDKernel.h
Log:
Changes to comments

Modified: trunk/plearn/ker/RationalQuadraticARDKernel.h
===================================================================
--- trunk/plearn/ker/RationalQuadraticARDKernel.h	2007-04-09 14:04:05 UTC (rev 6853)
+++ trunk/plearn/ker/RationalQuadraticARDKernel.h	2007-04-09 15:26:56 UTC (rev 6854)
@@ -2,7 +2,7 @@
 
 // RationalQuadraticARDKernel.h
 //
-// Copyright (C) 2006 Nicolas Chapados
+// Copyright (C) 2006-2007 Nicolas Chapados
 //
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
@@ -57,8 +57,8 @@
  *    k(x,y) = sf * [1 + (sum_i (x_i - y_i)^2 / w_i)/(2*alpha)]^(-alpha) + k_iid(x,y)
  *
  *  where sf is softplus(isp_signal_sigma), w_i is softplus(isp_global_sigma +
- *  isp_input_sigma[i]), and k_iid(x,y) is the result of IIDNoiseKernel kernel
- *  evaluation.
+ *  isp_input_sigma[i]), and k_iid(x,y) is the result of the IIDNoiseKernel
+ *  kernel evaluation.
  *
  *  Note that to make its operations more robust when used with unconstrained
  *  optimization of hyperparameters, all hyperparameters of this kernel are



From chapados at mail.berlios.de  Mon Apr  9 17:27:08 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Mon, 9 Apr 2007 17:27:08 +0200
Subject: [Plearn-commits] r6855 - trunk/plearn/ker
Message-ID: <200704091527.l39FR8vE019742@sheep.berlios.de>

Author: chapados
Date: 2007-04-09 17:27:08 +0200 (Mon, 09 Apr 2007)
New Revision: 6855

Modified:
   trunk/plearn/ker/RationalQuadraticARDKernel.cc
Log:
Changes to comments

Modified: trunk/plearn/ker/RationalQuadraticARDKernel.cc
===================================================================
--- trunk/plearn/ker/RationalQuadraticARDKernel.cc	2007-04-09 15:26:56 UTC (rev 6854)
+++ trunk/plearn/ker/RationalQuadraticARDKernel.cc	2007-04-09 15:27:08 UTC (rev 6855)
@@ -2,7 +2,7 @@
 
 // RationalQuadraticARDKernel.cc
 //
-// Copyright (C) 2006 Nicolas Chapados
+// Copyright (C) 2006-2007 Nicolas Chapados
 //
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:



From chapados at mail.berlios.de  Mon Apr  9 17:28:34 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Mon, 9 Apr 2007 17:28:34 +0200
Subject: [Plearn-commits] r6856 - in trunk: commands plearn/ker
Message-ID: <200704091528.l39FSYro019832@sheep.berlios.de>

Author: chapados
Date: 2007-04-09 17:28:33 +0200 (Mon, 09 Apr 2007)
New Revision: 6856

Modified:
   trunk/commands/plearn_noblas_inc.h
   trunk/plearn/ker/SquaredExponentialARDKernel.cc
   trunk/plearn/ker/SquaredExponentialARDKernel.h
Log:
Updated SquaredExponential to work in the inverse softplus domain; efficient derivatives not implemented yet

Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2007-04-09 15:27:08 UTC (rev 6855)
+++ trunk/commands/plearn_noblas_inc.h	2007-04-09 15:28:33 UTC (rev 6856)
@@ -111,7 +111,7 @@
 #include <plearn/ker/NegOutputCostFunction.h>
 #include <plearn/ker/PolynomialKernel.h>
 #include <plearn/ker/RationalQuadraticARDKernel.h>
-// #include <plearn/ker/SquaredExponentialARDKernel.h>
+#include <plearn/ker/SquaredExponentialARDKernel.h>
 #include <plearn/ker/ThresholdedKernel.h>
 #include <plearn/ker/VMatKernel.h>
 

Modified: trunk/plearn/ker/SquaredExponentialARDKernel.cc
===================================================================
--- trunk/plearn/ker/SquaredExponentialARDKernel.cc	2007-04-09 15:27:08 UTC (rev 6855)
+++ trunk/plearn/ker/SquaredExponentialARDKernel.cc	2007-04-09 15:28:33 UTC (rev 6856)
@@ -2,7 +2,7 @@
 
 // SquaredExponentialARDKernel.cc
 //
-// Copyright (C) 2006 Nicolas Chapados
+// Copyright (C) 2006-2007 Nicolas Chapados
 //
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
@@ -55,15 +55,16 @@
     "Similar to C.E. Rasmussen's GPML code (see http://www.gaussianprocess.org),\n"
     "this kernel function is specified as:\n"
     "\n"
-    "  k(x,y) = sf2 * exp(- 0.5 * (sum_i (x_i - y_i)^2 / w_i)) + delta_x,y*sn2\n"
+    "  k(x,y) = sf * exp(- 0.5 * (sum_i (x_i - y_i)^2 / w_i)) + k_iid(x,y)\n"
     "\n"
-    "where sf2 is the exp of the 'log_signal_sigma' option, sn2 is the exp of\n"
-    "the 'log_noise_sigma' option (added only if x==y), and w_i is\n"
-    "exp(log_global_sigma + log_input_sigma[i]).\n"
+    "where sf is softplus(isp_signal_sigma), w_i is softplus(isp_global_sigma +\n"
+    "isp_input_sigma[i]), and k_iid(x,y) is the result of the IIDNoiseKernel\n"
+    "kernel evaluation.\n"
     "\n"
     "Note that to make its operations more robust when used with unconstrained\n"
     "optimization of hyperparameters, all hyperparameters of this kernel are\n"
-    "specified in the log-domain.\n"
+    "specified in the inverse softplus domain.  See IIDNoiseKernel for more\n"
+    "explanations.\n"
     );
 
 
@@ -102,92 +103,116 @@
 real SquaredExponentialARDKernel::evaluate(const Vec& x1, const Vec& x2) const
 {
     PLASSERT( x1.size() == x2.size() );
-    PLASSERT( !m_log_input_sigma.size() || x1.size() == m_log_input_sigma.size() );
+    PLASSERT( !m_isp_input_sigma.size() || x1.size() == m_isp_input_sigma.size() );
 
     if (x1.size() == 0)
-        return exp(2*m_log_signal_sigma) + exp(2*m_log_noise_sigma);
+        return softplus(m_isp_signal_sigma) + inherited::evaluate(x1,x2);
     
     const real* px1 = x1.data();
     const real* px2 = x2.data();
-    real expval = 0.0;
-    real sum_sqdiff = 0.0;
+    real sf         = softplus(m_isp_signal_sigma);
+    real expval     = 0.0;
     
-    if (m_log_input_sigma.size() > 0) {
-        const real* pinpsig = m_log_input_sigma.data();
+    if (m_isp_input_sigma.size() > 0) {
+        const real* pinpsig = m_isp_input_sigma.data();
         for (int i=0, n=x1.size() ; i<n ; ++i) {
             real diff   = *px1++ - *px2++;
             real sqdiff = diff * diff;
-            sum_sqdiff += sqdiff;
-            expval     += sqdiff / exp(2*(m_log_global_sigma + *pinpsig++));
+            expval     += sqdiff / softplus(m_isp_global_sigma + *pinpsig++);
         }
     }
     else {
-        real global_sigma = exp(2*m_log_global_sigma);
+        real global_sigma = softplus(m_isp_global_sigma);
         for (int i=0, n=x1.size() ; i<n ; ++i) {
             real diff   = *px1++ - *px2++;
             real sqdiff = diff * diff;
-            sum_sqdiff += sqdiff;
             expval     += sqdiff / global_sigma;
         }
     }
 
-    // We add a noise variance only if x and y are equal (within machine tolerance)
-    real noise_cov = 0.0;
-    if (is_equal(sum_sqdiff, 0))
-        noise_cov = exp(2*m_log_noise_sigma);
-    return exp(2*m_log_signal_sigma + -0.5 * expval) + noise_cov;
+    // EXPERIMENTAL: Multiply noise_cov by kernel value if we have kronecker
+    // terms, otherwise disregard noise_cov
+    // real noise_cov = ( m_kronecker_indexes.size() > 0?
+    //                    inherited::evaluate(x1,x2) : 1.0 );
+    // return sf * exp(-0.5 * expval) * noise_cov;
+
+    real noise_cov = inherited::evaluate(x1,x2);
+    return sf * exp(-0.5 * expval) + noise_cov;
 }
 
 
-//#####  computeGramMatrixDerivative  #########################################
+//#####  computeGramMatrix  ###################################################
 
-/**
-void SquaredExponentialARDKernel::computeGramMatrixDerivative(
-    Mat& KD, const string& kernel_param, real epsilon) const
+void SquaredExponentialARDKernel::computeGramMatrix(Mat K) const
 {
-    static const string LSV = "log_signal_sigma";
-    static const string LNV = "log_noise_sigma";
-    static const string LGS = "log_global_sigma";
-    static const string LIS = "log_input_sigma[";
+    PLASSERT( !m_isp_input_sigma.size() || dataInputsize() == m_isp_input_sigma.size() );
+    PLASSERT( K.size() == 0 || m_data_cache.size() > 0 );  // Ensure data cached OK
 
-    const int W = nExamples();
-    KD.resize(W,W);
-    const int mod = KD.mod();
-    real* KDij;
-    real* KDji;
+    // Compute IID noise gram matrix
+    inherited::computeGramMatrix(K);
 
-    // log_signal_sigma
-    if (kernel_param == LSV) {
+    // Precompute some terms
+    real sf    = softplus(m_isp_signal_sigma);
+    m_input_sigma.resize(dataInputsize());
+    m_input_sigma.fill(m_isp_global_sigma);
+    if (m_isp_input_sigma.size() > 0)
+        m_input_sigma += m_isp_input_sigma;
+    for (int i=0, n=m_input_sigma.size() ; i<n ; ++i)
+        m_input_sigma[i] = softplus(m_input_sigma[i]);
 
-    }
+    // Compute Gram Matrix
+    int  l = data->length();
+    int  m = K.mod();
+    int  n = dataInputsize();
+    int  cache_mod = m_data_cache.mod();
 
-    // log_noise_sigma
-    else if (kernel_param == LNV) {
-        for (int i=0 ; i<W ; ++i) {
-            KDij = KD[i];
-            KDji = &KD[0][i];
-            for (int j=0 ; j<i ; ++j, Kji += m) {
-                // Below main diagonal, we have to check if x_i == x_j
-                
+    real *data_start = &m_data_cache(0,0);
+    real *Ki = K[0];                         // Start of current row
+    real *Kij;                               // Current element along row
+    real *input_sigma_data = m_input_sigma.data();
+    real *xi = data_start;
+    
+    for (int i=0 ; i<l ; ++i, xi += cache_mod, Ki+=m)
+    {
+        Kij = Ki;
+        real *xj = data_start;
+
+        for (int j=0; j<=i; ++j, xj += cache_mod) {
+            // Kernel evaluation per se
+            real *x1 = xi;
+            real *x2 = xj;
+            real *p_inpsigma = input_sigma_data;
+            real sum_wt = 0.0;
+            int  k = n;
+
+            // Use Duff's device to unroll the following loop:
+            //     while (k--) {
+            //         real diff = *x1++ - *x2++;
+            //         sum_wt += (diff * diff) / *p_inpsigma++;
+            //     }
+            real diff;
+            switch (k % 8) {
+            case 0: do { diff = *x1++ - *x2++; sum_wt += (diff*diff) / *p_inpsigma++;
+            case 7:      diff = *x1++ - *x2++; sum_wt += (diff*diff) / *p_inpsigma++;
+            case 6:      diff = *x1++ - *x2++; sum_wt += (diff*diff) / *p_inpsigma++;
+            case 5:      diff = *x1++ - *x2++; sum_wt += (diff*diff) / *p_inpsigma++;
+            case 4:      diff = *x1++ - *x2++; sum_wt += (diff*diff) / *p_inpsigma++;
+            case 3:      diff = *x1++ - *x2++; sum_wt += (diff*diff) / *p_inpsigma++;
+            case 2:      diff = *x1++ - *x2++; sum_wt += (diff*diff) / *p_inpsigma++;
+            case 1:      diff = *x1++ - *x2++; sum_wt += (diff*diff) / *p_inpsigma++;
+                       } while((k -= 8) > 0);
             }
+
+            // Update kernel matrix (already pre-filled with IID noise terms)
+            *Kij++ += sf * exp(-0.5 * sum_wt);
         }
-        
     }
-
-    // log_global_sigma
-    else if (kernel_param == LGS) {
-
+    if (cache_gram_matrix) {
+        gram_matrix.resize(l,l);
+        gram_matrix << K;
+        gram_matrix_is_cached = true;
     }
-
-    // log_input_sigma
-    else if (kernel_param.substr(0,16) == LIS) {
-
-    }
-    else
-        PLERROR("SquaredExponentialARDKernel::computeGramMatrixDerivative: "
-                "unknown hyperparameter '%s'", kernel_param.c_str());
 }
-*/
 
 
 //#####  makeDeepCopyFromShallowCopy  #########################################

Modified: trunk/plearn/ker/SquaredExponentialARDKernel.h
===================================================================
--- trunk/plearn/ker/SquaredExponentialARDKernel.h	2007-04-09 15:27:08 UTC (rev 6855)
+++ trunk/plearn/ker/SquaredExponentialARDKernel.h	2007-04-09 15:28:33 UTC (rev 6856)
@@ -2,7 +2,7 @@
 
 // SquaredExponentialARDKernel.h
 //
-// Copyright (C) 2006 Nicolas Chapados
+// Copyright (C) 2006-2007 Nicolas Chapados
 //
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
@@ -58,15 +58,16 @@
  *  Similar to C.E. Rasmussen's GPML code (see http://www.gaussianprocess.org),
  *  this kernel function is specified as:
  *
- *    k(x,y) = sf2 * exp(- 0.5 * (sum_i (x_i - y_i)^2 / w_i)) + delta_x,y*sn2
+ *    k(x,y) = sf * exp(- 0.5 * (sum_i (x_i - y_i)^2 / w_i)) + k_iid(x,y)
  *
- *  where sf2 is the exp of the 'log_signal_sigma' option, sn2 is the exp of
- *  the 'log_noise_sigma' option (added only if x==y), and w_i is
- *  exp(log_global_sigma + log_input_sigma[i]).
+ *  where sf is softplus(isp_signal_sigma), w_i is softplus(isp_global_sigma +
+ *  isp_input_sigma[i]), and k_iid(x,y) is the result of the IIDNoiseKernel
+ *  kernel evaluation.
  *
  *  Note that to make its operations more robust when used with unconstrained
  *  optimization of hyperparameters, all hyperparameters of this kernel are
- *  specified in the log-domain.
+ *  specified in the inverse softplus domain.  See IIDNoiseKernel for more
+ *  explanations.
  */
 class SquaredExponentialARDKernel : public ARDBaseKernel
 {
@@ -89,6 +90,9 @@
     //! Compute K(x1,x2).
     virtual real evaluate(const Vec& x1, const Vec& x2) const;
 
+    //! Compute the Gram Matrix.
+    virtual void computeGramMatrix(Mat K) const;
+    
     //! Directly compute the derivative with respect to hyperparameters
     //! (Faster than finite differences...)
     // virtual void computeGramMatrixDerivative(Mat& KD, const string& kernel_param,



From chapados at mail.berlios.de  Mon Apr  9 17:29:55 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Mon, 9 Apr 2007 17:29:55 +0200
Subject: [Plearn-commits] r6857 - trunk/plearn/ker
Message-ID: <200704091529.l39FTtel019896@sheep.berlios.de>

Author: chapados
Date: 2007-04-09 17:29:55 +0200 (Mon, 09 Apr 2007)
New Revision: 6857

Modified:
   trunk/plearn/ker/IIDNoiseKernel.cc
Log:
Do not add plain noise for a regular evaluate, as per "identity" considerations in Rasmussen/Williams -- more detailed comments to follow later

Modified: trunk/plearn/ker/IIDNoiseKernel.cc
===================================================================
--- trunk/plearn/ker/IIDNoiseKernel.cc	2007-04-09 15:28:33 UTC (rev 6856)
+++ trunk/plearn/ker/IIDNoiseKernel.cc	2007-04-09 15:29:55 UTC (rev 6857)
@@ -123,7 +123,7 @@
 {
     if (m_kronecker_indexes.size() != m_isp_kronecker_sigma.size())
         PLERROR("IIDNoiseKernel::build_: size of 'kronecker_indexes' (%d) "
-                "does not match that of 'iso_kronecker_sigma' (%d)",
+                "does not match that of 'isp_kronecker_sigma' (%d)",
                 m_kronecker_indexes.size(), m_isp_kronecker_sigma.size());
 }
 
@@ -133,8 +133,8 @@
 real IIDNoiseKernel::evaluate(const Vec& x1, const Vec& x2) const
 {
     real value = 0.0;
-    if (x1 == x2)
-        value += softplus(m_isp_noise_sigma);
+    // if (x1 == x2)
+    //     value += softplus(m_isp_noise_sigma);
 
     const int n = m_kronecker_indexes.size();
     if (n > 0) {



From yoshua at mail.berlios.de  Mon Apr  9 19:27:40 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Mon, 9 Apr 2007 19:27:40 +0200
Subject: [Plearn-commits] r6858 - trunk/scripts
Message-ID: <200704091727.l39HReg7028717@sheep.berlios.de>

Author: yoshua
Date: 2007-04-09 19:27:40 +0200 (Mon, 09 Apr 2007)
New Revision: 6858

Added:
   trunk/scripts/collectres
Log:
Python script to collect results from a bunch of PLearn learner experiments.


Added: trunk/scripts/collectres
===================================================================
--- trunk/scripts/collectres	2007-04-09 15:29:55 UTC (rev 6857)
+++ trunk/scripts/collectres	2007-04-09 17:27:40 UTC (rev 6858)
@@ -0,0 +1,119 @@
+#!/usr/bin/env python
+
+# pymake
+# Copyright (C) 2001 Pascal Vincent
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+
+import sys,string
+from plearn.vmat.PMat import *
+
+def selectres(loc_specs,a):
+  res = []
+  loc_mode = loc_specs[0]
+  if loc_mode=="pos":
+     row=int(loc_specs[1])
+     i=2
+     while len(loc_specs[i:])>0:
+       res.append(a[row,int(loc_specs[i])])
+       i+=1
+  else:
+    raise ValueError("Invalid <location-spec> mode, expected 'pos', 'mincol', or 'col', got "+loc_mode)    
+  return res
+
+def getres(loc_specs,filenames):
+  all_results = []
+  for filename in filenames:
+    try:
+      file_res = selectres(loc_specs,
+                           load_pmat_as_array(filename))
+      all_results.append([file_res,filename])
+    except ValueError,v:
+      print >>sys.stderr, "caught ValueError exception!"
+      print >>sys.stderr, v
+  return all_results
+
+def compare_res(x,y):
+  if x[0][0]<y[0][0]:
+    return -1
+  else:
+    return 1
+  
+def outputres(f,mode,results):
+  if mode=="min":
+    minval = 1e36
+    minfile = ""
+    selected = []
+    for res in results:
+      print res
+      val = res[0][0]
+      if val<minval:
+        minval=val
+        minfile=res[1]
+        selected=res[0]
+    for v in selected:
+      f.write(str(v)+" ")
+    f.write(minfile+"\n")
+  elif mode=="sort":
+    results.sort(compare_res)
+    for res in results:
+      for v in res[0]:
+        f.write(str(v)+" ")
+      f.write(res[1]+"\n")
+  elif mode=="plot":
+    pass
+  else:
+    raise ValueError("Invalid <spec> mode, expected 'min', 'sort', or 'plot', got "+mode)
+
+
+if __name__=='__main__':
+  args = sys.argv[:]
+  if len(args)==1:
+    print "Usage: collectres <output> <spec> <file1.pmat> <file2.pmat> ..."
+    print 
+    print "The <spec> can be the following:"
+    print '  "min <location-spec>" : identify the mininum of <location-spec> over the <file*.pmat>' 
+    print '  "sort <location-spec>" : make a sorted table of all the values at each <location-spec> over the <file*.pmat>' 
+    print '  "plot <location-spec> " : make a plot of all the values at <location-spec> over the <file*.pmat>' 
+    print "where <location-spec> can be the following:"
+    print '  "pos <row> <col0> [<col1> <col2>...]": keep only the value at positions (<row> <col0>) (<row> <col1>) ... in each pmat, and the first value may be used to minimize over files in the min command'
+    print '  "mincol <mcol> [<col1> <col2>...] ": keep only the minimum value in <mcol> of each pmat, keeping track of the <coli> values at that row'
+    print '  "col <column-nb>[<minrow> <maxrow>]": this is used with the plot command, to specify a column to plot, optionally from <minrow> to <maxrow> row inclusively'
+    sys.exit(1)
+  output = args[1]
+  filenames = args[3:]
+  speclist = string.split(args[2])
+  mode = speclist[0]
+  f=open(output,"w")
+  f.write("# "+args[0]+" "+output+" "+'"'+args[2]+'" ')
+  for file in filenames:
+    f.write(file+" ")
+  f.write("\n")
+  outputres(f,mode,getres(speclist[1:],filenames))
+  f.flush()


Property changes on: trunk/scripts/collectres
___________________________________________________________________
Name: svn:executable
   + *



From nouiz at mail.berlios.de  Mon Apr  9 20:07:59 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 9 Apr 2007 20:07:59 +0200
Subject: [Plearn-commits] r6859 - in trunk: plearn/ker
	plearn_learners/nearest_neighbors plearn_learners/online
	plearn_learners/regressors plearn_learners_experimental
	plearn_learners_experimental/SurfaceTemplate
Message-ID: <200704091807.l39I7x4J012267@sheep.berlios.de>

Author: nouiz
Date: 2007-04-09 20:07:57 +0200 (Mon, 09 Apr 2007)
New Revision: 6859

Modified:
   trunk/plearn/ker/PolynomialKernel.h
   trunk/plearn/ker/RationalQuadraticARDKernel.cc
   trunk/plearn_learners/nearest_neighbors/BallTreeNearestNeighbors.cc
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/RBMMultinomialLayer.cc
   trunk/plearn_learners/online/SquaredErrorCostModule.cc
   trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
   trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc
   trunk/plearn_learners_experimental/SurfaceTemplate/geometry.cc
Log:
cast of constant number (ex 2.0) to real as by default they are double. 
This is needed to compile in float instead of double



Modified: trunk/plearn/ker/PolynomialKernel.h
===================================================================
--- trunk/plearn/ker/PolynomialKernel.h	2007-04-09 17:27:40 UTC (rev 6858)
+++ trunk/plearn/ker/PolynomialKernel.h	2007-04-09 18:07:57 UTC (rev 6859)
@@ -87,7 +87,7 @@
     //! Evaluate kernel value from the value of the dot product.
     inline real evaluateFromDot(real dot_product) const
     {
-        return ipow(beta*dot_product + 1.0, n);
+        return ipow(beta*dot_product + real(1.0), n);
     }
 
     //! Declares the class options.

Modified: trunk/plearn/ker/RationalQuadraticARDKernel.cc
===================================================================
--- trunk/plearn/ker/RationalQuadraticARDKernel.cc	2007-04-09 17:27:40 UTC (rev 6858)
+++ trunk/plearn/ker/RationalQuadraticARDKernel.cc	2007-04-09 18:07:57 UTC (rev 6859)
@@ -150,7 +150,7 @@
 
     // We add the noise covariance as well
     real noise_cov = inherited::evaluate(x1,x2);
-    return sf * pow(1 + sum_wt / (2.*alpha), -alpha) + noise_cov;
+    return sf * pow(1 + sum_wt / (real(2.)*alpha), -alpha) + noise_cov;
 }
 
 
@@ -316,7 +316,7 @@
     real alpha = softplus(m_isp_alpha);
     real noise = m_noise_gram_cache(i,j);
     K -= noise;
-    real k     = pow(K / softplus(m_isp_signal_sigma), -1. / alpha);
+    real k     = pow(K / softplus(m_isp_signal_sigma), real(-1.) / alpha);
     real inner = (k - 1) * alpha * sigmoid(m_isp_global_sigma) / softplus(m_isp_global_sigma);
     return (K / k) * inner;
 }
@@ -337,7 +337,7 @@
     Vec& row_j   = *dataRow(j);
     real noise   = m_noise_gram_cache(i,j);
     K -= noise;
-    real k       = pow(K / softplus(m_isp_signal_sigma), -1. / alpha);
+    real k       = pow(K / softplus(m_isp_signal_sigma), real(-1.) / alpha);
     real diff    = row_i[arg] - row_j[arg];
     real sq_diff = diff * diff;
     real inner   = m_isp_global_sigma + m_isp_input_sigma[arg];
@@ -354,7 +354,7 @@
     real alpha = softplus(m_isp_alpha);
     real noise = m_noise_gram_cache(i,j);
     K         -= noise;
-    real k     = pow(K / softplus(m_isp_signal_sigma), -1. / alpha);
+    real k     = pow(K / softplus(m_isp_signal_sigma), real(-1.) / alpha);
     return sigmoid(m_isp_alpha) * K * (1 - pl_log(k) - 1 / k);
 }
 

Modified: trunk/plearn_learners/nearest_neighbors/BallTreeNearestNeighbors.cc
===================================================================
--- trunk/plearn_learners/nearest_neighbors/BallTreeNearestNeighbors.cc	2007-04-09 17:27:40 UTC (rev 6858)
+++ trunk/plearn_learners/nearest_neighbors/BallTreeNearestNeighbors.cc	2007-04-09 18:07:57 UTC (rev 6859)
@@ -712,7 +712,7 @@
      priority_queue< pair<real,int> >& q, BinBallTree node,
      const Vec& t, real& d2_sofar, real d2_pivot, const int k ) const
 {
-    real d_minp = max( sqrt(d2_pivot) - node->radius, 0.0 );
+    real d_minp = max( sqrt(d2_pivot) - node->radius, real(0.0) );
 #ifdef DEBUG_CHECK_NAN
     if (isnan(d_minp))
         PLERROR("BallTreeNearestNeighbors::BallKNN: d_minp is NaN");

Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-04-09 17:27:40 UTC (rev 6858)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-04-09 18:07:57 UTC (rev 6859)
@@ -886,7 +886,7 @@
         {
             // set the input of the joint layer
             Vec target_exp = classification_module->target_layer->expectation;
-            fill_one_hot( target_exp, (int) round(target[0]), 0., 1. );
+            fill_one_hot( target_exp, (int) round(target[0]), real(0.), real(1.) );
 
             joint_layer->setLearningRate( cd_learning_rate );
             layers[ n_layers-1 ]->setLearningRate( cd_learning_rate );
@@ -1054,7 +1054,7 @@
     }
 
     Vec target_exp = classification_module->target_layer->expectation;
-    fill_one_hot( target_exp, (int) round(target[0]), 0., 1. );
+    fill_one_hot( target_exp, (int) round(target[0]), real(0.), real(1.) );
 
     contrastiveDivergenceStep(
         get_pointer( joint_layer ),

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-04-09 17:27:40 UTC (rev 6858)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-04-09 18:07:57 UTC (rev 6859)
@@ -91,7 +91,7 @@
     computeExpectation();
 
     int i = random_gen->multinomial_sample( expectation );
-    fill_one_hot( sample, i, 0., 1. );
+    fill_one_hot( sample, i, real(0.), real(1.) );
 }
 
 void RBMMultinomialLayer::computeExpectation()

Modified: trunk/plearn_learners/online/SquaredErrorCostModule.cc
===================================================================
--- trunk/plearn_learners/online/SquaredErrorCostModule.cc	2007-04-09 17:27:40 UTC (rev 6858)
+++ trunk/plearn_learners/online/SquaredErrorCostModule.cc	2007-04-09 18:07:57 UTC (rev 6859)
@@ -131,7 +131,7 @@
         PLASSERT_MSG( input_diag_hessian.size() == input_size,
                       "Cannot resize input_diag_hessian AND accumulate into it"
                     );
-        input_diag_hessian += 2.;
+        input_diag_hessian += real(2.);
     }
     else
     {

Modified: trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2007-04-09 17:27:40 UTC (rev 6858)
+++ trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2007-04-09 18:07:57 UTC (rev 6859)
@@ -427,7 +427,7 @@
         real nll = 0;
         for (int i=0, n=output.size() ; i<n ; ++i) {
             real sigma = m_intervals[i].second - m_intervals[i].first;
-            sigma = max(sigma, 1e-15);        // Very minor regularization
+            sigma = max(sigma, real(1e-15));        // Very minor regularization
             real diff = target[i] - output[i];
             nll += diff*diff / (2.*sigma*sigma) + pl_log(sigma) + LN_2PI_OVER_2;
         }
@@ -461,7 +461,7 @@
     m_gram_inverse_product.resize(m_kernel_evaluations.size());
     product(m_gram_inverse_product, m_gram_inverse, m_kernel_evaluations);
     real sigma_reductor = dot(m_gram_inverse_product, m_kernel_evaluations);
-    real sigma = sqrt(max(0., base_sigma_sq - sigma_reductor + m_confidence_epsilon));
+    real sigma = sqrt(max(real(0.), base_sigma_sq - sigma_reductor + m_confidence_epsilon));
 
     // two-tailed
     const real multiplier = gauss_01_quantile((1+probability)/2);
@@ -544,7 +544,7 @@
     // As a preventive measure, never output negative variance, even though
     // this does not garantee the non-negative-definiteness of the matrix
     for (int i=0 ; i<N ; ++i)
-        covmat(i,i) = max(0.0, covmat(i,i) + m_confidence_epsilon);
+        covmat(i,i) = max(real(0.0), covmat(i,i) + m_confidence_epsilon);
 }
 
 

Modified: trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc
===================================================================
--- trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc	2007-04-09 17:27:40 UTC (rev 6858)
+++ trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc	2007-04-09 18:07:57 UTC (rev 6859)
@@ -595,7 +595,7 @@
                 }
                 else
                 {
-                    diffSquareMultiplyAcc(s->value,weights,input,1.0);
+                    diffSquareMultiplyAcc(s->value,weights,input,real(1.0));
                     sum++;
                 }
             }
@@ -612,7 +612,7 @@
                         }
                         else
                         {
-                            diffSquareMultiplyAcc(s->value,weights,input,1.0);
+                            diffSquareMultiplyAcc(s->value,weights,input,real(1.0));
                             sum++;
                         }
                     }

Modified: trunk/plearn_learners_experimental/SurfaceTemplate/geometry.cc
===================================================================
--- trunk/plearn_learners_experimental/SurfaceTemplate/geometry.cc	2007-04-09 17:27:40 UTC (rev 6858)
+++ trunk/plearn_learners_experimental/SurfaceTemplate/geometry.cc	2007-04-09 18:07:57 UTC (rev 6859)
@@ -67,7 +67,7 @@
         angle[0] = -atan2( rot(0,1), rot(1,1) );
     }
 
-    return( angle * ( 180.0 / Pi ) );
+    return( angle * ( real(180.0) / real(Pi) ) );
 }
 
 Mat rotationMatrixFromAngles( real rx, real ry, real rz )



From yoshua at mail.berlios.de  Mon Apr  9 20:51:58 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Mon, 9 Apr 2007 20:51:58 +0200
Subject: [Plearn-commits] r6860 - trunk/scripts
Message-ID: <200704091851.l39IpwLm015009@sheep.berlios.de>

Author: yoshua
Date: 2007-04-09 20:51:58 +0200 (Mon, 09 Apr 2007)
New Revision: 6860

Modified:
   trunk/scripts/collectres
Log:
Added more functionality.


Modified: trunk/scripts/collectres
===================================================================
--- trunk/scripts/collectres	2007-04-09 18:07:57 UTC (rev 6859)
+++ trunk/scripts/collectres	2007-04-09 18:51:58 UTC (rev 6860)
@@ -33,16 +33,37 @@
 
 import sys,string
 from plearn.vmat.PMat import *
+from numarray import *
 
 def selectres(loc_specs,a):
   res = []
   loc_mode = loc_specs[0]
   if loc_mode=="pos":
-     row=int(loc_specs[1])
-     i=2
-     while len(loc_specs[i:])>0:
-       res.append(a[row,int(loc_specs[i])])
-       i+=1
+    row=int(loc_specs[1])
+    i=2
+    while len(loc_specs[i:])>0:
+      res.append(a[row,int(loc_specs[i])])
+      i+=1
+  elif loc_mode=="mincol":
+    mcol = int(loc_specs[1])
+    mrow = argmin(a[:,mcol])
+    print "found min row = ",mrow," for col ",mcol,", with value=",a[mrow,mcol]
+    i=1
+    while len(loc_specs[i:])>0:
+      res.append(a[mrow,int(loc_specs[i])])
+      i+=1
+  elif loc_mode=="cols":
+    minrow = int(loc_specs[1])
+    maxrow = int(loc_specs[2])
+    if maxrow<0:
+      maxrow = a.shape[0]+maxrow
+    maxrow += 1
+    i=3
+    res = zeros([maxrow-minrow,len(loc_specs[i:])])
+    while len(loc_specs[i:])>0:
+      res[:,i-3]=a[minrow:maxrow,int(loc_specs[i])]
+      i+=1
+    print res.shape
   else:
     raise ValueError("Invalid <location-spec> mode, expected 'pos', 'mincol', or 'col', got "+loc_mode)    
   return res
@@ -87,7 +108,11 @@
         f.write(str(v)+" ")
       f.write(res[1]+"\n")
   elif mode=="plot":
-    pass
+    arrays = []
+    for res in results:
+      arrays.append(res[0])
+    a = concatenate(arrays,1)
+    f.write(array_str(a))
   else:
     raise ValueError("Invalid <spec> mode, expected 'min', 'sort', or 'plot', got "+mode)
 
@@ -100,11 +125,11 @@
     print "The <spec> can be the following:"
     print '  "min <location-spec>" : identify the mininum of <location-spec> over the <file*.pmat>' 
     print '  "sort <location-spec>" : make a sorted table of all the values at each <location-spec> over the <file*.pmat>' 
-    print '  "plot <location-spec> " : make a plot of all the values at <location-spec> over the <file*.pmat>' 
+    print '  "plot <location-spec> " : make a smarplot file for plotting of all the values at <location-spec> over the <file*.pmat>' 
     print "where <location-spec> can be the following:"
     print '  "pos <row> <col0> [<col1> <col2>...]": keep only the value at positions (<row> <col0>) (<row> <col1>) ... in each pmat, and the first value may be used to minimize over files in the min command'
-    print '  "mincol <mcol> [<col1> <col2>...] ": keep only the minimum value in <mcol> of each pmat, keeping track of the <coli> values at that row'
-    print '  "col <column-nb>[<minrow> <maxrow>]": this is used with the plot command, to specify a column to plot, optionally from <minrow> to <maxrow> row inclusively'
+    print '  "mincol <mcol> [<col1> <col2>...] ": keep only the minimum value in <mcol> of each pmat, keeping track of the <coli> values at the selected row'
+    print '  "cols <minrow> <maxrow> <column-nb> <column-nb>...": this is used with the plot command, to specify a column to plot, optionally from <minrow> to <maxrow> (-1 means mat.length-1) row inclusively'
     sys.exit(1)
   output = args[1]
   filenames = args[3:]



From saintmlx at mail.berlios.de  Mon Apr  9 21:04:20 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Mon, 9 Apr 2007 21:04:20 +0200
Subject: [Plearn-commits] r6861 - in trunk: plearn/base plearn/ker
	plearn/math plearn/var plearn/vmat
	plearn_learners/distributions plearn_learners/hyper
	plearn_learners/nearest_neighbors
	plearn_learners/unsupervised plearn_learners_experimental
	scripts/Skeletons
Message-ID: <200704091904.l39J4KFW016571@sheep.berlios.de>

Author: saintmlx
Date: 2007-04-09 21:04:15 +0200 (Mon, 09 Apr 2007)
New Revision: 6861

Modified:
   trunk/plearn/base/SetOption.cc
   trunk/plearn/ker/NegKernel.cc
   trunk/plearn/ker/PrecomputedKernel.cc
   trunk/plearn/ker/ScaledGaussianKernel.cc
   trunk/plearn/ker/ScaledGeneralizedDistanceRBFKernel.cc
   trunk/plearn/ker/ScaledLaplacianKernel.cc
   trunk/plearn/ker/SelectedOutputCostFunction.cc
   trunk/plearn/ker/WeightedCostFunction.cc
   trunk/plearn/math/Binner.cc
   trunk/plearn/math/ConditionalCDFSmoother.cc
   trunk/plearn/math/LimitedGaussianSmoother.cc
   trunk/plearn/math/ManualBinner.cc
   trunk/plearn/math/ScaledConditionalCDFSmoother.cc
   trunk/plearn/math/Smoother.cc
   trunk/plearn/math/SoftHistogramBinner.cc
   trunk/plearn/var/ArgminOfVariable.cc
   trunk/plearn/var/BinaryVariable.cc
   trunk/plearn/var/CCCostVariable.cc
   trunk/plearn/var/ConcatOfVariable.cc
   trunk/plearn/var/Func.cc
   trunk/plearn/var/LogSumVariable.cc
   trunk/plearn/var/MatRowVariable.cc
   trunk/plearn/var/MatrixElementsVariable.cc
   trunk/plearn/var/MatrixSumOfVariable.cc
   trunk/plearn/var/NaryVariable.cc
   trunk/plearn/var/PDistributionVariable.cc
   trunk/plearn/var/PotentialsVariable.cc
   trunk/plearn/var/ReIndexedTargetVariable.cc
   trunk/plearn/var/RowOfVariable.cc
   trunk/plearn/var/SourceVariable.cc
   trunk/plearn/var/SumOfVariable.cc
   trunk/plearn/var/SumOverBagsVariable.cc
   trunk/plearn/var/UnaryVariable.cc
   trunk/plearn/var/UnfoldedFuncVariable.cc
   trunk/plearn/var/UnfoldedSumOfVariable.cc
   trunk/plearn/var/Variable.cc
   trunk/plearn/var/VecElementVariable.cc
   trunk/plearn/vmat/BinSplitter.cc
   trunk/plearn/vmat/BootstrapSplitter.cc
   trunk/plearn/vmat/ClassSeparationSplitter.cc
   trunk/plearn/vmat/ConcatSetsSplitter.cc
   trunk/plearn/vmat/DBSplitter.cc
   trunk/plearn/vmat/ExplicitSplitter.cc
   trunk/plearn/vmat/FilterSplitter.cc
   trunk/plearn/vmat/KernelVMatrix.cc
   trunk/plearn/vmat/KernelVMatrix.h
   trunk/plearn/vmat/MultiTargetOneHotVMatrix.cc
   trunk/plearn/vmat/MultiTaskSeparationSplitter.cc
   trunk/plearn/vmat/NoSplitSplitter.cc
   trunk/plearn/vmat/RepeatSplitter.cc
   trunk/plearn/vmat/SourceVMatrixSplitter.cc
   trunk/plearn/vmat/StackedSplitter.cc
   trunk/plearn/vmat/TestInTrainSplitter.cc
   trunk/plearn/vmat/ToBagSplitter.cc
   trunk/plearn/vmat/TrainValidTestSplitter.cc
   trunk/plearn_learners/distributions/HistogramDistribution.cc
   trunk/plearn_learners/hyper/OptimizeOptionOracle.cc
   trunk/plearn_learners/nearest_neighbors/BallTreeNearestNeighbors.cc
   trunk/plearn_learners/nearest_neighbors/BinaryBallTree.cc
   trunk/plearn_learners/unsupervised/EntropyContrastLearner.cc
   trunk/plearn_learners/unsupervised/KMeansClustering.cc
   trunk/plearn_learners_experimental/Correspondence.cc
   trunk/plearn_learners_experimental/ICP.cc
   trunk/plearn_learners_experimental/MeshEdge.cc
   trunk/plearn_learners_experimental/MeshFace.cc
   trunk/plearn_learners_experimental/MeshGraph.cc
   trunk/plearn_learners_experimental/MeshMatch.cc
   trunk/plearn_learners_experimental/MeshVertex.cc
   trunk/plearn_learners_experimental/Molecule.cc
   trunk/plearn_learners_experimental/SurfaceMesh.cc
   trunk/plearn_learners_experimental/Template.cc
   trunk/scripts/Skeletons/Binner.cc
   trunk/scripts/Skeletons/Smoother.cc
Log:
- changed BaseClassName::makeDeepCopyFromShallowCopy to inherited::makeDeepCopyFromShallowCopy



Modified: trunk/plearn/base/SetOption.cc
===================================================================
--- trunk/plearn/base/SetOption.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/base/SetOption.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -73,7 +73,7 @@
 
 void SetOption::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Object::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
 }
 
 void SetOption::apply(PP<Object> obj, const map<string, string>& aliases)

Modified: trunk/plearn/ker/NegKernel.cc
===================================================================
--- trunk/plearn/ker/NegKernel.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/ker/NegKernel.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -57,7 +57,7 @@
 
 void NegKernel::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Kernel::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(ker, copies);
 }
 

Modified: trunk/plearn/ker/PrecomputedKernel.cc
===================================================================
--- trunk/plearn/ker/PrecomputedKernel.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/ker/PrecomputedKernel.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -71,7 +71,7 @@
 
 void PrecomputedKernel::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Kernel::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(ker, copies);
     deepCopyField(precomputedK, copies);
 }

Modified: trunk/plearn/ker/ScaledGaussianKernel.cc
===================================================================
--- trunk/plearn/ker/ScaledGaussianKernel.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/ker/ScaledGaussianKernel.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -50,7 +50,7 @@
 
 void ScaledGaussianKernel::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Kernel::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(phi, copies);
 }
 

Modified: trunk/plearn/ker/ScaledGeneralizedDistanceRBFKernel.cc
===================================================================
--- trunk/plearn/ker/ScaledGeneralizedDistanceRBFKernel.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/ker/ScaledGeneralizedDistanceRBFKernel.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -50,7 +50,7 @@
 
 void ScaledGeneralizedDistanceRBFKernel::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Kernel::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(phi, copies);
     deepCopyField(a, copies);
 }

Modified: trunk/plearn/ker/ScaledLaplacianKernel.cc
===================================================================
--- trunk/plearn/ker/ScaledLaplacianKernel.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/ker/ScaledLaplacianKernel.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -50,7 +50,7 @@
 
 void ScaledLaplacianKernel::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Kernel::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(phi, copies);
 }
 

Modified: trunk/plearn/ker/SelectedOutputCostFunction.cc
===================================================================
--- trunk/plearn/ker/SelectedOutputCostFunction.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/ker/SelectedOutputCostFunction.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -51,7 +51,7 @@
 
 void SelectedOutputCostFunction::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Kernel::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(costfunc, copies);
 }
 

Modified: trunk/plearn/ker/WeightedCostFunction.cc
===================================================================
--- trunk/plearn/ker/WeightedCostFunction.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/ker/WeightedCostFunction.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -50,7 +50,7 @@
 
 void WeightedCostFunction::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Kernel::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(costfunc, copies);
 }
 

Modified: trunk/plearn/math/Binner.cc
===================================================================
--- trunk/plearn/math/Binner.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/math/Binner.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -95,7 +95,7 @@
 
 void Binner::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Object::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
 
     // ### Call deepCopyField on all "pointer-like" fields 
     // ### that you wish to be deepCopied rather than 

Modified: trunk/plearn/math/ConditionalCDFSmoother.cc
===================================================================
--- trunk/plearn/math/ConditionalCDFSmoother.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/math/ConditionalCDFSmoother.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -82,7 +82,7 @@
 
 void ConditionalCDFSmoother::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Object::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(prior_cdf, copies);
 }
 

Modified: trunk/plearn/math/LimitedGaussianSmoother.cc
===================================================================
--- trunk/plearn/math/LimitedGaussianSmoother.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/math/LimitedGaussianSmoother.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -97,7 +97,7 @@
 
 void LimitedGaussianSmoother::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Object::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
 
     // ### Call deepCopyField on all "pointer-like" fields 
     // ### that you wish to be deepCopied rather than 

Modified: trunk/plearn/math/ManualBinner.cc
===================================================================
--- trunk/plearn/math/ManualBinner.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/math/ManualBinner.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -95,7 +95,7 @@
 
 void ManualBinner::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Object::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
 
     // ### Call deepCopyField on all "pointer-like" fields 
     // ### that you wish to be deepCopied rather than 

Modified: trunk/plearn/math/ScaledConditionalCDFSmoother.cc
===================================================================
--- trunk/plearn/math/ScaledConditionalCDFSmoother.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/math/ScaledConditionalCDFSmoother.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -100,7 +100,7 @@
 
 void ScaledConditionalCDFSmoother::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Object::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
 }
 
 

Modified: trunk/plearn/math/Smoother.cc
===================================================================
--- trunk/plearn/math/Smoother.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/math/Smoother.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -93,7 +93,7 @@
 
 void Smoother::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Object::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
 
     // ### Call deepCopyField on all "pointer-like" fields 
     // ### that you wish to be deepCopied rather than 

Modified: trunk/plearn/math/SoftHistogramBinner.cc
===================================================================
--- trunk/plearn/math/SoftHistogramBinner.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/math/SoftHistogramBinner.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -192,7 +192,7 @@
 /////////////////////////////////
 void SoftHistogramBinner::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Object::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
 
     // ### Call deepCopyField on all "pointer-like" fields 
     // ### that you wish to be deepCopied rather than 

Modified: trunk/plearn/var/ArgminOfVariable.cc
===================================================================
--- trunk/plearn/var/ArgminOfVariable.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/var/ArgminOfVariable.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -72,7 +72,7 @@
 
 void ArgminOfVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    NaryVariable::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(inputs, copies);
     deepCopyField(expression, copies);
     deepCopyField(values_of_v, copies);

Modified: trunk/plearn/var/BinaryVariable.cc
===================================================================
--- trunk/plearn/var/BinaryVariable.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/var/BinaryVariable.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -118,7 +118,7 @@
 
 void BinaryVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Variable::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     //deepCopyField(input1, copies);
     varDeepCopyField(input1, copies);
     //deepCopyField(input2, copies);

Modified: trunk/plearn/var/CCCostVariable.cc
===================================================================
--- trunk/plearn/var/CCCostVariable.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/var/CCCostVariable.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -119,7 +119,7 @@
 
 void CCCostVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    NaryVariable::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(distr, copies);
     deepCopyField(f_error, copies);
     deepCopyField(f_candidate, copies);

Modified: trunk/plearn/var/ConcatOfVariable.cc
===================================================================
--- trunk/plearn/var/ConcatOfVariable.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/var/ConcatOfVariable.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -112,7 +112,7 @@
 
 void ConcatOfVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    NaryVariable::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(distr, copies);
     deepCopyField(f, copies);
     deepCopyField(input_value, copies);

Modified: trunk/plearn/var/Func.cc
===================================================================
--- trunk/plearn/var/Func.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/var/Func.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -186,7 +186,7 @@
 
 void Function::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Object::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(inputs, copies);
     deepCopyField(outputs, copies);
     deepCopyField(fproppath, copies);

Modified: trunk/plearn/var/LogSumVariable.cc
===================================================================
--- trunk/plearn/var/LogSumVariable.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/var/LogSumVariable.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -70,7 +70,7 @@
 
 void LogSumVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    UnaryVariable::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(input_softmax, copies);
 }
 

Modified: trunk/plearn/var/MatRowVariable.cc
===================================================================
--- trunk/plearn/var/MatRowVariable.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/var/MatRowVariable.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -88,21 +88,12 @@
     w = 1;
 }
 
-
-
-
-
-
-
-
 void MatRowVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    UnaryVariable::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(m, copies);
 }
 
-
-
 void MatRowVariable::fprop()
 {  value << m(int(input->value[0])); }
 

Modified: trunk/plearn/var/MatrixElementsVariable.cc
===================================================================
--- trunk/plearn/var/MatrixElementsVariable.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/var/MatrixElementsVariable.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -116,7 +116,7 @@
 
 void MatrixElementsVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    NaryVariable::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     varDeepCopyField(i, copies);
     varDeepCopyField(j, copies);
     varDeepCopyField(expression, copies);

Modified: trunk/plearn/var/MatrixSumOfVariable.cc
===================================================================
--- trunk/plearn/var/MatrixSumOfVariable.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/var/MatrixSumOfVariable.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -107,7 +107,7 @@
 
 void MatrixSumOfVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    NaryVariable::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(distr, copies);
     deepCopyField(f, copies);
 }

Modified: trunk/plearn/var/NaryVariable.cc
===================================================================
--- trunk/plearn/var/NaryVariable.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/var/NaryVariable.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -68,7 +68,7 @@
 
 void NaryVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Variable::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(varray, copies);
     //for(int i=0; i<varray.size(); i++)
     //  deepCopyField(varray[i], copies);

Modified: trunk/plearn/var/PDistributionVariable.cc
===================================================================
--- trunk/plearn/var/PDistributionVariable.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/var/PDistributionVariable.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -93,7 +93,7 @@
 
 void PDistributionVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Variable::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(dist, copies);
 }
 

Modified: trunk/plearn/var/PotentialsVariable.cc
===================================================================
--- trunk/plearn/var/PotentialsVariable.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/var/PotentialsVariable.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -122,7 +122,7 @@
 
 void PotentialsVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    NaryVariable::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     
     varDeepCopyField(output, copies);
     varDeepCopyField(comp_input, copies);

Modified: trunk/plearn/var/ReIndexedTargetVariable.cc
===================================================================
--- trunk/plearn/var/ReIndexedTargetVariable.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/var/ReIndexedTargetVariable.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -172,7 +172,7 @@
 
 void ReIndexedTargetVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    BinaryVariable::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(source, copies);
     deepCopyField(row, copies);
     deepCopyField(values, copies);

Modified: trunk/plearn/var/RowOfVariable.cc
===================================================================
--- trunk/plearn/var/RowOfVariable.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/var/RowOfVariable.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -93,7 +93,7 @@
 
 void RowOfVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    UnaryVariable::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(distr, copies);
 }
 

Modified: trunk/plearn/var/SourceVariable.cc
===================================================================
--- trunk/plearn/var/SourceVariable.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/var/SourceVariable.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -67,7 +67,7 @@
 
 void SourceVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Variable::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(rows_to_update, copies);
 }
 

Modified: trunk/plearn/var/SumOfVariable.cc
===================================================================
--- trunk/plearn/var/SumOfVariable.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/var/SumOfVariable.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -144,7 +144,7 @@
 
 void SumOfVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    NaryVariable::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(distr, copies);
     deepCopyField(f, copies);
 }

Modified: trunk/plearn/var/SumOverBagsVariable.cc
===================================================================
--- trunk/plearn/var/SumOverBagsVariable.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/var/SumOverBagsVariable.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -198,7 +198,7 @@
 /////////////////////////////////
 void SumOverBagsVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    NaryVariable::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(vmat, copies);
     deepCopyField(f, copies);
     deepCopyField(output_value, copies);

Modified: trunk/plearn/var/UnaryVariable.cc
===================================================================
--- trunk/plearn/var/UnaryVariable.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/var/UnaryVariable.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -77,7 +77,7 @@
 
 void UnaryVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Variable::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     //deepCopyField(input, copies);
     varDeepCopyField(input, copies); // a cause d'une bug du compilateur
 }

Modified: trunk/plearn/var/UnfoldedFuncVariable.cc
===================================================================
--- trunk/plearn/var/UnfoldedFuncVariable.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/var/UnfoldedFuncVariable.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -133,7 +133,7 @@
 
 void UnfoldedFuncVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    NaryVariable::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(input_matrix, copies);
     deepCopyField(f, copies);
     deepCopyField(inputs, copies);

Modified: trunk/plearn/var/UnfoldedSumOfVariable.cc
===================================================================
--- trunk/plearn/var/UnfoldedSumOfVariable.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/var/UnfoldedSumOfVariable.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -130,7 +130,7 @@
 
 void UnfoldedSumOfVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    NaryVariable::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     varDeepCopyField(input_matrix, copies);
     varDeepCopyField(bag_size, copies);
     deepCopyField(f, copies);

Modified: trunk/plearn/var/Variable.cc
===================================================================
--- trunk/plearn/var/Variable.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/var/Variable.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -397,7 +397,7 @@
 
 void Variable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Object::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(value, copies);
     deepCopyField(gradient, copies);
     deepCopyField(matValue, copies);

Modified: trunk/plearn/var/VecElementVariable.cc
===================================================================
--- trunk/plearn/var/VecElementVariable.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/var/VecElementVariable.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -84,7 +84,7 @@
 
 void VecElementVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    UnaryVariable::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(v, copies);
 }
 

Modified: trunk/plearn/vmat/BinSplitter.cc
===================================================================
--- trunk/plearn/vmat/BinSplitter.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/vmat/BinSplitter.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -101,7 +101,7 @@
 
 void BinSplitter::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Splitter::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
 
     // ### Call deepCopyField on all "pointer-like" fields
     // ### that you wish to be deepCopied rather than

Modified: trunk/plearn/vmat/BootstrapSplitter.cc
===================================================================
--- trunk/plearn/vmat/BootstrapSplitter.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/vmat/BootstrapSplitter.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -123,7 +123,7 @@
 /////////////////////////////////
 void BootstrapSplitter::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Splitter::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(bootstrapped_sets, copies);
     deepCopyField(rgen,              copies);
 }

Modified: trunk/plearn/vmat/ClassSeparationSplitter.cc
===================================================================
--- trunk/plearn/vmat/ClassSeparationSplitter.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/vmat/ClassSeparationSplitter.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -128,7 +128,7 @@
 
 void ClassSeparationSplitter::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Splitter::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
 
     deepCopyField(classes, copies);
     deepCopyField(random_gen, copies);

Modified: trunk/plearn/vmat/ConcatSetsSplitter.cc
===================================================================
--- trunk/plearn/vmat/ConcatSetsSplitter.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/vmat/ConcatSetsSplitter.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -94,7 +94,7 @@
 
 void ConcatSetsSplitter::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Splitter::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
 
     // ### Call deepCopyField on all "pointer-like" fields
     // ### that you wish to be deepCopied rather than

Modified: trunk/plearn/vmat/DBSplitter.cc
===================================================================
--- trunk/plearn/vmat/DBSplitter.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/vmat/DBSplitter.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -80,7 +80,7 @@
 
 void DBSplitter::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Splitter::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
 
     // ### Call deepCopyField on all "pointer-like" fields
     // ### that you wish to be deepCopied rather than

Modified: trunk/plearn/vmat/ExplicitSplitter.cc
===================================================================
--- trunk/plearn/vmat/ExplicitSplitter.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/vmat/ExplicitSplitter.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -85,7 +85,7 @@
 
 void ExplicitSplitter::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Splitter::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(splitsets, copies);
 }
 

Modified: trunk/plearn/vmat/FilterSplitter.cc
===================================================================
--- trunk/plearn/vmat/FilterSplitter.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/vmat/FilterSplitter.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -97,7 +97,7 @@
 
 void FilterSplitter::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Splitter::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(filters, copies);
 }
 

Modified: trunk/plearn/vmat/KernelVMatrix.cc
===================================================================
--- trunk/plearn/vmat/KernelVMatrix.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/vmat/KernelVMatrix.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -101,19 +101,19 @@
     inherited::declareOptions(ol);
 }
 
-/*
-  IMPLEMENT_NAME_AND_DEEPCOPY(KernelVMatrix);
-  void KernelVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
-  {
-  Kernel::makeDeepCopyFromShallowCopy(copies);
-  deepCopyField(source1, copies);
-  deepCopyField(source2, copies);
-  deepCopyField(ker, copies);
-  deepCopyField(input1, copies);
-  deepCopyField(input2, copies);
-  }
-*/
 
+//  IMPLEMENT_NAME_AND_DEEPCOPY(KernelVMatrix);
+void KernelVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+    deepCopyField(source1, copies);
+    deepCopyField(source2, copies);
+    deepCopyField(ker, copies);
+    deepCopyField(input1, copies);
+    deepCopyField(input2, copies);
+}
+
+
 real KernelVMatrix::get(int i, int j) const
 {
 #ifdef BOUNDCHECK

Modified: trunk/plearn/vmat/KernelVMatrix.h
===================================================================
--- trunk/plearn/vmat/KernelVMatrix.h	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/vmat/KernelVMatrix.h	2007-04-09 19:04:15 UTC (rev 6861)
@@ -83,7 +83,7 @@
     virtual void build();
 
     //DECLARE_NAME_AND_DEEPCOPY(KernelVMatrix);
-    //virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
     virtual real get(int i, int j) const;
     virtual void getSubRow(int i, int j, Vec v) const;
 private:

Modified: trunk/plearn/vmat/MultiTargetOneHotVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MultiTargetOneHotVMatrix.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/vmat/MultiTargetOneHotVMatrix.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -425,7 +425,7 @@
 /////////////////////////////////
 // makeDeepCopyFromShallowCopy //
 /////////////////////////////////
-void MultiTargetOneHotVMatrix::makeDeepCopyFromShallowCopy(map<const void*, void*>& copies)
+void MultiTargetOneHotVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
   inherited::makeDeepCopyFromShallowCopy(copies);
   deepCopyField(data, copies);

Modified: trunk/plearn/vmat/MultiTaskSeparationSplitter.cc
===================================================================
--- trunk/plearn/vmat/MultiTaskSeparationSplitter.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/vmat/MultiTaskSeparationSplitter.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -113,7 +113,7 @@
 
 void MultiTaskSeparationSplitter::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Splitter::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
 
     deepCopyField(tasks, copies);
     deepCopyField(random_gen, copies);

Modified: trunk/plearn/vmat/NoSplitSplitter.cc
===================================================================
--- trunk/plearn/vmat/NoSplitSplitter.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/vmat/NoSplitSplitter.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -103,7 +103,7 @@
 /////////////////////////////////
 void NoSplitSplitter::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Splitter::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
 
     // ### Call deepCopyField on all "pointer-like" fields
     // ### that you wish to be deepCopied rather than

Modified: trunk/plearn/vmat/RepeatSplitter.cc
===================================================================
--- trunk/plearn/vmat/RepeatSplitter.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/vmat/RepeatSplitter.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -280,7 +280,7 @@
 /////////////////////////////////
 void RepeatSplitter::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Splitter::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
 
     // ### Call deepCopyField on all "pointer-like" fields
     // ### that you wish to be deepCopied rather than

Modified: trunk/plearn/vmat/SourceVMatrixSplitter.cc
===================================================================
--- trunk/plearn/vmat/SourceVMatrixSplitter.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/vmat/SourceVMatrixSplitter.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -109,7 +109,7 @@
 /////////////////////////////////
 void SourceVMatrixSplitter::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Splitter::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
 
     // ### Call deepCopyField on all "pointer-like" fields
     // ### that you wish to be deepCopied rather than

Modified: trunk/plearn/vmat/StackedSplitter.cc
===================================================================
--- trunk/plearn/vmat/StackedSplitter.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/vmat/StackedSplitter.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -148,7 +148,7 @@
 /////////////////////////////////
 void StackedSplitter::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Splitter::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
 
     // ### Call deepCopyField on all "pointer-like" fields
     // ### that you wish to be deepCopied rather than

Modified: trunk/plearn/vmat/TestInTrainSplitter.cc
===================================================================
--- trunk/plearn/vmat/TestInTrainSplitter.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/vmat/TestInTrainSplitter.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -110,7 +110,7 @@
 
 void TestInTrainSplitter::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Splitter::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
 
     // ### Call deepCopyField on all "pointer-like" fields
     // ### that you wish to be deepCopied rather than

Modified: trunk/plearn/vmat/ToBagSplitter.cc
===================================================================
--- trunk/plearn/vmat/ToBagSplitter.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/vmat/ToBagSplitter.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -162,7 +162,7 @@
 /////////////////////////////////
 void ToBagSplitter::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Splitter::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
 
     deepCopyField(bags_index, copies);
     deepCopyField(sub_splitter, copies);

Modified: trunk/plearn/vmat/TrainValidTestSplitter.cc
===================================================================
--- trunk/plearn/vmat/TrainValidTestSplitter.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn/vmat/TrainValidTestSplitter.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -171,7 +171,7 @@
 /////////////////////////////////
 void TrainValidTestSplitter::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Splitter::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
 
     // ### Call deepCopyField on all "pointer-like" fields
     // ### that you wish to be deepCopied rather than

Modified: trunk/plearn_learners/distributions/HistogramDistribution.cc
===================================================================
--- trunk/plearn_learners/distributions/HistogramDistribution.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn_learners/distributions/HistogramDistribution.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -187,7 +187,7 @@
 
 void HistogramDistribution::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    PLearner::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
 
     deepCopyField(bin_positions, copies);
     deepCopyField(bin_density, copies);

Modified: trunk/plearn_learners/hyper/OptimizeOptionOracle.cc
===================================================================
--- trunk/plearn_learners/hyper/OptimizeOptionOracle.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn_learners/hyper/OptimizeOptionOracle.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -268,7 +268,7 @@
 /////////////////////////////////
 // makeDeepCopyFromShallowCopy //
 /////////////////////////////////
-void OptimizeOptionOracle::makeDeepCopyFromShallowCopy(map<const void*, void*>& copies)
+void OptimizeOptionOracle::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 

Modified: trunk/plearn_learners/nearest_neighbors/BallTreeNearestNeighbors.cc
===================================================================
--- trunk/plearn_learners/nearest_neighbors/BallTreeNearestNeighbors.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn_learners/nearest_neighbors/BallTreeNearestNeighbors.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -143,7 +143,7 @@
 }
 
 
-void BallTreeNearestNeighbors::makeDeepCopyFromShallowCopy(map<const void*, void*>& copies)
+void BallTreeNearestNeighbors::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 

Modified: trunk/plearn_learners/nearest_neighbors/BinaryBallTree.cc
===================================================================
--- trunk/plearn_learners/nearest_neighbors/BinaryBallTree.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn_learners/nearest_neighbors/BinaryBallTree.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -127,7 +127,7 @@
     return this->parent;
 }
 
-void BinaryBallTree::makeDeepCopyFromShallowCopy(map<const void*, void*>& copies)
+void BinaryBallTree::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 

Modified: trunk/plearn_learners/unsupervised/EntropyContrastLearner.cc
===================================================================
--- trunk/plearn_learners/unsupervised/EntropyContrastLearner.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn_learners/unsupervised/EntropyContrastLearner.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -389,7 +389,7 @@
 }
 
 
-void EntropyContrastLearner::makeDeepCopyFromShallowCopy(map<const void*, void*>& copies)
+void EntropyContrastLearner::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 

Modified: trunk/plearn_learners/unsupervised/KMeansClustering.cc
===================================================================
--- trunk/plearn_learners/unsupervised/KMeansClustering.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn_learners/unsupervised/KMeansClustering.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -78,7 +78,7 @@
 }
 
 
-void KMeansClustering::makeDeepCopyFromShallowCopy(map<const void*, void*>& copies)
+void KMeansClustering::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(clusters_, copies);

Modified: trunk/plearn_learners_experimental/Correspondence.cc
===================================================================
--- trunk/plearn_learners_experimental/Correspondence.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn_learners_experimental/Correspondence.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -99,7 +99,7 @@
   build_();
 }
 
-void Correspondence::makeDeepCopyFromShallowCopy(map<const void*, void*>& copies)
+void Correspondence::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
   inherited::makeDeepCopyFromShallowCopy(copies);
 

Modified: trunk/plearn_learners_experimental/ICP.cc
===================================================================
--- trunk/plearn_learners_experimental/ICP.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn_learners_experimental/ICP.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -991,7 +991,7 @@
   return real(1);
 }
 
-void ICP::makeDeepCopyFromShallowCopy(map<const void*, void*>& copies)
+void ICP::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
   inherited::makeDeepCopyFromShallowCopy(copies);
 

Modified: trunk/plearn_learners_experimental/MeshEdge.cc
===================================================================
--- trunk/plearn_learners_experimental/MeshEdge.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn_learners_experimental/MeshEdge.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -99,7 +99,7 @@
   build_();
 }
 
-void MeshEdge::makeDeepCopyFromShallowCopy(map<const void*, void*>& copies)
+void MeshEdge::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
   inherited::makeDeepCopyFromShallowCopy(copies);
 }

Modified: trunk/plearn_learners_experimental/MeshFace.cc
===================================================================
--- trunk/plearn_learners_experimental/MeshFace.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn_learners_experimental/MeshFace.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -104,7 +104,7 @@
   build_();
 }
 
-void MeshFace::makeDeepCopyFromShallowCopy(map<const void*, void*>& copies)
+void MeshFace::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
   inherited::makeDeepCopyFromShallowCopy(copies);
 

Modified: trunk/plearn_learners_experimental/MeshGraph.cc
===================================================================
--- trunk/plearn_learners_experimental/MeshGraph.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn_learners_experimental/MeshGraph.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -95,7 +95,7 @@
   build_();
 }
 
-void MeshGraph::makeDeepCopyFromShallowCopy(map<const void*, void*>& copies)
+void MeshGraph::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
   inherited::makeDeepCopyFromShallowCopy(copies);
 

Modified: trunk/plearn_learners_experimental/MeshMatch.cc
===================================================================
--- trunk/plearn_learners_experimental/MeshMatch.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn_learners_experimental/MeshMatch.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -119,7 +119,7 @@
   build_();
 }
 
-void MeshMatch::makeDeepCopyFromShallowCopy(map<const void*, void*>& copies)
+void MeshMatch::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
   inherited::makeDeepCopyFromShallowCopy(copies);
 

Modified: trunk/plearn_learners_experimental/MeshVertex.cc
===================================================================
--- trunk/plearn_learners_experimental/MeshVertex.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn_learners_experimental/MeshVertex.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -103,7 +103,7 @@
   build_();
 }
 
-void MeshVertex::makeDeepCopyFromShallowCopy(map<const void*, void*>& copies)
+void MeshVertex::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
   inherited::makeDeepCopyFromShallowCopy(copies);
 

Modified: trunk/plearn_learners_experimental/Molecule.cc
===================================================================
--- trunk/plearn_learners_experimental/Molecule.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn_learners_experimental/Molecule.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -48,7 +48,7 @@
   build_();
 }
 
-void Molecule::makeDeepCopyFromShallowCopy(map<const void*, void*>& copies)
+void Molecule::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
   inherited::makeDeepCopyFromShallowCopy(copies);
 

Modified: trunk/plearn_learners_experimental/SurfaceMesh.cc
===================================================================
--- trunk/plearn_learners_experimental/SurfaceMesh.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn_learners_experimental/SurfaceMesh.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -129,7 +129,7 @@
   build_();
 }
 
-void SurfaceMesh::makeDeepCopyFromShallowCopy(map<const void*, void*>& copies)
+void SurfaceMesh::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
   inherited::makeDeepCopyFromShallowCopy(copies);
 

Modified: trunk/plearn_learners_experimental/Template.cc
===================================================================
--- trunk/plearn_learners_experimental/Template.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/plearn_learners_experimental/Template.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -29,7 +29,7 @@
   build_();
 }
 
-void Template::makeDeepCopyFromShallowCopy(map<const void*, void*>& copies)
+void Template::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
   inherited::makeDeepCopyFromShallowCopy(copies);
   deepCopyField(dev, copies);

Modified: trunk/scripts/Skeletons/Binner.cc
===================================================================
--- trunk/scripts/Skeletons/Binner.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/scripts/Skeletons/Binner.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -83,7 +83,7 @@
 /////////////////////////////////
 void DERIVEDCLASS::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Object::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
 
     // ### Call deepCopyField on all "pointer-like" fields
     // ### that you wish to be deepCopied rather than

Modified: trunk/scripts/Skeletons/Smoother.cc
===================================================================
--- trunk/scripts/Skeletons/Smoother.cc	2007-04-09 18:51:58 UTC (rev 6860)
+++ trunk/scripts/Skeletons/Smoother.cc	2007-04-09 19:04:15 UTC (rev 6861)
@@ -66,7 +66,7 @@
 
 void DERIVEDCLASS::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Object::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
 
     // ### Call deepCopyField on all "pointer-like" fields
     // ### that you wish to be deepCopied rather than



From chapados at mail.berlios.de  Mon Apr  9 21:15:17 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Mon, 9 Apr 2007 21:15:17 +0200
Subject: [Plearn-commits] r6862 -
	trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results
Message-ID: <200704091915.l39JFHC6017363@sheep.berlios.de>

Author: chapados
Date: 2007-04-09 21:15:17 +0200 (Mon, 09 Apr 2007)
New Revision: 6862

Modified:
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
Log:
New test results following non-inclusion of iid noise in test-time kernel evaluation

Modified: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2007-04-09 19:04:15 UTC (rev 6861)
+++ trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2007-04-09 19:15:17 UTC (rev 6862)
@@ -93,18 +93,18 @@
 nservers = 0 ;
 save_trainingset_prefix = ""  )
 
-!R 1 1 [ 15.0000000000000036 ] 
+!R 1 1 [ 14.9482509617863855 ] 
 !R 1 1 [ 14.4446958356354838 ] 
 !R 2 4  1  [ 
 13.4992090127011988 	
 14.4141233331847474 	
-15.0000000000000036 	
+14.9482509617863855 	
 14.4446958356354838 	
 ]
 1 [ 4  4  [ 
-0.510839430933994199 	0.283483493891058203 	-2.84217094304040074e-14 	-0.555619592261717088 	
-0.283483493891065308 	0.391896855149343415 	-2.84217094304040074e-14 	-0.369897017889257995 	
--2.13162820728030056e-14 	-2.48689957516035065e-14 	9.99997513100424861e-09 	-2.13162820728030056e-14 	
--0.555619592261709982 	-0.36989701788925089 	-1.77635683940025046e-14 	2.23845483383840982 	
+0.366757393114872932 	0.283483493891058203 	0.064276585896223537 	-0.555619592261717088 	
+0.283483493891065308 	0.247814817330222203 	0.103882973846445736 	-0.369897017889257995 	
+0.0642765858962341952 	0.103882973846449289 	0.141717642757632822 	0.185897571267378936 	
+-0.555619592261709982 	-0.36989701788925089 	0.185897571267382489 	2.09437279601928861 	
 ]
 ] 



From saintmlx at mail.berlios.de  Mon Apr  9 22:15:53 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Mon, 9 Apr 2007 22:15:53 +0200
Subject: [Plearn-commits] r6863 - in trunk: plearn/vmat
	plearn_learners/hyper plearn_learners/nearest_neighbors
	plearn_learners/unsupervised plearn_learners_experimental
Message-ID: <200704092015.l39KFrK2022382@sheep.berlios.de>

Author: saintmlx
Date: 2007-04-09 22:15:52 +0200 (Mon, 09 Apr 2007)
New Revision: 6863

Modified:
   trunk/plearn/vmat/MultiTargetOneHotVMatrix.h
   trunk/plearn_learners/hyper/OptimizeOptionOracle.h
   trunk/plearn_learners/nearest_neighbors/BallTreeNearestNeighbors.h
   trunk/plearn_learners/nearest_neighbors/BinaryBallTree.h
   trunk/plearn_learners/unsupervised/EntropyContrastLearner.h
   trunk/plearn_learners/unsupervised/KMeansClustering.h
   trunk/plearn_learners_experimental/Correspondence.h
   trunk/plearn_learners_experimental/ICP.h
   trunk/plearn_learners_experimental/MeshEdge.h
   trunk/plearn_learners_experimental/MeshFace.h
   trunk/plearn_learners_experimental/MeshGraph.h
   trunk/plearn_learners_experimental/MeshMatch.h
   trunk/plearn_learners_experimental/MeshVertex.h
   trunk/plearn_learners_experimental/Molecule.h
   trunk/plearn_learners_experimental/SurfaceMesh.h
   trunk/plearn_learners_experimental/Template.h
Log:
- changed map<const void*, void*> to CopiesMap where appropriate



Modified: trunk/plearn/vmat/MultiTargetOneHotVMatrix.h
===================================================================
--- trunk/plearn/vmat/MultiTargetOneHotVMatrix.h	2007-04-09 19:15:17 UTC (rev 6862)
+++ trunk/plearn/vmat/MultiTargetOneHotVMatrix.h	2007-04-09 20:15:52 UTC (rev 6863)
@@ -121,7 +121,7 @@
   virtual void build();
 
   //! Transforms a shallow copy into a deep copy.
-  virtual void makeDeepCopyFromShallowCopy(map<const void*, void*>& copies);
+  virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
   //! Declares name and deepCopy methods
   PLEARN_DECLARE_OBJECT(MultiTargetOneHotVMatrix);

Modified: trunk/plearn_learners/hyper/OptimizeOptionOracle.h
===================================================================
--- trunk/plearn_learners/hyper/OptimizeOptionOracle.h	2007-04-09 19:15:17 UTC (rev 6862)
+++ trunk/plearn_learners/hyper/OptimizeOptionOracle.h	2007-04-09 20:15:52 UTC (rev 6863)
@@ -117,7 +117,7 @@
     virtual void build();
 
     //! Transforms a shallow copy into a deep copy
-    virtual void makeDeepCopyFromShallowCopy(map<const void*, void*>& copies);
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
     //! Declares name and deepCopy methods
     PLEARN_DECLARE_OBJECT(OptimizeOptionOracle);

Modified: trunk/plearn_learners/nearest_neighbors/BallTreeNearestNeighbors.h
===================================================================
--- trunk/plearn_learners/nearest_neighbors/BallTreeNearestNeighbors.h	2007-04-09 19:15:17 UTC (rev 6862)
+++ trunk/plearn_learners/nearest_neighbors/BallTreeNearestNeighbors.h	2007-04-09 20:15:52 UTC (rev 6863)
@@ -153,7 +153,7 @@
     virtual void build();
 
     //! Transforms a shallow copy into a deep copy.
-    virtual void makeDeepCopyFromShallowCopy(map<const void*, void*>& copies);
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
     // Declares other standard object methods.
     PLEARN_DECLARE_OBJECT(BallTreeNearestNeighbors);

Modified: trunk/plearn_learners/nearest_neighbors/BinaryBallTree.h
===================================================================
--- trunk/plearn_learners/nearest_neighbors/BinaryBallTree.h	2007-04-09 19:15:17 UTC (rev 6862)
+++ trunk/plearn_learners/nearest_neighbors/BinaryBallTree.h	2007-04-09 20:15:52 UTC (rev 6863)
@@ -106,7 +106,7 @@
     virtual void build();
 
     //! Transforms a shallow copy into a deep copy
-    virtual void makeDeepCopyFromShallowCopy(map<const void*, void*>& copies);
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
     virtual void setFirstChild( const BinBallTree& first_child );
 

Modified: trunk/plearn_learners/unsupervised/EntropyContrastLearner.h
===================================================================
--- trunk/plearn_learners/unsupervised/EntropyContrastLearner.h	2007-04-09 19:15:17 UTC (rev 6862)
+++ trunk/plearn_learners/unsupervised/EntropyContrastLearner.h	2007-04-09 20:15:52 UTC (rev 6863)
@@ -148,7 +148,7 @@
     virtual void build();
 
     //! Transforms a shallow copy into a deep copy.
-    virtual void makeDeepCopyFromShallowCopy(map<const void*, void*>& copies);
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
     // Declares other standard object methods.
     // If your class is not instantiatable (it has pure virtual methods)

Modified: trunk/plearn_learners/unsupervised/KMeansClustering.h
===================================================================
--- trunk/plearn_learners/unsupervised/KMeansClustering.h	2007-04-09 19:15:17 UTC (rev 6862)
+++ trunk/plearn_learners/unsupervised/KMeansClustering.h	2007-04-09 20:15:52 UTC (rev 6863)
@@ -81,7 +81,7 @@
     virtual void build();
 
     //! Transforms a shallow copy into a deep copy.
-    virtual void makeDeepCopyFromShallowCopy(map<const void*, void*>& copies);
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
     // Declares other standard object methods.
     PLEARN_DECLARE_OBJECT(KMeansClustering);

Modified: trunk/plearn_learners_experimental/Correspondence.h
===================================================================
--- trunk/plearn_learners_experimental/Correspondence.h	2007-04-09 19:15:17 UTC (rev 6862)
+++ trunk/plearn_learners_experimental/Correspondence.h	2007-04-09 20:15:52 UTC (rev 6863)
@@ -105,7 +105,7 @@
 
   //! Transforms a shallow copy into a deep copy
   // (PLEASE IMPLEMENT IN .cc)
-  virtual void makeDeepCopyFromShallowCopy(map<const void*, void*>& copies);
+  virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
   Correspondence& operator=( const Correspondence& c );
 

Modified: trunk/plearn_learners_experimental/ICP.h
===================================================================
--- trunk/plearn_learners_experimental/ICP.h	2007-04-09 19:15:17 UTC (rev 6862)
+++ trunk/plearn_learners_experimental/ICP.h	2007-04-09 20:15:52 UTC (rev 6863)
@@ -1,4 +1,4 @@
-// -*- C++ -*-
+%// -*- C++ -*-
 
 // ICP.h
 //
@@ -194,7 +194,7 @@
 
   //! Transforms a shallow copy into a deep copy
   // (PLEASE IMPLEMENT IN .cc)
-  virtual void makeDeepCopyFromShallowCopy(map<const void*, void*>& copies);
+  virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
   int iterate();
 

Modified: trunk/plearn_learners_experimental/MeshEdge.h
===================================================================
--- trunk/plearn_learners_experimental/MeshEdge.h	2007-04-09 19:15:17 UTC (rev 6862)
+++ trunk/plearn_learners_experimental/MeshEdge.h	2007-04-09 20:15:52 UTC (rev 6863)
@@ -107,7 +107,7 @@
   virtual void build();
 
   //! Transforms a shallow copy into a deep copy
-  virtual void makeDeepCopyFromShallowCopy(map<const void*, void*>& copies);
+  virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
 /*
   //! operator= overloading

Modified: trunk/plearn_learners_experimental/MeshFace.h
===================================================================
--- trunk/plearn_learners_experimental/MeshFace.h	2007-04-09 19:15:17 UTC (rev 6862)
+++ trunk/plearn_learners_experimental/MeshFace.h	2007-04-09 20:15:52 UTC (rev 6863)
@@ -125,7 +125,7 @@
 
   //! Transforms a shallow copy into a deep copy
   // (PLEASE IMPLEMENT IN .cc)
-  virtual void makeDeepCopyFromShallowCopy(map<const void*, void*>& copies);
+  virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
 /*
   MeshFace& operator=( const MeshFace& f );

Modified: trunk/plearn_learners_experimental/MeshGraph.h
===================================================================
--- trunk/plearn_learners_experimental/MeshGraph.h	2007-04-09 19:15:17 UTC (rev 6862)
+++ trunk/plearn_learners_experimental/MeshGraph.h	2007-04-09 20:15:52 UTC (rev 6863)
@@ -111,7 +111,7 @@
   virtual void build();
 
   //! Transforms a shallow copy into a deep copy
-  virtual void makeDeepCopyFromShallowCopy(map<const void*, void*>& copies);
+  virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
 /*
   MeshGraph& operator=( const MeshGraph& g );

Modified: trunk/plearn_learners_experimental/MeshMatch.h
===================================================================
--- trunk/plearn_learners_experimental/MeshMatch.h	2007-04-09 19:15:17 UTC (rev 6862)
+++ trunk/plearn_learners_experimental/MeshMatch.h	2007-04-09 20:15:52 UTC (rev 6863)
@@ -114,7 +114,7 @@
 
   //! Transforms a shallow copy into a deep copy
   // (PLEASE IMPLEMENT IN .cc)
-  virtual void makeDeepCopyFromShallowCopy(map<const void*, void*>& copies);
+  virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
 /*
   MeshMatch& operator=( const MeshMatch& m );

Modified: trunk/plearn_learners_experimental/MeshVertex.h
===================================================================
--- trunk/plearn_learners_experimental/MeshVertex.h	2007-04-09 19:15:17 UTC (rev 6862)
+++ trunk/plearn_learners_experimental/MeshVertex.h	2007-04-09 20:15:52 UTC (rev 6863)
@@ -108,7 +108,7 @@
   virtual void build();
 
   //! Transforms a shallow copy into a deep copy
-  virtual void makeDeepCopyFromShallowCopy(map<const void*, void*>& copies);
+  virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
 /*
   //! operator= overloading

Modified: trunk/plearn_learners_experimental/Molecule.h
===================================================================
--- trunk/plearn_learners_experimental/Molecule.h	2007-04-09 19:15:17 UTC (rev 6862)
+++ trunk/plearn_learners_experimental/Molecule.h	2007-04-09 20:15:52 UTC (rev 6863)
@@ -50,7 +50,7 @@
   virtual void build();
 
   //! Transforms a shallow copy into a deep copy
-  virtual void makeDeepCopyFromShallowCopy(map<const void*, void*>& copies);
+  virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
 
 

Modified: trunk/plearn_learners_experimental/SurfaceMesh.h
===================================================================
--- trunk/plearn_learners_experimental/SurfaceMesh.h	2007-04-09 19:15:17 UTC (rev 6862)
+++ trunk/plearn_learners_experimental/SurfaceMesh.h	2007-04-09 20:15:52 UTC (rev 6863)
@@ -125,7 +125,7 @@
 
   //! Transforms a shallow copy into a deep copy
   // (PLEASE IMPLEMENT IN .cc)
-  virtual void makeDeepCopyFromShallowCopy(map<const void*, void*>& copies);
+  virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
 protected:
   virtual bool readVRMLCoordinate3_( ifstream& in,

Modified: trunk/plearn_learners_experimental/Template.h
===================================================================
--- trunk/plearn_learners_experimental/Template.h	2007-04-09 19:15:17 UTC (rev 6862)
+++ trunk/plearn_learners_experimental/Template.h	2007-04-09 20:15:52 UTC (rev 6863)
@@ -52,7 +52,7 @@
   virtual void build();
 
   //! Transforms a shallow copy into a deep copy
-  virtual void makeDeepCopyFromShallowCopy(map<const void*, void*>& copies);
+  virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
 
 



From larocheh at mail.berlios.de  Tue Apr 10 00:02:18 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Tue, 10 Apr 2007 00:02:18 +0200
Subject: [Plearn-commits] r6864 - trunk/plearn/var
Message-ID: <200704092202.l39M2IDg028243@sheep.berlios.de>

Author: larocheh
Date: 2007-04-10 00:02:18 +0200 (Tue, 10 Apr 2007)
New Revision: 6864

Modified:
   trunk/plearn/var/PlusColumnVariable.h
Log:
Added an inline function to create a PlusColumnVariable...


Modified: trunk/plearn/var/PlusColumnVariable.h
===================================================================
--- trunk/plearn/var/PlusColumnVariable.h	2007-04-09 20:15:52 UTC (rev 6863)
+++ trunk/plearn/var/PlusColumnVariable.h	2007-04-09 22:02:18 UTC (rev 6864)
@@ -77,6 +77,12 @@
 
 DECLARE_OBJECT_PTR(PlusColumnVariable);
 
+inline Var plusColumn(Var v1, Var v2)
+{ 
+    return new PlusColumnVariable(v1,v2); 
+}
+
+
 } // end of namespace PLearn
 
 #endif 



From larocheh at mail.berlios.de  Tue Apr 10 00:04:34 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Tue, 10 Apr 2007 00:04:34 +0200
Subject: [Plearn-commits] r6865 - trunk/plearn/math
Message-ID: <200704092204.l39M4Yb2028457@sheep.berlios.de>

Author: larocheh
Date: 2007-04-10 00:04:33 +0200 (Tue, 10 Apr 2007)
New Revision: 6865

Modified:
   trunk/plearn/math/StatsCollector.cc
   trunk/plearn/math/StatsCollector.h
Log:
Added the MEAN_LIFT statistic


Modified: trunk/plearn/math/StatsCollector.cc
===================================================================
--- trunk/plearn/math/StatsCollector.cc	2007-04-09 22:02:18 UTC (rev 6864)
+++ trunk/plearn/math/StatsCollector.cc	2007-04-09 22:04:33 UTC (rev 6865)
@@ -103,6 +103,7 @@
     "  - IQR          -  The interquartile range, i.e. PSEUDOQ(0.75) - PSEUDOQ(0.25)\n"
     "  - PRR          -  The pseudo robust range, i.e. PSEUDOQ(0.99) - PSEUDOQ(0.01)\n"
     "  - LIFT(f)      -  Lift computed at fraction f (0 <= f <= 1)\n"
+    "  - MEAN_LIFT    -  Area under lift curve, normalized by the number of examples\n"
     "  - NIPS_LIFT    -  Area under lift curve as computed in NIPS'2004 challenge\n"
     "  - PRBP         -  Precision / Recall Breakeven Point = value of precision and recall\n"
     "                    when they both are equal (computed for the positive class)\n"
@@ -124,6 +125,7 @@
     "      that the formulas on the web site and in the python script are different).\n"
     "  - LIFT(f) actually returns - 100 * LIFT(f), so that lower means better, and it is\n"
     "    scaled by 100, as it is common practice.\n"
+    "  - MEAN_LIFT actually returns -1 * MEAN_LIFT, so that lower means better.\n"
     "  - The comments about the LIFT also apply to the BRPB statistic.\n"
     "  - The skewness and kurtosis are computed in terms of UNCENTERED ACCUMULATORS,\n"
     "    i.e. sum{(x-a)^n}, where a is the first observation encountered, and n is some integer\n"
@@ -947,6 +949,7 @@
         statistics["IQR"]         = STATFUN(&StatsCollector::iqr);
         statistics["PRR"]         = STATFUN(&StatsCollector::prr);
         statistics["NIPS_LIFT"]   = STATFUN(&StatsCollector::nips_lift);
+        statistics["MEAN_LIFT"]   = STATFUN(&StatsCollector::mean_lift);
         statistics["PRBP"]        = STATFUN(&StatsCollector::prbp);
         statistics["DMODE"]       = STATFUN(&StatsCollector::dmode);
         init = true;
@@ -1094,6 +1097,27 @@
     return result;
 }
 
+///////////////
+// mean_lift //
+///////////////
+real StatsCollector::mean_lift() const
+{
+    if (more_than_maxnvalues)
+        PLWARNING("In StatsCollector::nips_lift - You need to increase "
+                  "'maxnvalues' (or set it to -1) to get an accurate "
+                  "statistic");
+    if (!sorted)
+        sort_values_by_magnitude();
+    real n_total = real(sorted_values.length());
+    real pos_fraction = int(round(PLearn::sum(sorted_values.column(1)))) / n_total;
+    int n_pos_in_k_minus_1 = -1;
+    real result = 0;
+    for (int k = 0; k < sorted_values.length(); k++)
+        result += lift(k + 1, n_pos_in_k_minus_1, n_pos_in_k_minus_1, pos_fraction);
+    result /= n_total;
+    return -result;
+}
+
 //////////
 // prbp //
 //////////

Modified: trunk/plearn/math/StatsCollector.h
===================================================================
--- trunk/plearn/math/StatsCollector.h	2007-04-09 22:02:18 UTC (rev 6864)
+++ trunk/plearn/math/StatsCollector.h	2007-04-09 22:04:33 UTC (rev 6865)
@@ -263,6 +263,7 @@
     //! must be the average fraction of positive examples in the dataset (= n+ / n).
     real lift(int k, int& n_pos_in_k, int n_pos_in_k_minus_1 = -1, real pos_fraction = -1) const;
     real nips_lift() const;   //!< NIPS_LIFT statistic (see help).
+    real mean_lift() const;   //!< MEAN_LIFT statistic (see help).
     real prbp() const;        //!< PRBP statistic (see help).
     //! discrete distribution mode
     real dmode() const;



From larocheh at mail.berlios.de  Tue Apr 10 00:07:53 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Tue, 10 Apr 2007 00:07:53 +0200
Subject: [Plearn-commits] r6866 - trunk/plearn/vmat
Message-ID: <200704092207.l39M7r1Q028800@sheep.berlios.de>

Author: larocheh
Date: 2007-04-10 00:07:53 +0200 (Tue, 10 Apr 2007)
New Revision: 6866

Modified:
   trunk/plearn/vmat/SplitWiseValidationVMatrix.cc
   trunk/plearn/vmat/SplitWiseValidationVMatrix.h
Log:
Added an option to compute the average over all splits...


Modified: trunk/plearn/vmat/SplitWiseValidationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SplitWiseValidationVMatrix.cc	2007-04-09 22:04:33 UTC (rev 6865)
+++ trunk/plearn/vmat/SplitWiseValidationVMatrix.cc	2007-04-09 22:07:53 UTC (rev 6866)
@@ -53,7 +53,7 @@
     );
 
 SplitWiseValidationVMatrix::SplitWiseValidationVMatrix()
-    : validation_error_column(-1)
+    : validation_error_column(-1), add_column_average(false)
 {}
 
 void SplitWiseValidationVMatrix::getNewRow(int i, const Vec& v) const
@@ -74,6 +74,9 @@
                   "Number of splits to consider, starting from the first one.\n"
                   "If < 0, then all splits are considered (based on the number\n"
                   "of splits of the first expdir).");
+    declareOption(ol, "add_column_average", &SplitWiseValidationVMatrix::add_column_average,
+                  OptionBase::buildoption,
+                  "Indication that the average of the columns should be added.");
 
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
@@ -96,6 +99,11 @@
             data.resize(nsplits,stats_i.width());
             data.fill(REAL_MAX);
         }
+        if(stats_i.length() <nsplits)
+        {
+            PLWARNING("In SplitWiseValidationVMatrix::build_(): split_stats_ppaths[%d]=%s does not have enough splits, it will be ignored", i, split_stats_ppaths[i].c_str());
+            continue;
+        }
         for(int j=0; j<nsplits; j++)
         {
             if(data(j,validation_error_column) > stats_i(j,validation_error_column))
@@ -105,6 +113,18 @@
     
     length_ = data.length();
     width_ = data.width();
+    if(add_column_average)
+    {
+        data.resize(data.length()+1, data.width());
+        data(data.length()-1).fill(0);
+        length_++;
+        for(int i=0; i<data.width(); i++)
+        {
+            for(int j=0; j<data.length()-1; j++)
+                data(data.length()-1,i) += data(j,i);
+            data(data.length()-1,i) = data(data.length()-1,i)/(data.length()-1);
+        }
+    }
 }
 
 // ### Nothing to add here, simply calls build_

Modified: trunk/plearn/vmat/SplitWiseValidationVMatrix.h
===================================================================
--- trunk/plearn/vmat/SplitWiseValidationVMatrix.h	2007-04-09 22:04:33 UTC (rev 6865)
+++ trunk/plearn/vmat/SplitWiseValidationVMatrix.h	2007-04-09 22:07:53 UTC (rev 6866)
@@ -66,6 +66,9 @@
     //! Number of splits to consider, starting from the first one
     int nsplits;
 
+    //! Indication that the average of the columns should be added
+    bool add_column_average;
+
 public:
     //#####  Public Member Functions  #########################################
 



From larocheh at mail.berlios.de  Tue Apr 10 00:42:53 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Tue, 10 Apr 2007 00:42:53 +0200
Subject: [Plearn-commits] r6867 - trunk/plearn_learners_experimental
Message-ID: <200704092242.l39MgrGx021618@sheep.berlios.de>

Author: larocheh
Date: 2007-04-10 00:42:52 +0200 (Tue, 10 Apr 2007)
New Revision: 6867

Modified:
   trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc
Log:
Still working on this...


Modified: trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc
===================================================================
--- trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc	2007-04-09 22:07:53 UTC (rev 6866)
+++ trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc	2007-04-09 22:42:52 UTC (rev 6867)
@@ -115,7 +115,9 @@
                   "Model type. Choose between:\n"
                   " - \"discriminative\"             (multiclass classifier)\n"
                   " - \"discriminative_1_vs_all\"    (1 vs all multitask classier)\n"
-                  " - \"generative\"                 (gaussian classifier)\n");
+                  " - \"generative\"                 (gaussian input)\n"
+                  " - \"generative_0-1\"             ([0,1] input)\n"
+        );
     declareOption(ol, "penalty_type", &LinearInductiveTransferClassifier::penalty_type,
                   OptionBase::buildoption,
                   "Penalty to use on the weights (for weight and bias decay).\n"
@@ -206,14 +208,22 @@
             weights =vconcat(-product(exp(s),square(weights)) & weights); // Making sure that the scaling factor is going to be positive
             output = affine_transform(input, weights);
         }
-        else
+        else if(model_type == "generative_0-1")
         {
+            PLERROR("Not implemented yet");
+            //weights = vconcat(columnSum(log(A/(exp(A)-1))) & weights);
+            //output = affine_transform(input, weights);
+        }
+        else if(model_type == "generative")
+        {
             weights =vconcat(-columnSum(square(weights)/transpose(duplicateRow(s,noutputs))) & 2*weights/transpose(duplicateRow(s,noutputs)));
             if(targetsize() == 1)
                 output = affine_transform(input, weights);
             else
                 output = exp(affine_transform(input, weights) - duplicateRow(dot(transpose(input)/s,input),noutputs))+REAL_EPSILON;
         }
+        else
+            PLERROR("In LinearInductiveTransferClassifier::build_(): model_type %s is not valid", model_type.c_str());
 
         Var sup_output;
         Var new_output;
@@ -287,7 +297,7 @@
         }
 
         // Build costs
-        if(model_type == "discriminative" || model_type == "discriminative_1_vs_all")
+        if(model_type == "discriminative" || model_type == "discriminative_1_vs_all" || model_type == "generative_0-1")
         {
             if(model_type == "discriminative")
             {
@@ -314,7 +324,7 @@
                 else
                 {
                     costs[0] = stable_cross_entropy(sup_output, sup_target, true);
-                    costs[1] = transpose(lift_output(sigmoid(sup_output), sup_target));
+                    costs[1] = transpose(lift_output(sigmoid(sup_output)+0.001, sup_target));
                 }
                 if(targetsize() == 1)
                 {
@@ -331,6 +341,33 @@
                         new_costs[i] = costs[i];
                 }
             }
+            if(model_type == "generative_0-1")
+            {
+                costs.resize(2);
+                new_costs.resize(2);
+                if(targetsize() == 1)
+                {
+                    costs[0] = sup_output;
+                    costs[1] = classification_loss(sigmoid(sup_output), sup_target);
+                }
+                else
+                {
+                    PLERROR("In LinearInductiveTransferClassifier::build_(): can't use generative_0-1 model with targetsize() != 1");
+                    costs[0] = sup_output;
+                    costs[1] = transpose(lift_output(sigmoid(exp(sup_output)+REAL_EPSILON), sup_target));
+                }
+                if(targetsize() == 1)
+                {
+                    new_costs[0] = new_output;
+                    new_costs[1] = classification_loss(new_output, new_target);
+                }
+                else
+                {
+                    new_costs.resize(costs.length());
+                    for(int i=0; i<new_costs.length(); i++)
+                        new_costs[i] = costs[i];
+                }
+            }
         }
         else if(model_type == "generative")
         {
@@ -437,6 +474,8 @@
     deepCopyField(f, copies);
     deepCopyField(test_costf, copies);
     deepCopyField(output_and_target_to_cost, copies);
+    deepCopyField(sup_test_costf, copies);
+    deepCopyField(sup_output_and_target_to_cost, copies);
 
     varDeepCopyField(A, copies);
     varDeepCopyField(s, copies);
@@ -490,7 +529,7 @@
     if(f.isNull()) // Net has not been properly built yet (because build was called before the learner had a proper training set)
         build();
     
-    if(model_type == "discriminative" || model_type == "discriminative_1_vs_all")
+    if(model_type == "discriminative" || model_type == "discriminative_1_vs_all" || model_type == "generative_0-1")
     {
         // number of samples seen by optimizer before each optimizer update
         int nsamples = batch_size>0 ? batch_size : l;
@@ -501,7 +540,7 @@
             optimizer->setToOptimize(params, totalcost);  
             optimizer->build();
         }
-        else PLERROR("DeepFeatureExtractor::train can't train without setting an optimizer first!");
+        else PLERROR("LinearInductiveTransferClassifier::train can't train without setting an optimizer first!");
 
         // number of optimizer stages corresponding to one learner stage (one epoch)
         int optstage_per_lstage = l/nsamples;
@@ -697,6 +736,10 @@
                 break;
         if(i>= targetv.length())
             PLERROR("In LinearInductiveTransferClassifier::computeCostsFromOutputs(): all targets are missing, can't compute cost");
+        //for(int j=i+1; j<targetv.length(); j++)
+        //    if(!is_missing(targetv[j]))
+        //        PLERROR("In LinearInductiveTransferClassifier::computeCostsFromOutputs(): there should be only one non-missing target");
+        //cout << "i=" << i << " ";
         if(model_type == "generative")
             costsv[costsv.length()-1] = costsv[i];
         else
@@ -709,9 +752,9 @@
 TVec<string> LinearInductiveTransferClassifier::getTestCostNames() const
 {
     TVec<string> costs_str;
-    if(model_type == "discriminative" || model_type == "discriminative_1_vs_all")
+    if(model_type == "discriminative" || model_type == "discriminative_1_vs_all" || model_type == "generative_0-1")
     {
-        if(model_type == "discriminative")
+        if(model_type == "discriminative" || model_type == "generative_0-1")
         {
             costs_str.resize(2);
             costs_str[0] = "NLL";



From yoshua at mail.berlios.de  Tue Apr 10 14:12:57 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Tue, 10 Apr 2007 14:12:57 +0200
Subject: [Plearn-commits] r6868 - trunk/scripts
Message-ID: <200704101212.l3ACCvtI001230@sheep.berlios.de>

Author: yoshua
Date: 2007-04-10 14:12:56 +0200 (Tue, 10 Apr 2007)
New Revision: 6868

Modified:
   trunk/scripts/collectres
Log:
Wrote but not tested the plot command.


Modified: trunk/scripts/collectres
===================================================================
--- trunk/scripts/collectres	2007-04-09 22:42:52 UTC (rev 6867)
+++ trunk/scripts/collectres	2007-04-10 12:12:56 UTC (rev 6868)
@@ -31,6 +31,8 @@
 #  This file is part of the PLearn library. For more information on the PLearn
 #  library, go to the PLearn Web site at www.plearn.org
 
+# see main below for help on using this script
+
 import sys,string
 from plearn.vmat.PMat import *
 from numarray import *
@@ -53,17 +55,22 @@
       res.append(a[mrow,int(loc_specs[i])])
       i+=1
   elif loc_mode=="cols":
-    minrow = int(loc_specs[1])
-    maxrow = int(loc_specs[2])
+    minrow = int(loc_specs[3])
+    maxrow = int(loc_specs[4])
     if maxrow<0:
       maxrow = a.shape[0]+maxrow
     maxrow += 1
-    i=3
-    res = zeros([maxrow-minrow,len(loc_specs[i:])])
+    i=5
+    b=[]
+    res = zeros([maxrow-minrow,0])
     while len(loc_specs[i:])>0:
-      res[:,i-3]=a[minrow:maxrow,int(loc_specs[i])]
-      i+=1
-    print res.shape
+      col = int(loc_specs[i])
+      if col<0:
+        b = array(range(minrow,maxrow))
+      else:
+        b=a[minrow:maxrow,col].copy()
+      res=concatenate([res,b.resize([maxrow-minrow,1])],1)
+      i+=2 # skip the column name
   else:
     raise ValueError("Invalid <location-spec> mode, expected 'pos', 'mincol', or 'col', got "+loc_mode)    
   return res
@@ -85,8 +92,42 @@
     return -1
   else:
     return 1
-  
-def outputres(f,mode,results):
+
+def write_array(f,a):
+  rows=a.shape[0]
+  cols=a.shape[1]
+  for r in range(0,rows):
+    for c in range(0,cols):
+      f.write(str(a[r,c])+" ")
+    f.write("\n")
+
+def sublist(liste,indices):
+  subliste=[]
+  l=len(liste)
+  for index in indices:
+    if index<l:
+      subliste.append(liste[index])
+    
+def distinct_experiment_names(filenames):
+  splitted_filenames = map(lambda fname: string.split(fname,"-"), filenames)
+  distinct_parts = []
+  not_finished = True
+  lengths = map(len,splitted_filenames)
+  len0 = lengths[0]
+  maxlen = max(lengths)
+  minlen = min(lengths)
+  for i in range(0,minlen):
+    all_equal=True
+    for splitted_filename in splitted_filenames[1:]:
+      if splitted_filename[i]!=splitted_filenames[0][i]:
+        all_equal=False
+        break
+    if not all_equal:
+      distinct_parts.append(i)
+  distinct_parts.append(range(minlen,maxlen))
+  return map(lambda sname: string.join(sublist(sname,distinct_parts),"-"),splitted_filenames)
+
+def outputres(f,mode,speclist,results):
   if mode=="min":
     minval = 1e36
     minfile = ""
@@ -108,11 +149,34 @@
         f.write(str(v)+" ")
       f.write(res[1]+"\n")
   elif mode=="plot":
+    # "plot cols <xlabel> <ylabel> <minrow> <maxrow> <x> <col1> <name1> <col2> <name2> ..." : make a smartplot file with <col1> as y-columns (with legend label <namei>), <x> as x-column (-1 means use the row indices). Axes are labeled <xlabel> and <ylabel>. Use rows from <minrow> to <maxrow> (-1 means last) row inclusively.'
+    #
+    # build the gnuplot command in the smartplot file header
+    filenames = map(lambda res: res[1], results) # collect the filenames from the results list
+    distinct_expnames = distinct_experiment_names(filenames) # keep only the option=value parts that are distinct among experiments
+    f.write('# to set y on log-scale insert this at the beginning below: set logscale y;\n')
+    # speclist has the elements: cols <xlabel> <ylabel> <minrow> <maxrow> <x> <col1> <name1> <col2> <name2> ...
+    f.write('#: set xlabel "'+speclist[1]+'"; set ylabel "'+speclist[2]+'"; plot ')
+    ncoln=len(speclist)-6 
+    ncol=int(ncol/2)
+    if ncol*2!=ncoln:
+      ValueError("Invalid plot <spec>: there should be one <name> per <col> to plot")
+    ncol+=1 # add 1 for <x>, now this is the number of columns per experiment
+    j = 0
+    for expname in distinct_expnames:
+      for i in range(0,ncol-1):
+        name = speclist[6+i*2+1]
+        f.write('@ using '+str(1+j*ncol)+':'+str(1+j*ncol+i)+' with lines t "'+name+'~'+expname'"')
+        if (i+2)<len(speclist):
+          f.write(",")
+    f.write("\n")
+    # concatenate vertically all the results from each experiment into one big array
     arrays = []
     for res in results:
       arrays.append(res[0])
     a = concatenate(arrays,1)
-    f.write(array_str(a))
+    # write the array to file, without any []
+    write_array(f,a)
   else:
     raise ValueError("Invalid <spec> mode, expected 'min', 'sort', or 'plot', got "+mode)
 
@@ -125,11 +189,10 @@
     print "The <spec> can be the following:"
     print '  "min <location-spec>" : identify the mininum of <location-spec> over the <file*.pmat>' 
     print '  "sort <location-spec>" : make a sorted table of all the values at each <location-spec> over the <file*.pmat>' 
-    print '  "plot <location-spec> " : make a smarplot file for plotting of all the values at <location-spec> over the <file*.pmat>' 
+    print '  "plot cols <xlabel> <ylabel> <minrow> <maxrow> <x> <col1> <name1> <col2> <name2> ..." : make a smartplot file with <col1> as y-columns (with legend label <namei>), <x> as x-column (-1 means use the row indices). Axes are labeled <xlabel> and <ylabel>. Use rows from <minrow> to <maxrow> (-1 means last) row inclusively.'
     print "where <location-spec> can be the following:"
     print '  "pos <row> <col0> [<col1> <col2>...]": keep only the value at positions (<row> <col0>) (<row> <col1>) ... in each pmat, and the first value may be used to minimize over files in the min command'
     print '  "mincol <mcol> [<col1> <col2>...] ": keep only the minimum value in <mcol> of each pmat, keeping track of the <coli> values at the selected row'
-    print '  "cols <minrow> <maxrow> <column-nb> <column-nb>...": this is used with the plot command, to specify a column to plot, optionally from <minrow> to <maxrow> (-1 means mat.length-1) row inclusively'
     sys.exit(1)
   output = args[1]
   filenames = args[3:]
@@ -140,5 +203,5 @@
   for file in filenames:
     f.write(file+" ")
   f.write("\n")
-  outputres(f,mode,getres(speclist[1:],filenames))
+  outputres(f,mode,speclist[1:],getres(speclist[1:],filenames))
   f.flush()



From tihocan at mail.berlios.de  Tue Apr 10 19:09:47 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 10 Apr 2007 19:09:47 +0200
Subject: [Plearn-commits] r6869 - trunk/plearn/math
Message-ID: <200704101709.l3AH9lGU012464@sheep.berlios.de>

Author: tihocan
Date: 2007-04-10 19:09:46 +0200 (Tue, 10 Apr 2007)
New Revision: 6869

Modified:
   trunk/plearn/math/StatsCollector.cc
   trunk/plearn/math/StatsCollector.h
Log:
Factorized code to avoid unnecessary code duplication

Modified: trunk/plearn/math/StatsCollector.cc
===================================================================
--- trunk/plearn/math/StatsCollector.cc	2007-04-10 12:12:56 UTC (rev 6868)
+++ trunk/plearn/math/StatsCollector.cc	2007-04-10 17:09:46 UTC (rev 6869)
@@ -1079,19 +1079,8 @@
 ///////////////
 real StatsCollector::nips_lift() const
 {
-    if (more_than_maxnvalues)
-        PLWARNING("In StatsCollector::nips_lift - You need to increase "
-                  "'maxnvalues' (or set it to -1) to get an accurate "
-                  "statistic");
-    if (!sorted)
-        sort_values_by_magnitude();
-    real n_total = real(sorted_values.length());
-    real pos_fraction = int(round(PLearn::sum(sorted_values.column(1)))) / n_total;
-    int n_pos_in_k_minus_1 = -1;
-    real result = 0;
-    for (int k = 0; k < sorted_values.length(); k++)
-        result += lift(k + 1, n_pos_in_k_minus_1, n_pos_in_k_minus_1, pos_fraction);
-    result /= n_total;
+    real pos_fraction;
+    real result = - mean_lift(&pos_fraction);
     real max_performance = 0.5 * (1 / pos_fraction - 1) * (pos_fraction + 1) + 1;
     result = (max_performance - result) / max_performance;
     return result;
@@ -1100,20 +1089,22 @@
 ///////////////
 // mean_lift //
 ///////////////
-real StatsCollector::mean_lift() const
+real StatsCollector::mean_lift(real* pos_fraction) const
 {
     if (more_than_maxnvalues)
-        PLWARNING("In StatsCollector::nips_lift - You need to increase "
+        PLWARNING("In StatsCollector::mean_lift - You need to increase "
                   "'maxnvalues' (or set it to -1) to get an accurate "
                   "statistic");
     if (!sorted)
         sort_values_by_magnitude();
     real n_total = real(sorted_values.length());
-    real pos_fraction = int(round(PLearn::sum(sorted_values.column(1)))) / n_total;
+    real pos_f = int(round(PLearn::sum(sorted_values.column(1)))) / n_total;
+    if (pos_fraction)
+        *pos_fraction = pos_f;
     int n_pos_in_k_minus_1 = -1;
     real result = 0;
     for (int k = 0; k < sorted_values.length(); k++)
-        result += lift(k + 1, n_pos_in_k_minus_1, n_pos_in_k_minus_1, pos_fraction);
+        result += lift(k + 1, n_pos_in_k_minus_1, n_pos_in_k_minus_1, pos_f);
     result /= n_total;
     return -result;
 }

Modified: trunk/plearn/math/StatsCollector.h
===================================================================
--- trunk/plearn/math/StatsCollector.h	2007-04-10 12:12:56 UTC (rev 6868)
+++ trunk/plearn/math/StatsCollector.h	2007-04-10 17:09:46 UTC (rev 6869)
@@ -263,30 +263,17 @@
     //! must be the average fraction of positive examples in the dataset (= n+ / n).
     real lift(int k, int& n_pos_in_k, int n_pos_in_k_minus_1 = -1, real pos_fraction = -1) const;
     real nips_lift() const;   //!< NIPS_LIFT statistic (see help).
-    real mean_lift() const;   //!< MEAN_LIFT statistic (see help).
+    //! MEAN_LIFT statistic (see help).
+    //! If provided, 'pos_fraction' is filled with the fraction of positive
+    //! examples seen in the updates so far.
+    real mean_lift(real* pos_fraction = NULL) const;
     real prbp() const;        //!< PRBP statistic (see help).
     //! discrete distribution mode
     real dmode() const;
     Vec dmodes() const;
 
-    //! currently understood statnames are :
-    //!   - E (mean)
-    //!   - V (variance)
-    //!   - STDDEV, STDERROR
-    //!   - MIN, MAX
-    //!   - SUM, SUMSQ
-    //!   - FIRST, LAST
-    //!   - N, NMISSING, NNONMISING
-    //!   - SHARPERATIO
-    //!   - EoverSKEW
-    //!   - EoverKURT
-    //!   - ZSTAT, PZ
-    //!   - PSEUDOQ(q)    (q is a fraction between 0 and 1)
-    //!   - IQR           (inter-quartile range)
-    //!   - PRR           (pseudo robust range)
-    //!   - SKEW          (skewness == E(X-mu)^3 / sigma^3)
-    //!   - KURT          (kurtosis == E(X-mu)^4 / sigma^4 - 3)
-    //!   - DMODE         (discrete distribution mode)
+    //! Compute a given statistic.
+    //! Currently understood statnames are listed in the class help.
     real getStat(const string& statname) const;
 
     //! simply calls inherited::build() then build_()



From yoshua at mail.berlios.de  Wed Apr 11 12:44:08 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Wed, 11 Apr 2007 12:44:08 +0200
Subject: [Plearn-commits] r6870 - trunk/scripts
Message-ID: <200704111044.l3BAi8BV026585@sheep.berlios.de>

Author: yoshua
Date: 2007-04-11 12:44:07 +0200 (Wed, 11 Apr 2007)
New Revision: 6870

Modified:
   trunk/scripts/collectres
Log:


Modified: trunk/scripts/collectres
===================================================================
--- trunk/scripts/collectres	2007-04-10 17:09:46 UTC (rev 6869)
+++ trunk/scripts/collectres	2007-04-11 10:44:07 UTC (rev 6870)
@@ -70,7 +70,10 @@
       else:
         b=a[minrow:maxrow,col].copy()
       res=concatenate([res,b.resize([maxrow-minrow,1])],1)
-      i+=2 # skip the column name
+      if i==5:
+        i=6
+      else:
+        i+=2 # skip the column name
   else:
     raise ValueError("Invalid <location-spec> mode, expected 'pos', 'mincol', or 'col', got "+loc_mode)    
   return res
@@ -107,25 +110,24 @@
   for index in indices:
     if index<l:
       subliste.append(liste[index])
+  # print "sublist(",liste,",",indices,")=",subliste
+  return subliste
     
 def distinct_experiment_names(filenames):
-  splitted_filenames = map(lambda fname: string.split(fname,"-"), filenames)
-  distinct_parts = []
-  not_finished = True
-  lengths = map(len,splitted_filenames)
-  len0 = lengths[0]
-  maxlen = max(lengths)
-  minlen = min(lengths)
-  for i in range(0,minlen):
-    all_equal=True
-    for splitted_filename in splitted_filenames[1:]:
-      if splitted_filename[i]!=splitted_filenames[0][i]:
-        all_equal=False
-        break
-    if not all_equal:
-      distinct_parts.append(i)
-  distinct_parts.append(range(minlen,maxlen))
-  return map(lambda sname: string.join(sublist(sname,distinct_parts),"-"),splitted_filenames)
+  splitted_filenames = map(lambda fname: string.split(fname.replace("/","-"),"-"), filenames)
+  distinct_names = []
+  for splitted_filename in splitted_filenames:
+    different_parts = []
+    for part in splitted_filename:
+      everywhere=True
+      for other_splitted_filename in splitted_filenames:
+        if not part in other_splitted_filename:
+          everywhere=False
+          break
+      if not everywhere:
+        different_parts.append(part)
+    distinct_names.append(string.join(different_parts,"-"))
+  return distinct_names
 
 def outputres(f,mode,speclist,results):
   if mode=="min":
@@ -158,7 +160,7 @@
     # speclist has the elements: cols <xlabel> <ylabel> <minrow> <maxrow> <x> <col1> <name1> <col2> <name2> ...
     f.write('#: set xlabel "'+speclist[1]+'"; set ylabel "'+speclist[2]+'"; plot ')
     ncoln=len(speclist)-6 
-    ncol=int(ncol/2)
+    ncol=int(ncoln/2)
     if ncol*2!=ncoln:
       ValueError("Invalid plot <spec>: there should be one <name> per <col> to plot")
     ncol+=1 # add 1 for <x>, now this is the number of columns per experiment
@@ -166,9 +168,10 @@
     for expname in distinct_expnames:
       for i in range(0,ncol-1):
         name = speclist[6+i*2+1]
-        f.write('@ using '+str(1+j*ncol)+':'+str(1+j*ncol+i)+' with lines t "'+name+'~'+expname'"')
-        if (i+2)<len(speclist):
+        f.write('@ using '+str(1+j*ncol)+':'+str(2+j*ncol+i)+' with lines t "'+name+'~'+expname+'"')
+        if (i+2)<ncol:
           f.write(",")
+      j=j+1
     f.write("\n")
     # concatenate vertically all the results from each experiment into one big array
     arrays = []



From yoshua at mail.berlios.de  Wed Apr 11 13:03:18 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Wed, 11 Apr 2007 13:03:18 +0200
Subject: [Plearn-commits] r6871 - trunk/scripts
Message-ID: <200704111103.l3BB3IXJ015019@sheep.berlios.de>

Author: yoshua
Date: 2007-04-11 13:03:09 +0200 (Wed, 11 Apr 2007)
New Revision: 6871

Modified:
   trunk/scripts/collectres
Log:
Improved the Usage printout


Modified: trunk/scripts/collectres
===================================================================
--- trunk/scripts/collectres	2007-04-11 10:44:07 UTC (rev 6870)
+++ trunk/scripts/collectres	2007-04-11 11:03:09 UTC (rev 6871)
@@ -104,15 +104,6 @@
       f.write(str(a[r,c])+" ")
     f.write("\n")
 
-def sublist(liste,indices):
-  subliste=[]
-  l=len(liste)
-  for index in indices:
-    if index<l:
-      subliste.append(liste[index])
-  # print "sublist(",liste,",",indices,")=",subliste
-  return subliste
-    
 def distinct_experiment_names(filenames):
   splitted_filenames = map(lambda fname: string.split(fname.replace("/","-"),"-"), filenames)
   distinct_names = []
@@ -187,15 +178,29 @@
 if __name__=='__main__':
   args = sys.argv[:]
   if len(args)==1:
-    print "Usage: collectres <output> <spec> <file1.pmat> <file2.pmat> ..."
+    print "Usage: collectres <outputfile> <spec> <file1.pmat> <file2.pmat> ..."
     print 
     print "The <spec> can be the following:"
-    print '  "min <location-spec>" : identify the mininum of <location-spec> over the <file*.pmat>' 
-    print '  "sort <location-spec>" : make a sorted table of all the values at each <location-spec> over the <file*.pmat>' 
-    print '  "plot cols <xlabel> <ylabel> <minrow> <maxrow> <x> <col1> <name1> <col2> <name2> ..." : make a smartplot file with <col1> as y-columns (with legend label <namei>), <x> as x-column (-1 means use the row indices). Axes are labeled <xlabel> and <ylabel>. Use rows from <minrow> to <maxrow> (-1 means last) row inclusively.'
-    print "where <location-spec> can be the following:"
-    print '  "pos <row> <col0> [<col1> <col2>...]": keep only the value at positions (<row> <col0>) (<row> <col1>) ... in each pmat, and the first value may be used to minimize over files in the min command'
-    print '  "mincol <mcol> [<col1> <col2>...] ": keep only the minimum value in <mcol> of each pmat, keeping track of the <coli> values at the selected row'
+    print '  "min <location-spec>" : identify the mininum of <location-spec> over the <file*.pmat>'
+    print '                          prints the values of the selected <location-spec> and the name'
+    print '                          of the minimizing <file.pmat> in the <outputfile>.'
+    print '  "sort <location-spec>" : make a sorted table of all the values at each <location-spec>'
+    print '                           over the <file*.pmat>. The output format is similar to "min" but'
+    print '                           it outputs the selected values for all the pmat files, sorted'
+    print '                           by the first value in the <location-spec>.'
+    print '  "plot cols <xlabel> <ylabel> <minrow> <maxrow> <x> <col1> <name1> <col2> <name2> ..." :'
+    print '                          make a smartplot file (see PLearn/scripts/smartplot) with column with'
+    print '                          indices <col1> in <fileX.pmat> as y-columns (with legend label <namei>),'
+    print '                          column with index <x> as x-column (or -1 for the row indices).'
+    print '                          Axes are labeled <xlabel> and <ylabel>. Use rows from <minrow> to <maxrow>'
+    print '                          row inclusively (<maxrow> = -1 means last row).'
+    print 
+    print "In the above, <location-spec> can be the following:"
+    print '  "pos <row> <col0> [<col1> <col2>...]": keep only the values at positions'
+    print '                          (<row> <col0>) (<row> <col1>) ... in each pmat file, and the first value'
+    print '                          may be used to minimize over files in the min command.'
+    print '  "mincol <mcol> [<col1> <col2>...] ": keep only the minimum value in colun <mcol> of each pmat,'
+    print '                          keeping track at the same time of the <coli> values at the min-selected row.'
     sys.exit(1)
   output = args[1]
   filenames = args[3:]



From yoshua at mail.berlios.de  Wed Apr 11 13:13:58 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Wed, 11 Apr 2007 13:13:58 +0200
Subject: [Plearn-commits] r6872 - trunk/scripts
Message-ID: <200704111113.l3BBDwcw025329@sheep.berlios.de>

Author: yoshua
Date: 2007-04-11 13:13:57 +0200 (Wed, 11 Apr 2007)
New Revision: 6872

Modified:
   trunk/scripts/collectres
Log:
Added an example of use in the comments of the source header.



Modified: trunk/scripts/collectres
===================================================================
--- trunk/scripts/collectres	2007-04-11 11:03:09 UTC (rev 6871)
+++ trunk/scripts/collectres	2007-04-11 11:13:57 UTC (rev 6872)
@@ -32,6 +32,25 @@
 #  library, go to the PLearn Web site at www.plearn.org
 
 # see main below for help on using this script
+#
+# Here is an example of use:
+#
+# First I found what were the best results for two competing algorithms
+# using a collectres sort command:
+#
+#   collectres letter-sgrad-best-valid-NLL "sort mincol 6 5 4 3" exp/letter-sgrad-*/Split0/LearnerExpdir/Strat0results.pmat
+#
+#   collectres letter-NatGradBlock-best-valid-NLL "sort mincol 6 5 4 3" exp/letter-NatGradBlock-*/Split0/LearnerExpdir/Strat0results.pmat
+#
+# Then I copy-pasted the names of the best experiment directories into a collectres plot command
+# to display their error curves:
+#
+#   collectres letter-sgrad-vs-BlockNatGrad-best-valid-NLL-plot "plot cols cpu-sec NLL 0 99 8 6 valid-NLL" exp/letter-sgrad-nout=26-nh1=400-nh2=400-nh3=0-slr=0.01-dc=0-n_train_it=1600001-epoch=16000-seed=2-minibatch=1-20070405:002553/Split0/LearnerExpdir/Strat0results.pmat exp/letter-natgradblockO-nout=26-nh1=400-nh2=400-nh3=0-slr=1e-5-dc=1e-6-n_train_it=1600001-epoch=16000-seed=1-minibatch=30-cov_minibatch=10-lambda=.03-k=3-gamma=.995-ren=1-ngv=0-20070401:220403/Split0/LearnerExpdir/Strat0results.pmat
+#
+# Finally I could display the curves (after possibly editing the gnuplot command in the output file) using
+#
+#   smartplot letter-sgrad-vs-BlockNatGrad-best-valid-NLL-plot.eps letter-sgrad-vs-BlockNatGrad-best-valid-NLL-plot
+#    
 
 import sys,string
 from plearn.vmat.PMat import *



From yoshua at mail.berlios.de  Wed Apr 11 13:18:39 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Wed, 11 Apr 2007 13:18:39 +0200
Subject: [Plearn-commits] r6873 - trunk/scripts
Message-ID: <200704111118.l3BBIdBS026327@sheep.berlios.de>

Author: yoshua
Date: 2007-04-11 13:18:38 +0200 (Wed, 11 Apr 2007)
New Revision: 6873

Modified:
   trunk/scripts/collectres
Log:
Clarified usage.


Modified: trunk/scripts/collectres
===================================================================
--- trunk/scripts/collectres	2007-04-11 11:13:57 UTC (rev 6872)
+++ trunk/scripts/collectres	2007-04-11 11:18:38 UTC (rev 6873)
@@ -199,7 +199,7 @@
   if len(args)==1:
     print "Usage: collectres <outputfile> <spec> <file1.pmat> <file2.pmat> ..."
     print 
-    print "The <spec> can be the following:"
+    print "The <spec> can be the following (note how the <spec> has to be surrounded by quotes):"
     print '  "min <location-spec>" : identify the mininum of <location-spec> over the <file*.pmat>'
     print '                          prints the values of the selected <location-spec> and the name'
     print '                          of the minimizing <file.pmat> in the <outputfile>.'
@@ -214,11 +214,11 @@
     print '                          Axes are labeled <xlabel> and <ylabel>. Use rows from <minrow> to <maxrow>'
     print '                          row inclusively (<maxrow> = -1 means last row).'
     print 
-    print "In the above, <location-spec> can be the following:"
-    print '  "pos <row> <col0> [<col1> <col2>...]": keep only the values at positions'
+    print "In the above, <location-spec> can be the following (but do not surround <location-spec> by quotes):"
+    print '  pos <row> <col0> [<col1> <col2>...] : keep only the values at positions'
     print '                          (<row> <col0>) (<row> <col1>) ... in each pmat file, and the first value'
     print '                          may be used to minimize over files in the min command.'
-    print '  "mincol <mcol> [<col1> <col2>...] ": keep only the minimum value in colun <mcol> of each pmat,'
+    print '  mincol <mcol> [<col1> <col2>...] : keep only the minimum value in colun <mcol> of each pmat,'
     print '                          keeping track at the same time of the <coli> values at the min-selected row.'
     sys.exit(1)
   output = args[1]



From yoshua at mail.berlios.de  Wed Apr 11 13:34:01 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Wed, 11 Apr 2007 13:34:01 +0200
Subject: [Plearn-commits] r6874 - trunk/scripts
Message-ID: <200704111134.l3BBY1G7017093@sheep.berlios.de>

Author: yoshua
Date: 2007-04-11 13:34:01 +0200 (Wed, 11 Apr 2007)
New Revision: 6874

Modified:
   trunk/scripts/collectres
Log:
Corrected problem with comma in gnuplot command fo plot


Modified: trunk/scripts/collectres
===================================================================
--- trunk/scripts/collectres	2007-04-11 11:18:38 UTC (rev 6873)
+++ trunk/scripts/collectres	2007-04-11 11:34:01 UTC (rev 6874)
@@ -175,11 +175,12 @@
       ValueError("Invalid plot <spec>: there should be one <name> per <col> to plot")
     ncol+=1 # add 1 for <x>, now this is the number of columns per experiment
     j = 0
+    nexp = len(distinct_expnames)
     for expname in distinct_expnames:
       for i in range(0,ncol-1):
         name = speclist[6+i*2+1]
         f.write('@ using '+str(1+j*ncol)+':'+str(2+j*ncol+i)+' with lines t "'+name+'~'+expname+'"')
-        if (i+2)<ncol:
+        if j<nexp-1:
           f.write(",")
       j=j+1
     f.write("\n")



From chapados at mail.berlios.de  Wed Apr 11 14:13:07 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Wed, 11 Apr 2007 14:13:07 +0200
Subject: [Plearn-commits] r6875 - trunk/plearn/ker
Message-ID: <200704111213.l3BCD7Gr020456@sheep.berlios.de>

Author: chapados
Date: 2007-04-11 14:13:05 +0200 (Wed, 11 Apr 2007)
New Revision: 6875

Added:
   trunk/plearn/ker/KroneckerBaseKernel.cc
   trunk/plearn/ker/KroneckerBaseKernel.h
Modified:
   trunk/plearn/ker/ARDBaseKernel.cc
   trunk/plearn/ker/ARDBaseKernel.h
   trunk/plearn/ker/IIDNoiseKernel.cc
   trunk/plearn/ker/IIDNoiseKernel.h
   trunk/plearn/ker/RationalQuadraticARDKernel.cc
   trunk/plearn/ker/RationalQuadraticARDKernel.h
   trunk/plearn/ker/SquaredExponentialARDKernel.cc
   trunk/plearn/ker/SquaredExponentialARDKernel.h
   trunk/plearn/ker/SummationKernel.cc
   trunk/plearn/ker/SummationKernel.h
Log:
* Complete rewrite of kernels for gaussian processes

* SummationKernel is now very efficient

* RationalQuadratic and SquareExponential no longer add noise part: must use
  an explicit SummationKernel along with an IIDNoiseKernel term

* Kronecker deltas can be multiplied in front of any kernel term, allowing
  gating the presence of a term on (a product of) equality constraint(s)
  in the input variables

* Analytic derivatives implemented for SquaredExponentialARDKernel

* All code has been tested and works, but pytest test-cases remain to be added



Modified: trunk/plearn/ker/ARDBaseKernel.cc
===================================================================
--- trunk/plearn/ker/ARDBaseKernel.cc	2007-04-11 11:34:01 UTC (rev 6874)
+++ trunk/plearn/ker/ARDBaseKernel.cc	2007-04-11 12:13:05 UTC (rev 6875)
@@ -2,7 +2,7 @@
 
 // ARDBaseKernel.cc
 //
-// Copyright (C) 2006 Nicolas Chapados
+// Copyright (C) 2006-2007 Nicolas Chapados
 //
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:

Modified: trunk/plearn/ker/ARDBaseKernel.h
===================================================================
--- trunk/plearn/ker/ARDBaseKernel.h	2007-04-11 11:34:01 UTC (rev 6874)
+++ trunk/plearn/ker/ARDBaseKernel.h	2007-04-11 12:13:05 UTC (rev 6875)
@@ -2,7 +2,7 @@
 
 // ARDBaseKernel.h
 //
-// Copyright (C) 2006 Nicolas Chapados
+// Copyright (C) 2006-2007 Nicolas Chapados
 //
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
@@ -40,7 +40,7 @@
 #ifndef ARDBaseKernel_INC
 #define ARDBaseKernel_INC
 
-#include <plearn/ker/IIDNoiseKernel.h>
+#include <plearn/ker/KroneckerBaseKernel.h>
 
 namespace PLearn {
 
@@ -55,12 +55,15 @@
  *
  *  Note that to make its operations more robust when used with unconstrained
  *  optimization of hyperparameters, all hyperparameters of this kernel are
- *  specified in the inverse softplus domain.  See IIDNoiseKernel for more
- *  explanations.
+ *  specified in the inverse softplus domain, hence the 'isp' prefix.  This is
+ *  used in preference to the log-domain used by Rasmussen and Williams in
+ *  their implementation of gaussian processes, due to numerical stability.
+ *  (It may happen that the optimizer jumps 'too far' along one hyperparameter
+ *  and this causes the Gram matrix to become extremely ill-conditioned.)
  */
-class ARDBaseKernel : public IIDNoiseKernel
+class ARDBaseKernel : public KroneckerBaseKernel
 {
-    typedef IIDNoiseKernel inherited;
+    typedef KroneckerBaseKernel inherited;
 
 public:
     //#####  Public Build Options  ############################################

Modified: trunk/plearn/ker/IIDNoiseKernel.cc
===================================================================
--- trunk/plearn/ker/IIDNoiseKernel.cc	2007-04-11 11:34:01 UTC (rev 6874)
+++ trunk/plearn/ker/IIDNoiseKernel.cc	2007-04-11 12:13:05 UTC (rev 6875)
@@ -2,7 +2,7 @@
 
 // IIDNoiseKernel.cc
 //
-// Copyright (C) 2006 Nicolas Chapados
+// Copyright (C) 2006-2007 Nicolas Chapados
 //
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
@@ -48,36 +48,36 @@
     "Kernel representing independent and identically-distributed observation noise",
     "This Kernel is typically used as a base class for covariance functions used\n"
     "in gaussian processes (see GaussianProcessRegressor).  It represents simple\n"
-    "i.i.d. additive noise:\n"
+    "i.i.d. additive noise that applies to 'identical training cases' i and j:\n"
     "\n"
-    "  k(x,y) = delta_x,y * sn\n"
+    "  k(D_i,D_j) = delta_i,j * sn\n"
     "\n"
-    "where delta_x,y is the Kronecker delta function, and sn is\n"
-    "softplus(isp_noise_sigma), with softplus(x) = log(1+exp(x)).\n"
+    "where D_i and D_j are elements from the current data set (established by\n"
+    "the setDataForKernelMatrix function), delta_i,j is the Kronecker delta\n"
+    "function, and sn is softplus(isp_noise_sigma), with softplus(x) =\n"
+    "log(1+exp(x)).  Note that 'identity' is not equivalent to 'vector\n"
+    "equality': in particular, at test-time, this noise is NEVER added.\n"
+    "Currently, two vectors are considered identical if and only if they are the\n"
+    "SAME ROW of the current data set, and hence the noise term is added only at\n"
+    "TRAIN-TIME across the diagonal of the Gram matrix (when the\n"
+    "computeGramMatrix() function is called).  This is why at test-time, no such\n"
+    "noise term is added.  The idea (see the book \"Gaussian Processes for\n"
+    "Machine Learning\" by Rasmussen and Williams for details) is that\n"
+    "observation noise only applies when A SPECIFIC OBSERVATION is drawn from\n"
+    "the GP distribution: if we sample a new point at the same x, we will get a\n"
+    "different realization for the noise, and hence the correlation between the\n"
+    "two noise realizations is zero.  This class can only be sure that two\n"
+    "observations are \"identical\" when they are presented all at once through\n"
+    "the data matrix.\n"
     "\n"
-    "In addition to comparing the complete x and y vectors, this kernel allows\n"
-    "adding a Kronecker delta when there is a match in only ONE DIMENSION.  This\n"
-    "may be generalized in the future to allow match according to a subset of\n"
-    "the input variables (but is not currently done for performance reasons).\n"
-    "With these terms, the kernel function takes the form:\n"
-    "\n"
-    "  k(x,y) = delta_x,y * sn + \\sum_i delta_x[kr(i)],y[kr(i)] * ks[i]\n"
-    "\n"
-    "where kr(i) is the i-th element of 'kronecker_indexes' (representing an\n"
-    "index into the input vectors), and ks[i]=softplus(isp_kronecker_sigma[i]).\n"
-    "\n"
-    "Note that to make its operations more robust when used with unconstrained\n"
-    "optimization of hyperparameters, all hyperparameters of this kernel are\n"
-    "specified in the inverse softplus domain, hence the 'isp' prefix.  This is\n"
-    "used in preference to the log-domain used by Rasmussen and Williams in\n"
-    "their implementation of gaussian processes, due to numerical stability.\n"
-    "(It may happen that the optimizer jumps 'too far' along one hyperparameter\n"
-    "and this causes the Gram matrix to become extremely ill-conditioned.)\n"
+    "The Kronecker terms computed by the base class are ADDDED to the noise\n"
+    "computed by this kernel (at test-time also).\n"
     );
 
 
 IIDNoiseKernel::IIDNoiseKernel()
-    : m_isp_noise_sigma(0.0)
+    : m_isp_noise_sigma(-100.0), /* very close to zero... */
+      m_isp_kronecker_sigma(100.0)
 { }
 
 
@@ -88,19 +88,14 @@
     declareOption(
         ol, "isp_noise_sigma", &IIDNoiseKernel::m_isp_noise_sigma,
         OptionBase::buildoption,
-        "Inverse softplus of the global noise variance.  Default value=0.0");
+        "Inverse softplus of the global noise variance.  Default value=-100.0\n"
+        "(very close to zero after we take softplus).");
 
     declareOption(
-        ol, "kronecker_indexes", &IIDNoiseKernel::m_kronecker_indexes,
-        OptionBase::buildoption,
-        "Element index in the input vectors that should be subject to additional\n"
-        "Kronecker delta terms");
-
-    declareOption(
         ol, "isp_kronecker_sigma", &IIDNoiseKernel::m_isp_kronecker_sigma,
         OptionBase::buildoption,
-        "Inverse softplus of the noise variance terms for the Kronecker deltas\n"
-        "associated with kronecker_indexes");
+        "Inverse softplus of the noise variance term for the product of\n"
+        "Kronecker deltas associated with kronecker_indexes, if specified.");
     
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
@@ -120,32 +115,14 @@
 //#####  build_  ##############################################################
 
 void IIDNoiseKernel::build_()
-{
-    if (m_kronecker_indexes.size() != m_isp_kronecker_sigma.size())
-        PLERROR("IIDNoiseKernel::build_: size of 'kronecker_indexes' (%d) "
-                "does not match that of 'isp_kronecker_sigma' (%d)",
-                m_kronecker_indexes.size(), m_isp_kronecker_sigma.size());
-}
+{ }
 
 
 //#####  evaluate  ############################################################
 
 real IIDNoiseKernel::evaluate(const Vec& x1, const Vec& x2) const
 {
-    real value = 0.0;
-    // if (x1 == x2)
-    //     value += softplus(m_isp_noise_sigma);
-
-    const int n = m_kronecker_indexes.size();
-    if (n > 0) {
-        int*  cur_index = m_kronecker_indexes.data();
-        real* cur_sigma = m_isp_kronecker_sigma.data();
-
-        for (int i=0 ; i<n ; ++i, ++cur_index, ++cur_sigma)
-            if (fast_is_equal(x1[*cur_index], x2[*cur_index]))
-                value += softplus(*cur_sigma);
-    }
-    return value;
+    return softplus(m_isp_kronecker_sigma) * inherited::evaluate(x1,x2);
 }
 
 
@@ -162,55 +139,26 @@
                 "of size %d x %d (currently of size %d x %d)",
                 data.length(), data.length(), K.length(), K.width());
                 
-    PLASSERT( K.size() == 0 || m_data_cache.size() > 0 );  // Ensure data cached OK
-
-    // Precompute some terms
-    real noise_sigma  = softplus(m_isp_noise_sigma);
-    m_kronecker_sigma.resize(m_isp_kronecker_sigma.size());
-    for (int i=0, n=m_isp_kronecker_sigma.size() ; i<n ; ++i)
-        m_kronecker_sigma[i] = softplus(m_isp_kronecker_sigma[i]);
-
-    // Prepare kronecker iteration
-    int   kronecker_num     = m_kronecker_indexes.size();
-    int*  kronecker_indexes = ( kronecker_num > 0?
-                                m_kronecker_indexes.data() : 0 );
-    real* kronecker_sigma   = ( kronecker_num > 0?
-                                m_kronecker_sigma.data() : 0 );
-
-    // Compute Gram Matrix
-    int  l = data->length();
-    int  m = K.mod();
-    int  cache_mod = m_data_cache.mod();
-
-    real *data_start = &m_data_cache(0,0);
-    real Kij;
-    real *Ki, *Kji;
-    real *xi = data_start;
+    // Compute Kronecker gram matrix. Multiply by kronecker sigma if there were
+    // any Kronecker terms.
+    inherited::computeGramMatrix(K);
+    if (m_kronecker_indexes.size() > 0)
+        K *= softplus(m_isp_kronecker_sigma);
     
-    for (int i=0 ; i<l ; ++i, xi += cache_mod) {
-        Ki  = K[i];
-        Kji = &K[0][i];
-        real *xj = data_start;
+    // Add iid noise contribution
+    real noise_sigma = softplus(m_isp_noise_sigma);
+    int  l   = data->length();
+    int  m   = K.mod() + 1;               // Mind the +1 to go along diagonal
+    real *Ki = K[0];
+    
+    for (int i=0 ; i<l ; ++i, Ki += m) {
+        *Ki += noise_sigma;
+    }
 
-        for (int j=0; j<=i; ++j, Kji += m, xj += cache_mod) {
-            // Kernel evaluation per se
-            if (i == j)
-                Kij = noise_sigma;
-            else
-                Kij = 0.0;
-
-            // Kronecker terms
-            if (kronecker_num > 0) {
-                int*  cur_index = kronecker_indexes;
-                real* cur_sigma = kronecker_sigma;
-                
-                for (int k=0 ; k<kronecker_num ; ++k, ++cur_index, ++cur_sigma)
-                    if (fast_is_equal(xi[*cur_index], xj[*cur_index]))
-                        Kij += *cur_sigma;
-            }
-            
-            *Ki++ = Kij;
-        }
+    if (cache_gram_matrix) {
+        gram_matrix.resize(l,l);
+        gram_matrix << K;
+        gram_matrix_is_cached = true;
     }
 }
 
@@ -220,10 +168,6 @@
 void IIDNoiseKernel::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
-
-    deepCopyField(m_kronecker_indexes,   copies);
-    deepCopyField(m_isp_kronecker_sigma, copies);
-    deepCopyField(m_kronecker_sigma,     copies);
 }
 
 
@@ -233,17 +177,13 @@
                                                  real epsilon) const
 {
     static const string INS("isp_noise_sigma");
-    static const string IKS("isp_kronecker_sigma[");
+    static const string IKS("isp_kronecker_sigma");
 
     if (kernel_param == INS) {
         if (!data)
             PLERROR("Kernel::computeGramMatrixDerivative should be called only after "
                     "setDataForKernelMatrix");
 
-        // For efficiency reasons, we only accumulate a derivative on the
-        // diagonal of the kernel matrix, even if two training examples happen
-        // to be EXACTLY identical.  (May change in the future if this turns
-        // out to be a problem).
         int W = nExamples();
         KD.resize(W,W);
         KD.fill(0.0);
@@ -251,74 +191,38 @@
         for (int i=0 ; i<W ; ++i)
             KD(i,i) = deriv;
     }
-    else if (string_begins_with(kernel_param, IKS) &&
-             kernel_param[kernel_param.size()-1] == ']')
-    {
-        int arg = tolong(kernel_param.substr(
-                             IKS.size(), kernel_param.size() - IKS.size() - 1));
-        PLASSERT( arg < m_kronecker_indexes.size() );
-
-        computeGramMatrixDerivKronecker(KD, arg);
+    else if (kernel_param == IKS) {
+        computeGramMatrixDerivKronecker(KD);
     }
     else
         inherited::computeGramMatrixDerivative(KD, kernel_param, epsilon);
 }
 
 
-//#####  derivKronecker  ######################################################
-
-real IIDNoiseKernel::derivKronecker(int i, int j, int arg, real K) const
-{
-    int index  = m_kronecker_indexes[arg];
-    Vec& row_i = *dataRow(i);
-    Vec& row_j = *dataRow(j);
-    if (fast_is_equal(row_i[index], row_j[index]))
-        return sigmoid(m_isp_kronecker_sigma[arg]);
-    else
-        return 0.0;
-}
-
-
 //#####  computeGramMatrixDerivKronecker  #####################################
 
-void IIDNoiseKernel::computeGramMatrixDerivKronecker(Mat& KD, int arg) const
+void IIDNoiseKernel::computeGramMatrixDerivKronecker(Mat& KD) const
 {
-    // Precompute some terms
-    real kronecker_sigma_arg = sigmoid(m_isp_kronecker_sigma[arg]);
-    int index = m_kronecker_indexes[arg];
-    
-    // Compute Gram Matrix derivative w.r.t. isp_kronecker_sigma[arg]
-    int  l = data->length();
+    // From the cached version of the Gram matrix, this function is easily
+    // implemented: we first copy the Gram to the KD matrix, subtract the IID
+    // noise contribution from the main diagonal, and multiply the remaining
+    // matrix (made up of 0/1 elements) by the derivative of the kronecker
+    // sigma hyperparameter.
 
-    // Variables that walk over the data matrix
-    int  cache_mod = m_data_cache.mod();
-    real *data_start = &m_data_cache(0,0);
-    real *xi = data_start+index;             // Iterator on data rows
-
-    // Variables that walk over the kernel derivative matrix (KD)
+    int l = data->length();
     KD.resize(l,l);
-    real* KDi = KD.data();                   // Start of row i
-    real* KDij;                              // Current element on row i
-    int   KD_mod = KD.mod();
+    PLASSERT_MSG(gram_matrix.width() == l && gram_matrix.length() == l,
+                 "To compute the derivative with respect to 'isp_kronecker_sigma',\n"
+                 "the Gram matrix must be precomputed and cached in IIDNoiseKernel.");
+    
+    KD << gram_matrix;
+    real noise_sigma = softplus(m_isp_noise_sigma);
+    for (int i=0 ; i<l ; ++i)
+        KD(i,i) -= noise_sigma;
 
-    // Iterate on rows of derivative matrix
-    for (int i=0 ; i<l ; ++i, xi += cache_mod, KDi += KD_mod)
-    {
-        KDij = KDi;
-        real xi_cur = *xi;
-        real *xj  = data_start+index;        // Inner iterator on data rows
-
-        // Iterate on columns of derivative matrix
-        for (int j=0 ; j <= i ; ++j, xj += cache_mod)
-        {
-            // Set into derivative matrix
-            *KDij++ = fast_is_equal(xi_cur, *xj)? kronecker_sigma_arg : 0.0;
-        }
-    }
+    KD *= sigmoid(m_isp_kronecker_sigma) / softplus(m_isp_kronecker_sigma);
 }
 
-
-
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/ker/IIDNoiseKernel.h
===================================================================
--- trunk/plearn/ker/IIDNoiseKernel.h	2007-04-11 11:34:01 UTC (rev 6874)
+++ trunk/plearn/ker/IIDNoiseKernel.h	2007-04-11 12:13:05 UTC (rev 6875)
@@ -2,7 +2,7 @@
 
 // IIDNoiseKernel.h
 //
-// Copyright (C) 2006 Nicolas Chapados
+// Copyright (C) 2006-2007 Nicolas Chapados
 //
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
@@ -40,7 +40,7 @@
 #ifndef IIDNoiseKernel_INC
 #define IIDNoiseKernel_INC
 
-#include <plearn/ker/MemoryCachedKernel.h>
+#include <plearn/ker/KroneckerBaseKernel.h>
 
 namespace PLearn {
 
@@ -49,50 +49,46 @@
  *
  *  This Kernel is typically used as a base class for covariance functions used
  *  in gaussian processes (see GaussianProcessRegressor).  It represents simple
- *  i.i.d. additive noise:
+ *  i.i.d. additive noise that applies to 'identical training cases' i and j:
  *
- *    k(x,y) = delta_x,y * sn
+ *    k(D_i,D_j) = delta_i,j * sn
  *
- *  where delta_x,y is the Kronecker delta function, and sn is
- *  softplus(isp_noise_sigma), with softplus(x) = log(1+exp(x)).
+ *  where D_i and D_j are elements from the current data set (established by
+ *  the setDataForKernelMatrix function), delta_i,j is the Kronecker delta
+ *  function, and sn is softplus(isp_noise_sigma), with softplus(x) =
+ *  log(1+exp(x)).  Note that 'identity' is not equivalent to 'vector
+ *  equality': in particular, at test-time, this noise is NEVER added.
+ *  Currently, two vectors are considered identical if and only if they are the
+ *  SAME ROW of the current data set, and hence the noise term is added only at
+ *  TRAIN-TIME across the diagonal of the Gram matrix (when the
+ *  computeGramMatrix() function is called).  This is why at test-time, no such
+ *  noise term is added.  The idea (see the book "Gaussian Processes for
+ *  Machine Learning" by Rasmussen and Williams for details) is that
+ *  observation noise only applies when A SPECIFIC OBSERVATION is drawn from
+ *  the GP distribution: if we sample a new point at the same x, we will get a
+ *  different realization for the noise, and hence the correlation between the
+ *  two noise realizations is zero.  This class can only be sure that two
+ *  observations are "identical" when they are presented all at once through
+ *  the data matrix.
  *
- *  In addition to comparing the complete x and y vectors, this kernel allows
- *  adding a Kronecker delta when there is a match in only ONE DIMENSION.  This
- *  may be generalized in the future to allow match according to a subset of
- *  the input variables (but is not currently done for performance reasons).
- *  With these terms, the kernel function takes the form:
- *
- *    k(x,y) = delta_x,y * sn + \sum_i delta_x[kr(i)],y[kr(i)] * ks[i]
- *
- *  where kr(i) is the i-th element of 'kronecker_indexes' (representing an
- *  index into the input vectors), and ks[i]=softplus(isp_kronecker_sigma[i]).
- *
- *  Note that to make its operations more robust when used with unconstrained
- *  optimization of hyperparameters, all hyperparameters of this kernel are
- *  specified in the inverse softplus domain, hence the 'isp' prefix.  This is
- *  used in preference to the log-domain used by Rasmussen and Williams in
- *  their implementation of gaussian processes, due to numerical stability.
- *  (It may happen that the optimizer jumps 'too far' along one hyperparameter
- *  and this causes the Gram matrix to become extremely ill-conditioned.)
+ *  The Kronecker terms computed by the base class are ADDDED to the noise 
+ *  computed by this kernel (at test-time also).
  */
-class IIDNoiseKernel : public MemoryCachedKernel
+class IIDNoiseKernel : public KroneckerBaseKernel
 {
-    typedef MemoryCachedKernel inherited;
+    typedef KroneckerBaseKernel inherited;
 
 public:
     //#####  Public Build Options  ############################################
 
-    //! Inverse softplus of the global noise variance.  Default value=0.0
+    //! Inverse softplus of the global noise variance.  Default value=-100.0
+    //! (very close to zero after we take softplus).
     real m_isp_noise_sigma;
-
-    //! Element index in the input vectors that should be subject to additional
-    //! Kronecker delta terms
-    TVec<int> m_kronecker_indexes;
-
-    //! Inverse softplus of the noise variance terms for the Kronecker deltas
-    //! associated with kronecker_indexes
-    Vec m_isp_kronecker_sigma;
     
+    //! Inverse softplus of the noise variance term for the product of
+    //! Kronecker deltas associated with kronecker_indexes, if specified.
+    real m_isp_kronecker_sigma;
+    
 public:
     //#####  Public Member Functions  #########################################
 
@@ -129,16 +125,10 @@
     //! Declares the class options.
     static void declareOptions(OptionList& ol);
 
-    //! Derivative function with respect to kronecker_indexes[arg] hyperparameter
-    real derivKronecker(int i, int j, int arg, real K) const;
-
-    //! Derivative w.r.t kronecker_indexes[arg] for WHOLE MATRIX
-    void computeGramMatrixDerivKronecker(Mat& KD, int arg) const;
+    //! Compute the derivative of the Gram matrix with respect to the Kronecker
+    //! sigma
+    void computeGramMatrixDerivKronecker(Mat& KD) const;
     
-protected:
-    //! Buffer for softplus of m_isp_kronecker_sigma
-    mutable Vec m_kronecker_sigma;
-    
 private:
     //! This does the actual building.
     void build_();

Added: trunk/plearn/ker/KroneckerBaseKernel.cc
===================================================================
--- trunk/plearn/ker/KroneckerBaseKernel.cc	2007-04-11 11:34:01 UTC (rev 6874)
+++ trunk/plearn/ker/KroneckerBaseKernel.cc	2007-04-11 12:13:05 UTC (rev 6875)
@@ -0,0 +1,201 @@
+// -*- C++ -*-
+
+// KroneckerBaseKernel.cc
+//
+// Copyright (C) 2007 Nicolas Chapados
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Nicolas Chapados
+
+/*! \file KroneckerBaseKernel.cc */
+
+#include <plearn/base/lexical_cast.h>
+#include <plearn/math/TMat_maths.h>
+#include "KroneckerBaseKernel.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    KroneckerBaseKernel,
+    "Base class for kernels that make use of Kronecker terms",
+    "This kernel allows the specification of product a of Kronecker delta terms\n"
+    "when there is a match of VALUE in ONE DIMENSION.  (This may be generalized\n"
+    "in the future to allow match according to a subset of the input variables,\n"
+    "but is not currently done for performance reasons).  With these terms, the\n"
+    "kernel function takes the form:\n"
+    "\n"
+    "  k(x,y) = \\product_i delta_x[kr(i)],y[kr(i)]\n"
+    "\n"
+    "where kr(i) is the i-th element of 'kronecker_indexes' (representing an\n"
+    "index into the input vectors)  Derived classes can either integrate these\n"
+    "terms additively (e.g. KroneckerBaseKernel) or multiplicatively\n"
+    "(e.g. ARDBaseKernel and derived classes).  Note that this class does not\n"
+    "provide any hyperparameter associated with this product; an hyperparameter\n"
+    "may be included by derived classes as required.  (Currently, only\n"
+    "IIDNoiseKernel needs one; in other kernels, this is absorbed by the global\n"
+    "function noise hyperparameter)\n"
+    "\n"
+    "The basic idea for Kronecker terms is to selectively build in parts of a\n"
+    "covariance function based on matches in the value of some input variables.\n"
+    "They are useful in conjunction with a \"covariance function builder\" such as\n"
+    "SummationKernel.\n"
+    );
+
+
+KroneckerBaseKernel::KroneckerBaseKernel()
+    : m_default_value(0.)
+{ }
+
+
+//#####  declareOptions  ######################################################
+
+void KroneckerBaseKernel::declareOptions(OptionList& ol)
+{
+    declareOption(
+        ol, "kronecker_indexes", &KroneckerBaseKernel::m_kronecker_indexes,
+        OptionBase::buildoption,
+        "Element index in the input vectors that should be subject to additional\n"
+        "Kronecker delta terms");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+
+//#####  build  ###############################################################
+
+void KroneckerBaseKernel::build()
+{
+    // ### Nothing to add here, simply calls build_
+    inherited::build();
+    build_();
+}
+
+
+//#####  build_  ##############################################################
+
+void KroneckerBaseKernel::build_()
+{ }
+
+
+//#####  evaluate  ############################################################
+
+real KroneckerBaseKernel::evaluate(const Vec& x1, const Vec& x2) const
+{
+    const int n = m_kronecker_indexes.size();
+    if (n > 0) {
+        int*  cur_index = m_kronecker_indexes.data();
+        for (int i=0 ; i<n ; ++i, ++cur_index)
+            if (! fast_is_equal(x1[*cur_index], x2[*cur_index]))
+                return 0.0;
+        return 1.0;
+    }
+    return m_default_value;
+}
+
+
+//#####  computeGramMatrix  ###################################################
+
+void KroneckerBaseKernel::computeGramMatrix(Mat K) const
+{
+    if (!data)
+        PLERROR("Kernel::computeGramMatrix: setDataForKernelMatrix not yet called");
+    if (!is_symmetric)
+        PLERROR("Kernel::computeGramMatrix: not supported for non-symmetric kernels");
+    if (K.length() != data.length() || K.width() != data.length())
+        PLERROR("Kernel::computeGramMatrix: the argument matrix K should be\n"
+                "of size %d x %d (currently of size %d x %d)",
+                data.length(), data.length(), K.length(), K.width());
+                
+    PLASSERT( K.size() == 0 || m_data_cache.size() > 0 );  // Ensure data cached OK
+
+    // Prepare kronecker iteration
+    int   kronecker_num     = m_kronecker_indexes.size();
+    int*  kronecker_indexes = ( kronecker_num > 0?
+                                m_kronecker_indexes.data() : 0 );
+
+    // Compute Gram Matrix
+    int  l = data->length();
+    int  m = K.mod();
+    int  cache_mod = m_data_cache.mod();
+
+    real *data_start = &m_data_cache(0,0);
+    real Kij = m_default_value;
+    real *Ki, *Kji;
+    real *xi = data_start;
+    
+    for (int i=0 ; i<l ; ++i, xi += cache_mod) {
+        Ki  = K[i];
+        Kji = &K[0][i];
+        real *xj = data_start;
+
+        for (int j=0; j<=i; ++j, Kji += m, xj += cache_mod) {
+            if (kronecker_num > 0) {
+                real  product = 1.0;
+                int*  cur_index = kronecker_indexes;
+
+                // Go over Kronecker terms, skipping over an eventual omitted term
+                for (int k=0 ; k<kronecker_num ; ++k, ++cur_index)
+                    if (! fast_is_equal(xi[*cur_index], xj[*cur_index])) {
+                        product = 0.0;
+                        break;
+                    }
+
+                Kij = product;
+            }
+            *Ki++ = Kij;
+        }
+    }
+}
+
+
+//#####  makeDeepCopyFromShallowCopy  #########################################
+
+void KroneckerBaseKernel::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+    deepCopyField(m_kronecker_indexes,   copies);
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/ker/KroneckerBaseKernel.h
===================================================================
--- trunk/plearn/ker/KroneckerBaseKernel.h	2007-04-11 11:34:01 UTC (rev 6874)
+++ trunk/plearn/ker/KroneckerBaseKernel.h	2007-04-11 12:13:05 UTC (rev 6875)
@@ -0,0 +1,147 @@
+// -*- C++ -*-
+
+// KroneckerBaseKernel.h
+//
+// Copyright (C) 2007 Nicolas Chapados
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Nicolas Chapados
+
+/*! \file KroneckerBaseKernel.h */
+
+
+#ifndef KroneckerBaseKernel_INC
+#define KroneckerBaseKernel_INC
+
+#include <plearn/ker/MemoryCachedKernel.h>
+
+namespace PLearn {
+
+/**
+ *  Base class for kernels that make use of Kronecker terms
+ *
+ *  This kernel allows the specification of product a of Kronecker delta terms
+ *  when there is a match of VALUE in ONE DIMENSION.  (This may be generalized
+ *  in the future to allow match according to a subset of the input variables,
+ *  but is not currently done for performance reasons).  With these terms, the
+ *  kernel function takes the form:
+ *
+ *    k(x,y) = \product_i delta_x[kr(i)],y[kr(i)]
+ *
+ *  where kr(i) is the i-th element of 'kronecker_indexes' (representing an
+ *  index into the input vectors).  Derived classes can either integrate these
+ *  terms additively (e.g. KroneckerBaseKernel) or multiplicatively
+ *  (e.g. ARDBaseKernel and derived classes).  Note that this class does not
+ *  provide any hyperparameter associated with this product; an hyperparameter
+ *  may be included by derived classes as required.  (Currently, only
+ *  IIDNoiseKernel needs one; in other kernels, this is absorbed by the global
+ *  function noise hyperparameter).
+ *
+ *  The basic idea for Kronecker terms is to selectively build in parts of a
+ *  covariance function based on matches in the value of some input variables.
+ *  They are useful in conjunction with a "covariance function builder" such as
+ *  SummationKernel.
+ */
+class KroneckerBaseKernel : public MemoryCachedKernel
+{
+    typedef MemoryCachedKernel inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! Element index in the input vectors that should be subject to additional
+    //! Kronecker delta terms
+    TVec<int> m_kronecker_indexes;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    KroneckerBaseKernel();
+
+
+    //#####  Kernel Member Functions  #########################################
+
+    //! Compute K(x1,x2).
+    virtual real evaluate(const Vec& x1, const Vec& x2) const;
+
+    //! Compute the Gram Matrix.  Note that this version DOES NOT CACHE
+    //! the results, since it is usually called by derived classes.
+    virtual void computeGramMatrix(Mat K) const;
+    
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(KroneckerBaseKernel);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    /**
+     *  Value to be used for kernel evaluation if there are no kronecker terms.
+     *  This is initialized to zero.  A derived class may set it to (e.g.) 1.0
+     *  be be sure that the default value is filled to something that can be
+     *  used multiplicatively, even when there are no Kronecker terms.
+     */
+    mutable real m_default_value;
+    
+protected:
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //! This does the actual building.
+    void build_();
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(KroneckerBaseKernel);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Modified: trunk/plearn/ker/RationalQuadraticARDKernel.cc
===================================================================
--- trunk/plearn/ker/RationalQuadraticARDKernel.cc	2007-04-11 11:34:01 UTC (rev 6874)
+++ trunk/plearn/ker/RationalQuadraticARDKernel.cc	2007-04-11 12:13:05 UTC (rev 6875)
@@ -51,13 +51,32 @@
     "Similar to C.E. Rasmussen's GPML code (see http://www.gaussianprocess.org),\n"
     "this kernel is specified as:\n"
     "\n"
-    "  k(x,y) = sf * [1 + (sum_i (x_i - y_i)^2 / w_i)/(2*alpha)]^(-alpha) + k_iid(x,y)\n"
+    "  k(x,y) = sf * [1 + (sum_i (x_i - y_i)^2 / w_i)/(2*alpha)]^(-alpha) * k_kron(x,y)\n"
     "\n"
     "where sf is softplus(isp_signal_sigma), w_i is softplus(isp_global_sigma +\n"
-    "isp_input_sigma[i]), and k_iid(x,y) is the result of IIDNoiseKernel kernel\n"
-    "evaluation.\n"
+    "isp_input_sigma[i]), and k_kron(x,y) is the result of the\n"
+    "KroneckerBaseKernel evaluation, or 1.0 if there are no Kronecker terms.\n"
+    "Note that since the Kronecker terms are incorporated multiplicatively, the\n"
+    "very presence of the term associated to this kernel can be gated by the\n"
+    "value of some input variable(s) (that are incorporated within one or more\n"
+    "Kronecker terms).\n"
     "\n"
-    "Note that to make its operations more robust when used with unconstrained\n"
+    "The current version of this class DOES NOT PROPERLY SUPPORT having both\n"
+    "isp_global_sigma and isp_input_sigma[i] be non-zero (and simultaneously\n"
+    "optimizing with respect to both classes of hyperparameters).  The contrary\n"
+    "situation will yield inconsistent behavior.\n"
+    "\n"
+    "Note that contrarily to previous versions that incorporated IID noise and\n"
+    "Kronecker terms ADDITIVELY, this version does not add any noise at all (and\n"
+    "as explained above incorporates the Kronecker terms multiplicatively).  For\n"
+    "best results, especially with moderately noisy data, IT IS IMPERATIVE to\n"
+    "use whis kernel within a SummationKernel in conjunction with an\n"
+    "IIDNoiseKernel, as follows (e.g. within a GaussianProcessRegressor):\n"
+    "\n"
+    "    kernel = SummationKernel(terms = [ RationalQuadraticARDKernel(),\n"
+    "                                       IIDNoiseKernel() ] )\n"
+    "\n"
+    "In order to make its operations more robust when used with unconstrained\n"
     "optimization of hyperparameters, all hyperparameters of this kernel are\n"
     "specified in the inverse softplus domain.  See IIDNoiseKernel for more\n"
     "explanations.\n"
@@ -98,7 +117,10 @@
 //#####  build_  ##############################################################
 
 void RationalQuadraticARDKernel::build_()
-{ }
+{
+    // Ensure that we multiply in Kronecker terms
+    inherited::m_default_value = 1.0;
+}
 
 
 //#####  makeDeepCopyFromShallowCopy  #########################################
@@ -106,8 +128,6 @@
 void RationalQuadraticARDKernel::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
-
-    deepCopyField(m_noise_gram_cache,        copies);
     deepCopyField(m_pow_minus_alpha_minus_1, copies);
 }
 
@@ -119,8 +139,12 @@
     PLASSERT( x1.size() == x2.size() );
     PLASSERT( !m_isp_input_sigma.size() || x1.size() == m_isp_input_sigma.size() );
 
+    real gating_term = inherited::evaluate(x1,x2);
+    if (fast_is_equal(gating_term, 0.0))
+        return 0.0;
+    
     if (x1.size() == 0)
-        return softplus(m_isp_signal_sigma) + inherited::evaluate(x1,x2);
+        return softplus(m_isp_signal_sigma) * gating_term;
     
     const real* px1 = x1.data();
     const real* px2 = x2.data();
@@ -148,9 +172,8 @@
         }
     }
 
-    // We add the noise covariance as well
-    real noise_cov = inherited::evaluate(x1,x2);
-    return sf * pow(1 + sum_wt / (real(2.)*alpha), -alpha) + noise_cov;
+    // Gate by Kronecker term
+    return sf * pow(1 + sum_wt / (real(2.)*alpha), -alpha) * gating_term;
 }
 
 
@@ -161,10 +184,8 @@
     PLASSERT( !m_isp_input_sigma.size() || dataInputsize() == m_isp_input_sigma.size() );
     PLASSERT( K.size() == 0 || m_data_cache.size() > 0 );  // Ensure data cached OK
 
-    // Compute IID noise gram matrix and save it
+    // Compute Kronecker gram matrix.  No need to cache it.
     inherited::computeGramMatrix(K);
-    m_noise_gram_cache.resize(K.length(), K.width());
-    m_noise_gram_cache << K;
 
     // Precompute some terms
     real sf    = softplus(m_isp_signal_sigma);
@@ -200,6 +221,7 @@
         real *xj = data_start;
         real *pow_cache_cur = pow_cache_row;
 
+        // This whole loop can be optimized further when a Kronecker term is 0
         for (int j=0; j<=i; ++j, xj += cache_mod) {
             // Kernel evaluation per se
             real *x1 = xi;
@@ -226,13 +248,13 @@
                        } while((k -= 8) > 0);
             }
 
+            // Multiplicatively update kernel matrix (already pre-filled with
+            // Kronecker terms, or 1.0 if no Kronecker terms, as per build_).
             real inner_pow   = 1 + sum_wt / (2.*alpha);
             real pow_alpha   = pow(inner_pow, -alpha);
-            real Kij_cur     = sf * pow_alpha;
+            real Kij_cur     = *Kij * sf * pow_alpha;       // Mind *Kij here
             *pow_cache_cur++ = Kij_cur / inner_pow;
-            
-            // Update kernel matrix (already pre-filled with IID noise terms)
-            *Kij++ += Kij_cur;
+            *Kij++           = Kij_cur;
         }
     }
     if (cache_gram_matrix) {
@@ -301,8 +323,7 @@
 
 real RationalQuadraticARDKernel::derivIspSignalSigma(int i, int j, int arg, real K) const
 {
-    real noise = m_noise_gram_cache(i,j);
-    return (K-noise)*sigmoid(m_isp_signal_sigma)/softplus(m_isp_signal_sigma);
+    return K*sigmoid(m_isp_signal_sigma)/softplus(m_isp_signal_sigma);
 }
 
 
@@ -311,14 +332,18 @@
 real RationalQuadraticARDKernel::derivIspGlobalSigma(int i, int j, int arg, real K) const
 {
     // The rational quadratic gives us:
-    //     K = s*k^(-alpha).
-    // Rederive the value of k == (K/s)^(-1/alpha)
+    //     K = s * k^(-alpha) * kron
+    // where kron is 0 or 1.  Rederive the value of k == (K/s)^(-1/alpha)
+    if (fast_is_equal(K, 0.))
+        return 0.;
     real alpha = softplus(m_isp_alpha);
-    real noise = m_noise_gram_cache(i,j);
-    K -= noise;
     real k     = pow(K / softplus(m_isp_signal_sigma), real(-1.) / alpha);
     real inner = (k - 1) * alpha * sigmoid(m_isp_global_sigma) / softplus(m_isp_global_sigma);
     return (K / k) * inner;
+
+    // Note: in the above expression for 'inner' there is the implicit
+    // assumption that the input_sigma[i] are zero, which allows the
+    // sigmoid/softplus term to be factored out of the norm summation.
 }
 
 
@@ -330,13 +355,13 @@
 real RationalQuadraticARDKernel::derivIspInputSigma(int i, int j, int arg, real K) const
 {
     // The rational quadratic gives us:
-    //     K = s*k^(-alpha).
-    // Rederive the value of k == (K/s)^(-1/alpha)
+    //     K = s * k^(-alpha) * kron
+    // where kron is 0 or 1.  Rederive the value of k == (K/s)^(-1/alpha)
+    if (fast_is_equal(K, 0.))
+        return 0.;
     real alpha   = softplus(m_isp_alpha);
     Vec& row_i   = *dataRow(i);
     Vec& row_j   = *dataRow(j);
-    real noise   = m_noise_gram_cache(i,j);
-    K -= noise;
     real k       = pow(K / softplus(m_isp_signal_sigma), real(-1.) / alpha);
     real diff    = row_i[arg] - row_j[arg];
     real sq_diff = diff * diff;
@@ -351,9 +376,12 @@
 
 real RationalQuadraticARDKernel::derivIspAlpha(int i, int j, int arg, real K) const
 {
+    // The rational quadratic gives us:
+    //     K = s * k^(-alpha) * kron
+    // where kron is 0 or 1.  Rederive the value of k == (K/s)^(-1/alpha)
+    if (fast_is_equal(K, 0.))
+        return 0.;
     real alpha = softplus(m_isp_alpha);
-    real noise = m_noise_gram_cache(i,j);
-    K         -= noise;
     real k     = pow(K / softplus(m_isp_signal_sigma), real(-1.) / alpha);
     return sigmoid(m_isp_alpha) * K * (1 - pl_log(k) - 1 / k);
 }
@@ -432,11 +460,6 @@
     real *pow_cache_row = m_pow_minus_alpha_minus_1.data();
     real *pow_cache_cur;
 
-    // Variables that walk over the noise cache
-    int   noise_cache_mod = m_noise_gram_cache.mod();
-    real *noise_cache_row = m_noise_gram_cache[0];
-    real *noise_cache_cur;
-    
     // Variables that walk over the kernel derivative matrix (KD)
     KD.resize(l,l);
     real* KDi = KD.data();                   // Start of row i
@@ -445,24 +468,26 @@
 
     // Iterate on rows of derivative matrix
     for (int i=0 ; i<l ; ++i, Ki += k_mod,
-             KDi += KD_mod, pow_cache_row += pow_cache_mod,
-             noise_cache_row += noise_cache_mod)
+             KDi += KD_mod, pow_cache_row += pow_cache_mod)
     {
         Kij  = Ki;
         KDij = KDi;
         pow_cache_cur   = pow_cache_row;
-        noise_cache_cur = noise_cache_row;
 
         // Iterate on columns of derivative matrix
-        for (int j=0 ; j <= i
-                 ; ++j, ++Kij, ++noise_cache_cur, ++pow_cache_cur)
+        for (int j=0 ; j <= i ; ++j, ++Kij, ++pow_cache_cur)
         {
-            real K      = *Kij - *noise_cache_cur;
-            real k      = K / *pow_cache_cur;
-            real KD_cur = alpha_sigmoid * K * (1 - pl_log(k) - 1/k);
+            real pow_cur = *pow_cache_cur;
+            if (fast_is_equal(pow_cur, 0)) 
+                *KDij++ = 0.;
+            else {
+                real K      = *Kij;
+                real k      = K / pow_cur;
+                real KD_cur = alpha_sigmoid * K * (1 - pl_log(k) - 1/k);
             
-            // Set into derivative matrix
-            *KDij++ = KD_cur;
+                // Set into derivative matrix
+                *KDij++ = KD_cur;
+            }
         }
     }
 }

Modified: trunk/plearn/ker/RationalQuadraticARDKernel.h
===================================================================
--- trunk/plearn/ker/RationalQuadraticARDKernel.h	2007-04-11 11:34:01 UTC (rev 6874)
+++ trunk/plearn/ker/RationalQuadraticARDKernel.h	2007-04-11 12:13:05 UTC (rev 6875)
@@ -54,13 +54,32 @@
  *  Similar to C.E. Rasmussen's GPML code (see http://www.gaussianprocess.org),
  *  this kernel is specified as:
  *
- *    k(x,y) = sf * [1 + (sum_i (x_i - y_i)^2 / w_i)/(2*alpha)]^(-alpha) + k_iid(x,y)
+ *    k(x,y) = sf * [1 + (sum_i (x_i - y_i)^2 / w_i)/(2*alpha)]^(-alpha) * k_kron(x,y)
  *
  *  where sf is softplus(isp_signal_sigma), w_i is softplus(isp_global_sigma +
- *  isp_input_sigma[i]), and k_iid(x,y) is the result of the IIDNoiseKernel
- *  kernel evaluation.
+ *  isp_input_sigma[i]), and k_kron(x,y) is the result of the
+ *  KroneckerBaseKernel evaluation, or 1.0 if there are no Kronecker terms.
+ *  Note that since the Kronecker terms are incorporated multiplicatively, the
+ *  very presence of the term associated to this kernel can be gated by the
+ *  value of some input variable(s) (that are incorporated within one or more
+ *  Kronecker terms).
  *
- *  Note that to make its operations more robust when used with unconstrained
+ *  The current version of this class DOES NOT PROPERLY SUPPORT having both
+ *  isp_global_sigma and isp_input_sigma[i] be non-zero (and simultaneously
+ *  optimizing with respect to both classes of hyperparameters).  The contrary
+ *  situation will yield inconsistent behavior.
+ *
+ *  Note that contrarily to previous versions that incorporated IID noise and
+ *  Kronecker terms ADDITIVELY, this version does not add any noise at all (and
+ *  as explained above incorporates the Kronecker terms multiplicatively).  For
+ *  best results, especially with moderately noisy data, IT IS IMPERATIVE to
+ *  use whis kernel within a SummationKernel in conjunction with an
+ *  IIDNoiseKernel, as follows (e.g. within a GaussianProcessRegressor):
+ *
+ *      kernel = SummationKernel(terms = [ RationalQuadraticARDKernel(),
+ *                                         IIDNoiseKernel() ] )
+ *
+ *  In order to make its operations more robust when used with unconstrained
  *  optimization of hyperparameters, all hyperparameters of this kernel are
  *  specified in the inverse softplus domain.  See IIDNoiseKernel for more
  *  explanations.
@@ -132,12 +151,12 @@
     void computeGramMatrixDerivIspAlpha(Mat& KD) const;
     
 protected:
-    //! Cached version of IID noise gram matrix
-    mutable Mat m_noise_gram_cache;
-
     /**
-     *  Cached version of the K / k terms, useful for computing derivatives
-     *      pow(1 + sum_wt / (2*alpha), -alpha-1)
+     *  Cached version of the K / k terms, namely:
+     *
+     *      pow(1 + sum_wt / (2*alpha), -alpha-1) * sf * kron
+     *
+     *  This is useful for computing derivatives,
      */
     mutable Mat m_pow_minus_alpha_minus_1;
 

Modified: trunk/plearn/ker/SquaredExponentialARDKernel.cc
===================================================================
--- trunk/plearn/ker/SquaredExponentialARDKernel.cc	2007-04-11 11:34:01 UTC (rev 6874)
+++ trunk/plearn/ker/SquaredExponentialARDKernel.cc	2007-04-11 12:13:05 UTC (rev 6875)
@@ -55,12 +55,31 @@
     "Similar to C.E. Rasmussen's GPML code (see http://www.gaussianprocess.org),\n"
     "this kernel function is specified as:\n"
     "\n"
-    "  k(x,y) = sf * exp(- 0.5 * (sum_i (x_i - y_i)^2 / w_i)) + k_iid(x,y)\n"
+    "  k(x,y) = sf * exp(- 0.5 * (sum_i (x_i - y_i)^2 / w_i)) * k_kron(x,y)\n"
     "\n"
     "where sf is softplus(isp_signal_sigma), w_i is softplus(isp_global_sigma +\n"
-    "isp_input_sigma[i]), and k_iid(x,y) is the result of the IIDNoiseKernel\n"
-    "kernel evaluation.\n"
+    "isp_input_sigma[i]), and k_kron(x,y) is the result of the\n"
+    "KroneckerBaseKernel evaluation, or 1.0 if there are no Kronecker terms.\n"
+    "Note that since the Kronecker terms are incorporated multiplicatively, the\n"
+    "very presence of the term associated to this kernel can be gated by the\n"
+    "value of some input variable(s) (that are incorporated within one or more\n"
+    "Kronecker terms).\n"
     "\n"
+    "The current version of this class DOES NOT ALLOW differentiating the Kernel\n"
+    "matrix with respect to the Kronecker hyperparameters.  These parameters are\n"
+    "redundant due to the presence of the global sf above; they should be set to\n"
+    "1.0 and left untouched by hyperoptimization.\n"
+    "\n"
+    "Note that contrarily to previous versions that incorporated IID noise and\n"
+    "Kronecker terms ADDITIVELY, this version does not add any noise at all (and\n"
+    "as explained above incorporates the Kronecker terms multiplicatively).  For\n"
+    "best results, especially with moderately noisy data, IT IS IMPERATIVE to\n"
+    "use whis kernel within a SummationKernel in conjunction with an\n"
+    "IIDNoiseKernel, as follows (e.g. within a GaussianProcessRegressor):\n"
+    "\n"
+    "    kernel = SummationKernel(terms = [ SquaredExponentialARDKernel(),\n"
+    "                                       IIDNoiseKernel() ] )\n"
+    "\n"
     "Note that to make its operations more robust when used with unconstrained\n"
     "optimization of hyperparameters, all hyperparameters of this kernel are\n"
     "specified in the inverse softplus domain.  See IIDNoiseKernel for more\n"
@@ -95,6 +114,8 @@
 
 void SquaredExponentialARDKernel::build_()
 {
+    // Ensure that we multiply in Kronecker terms
+    inherited::m_default_value = 1.0;
 }
 
 
@@ -105,8 +126,12 @@
     PLASSERT( x1.size() == x2.size() );
     PLASSERT( !m_isp_input_sigma.size() || x1.size() == m_isp_input_sigma.size() );
 
+    real gating_term = inherited::evaluate(x1,x2);
+    if (fast_is_equal(gating_term, 0.0))
+        return 0.0;
+    
     if (x1.size() == 0)
-        return softplus(m_isp_signal_sigma) + inherited::evaluate(x1,x2);
+        return softplus(m_isp_signal_sigma) * gating_term;
     
     const real* px1 = x1.data();
     const real* px2 = x2.data();
@@ -130,14 +155,8 @@
         }
     }
 
-    // EXPERIMENTAL: Multiply noise_cov by kernel value if we have kronecker
-    // terms, otherwise disregard noise_cov
-    // real noise_cov = ( m_kronecker_indexes.size() > 0?
-    //                    inherited::evaluate(x1,x2) : 1.0 );
-    // return sf * exp(-0.5 * expval) * noise_cov;
-
-    real noise_cov = inherited::evaluate(x1,x2);
-    return sf * exp(-0.5 * expval) + noise_cov;
+    // Gate by Kronecker term
+    return sf * exp(-0.5 * expval) * gating_term;
 }
 
 
@@ -148,8 +167,10 @@
     PLASSERT( !m_isp_input_sigma.size() || dataInputsize() == m_isp_input_sigma.size() );
     PLASSERT( K.size() == 0 || m_data_cache.size() > 0 );  // Ensure data cached OK
 
-    // Compute IID noise gram matrix
+    // Compute Kronecker gram matrix and save it
     inherited::computeGramMatrix(K);
+    m_kron_gram_cache.resize(K.length(), K.width());
+    m_kron_gram_cache << K;
 
     // Precompute some terms
     real sf    = softplus(m_isp_signal_sigma);
@@ -203,8 +224,10 @@
                        } while((k -= 8) > 0);
             }
 
-            // Update kernel matrix (already pre-filled with IID noise terms)
-            *Kij++ += sf * exp(-0.5 * sum_wt);
+            // Multiplicatively update kernel matrix (already pre-filled with
+            // Kronecker terms, or 1.0 if no Kronecker terms, as per build_).
+            real Kij_cur = *Kij * sf * exp(-0.5 * sum_wt);
+            *Kij++ = Kij_cur;
         }
     }
     if (cache_gram_matrix) {
@@ -215,11 +238,125 @@
 }
 
 
+//#####  computeGramMatrixDerivative  #########################################
+
+void SquaredExponentialARDKernel::computeGramMatrixDerivative(
+    Mat& KD, const string& kernel_param, real epsilon) const
+{
+    static const string ISS("isp_signal_sigma");
+    static const string IGS("isp_global_sigma");
+    static const string IIS("isp_input_sigma[");
+
+    if (kernel_param == ISS) {
+        computeGramMatrixDerivNV<
+            SquaredExponentialARDKernel,
+            &SquaredExponentialARDKernel::derivIspSignalSigma>(KD, this, -1);
+    }
+    else if (kernel_param == IGS) {
+        computeGramMatrixDerivNV<
+            SquaredExponentialARDKernel,
+            &SquaredExponentialARDKernel::derivIspGlobalSigma>(KD, this, -1);
+    }
+    else if (string_begins_with(kernel_param, IIS) &&
+             kernel_param[kernel_param.size()-1] == ']')
+    {
+        int arg = tolong(kernel_param.substr(
+                             IIS.size(), kernel_param.size() - IIS.size() - 1));
+        PLASSERT( arg < m_isp_input_sigma.size() );
+
+        computeGramMatrixDerivIspInputSigma(KD, arg);
+
+    }
+    else
+        inherited::computeGramMatrixDerivative(KD, kernel_param, epsilon);
+}
+
+
+//#####  derivIspSignalSigma  #################################################
+
+real SquaredExponentialARDKernel::derivIspSignalSigma(int i, int j, int arg, real K) const
+{
+    return K*sigmoid(m_isp_signal_sigma)/softplus(m_isp_signal_sigma);
+}
+
+
+//#####  derivIspGlobalSigma  #################################################
+
+real SquaredExponentialARDKernel::derivIspGlobalSigma(int i, int j, int arg, real K) const
+{
+    if (fast_is_equal(K,0.))
+        return 0.;
+
+    // The norm term inside the exponential may be accessed as Log(K/(sf*kron))
+    real kron  = m_kron_gram_cache(i,j);
+    real inner = pl_log(K / (kron * softplus(m_isp_signal_sigma)));
+    return K * inner * sigmoid(m_isp_global_sigma) / softplus(m_isp_global_sigma);
+
+    // Note: in the above expression for 'inner' there is the implicit
+    // assumption that the input_sigma[i] are zero, which allows the
+    // sigmoid/softplus term to be factored out of the norm summation.
+}
+
+
+//#####  computeGramMatrixDerivIspInputSigma  #################################
+
+void SquaredExponentialARDKernel::computeGramMatrixDerivIspInputSigma(Mat& KD,
+                                                                      int arg) const
+{
+    // Precompute some terms
+    real input_sigma_arg = m_input_sigma[arg];
+    real input_sigma_sq  = input_sigma_arg * input_sigma_arg;
+    real input_sigmoid   = sigmoid(m_isp_global_sigma + m_isp_input_sigma[arg]);
+    
+    // Compute Gram Matrix derivative w.r.t. isp_input_sigma[arg]
+    int  l = data->length();
+
+    // Variables that walk over the data matrix
+    int  cache_mod = m_data_cache.mod();
+    real *data_start = &m_data_cache(0,0);
+    real *xi = data_start+arg;               // Iterator on data rows
+
+    // Variables that walk over the gram cache
+    int   gram_cache_mod = gram_matrix.mod();
+    real *gram_cache_row = gram_matrix.data();
+    real *gram_cache_cur;
+    
+    // Variables that walk over the kernel derivative matrix (KD)
+    KD.resize(l,l);
+    real* KDi = KD.data();                   // Start of row i
+    real* KDij;                              // Current element on row i
+    int   KD_mod = KD.mod();
+
+    // Iterate on rows of derivative matrix
+    for (int i=0 ; i<l ; ++i, xi += cache_mod, KDi += KD_mod,
+             gram_cache_row += gram_cache_mod)
+    {
+        KDij = KDi;
+        real *xj  = data_start+arg;           // Inner iterator on data rows
+        gram_cache_cur = gram_cache_row;
+
+        // Iterate on columns of derivative matrix
+        for (int j=0 ; j <= i
+                 ; ++j, xj += cache_mod, ++gram_cache_cur)
+        {
+            real diff    = *xi - *xj;
+            real sq_diff = diff * diff;
+            real KD_cur  = 0.5 * *gram_cache_cur *
+                           input_sigmoid * sq_diff / input_sigma_sq;
+
+            // Set into derivative matrix
+            *KDij++ = KD_cur;
+        }
+    }
+}
+
+
 //#####  makeDeepCopyFromShallowCopy  #########################################
 
 void SquaredExponentialARDKernel::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
+    deepCopyField(m_kron_gram_cache, copies);
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn/ker/SquaredExponentialARDKernel.h
===================================================================
--- trunk/plearn/ker/SquaredExponentialARDKernel.h	2007-04-11 11:34:01 UTC (rev 6874)
+++ trunk/plearn/ker/SquaredExponentialARDKernel.h	2007-04-11 12:13:05 UTC (rev 6875)
@@ -58,12 +58,31 @@
  *  Similar to C.E. Rasmussen's GPML code (see http://www.gaussianprocess.org),
  *  this kernel function is specified as:
  *
- *    k(x,y) = sf * exp(- 0.5 * (sum_i (x_i - y_i)^2 / w_i)) + k_iid(x,y)
+ *    k(x,y) = sf * exp(- 0.5 * (sum_i (x_i - y_i)^2 / w_i)) * k_kron(x,y)
  *
  *  where sf is softplus(isp_signal_sigma), w_i is softplus(isp_global_sigma +
- *  isp_input_sigma[i]), and k_iid(x,y) is the result of the IIDNoiseKernel
- *  kernel evaluation.
+ *  isp_input_sigma[i]), and k_kron(x,y) is the result of the
+ *  KroneckerBaseKernel evaluation, or 1.0 if there are no Kronecker terms.
+ *  Note that since the Kronecker terms are incorporated multiplicatively, the
+ *  very presence of the term associated to this kernel can be gated by the
+ *  value of some input variable(s) (that are incorporated within one or more
+ *  Kronecker terms).
  *
+ *  The current version of this class DOES NOT ALLOW differentiating the Kernel
+ *  matrix with respect to the Kronecker hyperparameters.  These parameters are
+ *  redundant due to the presence of the global sf above; they should be set to
+ *  1.0 and left untouched by hyperoptimization.
+ *
+ *  Note that contrarily to previous versions that incorporated IID noise and
+ *  Kronecker terms ADDITIVELY, this version does not add any noise at all (and
+ *  as explained above incorporates the Kronecker terms multiplicatively).  For
+ *  best results, especially with moderately noisy data, IT IS IMPERATIVE to
+ *  use whis kernel within a SummationKernel in conjunction with an
+ *  IIDNoiseKernel, as follows (e.g. within a GaussianProcessRegressor):
+ *
+ *      kernel = SummationKernel(terms = [ SquaredExponentialARDKernel(),
+ *                                         IIDNoiseKernel() ] )
+ *
  *  Note that to make its operations more robust when used with unconstrained
  *  optimization of hyperparameters, all hyperparameters of this kernel are
  *  specified in the inverse softplus domain.  See IIDNoiseKernel for more
@@ -95,8 +114,8 @@
     
     //! Directly compute the derivative with respect to hyperparameters
     //! (Faster than finite differences...)
-    // virtual void computeGramMatrixDerivative(Mat& KD, const string& kernel_param,
-    //                                          real epsilon=1e-6) const;
+    virtual void computeGramMatrixDerivative(Mat& KD, const string& kernel_param,
+                                             real epsilon=1e-6) const;
     
 
     //#####  PLearn::Object Protocol  #########################################
@@ -114,6 +133,19 @@
     //! Declares the class options.
     static void declareOptions(OptionList& ol);
 
+    //! Derivative function with respect to isp_signal_sigma
+    real derivIspSignalSigma(int i, int j, int arg, real K) const;
+
+    //! Derivative function with respect to isp_global_sigma
+    real derivIspGlobalSigma(int i, int j, int arg, real K) const;
+    
+    // Compute derivative w.r.t. isp_input_sigma[arg] for WHOLE MATRIX
+    void computeGramMatrixDerivIspInputSigma(Mat& KD, int arg) const;
+    
+protected:
+    //! Cached version of Kronecker gram matrix
+    mutable Mat m_kron_gram_cache;
+
 private:
     //! This does the actual building.
     void build_();

Modified: trunk/plearn/ker/SummationKernel.cc
===================================================================
--- trunk/plearn/ker/SummationKernel.cc	2007-04-11 11:34:01 UTC (rev 6874)
+++ trunk/plearn/ker/SummationKernel.cc	2007-04-11 12:13:05 UTC (rev 6875)
@@ -36,7 +36,9 @@
 
 /*! \file SummationKernel.cc */
 
-
+#include <plearn/base/stringutils.h>
+#include <plearn/base/lexical_cast.h>
+#include <plearn/vmat/SelectColumnsVMatrix.h>
 #include "SummationKernel.h"
 
 namespace PLearn {
@@ -121,6 +123,40 @@
 }
 
 
+//#####  setDataForKernelMatrix  ##############################################
+
+void SummationKernel::setDataForKernelMatrix(VMat the_data)
+{
+    inherited::setDataForKernelMatrix(the_data);
+    bool split_inputs = m_input_indexes.size() > 0;
+    for (int i=0, n=m_terms.size() ; i<n ; ++i) {
+        if (split_inputs && m_input_indexes[i].size() > 0) {
+            VMat sub_inputs = new SelectColumnsVMatrix(the_data, m_input_indexes[i]);
+            m_terms[i]->setDataForKernelMatrix(sub_inputs);
+        }
+        else
+            m_terms[i]->setDataForKernelMatrix(the_data);
+    }
+}
+
+
+//#####  addDataForKernelMatrix  ##############################################
+
+void SummationKernel::addDataForKernelMatrix(const Vec& newRow)
+{
+    inherited::addDataForKernelMatrix(newRow);
+    bool split_inputs = m_input_indexes.size() > 0;
+    for (int i=0, n=m_terms.size() ; i<n ; ++i) {
+        if (split_inputs && m_input_indexes[i].size() > 0) {
+            selectElements(newRow, m_input_indexes[i], m_input_buf1[i]);
+            m_terms[i]->addDataForKernelMatrix(m_input_buf1[i]);
+        }
+        else
+            m_terms[i]->addDataForKernelMatrix(newRow);
+    }
+}
+
+
 //#####  evaluate  ############################################################
 
 real SummationKernel::evaluate(const Vec& x1, const Vec& x2) const
@@ -141,6 +177,61 @@
 }
 
 
+//#####  computeGramMatrix  ###################################################
+
+void SummationKernel::computeGramMatrix(Mat K) const
+{
+    // Assume that K has the right size; will have error in subkernels
+    // evaluation if not the right size in any case.
+    m_gram_buf.resize(K.width(), K.length());
+
+    for (int i=0, n=m_terms.size() ; i<n ; ++i) {
+        if (i==0)
+            m_terms[i]->computeGramMatrix(K);
+        else {
+            m_terms[i]->computeGramMatrix(m_gram_buf);
+            K += m_gram_buf;
+        }
+    }
+}
+
+
+//#####  computeGramMatrixDerivative  #########################################
+void SummationKernel::computeGramMatrixDerivative(
+    Mat& KD, const string& kernel_param, real epsilon) const
+{
+    // Find which term we want to compute the derivative for
+    if (string_begins_with(kernel_param, "terms[")) {
+        string::size_type rest = kernel_param.find("].");
+        if (rest == string::npos)
+            PLERROR("%s: malformed hyperparameter name for computing derivative '%s'",
+                    __FUNCTION__, kernel_param.c_str());
+
+        string sub_param  = kernel_param.substr(rest+2);
+        string term_index = kernel_param.substr(6,rest-6); // len("terms[") == 6
+        int i = lexical_cast<int>(term_index);
+        if (i < 0 || i >= m_terms.size())
+            PLERROR("%s: out of bounds access to term %d when computing derivative\n"
+                    "for kernel parameter '%d'; only %d terms (0..%d) are available\n"
+                    "in the SummationKernel", __FUNCTION__, i, kernel_param.c_str(),
+                    m_terms.size(), m_terms.size()-1);
+        
+        m_terms[i]->computeGramMatrixDerivative(KD, sub_param, epsilon);
+    }
+    else
+        inherited::computeGramMatrixDerivative(KD, kernel_param, epsilon);
+
+    // Compare against finite differences
+    // Mat KD1;
+    // Kernel::computeGramMatrixDerivative(KD1, kernel_param, epsilon);
+    // cerr << "Kernel hyperparameter: " << kernel_param << endl;
+    // cerr << "Analytic derivative (200th row):" << endl
+    //      << KD(200) << endl
+    //      << "Finite differences:" << endl
+    //      << KD1(200) << endl;
+}
+
+
 //#####  makeDeepCopyFromShallowCopy  #########################################
 
 void SummationKernel::makeDeepCopyFromShallowCopy(CopiesMap& copies)
@@ -151,6 +242,7 @@
     deepCopyField(m_input_indexes,  copies);
     deepCopyField(m_input_buf1,     copies);
     deepCopyField(m_input_buf2,     copies);
+    deepCopyField(m_gram_buf,       copies);
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn/ker/SummationKernel.h
===================================================================
--- trunk/plearn/ker/SummationKernel.h	2007-04-11 11:34:01 UTC (rev 6874)
+++ trunk/plearn/ker/SummationKernel.h	2007-04-11 12:13:05 UTC (rev 6875)
@@ -84,9 +84,23 @@
 
     //#####  Kernel Member Functions  #########################################
 
+    //! Distribute to terms (sub-kernels) in the summation, subsetting if required
+    virtual void setDataForKernelMatrix(VMat the_data);
+
+    //! Distribute to terms (sub-kernels) in the summation, subsetting if required
+    virtual void addDataForKernelMatrix(const Vec& newRow);
+
     //! Compute K(x1,x2).
     virtual real evaluate(const Vec& x1, const Vec& x2) const;
 
+    //! Compute the Gram Matrix by calling subkernels computeGramMatrix
+    virtual void computeGramMatrix(Mat K) const;
+    
+    //! Directly compute the derivative with respect to hyperparameters
+    //! (Faster than finite differences...)
+    virtual void computeGramMatrixDerivative(Mat& KD, const string& kernel_param,
+                                             real epsilon=1e-6) const;
+    
 
     //#####  PLearn::Object Protocol  #########################################
 
@@ -104,6 +118,9 @@
     TVec<Vec> m_input_buf1;
     TVec<Vec> m_input_buf2;
 
+    //! Temporary buffer for Gram matrix accumulation
+    mutable Mat m_gram_buf;
+
 protected:
     //! Declares the class options.
     static void declareOptions(OptionList& ol);



From chapados at mail.berlios.de  Wed Apr 11 15:57:59 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Wed, 11 Apr 2007 15:57:59 +0200
Subject: [Plearn-commits] r6876 - trunk/plearn/ker
Message-ID: <200704111357.l3BDvxIb027561@sheep.berlios.de>

Author: chapados
Date: 2007-04-11 15:57:59 +0200 (Wed, 11 Apr 2007)
New Revision: 6876

Modified:
   trunk/plearn/ker/IIDNoiseKernel.cc
   trunk/plearn/ker/RationalQuadraticARDKernel.cc
   trunk/plearn/ker/RationalQuadraticARDKernel.h
   trunk/plearn/ker/SquaredExponentialARDKernel.cc
   trunk/plearn/ker/SquaredExponentialARDKernel.h
   trunk/plearn/ker/SummationKernel.cc
Log:
Minor bug fixes in new kernels and further performance improvements in bprop

Modified: trunk/plearn/ker/IIDNoiseKernel.cc
===================================================================
--- trunk/plearn/ker/IIDNoiseKernel.cc	2007-04-11 12:13:05 UTC (rev 6875)
+++ trunk/plearn/ker/IIDNoiseKernel.cc	2007-04-11 13:57:59 UTC (rev 6876)
@@ -77,7 +77,7 @@
 
 IIDNoiseKernel::IIDNoiseKernel()
     : m_isp_noise_sigma(-100.0), /* very close to zero... */
-      m_isp_kronecker_sigma(100.0)
+      m_isp_kronecker_sigma(-100.0)
 { }
 
 

Modified: trunk/plearn/ker/RationalQuadraticARDKernel.cc
===================================================================
--- trunk/plearn/ker/RationalQuadraticARDKernel.cc	2007-04-11 12:13:05 UTC (rev 6875)
+++ trunk/plearn/ker/RationalQuadraticARDKernel.cc	2007-04-11 13:57:59 UTC (rev 6876)
@@ -276,9 +276,11 @@
     static const string IAL("isp_alpha");
 
     if (kernel_param == ISS) {
-        computeGramMatrixDerivNV<
-            RationalQuadraticARDKernel,
-            &RationalQuadraticARDKernel::derivIspSignalSigma>(KD, this, -1);
+        computeGramMatrixDerivIspSignalSigma(KD);
+
+        // computeGramMatrixDerivNV<
+        //     RationalQuadraticARDKernel,
+        //     &RationalQuadraticARDKernel::derivIspSignalSigma>(KD, this, -1);
     }
     else if (kernel_param == IGS) {
         computeGramMatrixDerivNV<
@@ -387,6 +389,22 @@
 }
 
 
+//#####  computeGramMatrixDerivIspSignalSigma  ################################
+
+void RationalQuadraticARDKernel::computeGramMatrixDerivIspSignalSigma(Mat& KD) const
+{
+    int l = data->length();
+    KD.resize(l,l);
+    PLASSERT_MSG(
+        gram_matrix.width() == l && gram_matrix.length() == l,
+        "To compute the derivative with respect to 'isp_signal_sigma', the\n"
+        "Gram matrix must be precomputed and cached in SquaredExponentialARDKernel.");
+    
+    KD << gram_matrix;
+    KD *= sigmoid(m_isp_signal_sigma)/softplus(m_isp_signal_sigma);
+}
+
+
 //#####  computeGramMatrixDerivIspInputSigma  #################################
 
 void RationalQuadraticARDKernel::computeGramMatrixDerivIspInputSigma(Mat& KD,
@@ -449,9 +467,13 @@
     
     // Compute Gram Matrix derivative w.r.t. isp_alpha
     int  l     = data->length();
-    int  k_mod = gram_matrix.mod();
+    PLASSERT_MSG(
+        gram_matrix.width() == l && gram_matrix.length() == l,
+        "To compute the derivative with respect to 'isp_alpha', the\n"
+        "Gram matrix must be precomputed and cached in RationalQuadraticARDKernel.");
 
     // Variables that walk over the pre-computed kernel matrix (K) 
+    int  k_mod = gram_matrix.mod();
     real *Ki = &gram_matrix(0,0);            // Current row of kernel matrix
     real *Kij;                               // Current element of kernel matrix
 

Modified: trunk/plearn/ker/RationalQuadraticARDKernel.h
===================================================================
--- trunk/plearn/ker/RationalQuadraticARDKernel.h	2007-04-11 12:13:05 UTC (rev 6875)
+++ trunk/plearn/ker/RationalQuadraticARDKernel.h	2007-04-11 13:57:59 UTC (rev 6876)
@@ -144,6 +144,9 @@
     //! Derivative function with respect to isp_alpha
     real derivIspAlpha(int i, int j, int arg, real K) const;
 
+    // Compute derivative w.r.t. isp_signal_sigma for WHOLE MATRIX
+    void computeGramMatrixDerivIspSignalSigma(Mat& KD) const;
+    
     // Compute derivative w.r.t. isp_input_sigma[arg] for WHOLE MATRIX
     void computeGramMatrixDerivIspInputSigma(Mat& KD, int arg) const;
     

Modified: trunk/plearn/ker/SquaredExponentialARDKernel.cc
===================================================================
--- trunk/plearn/ker/SquaredExponentialARDKernel.cc	2007-04-11 12:13:05 UTC (rev 6875)
+++ trunk/plearn/ker/SquaredExponentialARDKernel.cc	2007-04-11 13:57:59 UTC (rev 6876)
@@ -167,10 +167,8 @@
     PLASSERT( !m_isp_input_sigma.size() || dataInputsize() == m_isp_input_sigma.size() );
     PLASSERT( K.size() == 0 || m_data_cache.size() > 0 );  // Ensure data cached OK
 
-    // Compute Kronecker gram matrix and save it
+    // Compute Kronecker gram matrix
     inherited::computeGramMatrix(K);
-    m_kron_gram_cache.resize(K.length(), K.width());
-    m_kron_gram_cache << K;
 
     // Precompute some terms
     real sf    = softplus(m_isp_signal_sigma);
@@ -248,9 +246,11 @@
     static const string IIS("isp_input_sigma[");
 
     if (kernel_param == ISS) {
-        computeGramMatrixDerivNV<
-            SquaredExponentialARDKernel,
-            &SquaredExponentialARDKernel::derivIspSignalSigma>(KD, this, -1);
+        computeGramMatrixDerivIspSignalSigma(KD);
+        
+        // computeGramMatrixDerivNV<
+        //     SquaredExponentialARDKernel,
+        //     &SquaredExponentialARDKernel::derivIspSignalSigma>(KD, this, -1);
     }
     else if (kernel_param == IGS) {
         computeGramMatrixDerivNV<
@@ -276,6 +276,7 @@
 
 real SquaredExponentialARDKernel::derivIspSignalSigma(int i, int j, int arg, real K) const
 {
+    // (No longer used; see computeGramMatrixDerivIspInputSigma below)
     return K*sigmoid(m_isp_signal_sigma)/softplus(m_isp_signal_sigma);
 }
 
@@ -287,10 +288,9 @@
     if (fast_is_equal(K,0.))
         return 0.;
 
-    // The norm term inside the exponential may be accessed as Log(K/(sf*kron))
-    real kron  = m_kron_gram_cache(i,j);
-    real inner = pl_log(K / (kron * softplus(m_isp_signal_sigma)));
-    return K * inner * sigmoid(m_isp_global_sigma) / softplus(m_isp_global_sigma);
+    // The norm term inside the exponential may be accessed as Log(K/sf)
+    real inner = pl_log(K / softplus(m_isp_signal_sigma));
+    return - K * inner * sigmoid(m_isp_global_sigma) / softplus(m_isp_global_sigma);
 
     // Note: in the above expression for 'inner' there is the implicit
     // assumption that the input_sigma[i] are zero, which allows the
@@ -298,6 +298,22 @@
 }
 
 
+//#####  computeGramMatrixDerivIspSignalSigma  ################################
+
+void SquaredExponentialARDKernel::computeGramMatrixDerivIspSignalSigma(Mat& KD) const
+{
+    int l = data->length();
+    KD.resize(l,l);
+    PLASSERT_MSG(
+        gram_matrix.width() == l && gram_matrix.length() == l,
+        "To compute the derivative with respect to 'isp_signal_sigma', the\n"
+        "Gram matrix must be precomputed and cached in SquaredExponentialARDKernel.");
+    
+    KD << gram_matrix;
+    KD *= sigmoid(m_isp_signal_sigma)/softplus(m_isp_signal_sigma);
+}
+
+
 //#####  computeGramMatrixDerivIspInputSigma  #################################
 
 void SquaredExponentialARDKernel::computeGramMatrixDerivIspInputSigma(Mat& KD,
@@ -310,6 +326,10 @@
     
     // Compute Gram Matrix derivative w.r.t. isp_input_sigma[arg]
     int  l = data->length();
+    PLASSERT_MSG(
+        gram_matrix.width() == l && gram_matrix.length() == l,
+        "To compute the derivative with respect to 'isp_input_sigma[i]', the\n"
+        "Gram matrix must be precomputed and cached in SquaredExponentialARDKernel.");
 
     // Variables that walk over the data matrix
     int  cache_mod = m_data_cache.mod();
@@ -356,7 +376,6 @@
 void SquaredExponentialARDKernel::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
-    deepCopyField(m_kron_gram_cache, copies);
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn/ker/SquaredExponentialARDKernel.h
===================================================================
--- trunk/plearn/ker/SquaredExponentialARDKernel.h	2007-04-11 12:13:05 UTC (rev 6875)
+++ trunk/plearn/ker/SquaredExponentialARDKernel.h	2007-04-11 13:57:59 UTC (rev 6876)
@@ -139,13 +139,12 @@
     //! Derivative function with respect to isp_global_sigma
     real derivIspGlobalSigma(int i, int j, int arg, real K) const;
     
+    // Compute derivative w.r.t. isp_signal_sigma for WHOLE MATRIX
+    void computeGramMatrixDerivIspSignalSigma(Mat& KD) const;
+    
     // Compute derivative w.r.t. isp_input_sigma[arg] for WHOLE MATRIX
     void computeGramMatrixDerivIspInputSigma(Mat& KD, int arg) const;
     
-protected:
-    //! Cached version of Kronecker gram matrix
-    mutable Mat m_kron_gram_cache;
-
 private:
     //! This does the actual building.
     void build_();

Modified: trunk/plearn/ker/SummationKernel.cc
===================================================================
--- trunk/plearn/ker/SummationKernel.cc	2007-04-11 12:13:05 UTC (rev 6875)
+++ trunk/plearn/ker/SummationKernel.cc	2007-04-11 13:57:59 UTC (rev 6876)
@@ -222,13 +222,13 @@
         inherited::computeGramMatrixDerivative(KD, kernel_param, epsilon);
 
     // Compare against finite differences
-    // Mat KD1;
-    // Kernel::computeGramMatrixDerivative(KD1, kernel_param, epsilon);
-    // cerr << "Kernel hyperparameter: " << kernel_param << endl;
-    // cerr << "Analytic derivative (200th row):" << endl
-    //      << KD(200) << endl
-    //      << "Finite differences:" << endl
-    //      << KD1(200) << endl;
+    Mat KD1;
+    Kernel::computeGramMatrixDerivative(KD1, kernel_param, epsilon);
+    cerr << "Kernel hyperparameter: " << kernel_param << endl;
+    cerr << "Analytic derivative (200th row):" << endl
+         << KD(200) << endl
+         << "Finite differences:" << endl
+         << KD1(200) << endl;
 }
 
 



From chapados at mail.berlios.de  Wed Apr 11 15:58:34 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Wed, 11 Apr 2007 15:58:34 +0200
Subject: [Plearn-commits] r6877 - trunk/commands
Message-ID: <200704111358.l3BDwYfH027619@sheep.berlios.de>

Author: chapados
Date: 2007-04-11 15:58:34 +0200 (Wed, 11 Apr 2007)
New Revision: 6877

Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
Added new gaussian-processes kernels to plearn includes

Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2007-04-11 13:57:59 UTC (rev 6876)
+++ trunk/commands/plearn_noblas_inc.h	2007-04-11 13:58:34 UTC (rev 6877)
@@ -108,10 +108,12 @@
 #include <plearn/ker/EpanechnikovKernel.h>
 #include <plearn/ker/GaussianKernel.h>
 #include <plearn/ker/GeodesicDistanceKernel.h>
+#include <plearn/ker/IIDNoiseKernel.h>
 #include <plearn/ker/NegOutputCostFunction.h>
 #include <plearn/ker/PolynomialKernel.h>
 #include <plearn/ker/RationalQuadraticARDKernel.h>
 #include <plearn/ker/SquaredExponentialARDKernel.h>
+#include <plearn/ker/SummationKernel.h>
 #include <plearn/ker/ThresholdedKernel.h>
 #include <plearn/ker/VMatKernel.h>
 



From chapados at mail.berlios.de  Wed Apr 11 16:20:17 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Wed, 11 Apr 2007 16:20:17 +0200
Subject: [Plearn-commits] r6878 - in
	trunk/plearn_learners/regressors/test/GaussianProcessRegressor: .
	.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results
Message-ID: <200704111420.l3BEKH4N029319@sheep.berlios.de>

Author: chapados
Date: 2007-04-11 16:20:16 +0200 (Wed, 11 Apr 2007)
New Revision: 6878

Modified:
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/learner_hyperopt.plearn
Log:
New test results for GaussianProcessRegressor

Modified: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2007-04-11 13:58:34 UTC (rev 6877)
+++ trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2007-04-11 14:20:16 UTC (rev 6878)
@@ -3,15 +3,25 @@
 !R 0 
 !R 0 
 !R 1 GaussianProcessRegressor(
-kernel = *1 ->RationalQuadraticARDKernel(
-isp_alpha = 14.8530952696338776 ;
-isp_signal_sigma = 29.6219285856948247 ;
+kernel = *1 ->SummationKernel(
+terms = 2 [ *2 ->RationalQuadraticARDKernel(
+isp_alpha = 14.853095269456114 ;
+isp_signal_sigma = 29.6219285853971783 ;
 isp_global_sigma = 0 ;
-isp_input_sigma = 1 [ 22.2544311481394068 ] ;
-isp_noise_sigma = -1.86446658049698821 ;
+isp_input_sigma = 1 [ 22.2544311476688854 ] ;
 kronecker_indexes = []
 ;
-isp_kronecker_sigma = []
+cache_threshold = 1000000 ;
+is_symmetric = 1 ;
+report_progress = 0 ;
+specify_dataset = *0 ;
+cache_gram_matrix = 1 ;
+data_inputsize = 1 ;
+n_examples = 5  )
+*3 ->IIDNoiseKernel(
+isp_noise_sigma = -1.86446658051810465 ;
+isp_kronecker_sigma = 100 ;
+kronecker_indexes = []
 ;
 cache_threshold = 1000000 ;
 is_symmetric = 1 ;
@@ -20,14 +30,23 @@
 cache_gram_matrix = 1 ;
 data_inputsize = 1 ;
 n_examples = 5  )
+] ;
+input_indexes = []
 ;
+is_symmetric = 1 ;
+report_progress = 0 ;
+specify_dataset = *0 ;
+cache_gram_matrix = 0 ;
+data_inputsize = 1 ;
+n_examples = 5  )
+;
 weight_decay = 0 ;
 include_bias = 1 ;
 compute_confidence = 1 ;
 confidence_epsilon = 1.00000000000000002e-08 ;
-hyperparameters = 3 [ ("isp_signal_sigma" , "0.0" )("isp_noise_sigma" , "0.0" )("isp_alpha" , "0.0" )] ;
-ARD_hyperprefix_initval = ("isp_input_sigma" , "0.0" );
-optimizer = *2 ->ConjGradientOptimizer(
+hyperparameters = 3 [ ("terms[0].isp_signal_sigma" , "0.0" )("terms[0].isp_alpha" , "0.0" )("terms[1].isp_noise_sigma" , "0.0" )] ;
+ARD_hyperprefix_initval = ("terms[0].isp_input_sigma" , "0.0" );
+optimizer = *4 ->ConjGradientOptimizer(
 verbosity = 1 ;
 expected_red = 1 ;
 no_negative_gamma = 1 ;
@@ -43,23 +62,23 @@
 ;
 save_gram_matrix = 0 ;
 alpha = 5  1  [ 
--1.11770047929343908 	
--1.44398600686711642 	
-2.34477475121424828 	
-0.359163702824522091 	
--0.273169390814828383 	
+-1.11770047932178263 	
+-1.44398600688429668 	
+2.34477475126346269 	
+0.359163702821040154 	
+-0.273169390815487967 	
 ]
 ;
 gram_inverse = 5  5  [ 
-2.17384303486338926 	-0.0428570226335798429 	-2.24278913027230908 	0.211187944943547978 	-0.0160008498421014035 	
--0.042857022633580058 	2.63229682421500977 	-2.36514023157865649 	-0.310986674741217162 	0.0223156036853405411 	
--2.24278913027230908 	-2.36514023157865605 	4.57667695971973743 	0.0265284039079978357 	-8.51433618825349326e-05 	
-0.211187944943548006 	-0.310986674741217162 	0.0265284039079978426 	0.113894354038488216 	-0.0105928862093286214 	
--0.0160008498421014035 	0.0223156036853405376 	-8.51433618825353527e-05 	-0.0105928862093286214 	0.034633631441663322 	
+2.17384303487071584 	-0.0428570225967640356 	-2.24278913031203198 	0.211187944939461469 	-0.0160008498413865587 	
+-0.0428570225967640217 	2.63229682422365707 	-2.36514023162738685 	-0.310986674737835034 	0.0223156036844845661 	
+-2.24278913031203198 	-2.3651402316273864 	4.57667695980610389 	0.0265284039100822065 	-8.51433620477555936e-05 	
+0.211187944939461497 	-0.310986674737834978 	0.0265284039100822031 	0.113894354037312281 	-0.0105928862088902134 	
+-0.0160008498413865621 	0.0223156036844845695 	-8.51433620477557969e-05 	-0.0105928862088902134 	0.0346336314419248142 	
 ]
 ;
 target_mean = 1 [ 10 ] ;
-training_inputs = *3 ->MemoryVMatrix(
+training_inputs = *5 ->MemoryVMatrix(
 data = 5  1  [ 
 5 	
 6 	
@@ -93,18 +112,18 @@
 nservers = 0 ;
 save_trainingset_prefix = ""  )
 
-!R 1 1 [ 14.9482509617863855 ] 
-!R 1 1 [ 14.4446958356354838 ] 
+!R 1 1 [ 14.9482509617879042 ] 
+!R 1 1 [ 14.4446958356286146 ] 
 !R 2 4  1  [ 
-13.4992090127011988 	
-14.4141233331847474 	
-14.9482509617863855 	
-14.4446958356354838 	
+13.4992090127011366 	
+14.414123333186323 	
+14.9482509617879042 	
+14.4446958356286146 	
 ]
 1 [ 4  4  [ 
-0.366757393114872932 	0.283483493891058203 	0.064276585896223537 	-0.555619592261717088 	
-0.283483493891065308 	0.247814817330222203 	0.103882973846445736 	-0.369897017889257995 	
-0.0642765858962341952 	0.103882973846449289 	0.141717642757632822 	0.185897571267378936 	
--0.555619592261709982 	-0.36989701788925089 	0.185897571267382489 	2.09437279601928861 	
+0.36675739311799932 	0.283483493893399441 	0.064276585894635474 	-0.555619592274418039 	
+0.283483493893395888 	0.247814817331955928 	0.103882973844278581 	-0.369897017903692671 	
+0.0642765858946248159 	0.103882973844275028 	0.141717642754883022 	0.185897571263485162 	
+-0.555619592274439356 	-0.369897017903710434 	0.185897571263470951 	2.0943727960689591 	
 ]
 ] 

Modified: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/learner_hyperopt.plearn
===================================================================
--- trunk/plearn_learners/regressors/test/GaussianProcessRegressor/learner_hyperopt.plearn	2007-04-11 13:58:34 UTC (rev 6877)
+++ trunk/plearn_learners/regressors/test/GaussianProcessRegressor/learner_hyperopt.plearn	2007-04-11 14:20:16 UTC (rev 6878)
@@ -1,16 +1,13 @@
 GaussianProcessRegressor(
-    kernel                  = RationalQuadraticARDKernel(isp_signal_sigma = 0.0,
-                                                         isp_noise_sigma  = 0.0,
-                                                         isp_alpha        = 0.0,
-                                                         isp_global_sigma = 0.0,
-                                                         isp_input_sigma  = [ 0.0 ]),
+    kernel                  = SummationKernel(terms = [ RationalQuadraticARDKernel(),
+                                                        IIDNoiseKernel() ]),
     weight_decay            = 0,
     include_bias            = 1,
     compute_confidence      = 1,
-    ARD_hyperprefix_initval = ("isp_input_sigma", 0.0),
-    hyperparameters         = [ ("isp_signal_sigma", 0.0) ,
-                                ("isp_noise_sigma",  0.0) ,
-                                ("isp_alpha",        0.0) ],
+    ARD_hyperprefix_initval = ("terms[0].isp_input_sigma", 0.0),
+    hyperparameters         = [ ("terms[0].isp_signal_sigma", 0.0) ,
+                                ("terms[0].isp_alpha",        0.0) ,
+                                ("terms[1].isp_noise_sigma",  0.0) ],
     optimizer               = ConjGradientOptimizer(nstages     = 1,
                                                     sigma       = 0.1,
                                                     rho         = 0.05,



From chapados at mail.berlios.de  Wed Apr 11 17:20:29 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Wed, 11 Apr 2007 17:20:29 +0200
Subject: [Plearn-commits] r6879 - trunk/plearn/ker
Message-ID: <200704111520.l3BFKTlx001317@sheep.berlios.de>

Author: chapados
Date: 2007-04-11 17:20:29 +0200 (Wed, 11 Apr 2007)
New Revision: 6879

Modified:
   trunk/plearn/ker/SummationKernel.cc
Log:
Commented out debug-code to print derivative results obtained by finite differences

Modified: trunk/plearn/ker/SummationKernel.cc
===================================================================
--- trunk/plearn/ker/SummationKernel.cc	2007-04-11 14:20:16 UTC (rev 6878)
+++ trunk/plearn/ker/SummationKernel.cc	2007-04-11 15:20:29 UTC (rev 6879)
@@ -222,13 +222,13 @@
         inherited::computeGramMatrixDerivative(KD, kernel_param, epsilon);
 
     // Compare against finite differences
-    Mat KD1;
-    Kernel::computeGramMatrixDerivative(KD1, kernel_param, epsilon);
-    cerr << "Kernel hyperparameter: " << kernel_param << endl;
-    cerr << "Analytic derivative (200th row):" << endl
-         << KD(200) << endl
-         << "Finite differences:" << endl
-         << KD1(200) << endl;
+    // Mat KD1;
+    // Kernel::computeGramMatrixDerivative(KD1, kernel_param, epsilon);
+    // cerr << "Kernel hyperparameter: " << kernel_param << endl;
+    // cerr << "Analytic derivative (200th row):" << endl
+    //      << KD(200) << endl
+    //      << "Finite differences:" << endl
+    //      << KD1(200) << endl;
 }
 
 



From chapados at mail.berlios.de  Wed Apr 11 18:40:46 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Wed, 11 Apr 2007 18:40:46 +0200
Subject: [Plearn-commits] r6880 -
	trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results
Message-ID: <200704111640.l3BGekdU029220@sheep.berlios.de>

Author: chapados
Date: 2007-04-11 18:40:45 +0200 (Wed, 11 Apr 2007)
New Revision: 6880

Modified:
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
Log:
New test results

Modified: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2007-04-11 15:20:29 UTC (rev 6879)
+++ trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2007-04-11 16:40:45 UTC (rev 6880)
@@ -5,10 +5,10 @@
 !R 1 GaussianProcessRegressor(
 kernel = *1 ->SummationKernel(
 terms = 2 [ *2 ->RationalQuadraticARDKernel(
-isp_alpha = 14.853095269456114 ;
-isp_signal_sigma = 29.6219285853971783 ;
+isp_alpha = 14.8530952695687528 ;
+isp_signal_sigma = 29.6219285855842109 ;
 isp_global_sigma = 0 ;
-isp_input_sigma = 1 [ 22.2544311476688854 ] ;
+isp_input_sigma = 1 [ 22.2544311479547581 ] ;
 kronecker_indexes = []
 ;
 cache_threshold = 1000000 ;
@@ -19,8 +19,8 @@
 data_inputsize = 1 ;
 n_examples = 5  )
 *3 ->IIDNoiseKernel(
-isp_noise_sigma = -1.86446658051810465 ;
-isp_kronecker_sigma = 100 ;
+isp_noise_sigma = -1.86446658049196934 ;
+isp_kronecker_sigma = -100 ;
 kronecker_indexes = []
 ;
 cache_threshold = 1000000 ;
@@ -62,19 +62,19 @@
 ;
 save_gram_matrix = 0 ;
 alpha = 5  1  [ 
--1.11770047932178263 	
--1.44398600688429668 	
-2.34477475126346269 	
-0.359163702821040154 	
--0.273169390815487967 	
+-1.11770047929104055 	
+-1.44398600685698719 	
+2.34477475120374246 	
+0.35916370282251725 	
+-0.273169390814979818 	
 ]
 ;
 gram_inverse = 5  5  [ 
-2.17384303487071584 	-0.0428570225967640356 	-2.24278913031203198 	0.211187944939461469 	-0.0160008498413865587 	
--0.0428570225967640217 	2.63229682422365707 	-2.36514023162738685 	-0.310986674737835034 	0.0223156036844845661 	
--2.24278913031203198 	-2.3651402316273864 	4.57667695980610389 	0.0265284039100822065 	-8.51433620477555936e-05 	
-0.211187944939461497 	-0.310986674737834978 	0.0265284039100822031 	0.113894354037312281 	-0.0105928862088902134 	
--0.0160008498413865621 	0.0223156036844845695 	-8.51433620477557969e-05 	-0.0105928862088902134 	0.0346336314419248142 	
+2.17384303484850605 	-0.0428570226279049171 	-2.24278913026098703 	0.211187944941121059 	-0.0160008498417494489 	
+-0.0428570226279050698 	2.63229682419555111 	-2.36514023156742859 	-0.310986674738180424 	0.0223156036848867825 	
+-2.24278913026098703 	-2.36514023156742859 	4.57667695969712351 	0.0265284039081269407 	-8.51433619161135006e-05 	
+0.211187944941121086 	-0.31098667473818048 	0.0265284039081269442 	0.113894354037727866 	-0.0105928862091330695 	
+-0.0160008498417494489 	0.0223156036848867825 	-8.51433619161134599e-05 	-0.0105928862091330678 	0.0346336314417550195 	
 ]
 ;
 target_mean = 1 [ 10 ] ;
@@ -112,18 +112,18 @@
 nservers = 0 ;
 save_trainingset_prefix = ""  )
 
-!R 1 1 [ 14.9482509617879042 ] 
-!R 1 1 [ 14.4446958356286146 ] 
+!R 1 1 [ 14.9482509617864245 ] 
+!R 1 1 [ 14.4446958356311814 ] 
 !R 2 4  1  [ 
-13.4992090127011366 	
-14.414123333186323 	
-14.9482509617879042 	
-14.4446958356286146 	
+13.4992090127012432 	
+14.4141233331850245 	
+14.9482509617864245 	
+14.4446958356311814 	
 ]
 1 [ 4  4  [ 
-0.36675739311799932 	0.283483493893399441 	0.064276585894635474 	-0.555619592274418039 	
-0.283483493893395888 	0.247814817331955928 	0.103882973844278581 	-0.369897017903692671 	
-0.0642765858946248159 	0.103882973844275028 	0.141717642754883022 	0.185897571263485162 	
--0.555619592274439356 	-0.369897017903710434 	0.185897571263470951 	2.0943727960689591 	
+0.366757393118727626 	0.283483493894024718 	0.0642765858965077541 	-0.555619592268588036 	
+0.283483493894031824 	0.24781481733290095 	0.103882973846950222 	-0.369897017895095104 	
+0.064276585896521965 	0.103882973846953774 	0.141717642758300733 	0.185897571267990003 	
+-0.555619592268573825 	-0.369897017895084446 	0.185897571267993555 	2.09437279604544013 	
 ]
 ] 



From saintmlx at mail.berlios.de  Wed Apr 11 20:06:02 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Wed, 11 Apr 2007 20:06:02 +0200
Subject: [Plearn-commits] r6881 - in trunk: plearn/vmat
	plearn_learners/testers
Message-ID: <200704111806.l3BI62V8024950@sheep.berlios.de>

Author: saintmlx
Date: 2007-04-11 20:06:01 +0200 (Wed, 11 Apr 2007)
New Revision: 6881

Modified:
   trunk/plearn/vmat/Splitter.cc
   trunk/plearn/vmat/Splitter.h
   trunk/plearn_learners/testers/PTester.cc
Log:
- mod. so that splitter->setDataSet can be done in PTester::perform instead of PTester::perform1Split



Modified: trunk/plearn/vmat/Splitter.cc
===================================================================
--- trunk/plearn/vmat/Splitter.cc	2007-04-11 16:40:45 UTC (rev 6880)
+++ trunk/plearn/vmat/Splitter.cc	2007-04-11 18:06:01 UTC (rev 6881)
@@ -60,6 +60,16 @@
     "A splitter is an essential part of a PTester.\n"
     );
 
+void Splitter::declareOptions(OptionList& ol)
+{
+    declareOption(ol,"dataset", &Splitter::dataset,
+                  OptionBase::buildoption | OptionBase::nosave 
+                  | OptionBase::remotetransmit,
+                  "Dataset to split.");
+  
+    inherited::declareOptions(ol);
+}
+
 void Splitter::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     deepCopyField(dataset, copies);

Modified: trunk/plearn/vmat/Splitter.h
===================================================================
--- trunk/plearn/vmat/Splitter.h	2007-04-11 16:40:45 UTC (rev 6880)
+++ trunk/plearn/vmat/Splitter.h	2007-04-11 18:06:01 UTC (rev 6881)
@@ -97,6 +97,9 @@
     //! Transforms a shallow copy into a deep copy
     virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
+protected:    
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
 };
 
 // Declares a few other classes and functions related to this class

Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2007-04-11 16:40:45 UTC (rev 6880)
+++ trunk/plearn_learners/testers/PTester.cc	2007-04-11 18:06:01 UTC (rev 6881)
@@ -760,8 +760,6 @@
 
     const int nstats = statnames_processed.length();
 
-    splitter->setDataSet(dataset);
-
     TVec<string> testcostnames = learner->getTestCostNames();
     TVec<string> traincostnames = learner->getTrainCostNames();
 
@@ -992,6 +990,8 @@
             PLearn::save(expdir / "tester.psave", *this);
     }
 
+    splitter->setDataSet(dataset);
+
     const int nsplits = splitter->nsplits();
     if (nsplits > 1)
         call_forget = true;



From chapados at mail.berlios.de  Thu Apr 12 20:59:02 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Thu, 12 Apr 2007 20:59:02 +0200
Subject: [Plearn-commits] r6882 - trunk/plearn/ker
Message-ID: <200704121859.l3CIx2jv025176@sheep.berlios.de>

Author: chapados
Date: 2007-04-12 20:59:02 +0200 (Thu, 12 Apr 2007)
New Revision: 6882

Modified:
   trunk/plearn/ker/IIDNoiseKernel.cc
   trunk/plearn/ker/IIDNoiseKernel.h
   trunk/plearn/ker/SummationKernel.cc
   trunk/plearn/ker/SummationKernel.h
Log:
Final semantic change to IID noise interpretation: ensure that covariance between a train and test point is always zero, but a test point with itself gets some noise

Modified: trunk/plearn/ker/IIDNoiseKernel.cc
===================================================================
--- trunk/plearn/ker/IIDNoiseKernel.cc	2007-04-11 18:06:01 UTC (rev 6881)
+++ trunk/plearn/ker/IIDNoiseKernel.cc	2007-04-12 18:59:02 UTC (rev 6882)
@@ -122,10 +122,25 @@
 
 real IIDNoiseKernel::evaluate(const Vec& x1, const Vec& x2) const
 {
-    return softplus(m_isp_kronecker_sigma) * inherited::evaluate(x1,x2);
+    // Assume that if x1 and x2 are identical, they are actually the same
+    // instance of a data point.  This should not be called to compare a train
+    // point against a test point (use evaluate_i_x for this purpose).
+    return (x1 == x2? softplus(m_isp_noise_sigma) : 0.0) +
+        softplus(m_isp_kronecker_sigma) * inherited::evaluate(x1,x2);
 }
 
 
+//#####  evaluate_i_x  ########################################################
+
+real IIDNoiseKernel::evaluate_i_x(int i, const Vec& x, real) const
+{
+    // Noise component is ZERO between a test and any train example
+    Vec* train_row = dataRow(i);
+    PLASSERT( train_row );
+    return softplus(m_isp_kronecker_sigma) * inherited::evaluate(*train_row, x);
+}
+
+
 //#####  computeGramMatrix  ###################################################
 
 void IIDNoiseKernel::computeGramMatrix(Mat K) const

Modified: trunk/plearn/ker/IIDNoiseKernel.h
===================================================================
--- trunk/plearn/ker/IIDNoiseKernel.h	2007-04-11 18:06:01 UTC (rev 6881)
+++ trunk/plearn/ker/IIDNoiseKernel.h	2007-04-12 18:59:02 UTC (rev 6882)
@@ -98,9 +98,12 @@
 
     //#####  Kernel Member Functions  #########################################
 
-    //! Compute K(x1,x2).
+    //! Compute K(x1,x2).  This DOES include noise if x1 == x2.
     virtual real evaluate(const Vec& x1, const Vec& x2) const;
 
+    //! Always zero by independence
+    virtual real evaluate_i_x(int i, const Vec& x, real) const;
+    
     //! Compute the Gram Matrix.  Note that this version DOES NOT CACHE
     //! the results, since it is usually called by derived classes.
     virtual void computeGramMatrix(Mat K) const;

Modified: trunk/plearn/ker/SummationKernel.cc
===================================================================
--- trunk/plearn/ker/SummationKernel.cc	2007-04-11 18:06:01 UTC (rev 6881)
+++ trunk/plearn/ker/SummationKernel.cc	2007-04-12 18:59:02 UTC (rev 6882)
@@ -177,6 +177,24 @@
 }
 
 
+//#####  evaluate_i_x  ########################################################
+
+real SummationKernel::evaluate_i_x(int j, const Vec& x, real) const
+{
+    real kernel_value = 0.0;
+    bool split_inputs = m_input_indexes.size() > 0;
+    for (int i=0, n=m_terms.size() ; i<n ; ++i) {
+        if (split_inputs && m_input_indexes[i].size() > 0) {
+            selectElements(x, m_input_indexes[i], m_input_buf1[i]);
+            kernel_value += m_terms[i]->evaluate_i_x(j, m_input_buf1[i]);
+        }
+        else
+            kernel_value += m_terms[i]->evaluate_i_x(j, x);
+    }
+    return kernel_value;
+}
+
+
 //#####  computeGramMatrix  ###################################################
 
 void SummationKernel::computeGramMatrix(Mat K) const

Modified: trunk/plearn/ker/SummationKernel.h
===================================================================
--- trunk/plearn/ker/SummationKernel.h	2007-04-11 18:06:01 UTC (rev 6881)
+++ trunk/plearn/ker/SummationKernel.h	2007-04-12 18:59:02 UTC (rev 6882)
@@ -93,6 +93,9 @@
     //! Compute K(x1,x2).
     virtual real evaluate(const Vec& x1, const Vec& x2) const;
 
+    //! Evaluate a test example x against a train example given by its index
+    virtual real evaluate_i_x(int i, const Vec& x, real) const;
+    
     //! Compute the Gram Matrix by calling subkernels computeGramMatrix
     virtual void computeGramMatrix(Mat K) const;
     



From chapados at mail.berlios.de  Thu Apr 12 21:01:56 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Thu, 12 Apr 2007 21:01:56 +0200
Subject: [Plearn-commits] r6883 - trunk/plearn_learners/regressors
Message-ID: <200704121901.l3CJ1uVo025612@sheep.berlios.de>

Author: chapados
Date: 2007-04-12 21:01:55 +0200 (Thu, 12 Apr 2007)
New Revision: 6883

Modified:
   trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
   trunk/plearn_learners/regressors/GaussianProcessRegressor.h
Log:
Changed the type of the train_inputs member from VMat to Mat for ease of interfacing with programmatic getOption/setOption

Modified: trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2007-04-12 18:59:02 UTC (rev 6882)
+++ trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2007-04-12 19:01:55 UTC (rev 6883)
@@ -2,7 +2,7 @@
 
 // GaussianProcessRegressor.cc
 //
-// Copyright (C) 2006 Nicolas Chapados 
+// Copyright (C) 2006-2007 Nicolas Chapados 
 // 
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
@@ -232,7 +232,7 @@
                 m_kernel->classname().c_str());
     
     // If we are reloading the model, set the training inputs into the kernel
-    if (m_training_inputs)
+    if (m_training_inputs.size() > 0)
         m_kernel->setDataForKernelMatrix(m_training_inputs);
 
     // If we specified hyperparameters without an optimizer, complain.
@@ -380,7 +380,7 @@
 
 void GaussianProcessRegressor::computeOutput(const Vec& input, Vec& output) const
 {
-    PLASSERT( m_kernel && m_alpha.isNotNull() && m_training_inputs );
+    PLASSERT( m_kernel && m_alpha.isNotNull() && m_training_inputs.size() > 0 );
     PLASSERT( m_alpha.width()  == output.size() );
     PLASSERT( m_alpha.length() == m_training_inputs.length() );
     PLASSERT( input.size()     == m_training_inputs.width()  );
@@ -393,7 +393,7 @@
 void GaussianProcessRegressor::computeOutputAux(
     const Vec& input, Vec& output, Vec& kernel_evaluations) const
 {
-    m_kernel->evaluate_all_x_i(input, kernel_evaluations);
+    m_kernel->evaluate_all_i_x(input, kernel_evaluations);
 
     // Finally compute k(x,x_i) * (M + \lambda I)^-1 y
     product(Mat(1, output.size(), output),
@@ -479,7 +479,7 @@
 void GaussianProcessRegressor::computeOutputCovMat(
     const Mat& inputs, Mat& outputs, TVec<Mat>& covariance_matrices) const
 {
-    PLASSERT( m_kernel && m_alpha.isNotNull() && m_training_inputs );
+    PLASSERT( m_kernel && m_alpha.isNotNull() && m_training_inputs.size() > 0 );
     PLASSERT( m_alpha.width()  == outputsize() );
     PLASSERT( m_alpha.length() == m_training_inputs.length() );
     PLASSERT( inputs.width()   == m_training_inputs.width()  );

Modified: trunk/plearn_learners/regressors/GaussianProcessRegressor.h
===================================================================
--- trunk/plearn_learners/regressors/GaussianProcessRegressor.h	2007-04-12 18:59:02 UTC (rev 6882)
+++ trunk/plearn_learners/regressors/GaussianProcessRegressor.h	2007-04-12 19:01:55 UTC (rev 6883)
@@ -285,7 +285,7 @@
     
     /// Saved version of the training set inputs, which must be kept along for
     /// carrying out kernel evaluations with the test point
-    VMat m_training_inputs;
+    Mat m_training_inputs;
 
     /// Buffer for kernel evaluations at test time
     mutable Vec m_kernel_evaluations;



From dorionc at mail.berlios.de  Thu Apr 12 22:10:38 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Thu, 12 Apr 2007 22:10:38 +0200
Subject: [Plearn-commits] r6884 - in trunk/python_modules/plearn: math
	parallel report
Message-ID: <200704122010.l3CKAcVX031551@sheep.berlios.de>

Author: dorionc
Date: 2007-04-12 22:10:37 +0200 (Thu, 12 Apr 2007)
New Revision: 6884

Modified:
   trunk/python_modules/plearn/math/arrays.py
   trunk/python_modules/plearn/parallel/dispatch.py
   trunk/python_modules/plearn/report/formatter.py
Log:
Minor fixes

Modified: trunk/python_modules/plearn/math/arrays.py
===================================================================
--- trunk/python_modules/plearn/math/arrays.py	2007-04-12 19:01:55 UTC (rev 6883)
+++ trunk/python_modules/plearn/math/arrays.py	2007-04-12 20:10:37 UTC (rev 6884)
@@ -193,6 +193,8 @@
 
 def hasNaN(f):
     f = ravel(f)
+    if len(f)==0:
+        return False
     f = choose(isNaN(f), (0, 1))
     return sum(f)
     

Modified: trunk/python_modules/plearn/parallel/dispatch.py
===================================================================
--- trunk/python_modules/plearn/parallel/dispatch.py	2007-04-12 19:01:55 UTC (rev 6883)
+++ trunk/python_modules/plearn/parallel/dispatch.py	2007-04-12 20:10:37 UTC (rev 6884)
@@ -1,4 +1,4 @@
-import inspect, logging, operator, os, select, signal, sys, time
+import fcntl, inspect, logging, operator, os, select, signal, sys, time
 
 from popen2 import Popen4
 from datetime import datetime, timedelta

Modified: trunk/python_modules/plearn/report/formatter.py
===================================================================
--- trunk/python_modules/plearn/report/formatter.py	2007-04-12 19:01:55 UTC (rev 6883)
+++ trunk/python_modules/plearn/report/formatter.py	2007-04-12 20:10:37 UTC (rev 6884)
@@ -125,13 +125,18 @@
         lwriter(r"\end{landscape}")
 
 def latexTableLine(line, writer=DEFAULT_WRITER):
+    endl = r"\\"
     handling_multicol = [] # For \multicolumn...
     for elem in line: 
         if elem is None:
-            assert handling_multicol and handling_multicol[-1].find("multicol") != -1
+            assert handling_multicol \
+                and ( handling_multicol[-1].find("multicol") != -1
+                      or handling_multicol[-1].find("hline") != -1 )
+        elif elem == "NOENDL":
+            endl = ""
         else:
-            handling_multicol.append(elem)
-    writer('&'.join(handling_multicol) + r"\\" + "\n")
+            handling_multicol.append(elem)            
+    writer('&'.join(handling_multicol) + endl + "\n")
 
 def vpaddingLine(vpadding, length):
     vpad = r"\raisebox{%.3fcm}{\rule{0pt}{%.3fcm}}"%(-0.5*vpadding, vpadding)
@@ -267,4 +272,3 @@
     assert os.path.exists(pdf_name), "PDF could not be created!"
     os.system("pdflatex %s >& /dev/null"%file_name)
     os.system("pdflatex %s >& /dev/null"%file_name)
-    os.system("pdflatex %s >& /dev/null"%file_name)



From chapados at mail.berlios.de  Thu Apr 12 22:47:35 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Thu, 12 Apr 2007 22:47:35 +0200
Subject: [Plearn-commits] r6885 -
	trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results
Message-ID: <200704122047.l3CKlZNb001439@sheep.berlios.de>

Author: chapados
Date: 2007-04-12 22:47:35 +0200 (Thu, 12 Apr 2007)
New Revision: 6885

Modified:
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
Log:
New test results to reflect changes in VMat/Mat and addition of IID noise on test diagonal covariance

Modified: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2007-04-12 20:10:37 UTC (rev 6884)
+++ trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2007-04-12 20:47:35 UTC (rev 6885)
@@ -78,8 +78,7 @@
 ]
 ;
 target_mean = 1 [ 10 ] ;
-training_inputs = *5 ->MemoryVMatrix(
-data = 5  1  [ 
+training_inputs = 5  1  [ 
 5 	
 6 	
 5.5 	
@@ -87,18 +86,6 @@
 20 	
 ]
 ;
-source = *0 ;
-fieldnames = []
-;
-writable = 0 ;
-length = 5 ;
-width = 1 ;
-inputsize = 1 ;
-targetsize = 0 ;
-weightsize = 0 ;
-extrasize = 0 ;
-metadatadir = ""  )
-;
 seed = 1827 ;
 stage = 10 ;
 n_examples = 5 ;
@@ -121,9 +108,9 @@
 14.4446958356311814 	
 ]
 1 [ 4  4  [ 
-0.366757393118727626 	0.283483493894024718 	0.0642765858965077541 	-0.555619592268588036 	
-0.283483493894031824 	0.24781481733290095 	0.103882973846950222 	-0.369897017895095104 	
-0.064276585896521965 	0.103882973846953774 	0.141717642758300733 	0.185897571267990003 	
--0.555619592268573825 	-0.369897017895084446 	0.185897571267993555 	2.09437279604544013 	
+0.510839430938520356 	0.283483493894024718 	0.0642765858965077541 	-0.555619592268588036 	
+0.283483493894031824 	0.391896855152693624 	0.103882973846950222 	-0.369897017895095104 	
+0.064276585896521965 	0.103882973846953774 	0.285799680578093407 	0.185897571267990003 	
+-0.555619592268573825 	-0.369897017895084446 	0.185897571267993555 	2.23845483386523281 	
 ]
 ] 



From tihocan at mail.berlios.de  Fri Apr 13 15:56:56 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 13 Apr 2007 15:56:56 +0200
Subject: [Plearn-commits] r6886 - trunk/commands
Message-ID: <200704131356.l3DDuuuB009980@sheep.berlios.de>

Author: tihocan
Date: 2007-04-13 15:56:53 +0200 (Fri, 13 Apr 2007)
New Revision: 6886

Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
Including AdaBoost

Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2007-04-12 20:47:35 UTC (rev 6885)
+++ trunk/commands/plearn_noblas_inc.h	2007-04-13 13:56:53 UTC (rev 6886)
@@ -163,6 +163,7 @@
 #include <plearn_learners/hyper/HyperLearner.h>
 
 // Meta
+#include <plearn_learners/meta/AdaBoost.h>
 #include <plearn_learners/meta/BaggingLearner.h>
 
 // Regressors



From tihocan at mail.berlios.de  Fri Apr 13 15:57:14 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 13 Apr 2007 15:57:14 +0200
Subject: [Plearn-commits] r6887 - trunk/plearn_learners/meta
Message-ID: <200704131357.l3DDvEmF010045@sheep.berlios.de>

Author: tihocan
Date: 2007-04-13 15:57:13 +0200 (Fri, 13 Apr 2007)
New Revision: 6887

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
Fixed typos in help

Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2007-04-13 13:56:53 UTC (rev 6886)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2007-04-13 13:57:13 UTC (rev 6887)
@@ -153,7 +153,7 @@
     declareOption(ol, "weight_by_resampling", &AdaBoost::weight_by_resampling,
                   OptionBase::buildoption,
                   "Whether to train the weak learner using resampling"
-                  "to represent the weighting\n"
+                  " to represent the weighting\n"
                   "given to examples. If false then give these weights "
                   "explicitly in the training set\n"
                   "of the weak learner (note that some learners can accomodate "
@@ -163,7 +163,7 @@
                   OptionBase::buildoption,
                   "To interpret the output of the learner as a class, it is "
                   "compared to this\n"
-                  "threshold: class 1 if greather than output_threshold, class "
+                  "threshold: class 1 if greater than output_threshold, class "
                   "0 otherwise.\n");
 
     declareOption(ol, "provide_learner_expdir", &AdaBoost::provide_learner_expdir,



From chapados at mail.berlios.de  Sat Apr 14 04:39:28 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Sat, 14 Apr 2007 04:39:28 +0200
Subject: [Plearn-commits] r6888 - trunk/plearn/ker
Message-ID: <200704140239.l3E2dSeq009451@sheep.berlios.de>

Author: chapados
Date: 2007-04-14 04:39:27 +0200 (Sat, 14 Apr 2007)
New Revision: 6888

Modified:
   trunk/plearn/ker/ARDBaseKernel.h
   trunk/plearn/ker/KroneckerBaseKernel.cc
   trunk/plearn/ker/KroneckerBaseKernel.h
   trunk/plearn/ker/RationalQuadraticARDKernel.cc
   trunk/plearn/ker/RationalQuadraticARDKernel.h
   trunk/plearn/ker/SquaredExponentialARDKernel.cc
Log:
Improved robustness in case optimization tries to explore VERY SMALL values in the inverse softplus domain

Modified: trunk/plearn/ker/ARDBaseKernel.h
===================================================================
--- trunk/plearn/ker/ARDBaseKernel.h	2007-04-13 13:57:13 UTC (rev 6887)
+++ trunk/plearn/ker/ARDBaseKernel.h	2007-04-14 02:39:27 UTC (rev 6888)
@@ -77,13 +77,13 @@
      *  fixed value (and not be varied during the optimization).  Default
      *  value=0.0.
      */
-    real m_isp_global_sigma;
+    mutable real m_isp_global_sigma;
 
     /**
      *  If specified, contain input-specific length-scales that can be
      *  individually optimized for (these are the ARD hyperparameters).
      */
-    Vec m_isp_input_sigma;
+    mutable Vec m_isp_input_sigma;
 
 public:
     //#####  Public Member Functions  #########################################

Modified: trunk/plearn/ker/KroneckerBaseKernel.cc
===================================================================
--- trunk/plearn/ker/KroneckerBaseKernel.cc	2007-04-13 13:57:13 UTC (rev 6887)
+++ trunk/plearn/ker/KroneckerBaseKernel.cc	2007-04-14 02:39:27 UTC (rev 6888)
@@ -177,6 +177,19 @@
 }
 
 
+//#####  softplusFloor  #######################################################
+
+real KroneckerBaseKernel::softplusFloor(real& value, real floor)
+{
+    real sp = softplus(value);
+    if (sp < floor) {
+        value = pl_log(exp(floor)-1);           // inverse soft-plus
+        return floor;
+    }
+    return sp;
+}
+
+
 //#####  makeDeepCopyFromShallowCopy  #########################################
 
 void KroneckerBaseKernel::makeDeepCopyFromShallowCopy(CopiesMap& copies)

Modified: trunk/plearn/ker/KroneckerBaseKernel.h
===================================================================
--- trunk/plearn/ker/KroneckerBaseKernel.h	2007-04-13 13:57:13 UTC (rev 6887)
+++ trunk/plearn/ker/KroneckerBaseKernel.h	2007-04-14 02:39:27 UTC (rev 6888)
@@ -121,6 +121,15 @@
     //! Declares the class options.
     static void declareOptions(OptionList& ol);
 
+    /**
+     *  Utility function for derived classes: return the softplus of its
+     *  argument, but if the softplus would fall below the given floor, then
+     *  return the floor AND MODIFY the original argument to represent the
+     *  inverse softplus of the floor.  This allows preventing some variables
+     *  from getting too small during optimization.
+     */
+    static real softplusFloor(real& value, real floor=1e-6);
+
 private:
     //! This does the actual building.
     void build_();

Modified: trunk/plearn/ker/RationalQuadraticARDKernel.cc
===================================================================
--- trunk/plearn/ker/RationalQuadraticARDKernel.cc	2007-04-13 13:57:13 UTC (rev 6887)
+++ trunk/plearn/ker/RationalQuadraticARDKernel.cc	2007-04-14 02:39:27 UTC (rev 6888)
@@ -187,16 +187,21 @@
     // Compute Kronecker gram matrix.  No need to cache it.
     inherited::computeGramMatrix(K);
 
-    // Precompute some terms
+    // Precompute some terms. Make sure that the alpha and input sigmas don't
+    // get too small
     real sf    = softplus(m_isp_signal_sigma);
-    real alpha = softplus(m_isp_alpha);
+    real alpha = softplusFloor(m_isp_alpha, 1e-6);
     m_input_sigma.resize(dataInputsize());
-    m_input_sigma.fill(m_isp_global_sigma);
-    if (m_isp_input_sigma.size() > 0)
-        m_input_sigma += m_isp_input_sigma;
-    for (int i=0, n=m_input_sigma.size() ; i<n ; ++i)
+    softplusFloor(m_isp_global_sigma, 1e-6);
+    m_input_sigma.fill(m_isp_global_sigma);  // Still in ISP domain
+    for (int i=0, n=m_input_sigma.size() ; i<n ; ++i) {
+        if (m_isp_input_sigma.size() > 0) {
+            softplusFloor(m_isp_input_sigma[i], 1e-6);
+            m_input_sigma[i] += m_isp_input_sigma[i];
+        }
         m_input_sigma[i] = softplus(m_input_sigma[i]);
-
+    }
+    
     // Prepare the cache for the pow terms
     m_pow_minus_alpha_minus_1.resize(K.length(), K.width());
     int   pow_cache_mod = m_pow_minus_alpha_minus_1.mod();

Modified: trunk/plearn/ker/RationalQuadraticARDKernel.h
===================================================================
--- trunk/plearn/ker/RationalQuadraticARDKernel.h	2007-04-13 13:57:13 UTC (rev 6887)
+++ trunk/plearn/ker/RationalQuadraticARDKernel.h	2007-04-14 02:39:27 UTC (rev 6888)
@@ -93,7 +93,7 @@
 
     //! Inverse softplus of the alpha parameter in the rational-quadratic kernel.
     //! Default value=0.0
-    real m_isp_alpha;
+    mutable real m_isp_alpha;
 
 public:
     //#####  Public Member Functions  #########################################

Modified: trunk/plearn/ker/SquaredExponentialARDKernel.cc
===================================================================
--- trunk/plearn/ker/SquaredExponentialARDKernel.cc	2007-04-13 13:57:13 UTC (rev 6887)
+++ trunk/plearn/ker/SquaredExponentialARDKernel.cc	2007-04-14 02:39:27 UTC (rev 6888)
@@ -170,14 +170,19 @@
     // Compute Kronecker gram matrix
     inherited::computeGramMatrix(K);
 
-    // Precompute some terms
+    // Precompute some terms. Make sure that the input sigmas don't get too
+    // small
     real sf    = softplus(m_isp_signal_sigma);
     m_input_sigma.resize(dataInputsize());
-    m_input_sigma.fill(m_isp_global_sigma);
-    if (m_isp_input_sigma.size() > 0)
-        m_input_sigma += m_isp_input_sigma;
-    for (int i=0, n=m_input_sigma.size() ; i<n ; ++i)
+    softplusFloor(m_isp_global_sigma, 1e-6);
+    m_input_sigma.fill(m_isp_global_sigma);  // Still in ISP domain
+    for (int i=0, n=m_input_sigma.size() ; i<n ; ++i) {
+        if (m_isp_input_sigma.size() > 0) {
+            softplusFloor(m_isp_input_sigma[i], 1e-6);
+            m_input_sigma[i] += m_isp_input_sigma[i];
+        }
         m_input_sigma[i] = softplus(m_input_sigma[i]);
+    }
 
     // Compute Gram Matrix
     int  l = data->length();



From nouiz at mail.berlios.de  Sun Apr 15 02:43:23 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Sun, 15 Apr 2007 02:43:23 +0200
Subject: [Plearn-commits] r6889 - trunk/scripts
Message-ID: <200704150043.l3F0hNje011838@sheep.berlios.de>

Author: nouiz
Date: 2007-04-15 02:43:22 +0200 (Sun, 15 Apr 2007)
New Revision: 6889

Added:
   trunk/scripts/multipymake
Log:
Added a file that make multiple compilation of one file with different compilation parameter. See the file header for example


Added: trunk/scripts/multipymake
===================================================================
--- trunk/scripts/multipymake	2007-04-14 02:39:27 UTC (rev 6888)
+++ trunk/scripts/multipymake	2007-04-15 00:43:22 UTC (rev 6889)
@@ -0,0 +1,32 @@
+#!/bin/bash
+#Script that compile a target multiple time with different compilation option
+#To differenciate the different target we create hardlink to the original file
+#Exemple: multipymake plearn -float -double
+#         This will create the hard link: plearn-float.cc and plearn-double.cc
+#         This will execute pymake -float -link plearn-float.cc and
+#                           pymake -double -link plearn-double.cc
+#Exemple2: multipymake plearn "-float -opt" "-double -opt"
+#         This will create the hard link: plearn-float.cc and plearn-double.cc
+#         This will execute pymake -float -opt -link plearn-float.cc and
+#                           pymake -double -opt -link plearn-double.cc
+
+if [ $# -ge 3 ]; then
+    BASEPROG=$1
+    shift
+    EXT=$@
+else
+    echo "Usage: $0 <base_prog> <List of parameter> ..."
+ fi
+ALL=" -link "
+
+for i in "${EXT[@]}";
+  do
+  ln ${BASEPROG}.cc ${BASEPROG}${i}.cc
+done
+
+for i in "${EXT[@]}";
+  do
+  echo -n "Compiling ${BASEPROG}${i}.cc..."
+  pymake $i $ALL ${BASEPROG}${i}.cc &>/dev/null|| ( echo "Build failed for $i"; exit)
+  echo "Ended with status: $?"
+done


Property changes on: trunk/scripts/multipymake
___________________________________________________________________
Name: svn:executable
   + *



From nouiz at mail.berlios.de  Sun Apr 15 04:59:06 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Sun, 15 Apr 2007 04:59:06 +0200
Subject: [Plearn-commits] r6890 - trunk/scripts
Message-ID: <200704150259.l3F2x6Zo018156@sheep.berlios.de>

Author: nouiz
Date: 2007-04-15 04:59:05 +0200 (Sun, 15 Apr 2007)
New Revision: 6890

Modified:
   trunk/scripts/multipymake
Log:
Corrected bug


Modified: trunk/scripts/multipymake
===================================================================
--- trunk/scripts/multipymake	2007-04-15 00:43:22 UTC (rev 6889)
+++ trunk/scripts/multipymake	2007-04-15 02:59:05 UTC (rev 6890)
@@ -6,6 +6,7 @@
 #         This will execute pymake -float -link plearn-float.cc and
 #                           pymake -double -link plearn-double.cc
 #Exemple2: multipymake plearn "-float -opt" "-double -opt"
+#!!Do not work, must remove space from name
 #         This will create the hard link: plearn-float.cc and plearn-double.cc
 #         This will execute pymake -float -opt -link plearn-float.cc and
 #                           pymake -double -opt -link plearn-double.cc
@@ -13,20 +14,21 @@
 if [ $# -ge 3 ]; then
     BASEPROG=$1
     shift
-    EXT=$@
 else
     echo "Usage: $0 <base_prog> <List of parameter> ..."
  fi
 ALL=" -link "
 
-for i in "${EXT[@]}";
+for i in "$@";
   do
-  ln ${BASEPROG}.cc ${BASEPROG}${i}.cc
+  iname=${i//\ /_}
+  ln ${BASEPROG}.cc ${BASEPROG}${iname}.cc
 done
 
-for i in "${EXT[@]}";
+for i in "$@";
   do
+  iname=${i//\ /_}
   echo -n "Compiling ${BASEPROG}${i}.cc..."
-  pymake $i $ALL ${BASEPROG}${i}.cc &>/dev/null|| ( echo "Build failed for $i"; exit)
+  pymake $i $ALL ${BASEPROG}${iname}.cc &>/dev/null|| ( echo "Build failed for $i"; exit)
   echo "Ended with status: $?"
 done



From chapados at mail.berlios.de  Sun Apr 15 23:44:45 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Sun, 15 Apr 2007 23:44:45 +0200
Subject: [Plearn-commits] r6891 - trunk/plearn/ker
Message-ID: <200704152144.l3FLijFf029015@sheep.berlios.de>

Author: chapados
Date: 2007-04-15 23:44:43 +0200 (Sun, 15 Apr 2007)
New Revision: 6891

Added:
   trunk/plearn/ker/NeuralNetworkARDKernel.cc
   trunk/plearn/ker/NeuralNetworkARDKernel.h
Modified:
   trunk/plearn/ker/RationalQuadraticARDKernel.cc
   trunk/plearn/ker/RationalQuadraticARDKernel.h
   trunk/plearn/ker/SquaredExponentialARDKernel.cc
   trunk/plearn/ker/SquaredExponentialARDKernel.h
Log:
Added NeuralNetworkARDKernel for Gaussian processes and fixes to other kernels

Added: trunk/plearn/ker/NeuralNetworkARDKernel.cc
===================================================================
--- trunk/plearn/ker/NeuralNetworkARDKernel.cc	2007-04-15 02:59:05 UTC (rev 6890)
+++ trunk/plearn/ker/NeuralNetworkARDKernel.cc	2007-04-15 21:44:43 UTC (rev 6891)
@@ -0,0 +1,402 @@
+// -*- C++ -*-
+
+// NeuralNetworkARDKernel.cc
+//
+// Copyright (C) 2007 Nicolas Chapados
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Nicolas Chapados
+
+/*! \file NeuralNetworkARDKernel.cc */
+
+
+#include "NeuralNetworkARDKernel.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    NeuralNetworkARDKernel,
+    "Neural network kernel that can be used for Automatic Relevance Determination",
+    "This kernel is designed to be used within a GaussianProcessRegressor.  It\n"
+    "is similar to the \"arcsin\" kernel of C.E. Rasmussen's GPML code (see\n"
+    "http://www.gaussianprocess.org), but can be used with full Automatic\n"
+    "Relevance Determination (ARD).  It takes the form:\n"
+    "\n"
+    "  k(x,y) = sf * asin(2*x*P*y / sqrt((1+2*x*P*x)*(1+2*y*P*y))) * k_kron(x,y)\n"
+    "\n"
+    "where sf is softplus(isp_signal_sigma), P is softplus(isp_global_sigma +\n"
+    "isp_input_sigma[i])^-2 times the unit matrix, where the x and y vectors on\n"
+    "the right-hand-side have an extra bias (1.0) added in front.  (Note that if\n"
+    "ARD is desired, the number of elements provided for isp_input_sigma must be\n"
+    "ONE MORE than the number of inputs, and the first element of the\n"
+    "isp_input_sigma vector corresponds to this bias).  Also note that in\n"
+    "keeping with Rasmussen and Williams, we raise these elements to the -2\n"
+    "power, so these hyperparameters can be interpreted as true length-scales.\n"
+    "The last factor k_kron(x,y) is the result of the KroneckerBaseKernel\n"
+    "evaluation, or 1.0 if there are no Kronecker terms.  Note that since the\n"
+    "Kronecker terms are incorporated multiplicatively, the very presence of the\n"
+    "term associated to this kernel can be gated by the value of some input\n"
+    "variable(s) (that are incorporated within one or more Kronecker terms).\n"
+    "\n"
+    "See SquaredExponentialARDKernel for more information about using this\n"
+    "kernel within a SummationKernel in order to add IID noise to the examples.\n"
+    "\n"
+    "Note that to make its operations more robust when used with unconstrained\n"
+    "optimization of hyperparameters, all hyperparameters of this kernel are\n"
+    "specified in the inverse softplus domain.  See IIDNoiseKernel for more\n"
+    "explanations.\n"
+    );
+
+
+NeuralNetworkARDKernel::NeuralNetworkARDKernel()
+{ }
+
+
+//#####  declareOptions  ######################################################
+
+void NeuralNetworkARDKernel::declareOptions(OptionList& ol)
+{
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+
+//#####  build  ###############################################################
+
+void NeuralNetworkARDKernel::build()
+{
+    // ### Nothing to add here, simply calls build_
+    inherited::build();
+    build_();
+}
+
+
+//#####  build_  ##############################################################
+
+void NeuralNetworkARDKernel::build_()
+{
+    // Ensure that we multiply in Kronecker terms
+    inherited::m_default_value = 1.0;
+}
+
+
+//#####  evaluate  ############################################################
+
+real NeuralNetworkARDKernel::evaluate(const Vec& x1, const Vec& x2) const
+{
+    PLASSERT( x1.size() == x2.size() );
+    PLASSERT( !m_isp_input_sigma.size() || x1.size()+1 == m_isp_input_sigma.size() );
+
+    real gating_term = inherited::evaluate(x1,x2);
+    if (fast_is_equal(gating_term, 0.0) || x1.size() == 0)
+        return 0.0;
+    
+    const real* px1 = x1.data();
+    const real* px2 = x2.data();
+    real sf         = softplus(m_isp_signal_sigma);
+    real dot_x1_x1;
+    real dot_x2_x2;
+    real dot_x1_x2;
+    
+    if (m_isp_input_sigma.size() > 0) {
+        const real* pinpsig = m_isp_input_sigma.data();
+        real sigma = softplus(*pinpsig++);
+        sigma *= sigma;
+        sigma  = 2. / sigma;
+
+        // Handle bias
+        dot_x1_x1 = dot_x2_x2 = dot_x1_x2 = sigma;
+ 
+        for (int i=0, n=x1.size() ; i<n ; ++i, ++px1, ++px2) {
+            sigma  = softplus(*pinpsig++);
+            sigma *= sigma;
+            sigma  = 2. / sigma;
+
+            dot_x1_x2 += *px1 * *px2 * sigma;
+            dot_x1_x1 += *px1 * *px1 * sigma;
+            dot_x2_x2 += *px2 * *px2 * sigma;
+        }
+    }
+    else {
+        real global_sigma = softplus(m_isp_global_sigma);
+        global_sigma *= global_sigma;
+        global_sigma  = 2. / global_sigma;
+
+        // Handle bias for x1 and x2
+        dot_x1_x1 = dot_x2_x2 = dot_x1_x2 = 1;
+        
+        for (int i=0, n=x1.size() ; i<n ; ++i, ++px1, ++px2) {
+            dot_x1_x2 += *px1 * *px2;
+            dot_x1_x1 += *px1 * *px1;
+            dot_x2_x2 += *px2 * *px2;
+        }
+        dot_x1_x2 *= global_sigma;
+        dot_x1_x1 *= global_sigma;
+        dot_x2_x2 *= global_sigma;
+    }
+
+    // Gate by Kronecker term
+    return sf * asin(dot_x1_x2 / sqrt((1 + dot_x1_x1) * (1 + dot_x2_x2))) * gating_term;
+}
+
+
+//#####  computeGramMatrix  ###################################################
+
+#define DUFF_DOTLOOP                            \
+        sigma = *p_inpsigma++;                  \
+        dot_x1_x2 += *x1 * *x2 * sigma;         \
+        dot_x1_x1 += *x1 * *x1 * sigma;         \
+        dot_x2_x2 += *x2 * *x2 * sigma;         \
+        ++x1;                                   \
+        ++x2;
+
+void NeuralNetworkARDKernel::computeGramMatrix(Mat K) const
+{
+    PLASSERT( !m_isp_input_sigma.size() || dataInputsize()+1 == m_isp_input_sigma.size() );
+    PLASSERT( K.size() == 0 || m_data_cache.size() > 0 );  // Ensure data cached OK
+
+    // Compute Kronecker gram matrix
+    inherited::computeGramMatrix(K);
+
+    // Precompute some terms. Make sure that the input sigmas don't get too
+    // small
+    real sf = softplus(m_isp_signal_sigma);
+    m_input_sigma.resize(dataInputsize() + 1);
+    softplusFloor(m_isp_global_sigma, 1e-6);
+    m_input_sigma.fill(m_isp_global_sigma);  // Still in ISP domain
+    for (int i=0, n=m_input_sigma.size() ; i<n ; ++i) {
+        if (m_isp_input_sigma.size() > 0) {
+            softplusFloor(m_isp_input_sigma[i], 1e-6);
+            m_input_sigma[i] += m_isp_input_sigma[i];
+        }
+        m_input_sigma[i]  = softplus(m_input_sigma[i]);
+        m_input_sigma[i] *= m_input_sigma[i];
+        m_input_sigma[i]  = 2. / m_input_sigma[i];
+    }
+
+    // Compute Gram Matrix
+    int  l = data->length();
+    int  m = K.mod();
+    int  n = dataInputsize();
+    int  cache_mod = m_data_cache.mod();
+
+    real *data_start = &m_data_cache(0,0);
+    real *Ki = K[0];                         // Start of current row
+    real *Kij;                               // Current element along row
+    real *input_sigma_data = m_input_sigma.data();
+    real *xi = data_start;
+    
+    for (int i=0 ; i<l ; ++i, xi += cache_mod, Ki+=m)
+    {
+        Kij = Ki;
+        real *xj = data_start;
+
+        for (int j=0; j<=i; ++j, xj += cache_mod) {
+            // Kernel evaluation per se
+            real *x1 = xi;
+            real *x2 = xj;
+            real *p_inpsigma = input_sigma_data;
+            int  k = n;
+
+            // Handle the bias for x1 and x2
+            real sigma     = *p_inpsigma++;
+            real dot_x1_x1 = sigma;
+            real dot_x2_x2 = sigma;
+            real dot_x1_x2 = sigma;
+
+            switch (k % 8) {
+            case 0: do {  DUFF_DOTLOOP
+            case 7:       DUFF_DOTLOOP
+            case 6:       DUFF_DOTLOOP
+            case 5:       DUFF_DOTLOOP
+            case 4:       DUFF_DOTLOOP
+            case 3:       DUFF_DOTLOOP
+            case 2:       DUFF_DOTLOOP
+            case 1:       DUFF_DOTLOOP  } while((k -= 8) > 0);
+            }
+
+            // Multiplicatively update kernel matrix (already pre-filled with
+            // Kronecker terms, or 1.0 if no Kronecker terms, as per build_).
+            real Kij_cur = *Kij * sf * asin(dot_x1_x2 / sqrt((1 + dot_x1_x1) * (1 + dot_x2_x2)));
+            *Kij++ = Kij_cur;
+        }
+    }
+    if (cache_gram_matrix) {
+        gram_matrix.resize(l,l);
+        gram_matrix << K;
+        gram_matrix_is_cached = true;
+    }
+}
+
+
+//#####  computeGramMatrixDerivative  #########################################
+
+void NeuralNetworkARDKernel::computeGramMatrixDerivative(
+    Mat& KD, const string& kernel_param, real epsilon) const
+{
+    static const string ISS("isp_signal_sigma");
+    static const string IGS("isp_global_sigma");
+    static const string IIS("isp_input_sigma[");
+
+    if (kernel_param == ISS) {
+        computeGramMatrixDerivIspSignalSigma(KD);
+    }
+    // else if (kernel_param == IGS) {
+    //     computeGramMatrixDerivNV<
+    //         NeuralNetworkARDKernel,
+    //         &NeuralNetworkARDKernel::derivIspGlobalSigma>(KD, this, -1);
+    // }
+    // else if (string_begins_with(kernel_param, IIS) &&
+    //          kernel_param[kernel_param.size()-1] == ']')
+    // {
+    //     int arg = tolong(kernel_param.substr(
+    //                          IIS.size(), kernel_param.size() - IIS.size() - 1));
+    //     PLASSERT( arg < m_isp_input_sigma.size() );
+    // 
+    //     computeGramMatrixDerivIspInputSigma(KD, arg);
+    // 
+    // }
+    else
+        inherited::computeGramMatrixDerivative(KD, kernel_param, epsilon);
+}
+
+
+//#####  derivIspGlobalSigma  #################################################
+
+real NeuralNetworkARDKernel::derivIspGlobalSigma(int i, int j, int arg, real K) const
+{
+    if (fast_is_equal(K,0.))
+        return 0.;
+
+    // The norm term inside the exponential may be accessed as Log(K/sf)
+    real inner = pl_log(K / softplus(m_isp_signal_sigma));
+    return - K * inner * sigmoid(m_isp_global_sigma) / softplus(m_isp_global_sigma);
+
+    // Note: in the above expression for 'inner' there is the implicit
+    // assumption that the input_sigma[i] are zero, which allows the
+    // sigmoid/softplus term to be factored out of the norm summation.
+}
+
+
+//#####  computeGramMatrixDerivIspSignalSigma  ################################
+
+void NeuralNetworkARDKernel::computeGramMatrixDerivIspSignalSigma(Mat& KD) const
+{
+    int l = data->length();
+    KD.resize(l,l);
+    PLASSERT_MSG(
+        gram_matrix.width() == l && gram_matrix.length() == l,
+        "To compute the derivative with respect to 'isp_signal_sigma', the\n"
+        "Gram matrix must be precomputed and cached in NeuralNetworkARDKernel.");
+    
+    KD << gram_matrix;
+    KD *= sigmoid(m_isp_signal_sigma)/softplus(m_isp_signal_sigma);
+}
+
+
+//#####  computeGramMatrixDerivIspInputSigma  #################################
+
+void NeuralNetworkARDKernel::computeGramMatrixDerivIspInputSigma(Mat& KD,
+                                                                      int arg) const
+{
+    // Precompute some terms
+    real input_sigma_arg = m_input_sigma[arg];
+    real input_sigma_sq  = input_sigma_arg * input_sigma_arg;
+    real input_sigmoid   = sigmoid(m_isp_global_sigma + m_isp_input_sigma[arg]);
+    
+    // Compute Gram Matrix derivative w.r.t. isp_input_sigma[arg]
+    int  l = data->length();
+    PLASSERT_MSG(
+        gram_matrix.width() == l && gram_matrix.length() == l,
+        "To compute the derivative with respect to 'isp_input_sigma[i]', the\n"
+        "Gram matrix must be precomputed and cached in NeuralNetworkARDKernel.");
+
+    // Variables that walk over the data matrix
+    int  cache_mod = m_data_cache.mod();
+    real *data_start = &m_data_cache(0,0);
+    real *xi = data_start+arg;               // Iterator on data rows
+
+    // Variables that walk over the gram cache
+    int   gram_cache_mod = gram_matrix.mod();
+    real *gram_cache_row = gram_matrix.data();
+    real *gram_cache_cur;
+    
+    // Variables that walk over the kernel derivative matrix (KD)
+    KD.resize(l,l);
+    real* KDi = KD.data();                   // Start of row i
+    real* KDij;                              // Current element on row i
+    int   KD_mod = KD.mod();
+
+    // Iterate on rows of derivative matrix
+    for (int i=0 ; i<l ; ++i, xi += cache_mod, KDi += KD_mod,
+             gram_cache_row += gram_cache_mod)
+    {
+        KDij = KDi;
+        real *xj  = data_start+arg;           // Inner iterator on data rows
+        gram_cache_cur = gram_cache_row;
+
+        // Iterate on columns of derivative matrix
+        for (int j=0 ; j <= i
+                 ; ++j, xj += cache_mod, ++gram_cache_cur)
+        {
+            real diff    = *xi - *xj;
+            real sq_diff = diff * diff;
+            real KD_cur  = 0.5 * *gram_cache_cur *
+                           input_sigmoid * sq_diff / input_sigma_sq;
+
+            // Set into derivative matrix
+            *KDij++ = KD_cur;
+        }
+    }
+}
+
+
+//#####  makeDeepCopyFromShallowCopy  #########################################
+
+void NeuralNetworkARDKernel::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/ker/NeuralNetworkARDKernel.h
===================================================================
--- trunk/plearn/ker/NeuralNetworkARDKernel.h	2007-04-15 02:59:05 UTC (rev 6890)
+++ trunk/plearn/ker/NeuralNetworkARDKernel.h	2007-04-15 21:44:43 UTC (rev 6891)
@@ -0,0 +1,157 @@
+// -*- C++ -*-
+
+// NeuralNetworkARDKernel.h
+//
+// Copyright (C) 2007 Nicolas Chapados
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Nicolas Chapados
+
+/*! \file NeuralNetworkARDKernel.h */
+
+
+#ifndef NeuralNetworkARDKernel_INC
+#define NeuralNetworkARDKernel_INC
+
+#include <plearn/ker/ARDBaseKernel.h>
+
+namespace PLearn {
+
+/**
+ *  Neural network kernel that can be used for Automatic Relevance
+ *  Determination
+ *
+ *  This kernel is designed to be used within a GaussianProcessRegressor.  It
+ *  is similar to the "arcsin" kernel of C.E. Rasmussen's GPML code (see
+ *  http://www.gaussianprocess.org), but can be used with full Automatic
+ *  Relevance Determination (ARD).  It takes the form:
+ *
+ *    k(x,y) = sf * asin(2*x*P*y / sqrt((1+2*x*P*x)*(1+2*y*P*y))) * k_kron(x,y)
+ *
+ *  where sf is softplus(isp_signal_sigma), P is softplus(isp_global_sigma +
+ *  isp_input_sigma[i])^-2 times the unit matrix, where the x and y vectors on
+ *  the right-hand-side have an extra bias (1.0) added in front.  (Note that if
+ *  ARD is desired, the number of elements provided for isp_input_sigma must be
+ *  ONE MORE than the number of inputs, and the first element of the
+ *  isp_input_sigma vector corresponds to this bias).  Also note that in
+ *  keeping with Rasmussen and Williams, we raise these elements to the -2
+ *  power, so these hyperparameters can be interpreted as true length-scales.
+ *  The last factor k_kron(x,y) is the result of the KroneckerBaseKernel
+ *  evaluation, or 1.0 if there are no Kronecker terms.  Note that since the
+ *  Kronecker terms are incorporated multiplicatively, the very presence of the
+ *  term associated to this kernel can be gated by the value of some input
+ *  variable(s) (that are incorporated within one or more Kronecker terms).
+ *
+ *  See SquaredExponentialARDKernel for more information about using this
+ *  kernel within a SummationKernel in order to add IID noise to the examples.
+ *
+ *  Note that to make its operations more robust when used with unconstrained
+ *  optimization of hyperparameters, all hyperparameters of this kernel are
+ *  specified in the inverse softplus domain.  See IIDNoiseKernel for more
+ *  explanations.
+ */
+class NeuralNetworkARDKernel : public ARDBaseKernel
+{
+    typedef ARDBaseKernel inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    // (No new options other than those inherited)
+    
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    NeuralNetworkARDKernel();
+
+
+    //#####  Kernel Member Functions  #########################################
+
+    //! Compute K(x1,x2).
+    virtual real evaluate(const Vec& x1, const Vec& x2) const;
+
+    //! Compute the Gram Matrix.
+    virtual void computeGramMatrix(Mat K) const;
+    
+    //! Directly compute the derivative with respect to hyperparameters;
+    //! for now, this mostly maps to finite differences
+    virtual void computeGramMatrixDerivative(Mat& KD, const string& kernel_param,
+                                             real epsilon=1e-6) const;
+    
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(NeuralNetworkARDKernel);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+    //! Derivative function with respect to isp_global_sigma
+    real derivIspGlobalSigma(int i, int j, int arg, real K) const;
+    
+    // Compute derivative w.r.t. isp_signal_sigma for WHOLE MATRIX
+    void computeGramMatrixDerivIspSignalSigma(Mat& KD) const;
+    
+    // Compute derivative w.r.t. isp_input_sigma[arg] for WHOLE MATRIX
+    void computeGramMatrixDerivIspInputSigma(Mat& KD, int arg) const;
+    
+private:
+    //! This does the actual building.
+    void build_();
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(NeuralNetworkARDKernel);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Modified: trunk/plearn/ker/RationalQuadraticARDKernel.cc
===================================================================
--- trunk/plearn/ker/RationalQuadraticARDKernel.cc	2007-04-15 02:59:05 UTC (rev 6890)
+++ trunk/plearn/ker/RationalQuadraticARDKernel.cc	2007-04-15 21:44:43 UTC (rev 6891)
@@ -61,11 +61,6 @@
     "value of some input variable(s) (that are incorporated within one or more\n"
     "Kronecker terms).\n"
     "\n"
-    "The current version of this class DOES NOT PROPERLY SUPPORT having both\n"
-    "isp_global_sigma and isp_input_sigma[i] be non-zero (and simultaneously\n"
-    "optimizing with respect to both classes of hyperparameters).  The contrary\n"
-    "situation will yield inconsistent behavior.\n"
-    "\n"
     "Note that contrarily to previous versions that incorporated IID noise and\n"
     "Kronecker terms ADDITIVELY, this version does not add any noise at all (and\n"
     "as explained above incorporates the Kronecker terms multiplicatively).  For\n"

Modified: trunk/plearn/ker/RationalQuadraticARDKernel.h
===================================================================
--- trunk/plearn/ker/RationalQuadraticARDKernel.h	2007-04-15 02:59:05 UTC (rev 6890)
+++ trunk/plearn/ker/RationalQuadraticARDKernel.h	2007-04-15 21:44:43 UTC (rev 6891)
@@ -64,11 +64,6 @@
  *  value of some input variable(s) (that are incorporated within one or more
  *  Kronecker terms).
  *
- *  The current version of this class DOES NOT PROPERLY SUPPORT having both
- *  isp_global_sigma and isp_input_sigma[i] be non-zero (and simultaneously
- *  optimizing with respect to both classes of hyperparameters).  The contrary
- *  situation will yield inconsistent behavior.
- *
  *  Note that contrarily to previous versions that incorporated IID noise and
  *  Kronecker terms ADDITIVELY, this version does not add any noise at all (and
  *  as explained above incorporates the Kronecker terms multiplicatively).  For

Modified: trunk/plearn/ker/SquaredExponentialARDKernel.cc
===================================================================
--- trunk/plearn/ker/SquaredExponentialARDKernel.cc	2007-04-15 02:59:05 UTC (rev 6890)
+++ trunk/plearn/ker/SquaredExponentialARDKernel.cc	2007-04-15 21:44:43 UTC (rev 6891)
@@ -65,11 +65,6 @@
     "value of some input variable(s) (that are incorporated within one or more\n"
     "Kronecker terms).\n"
     "\n"
-    "The current version of this class DOES NOT ALLOW differentiating the Kernel\n"
-    "matrix with respect to the Kronecker hyperparameters.  These parameters are\n"
-    "redundant due to the presence of the global sf above; they should be set to\n"
-    "1.0 and left untouched by hyperoptimization.\n"
-    "\n"
     "Note that contrarily to previous versions that incorporated IID noise and\n"
     "Kronecker terms ADDITIVELY, this version does not add any noise at all (and\n"
     "as explained above incorporates the Kronecker terms multiplicatively).  For\n"

Modified: trunk/plearn/ker/SquaredExponentialARDKernel.h
===================================================================
--- trunk/plearn/ker/SquaredExponentialARDKernel.h	2007-04-15 02:59:05 UTC (rev 6890)
+++ trunk/plearn/ker/SquaredExponentialARDKernel.h	2007-04-15 21:44:43 UTC (rev 6891)
@@ -68,11 +68,6 @@
  *  value of some input variable(s) (that are incorporated within one or more
  *  Kronecker terms).
  *
- *  The current version of this class DOES NOT ALLOW differentiating the Kernel
- *  matrix with respect to the Kronecker hyperparameters.  These parameters are
- *  redundant due to the presence of the global sf above; they should be set to
- *  1.0 and left untouched by hyperoptimization.
- *
  *  Note that contrarily to previous versions that incorporated IID noise and
  *  Kronecker terms ADDITIVELY, this version does not add any noise at all (and
  *  as explained above incorporates the Kronecker terms multiplicatively).  For



From ducharme at mail.berlios.de  Mon Apr 16 15:19:10 2007
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Mon, 16 Apr 2007 15:19:10 +0200
Subject: [Plearn-commits] r6892 - tags
Message-ID: <200704161319.l3GDJAql017927@sheep.berlios.de>

Author: ducharme
Date: 2007-04-16 15:19:10 +0200 (Mon, 16 Apr 2007)
New Revision: 6892

Added:
   tags/finlearn-20070416/
Log:
Tag pour release finlearn 20070416

Copied: tags/finlearn-20070416 (from rev 6891, trunk)



From tihocan at mail.berlios.de  Mon Apr 16 15:25:25 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 16 Apr 2007 15:25:25 +0200
Subject: [Plearn-commits] r6893 - trunk/plearn/vmat
Message-ID: <200704161325.l3GDPPcp018474@sheep.berlios.de>

Author: tihocan
Date: 2007-04-16 15:25:25 +0200 (Mon, 16 Apr 2007)
New Revision: 6893

Modified:
   trunk/plearn/vmat/Splitter.cc
Log:
Added call to inherited::makeDeepCopyFromShallowCopy for consistency

Modified: trunk/plearn/vmat/Splitter.cc
===================================================================
--- trunk/plearn/vmat/Splitter.cc	2007-04-16 13:19:10 UTC (rev 6892)
+++ trunk/plearn/vmat/Splitter.cc	2007-04-16 13:25:25 UTC (rev 6893)
@@ -72,6 +72,7 @@
 
 void Splitter::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(dataset, copies);
 }
 



From tihocan at mail.berlios.de  Mon Apr 16 15:41:54 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 16 Apr 2007 15:41:54 +0200
Subject: [Plearn-commits] r6894 - trunk/plearn/base
Message-ID: <200704161341.l3GDfsND019859@sheep.berlios.de>

Author: tihocan
Date: 2007-04-16 15:41:54 +0200 (Mon, 16 Apr 2007)
New Revision: 6894

Modified:
   trunk/plearn/base/HelpSystem.cc
Log:
Adding # prefix in front of virtual classes help

Modified: trunk/plearn/base/HelpSystem.cc
===================================================================
--- trunk/plearn/base/HelpSystem.cc	2007-04-16 13:25:25 UTC (rev 6893)
+++ trunk/plearn/base/HelpSystem.cc	2007-04-16 13:41:54 UTC (rev 6894)
@@ -329,18 +329,19 @@
     s+= "################################################################## \n\n";
 
     // Display basic help
-    s+= "## " + entry.one_line_descr + "\n\n";
-    string ml_help = "# " + entry.multi_line_help;
-    search_replace(ml_help, "\n", "\n# ");
-    s+= ml_help + "\n\n";
+    s+= addprefix("# ", entry.one_line_descr) + "\n\n";
+    s+= addprefix("# ", entry.multi_line_help) + "\n\n";
 
     if(entry.constructor) // it's an instantiable class
         obj = (*entry.constructor)();
-    else
-        s+= "Note: " + classname 
+    else {
+        string virtual_help =
+            "Note: " + classname 
             + " is a base-class with pure virtual methods that cannot be instantiated directly.\n" 
             "(default values for build options can only be displayed for instantiable classes, \n"
             " so you'll only see question marks here.)\n\n";
+        s += addprefix("# ", virtual_help);
+    }
       
     s+= "################################################################## \n"
         "##                         Build Options                        ## \n"



From yoshua at mail.berlios.de  Mon Apr 16 22:43:09 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Mon, 16 Apr 2007 22:43:09 +0200
Subject: [Plearn-commits] r6895 - in trunk/plearn_learners/generic: .
	EXPERIMENTAL
Message-ID: <200704162043.l3GKh9Xn002474@sheep.berlios.de>

Author: yoshua
Date: 2007-04-16 22:43:03 +0200 (Mon, 16 Apr 2007)
New Revision: 6895

Added:
   trunk/plearn_learners/generic/NatGradEstimator.cc
   trunk/plearn_learners/generic/NatGradEstimator.h
Removed:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.h
Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
Log:
Moved NatGradEstimator.{h,cc} from plearn_learners/generic/EXPERIMENTAL to plearn_learners/generic
and changed the corresponding include in NatGradNNet


Deleted: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.cc	2007-04-16 13:41:54 UTC (rev 6894)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.cc	2007-04-16 20:43:03 UTC (rev 6895)
@@ -1,299 +0,0 @@
-// -*- C++ -*-
-
-// NatGradEstimator.cc
-//
-// Copyright (C) 2007 yoshua Bengio
-//
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-//
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-//
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-//
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-//
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-//
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-// Authors: yoshua Bengio
-
-/*! \file NatGradEstimator.cc */
-
-
-#include "NatGradEstimator.h"
-#include <plearn/math/TMat_maths.h>
-#include <plearn/math/plapack.h>
-
-namespace PLearn {
-using namespace std;
-
-PLEARN_IMPLEMENT_OBJECT(
-    NatGradEstimator,
-    "Convert a sequence of gradients into covariance-corrected (natural gradient) directions.\n",
-    "The algorithm used for converting a sequence of n-dimensional gradients g_t\n"
-    "into covariance-corrected update directions v_t is the following:\n\n"
-    "operator(int t, Vec g, Vec v): (reads g and writes v)\n"
-    "    i = t%b   /* denoting b = cov_minibatch_size */\n"
-    "    extend X by a (k+i)-th column gamma^{\frac{-i}{2}} g\n"
-    "    extend G by a (k+i)-th column and row, with G_{k+i,.}=X'_{k+1,.} X\n"
-    "      and idem for the symmetric sub-column\n"
-    "    extend vectors r and a by (k+i)-th element, r_{k+i-1}=0, r_{k+i}=gamma^{\frac{-i}{2}}\n"
-    "    Solve linear system (G + gamma^{-k} lambda I) a = r in a\n"
-    "    v = X a (1 - gamma)/(1 - gamma^t)\n"
-    "    if i+1==b\n"
-    "       (V,D) = leading_eigendecomposition(G,k)\n"
-    "       U = gamma^{b/2} X V\n"
-    "\n\n"
-    "See technical report 'A new insight on the natural gradient' for justifications\n"
-    );
-
-NatGradEstimator::NatGradEstimator()
-    /* ### Initialize all fields to their default value */
-    : cov_minibatch_size(10),
-      lambda(1),
-      n_eigen(10),
-      gamma(0.99),
-      n_dim(-1),
-      verbosity(0),
-      renormalize(true),
-      previous_t(-1)
-{
-    build();
-}
-
-// ### Nothing to add here, simply calls build_
-void NatGradEstimator::build()
-{
-    inherited::build();
-    build_();
-}
-
-void NatGradEstimator::makeDeepCopyFromShallowCopy(CopiesMap& copies)
-{
-    inherited::makeDeepCopyFromShallowCopy(copies);
-
-    // ### Call deepCopyField on all "pointer-like" fields
-    // ### that you wish to be deepCopied rather than
-    // ### shallow-copied.
-    // ### ex:
-    deepCopyField(Ut, copies);
-    deepCopyField(D, copies);
-    deepCopyField(Xt, copies);
-    deepCopyField(G, copies);
-    deepCopyField(r, copies);
-    deepCopyField(Vt, copies);
-    deepCopyField(Vkt, copies);
-    deepCopyField(A, copies);
-    deepCopyField(pivots, copies);
-}
-
-void NatGradEstimator::declareOptions(OptionList& ol)
-{
-    // ### Declare all of this object's options here.
-    // ### For the "flags" of each option, you should typically specify
-    // ### one of OptionBase::buildoption, OptionBase::learntoption or
-    // ### OptionBase::tuningoption. If you don't provide one of these three,
-    // ### this option will be ignored when loading values from a script.
-    // ### You can also combine flags, for example with OptionBase::nosave:
-    // ### (OptionBase::buildoption | OptionBase::nosave)
-
-    // ### ex:
-    declareOption(ol, "cov_minibatch_size", &NatGradEstimator::cov_minibatch_size,
-                  OptionBase::buildoption,
-                  "Covariance estimator minibatch size, i.e. number of calls\n"
-                  "to operator() before re-estimating the principal\n"
-                  "eigenvectors/values. Note that each such re-computation will\n"
-                  "cost O(n_eigen * n)");
-    declareOption(ol, "lambda", &NatGradEstimator::lambda,
-                  OptionBase::buildoption,
-                  "Initial variance. The first covariance is assumed to be\n"
-                  "lambda times the identity. Default = 1.\n");
-    declareOption(ol, "n_eigen", &NatGradEstimator::n_eigen,
-                  OptionBase::buildoption,
-                  "Number of principal eigenvectors of the covariance matrix\n"
-                  "that are kept in its approximation.\n");
-    declareOption(ol, "gamma", &NatGradEstimator::gamma,
-                  OptionBase::buildoption,
-                  "Forgetting factor in moving average estimator of covariance. 0<gamma<1.\n");
-    declareOption(ol, "amari_version", &NatGradEstimator::amari_version,
-                  OptionBase::buildoption,
-                  "Instead of our tricks, use the formula Ginv <-- (1+eps) Ginv - eps Ginv g g' Ginv\n"
-                  "to estimate the inverse of the covariance matrix, and multiply it with g at each step.\n");
-    declareOption(ol, "verbosity", &NatGradEstimator::verbosity,
-                  OptionBase::buildoption,
-                  "Verbosity level\n");
-    declareOption(ol, "renormalize", &NatGradEstimator::renormalize,
-                  OptionBase::buildoption,
-                  "Wether to renormalize z wrt scaling that gamma produces\n");
-
-    declareOption(ol, "n_dim", &NatGradEstimator::n_dim,
-                  OptionBase::learntoption,
-                  "Number of dimensions of the gradient vectors\n");
-    declareOption(ol, "Ut", &NatGradEstimator::Ut,
-                  OptionBase::learntoption,
-                  "Estimated scaled principal eigenvectors of the gradients covariance matrix\n"
-                  "(stored in the rows of Ut)\n");
-    declareOption(ol, "G", &NatGradEstimator::G,
-                  OptionBase::learntoption,
-                  "Gram matrix growing during a minibatch\n");
-    declareOption(ol, "previous_t", &NatGradEstimator::previous_t,
-                  OptionBase::learntoption,
-                  "Value of t at previous call of operator()\n");
-    declareOption(ol, "Xt", &NatGradEstimator::Xt,
-                  OptionBase::learntoption,
-                  "contains in its rows the scaled eigenvectors and g's\n"
-                  "seen since the beginning of the minibatch.\n");
-
-    // Now call the parent class' declareOptions
-    inherited::declareOptions(ol);
-}
-
-void NatGradEstimator::build_()
-{
-    init();
-}
-
-void NatGradEstimator::init()
-{
-    if (n_dim>=0)
-    {
-        PLASSERT_MSG(n_dim>0, "NatGradEstimator::init(), n_dim should be > 0");
-        PLASSERT_MSG(gamma<1 && gamma>0, "NatGradEstimator::init(), gamma should be < 1 and >0");
-        Ut.resize(n_eigen,n_dim);
-        Vt.resize(n_eigen+1,n_eigen+cov_minibatch_size);
-        Vkt = Vt.subMatRows(0,n_eigen);
-        D.resize(n_eigen+1);
-        G.resize(n_eigen + cov_minibatch_size, n_eigen + cov_minibatch_size);
-        A.resize(n_eigen + cov_minibatch_size, n_eigen + cov_minibatch_size);
-        G.clear();
-        Xt.resize(n_eigen+cov_minibatch_size, n_dim);
-        Xt.clear();
-        r.resize(n_eigen);
-    }
-}
-void NatGradEstimator::operator()(int t, const Vec& g, Vec v)
-{
-    if (t!=0)
-        PLASSERT_MSG(t==previous_t+1, "NatGradEstimator() should be called sequentially!");
-    if  (n_dim<0) 
-    {
-        PLASSERT_MSG(t==0, "The first call to NatGradEstimator() should be with t=0\n");
-        n_dim = g.length();
-        v.resize(n_dim);
-        init();
-    }
-    int i = t % cov_minibatch_size;
-    int n = n_eigen+i;
-    Xt.resize(n+1,n_dim);
-    Vec newX = Xt(n);
-    real rn = pow(gamma,real(-0.5*(i+1)));
-    multiply(g,rn,newX);
-    G.resize(n+1,n+1);
-    Vec newG=G(n);
-    product(newG,Xt,newX);
-    G.column(n) << newG;
-    r.resize(n+1);
-    r.clear();
-    r[n] = rn;
-    // solve linear system (G + \gamma^{-k} \lambda I) a = r
-    pivots.resize(n);
-    A.resize(n+1,n+1);
-    A << G;
-    real rn2 = rn*rn;
-    real coef = rn2*lambda;
-    for (int i=0;i<=n;i++)
-        A(i,i) += coef;
-    Mat r_row = r.toMat(1,n+1);
-    int status = lapackSolveLinearSystem(A,r_row,pivots);
-    if (status!=0)
-        PLWARNING("NatGradEstimator: lapackSolveLinearSystem returned %d\n:",status);
-    if (verbosity>1 && i%(cov_minibatch_size/3)==0)
-        cout << "solution r = " << r << endl;
-    // solution is in r
-    transposeProduct(v, Xt, r);
-    if (renormalize)
-        v*=(1 - pow(gamma,real(t+1)))/(1 - gamma);
-        //v/=(1 - pow(gamma,real(t+1)))/(1 - gamma);
-/*    {
-        real gnorm = dot(g,g);
-        real vnorm = dot(v,v);
-        g*=sqrt(vnorm/gnorm);
-    }
-*/
-    if (verbosity>0 && i%(cov_minibatch_size/2)==0)
-    {
-        real gnorm = dot(g,g);
-        real vnorm = dot(v,v);
-        real angle = acos(dot(v,g)/sqrt(gnorm*vnorm))*360/(2*3.14159);
-        cout << "angle(g,v)="<<angle<<", norm ratio="<<vnorm/gnorm<<endl;
-    }
-
-    // recompute the eigen-decomposition
-    if (i+1==cov_minibatch_size)
-    {
-        // get eigen-decomposition, with one more eigen-x than necessary to check if coherent with lambda
-        eigenVecOfSymmMat(G,n_eigen+1,D,Vt);
-        
-        // convert eigenvectors Vt of G into eigenvectors U of C
-        product(Ut,Vkt,Xt);
-        Ut *= 1.0/rn;
-        D *= 1.0/rn2;
-        for (int j=0;j<n_eigen;j++) 
-            if (D[j]<1e-10)
-                PLWARNING("NatGradEstimator: very small eigenvalue %d = %g\n",j,D[j]);
-        if (verbosity>0) // verifier Ut U = D/
-        {
-            static Mat Dmat;
-            cout << "eigenvalues = " << D << endl;
-            if (verbosity>2)
-            {
-                Dmat.resize(n_eigen,n_eigen);
-                productTranspose(Dmat,Ut,Ut);
-                for (int j=0;j<n_eigen;j++) 
-                    Dmat(j,j)-=D[j];
-                cout << "norm(U' U - D)/(n_eigen*n_eigen) = " << sumsquare(Dmat.toVec())/n_eigen << endl;
-            }
-        }
-        // prepare for next minibatch
-        Xt.resize(n_eigen,n_dim);
-        Xt << Ut;
-        G.resize(n_eigen,n_eigen);
-        G.clear();
-        for (int j=0;j<n_eigen;j++)
-            G(j,j) = D[j];
-    }
-    previous_t = t;
-}
-
-} // end of namespace PLearn
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:"stroustrup"
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Deleted: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.h	2007-04-16 13:41:54 UTC (rev 6894)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.h	2007-04-16 20:43:03 UTC (rev 6895)
@@ -1,199 +0,0 @@
-// -*- C++ -*-
-
-// NatGradEstimator.h
-//
-// Copyright (C) 2007 yoshua Bengio
-//
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-//
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-//
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-//
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-//
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-//
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-// Authors: yoshua Bengio
-
-/*! \file NatGradEstimator.h */
-
-
-#ifndef NatGradEstimator_INC
-#define NatGradEstimator_INC
-
-#include <plearn/base/Object.h>
-#include <plearn/math/TMat_impl.h>
-
-namespace PLearn {
-
-/**
- * Class used for converting a sequence of n-dimensional gradients g_t
- * into covariance-corrected update directions v_t, approximating
- *     v_t = inv(C_t) g_t,
- * with C_t = gamma C_{t-1} + g_t g_t'.
- * 
- * There is a main method, the operator(), which takes a g_t and fills v_t.
- * The process can be initialized by init(). 
- */
-class NatGradEstimator : public Object
-{
-    typedef Object inherited;
-
-public:
-    //#####  Public Build Options  ############################################
-
-    //! ### declare public option fields (such as build options) here
-
-    //! mini-batch size for covariance eigen-decomposition
-    int cov_minibatch_size;
-
-    //! regularization coefficient of covariance matrix (initial values on diagonal)
-    real lambda;
-
-    //! number of eigenvectors-eigenvalues that is preserved of the covariance matrix
-    int n_eigen;
-
-    //! forgetting factor in moving average estimator of covariance
-    real gamma;
-
-    //! number of input dimensions (size of g_t or v_t)
-    int n_dim;
-
-    //! verbosity level, track improvement, spectrum, etgc.
-    int verbosity;
-
-    bool renormalize;
-
-    //! use the formula Ginv <-- (1+eps) Ginv - eps Ginv g g' Ginv
-    //! to estimate the inverse of the covariance matrix
-    bool amari_version;
-
-public:
-    //#####  Public Member Functions  #########################################
-
-    //! Default constructor
-    // ### Make sure the implementation in the .cc
-    // ### initializes all fields to reasonable default values.
-    NatGradEstimator();
-
-    // Your other public member functions go here
-
-    //! initialize the object to start collecting covariance statistics from fresh
-    void init();
-
-    //! main method of this class: reads from the gradient "g" field
-    //! and writes into the "v" field an estimator of inv(cov) g.
-    //! The argument is an index over examples, which is used to
-    //! know when cycling through a minibatch. The statistics on
-    //! the covariance are updated.
-    void operator()(int t, const Vec& g, Vec v);
-
-    //#####  PLearn::Object Protocol  #########################################
-
-    // Declares other standard object methods.
-    // ### If your class is not instantiatable (it has pure virtual methods)
-    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
-    PLEARN_DECLARE_OBJECT(NatGradEstimator);
-
-    // Simply calls inherited::build() then build_()
-    virtual void build();
-
-    //! Transforms a shallow copy into a deep copy
-    // (PLEASE IMPLEMENT IN .cc)
-    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
-
-protected:
-    //#####  Protected Options  ###############################################
-
-    // ### Declare protected option fields (such as learned parameters) here
-
-    //! k principal eigenvectors of the estimated covariance matrix
-    Mat Ut; // dimension = n_eigen x n_dim
-
-    //! k principal eigenvalues of the estimated covariance matrix
-    Vec D; 
-
-    //! contains in its rows the scaled eigenvectors and g's seen since the beginning of the minibatch
-    Mat Xt;
-
-    //! Gram matrix = X' X = Xt Xt'
-    Mat G; 
-
-    //! previous value of t
-    int previous_t;
-
-protected:
-    //#####  Protected Member Functions  ######################################
-
-    //! Declares the class options.
-    // (PLEASE IMPLEMENT IN .cc)
-    static void declareOptions(OptionList& ol);
-
-private:
-    //#####  Private Member Functions  ########################################
-
-    //! This does the actual building.
-    // (PLEASE IMPLEMENT IN .cc)
-    void build_();
-
-private:
-    //#####  Private Data Members  ############################################
-
-    // The rest of the private stuff goes here
-
-    Vec r; //! rhs of linear system (G + gamma^{-i} lambda I) a = r
-    Mat Vt;
-    Mat Vkt; //! sub-matrix of Vt with first n_eigen eigen-vectors
-    Mat A; // matrix for linear system solving
-    TVec<int> pivots; // pivots for linear system solving
-    /*
-    //! temporary buffer
-    Vec tmp_v;
-    //! Gram matrix, of dimension (k + minibatch_size, k + minibatch_size)
-    //! and its sub-matrices
-    Mat M, M11, M12, M21, M22;
-    //! k+1 eigenvectors of the Gram matrix (in the rows)
-    Mat Vbt; //! sub-matrix of Vt with last cov_minibatch_size elements of each eigen-vector
-    //! temp for new value of Ut
-    Mat newUt;
-    */
-};
-
-// Declares a few other classes and functions related to this class
-DECLARE_OBJECT_PTR(NatGradEstimator);
-
-} // end of namespace PLearn
-
-#endif
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:"stroustrup"
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-04-16 13:41:54 UTC (rev 6894)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-04-16 20:43:03 UTC (rev 6895)
@@ -41,7 +41,7 @@
 #define NatGradNNet_INC
 
 #include <plearn_learners/generic/PLearner.h>
-#include <plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.h>
+#include <plearn_learners/generic/NatGradEstimator.h>
 #include <plearn/sys/Profiler.h>
 
 namespace PLearn {

Copied: trunk/plearn_learners/generic/NatGradEstimator.cc (from rev 6894, trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.cc)

Copied: trunk/plearn_learners/generic/NatGradEstimator.h (from rev 6894, trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.h)



From chapados at mail.berlios.de  Tue Apr 17 14:26:34 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Tue, 17 Apr 2007 14:26:34 +0200
Subject: [Plearn-commits] r6896 - trunk/commands
Message-ID: <200704171226.l3HCQYqP004702@sheep.berlios.de>

Author: chapados
Date: 2007-04-17 14:26:34 +0200 (Tue, 17 Apr 2007)
New Revision: 6896

Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
Added some useful kernels for Gaussian Processes

Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2007-04-16 20:43:03 UTC (rev 6895)
+++ trunk/commands/plearn_noblas_inc.h	2007-04-17 12:26:34 UTC (rev 6896)
@@ -110,6 +110,7 @@
 #include <plearn/ker/GeodesicDistanceKernel.h>
 #include <plearn/ker/IIDNoiseKernel.h>
 #include <plearn/ker/NegOutputCostFunction.h>
+#include <plearn/ker/NeuralNetworkARDKernel.h>
 #include <plearn/ker/PolynomialKernel.h>
 #include <plearn/ker/RationalQuadraticARDKernel.h>
 #include <plearn/ker/SquaredExponentialARDKernel.h>



From saintmlx at mail.berlios.de  Tue Apr 17 17:24:15 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 17 Apr 2007 17:24:15 +0200
Subject: [Plearn-commits] r6897 - trunk/plearn/vmat
Message-ID: <200704171524.l3HFOFUc017323@sheep.berlios.de>

Author: saintmlx
Date: 2007-04-17 17:24:14 +0200 (Tue, 17 Apr 2007)
New Revision: 6897

Modified:
   trunk/plearn/vmat/Splitter.cc
Log:
- Splitter::dataset is now a learntoption



Modified: trunk/plearn/vmat/Splitter.cc
===================================================================
--- trunk/plearn/vmat/Splitter.cc	2007-04-17 12:26:34 UTC (rev 6896)
+++ trunk/plearn/vmat/Splitter.cc	2007-04-17 15:24:14 UTC (rev 6897)
@@ -63,7 +63,7 @@
 void Splitter::declareOptions(OptionList& ol)
 {
     declareOption(ol,"dataset", &Splitter::dataset,
-                  OptionBase::buildoption | OptionBase::nosave 
+                  OptionBase::learntoption | OptionBase::nosave 
                   | OptionBase::remotetransmit,
                   "Dataset to split.");
   



From tihocan at mail.berlios.de  Tue Apr 17 21:08:57 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 17 Apr 2007 21:08:57 +0200
Subject: [Plearn-commits] r6898 - trunk/plearn/base
Message-ID: <200704171908.l3HJ8v21011658@sheep.berlios.de>

Author: tihocan
Date: 2007-04-17 21:08:57 +0200 (Tue, 17 Apr 2007)
New Revision: 6898

Modified:
   trunk/plearn/base/HelpSystem.cc
Log:
Cosmetic change to virtual class help (lines were a bit too long)

Modified: trunk/plearn/base/HelpSystem.cc
===================================================================
--- trunk/plearn/base/HelpSystem.cc	2007-04-17 15:24:14 UTC (rev 6897)
+++ trunk/plearn/base/HelpSystem.cc	2007-04-17 19:08:57 UTC (rev 6898)
@@ -337,9 +337,10 @@
     else {
         string virtual_help =
             "Note: " + classname 
-            + " is a base-class with pure virtual methods that cannot be instantiated directly.\n" 
-            "(default values for build options can only be displayed for instantiable classes, \n"
-            " so you'll only see question marks here.)\n\n";
+            + " is a base-class with pure virtual methods that\n"
+            "cannot be instantiated directly (default values for build options\n"
+            "can only be displayed for instantiable classes, so you will only\n"
+            "see question marks here).\n\n";
         s += addprefix("# ", virtual_help);
     }
       



From tihocan at mail.berlios.de  Tue Apr 17 21:11:49 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 17 Apr 2007 21:11:49 +0200
Subject: [Plearn-commits] r6899 - trunk/plearn_learners/online
Message-ID: <200704171911.l3HJBnaR011951@sheep.berlios.de>

Author: tihocan
Date: 2007-04-17 21:11:49 +0200 (Tue, 17 Apr 2007)
New Revision: 6899

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
Log:
- Proper initialization of 'n_classes' member
- More safety checks and error messages
- Fixed erroneous crash in build when no train set has been obtained yet (thus no targetsize available)
- Added missing deep copy statements


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-04-17 19:08:57 UTC (rev 6898)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-04-17 19:11:49 UTC (rev 6899)
@@ -47,8 +47,8 @@
 
 PLEARN_IMPLEMENT_OBJECT(
     DeepBeliefNet,
-    "Neural net, learned layer-wise in a greedy fashion",
-    "This version support different unit types, different connection types,\n"
+    "Neural network, learned layer-wise in a greedy fashion.",
+    "This version supports different unit types, different connection types,\n"
     "and different cost functions, including the NLL in classification.\n");
 
 DeepBeliefNet::DeepBeliefNet() :
@@ -56,6 +56,7 @@
     grad_learning_rate( 0. ),
     grad_decrease_ct( 0. ),
     // grad_weight_decay( 0. ),
+    n_classes(-1),
     use_classification_cost( true ),
     reconstruct_layerwise( false ),
     n_layers( 0 ),
@@ -90,7 +91,7 @@
 
     declareOption(ol, "n_classes", &DeepBeliefNet::n_classes,
                   OptionBase::buildoption,
-                  "Number of classes in the training set\n"
+                  "Number of classes in the training set:\n"
                   "  - 0 means we are doing regression,\n"
                   "  - 1 means we have two classes, but only one output,\n"
                   "  - 2 means we also have two classes, but two outputs"
@@ -236,7 +237,11 @@
     MODULE_LOG << "build_() called" << endl;
 
     // Initialize some learnt variables
-    n_layers = layers.length();
+    if (layers.isEmpty())
+        PLERROR("In DeepBeliefNet::build_ - You must provide at least one RBM "
+                "layer through the 'layers' option");
+    else
+        n_layers = layers.length();
 
     if( training_schedule.length() != n_layers-1  && !online )
     {
@@ -298,8 +303,11 @@
         expectation_gradients[i].resize( layers[i]->size );
     }
     layers[n_layers-1]->random_gen = random_gen;
-    activation_gradients[n_layers-1].resize( layers[n_layers-1]->size );
-    expectation_gradients[n_layers-1].resize( layers[n_layers-1]->size );
+    int last_layer_size = layers[n_layers-1]->size;
+    PLASSERT_MSG(last_layer_size >= 0,
+                 "Size of last layer must be non-negative");
+    activation_gradients[n_layers-1].resize(last_layer_size);
+    expectation_gradients[n_layers-1].resize(last_layer_size);
 }
 
 void DeepBeliefNet::build_classification_cost()
@@ -317,6 +325,9 @@
     target_layer->random_gen = random_gen;
     target_layer->build();
 
+    PLASSERT_MSG(n_layers >= 2, "You must specify at least two layers (the "
+            "input layer and one hidden layer)");
+
     classification_module = new RBMClassificationModule();
     classification_module->previous_to_last = connections[n_layers-2];
     classification_module->last_layer =
@@ -343,19 +354,22 @@
 {
     MODULE_LOG << "build_final_cost() called" << endl;
 
+    PLASSERT_MSG(final_cost->input_size >= 0, "The input size of the final "
+            "cost must be non-negative");
+
     final_cost_gradient.resize( final_cost->input_size );
     final_cost->setLearningRate( grad_learning_rate );
 
     if( final_module )
     {
         if( layers[n_layers-1]->size != final_module->input_size )
-            PLERROR("DeepBeliefNet::build_final_cost() - \n"
+            PLERROR("DeepBeliefNet::build_final_cost() - "
                     "layers[%i]->size (%d) != final_module->input_size (%d)."
                     "\n", n_layers-1, layers[n_layers-1]->size,
                     final_module->input_size);
 
         if( final_module->output_size != final_cost->input_size )
-            PLERROR("DeepBeliefNet::build_final_cost() - \n"
+            PLERROR("DeepBeliefNet::build_final_cost() - "
                     "final_module->output_size (%d) != final_cost->input_size."
                     "\n", n_layers-1, layers[n_layers-1]->size,
                     final_module->input_size);
@@ -365,7 +379,7 @@
     else
     {
         if( layers[n_layers-1]->size != final_cost->input_size )
-            PLERROR("DeepBeliefNet::build_final_cost() - \n"
+            PLERROR("DeepBeliefNet::build_final_cost() - "
                     "layers[%i]->size (%d) != final_cost->input_size (%d)."
                     "\n", n_layers-1, layers[n_layers-1]->size,
                     final_cost->input_size);
@@ -375,65 +389,75 @@
     if( n_classes == 0 ) // regression
     {
         if( final_cost->input_size != targetsize() )
-            PLERROR("DeepBeliefNet::build_final_cost() - \n"
-                    "final_cost->input_size (%d) != targetsize() (%d),\n"
+            PLERROR("DeepBeliefNet::build_final_cost() - "
+                    "final_cost->input_size (%d) != targetsize() (%d), "
                     "although we are doing regression (n_classes == 0).\n",
                     final_cost->input_size, targetsize());
     }
     else
     {
         if( final_cost->input_size != n_classes )
-            PLERROR("DeepBeliefNet::build_final_cost() - \n"
-                    "final_cost->input_size (%d) != n_classes (%d),\n"
+            PLERROR("DeepBeliefNet::build_final_cost() - "
+                    "final_cost->input_size (%d) != n_classes (%d), "
                     "although we are doing classification (n_classes != 0).\n",
                     final_cost->input_size, n_classes);
 
-        if( targetsize() != 1 )
-            PLERROR("DeepBeliefNet::build_final_cost() - \n"
-                    "targetsize() (%d) != 1,\n"
-                    "although we are doing regression (n_classes == 0).\n",
+        if( targetsize_ >= 0 && targetsize() != 1 )
+            PLERROR("DeepBeliefNet::build_final_cost() - "
+                    "targetsize() (%d) != 1, "
+                    "although we are doing classification (n_classes != 0).\n",
                     targetsize());
     }
 
 }
 
+///////////
+// build //
+///////////
 void DeepBeliefNet::build()
 {
     inherited::build();
     build_();
 }
 
-
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
 void DeepBeliefNet::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
-    // deepCopyField(, copies);
-
-    deepCopyField(training_schedule, copies);
-    deepCopyField(layers, copies);
-    deepCopyField(connections, copies);
-    deepCopyField(final_module, copies);
-    deepCopyField(final_cost, copies);
-    deepCopyField(partial_costs, copies);
-    deepCopyField(classification_module, copies);
-    deepCopyField(timer, copies);
-    deepCopyField(classification_cost, copies);
-    deepCopyField(joint_layer, copies);
-    deepCopyField(activation_gradients, copies);
-    deepCopyField(expectation_gradients, copies);
-    deepCopyField(final_cost_input, copies);
-    deepCopyField(final_cost_value, copies);
-    deepCopyField(final_cost_output, copies);
-    deepCopyField(class_output, copies);
-    deepCopyField(class_gradient, copies);
-    deepCopyField(final_cost_gradient, copies);
-    deepCopyField(pos_down_values, copies);
-    deepCopyField(pos_up_values, copies);
-
+    deepCopyField(training_schedule,        copies);
+    deepCopyField(layers,                   copies);
+    deepCopyField(connections,              copies);
+    deepCopyField(final_module,             copies);
+    deepCopyField(final_cost,               copies);
+    deepCopyField(partial_costs,            copies);
+    deepCopyField(classification_module,    copies);
+    deepCopyField(timer,                    copies);
+    deepCopyField(classification_cost,      copies);
+    deepCopyField(joint_layer,              copies);
+    deepCopyField(activation_gradients,     copies);
+    deepCopyField(expectation_gradients,    copies);
+    deepCopyField(final_cost_input,         copies);
+    deepCopyField(final_cost_value,         copies);
+    deepCopyField(final_cost_output,        copies);
+    deepCopyField(class_output,             copies);
+    deepCopyField(class_gradient,           copies);
+    deepCopyField(class_input_gradient,     copies);
+    deepCopyField(final_cost_gradient,      copies);
+    deepCopyField(save_layer_activation,    copies);
+    deepCopyField(save_layer_expectation,   copies);
+    deepCopyField(pos_down_values,          copies);
+    deepCopyField(pos_up_values,            copies);
+    deepCopyField(final_cost_indices,       copies);
+    deepCopyField(partial_cost_indices,     copies);
 }
 
 
+////////////////
+// outputsize //
+////////////////
 int DeepBeliefNet::outputsize() const
 {
     int out_size = 0;



From tihocan at mail.berlios.de  Tue Apr 17 21:12:47 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 17 Apr 2007 21:12:47 +0200
Subject: [Plearn-commits] r6900 - trunk/plearn_learners/online
Message-ID: <200704171912.l3HJClrZ012046@sheep.berlios.de>

Author: tihocan
Date: 2007-04-17 21:12:46 +0200 (Tue, 17 Apr 2007)
New Revision: 6900

Modified:
   trunk/plearn_learners/online/RBMConnection.h
Log:
More dOxygen help

Modified: trunk/plearn_learners/online/RBMConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMConnection.h	2007-04-17 19:11:49 UTC (rev 6899)
+++ trunk/plearn_learners/online/RBMConnection.h	2007-04-17 19:12:46 UTC (rev 6900)
@@ -52,7 +52,6 @@
 /**
  * Virtual class for the parameters between two layers of an RBM.
  *
- * @todo: yes
  */
 class RBMConnection: public OnlineLearningModule
 {
@@ -76,10 +75,10 @@
 
     //#####  Not Options  #####################################################
 
-    //! Number of units on down layer
+    //! Number of units in down layer.
     int down_size;
 
-    //! Number of units on up layer
+    //! Number of units in up layer.
     int up_size;
 
 public:
@@ -96,10 +95,14 @@
     //! Sets the momentum
     virtual void setMomentum( real the_momentum );
 
-    //! Sets input_vec to input, and going_up to false
+    //! Sets input_vec to input, and going_up to false.
+    //! Note that no data copy is made, so input should not be modified
+    //! afterwards.
     virtual void setAsUpInput( const Vec& input ) const;
 
     //! Sets input_vec to input, and going_up to true
+    //! Note that no data copy is made, so input should not be modified
+    //! afterwards.
     virtual void setAsDownInput( const Vec& input ) const;
 
     //! Accumulates positive phase statistics to *_pos_stats



From tihocan at mail.berlios.de  Tue Apr 17 21:13:37 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 17 Apr 2007 21:13:37 +0200
Subject: [Plearn-commits] r6901 - trunk/plearn_learners/online
Message-ID: <200704171913.l3HJDbIB012147@sheep.berlios.de>

Author: tihocan
Date: 2007-04-17 21:13:36 +0200 (Tue, 17 Apr 2007)
New Revision: 6901

Modified:
   trunk/plearn_learners/online/RBMConnection.cc
Log:
More explicit help, and minor code cosmetic changes.


Modified: trunk/plearn_learners/online/RBMConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMConnection.cc	2007-04-17 19:12:46 UTC (rev 6900)
+++ trunk/plearn_learners/online/RBMConnection.cc	2007-04-17 19:13:36 UTC (rev 6901)
@@ -51,6 +51,9 @@
     "Virtual class for the linear transformation between two layers of an RBM",
     "");
 
+///////////////////
+// RBMConnection //
+///////////////////
 RBMConnection::RBMConnection( real the_learning_rate ) :
     learning_rate(the_learning_rate),
     momentum(0.),
@@ -62,15 +65,18 @@
 {
 }
 
+////////////////////
+// declareOptions //
+////////////////////
 void RBMConnection::declareOptions(OptionList& ol)
 {
     declareOption(ol, "down_size", &RBMConnection::down_size,
                   OptionBase::buildoption,
-                  "The size of the up layer");
+                  "Number of units in down layer.");
 
     declareOption(ol, "up_size", &RBMConnection::up_size,
                   OptionBase::buildoption,
-                  "The size of the up layer");
+                  "Number of units in up layer.");
 
     declareOption(ol, "learning_rate", &RBMConnection::learning_rate,
                   OptionBase::buildoption,
@@ -104,6 +110,9 @@
                     "Equals to up_size");
 }
 
+////////////
+// build_ //
+////////////
 void RBMConnection::build_()
 {
     string im = lowerstring( initialization_method );
@@ -129,6 +138,9 @@
         output_size = up_size;
 }
 
+///////////
+// build //
+///////////
 void RBMConnection::build()
 {
     inherited::build();
@@ -136,6 +148,9 @@
 }
 
 
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
 void RBMConnection::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
@@ -143,16 +158,25 @@
     deepCopyField(input_vec, copies);
 }
 
+/////////////////////
+// setLearningRate //
+/////////////////////
 void RBMConnection::setLearningRate( real the_learning_rate )
 {
     learning_rate = the_learning_rate;
 }
 
+/////////////////
+// setMomentum //
+/////////////////
 void RBMConnection::setMomentum( real the_momentum )
 {
     momentum = the_momentum;
 }
 
+//////////////////
+// setAsUpInput //
+//////////////////
 void RBMConnection::setAsUpInput( const Vec& input ) const
 {
     PLASSERT( input.size() == up_size );
@@ -160,6 +184,9 @@
     going_up = false;
 }
 
+////////////////////
+// setAsDownInput //
+////////////////////
 void RBMConnection::setAsDownInput( const Vec& input ) const
 {
     PLASSERT( input.size() == down_size );
@@ -167,6 +194,9 @@
     going_up = true;
 }
 
+////////////
+// update //
+////////////
 void RBMConnection::update( const Vec& pos_down_values,
                             const Vec& pos_up_values,
                             const Vec& neg_down_values,
@@ -178,7 +208,9 @@
     update();
 }
 
-//! given the input, compute the output (possibly resize it  appropriately)
+///////////
+// fprop //
+///////////
 void RBMConnection::fprop(const Vec& input, Vec& output) const
 {
     // propagates the activations.



From tihocan at mail.berlios.de  Tue Apr 17 21:14:29 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 17 Apr 2007 21:14:29 +0200
Subject: [Plearn-commits] r6902 - trunk/plearn_learners/online
Message-ID: <200704171914.l3HJETaD012196@sheep.berlios.de>

Author: tihocan
Date: 2007-04-17 21:14:29 +0200 (Tue, 17 Apr 2007)
New Revision: 6902

Modified:
   trunk/plearn_learners/online/OnlineLearningModule.h
Log:
Minor edits to comments.


Modified: trunk/plearn_learners/online/OnlineLearningModule.h
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.h	2007-04-17 19:13:36 UTC (rev 6901)
+++ trunk/plearn_learners/online/OnlineLearningModule.h	2007-04-17 19:14:29 UTC (rev 6902)
@@ -59,9 +59,6 @@
  *      information about how the input should also have changed
  *      (i.e. input gradient)
  *
- * @todo write all Object methods, compile, and test somehow
- *
- * @deprecated
  */
 class OnlineLearningModule : public Object
 {
@@ -70,9 +67,6 @@
 public:
     //#####  Public Build Options  ############################################
 
-    //! ### declare public option fields (such as build options) here
-    //! Start your comments with Doxygen-compatible comments such as //!
-
     //! input size
     int input_size;
 
@@ -103,7 +97,7 @@
 
     // Your other public member functions go here
 
-    //! given the input, compute the output (possibly resize it  appropriately)
+    //! given the input, compute the output (possibly resize it appropriately)
     virtual void fprop(const Vec& input, Vec& output) const = 0;
 
     //! Adapt based on the output gradient: this method should only



From tihocan at mail.berlios.de  Tue Apr 17 21:14:59 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 17 Apr 2007 21:14:59 +0200
Subject: [Plearn-commits] r6903 - trunk/plearn_learners/online
Message-ID: <200704171914.l3HJExl0012232@sheep.berlios.de>

Author: tihocan
Date: 2007-04-17 21:14:59 +0200 (Tue, 17 Apr 2007)
New Revision: 6903

Modified:
   trunk/plearn_learners/online/OnlineLearningModule.cc
Log:
Added missing deep copy statements

Modified: trunk/plearn_learners/online/OnlineLearningModule.cc
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.cc	2007-04-17 19:14:29 UTC (rev 6902)
+++ trunk/plearn_learners/online/OnlineLearningModule.cc	2007-04-17 19:14:59 UTC (rev 6903)
@@ -91,6 +91,9 @@
                  tmp_input_diag_hessian, output_diag_hessian);
 }
 
+//////////////////
+// bbpropUpdate //
+//////////////////
 void OnlineLearningModule::bbpropUpdate(const Vec& input, const Vec& output,
                                         Vec& input_gradient,
                                         const Vec& output_gradient,
@@ -104,26 +107,41 @@
             "'bpropUpdate'.\n");
 }
 
+/////////////////////
+// setLearningRate //
+/////////////////////
 void OnlineLearningModule::setLearningRate( real dynamic_learning_rate )
 {
-    PLWARNING("OnlineLearningModule does not have a learning rate that can be\n"
-              "changed from outside.\n"
-              "If your derived class has one, please implement setLearningrate()"
-              " in it.\n");
+    PLWARNING("In OnlineLearningModule::setLearningRate - The derived class "
+            "(%s) does not have a learning rate that can be changed from "
+            "outside. If it should have one, please implement setLearningRate "
+            "in it", classname().c_str());
 }
 
 
+///////////
+// build //
+///////////
 void OnlineLearningModule::build()
 {
     inherited::build();
     build_();
 }
 
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
 void OnlineLearningModule::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
+    deepCopyField(random_gen,             copies);
+    deepCopyField(tmp_input_gradient,     copies);
+    deepCopyField(tmp_input_diag_hessian, copies);
 }
 
+////////////////////
+// declareOptions //
+////////////////////
 void OnlineLearningModule::declareOptions(OptionList& ol)
 {
     declareOption(ol, "input_size", &OnlineLearningModule::input_size,



From tihocan at mail.berlios.de  Tue Apr 17 21:16:29 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 17 Apr 2007 21:16:29 +0200
Subject: [Plearn-commits] r6904 - trunk/plearn_learners/online
Message-ID: <200704171916.l3HJGTI8012304@sheep.berlios.de>

Author: tihocan
Date: 2007-04-17 21:16:28 +0200 (Tue, 17 Apr 2007)
New Revision: 6904

Modified:
   trunk/plearn_learners/online/RBMLayer.h
Log:
- Removed private members 'tmp_input_gradient' and 'tmp_input_diag_hessian' that were not used, and exist already in the parent class
- Fixed some typos in comments


Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2007-04-17 19:14:59 UTC (rev 6903)
+++ trunk/plearn_learners/online/RBMLayer.h	2007-04-17 19:16:28 UTC (rev 6904)
@@ -176,13 +176,13 @@
     virtual void forget();
 
     //! Set the internal bias values to rbm_bias
-    virtual void getAllBias(const Vec& rbm_bias);
+    virtual void setAllBias(const Vec& rbm_bias);
 
-    //! Computes the contrastive divergence bias with respect to the bias
-    //! (or activations, which is equivalent)
+    //! Computes the contrastive divergence gradient with respect to the bias
+    //! (or activations, which is equivalent).
     virtual void bpropCD(Vec& bias_gradient);
 
-    //! Computes the contrastive divergence bias with respect to the bias
+    //! Computes the contrastive divergence gradient with respect to the bias
     //! (or activations, which is equivalent), given the positive and
     //! negative phase values.
     virtual void bpropCD(const Vec& pos_values, const Vec& neg_values,
@@ -232,11 +232,6 @@
 private:
     //#####  Private Data Members  ############################################
 
-    // The rest of the private stuff goes here
-    //! Stores the computed input gradient (useful when accumulate)
-    Vec tmp_input_gradient;
-    //! Stores the computed input diag hessian (useful when accumulate)
-    Vec tmp_input_diag_hessian;
 };
 
 // Declares a few other classes and functions related to this class



From tihocan at mail.berlios.de  Tue Apr 17 21:16:57 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 17 Apr 2007 21:16:57 +0200
Subject: [Plearn-commits] r6905 - trunk/plearn_learners/online
Message-ID: <200704171916.l3HJGvpf012376@sheep.berlios.de>

Author: tihocan
Date: 2007-04-17 21:16:56 +0200 (Tue, 17 Apr 2007)
New Revision: 6905

Modified:
   trunk/plearn_learners/online/RBMLayer.cc
Log:
Cosmetic changes

Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2007-04-17 19:16:28 UTC (rev 6904)
+++ trunk/plearn_learners/online/RBMLayer.cc	2007-04-17 19:16:56 UTC (rev 6905)
@@ -147,13 +147,13 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
-    deepCopyField(activation, copies);
-    deepCopyField(sample, copies);
-    deepCopyField(expectation, copies);
-    deepCopyField(bias, copies);
+    deepCopyField(bias,           copies);
+    deepCopyField(activation,     copies);
+    deepCopyField(sample,         copies);
+    deepCopyField(expectation,    copies);
     deepCopyField(bias_pos_stats, copies);
     deepCopyField(bias_neg_stats, copies);
-    deepCopyField(bias_inc, copies);
+    deepCopyField(bias_inc,       copies);
 }
 
 
@@ -274,6 +274,9 @@
     clearStats();
 }
 
+////////////
+// update //
+////////////
 void RBMLayer::update( const Vec& pos_values, const Vec& neg_values)
 {
     // bias -= learning_rate * (pos_values - neg_values)
@@ -298,12 +301,18 @@
     }
 }
 
-void RBMLayer::getAllBias(const Vec& rbm_bias)
+////////////////
+// setAllBias //
+////////////////
+void RBMLayer::setAllBias(const Vec& rbm_bias)
 {
     PLASSERT( rbm_bias.size() == size );
     bias << rbm_bias;
 }
 
+/////////////
+// bpropCD //
+/////////////
 void RBMLayer::bpropCD(Vec& bias_gradient)
 {
     // grad = bias_pos_stats/pos_count - bias_neg_stats/neg_count



From tihocan at mail.berlios.de  Tue Apr 17 21:18:34 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 17 Apr 2007 21:18:34 +0200
Subject: [Plearn-commits] r6906 - trunk/commands/PLearnCommands
Message-ID: <200704171918.l3HJIYuE012459@sheep.berlios.de>

Author: tihocan
Date: 2007-04-17 21:18:34 +0200 (Tue, 17 Apr 2007)
New Revision: 6906

Modified:
   trunk/commands/PLearnCommands/RunCommand.cc
Log:
Can now use the 'run' command on a script whose names has explicit variable values, for instance plearn run myscript.plearn::learning_rate=1

Modified: trunk/commands/PLearnCommands/RunCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/RunCommand.cc	2007-04-17 19:16:56 UTC (rev 6905)
+++ trunk/commands/PLearnCommands/RunCommand.cc	2007-04-17 19:18:34 UTC (rev 6906)
@@ -62,22 +62,38 @@
 //! The actual implementation of the 'RunCommand' command 
 void RunCommand::run(const vector<string>& args)
 {
-    string scriptfile = args[0];
-    if (!isfile(scriptfile))
-        PLERROR("Non-existent script file: %s\n",scriptfile.c_str());
+    const vector<string>* the_args = &args;
+    vector<string> args_augmented;
+    PPath scriptfile = args[0];
+    if (!isfile(scriptfile)) {
+        // There is no file with this exact name. Maybe there are parameters
+        // appended to the name?
+        string base;
+        map<string, string> params;
+        parseBaseAndParameters(scriptfile, base, params);
+        if (!isfile(base))
+            PLERROR("Non-existent script file: %s\n",scriptfile.c_str());
+        // Add new arguments.
+        args_augmented = args;
+        map<string, string>::const_iterator it = params.begin();
+        for (; it != params.end(); it++)
+            args_augmented.push_back(it->first + "=" + it->second);
+        the_args = &args_augmented;
+        scriptfile = base;
+    }
 
-    const string extension = extract_extension(scriptfile);
+    string extension = scriptfile.extension();
     string script;
 
     PP<PyPLearnScript> pyplearn_script;
     PStream in;
 
-    if (extension == ".pyplearn")
+    if (extension == "pyplearn")
     {
         // Make a copy of args with the first argument (the name of the script)
         // removed, leaving the first argument to the script at index 0.
-        vector<string> pyplearn_args(args.size()-1);
-        copy(args.begin() + 1, args.end(), pyplearn_args.begin());
+        vector<string> pyplearn_args(the_args->size()-1);
+        copy(the_args->begin() + 1, the_args->end(), pyplearn_args.begin());
     
         pyplearn_script = PyPLearnScript::process(scriptfile, pyplearn_args);
         script          = pyplearn_script->getScript();
@@ -90,13 +106,13 @@
 
         in = openString( script, PStream::plearn_ascii );
     }
-    else if(extension==".plearn")  // perform plearn macro expansion
+    else if(extension=="plearn")  // perform plearn macro expansion
     {
         map<string, string> vars;
         // populate vars with the arguments passed on the command line
-        for (unsigned int i=1; i<args.size(); i++)
+        for (unsigned int i=1; i<the_args->size(); i++)
         {
-            string option = args[i];
+            string option = (*the_args)[i];
             // Skip --foo command-lines options.
             if (option.size() < 2 || option.substr(0, 2) != "--")
             {
@@ -108,7 +124,7 @@
         script = readFileAndMacroProcess(scriptfile, vars);
         in = openString( script, PStream::plearn_ascii );
     }
-    else if(extension==".psave") // do not perform plearn macro expansion
+    else if(extension=="psave") // do not perform plearn macro expansion
     {
         in = openFile(scriptfile, PStream::plearn_ascii);
     }



From tihocan at mail.berlios.de  Tue Apr 17 21:19:38 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 17 Apr 2007 21:19:38 +0200
Subject: [Plearn-commits] r6907 - trunk/plearn_learners/online
Message-ID: <200704171919.l3HJJcRa012534@sheep.berlios.de>

Author: tihocan
Date: 2007-04-17 21:19:38 +0200 (Tue, 17 Apr 2007)
New Revision: 6907

Modified:
   trunk/plearn_learners/online/RBMMatrixConnection.h
Log:
Removed useless comment

Modified: trunk/plearn_learners/online/RBMMatrixConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.h	2007-04-17 19:18:34 UTC (rev 6906)
+++ trunk/plearn_learners/online/RBMMatrixConnection.h	2007-04-17 19:19:38 UTC (rev 6907)
@@ -49,7 +49,6 @@
 /**
  * Stores and learns the parameters between two linear layers of an RBM.
  *
- * @todo: yes
  */
 class RBMMatrixConnection: public RBMConnection
 {



From tihocan at mail.berlios.de  Tue Apr 17 21:20:22 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 17 Apr 2007 21:20:22 +0200
Subject: [Plearn-commits] r6908 - trunk/plearn_learners/online
Message-ID: <200704171920.l3HJKMeF012612@sheep.berlios.de>

Author: tihocan
Date: 2007-04-17 21:20:22 +0200 (Tue, 17 Apr 2007)
New Revision: 6908

Modified:
   trunk/plearn_learners/online/RBMBinomialLayer.cc
   trunk/plearn_learners/online/RBMBinomialLayer.h
Log:
Minor comments and help changes

Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-04-17 19:19:38 UTC (rev 6907)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-04-17 19:20:22 UTC (rev 6908)
@@ -47,7 +47,7 @@
 
 PLEARN_IMPLEMENT_OBJECT(
     RBMBinomialLayer,
-    "Layer in an RBM formed with binomial units",
+    "Layer in an RBM formed with binomial units.",
     "");
 
 RBMBinomialLayer::RBMBinomialLayer( real the_learning_rate ) :

Modified: trunk/plearn_learners/online/RBMBinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.h	2007-04-17 19:19:38 UTC (rev 6907)
+++ trunk/plearn_learners/online/RBMBinomialLayer.h	2007-04-17 19:20:22 UTC (rev 6908)
@@ -48,7 +48,6 @@
 /**
  * Layer in an RBM formed with binomial units
  *
- * @todo: yes
  */
 class RBMBinomialLayer: public RBMLayer
 {



From tihocan at mail.berlios.de  Tue Apr 17 21:21:20 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 17 Apr 2007 21:21:20 +0200
Subject: [Plearn-commits] r6909 - trunk/plearn_learners/online
Message-ID: <200704171921.l3HJLK9A012698@sheep.berlios.de>

Author: tihocan
Date: 2007-04-17 21:21:20 +0200 (Tue, 17 Apr 2007)
New Revision: 6909

Modified:
   trunk/plearn_learners/online/ModuleStackModule.cc
Log:
Hiding options 'input_size' and 'output_size' since they are set at build time

Modified: trunk/plearn_learners/online/ModuleStackModule.cc
===================================================================
--- trunk/plearn_learners/online/ModuleStackModule.cc	2007-04-17 19:20:22 UTC (rev 6908)
+++ trunk/plearn_learners/online/ModuleStackModule.cc	2007-04-17 19:21:20 UTC (rev 6909)
@@ -56,12 +56,11 @@
 {
 }
 
+////////////////////
+// declareOptions //
+////////////////////
 void ModuleStackModule::declareOptions(OptionList& ol)
 {
-    // declareOption(ol, "", &ModuleStackModule::,
-    //               OptionBase::buildoption,
-    //               "");
-
     declareOption(ol, "modules", &ModuleStackModule::modules,
                   OptionBase::buildoption,
                   "The underlying modules");
@@ -72,8 +71,22 @@
 
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
+
+    // Hide unused options.
+
+    redeclareOption(ol, "input_size", &ModuleStackModule::input_size,
+            OptionBase::nosave,
+            "Set at build time.");
+
+    redeclareOption(ol, "output_size", &ModuleStackModule::output_size,
+            OptionBase::nosave,
+            "Set at build time.");
+
 }
 
+////////////
+// build_ //
+////////////
 void ModuleStackModule::build_()
 {
     // TODO: Do something with the random generator?
@@ -88,10 +101,15 @@
 
         input_size = modules[0]->input_size;
         output_size = modules[n_modules-1]->output_size;
+    } else {
+        input_size = -1;
+        output_size = -1;
     }
 }
 
-// ### Nothing to add here, simply calls build_
+///////////
+// build //
+///////////
 void ModuleStackModule::build()
 {
     inherited::build();



From tihocan at mail.berlios.de  Tue Apr 17 21:22:33 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 17 Apr 2007 21:22:33 +0200
Subject: [Plearn-commits] r6910 - trunk/plearn_learners/online
Message-ID: <200704171922.l3HJMXKx012803@sheep.berlios.de>

Author: tihocan
Date: 2007-04-17 21:22:33 +0200 (Tue, 17 Apr 2007)
New Revision: 6910

Modified:
   trunk/plearn_learners/online/CombiningCostsModule.h
   trunk/plearn_learners/online/NLLCostModule.h
Log:
Overriding setLearningRate to get rid of warning at run time

Modified: trunk/plearn_learners/online/CombiningCostsModule.h
===================================================================
--- trunk/plearn_learners/online/CombiningCostsModule.h	2007-04-17 19:21:20 UTC (rev 6909)
+++ trunk/plearn_learners/online/CombiningCostsModule.h	2007-04-17 19:22:33 UTC (rev 6910)
@@ -95,6 +95,9 @@
     //! Calls this method on the sub_costs
     virtual void bbpropUpdate(const Vec& input, const Vec& target, real cost);
 
+    //! Overridden to do nothing (no warning message in particular).
+    virtual void setLearningRate(real dynamic_learning_rate) {}
+
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from
     //! build().

Modified: trunk/plearn_learners/online/NLLCostModule.h
===================================================================
--- trunk/plearn_learners/online/NLLCostModule.h	2007-04-17 19:21:20 UTC (rev 6909)
+++ trunk/plearn_learners/online/NLLCostModule.h	2007-04-17 19:22:33 UTC (rev 6910)
@@ -90,6 +90,9 @@
     {
     }
 
+    //! Overridden to do nothing (in particular, no warning).
+    virtual void setLearningRate(real dynamic_learning_rate) {}
+
     //! Indicates the name of the computed costs
     virtual TVec<string> name();
 



From tihocan at mail.berlios.de  Tue Apr 17 21:22:44 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 17 Apr 2007 21:22:44 +0200
Subject: [Plearn-commits] r6911 - trunk/plearn_learners/online
Message-ID: <200704171922.l3HJMi4m012885@sheep.berlios.de>

Author: tihocan
Date: 2007-04-17 21:22:43 +0200 (Tue, 17 Apr 2007)
New Revision: 6911

Modified:
   trunk/plearn_learners/online/SoftmaxModule.h
Log:
More explicit dOxygen help

Modified: trunk/plearn_learners/online/SoftmaxModule.h
===================================================================
--- trunk/plearn_learners/online/SoftmaxModule.h	2007-04-17 19:22:33 UTC (rev 6910)
+++ trunk/plearn_learners/online/SoftmaxModule.h	2007-04-17 19:22:43 UTC (rev 6911)
@@ -85,8 +85,7 @@
     //! build().
     virtual void forget();
 
-    // Allows to change the learning rate, if the derived class has one and
-    // allows to do so.
+    //! Overridden to do nothing (no warning message in particular).
     virtual void setLearningRate(real dynamic_learning_rate);
 
     //#####  PLearn::Object Protocol  #########################################



From tihocan at mail.berlios.de  Tue Apr 17 21:23:29 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 17 Apr 2007 21:23:29 +0200
Subject: [Plearn-commits] r6912 - trunk/plearn_learners/generic
Message-ID: <200704171923.l3HJNTg6013031@sheep.berlios.de>

Author: tihocan
Date: 2007-04-17 21:23:29 +0200 (Tue, 17 Apr 2007)
New Revision: 6912

Modified:
   trunk/plearn_learners/generic/AddCostToLearner.cc
Log:
Added NLL as potential added cost

Modified: trunk/plearn_learners/generic/AddCostToLearner.cc
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.cc	2007-04-17 19:22:43 UTC (rev 6911)
+++ trunk/plearn_learners/generic/AddCostToLearner.cc	2007-04-17 19:23:29 UTC (rev 6912)
@@ -122,14 +122,15 @@
         "   component-wise. Otherwise, the target must be a one-dimensional\n"
         "   vector (an integer corresponding to the class), and the output\n"
         "   from the sub-learner is interpreted as a vector of weights for\n"
-        "   each class."
+        "   each class.\n"
         " - 'binary_class_error': classification error for a one-dimensional\n"
         "   target that must be either 0 or 1. The output must also be one-\n"
         "   dimensional, and is interpreted as the predicted probability for\n"
         "   class 1 (thus class 1 is chosen when the output is > 0.5)\n"
         " - 'lift_output': to compute the lift cost (for the positive class)\n"
         " - 'opposite_lift_output': to compute the lift cost (for the negative) class\n"
-        " - 'cross_entropy': t*log(o) + (1-t)*log(1-o)\n"
+        " - 'cross_entropy': -t*log(o) - (1-t)*log(1-o)\n"
+        " - 'NLL': -log(o[t])\n"
         " - 'mse': the mean squared error (o - t)^2\n"
         " - 'squared_norm_reconstruction_error': | ||i||^2 - ||o||^2 |\n"
     );
@@ -227,6 +228,10 @@
         } else if (c == "squared_norm_reconstruction_error") {
         } else if (c == "class_error") {
         } else if (c == "binary_class_error") {
+        } else if (c == "NLL") {
+            // Output should be in [0,1].
+            output_min = max(output_min, real(0));
+            output_max = min(output_max, real(1));
         } else {
             PLERROR("In AddCostToLearner::build_ - Invalid cost requested (make sure you are using the new costs syntax)");
         }
@@ -430,6 +435,15 @@
 #endif
             cross_entropy_prop.fprop();
             costs[ind_cost] = cross_entropy_var->valuedata[0];
+        } else if (c == "NLL") {
+            PLASSERT_MSG(fast_exact_is_equal(desired_target[0],
+                        round(desired_target[0])), "The target must be an "
+                    "integer");
+            int class_target = int(round(desired_target[0]));
+            PLASSERT_MSG(class_target < sub_learner_output.length(),
+                    "The sub learner output must have a size equal to the "
+                    "number of classes");
+            costs[ind_cost] = - pl_log(sub_learner_output[class_target]);
         } else if (c == "class_error") {
             int output_length = sub_learner_output.length();
             bool good = true;



From tihocan at mail.berlios.de  Tue Apr 17 21:24:23 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 17 Apr 2007 21:24:23 +0200
Subject: [Plearn-commits] r6913 - trunk/plearn_learners/online
Message-ID: <200704171924.l3HJONep013114@sheep.berlios.de>

Author: tihocan
Date: 2007-04-17 21:24:23 +0200 (Tue, 17 Apr 2007)
New Revision: 6913

Modified:
   trunk/plearn_learners/online/SoftmaxModule.cc
Log:
Option 'output_size' is now hidden, since it is set at build time

Modified: trunk/plearn_learners/online/SoftmaxModule.cc
===================================================================
--- trunk/plearn_learners/online/SoftmaxModule.cc	2007-04-17 19:23:29 UTC (rev 6912)
+++ trunk/plearn_learners/online/SoftmaxModule.cc	2007-04-17 19:24:23 UTC (rev 6913)
@@ -46,36 +46,52 @@
 
 PLEARN_IMPLEMENT_OBJECT(
     SoftmaxModule,
-    "Computes the softmax function on a vector",
-    "");
+    "Computes the softmax function on a vector.",
+    ""
+);
 
+///////////////////
+// SoftmaxModule //
+///////////////////
 SoftmaxModule::SoftmaxModule()
-{
-}
+{}
 
+////////////////////
+// declareOptions //
+////////////////////
 void SoftmaxModule::declareOptions(OptionList& ol)
 {
-    // declareOption(ol, "myoption", &SoftmaxModule::myoption,
-    //               OptionBase::buildoption,
-    //               "Help text describing this option");
-
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
+
+    // Hide unused options.
+
+    declareOption(ol, "output_size", &SoftmaxModule::output_size,
+            OptionBase::nosave,
+            "Set at build time.");
+
 }
 
+////////////
+// build_ //
+////////////
 void SoftmaxModule::build_()
 {
     output_size = input_size;
 }
 
-// ### Nothing to add here, simply calls build_
+///////////
+// build //
+///////////
 void SoftmaxModule::build()
 {
     inherited::build();
     build_();
 }
 
-
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
 void SoftmaxModule::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
@@ -120,16 +136,23 @@
     }
 }
 
-//! reset the parameters to the state they would be BEFORE starting training.
-//! Note that this method is necessarily called from build().
+////////////
+// forget //
+////////////
 void SoftmaxModule::forget()
 {
 }
 
+/////////////////////
+// setLearningRate //
+/////////////////////
 void SoftmaxModule::setLearningRate(real dynamic_learning_rate)
 {
 }
 
+//////////////////
+// bbpropUpdate //
+//////////////////
 void SoftmaxModule::bbpropUpdate(const Vec& input, const Vec& output,
                                  Vec& input_gradient,
                                  const Vec& output_gradient,
@@ -137,7 +160,7 @@
                                  const Vec& output_diag_hessian,
                                  bool accumulate)
 {
-    PLERROR( "Not implemented yet, please come back later or complaint to"
+    PLERROR( "Not implemented yet, please come back later or complain to"
              " lamblinp." );
 }
 



From nouiz at mail.berlios.de  Tue Apr 17 21:39:36 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 17 Apr 2007 21:39:36 +0200
Subject: [Plearn-commits] r6914 - trunk/scripts
Message-ID: <200704171939.l3HJdaEB020609@sheep.berlios.de>

Author: nouiz
Date: 2007-04-17 21:39:35 +0200 (Tue, 17 Apr 2007)
New Revision: 6914

Modified:
   trunk/scripts/multipymake
Log:
Allow to compile only one target
BUGFIX


Modified: trunk/scripts/multipymake
===================================================================
--- trunk/scripts/multipymake	2007-04-17 19:24:23 UTC (rev 6913)
+++ trunk/scripts/multipymake	2007-04-17 19:39:35 UTC (rev 6914)
@@ -11,11 +11,12 @@
 #         This will execute pymake -float -opt -link plearn-float.cc and
 #                           pymake -double -opt -link plearn-double.cc
 
-if [ $# -ge 3 ]; then
+if [ $# -ge 2 ]; then
     BASEPROG=$1
     shift
 else
     echo "Usage: $0 <base_prog> <List of parameter> ..."
+    exit
  fi
 ALL=" -link "
 



From yoshua at mail.berlios.de  Wed Apr 18 14:34:49 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Wed, 18 Apr 2007 14:34:49 +0200
Subject: [Plearn-commits] r6915 - trunk/plearn_learners/online
Message-ID: <200704181234.l3ICYnpQ013098@sheep.berlios.de>

Author: yoshua
Date: 2007-04-18 14:34:48 +0200 (Wed, 18 Apr 2007)
New Revision: 6915

Modified:
   trunk/plearn_learners/online/OnlineLearningModule.cc
   trunk/plearn_learners/online/OnlineLearningModule.h
   trunk/plearn_learners/online/RBMLayer.h
Log:
Modifications to move towards efficient mini-batch computations


Modified: trunk/plearn_learners/online/OnlineLearningModule.cc
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.cc	2007-04-17 19:39:35 UTC (rev 6914)
+++ trunk/plearn_learners/online/OnlineLearningModule.cc	2007-04-18 12:34:48 UTC (rev 6915)
@@ -64,6 +64,22 @@
 {
 }
 
+//! default inefficient implementation of mini-batch fprop
+void OnlineLearningModule::fprop(const Mat& input, Mat& output) const
+{
+    int n=input.length();
+#ifdef BOUNDCHECK
+    if (n!=output.length())
+        PLERROR("OnlineLearningModule::fprop for matrices: inconsistent lengths of argument matrices\n");
+#endif
+    for (int i=0;i<n;i++)
+    {
+        Vec input_i = input(i);
+        Vec output_i = output(i);
+        fprop(input_i,output_i);
+    }
+}
+
 void OnlineLearningModule::bpropUpdate(const Vec& input, const Vec& output,
                                        Vec& input_gradient,
                                        const Vec& output_gradient,
@@ -81,6 +97,34 @@
     bpropUpdate(input, output, tmp_input_gradient, output_gradient);
 }
 
+void OnlineLearningModule::bpropUpdate(const Mat& input, const Mat& output,
+                                       Mat& input_gradient,
+                                       const Mat& output_gradient,
+                                       bool accumulate)
+{
+    int n=input.length();
+#ifdef BOUNDCHECK
+    if (n!=output.length() || n!=output_gradient.length())
+        PLERROR("OnlineLearningModule::bpropUpdate for matrices: inconsistent lengths of argument matrices\n");
+#endif
+    if (n!=input_gradient.length())
+        input_gradient.resize(n,input.width());
+    for (int i=0;i<n;i++)
+    {
+        Vec input_i = input(i);
+        Vec output_i = output(i);
+        Vec input_gradient_i = input_gradient(i);
+        Vec output_gradient_i = output_gradient(i);
+        bpropUpdate(input_i,output_i,input_gradient_i,output_gradient_i,accumulate);
+    }
+}
+
+void OnlineLearningModule::bpropUpdate(const Mat& input, const Mat& output,
+                                       const Mat& output_gradient)
+{
+    bpropUpdate(input, output, tmpm_input_gradient, output_gradient);
+}
+
 //! Default method for bbpropUpdate functions, so that it compiles but crashes
 //! if not implemented but used.
 void OnlineLearningModule::bbpropUpdate(const Vec& input, const Vec& output,

Modified: trunk/plearn_learners/online/OnlineLearningModule.h
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.h	2007-04-17 19:39:35 UTC (rev 6914)
+++ trunk/plearn_learners/online/OnlineLearningModule.h	2007-04-18 12:34:48 UTC (rev 6915)
@@ -99,6 +99,7 @@
 
     //! given the input, compute the output (possibly resize it appropriately)
     virtual void fprop(const Vec& input, Vec& output) const = 0;
+    virtual void fprop(const Mat& input, Mat& output) const;
 
     //! Adapt based on the output gradient: this method should only
     //! be called just after a corresponding fprop; it should be
@@ -111,6 +112,8 @@
     //! AND IGNORES INPUT GRADIENT.
     virtual void bpropUpdate(const Vec& input, const Vec& output,
                              const Vec& output_gradient);
+    virtual void bpropUpdate(const Mat& input, const Mat& output,
+                             const Mat& output_gradient);
 
     //! this version allows to obtain the input gradient as well
     //! N.B. THE DEFAULT IMPLEMENTATION JUST RAISES A PLERROR.
@@ -119,6 +122,9 @@
     virtual void bpropUpdate(const Vec& input, const Vec& output,
                              Vec& input_gradient, const Vec& output_gradient,
                              bool accumulate=false);
+    virtual void bpropUpdate(const Mat& input, const Mat& output,
+                             Mat& input_gradient, const Mat& output_gradient,
+                             bool accumulate=false);
 
     //! Similar to bpropUpdate, but adapt based also on the estimation
     //! of the diagonal of the Hessian matrix, and propagates this
@@ -181,6 +187,7 @@
 protected:
     // Also used in CostModule for instance
     mutable Vec tmp_input_gradient;
+    mutable Mat tmpm_input_gradient;
     mutable Vec tmp_input_diag_hessian;
 
 

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2007-04-17 19:39:35 UTC (rev 6914)
+++ trunk/plearn_learners/online/RBMLayer.h	2007-04-18 12:34:48 UTC (rev 6915)
@@ -82,12 +82,15 @@
 
     //! activation value: \sum Wx + b
     Vec activation;
+    Mat activations; // for mini-batch operations
 
     //! Contains a sample of the random variable in this layer
     Vec sample;
+    Mat samples;  
 
     //! Contains the expected value of the random variable in this layer
     Vec expectation;
+    Mat expectations; // for mini-batch operations
 
     //! flags that expectation was computed based on most recently computed
     //! value of activation
@@ -149,22 +152,27 @@
     //! Computes the negative log-likelihood of target given the
     //! internal activations of the layer
     virtual real fpropNLL(const Vec& target);
+    virtual real fpropNLL(const Mat& target);
 
     //! Computes the gradient of the negative log-likelihood of target
     //! with respect to the layer's bias, given the internal activations
     virtual void bpropNLL(const Vec& target, real nll, Vec& bias_gradient);
+    virtual void bpropNLL(const Mat& target, real nll, Mat& bias_gradient);
 
     //! Accumulates positive phase statistics
     virtual void accumulatePosStats( const Vec& pos_values );
+    virtual void accumulatePosStats( const Mat& pos_values );
 
     //! Accumulates negative phase statistics
     virtual void accumulateNegStats( const Vec& neg_values );
+    virtual void accumulateNegStats( const Mat& neg_values );
 
     //! Update parameters according to accumulated statistics
     virtual void update();
 
     //! Update parameters according to one pair of vectors
     virtual void update( const Vec& pos_values, const Vec& neg_values );
+    virtual void update( const Mat& pos_values, const Mat& neg_values );
 
     //! resets activations, sample and expectation fields
     virtual void reset();



From yoshua at mail.berlios.de  Wed Apr 18 18:06:26 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Wed, 18 Apr 2007 18:06:26 +0200
Subject: [Plearn-commits] r6916 - trunk/plearn_learners/online
Message-ID: <200704181606.l3IG6Q1M028886@sheep.berlios.de>

Author: yoshua
Date: 2007-04-18 18:06:23 +0200 (Wed, 18 Apr 2007)
New Revision: 6916

Modified:
   trunk/plearn_learners/online/RBMLayer.h
Log:


Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2007-04-18 12:34:48 UTC (rev 6915)
+++ trunk/plearn_learners/online/RBMLayer.h	2007-04-18 16:06:23 UTC (rev 6916)
@@ -152,27 +152,22 @@
     //! Computes the negative log-likelihood of target given the
     //! internal activations of the layer
     virtual real fpropNLL(const Vec& target);
-    virtual real fpropNLL(const Mat& target);
 
     //! Computes the gradient of the negative log-likelihood of target
     //! with respect to the layer's bias, given the internal activations
     virtual void bpropNLL(const Vec& target, real nll, Vec& bias_gradient);
-    virtual void bpropNLL(const Mat& target, real nll, Mat& bias_gradient);
 
     //! Accumulates positive phase statistics
     virtual void accumulatePosStats( const Vec& pos_values );
-    virtual void accumulatePosStats( const Mat& pos_values );
 
     //! Accumulates negative phase statistics
     virtual void accumulateNegStats( const Vec& neg_values );
-    virtual void accumulateNegStats( const Mat& neg_values );
 
     //! Update parameters according to accumulated statistics
     virtual void update();
 
     //! Update parameters according to one pair of vectors
     virtual void update( const Vec& pos_values, const Vec& neg_values );
-    virtual void update( const Mat& pos_values, const Mat& neg_values );
 
     //! resets activations, sample and expectation fields
     virtual void reset();



From tihocan at mail.berlios.de  Wed Apr 18 19:08:17 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 18 Apr 2007 19:08:17 +0200
Subject: [Plearn-commits] r6917 - trunk/plearn/vmat
Message-ID: <200704181708.l3IH8HvE016035@sheep.berlios.de>

Author: tihocan
Date: 2007-04-18 19:08:16 +0200 (Wed, 18 Apr 2007)
New Revision: 6917

Modified:
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
Added utility method 'getExamples' to easily obtain a subset of examples

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2007-04-18 16:06:23 UTC (rev 6916)
+++ trunk/plearn/vmat/VMatrix.cc	2007-04-18 17:08:16 UTC (rev 6917)
@@ -511,6 +511,32 @@
         weight = get(i,inputsize_+targetsize_);
 }
 
+/////////////////
+// getExamples //
+/////////////////
+void VMatrix::getExamples(int i_start, int length, Mat& inputs, Mat& targets,
+                          Vec& weights, Mat* extras)
+{
+    inputs.resize(length, inputsize());
+    targets.resize(length, targetsize());
+    weights.resize(length);
+    if (extras)
+        extras->resize(length, extrasize());
+    Vec input, target, extra;
+    for (int k = 0; k < length; k++) {
+        input = inputs(k);
+        target = targets(k);
+        getExample(i_start + k, input, target, weights[k]);
+        if (extras) {
+            extra = (*extras)(k);
+            getExtra(i_start + k, extra);
+        }
+    }
+}
+
+//////////////
+// getExtra //
+//////////////
 void VMatrix::getExtra(int i, Vec& extra)
 {
     if(inputsize_<0 || targetsize_<0 || weightsize_<0 || extrasize_<0)

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2007-04-18 16:06:23 UTC (rev 6916)
+++ trunk/plearn/vmat/VMatrix.h	2007-04-18 17:08:16 UTC (rev 6917)
@@ -436,6 +436,12 @@
      */
     virtual void getExample(int i, Vec& input, Vec& target, real& weight);
 
+    //! Obtain a subset of 'length' examples, starting from 'i_start'.
+    //! The 'extra' matrix is provided as a pointer so that it can be omitted
+    //! without significant overhead.
+    void getExamples(int i_start, int length, Mat& inputs, Mat& targets,
+                     Vec& weights, Mat* extra = NULL);
+
     /**
      *  Complements the getExample method, fetching the the extrasize_ "extra"
      *  fields expected to appear after the input, target and weight fields



From tihocan at mail.berlios.de  Wed Apr 18 19:08:37 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 18 Apr 2007 19:08:37 +0200
Subject: [Plearn-commits] r6918 - trunk/plearn_learners/online
Message-ID: <200704181708.l3IH8b3J016352@sheep.berlios.de>

Author: tihocan
Date: 2007-04-18 19:08:35 +0200 (Wed, 18 Apr 2007)
New Revision: 6918

Modified:
   trunk/plearn_learners/online/RBMLayer.cc
Log:
Added missing deep copy statements

Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2007-04-18 17:08:16 UTC (rev 6917)
+++ trunk/plearn_learners/online/RBMLayer.cc	2007-04-18 17:08:35 UTC (rev 6918)
@@ -149,8 +149,11 @@
 
     deepCopyField(bias,           copies);
     deepCopyField(activation,     copies);
+    deepCopyField(activations,    copies);
     deepCopyField(sample,         copies);
+    deepCopyField(samples,        copies);
     deepCopyField(expectation,    copies);
+    deepCopyField(expectations,   copies);
     deepCopyField(bias_pos_stats, copies);
     deepCopyField(bias_neg_stats, copies);
     deepCopyField(bias_inc,       copies);



From tihocan at mail.berlios.de  Wed Apr 18 19:10:21 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 18 Apr 2007 19:10:21 +0200
Subject: [Plearn-commits] r6919 - trunk/plearn_learners/online
Message-ID: <200704181710.l3IHALgt018032@sheep.berlios.de>

Author: tihocan
Date: 2007-04-18 19:10:20 +0200 (Wed, 18 Apr 2007)
New Revision: 6919

Modified:
   trunk/plearn_learners/online/OnlineLearningModule.cc
Log:
Added missing deep copy statement

Modified: trunk/plearn_learners/online/OnlineLearningModule.cc
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.cc	2007-04-18 17:08:35 UTC (rev 6918)
+++ trunk/plearn_learners/online/OnlineLearningModule.cc	2007-04-18 17:10:20 UTC (rev 6919)
@@ -180,6 +180,7 @@
     inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(random_gen,             copies);
     deepCopyField(tmp_input_gradient,     copies);
+    deepCopyField(tmpm_input_gradient,    copies);
     deepCopyField(tmp_input_diag_hessian, copies);
 }
 



From tihocan at mail.berlios.de  Wed Apr 18 19:20:40 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 18 Apr 2007 19:20:40 +0200
Subject: [Plearn-commits] r6920 - trunk/plearn_learners/online
Message-ID: <200704181720.l3IHKepR030454@sheep.berlios.de>

Author: tihocan
Date: 2007-04-18 19:20:39 +0200 (Wed, 18 Apr 2007)
New Revision: 6920

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
Log:
Starting work to implement a mini-batch version (not done yet)

Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-04-18 17:10:20 UTC (rev 6919)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-04-18 17:20:39 UTC (rev 6920)
@@ -51,9 +51,13 @@
     "This version supports different unit types, different connection types,\n"
     "and different cost functions, including the NLL in classification.\n");
 
+///////////////////
+// DeepBeliefNet //
+///////////////////
 DeepBeliefNet::DeepBeliefNet() :
     cd_learning_rate( 0. ),
     grad_learning_rate( 0. ),
+    batch_size(1),
     grad_decrease_ct( 0. ),
     // grad_weight_decay( 0. ),
     n_classes(-1),
@@ -61,16 +65,19 @@
     reconstruct_layerwise( false ),
     n_layers( 0 ),
     online ( false ),
+    minibatch_size(0),
     final_module_has_learning_rate( false ),
     final_cost_has_learning_rate( false ),
     nll_cost_index( -1 ),
     class_cost_index( -1 ),
     recons_cost_index( -1 )
-
 {
     random_gen = new PRandom( seed_ );
 }
 
+////////////////////
+// declareOptions //
+////////////////////
 void DeepBeliefNet::declareOptions(OptionList& ol)
 {
     declareOption(ol, "cd_learning_rate", &DeepBeliefNet::cd_learning_rate,
@@ -83,6 +90,10 @@
                   "The decrease constant of the learning rate used during"
                   " gradient descent");
 
+    declareOption(ol, "batch_size", &DeepBeliefNet::batch_size,
+                  OptionBase::buildoption,
+        "Training batch size (1=stochastic learning, 0=full batch learning).");
+
     /* NOT IMPLEMENTED YET
     declareOption(ol, "grad_weight_decay", &DeepBeliefNet::grad_weight_decay,
                   OptionBase::buildoption,
@@ -205,6 +216,10 @@
                   OptionBase::learntoption,
                   "Number of layers");
 
+    declareOption(ol, "minibatch_size", &DeepBeliefNet::minibatch_size,
+                  OptionBase::learntoption,
+                  "Size of a mini-batch.");
+
     /*
     declareOption(ol, "n_final_costs", &DeepBeliefNet::n_final_costs,
                   OptionBase::learntoption,
@@ -221,18 +236,12 @@
     inherited::declareOptions(ol);
 }
 
+////////////
+// build_ //
+////////////
 void DeepBeliefNet::build_()
 {
-    // ### This method should do the real building of the object,
-    // ### according to set 'options', in *any* situation.
-    // ### Typical situations include:
-    // ###  - Initial building of an object from a few user-specified options
-    // ###  - Building of a "reloaded" object: i.e. from the complete set of
-    // ###    all serialised options.
-    // ###  - Updating or "re-building" of an object after a few "tuning"
-    // ###    options have been modified.
-    // ### You should assume that the parent class' build_() has already been
-    // ### called.
+    PLASSERT( batch_size >= 0 );
 
     MODULE_LOG << "build_() called" << endl;
 
@@ -266,6 +275,9 @@
     // build_costs(); /* ? */
 }
 
+//////////////////////////////////
+// build_layers_and_connections //
+//////////////////////////////////
 void DeepBeliefNet::build_layers_and_connections()
 {
     MODULE_LOG << "build_layers_and_connections() called" << endl;
@@ -279,7 +291,9 @@
         PLASSERT( layers[0]->size == inputsize() );
 
     activation_gradients.resize( n_layers );
+    activations_gradients.resize( n_layers );
     expectation_gradients.resize( n_layers );
+    expectations_gradients.resize( n_layers );
 
     for( int i=0 ; i<n_layers-1 ; i++ )
     {
@@ -310,10 +324,17 @@
     expectation_gradients[n_layers-1].resize(last_layer_size);
 }
 
+///////////////////////////////
+// build_classification_cost //
+///////////////////////////////
 void DeepBeliefNet::build_classification_cost()
 {
     MODULE_LOG << "build_classification_cost() called" << endl;
 
+    PLASSERT_MSG(batch_size == 1, "DeepBeliefNet::build_classification_cost - "
+            "This method has not been verified yet for minibatch "
+            "compatibility");
+
     PP<RBMMatrixConnection> last_to_target = new RBMMatrixConnection();
     last_to_target->up_size = layers[n_layers-1]->size;
     last_to_target->down_size = n_classes;
@@ -350,6 +371,9 @@
     joint_layer->build();
 }
 
+//////////////////////
+// build_final_cost //
+//////////////////////
 void DeepBeliefNet::build_final_cost()
 {
     MODULE_LOG << "build_final_cost() called" << endl;
@@ -425,6 +449,8 @@
 /////////////////////////////////
 void DeepBeliefNet::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
+    // TODO Add missing fields.
+
     inherited::makeDeepCopyFromShallowCopy(copies);
 
     deepCopyField(training_schedule,        copies);
@@ -446,6 +472,7 @@
     deepCopyField(class_gradient,           copies);
     deepCopyField(class_input_gradient,     copies);
     deepCopyField(final_cost_gradient,      copies);
+    deepCopyField(final_cost_gradients,     copies);
     deepCopyField(save_layer_activation,    copies);
     deepCopyField(save_layer_expectation,   copies);
     deepCopyField(pos_down_values,          copies);
@@ -472,18 +499,11 @@
     return out_size;
 }
 
+////////////
+// forget //
+////////////
 void DeepBeliefNet::forget()
 {
-    //! (Re-)initialize the PLearner in its fresh state (that state may depend
-    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
-    //! a fresh learner!)
-    /*!
-      A typical forget() method should do the following:
-      - call inherited::forget() to initialize its random number generator
-        with the 'seed' option
-      - initialize the learner's parameters, using this random generator
-      - stage = 0
-    */
     inherited::forget();
 
     for( int i=0 ; i<n_layers-1 ; i++ )
@@ -505,55 +525,48 @@
         for( int i=0 ; i<n_layers-1 ; i++ )
             if( partial_costs[i] )
                 partial_costs[i]->forget();
-
-    stage = 0;
 }
 
+///////////
+// train //
+///////////
 void DeepBeliefNet::train()
 {
     MODULE_LOG << "train() called " << endl;
     MODULE_LOG << "  training_schedule = " << training_schedule << endl;
     MODULE_LOG << "stage = " << stage << ", target nstages = " << nstages << endl;
 
-    // The role of the train method is to bring the learner up to
-    // stage==nstages, updating train_stats with training costs measured
-    // on-line in the process.
+    PLASSERT( train_set );
+    if (stage == 0) {
+        // Training set-dependent initialization.
+        minibatch_size = batch_size > 0 ? batch_size : train_set->length();
+        for (int i = 0 ; i < n_layers; i++) {
+            activations_gradients[i].resize(minibatch_size, layers[i]->size);
+            expectation_gradients[i].resize(minibatch_size, layers[i]->size);
+        }
+        if (final_cost)
+            final_cost_gradients.resize(minibatch_size, final_cost->input_size);
+    }
 
-    /* TYPICAL CODE:
+    layers[n_layers-1]->random_gen = random_gen;
+    int last_layer_size = layers[n_layers-1]->size;
+    PLASSERT_MSG(last_layer_size >= 0,
+                 "Size of last layer must be non-negative");
+    activation_gradients[n_layers-1].resize(last_layer_size);
+    expectation_gradients[n_layers-1].resize(last_layer_size);
 
-    static Vec input;  // static so we don't reallocate memory each time...
-    static Vec target; // (but be careful that static means shared!)
-    input.resize(inputsize());    // the train_set's inputsize()
-    target.resize(targetsize());  // the train_set's targetsize()
-    real weight;
-
-    // This generic PLearner method does a number of standard stuff useful for
-    // (almost) any learner, and return 'false' if no training should take
-    // place. See PLearner.h for more details.
-    if (!initTrain())
-        return;
-
-    while(stage<nstages)
-    {
-        // clear statistics of previous epoch
-        train_stats->forget();
-
-        //... train for 1 stage, and update train_stats,
-        // using train_set->getExample(input, target, weight)
-        // and train_stats->update(train_costs)
-
-        ++stage;
-        train_stats->finalize(); // finalize statistics for this epoch
-    }
-    */
-
     Vec input( inputsize() );
     Vec target( targetsize() );
     real weight; // unused
+    Mat inputs(minibatch_size, inputsize());
+    Mat targets(minibatch_size, targetsize());
+    Vec weights;
 
     TVec<string> train_cost_names = getTrainCostNames() ;
     Vec train_costs( train_cost_names.length() );
+    Mat train_costs_m(minibatch_size, train_cost_names.length());
     train_costs.fill(MISSING_VALUE) ;
+    train_costs_m.fill(MISSING_VALUE);
 
     //give the indexes the right values
 
@@ -603,6 +616,9 @@
     if (online)
         // train all layers simultaneously AND fine-tuning as well!
     {
+        PLASSERT_MSG(batch_size == 1, "Online mode not implemented for "
+                "mini-batch learning yet");
+
         if( report_progress && stage < nstages )
             pb = new ProgressBar( "Training "+classname(),
                                   nstages - stage );
@@ -645,9 +661,15 @@
 
             for( ; stage<end_stage ; stage++ )
             {
-                int sample = stage % nsamples;
-                train_set->getExample(sample, input, target, weight);
-                greedyStep( input, target, i );
+                int sample_start = (stage * minibatch_size) % nsamples;
+                if (batch_size > 1) {
+                    train_set->getExamples(sample_start, minibatch_size,
+                            inputs, targets, weights);
+                    greedyStep( inputs, targets, i );
+                } else {
+                    train_set->getExample(sample_start, input, target, weight);
+                    greedyStep( input, target, i );
+                }
 
                 if( pb )
                     if( i == 0 )
@@ -660,6 +682,9 @@
         // possible supervised part
         if( use_classification_cost )
         {
+            PLASSERT_MSG(batch_size == 1, "'use_classification_cost' code not "
+                    "verified with mini-batch learning yet");
+
             MODULE_LOG << "Training the classification module" << endl;
 
             int end_stage = min( training_schedule[n_layers-2], nstages );
@@ -692,10 +717,12 @@
         }
 
         /**** compute reconstruction error*****/
-        RBMLayer * down_layer = get_pointer(layers[0]) ;
-        RBMLayer * up_layer =  get_pointer(layers[1]) ;
-        RBMConnection * parameters = get_pointer(connections[0]);
+        PP<RBMLayer> down_layer = get_pointer(layers[0]) ;
+        PP<RBMLayer> up_layer =  get_pointer(layers[1]) ;
+        PP<RBMConnection> parameters = get_pointer(connections[0]);
 
+        // TODO Do we really want to systematically compute this reconstruction
+        // error?
         for(int train_index = 0 ; train_index < nsamples ; train_index++)
         {
 
@@ -743,19 +770,30 @@
 
         setLearningRate( grad_learning_rate );
 
-        int begin_sample = stage % nsamples;
+        train_stats->forget();
+        bool update_stats = false;
         for( ; stage<nstages ; stage++ )
         {
-            int sample = stage % nsamples;
-            if( sample == begin_sample )
-                train_stats->forget();
+            int sample_start = (stage * minibatch_size) % nsamples;
+            // Only update train statistics for the last 'epoch', i.e. last
+            // 'nsamples' seen.
+            update_stats = update_stats ||
+                stage >= (nstages - nsamples / minibatch_size);
+
             if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
                 setLearningRate( grad_learning_rate
                                  / (1. + grad_decrease_ct * (stage - init_stage) ) );
 
-            train_set->getExample( sample, input, target, weight );
-            fineTuningStep( input, target, train_costs );
-            train_stats->update( train_costs );
+            if (minibatch_size > 1) {
+                train_set->getExamples(sample_start, minibatch_size, inputs,
+                        targets, weights);
+                fineTuningStep(inputs, targets, train_costs);
+            } else {
+                train_set->getExample( sample_start, input, target, weight );
+                fineTuningStep( input, target, train_costs );
+            }
+            if (update_stats)
+                train_stats->update( train_costs );
 
             if( pb )
                 pb->update( stage - init_stage + 1 );
@@ -768,8 +806,13 @@
     train_stats->update( train_costs ) ;
 
     train_stats->finalize();
+
+    // TODO Conversion to mini-batch: CONTINUE HERE
 }
 
+////////////////
+// onlineStep //
+////////////////
 void DeepBeliefNet::onlineStep( const Vec& input, const Vec& target,
                                 Vec& train_costs)
 {
@@ -831,29 +874,57 @@
     {
         if( final_module )
         {
-            final_module->fprop( layers[ n_layers-1 ]->expectation,
-                                 final_cost_input );
-            final_cost->fprop( final_cost_input, target,
-                               final_cost_value );
-            final_cost->bpropUpdate( final_cost_input, target,
-                                     final_cost_value[0],
-                                     final_cost_gradient );
+            /*
+            if (minibatch_size > 1) {
+                final_module->fprop( layers[ n_layers-1 ]->expectations,
+                        final_cost_inputs );
+                final_cost->fprop( final_cost_inputs, targets,
+                        final_cost_values );
+                final_cost->bpropUpdate(final_cost_inputs, targets,
+                        final_cost_values.column(0),
+                        final_cost_gradients );
 
-            final_module->bpropUpdate(
-                                      layers[ n_layers-1 ]->expectation,
-                                      final_cost_input,
-                                      expectation_gradients[ n_layers-1 ],
-                                      final_cost_gradient, true );
+                final_module->bpropUpdate(
+                        layers[ n_layers-1 ]->expectations,
+                        final_cost_inputs,
+                        expectations_gradients[ n_layers-1 ],
+                        final_cost_gradients, true );
+            } else {*/
+                final_module->fprop( layers[ n_layers-1 ]->expectation,
+                        final_cost_input );
+                final_cost->fprop( final_cost_input, target,
+                        final_cost_value );
+                final_cost->bpropUpdate( final_cost_input, target,
+                        final_cost_value[0],
+                        final_cost_gradient );
+
+                final_module->bpropUpdate(
+                        layers[ n_layers-1 ]->expectation,
+                        final_cost_input,
+                        expectation_gradients[ n_layers-1 ],
+                        final_cost_gradient, true );
+            //}
         }
         else
         {
-            final_cost->fprop( layers[ n_layers-1 ]->expectation,
-                               target,
-                               final_cost_value );
-            final_cost->bpropUpdate( layers[ n_layers-1 ]->expectation,
-                                     target, final_cost_value[0],
-                                     expectation_gradients[n_layers-1],
-                                     true);
+            /*
+            if (minibatch_size > 1) {
+                final_cost->fprop( layers[ n_layers-1 ]->expectations,
+                        targets,
+                        final_cost_values );
+                final_cost->bpropUpdate( layers[ n_layers-1 ]->expectations,
+                        targets, final_cost_values.column(0),
+                        expectations_gradients[n_layers-1],
+                        true);
+            } else {*/
+                final_cost->fprop( layers[ n_layers-1 ]->expectation,
+                        target,
+                        final_cost_value );
+                final_cost->bpropUpdate( layers[ n_layers-1 ]->expectation,
+                        target, final_cost_value[0],
+                        expectation_gradients[n_layers-1],
+                        true);
+            //}
         }
 
         for (int j=0;j<final_cost_indices.length();j++)

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2007-04-18 17:10:20 UTC (rev 6919)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2007-04-18 17:20:39 UTC (rev 6920)
@@ -70,6 +70,8 @@
     //! The learning rate used during the gradient descent
     real grad_learning_rate;
 
+    int batch_size;
+
     //! The decrease constant of the learning rate used during gradient descent
     real grad_decrease_ct;
 
@@ -205,11 +207,22 @@
 
     void greedyStep( const Vec& input, const Vec& target, int index );
 
+    //! TODO Document and implement.
+    void greedyStep( const Mat& inputs, const Mat& targets, int index ) {
+        PLASSERT(false);
+    }
+
     void jointGreedyStep( const Vec& input, const Vec& target );
 
     void fineTuningStep( const Vec& input, const Vec& target,
                          Vec& train_costs );
 
+    //! TODO Document and implement.
+    void fineTuningStep( const Mat& inputs, const Mat& targets,
+                         Vec& train_costs ) {
+        PLASSERT(false);
+    }
+
     void contrastiveDivergenceStep( const PP<RBMLayer>& down_layer,
                                     const PP<RBMConnection>& connection,
                                     const PP<RBMLayer>& up_layer,
@@ -247,23 +260,29 @@
     virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
 protected:
+
+    int minibatch_size;
+    
     //#####  Not Options  #####################################################
 
     //! Stores the gradient of the cost wrt the activations
     //! (at the input of the layers)
     mutable TVec<Vec> activation_gradients;
+    mutable TVec<Mat> activations_gradients; //!< For mini-batch.
 
     //! Stores the gradient of the cost wrt the expectations
     //! (at the output of the layers)
     mutable TVec<Vec> expectation_gradients;
+    mutable TVec<Mat> expectations_gradients; //!< For mini-batch.
 
-    //!
     mutable Vec final_cost_input;
+    mutable Mat final_cost_inputs; //!< For mini-batch.
 
     mutable Vec final_cost_value;
+    mutable Vec final_cost_values; //!< For mini-batch.
 
     mutable Vec final_cost_output;
-    //!
+
     mutable Vec class_output;
 
     mutable Vec class_gradient;
@@ -272,6 +291,7 @@
 
     //! Stores the gradient of the cost at the input of final_cost
     mutable Vec final_cost_gradient;
+    mutable Mat final_cost_gradients; //!< For mini-batch.
 
     //! buffers bottom layer activation during onlineStep 
     mutable Vec save_layer_activation;



From nouiz at mail.berlios.de  Thu Apr 19 16:43:29 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 19 Apr 2007 16:43:29 +0200
Subject: [Plearn-commits] r6921 - trunk
Message-ID: <200704191443.l3JEhTsj003880@sheep.berlios.de>

Author: nouiz
Date: 2007-04-19 16:43:21 +0200 (Thu, 19 Apr 2007)
New Revision: 6921

Modified:
   trunk/pymake.config.model
Log:
Added -goto as an option. This will link with the goto lib as blas


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2007-04-18 17:20:39 UTC (rev 6920)
+++ trunk/pymake.config.model	2007-04-19 14:43:21 UTC (rev 6921)
@@ -419,7 +419,7 @@
   [ 'python23', 'python24', 'python25' ],
   
   [ 'blas', 'p3blas','p4blas','athlonblas','pentiumblas', 'mammouthblas',
-    'noblas', 'veclib', 'scs'],
+    'noblas', 'veclib', 'scs', 'goto' ],
   
   [ 'logging=dbg', 'logging=mand', 'logging=imp', 'logging=normal', 'logging=extreme' ]
 ]
@@ -684,6 +684,12 @@
               cpp_definitions = ['USE_BLAS_SPECIALISATIONS'],
               linkeroptions = '-lscs -lpthread' )
 
+pymakeOption( name = 'goto',
+              description = 'compilation and linking using GOTO for BLAS',
+              cpp_definitions = ['USE_BLAS_SPECIALISATIONS'],
+              linkeroptions = '-L' + libdir +'goto -llapack -lgoto -lgfortran'
+              )
+
 cpp_variables += ['USE_BLAS_SPECIALISATIONS']
 
 



From tihocan at mail.berlios.de  Thu Apr 19 20:58:24 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 19 Apr 2007 20:58:24 +0200
Subject: [Plearn-commits] r6922 - trunk/plearn_learners/online
Message-ID: <200704191858.l3JIwOpG003679@sheep.berlios.de>

Author: tihocan
Date: 2007-04-19 20:58:24 +0200 (Thu, 19 Apr 2007)
New Revision: 6922

Modified:
   trunk/plearn_learners/online/CostModule.h
Log:
Templates for new methods for minibatch computations

Modified: trunk/plearn_learners/online/CostModule.h
===================================================================
--- trunk/plearn_learners/online/CostModule.h	2007-04-19 14:43:21 UTC (rev 6921)
+++ trunk/plearn_learners/online/CostModule.h	2007-04-19 18:58:24 UTC (rev 6922)
@@ -71,9 +71,16 @@
     //! given the input and target, compute the main output (cost)
     virtual void fprop(const Vec& input, const Vec& target, real& cost ) const;
 
+
     //! this version allows for several costs
     virtual void fprop(const Vec& input, const Vec& target, Vec& cost ) const;
 
+    // TODO Document and implement (sub-classes too).
+    virtual void fprop(const Mat& inputs, const Mat& targets, Mat& costs ) const
+    {
+        PLASSERT( false );
+    }
+
     //! this version is provided for compatibility with the parent class
     virtual void fprop(const Vec& input_and_target, Vec& output) const;
 
@@ -90,6 +97,16 @@
                              const Vec& output_gradient,
                              bool accumulate=false);
 
+
+    // TODO Had to override to compile, this is weird.
+    virtual void bpropUpdate(const Mat& input, const Mat& output,
+                             Mat& input_gradient, const Mat& output_gradient,
+                             bool accumulate=false) 
+    {
+        inherited::bpropUpdate(input, output, input_gradient, output_gradient,
+                accumulate);
+    }
+
     //! Similar to bpropUpdate, but adapt based also on the estimation
     //! of the diagonal of the Hessian matrix, and propagates this back.
     virtual void bbpropUpdate(const Vec& input, const Vec& target, real cost,



From tihocan at mail.berlios.de  Thu Apr 19 20:59:42 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 19 Apr 2007 20:59:42 +0200
Subject: [Plearn-commits] r6923 - trunk/plearn_learners/online
Message-ID: <200704191859.l3JIxg3O003762@sheep.berlios.de>

Author: tihocan
Date: 2007-04-19 20:59:42 +0200 (Thu, 19 Apr 2007)
New Revision: 6923

Modified:
   trunk/plearn_learners/online/RBMConnection.h
Log:
Templates for new methods for minibatch computations

Modified: trunk/plearn_learners/online/RBMConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMConnection.h	2007-04-19 18:58:24 UTC (rev 6922)
+++ trunk/plearn_learners/online/RBMConnection.h	2007-04-19 18:59:42 UTC (rev 6923)
@@ -100,11 +100,22 @@
     //! afterwards.
     virtual void setAsUpInput( const Vec& input ) const;
 
+    // TODO Implement, document, add to subclasses.
+    virtual void setAsUpInputs( const Mat& inputs ) const
+    {
+        PLASSERT( false );
+    }
+
     //! Sets input_vec to input, and going_up to true
     //! Note that no data copy is made, so input should not be modified
     //! afterwards.
     virtual void setAsDownInput( const Vec& input ) const;
 
+    // TODO Implement, document, add to subclasses.
+    virtual void setAsDownInputs( const Mat& inputs ) const {
+        PLASSERT( false );
+    }
+
     //! Accumulates positive phase statistics to *_pos_stats
     virtual void accumulatePosStats( const Vec& down_values,
                                      const Vec& up_values ) = 0;
@@ -124,6 +135,15 @@
                          const Vec& neg_down_values,
                          const Vec& neg_up_values);
 
+    // TODO Implement (in sub-classes too).
+    virtual void update( const Mat& pos_down_values,
+                         const Mat& pos_up_values,
+                         const Mat& neg_down_values,
+                         const Mat& neg_up_values)
+    {
+        PLASSERT( false );
+    }
+
     //! Clear all information accumulated during stats
     virtual void clearStats() = 0;
 



From tihocan at mail.berlios.de  Thu Apr 19 21:01:14 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 19 Apr 2007 21:01:14 +0200
Subject: [Plearn-commits] r6924 - trunk/plearn_learners/online
Message-ID: <200704191901.l3JJ1EZP003901@sheep.berlios.de>

Author: tihocan
Date: 2007-04-19 21:01:14 +0200 (Thu, 19 Apr 2007)
New Revision: 6924

Modified:
   trunk/plearn_learners/online/RBMLayer.h
Log:
Templates for new methods for minibatch computations

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2007-04-19 18:59:42 UTC (rev 6923)
+++ trunk/plearn_learners/online/RBMLayer.h	2007-04-19 19:01:14 UTC (rev 6924)
@@ -124,6 +124,11 @@
     //! generate a sample, and update the sample field
     virtual void generateSample() = 0 ;
 
+    //! Generate a given number of samples, stored in the 'samples' field.
+    void generateSamples(int n_samples) {
+        PLASSERT(false);
+    }
+
     //! compute the expectation
     virtual void computeExpectation() = 0 ;
 
@@ -143,6 +148,13 @@
                              const Vec& output_gradient,
                              bool accumulate=false) = 0 ;
 
+    virtual void bpropUpdate(const Mat& inputs, const Mat& outputs,
+                             Mat& input_gradients,
+                             const Mat& output_gradients,
+                             bool accumulate=false) {
+        PLASSERT( false );
+    }
+
     //! back-propagates the output gradient to the input and the bias
     virtual void bpropUpdate(const Vec& input, const Vec& rbm_bias,
                              const Vec& output,
@@ -169,6 +181,11 @@
     //! Update parameters according to one pair of vectors
     virtual void update( const Vec& pos_values, const Vec& neg_values );
 
+    // TODO Implement (in sub-classes too).
+    virtual void update( const Mat& pos_values, const Mat& neg_values ) {
+        PLASSERT( false );
+    }
+
     //! resets activations, sample and expectation fields
     virtual void reset();
 



From tihocan at mail.berlios.de  Thu Apr 19 21:10:15 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 19 Apr 2007 21:10:15 +0200
Subject: [Plearn-commits] r6925 - trunk/plearn_learners/online
Message-ID: <200704191910.l3JJAFGP004409@sheep.berlios.de>

Author: tihocan
Date: 2007-04-19 21:10:14 +0200 (Thu, 19 Apr 2007)
New Revision: 6925

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
Log:
More work towards mini-batches


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-04-19 19:01:14 UTC (rev 6924)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-04-19 19:10:14 UTC (rev 6925)
@@ -475,8 +475,8 @@
     deepCopyField(final_cost_gradients,     copies);
     deepCopyField(save_layer_activation,    copies);
     deepCopyField(save_layer_expectation,   copies);
-    deepCopyField(pos_down_values,          copies);
-    deepCopyField(pos_up_values,            copies);
+    deepCopyField(pos_down_val,          copies);
+    deepCopyField(pos_up_val,            copies);
     deepCopyField(final_cost_indices,       copies);
     deepCopyField(partial_cost_indices,     copies);
 }
@@ -787,13 +787,17 @@
             if (minibatch_size > 1) {
                 train_set->getExamples(sample_start, minibatch_size, inputs,
                         targets, weights);
-                fineTuningStep(inputs, targets, train_costs);
+                fineTuningStep(inputs, targets, train_costs_m);
             } else {
                 train_set->getExample( sample_start, input, target, weight );
                 fineTuningStep( input, target, train_costs );
             }
             if (update_stats)
-                train_stats->update( train_costs );
+                if (minibatch_size > 1)
+                    for (int k = 0; k < minibatch_size; k++)
+                        train_stats->update(train_costs_m(k));
+                else
+                    train_stats->update( train_costs );
 
             if( pb )
                 pb->update( stage - init_stage + 1 );
@@ -807,7 +811,6 @@
 
     train_stats->finalize();
 
-    // TODO Conversion to mini-batch: CONTINUE HERE
 }
 
 ////////////////
@@ -816,6 +819,8 @@
 void DeepBeliefNet::onlineStep( const Vec& input, const Vec& target,
                                 Vec& train_costs)
 {
+    PLASSERT_MSG(batch_size == 1, "Not implemented for mini-batches");
+
     TVec<Vec> cost;
     if (partial_costs)
         cost.resize(n_layers-1);
@@ -1048,6 +1053,9 @@
 
 }
 
+////////////////
+// greedyStep //
+////////////////
 void DeepBeliefNet::greedyStep( const Vec& input, const Vec& target, int index )
 {
     PLASSERT( index < n_layers );
@@ -1098,9 +1106,69 @@
                                true );
 }
 
+/////////////////
+// greedySteps //
+/////////////////
+void DeepBeliefNet::greedyStep( const Mat& inputs, const Mat& targets, int index )
+{
+    PLASSERT( index < n_layers );
+
+    layers[0]->expectations << inputs;
+    for( int i=0 ; i<=index ; i++ )
+    {
+        connections[i]->setAsDownInputs( layers[i]->expectations );
+        layers[i+1]->getAllActivations( connections[i] );
+        layers[i+1]->computeExpectation(); // TODO Ensure it fills expectations
+    }
+
+    // TODO: add another learning rate?
+    if( partial_costs && partial_costs[ index ] )
+    {
+        PLASSERT_MSG(batch_size == 1, "Not implemented for mini-batches");
+        /*
+        // put appropriate learning rate
+        connections[ index ]->setLearningRate( grad_learning_rate );
+        layers[ index+1 ]->setLearningRate( grad_learning_rate );
+
+        // Backward pass
+        real cost;
+        partial_costs[ index ]->fprop( layers[ index+1 ]->expectation,
+                                       target, cost );
+
+        partial_costs[ index ]->bpropUpdate( layers[ index+1 ]->expectation,
+                                             target, cost,
+                                             expectation_gradients[ index+1 ]
+                                             );
+
+        layers[ index+1 ]->bpropUpdate( layers[ index+1 ]->activation,
+                                        layers[ index+1 ]->expectation,
+                                        activation_gradients[ index+1 ],
+                                        expectation_gradients[ index+1 ] );
+
+        connections[ index ]->bpropUpdate( layers[ index ]->expectation,
+                                           layers[ index+1 ]->activation,
+                                           expectation_gradients[ index ],
+                                           activation_gradients[ index+1 ] );
+
+        // put back old learning rate
+        connections[ index ]->setLearningRate( cd_learning_rate );
+        layers[ index+1 ]->setLearningRate( cd_learning_rate );
+        */
+    }
+
+    contrastiveDivergenceStep( layers[ index ],
+                               connections[ index ],
+                               layers[ index+1 ],
+                               true );
+}
+
+/////////////////////
+// jointGreedyStep //
+/////////////////////
 void DeepBeliefNet::jointGreedyStep( const Vec& input, const Vec& target )
 {
     PLASSERT( joint_layer );
+    PLASSERT_MSG(batch_size == 1, "Not implemented for mini-batches");
 
     layers[0]->expectation << input;
     for( int i=0 ; i<n_layers-2 ; i++ )
@@ -1157,6 +1225,9 @@
         layers[ n_layers-1 ] );
 }
 
+////////////////////
+// fineTuningStep //
+////////////////////
 void DeepBeliefNet::fineTuningStep( const Vec& input, const Vec& target,
                                     Vec& train_costs )
 {
@@ -1217,7 +1288,7 @@
             activation_gradients[ n_layers-1 ] );
     }
     else  {
-        expectation_gradients[ n_layers-2 ]->clear();
+        expectation_gradients[ n_layers-2 ].clear();
     }
 
     if( use_classification_cost )
@@ -1263,53 +1334,216 @@
     }
 }
 
-// assumes that down_layer->expectation is set
+////////////////////
+// fineTuningStep //
+////////////////////
+void DeepBeliefNet::fineTuningStep(const Mat& inputs, const Mat& targets,
+                                   Mat& train_costs )
+{
+    final_cost_values.resize(0, 0);
+    // fprop
+    layers[0]->expectations << inputs;
+    for( int i=0 ; i<n_layers-2 ; i++ )
+    {
+        connections[i]->setAsDownInputs( layers[i]->expectations );
+        layers[i+1]->getAllActivations( connections[i] );
+        layers[i+1]->computeExpectation(); // TODO Ensure it fills expectations
+    }
+
+    if( final_cost )
+    {
+        connections[ n_layers-2 ]->setAsDownInputs(
+            layers[ n_layers-2 ]->expectations );
+        // TODO Also ensure getAllActivations fills everything.
+        layers[ n_layers-1 ]->getAllActivations( connections[ n_layers-2 ] );
+        layers[ n_layers-1 ]->computeExpectation();
+
+        if( final_module )
+        {
+            final_module->fprop( layers[ n_layers-1 ]->expectations,
+                                 final_cost_inputs );
+            final_cost->fprop( final_cost_inputs, targets, final_cost_values );
+
+            Mat optimized_costs = final_cost_values.column(0);
+            final_cost->bpropUpdate( final_cost_inputs, targets,
+                                     optimized_costs,
+                                     final_cost_gradients );
+            final_module->bpropUpdate( layers[ n_layers-1 ]->expectations,
+                                       final_cost_inputs,
+                                       expectations_gradients[ n_layers-1 ],
+                                       final_cost_gradients );
+        }
+        else
+        {
+            final_cost->fprop( layers[ n_layers-1 ]->expectations, targets,
+                               final_cost_values );
+
+            Mat optimized_costs = final_cost_values.column(0);
+            final_cost->bpropUpdate( layers[ n_layers-1 ]->expectations,
+                                     targets, optimized_costs,
+                                     expectations_gradients[ n_layers-1 ] );
+        }
+
+        for (int k = 0; k < minibatch_size; k++)
+            for (int j=0;j<final_cost_indices.length();j++)
+                train_costs(k, final_cost_indices[j]) =
+                    final_cost_values(k, j);
+
+        layers[ n_layers-1 ]->bpropUpdate( layers[ n_layers-1 ]->activations,
+                                           layers[ n_layers-1 ]->expectations,
+                                           activations_gradients[ n_layers-1 ],
+                                           expectations_gradients[ n_layers-1 ]
+                                         );
+
+        connections[ n_layers-2 ]->bpropUpdate(
+            layers[ n_layers-2 ]->expectations,
+            layers[ n_layers-1 ]->activations,
+            expectations_gradients[ n_layers-2 ],
+            activations_gradients[ n_layers-1 ] );
+    }
+    else  {
+        expectations_gradients[ n_layers-2 ].clear();
+    }
+
+    if( use_classification_cost )
+    {
+        PLASSERT_MSG(batch_size == 1, "Not implemented for mini-batches");
+        /*
+        classification_module->fprop( layers[ n_layers-2 ]->expectation,
+                                      class_output );
+        real nll_cost;
+
+        // This doesn't work. gcc bug?
+        // classification_cost->fprop( class_output, target, cost );
+        classification_cost->CostModule::fprop( class_output, target,
+                                                nll_cost );
+
+        real class_error =
+            ( argmax(class_output) == (int) round(target[0]) ) ? 0
+                                                               : 1;
+
+        train_costs[nll_cost_index] = nll_cost;
+        train_costs[class_cost_index] = class_error;
+
+        classification_cost->bpropUpdate( class_output, target, nll_cost,
+                                          class_gradient );
+
+        classification_module->bpropUpdate( layers[ n_layers-2 ]->expectation,
+                                            class_output,
+                                            class_input_gradient,
+                                            class_gradient );
+
+        expectation_gradients[n_layers-2] += class_input_gradient;
+        */
+    }
+
+    for( int i=n_layers-2 ; i>0 ; i-- )
+    {
+        layers[i]->bpropUpdate( layers[i]->activations,
+                                layers[i]->expectations,
+                                activations_gradients[i],
+                                expectations_gradients[i] );
+
+        connections[i-1]->bpropUpdate( layers[i-1]->expectations,
+                                       layers[i]->activations,
+                                       expectations_gradients[i-1],
+                                       activations_gradients[i] );
+    }
+}
+
+///////////////////////////////
+// contrastiveDivergenceStep //
+///////////////////////////////
 void DeepBeliefNet::contrastiveDivergenceStep(
     const PP<RBMLayer>& down_layer,
     const PP<RBMConnection>& connection,
     const PP<RBMLayer>& up_layer,
     bool nofprop)
 {
+    bool mbatch = minibatch_size > 1;
+
     // positive phase
     if (!nofprop)
     {
-        connection->setAsDownInput( down_layer->expectation );
+        if (mbatch)
+            connection->setAsDownInputs( down_layer->expectations );
+        else
+            connection->setAsDownInput( down_layer->expectation );
         up_layer->getAllActivations( connection );
         up_layer->computeExpectation();
     }
-    up_layer->generateSample();
 
-    // accumulate positive stats using the expectation
-    // we deep-copy because the value will change during negative phase
-    pos_down_values.resize( down_layer->size );
-    pos_up_values.resize( up_layer->size );
+    if (mbatch) {
+        up_layer->generateSamples(minibatch_size);
 
-    pos_down_values << down_layer->expectation;
-    pos_up_values << up_layer->expectation;
+        // accumulate positive stats using the expectation
+        // we deep-copy because the value will change during negative phase
+        pos_down_vals.resize(minibatch_size, down_layer->size);
+        pos_up_vals.resize(minibatch_size, up_layer->size);
 
-    // down propagation, starting from a sample of up_layer
-    connection->setAsUpInput( up_layer->sample );
+        pos_down_vals << down_layer->expectations;
+        pos_up_vals << up_layer->expectations;
+
+        // down propagation, starting from a sample of up_layer
+        connection->setAsUpInputs( up_layer->samples );
+    } else {
+        up_layer->generateSample();
+
+        // accumulate positive stats using the expectation
+        // we deep-copy because the value will change during negative phase
+        pos_down_val.resize( down_layer->size );
+        pos_up_val.resize( up_layer->size );
+
+        pos_down_val << down_layer->expectation;
+        pos_up_val << up_layer->expectation;
+
+        // down propagation, starting from a sample of up_layer
+        connection->setAsUpInput( up_layer->sample );
+    }
+
     down_layer->getAllActivations( connection );
-    down_layer->generateSample();
+    if (mbatch) {
+        down_layer->generateSamples(minibatch_size);
+        // negative phase
+        connection->setAsDownInputs( down_layer->samples );
+    } else {
+        down_layer->generateSample();
+        // negative phase
+        connection->setAsDownInput( down_layer->sample );
+    }
 
-    // negative phase
-    connection->setAsDownInput( down_layer->sample );
     up_layer->getAllActivations( connection );
     up_layer->computeExpectation();
 
-    // accumulate negative stats
-    // no need to deep-copy because the values won't change before update
-    Vec neg_down_values = down_layer->sample;
-    Vec neg_up_values = up_layer->expectation;
+    if (mbatch) {
+        // accumulate negative stats
+        // no need to deep-copy because the values won't change before update
+        Mat neg_down_vals = down_layer->samples;
+        Mat neg_up_vals = up_layer->expectations;
 
-    // update
-    down_layer->update( pos_down_values, neg_down_values );
-    connection->update( pos_down_values, pos_up_values,
-                        neg_down_values, neg_up_values );
-    up_layer->update( pos_up_values, neg_up_values );
+        // update
+        down_layer->update( pos_down_vals, neg_down_vals );
+        connection->update( pos_down_vals, pos_up_vals,
+                neg_down_vals, neg_up_vals );
+        up_layer->update( pos_up_vals, neg_up_vals );
+    } else {
+        // accumulate negative stats
+        // no need to deep-copy because the values won't change before update
+        Vec neg_down_val = down_layer->sample;
+        Vec neg_up_val = up_layer->expectation;
+
+        // update
+        down_layer->update( pos_down_val, neg_down_val );
+        connection->update( pos_down_val, pos_up_val,
+                neg_down_val, neg_up_val );
+        up_layer->update( pos_up_val, neg_up_val );
+    }
 }
 
 
+///////////////////
+// computeOutput //
+///////////////////
 void DeepBeliefNet::computeOutput(const Vec& input, Vec& output) const
 {
     // Compute the output from the input.

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2007-04-19 19:01:14 UTC (rev 6924)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2007-04-19 19:10:14 UTC (rev 6925)
@@ -207,22 +207,20 @@
 
     void greedyStep( const Vec& input, const Vec& target, int index );
 
-    //! TODO Document and implement.
-    void greedyStep( const Mat& inputs, const Mat& targets, int index ) {
-        PLASSERT(false);
-    }
+    //! Greedy step with mini-batches.
+    void greedyStep(const Mat& inputs, const Mat& targets, int index);
 
     void jointGreedyStep( const Vec& input, const Vec& target );
 
     void fineTuningStep( const Vec& input, const Vec& target,
                          Vec& train_costs );
 
-    //! TODO Document and implement.
+    //! Fine tuning step with mini-batches.
     void fineTuningStep( const Mat& inputs, const Mat& targets,
-                         Vec& train_costs ) {
-        PLASSERT(false);
-    }
+                         Mat& train_costs );
 
+    //! Perform a step of contrastive divergence, assuming that
+    //! down_layer->expectation(s) is set.
     void contrastiveDivergenceStep( const PP<RBMLayer>& down_layer,
                                     const PP<RBMConnection>& connection,
                                     const PP<RBMLayer>& up_layer,
@@ -279,7 +277,7 @@
     mutable Mat final_cost_inputs; //!< For mini-batch.
 
     mutable Vec final_cost_value;
-    mutable Vec final_cost_values; //!< For mini-batch.
+    mutable Mat final_cost_values; //!< For mini-batch.
 
     mutable Vec final_cost_output;
 
@@ -306,8 +304,10 @@
     bool final_cost_has_learning_rate;
 
     //! Store a copy of the positive phase values
-    mutable Vec pos_down_values;
-    mutable Vec pos_up_values;
+    mutable Vec pos_down_val;
+    mutable Vec pos_up_val;
+    mutable Mat pos_down_vals;
+    mutable Mat pos_up_vals;
 
     //! Keeps the index of the NLL cost in train_costs
     int nll_cost_index;



From tihocan at mail.berlios.de  Fri Apr 20 21:30:23 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 20 Apr 2007 21:30:23 +0200
Subject: [Plearn-commits] r6926 - trunk/plearn_learners/online
Message-ID: <200704201930.l3KJUNxC009199@sheep.berlios.de>

Author: tihocan
Date: 2007-04-20 21:30:22 +0200 (Fri, 20 Apr 2007)
New Revision: 6926

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/RBMBinomialLayer.cc
   trunk/plearn_learners/online/RBMBinomialLayer.h
   trunk/plearn_learners/online/RBMConnection.cc
   trunk/plearn_learners/online/RBMConnection.h
   trunk/plearn_learners/online/RBMGaussianLayer.h
   trunk/plearn_learners/online/RBMLayer.cc
   trunk/plearn_learners/online/RBMLayer.h
   trunk/plearn_learners/online/RBMMatrixConnection.cc
   trunk/plearn_learners/online/RBMMatrixConnection.h
   trunk/plearn_learners/online/RBMMixedConnection.h
   trunk/plearn_learners/online/RBMMixedLayer.cc
   trunk/plearn_learners/online/RBMMixedLayer.h
   trunk/plearn_learners/online/RBMMultinomialLayer.h
   trunk/plearn_learners/online/RBMTruncExpLayer.h
Log:
- More coherent interpretation of the nstages option in DeepBeliefNet
- More work towards a mini-batch implementation of deep belief nets


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-04-19 19:10:14 UTC (rev 6925)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-04-20 19:30:22 UTC (rev 6926)
@@ -661,16 +661,19 @@
 
             for( ; stage<end_stage ; stage++ )
             {
-                int sample_start = (stage * minibatch_size) % nsamples;
-                if (batch_size > 1) {
-                    train_set->getExamples(sample_start, minibatch_size,
-                            inputs, targets, weights);
-                    greedyStep( inputs, targets, i );
-                } else {
-                    train_set->getExample(sample_start, input, target, weight);
-                    greedyStep( input, target, i );
+                // Do a step every 'minibatch_size' examples.
+                if (stage % minibatch_size == 0) {
+                    int sample_start = stage % nsamples;
+                    if (batch_size > 1) {
+                        train_set->getExamples(sample_start, minibatch_size,
+                                inputs, targets, weights);
+                        greedyStep( inputs, targets, i );
+                    } else {
+                        train_set->getExample(sample_start, input, target, weight);
+                        greedyStep( input, target, i );
+                    }
+
                 }
-
                 if( pb )
                     if( i == 0 )
                         pb->update( stage + 1 );
@@ -726,6 +729,8 @@
         for(int train_index = 0 ; train_index < nsamples ; train_index++)
         {
 
+            PLASSERT_MSG(batch_size == 1, "Not implemented for mini-batches");
+
             train_set->getExample( train_index, input, target, weight );
 
             down_layer->expectation << input;
@@ -774,31 +779,33 @@
         bool update_stats = false;
         for( ; stage<nstages ; stage++ )
         {
-            int sample_start = (stage * minibatch_size) % nsamples;
-            // Only update train statistics for the last 'epoch', i.e. last
-            // 'nsamples' seen.
-            update_stats = update_stats ||
-                stage >= (nstages - nsamples / minibatch_size);
+            // Update every 'minibatch_size' samples.
+            if (stage % minibatch_size == 0) {
+                int sample_start = stage % nsamples;
+                // Only update train statistics for the last 'epoch', i.e. last
+                // 'nsamples' seen.
+                update_stats = update_stats || stage >= nstages - nsamples;
 
-            if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
-                setLearningRate( grad_learning_rate
-                                 / (1. + grad_decrease_ct * (stage - init_stage) ) );
+                if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
+                    setLearningRate( grad_learning_rate
+                            / (1. + grad_decrease_ct * (stage - init_stage) ) );
 
-            if (minibatch_size > 1) {
-                train_set->getExamples(sample_start, minibatch_size, inputs,
-                        targets, weights);
-                fineTuningStep(inputs, targets, train_costs_m);
-            } else {
-                train_set->getExample( sample_start, input, target, weight );
-                fineTuningStep( input, target, train_costs );
+                if (minibatch_size > 1) {
+                    train_set->getExamples(sample_start, minibatch_size, inputs,
+                            targets, weights);
+                    fineTuningStep(inputs, targets, train_costs_m);
+                } else {
+                    train_set->getExample( sample_start, input, target, weight );
+                    fineTuningStep( input, target, train_costs );
+                }
+                if (update_stats)
+                    if (minibatch_size > 1)
+                        for (int k = 0; k < minibatch_size; k++)
+                            train_stats->update(train_costs_m(k));
+                    else
+                        train_stats->update( train_costs );
+
             }
-            if (update_stats)
-                if (minibatch_size > 1)
-                    for (int k = 0; k < minibatch_size; k++)
-                        train_stats->update(train_costs_m(k));
-                else
-                    train_stats->update( train_costs );
-
             if( pb )
                 pb->update( stage - init_stage + 1 );
         }
@@ -881,7 +888,7 @@
         {
             /*
             if (minibatch_size > 1) {
-                final_module->fprop( layers[ n_layers-1 ]->expectations,
+                final_module->fprop( layers[ n_layers-1 ]->getExpectations(),
                         final_cost_inputs );
                 final_cost->fprop( final_cost_inputs, targets,
                         final_cost_values );
@@ -890,7 +897,7 @@
                         final_cost_gradients );
 
                 final_module->bpropUpdate(
-                        layers[ n_layers-1 ]->expectations,
+                        layers[ n_layers-1 ]->getExpectations(),
                         final_cost_inputs,
                         expectations_gradients[ n_layers-1 ],
                         final_cost_gradients, true );
@@ -914,10 +921,10 @@
         {
             /*
             if (minibatch_size > 1) {
-                final_cost->fprop( layers[ n_layers-1 ]->expectations,
+                final_cost->fprop( layers[ n_layers-1 ]->getExpectations(),
                         targets,
                         final_cost_values );
-                final_cost->bpropUpdate( layers[ n_layers-1 ]->expectations,
+                final_cost->bpropUpdate( layers[ n_layers-1 ]->getExpectations(),
                         targets, final_cost_values.column(0),
                         expectations_gradients[n_layers-1],
                         true);
@@ -1113,12 +1120,12 @@
 {
     PLASSERT( index < n_layers );
 
-    layers[0]->expectations << inputs;
+    layers[0]->setExpectations(inputs);
     for( int i=0 ; i<=index ; i++ )
     {
-        connections[i]->setAsDownInputs( layers[i]->expectations );
-        layers[i+1]->getAllActivations( connections[i] );
-        layers[i+1]->computeExpectation(); // TODO Ensure it fills expectations
+        connections[i]->setAsDownInputs( layers[i]->getExpectations() );
+        layers[i+1]->getAllActivations( connections[i], 0, true );
+        layers[i+1]->computeExpectations();
     }
 
     // TODO: add another learning rate?
@@ -1342,25 +1349,26 @@
 {
     final_cost_values.resize(0, 0);
     // fprop
-    layers[0]->expectations << inputs;
+    layers[0]->getExpectations() << inputs;
     for( int i=0 ; i<n_layers-2 ; i++ )
     {
-        connections[i]->setAsDownInputs( layers[i]->expectations );
-        layers[i+1]->getAllActivations( connections[i] );
-        layers[i+1]->computeExpectation(); // TODO Ensure it fills expectations
+        connections[i]->setAsDownInputs( layers[i]->getExpectations() );
+        layers[i+1]->getAllActivations( connections[i], 0, true );
+        layers[i+1]->computeExpectations();
     }
 
     if( final_cost )
     {
         connections[ n_layers-2 ]->setAsDownInputs(
-            layers[ n_layers-2 ]->expectations );
+            layers[ n_layers-2 ]->getExpectations() );
         // TODO Also ensure getAllActivations fills everything.
-        layers[ n_layers-1 ]->getAllActivations( connections[ n_layers-2 ] );
-        layers[ n_layers-1 ]->computeExpectation();
+        layers[ n_layers-1 ]->getAllActivations(connections[n_layers-2],
+                                                0, true);
+        layers[ n_layers-1 ]->computeExpectations();
 
         if( final_module )
         {
-            final_module->fprop( layers[ n_layers-1 ]->expectations,
+            final_module->fprop( layers[ n_layers-1 ]->getExpectations(),
                                  final_cost_inputs );
             final_cost->fprop( final_cost_inputs, targets, final_cost_values );
 
@@ -1368,18 +1376,18 @@
             final_cost->bpropUpdate( final_cost_inputs, targets,
                                      optimized_costs,
                                      final_cost_gradients );
-            final_module->bpropUpdate( layers[ n_layers-1 ]->expectations,
+            final_module->bpropUpdate( layers[ n_layers-1 ]->getExpectations(),
                                        final_cost_inputs,
                                        expectations_gradients[ n_layers-1 ],
                                        final_cost_gradients );
         }
         else
         {
-            final_cost->fprop( layers[ n_layers-1 ]->expectations, targets,
+            final_cost->fprop( layers[ n_layers-1 ]->getExpectations(), targets,
                                final_cost_values );
 
             Mat optimized_costs = final_cost_values.column(0);
-            final_cost->bpropUpdate( layers[ n_layers-1 ]->expectations,
+            final_cost->bpropUpdate( layers[ n_layers-1 ]->getExpectations(),
                                      targets, optimized_costs,
                                      expectations_gradients[ n_layers-1 ] );
         }
@@ -1390,13 +1398,13 @@
                     final_cost_values(k, j);
 
         layers[ n_layers-1 ]->bpropUpdate( layers[ n_layers-1 ]->activations,
-                                           layers[ n_layers-1 ]->expectations,
+                                           layers[ n_layers-1 ]->getExpectations(),
                                            activations_gradients[ n_layers-1 ],
                                            expectations_gradients[ n_layers-1 ]
                                          );
 
         connections[ n_layers-2 ]->bpropUpdate(
-            layers[ n_layers-2 ]->expectations,
+            layers[ n_layers-2 ]->getExpectations(),
             layers[ n_layers-1 ]->activations,
             expectations_gradients[ n_layers-2 ],
             activations_gradients[ n_layers-1 ] );
@@ -1440,11 +1448,11 @@
     for( int i=n_layers-2 ; i>0 ; i-- )
     {
         layers[i]->bpropUpdate( layers[i]->activations,
-                                layers[i]->expectations,
+                                layers[i]->getExpectations(),
                                 activations_gradients[i],
                                 expectations_gradients[i] );
 
-        connections[i-1]->bpropUpdate( layers[i-1]->expectations,
+        connections[i-1]->bpropUpdate( layers[i-1]->getExpectations(),
                                        layers[i]->activations,
                                        expectations_gradients[i-1],
                                        activations_gradients[i] );
@@ -1465,27 +1473,36 @@
     // positive phase
     if (!nofprop)
     {
-        if (mbatch)
-            connection->setAsDownInputs( down_layer->expectations );
-        else
+        if (mbatch) {
+            connection->setAsDownInputs( down_layer->getExpectations() );
+            up_layer->getAllActivations( connection, 0, true );
+            up_layer->computeExpectations();
+        } else {
             connection->setAsDownInput( down_layer->expectation );
-        up_layer->getAllActivations( connection );
-        up_layer->computeExpectation();
+            up_layer->getAllActivations( connection );
+            up_layer->computeExpectation();
+        }
     }
 
     if (mbatch) {
-        up_layer->generateSamples(minibatch_size);
+        up_layer->generateSamples();
 
         // accumulate positive stats using the expectation
         // we deep-copy because the value will change during negative phase
         pos_down_vals.resize(minibatch_size, down_layer->size);
         pos_up_vals.resize(minibatch_size, up_layer->size);
 
-        pos_down_vals << down_layer->expectations;
-        pos_up_vals << up_layer->expectations;
+        pos_down_vals << down_layer->getExpectations();
+        pos_up_vals << up_layer->getExpectations();
 
         // down propagation, starting from a sample of up_layer
         connection->setAsUpInputs( up_layer->samples );
+
+        down_layer->getAllActivations( connection, 0, true );
+
+        down_layer->generateSamples();
+        // negative phase
+        connection->setAsDownInputs( down_layer->samples );
     } else {
         up_layer->generateSample();
 
@@ -1499,27 +1516,25 @@
 
         // down propagation, starting from a sample of up_layer
         connection->setAsUpInput( up_layer->sample );
-    }
 
-    down_layer->getAllActivations( connection );
-    if (mbatch) {
-        down_layer->generateSamples(minibatch_size);
-        // negative phase
-        connection->setAsDownInputs( down_layer->samples );
-    } else {
+        down_layer->getAllActivations( connection );
+
         down_layer->generateSample();
         // negative phase
         connection->setAsDownInput( down_layer->sample );
     }
 
-    up_layer->getAllActivations( connection );
-    up_layer->computeExpectation();
+    up_layer->getAllActivations( connection, 0, mbatch );
+    if (mbatch)
+        up_layer->computeExpectations();
+    else
+        up_layer->computeExpectation();
 
     if (mbatch) {
         // accumulate negative stats
         // no need to deep-copy because the values won't change before update
         Mat neg_down_vals = down_layer->samples;
-        Mat neg_up_vals = up_layer->expectations;
+        Mat neg_up_vals = up_layer->getExpectations();
 
         // update
         down_layer->update( pos_down_vals, neg_down_vals );

Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-04-19 19:10:14 UTC (rev 6925)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-04-20 19:30:22 UTC (rev 6926)
@@ -67,26 +67,9 @@
     bias_neg_stats.resize( the_size );
 }
 
-/*
-//! Uses "rbmp" to obtain the activations of unit "i" of this layer.
-//! This activation vector is computed by the "i+offset"-th unit of "rbmp"
-void RBMBinomialLayer::getUnitActivations( int i, PP<RBMParameters> rbmp,
-                                           int offset )
-{
-    Vec activation = activations.subVec( i, 1 );
-    rbmp->computeUnitActivations( i+offset, 1, activation );
-    expectation_is_up_to_date = false;
-}
-
-//! Uses "rbmp" to obtain the activations of all units in this layer.
-//! Unit 0 of this layer corresponds to unit "offset" of "rbmp".
-void RBMBinomialLayer::getAllActivations( PP<RBMParameters> rbmp, int offset )
-{
-    rbmp->computeUnitActivations( offset, size, activations );
-    expectation_is_up_to_date = false;
-}
-*/
-
+////////////////////
+// generateSample //
+////////////////////
 void RBMBinomialLayer::generateSample()
 {
     computeExpectation();
@@ -95,6 +78,23 @@
         sample[i] = random_gen->binomial_sample( expectation[i] );
 }
 
+/////////////////////
+// generateSamples //
+/////////////////////
+void RBMBinomialLayer::generateSamples()
+{
+    computeExpectations();
+    int mbatch_size = expectations.length();
+    samples.resize(mbatch_size, size);
+
+    for (int k = 0; k < mbatch_size; k++)
+        for (int i=0 ; i<size ; i++)
+            samples(k, i) = random_gen->binomial_sample( expectations(k, i) );
+}
+
+////////////////////////
+// computeExpectation //
+////////////////////////
 void RBMBinomialLayer::computeExpectation()
 {
     if( expectation_is_up_to_date )
@@ -106,6 +106,26 @@
     expectation_is_up_to_date = true;
 }
 
+/////////////////////////
+// computeExpectations //
+/////////////////////////
+void RBMBinomialLayer::computeExpectations()
+{
+    if( expectations_are_up_to_date )
+        return;
+
+    int mbatch_size = activations.length();
+    expectations.resize(mbatch_size, size);
+    for (int k = 0; k < mbatch_size; k++)
+        for (int i = 0 ; i < size ; i++)
+            expectations(k, i) = sigmoid(-activations(k, i));
+
+    expectations_are_up_to_date = true;
+}
+
+///////////
+// fprop //
+///////////
 void RBMBinomialLayer::fprop( const Vec& input, Vec& output ) const
 {
     PLASSERT( input.size() == input_size );

Modified: trunk/plearn_learners/online/RBMBinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.h	2007-04-19 19:10:14 UTC (rev 6925)
+++ trunk/plearn_learners/online/RBMBinomialLayer.h	2007-04-20 19:30:22 UTC (rev 6926)
@@ -67,24 +67,18 @@
     //! Constructor from the number of units
     RBMBinomialLayer( int the_size, real the_learning_rate=0. );
 
-
-    // Your other public member functions go here
-
-    //! Uses "rbmp" to obtain the activations of unit "i" of this layer.
-    //! This activation vector is computed by the "i+offset"-th unit of "rbmp"
-//    virtual void getUnitActivations( int i, PP<RBMParameters> rbmp,
-//                                     int offset=0 );
-
-    //! Uses "rbmp" to obtain the activations of all units in this layer.
-    //! Unit 0 of this layer corresponds to unit "offset" of "rbmp".
-//    virtual void getAllActivations( PP<RBMParameters> rbmp, int offset=0 ) ;
-
     //! generate a sample, and update the sample field
     virtual void generateSample() ;
 
-    //! compute the expectation
+    //! Inherited.
+    virtual void generateSamples();
+
+    //! Compute expectation.
     virtual void computeExpectation() ;
 
+    //! Compute mini-batch expectations.
+    virtual void computeExpectations();
+
     //! forward propagation
     virtual void fprop( const Vec& input, Vec& output ) const;
 

Modified: trunk/plearn_learners/online/RBMConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMConnection.cc	2007-04-19 19:10:14 UTC (rev 6925)
+++ trunk/plearn_learners/online/RBMConnection.cc	2007-04-20 19:30:22 UTC (rev 6926)
@@ -156,6 +156,7 @@
     inherited::makeDeepCopyFromShallowCopy(copies);
 
     deepCopyField(input_vec, copies);
+    PLASSERT_MSG(false, "Not fully implemented");
 }
 
 /////////////////////
@@ -184,6 +185,16 @@
     going_up = false;
 }
 
+///////////////////
+// setAsUpInputs //
+///////////////////
+void RBMConnection::setAsUpInputs( const Mat& inputs ) const
+{
+    PLASSERT( inputs.width() == up_size );
+    inputs_mat = inputs;
+    going_up = false;
+}
+
 ////////////////////
 // setAsDownInput //
 ////////////////////
@@ -194,6 +205,16 @@
     going_up = true;
 }
 
+/////////////////////
+// setAsDownInputs //
+/////////////////////
+void RBMConnection::setAsDownInputs( const Mat& inputs ) const
+{
+    PLASSERT( inputs.width() == down_size );
+    inputs_mat = inputs;
+    going_up = true;
+}
+
 ////////////
 // update //
 ////////////

Modified: trunk/plearn_learners/online/RBMConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMConnection.h	2007-04-19 19:10:14 UTC (rev 6925)
+++ trunk/plearn_learners/online/RBMConnection.h	2007-04-20 19:30:22 UTC (rev 6926)
@@ -95,26 +95,25 @@
     //! Sets the momentum
     virtual void setMomentum( real the_momentum );
 
-    //! Sets input_vec to input, and going_up to false.
-    //! Note that no data copy is made, so input should not be modified
+    //! Sets 'input_vec' to 'input', and 'going_up' to false.
+    //! Note that no data copy is made, so 'input' should not be modified
     //! afterwards.
     virtual void setAsUpInput( const Vec& input ) const;
 
-    // TODO Implement, document, add to subclasses.
-    virtual void setAsUpInputs( const Mat& inputs ) const
-    {
-        PLASSERT( false );
-    }
+    //! Set 'inputs_mat' to 'inputs', and 'going_up' to false.
+    //! Note that no data copy is made, so 'inputs' should not be modified
+    //! afterwards.
+    virtual void setAsUpInputs( const Mat& inputs ) const;
 
-    //! Sets input_vec to input, and going_up to true
-    //! Note that no data copy is made, so input should not be modified
+    //! Sets 'input_vec' to 'input', and 'going_up' to true.
+    //! Note that no data copy is made, so 'input' should not be modified
     //! afterwards.
     virtual void setAsDownInput( const Vec& input ) const;
 
-    // TODO Implement, document, add to subclasses.
-    virtual void setAsDownInputs( const Mat& inputs ) const {
-        PLASSERT( false );
-    }
+    //! Set 'inputs_mat' to 'inputs', and 'going_up' to true.
+    //! Note that no data copy is made, so 'inputs' should not be modified
+    //! afterwards.
+    virtual void setAsDownInputs( const Mat& inputs ) const;
 
     //! Accumulates positive phase statistics to *_pos_stats
     virtual void accumulatePosStats( const Vec& down_values,
@@ -154,6 +153,11 @@
                                  const Vec& activations,
                                  bool accumulate=false ) const = 0;
 
+    //! Same as 'computeProduct' but for mini-batches.
+    virtual void computeProducts(int start, int length,
+                                 Mat& activations,
+                                 bool accumulate=false ) const = 0;
+
     //! given the input, compute the output (possibly resize it  appropriately)
     virtual void fprop(const Vec& input, Vec& output) const;
 
@@ -182,14 +186,18 @@
 protected:
     //#####  Not Options  #####################################################
 
-    //! Points to current input vector
+    //! Pointer to current input vector.
     mutable Vec input_vec;
 
+    //! Pointer to current inputs matrix.
+    mutable Mat inputs_mat;
+
     //! Tells if input_vec comes from down (true) or up (false)
     mutable bool going_up;
 
     //! Number of examples accumulated in *_pos_stats
     int pos_count;
+
     //! Number of examples accumulated in *_neg_stats
     int neg_count;
 

Modified: trunk/plearn_learners/online/RBMGaussianLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.h	2007-04-19 19:10:14 UTC (rev 6925)
+++ trunk/plearn_learners/online/RBMGaussianLayer.h	2007-04-20 19:30:22 UTC (rev 6926)
@@ -70,24 +70,15 @@
     RBMGaussianLayer( int the_size, real the_learning_rate=0. );
 
 
-    // Your other public member functions go here
-/*
-    //! Uses "rbmp" to obtain the activations of unit "i" of this layer.
-    //! This activation vector is computed by the "i+offset"-th unit of "rbmp"
-    virtual void getUnitActivations( int i, PP<RBMParameters> rbmp,
-                                     int offset=0 );
-
-    //! Uses "rbmp" to obtain the activations of all units in this layer.
-    //! Unit 0 of this layer corresponds to unit "offset" of "rbmp".
-    virtual void getAllActivations( PP<RBMParameters> rbmp, int offset=0 ) ;
-*/
-
     //! compute a sample, and update the sample field
     virtual void generateSample() ;
 
     //! compute the expectation
     virtual void computeExpectation() ;
 
+    //! Not implemented.
+    virtual void computeExpectations() { PLASSERT( false ); }
+
     //! compute the standard deviation
     virtual void computeStdDeviation() ;
 

Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2007-04-19 19:10:14 UTC (rev 6925)
+++ trunk/plearn_learners/online/RBMLayer.cc	2007-04-20 19:30:22 UTC (rev 6926)
@@ -56,6 +56,7 @@
     momentum(0.),
     size(-1),
     expectation_is_up_to_date(false),
+    expectations_are_up_to_date(false),
     pos_count(0),
     neg_count(0)
 {
@@ -67,6 +68,7 @@
     sample.clear();
     expectation.clear();
     expectation_is_up_to_date = false;
+    expectations_are_up_to_date = false;
 }
 
 void RBMLayer::clearStats()
@@ -130,6 +132,7 @@
     sample.resize( size );
     expectation.resize( size );
     expectation_is_up_to_date = false;
+    expectations_are_up_to_date = false;
 
     bias.resize( size );
     bias_pos_stats.resize( size );
@@ -177,18 +180,41 @@
     rbmc->computeProduct( i+offset, 1, act );
     act[0] += bias[i];
     expectation_is_up_to_date = false;
+    expectations_are_up_to_date = false;
 }
 
-void RBMLayer::getAllActivations( PP<RBMConnection> rbmc, int offset )
+///////////////////////
+// getAllActivations //
+///////////////////////
+void RBMLayer::getAllActivations( PP<RBMConnection> rbmc, int offset,
+                                  bool minibatch)
 {
-    rbmc->computeProduct( offset, size, activation );
-    activation += bias;
+    if (minibatch) {
+        rbmc->computeProducts( offset, size, activations );
+        activations += bias;
+    } else {
+        rbmc->computeProduct( offset, size, activation );
+        activation += bias;
+    }
     expectation_is_up_to_date = false;
+    expectations_are_up_to_date = false;
 }
 
-// unefficient
+
+/////////////////////
+// getExpectations //
+/////////////////////
+Mat& RBMLayer::getExpectations() {
+    return this->expectations;
+}
+
+///////////
+// fprop //
+///////////
 void RBMLayer::fprop( const Vec& input, Vec& output ) const
 {
+    // Note: inefficient.
+
     // Yes it's ugly, blame the const plague
     RBMLayer* This = const_cast<RBMLayer*>(this);
 
@@ -198,6 +224,7 @@
     This->activation << input;
     This->activation += bias;
     This->expectation_is_up_to_date = false;
+    This->expectations_are_up_to_date = false;
 
     output << This->expectation;
 }
@@ -227,18 +254,27 @@
     PLERROR("In RBMLayer::bpropNLL(): not implemented");
 }
 
+////////////////////////
+// accumulatePosStats //
+////////////////////////
 void RBMLayer::accumulatePosStats( const Vec& pos_values )
 {
     bias_pos_stats += pos_values;
     pos_count++;
 }
 
+////////////////////////
+// accumulateNegStats //
+////////////////////////
 void RBMLayer::accumulateNegStats( const Vec& neg_values )
 {
     bias_neg_stats += neg_values;
     neg_count++;
 }
 
+////////////
+// update //
+////////////
 void RBMLayer::update()
 {
     // bias -= learning_rate * (bias_pos_stats/pos_count
@@ -313,6 +349,15 @@
     bias << rbm_bias;
 }
 
+/////////////////////
+// setExpectations //
+/////////////////////
+void RBMLayer::setExpectations(const Mat& the_expectations)
+{
+    expectations.resize(the_expectations.length(), the_expectations.width());
+    expectations << the_expectations;
+}
+
 /////////////
 // bpropCD //
 /////////////

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2007-04-19 19:10:14 UTC (rev 6925)
+++ trunk/plearn_learners/online/RBMLayer.h	2007-04-20 19:30:22 UTC (rev 6926)
@@ -90,14 +90,15 @@
 
     //! Contains the expected value of the random variable in this layer
     Vec expectation;
-    Mat expectations; // for mini-batch operations
 
     //! flags that expectation was computed based on most recently computed
     //! value of activation
     bool expectation_is_up_to_date;
 
+    //! Indicate whether expectations were computed based on most recently
+    //! computed values of activations.
+    bool expectations_are_up_to_date;
 
-
 public:
     //#####  Public Member Functions  #########################################
 
@@ -112,6 +113,12 @@
     //! Sets the momentum
     virtual void setMomentum( real the_momentum );
 
+    //! Copy the given expectations in the 'expectations' matrix.
+    void setExpectations(const Mat& the_expectations);
+
+    //! Accessor to the 'expectations' matrix.
+    Mat& getExpectations();
+
     //! Uses "rbmc" to compute the activation of unit "i" of this layer.
     //! This activation is computed by the "i+offset"-th unit of "rbmc"
     virtual void getUnitActivation( int i, PP<RBMConnection> rbmc,
@@ -119,19 +126,21 @@
 
     //! Uses "rbmc" to obtain the activations of all units in this layer.
     //! Unit 0 of this layer corresponds to unit "offset" of "rbmc".
-    virtual void getAllActivations( PP<RBMConnection> rbmc, int offset=0 );
+    virtual void getAllActivations( PP<RBMConnection> rbmc, int offset = 0,
+                                    bool minibatch = false);
 
     //! generate a sample, and update the sample field
     virtual void generateSample() = 0 ;
 
-    //! Generate a given number of samples, stored in the 'samples' field.
-    void generateSamples(int n_samples) {
-        PLASSERT(false);
-    }
+    //! Generate a mini-batch set of samples.
+    virtual void generateSamples() = 0;
 
-    //! compute the expectation
+    //! Compute expectation.
     virtual void computeExpectation() = 0 ;
 
+    //! Compute expectations (mini-batch).
+    virtual void computeExpectations() = 0 ;
+
     //! Adds the bias to input, consider this as the activation, then compute
     //! the expectation
     virtual void fprop( const Vec& input, Vec& output ) const;
@@ -236,6 +245,9 @@
     //! Count of negative examples
     int neg_count;
 
+    //! Expectations for mini-batch operations.
+    //! It is protected to encourage the use of accessors.
+    Mat expectations;
 
 protected:
     //#####  Protected Member Functions  ######################################

Modified: trunk/plearn_learners/online/RBMMatrixConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.cc	2007-04-19 19:10:14 UTC (rev 6925)
+++ trunk/plearn_learners/online/RBMMatrixConnection.cc	2007-04-20 19:30:22 UTC (rev 6926)
@@ -230,6 +230,9 @@
     }
 }
 
+////////////////
+// clearStats //
+////////////////
 void RBMMatrixConnection::clearStats()
 {
     weights_pos_stats.clear();
@@ -239,6 +242,9 @@
     neg_count = 0;
 }
 
+////////////////////
+// computeProduct //
+////////////////////
 void RBMMatrixConnection::computeProduct( int start, int length,
                                           const Vec& activations,
                                           bool accumulate ) const
@@ -273,7 +279,46 @@
     }
 }
 
-//! this version allows to obtain the input gradient as well
+/////////////////////
+// computeProducts //
+/////////////////////
+void RBMMatrixConnection::computeProducts(int start, int length,
+                                          Mat& activations,
+                                          bool accumulate ) const
+{
+    activations.resize(inputs_mat.length(), length);
+    if( going_up )
+    {
+        PLASSERT( start+length <= up_size );
+        // activations(k, i-start) += sum_j weights(i,j) inputs_mat(k, j)
+
+        if( accumulate )
+            productTransposeAcc(activations,
+                    inputs_mat,
+                    weights.subMatRows(start,length));
+        else
+            productTranspose(activations,
+                    inputs_mat,
+                    weights.subMatRows(start,length));
+    }
+    else
+    {
+        PLASSERT( start+length <= down_size );
+        // activations(k, i-start) += sum_j weights(j,i) inputs_mat(k, j)
+        if( accumulate )
+            transposeProductAcc( activations,
+                                 weights.subMatColumns(start,length),
+                                 inputs_mat );
+        else
+            transposeProduct( activations,
+                              weights.subMatColumns(start,length),
+                              inputs_mat );
+    }
+}
+
+/////////////////
+// bpropUpdate //
+/////////////////
 void RBMMatrixConnection::bpropUpdate(const Vec& input, const Vec& output,
                                       Vec& input_gradient,
                                       const Vec& output_gradient,

Modified: trunk/plearn_learners/online/RBMMatrixConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.h	2007-04-19 19:10:14 UTC (rev 6925)
+++ trunk/plearn_learners/online/RBMMatrixConnection.h	2007-04-20 19:30:22 UTC (rev 6926)
@@ -109,6 +109,11 @@
                                  const Vec& activations,
                                  bool accumulate=false ) const;
 
+    //! Same as 'computeProduct' but for mini-batches.
+    virtual void computeProducts(int start, int length,
+                                 Mat& activations,
+                                 bool accumulate=false ) const;
+
     //! Adapt based on the output gradient: this method should only
     //! be called just after a corresponding fprop; it should be
     //! called with the same arguments as fprop for the first two arguments

Modified: trunk/plearn_learners/online/RBMMixedConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMMixedConnection.h	2007-04-19 19:10:14 UTC (rev 6925)
+++ trunk/plearn_learners/online/RBMMixedConnection.h	2007-04-20 19:30:22 UTC (rev 6926)
@@ -83,6 +83,14 @@
     virtual void setAsUpInput( const Vec& input ) const;
     virtual void setAsDownInput( const Vec& input ) const;
 
+    virtual void setAsUpInputs( const Mat& inputs ) const {
+        PLASSERT( false ); // Not implemented.
+    }
+
+    virtual void setAsDownInputs( const Mat& input ) const {
+        PLASSERT( false ); // Not implemented.
+    }
+
     //! Accumulates positive phase statistics to *_pos_stats
     virtual void accumulatePosStats( const Vec& down_values,
                                      const Vec& up_values );
@@ -100,7 +108,6 @@
                          const Vec& pos_up_values,
                          const Vec& neg_down_values,
                          const Vec& neg_up_values );
-
     //! Clear all information accumulated during stats
     virtual void clearStats();
 
@@ -111,6 +118,14 @@
                                  const Vec& activations,
                                  bool accumulate=false ) const;
 
+    //! Same as 'computeProduct' but for mini-batches.
+    virtual void computeProducts(int start, int length,
+                                 Mat& activations,
+                                 bool accumulate=false ) const
+    {
+        PLASSERT( false ); // Not implemented.
+    }
+
     //! Adapt based on the output gradient: this method should only
     //! be called just after a corresponding fprop; it should be
     //! called with the same arguments as fprop for the first two arguments

Modified: trunk/plearn_learners/online/RBMMixedLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.cc	2007-04-19 19:10:14 UTC (rev 6925)
+++ trunk/plearn_learners/online/RBMMixedLayer.cc	2007-04-20 19:30:22 UTC (rev 6926)
@@ -78,6 +78,9 @@
 }
 
 
+///////////////////////
+// getUnitActivation //
+///////////////////////
 void RBMMixedLayer::getUnitActivation( int i, PP<RBMConnection> rbmc,
                                        int offset )
 {
@@ -87,14 +90,21 @@
     sub_layers[j]->expectation_is_up_to_date = false;
 }
 
-void RBMMixedLayer::getAllActivations( PP<RBMConnection> rbmc, int offset )
+///////////////////////
+// getAllActivations //
+///////////////////////
+void RBMMixedLayer::getAllActivations( PP<RBMConnection> rbmc, int offset,
+                                       bool minibatch )
 {
-    inherited::getAllActivations( rbmc, offset );
+    inherited::getAllActivations( rbmc, offset, minibatch );
     for( int i=0 ; i<n_layers ; i++ )
         sub_layers[i]->expectation_is_up_to_date = false;
 }
 
 
+////////////////////
+// generateSample //
+////////////////////
 void RBMMixedLayer::generateSample()
 {
     for( int i=0 ; i<n_layers ; i++ )

Modified: trunk/plearn_learners/online/RBMMixedLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.h	2007-04-19 19:10:14 UTC (rev 6925)
+++ trunk/plearn_learners/online/RBMMixedLayer.h	2007-04-20 19:30:22 UTC (rev 6926)
@@ -84,14 +84,21 @@
 
     //! Uses "rbmc" to obtain the activations of all units in this layer.
     //! Unit 0 of this layer corresponds to unit "offset" of "rbmc".
-    virtual void getAllActivations( PP<RBMConnection> rbmc, int offset=0 );
+    virtual void getAllActivations( PP<RBMConnection> rbmc, int offset=0,
+                                    bool minibatch = false );
 
     //! compute a sample, and update the sample field
     virtual void generateSample() ;
 
+    //! Not implemented.
+    virtual void generateSamples() { PLASSERT( false ); }
+
     //! compute the expectation
     virtual void computeExpectation() ;
 
+    //! Not implemented.
+    virtual void computeExpectations() { PLASSERT( false ); }
+
     //! forward propagation
     virtual void fprop( const Vec& input, Vec& output ) const;
 

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.h	2007-04-19 19:10:14 UTC (rev 6925)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.h	2007-04-20 19:30:22 UTC (rev 6926)
@@ -69,23 +69,18 @@
     RBMMultinomialLayer( int the_size, real the_learning_rate=0. );
 
 
-    // Your other public member functions go here
-/*
-    //! Uses "rbmp" to obtain the activations of unit "i" of this layer.
-    //! This activation vector is computed by the "i+offset"-th unit of "rbmp"
-    virtual void getUnitActivations( int i, PP<RBMParameters> rbmp,
-                                     int offset=0 );
-
-    //! Uses "rbmp" to obtain the activations of all units in this layer.
-    //! Unit 0 of this layer corresponds to unit "offset" of "rbmp".
-    virtual void getAllActivations( PP<RBMParameters> rbmp, int offset=0 ) ;
-*/
     //! generate a sample, and update the sample field
     virtual void generateSample() ;
 
+    //! Not implemented.
+    virtual void generateSamples() { PLASSERT( false ); }
+
     //! compute the expectation
     virtual void computeExpectation() ;
 
+    //! Not implemented.
+    virtual void computeExpectations() { PLASSERT( false ); }
+
     //! forward propagation
     virtual void fprop( const Vec& input, Vec& output ) const;
 

Modified: trunk/plearn_learners/online/RBMTruncExpLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMTruncExpLayer.h	2007-04-19 19:10:14 UTC (rev 6925)
+++ trunk/plearn_learners/online/RBMTruncExpLayer.h	2007-04-20 19:30:22 UTC (rev 6926)
@@ -69,24 +69,15 @@
     RBMTruncExpLayer( int the_size, real the_learning_rate=0. );
 
 
-    // Your other public member functions go here
-/*
-    //! Uses "rbmp" to obtain the activations of unit "i" of this layer.
-    //! This activation vector is computed by the "i+offset"-th unit of "rbmp"
-    virtual void getUnitActivations( int i, PP<RBMParameters> rbmp,
-                                     int offset=0 );
-
-    //! Uses "rbmp" to obtain the activations of all units in this layer.
-    //! Unit 0 of this layer corresponds to unit "offset" of "rbmp".
-    virtual void getAllActivations( PP<RBMParameters> rbmp, int offset=0 ) ;
-*/
-
     //! generate a sample, and update the sample field
     virtual void generateSample() ;
 
     //! compute the expectation
     virtual void computeExpectation() ;
 
+    //! Not implemented.
+    virtual void computeExpectations() { PLASSERT( false ); }
+
     //! forward propagation
     virtual void fprop( const Vec& input, Vec& output ) const;
 



From tihocan at mail.berlios.de  Mon Apr 23 17:30:30 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 23 Apr 2007 17:30:30 +0200
Subject: [Plearn-commits] r6927 - trunk/plearn_learners/online
Message-ID: <200704231530.l3NFUU6k002236@sheep.berlios.de>

Author: tihocan
Date: 2007-04-23 17:30:27 +0200 (Mon, 23 Apr 2007)
New Revision: 6927

Modified:
   trunk/plearn_learners/online/RBMConv2DConnection.h
   trunk/plearn_learners/online/RBMGaussianLayer.h
   trunk/plearn_learners/online/RBMTruncExpLayer.h
Log:
Fixed compilation

Modified: trunk/plearn_learners/online/RBMConv2DConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMConv2DConnection.h	2007-04-20 19:30:22 UTC (rev 6926)
+++ trunk/plearn_learners/online/RBMConv2DConnection.h	2007-04-23 15:30:27 UTC (rev 6927)
@@ -134,6 +134,13 @@
                                  const Vec& activations,
                                  bool accumulate=false ) const;
 
+    virtual void computeProducts(int start, int length,
+                                 Mat& activations,
+                                 bool accumulate=false ) const
+    {
+        PLASSERT( false ); // Not implemented.
+    }
+
     //! Adapt based on the output gradient: this method should only
     //! be called just after a corresponding fprop; it should be
     //! called with the same arguments as fprop for the first two arguments

Modified: trunk/plearn_learners/online/RBMGaussianLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.h	2007-04-20 19:30:22 UTC (rev 6926)
+++ trunk/plearn_learners/online/RBMGaussianLayer.h	2007-04-23 15:30:27 UTC (rev 6927)
@@ -73,6 +73,11 @@
     //! compute a sample, and update the sample field
     virtual void generateSample() ;
 
+    virtual void generateSamples()
+    {
+        PLASSERT( false ); // Not implemented.
+    }
+
     //! compute the expectation
     virtual void computeExpectation() ;
 

Modified: trunk/plearn_learners/online/RBMTruncExpLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMTruncExpLayer.h	2007-04-20 19:30:22 UTC (rev 6926)
+++ trunk/plearn_learners/online/RBMTruncExpLayer.h	2007-04-23 15:30:27 UTC (rev 6927)
@@ -72,6 +72,10 @@
     //! generate a sample, and update the sample field
     virtual void generateSample() ;
 
+    virtual void generateSamples() {
+        PLASSERT( false ); // Not implemented.
+    }
+
     //! compute the expectation
     virtual void computeExpectation() ;
 



From nouiz at mail.berlios.de  Mon Apr 23 17:59:36 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 23 Apr 2007 17:59:36 +0200
Subject: [Plearn-commits] r6928 - in trunk: . speedtest
Message-ID: <200704231559.l3NFxa8k006821@sheep.berlios.de>

Author: nouiz
Date: 2007-04-23 17:59:36 +0200 (Mon, 23 Apr 2007)
New Revision: 6928

Added:
   trunk/speedtest/
   trunk/speedtest/xgemm.c
Log:
Added directory where I put my programme todo speed testing


Added: trunk/speedtest/xgemm.c
===================================================================
--- trunk/speedtest/xgemm.c	2007-04-23 15:30:27 UTC (rev 6927)
+++ trunk/speedtest/xgemm.c	2007-04-23 15:59:36 UTC (rev 6928)
@@ -0,0 +1,226 @@
+
+/* Includes, system */
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <math.h>
+
+#ifdef NVIDIA
+  /* Includes, cuda */
+  #include <cublas.h>
+#else
+  /* Includes, cblas */
+  #include <gsl/gsl_cblas.h>
+#endif
+#ifdef DOUBLE
+  typedef double real;
+  #define cblas_xgemm cblas_dgemm
+#else
+  typedef float real;
+  #define cblas_xgemm cblas_sgemm
+#endif
+//#include <iostream>
+//using namespace std;
+
+/* Host implementation of a simple version of sgemm */
+static void simple_xgemm(int n, real alpha, const real *A, const real *B,
+                         real beta, real *C)
+{
+    int i;
+    int j;
+    int k;
+    for (i = 0; i < n; ++i) {
+        for (j = 0; j < n; ++j) {
+            float prod = 0;
+            for (k = 0; k < n; ++k) {
+                prod += A[k * n + i] * B[j * n + k];
+            }
+            C[j * n + i] = alpha * prod + beta * C[j * n + i];
+        }
+    }
+}
+/* Main */
+int main(int argc, char** argv)
+{    
+  if (argc!=5){ 
+    fprintf (stderr, "Usage: %s <sizeM> <sizeN> <sizeK> <Nb iter>\n",argv[0]); 
+    exit(0); 
+  } 
+  const int M=strtol(argv[1],0,0);
+  const int N=strtol(argv[2],0,0);
+  const int K=strtol(argv[3],0,0);
+  const int NBITER=strtol(argv[4],0,0);
+  const int NA= M * K;
+  const int NB= K * N;
+  const int NC= M * N;
+  real* h_A;
+  real* h_B;
+  real* h_C;
+  const real alpha = 1.0f;
+  const real beta = 0.0f;
+#ifdef NVIDIA
+  cublasStatus status;
+  real* d_A = 0;
+  real* d_B = 0;
+  real* d_C = 0;
+#endif
+
+#ifdef COMPARE
+  real* h_C_ref;
+  real error_norm;
+  real ref_norm;
+  real diff;
+#endif
+
+    /* Allocate host memory for the matrices */
+    h_A = (real*)malloc(NA * sizeof(h_A[0]));
+    if (h_A == 0) {
+        fprintf (stderr, "!!!! host memory allocation error (A)\n");
+        return EXIT_FAILURE;
+    }
+    h_B = (real*)malloc(NB * sizeof(h_B[0]));
+    if (h_B == 0) {
+        fprintf (stderr, "!!!! host memory allocation error (B)\n");
+        return EXIT_FAILURE;
+    }
+    h_C = (real*)malloc(NC * sizeof(h_C[0]));
+    if (h_C == 0) {
+        fprintf (stderr, "!!!! host memory allocation error (C)\n");
+        return EXIT_FAILURE;
+    }
+
+    for (int i = 0; i < NA; ++i) h_A[i] = M_PI+(real)i;
+    for (int i = 0; i < NB; ++i) h_B[i] = M_PI+(real)i;
+
+#ifdef NVIDIA
+    /* Initialize CUBLAS */
+    status = cublasInit();
+    if (status != CUBLAS_STATUS_SUCCESS) {
+        fprintf (stderr, "!!!! CUBLAS initialization error\n");
+        return EXIT_FAILURE;
+    }
+    /* Allocate device memory for the matrices */
+    status = cublasAlloc(NA, sizeof(d_A[0]), (void**)&d_A);
+    if (status != CUBLAS_STATUS_SUCCESS) {
+        fprintf (stderr, "!!!! device memory allocation error (A)\n");
+        return EXIT_FAILURE;
+    }
+    status = cublasAlloc(NB, sizeof(d_B[0]), (void**)&d_B);
+    if (status != CUBLAS_STATUS_SUCCESS) {
+        fprintf (stderr, "!!!! device memory allocation error (B)\n");
+        return EXIT_FAILURE;
+    }
+    status = cublasAlloc(NC, sizeof(d_C[0]), (void**)&d_C);
+    if (status != CUBLAS_STATUS_SUCCESS) {
+        fprintf (stderr, "!!!! device memory allocation error (C)\n");
+        return EXIT_FAILURE;
+    }
+
+    /* Initialize the device matrices with the host matrices */
+    status = cublasSetVector(NA, sizeof(h_A[0]), h_A, 1, d_A, 1);
+    if (status != CUBLAS_STATUS_SUCCESS) {
+        fprintf (stderr, "!!!! device access error (write A)\n");
+        return EXIT_FAILURE;
+    }
+    status = cublasSetVector(NB, sizeof(h_B[0]), h_B, 1, d_B, 1);
+    if (status != CUBLAS_STATUS_SUCCESS) {
+        fprintf (stderr, "!!!! device access error (write B)\n");
+        return EXIT_FAILURE;
+    }
+    status = cublasSetVector(NC, sizeof(h_C[0]), h_C, 1, d_C, 1);
+    if (status != CUBLAS_STATUS_SUCCESS) {
+        fprintf (stderr, "!!!! device access error (write C)\n");
+        return EXIT_FAILURE;
+    }
+
+    /* Clear last error */
+    cublasGetError();
+#endif
+#ifdef COMPARE
+    /* Performs operation using plain C code */
+    for (int i=0;i<NBITER;i++)
+      simple_xgemm(N, alpha, h_A, h_B, beta, h_C);
+    h_C_ref = h_C;
+    /* Allocate host memory for reading back the result from device memory */
+    h_C = (float*)malloc(NC * sizeof(h_C[0]));
+    if (h_C == 0) {
+        fprintf (stderr, "!!!! host memory allocation error (C)\n");
+        return EXIT_FAILURE;
+    }
+#endif
+#ifdef NVIDIA
+    /* Performs operation using cublas */
+    for (int i=0;i<NBITER;i++)
+      cublasSgemm('n', 'n', M, N, K, alpha, d_A, M, d_B, K, beta, d_C, K);
+
+    status = cublasGetError();
+    if (status != CUBLAS_STATUS_SUCCESS) {
+        fprintf (stderr, "!!!! kernel execution error.\n");
+        return EXIT_FAILURE;
+    }
+    /* Read the result back */
+    status = cublasGetVector(NC, sizeof(h_C[0]), d_C, 1, h_C, 1);
+    if (status != CUBLAS_STATUS_SUCCESS) {
+        fprintf (stderr, "!!!! device access error (read C)\n");
+        return EXIT_FAILURE;
+    }
+#elif defined( CXGEMM )
+    for (int i=0;i<NBITER;i++)
+      simple_xgemm(N, alpha, h_A, h_B, beta, h_C);
+#else
+    for (int i=0;i<NBITER;i++)
+      cblas_xgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, M,N,K, 1.0, h_A, K, h_B, N, 0.0, h_C, N);
+#endif
+#ifdef COMPARE
+    /* Check result against reference */
+    error_norm = 0;
+    ref_norm = 0;
+    for (int i = 0; i < NC; ++i) {
+        diff = h_C_ref[i] - h_C[i];
+        error_norm += diff * diff;
+        ref_norm += h_C_ref[i] * h_C_ref[i];
+    }
+    error_norm = (float)sqrt((double)error_norm);
+    ref_norm = (float)sqrt((double)ref_norm);
+    if (fabs(ref_norm) < 1e-7) {
+        fprintf (stderr, "!!!! reference norm is 0\n");
+        return EXIT_FAILURE;
+    }
+    printf( "Test %s\n", (error_norm / ref_norm < 1e-6f) ? "PASSED" : "FAILED");
+#endif
+
+    /* Memory clean up */
+    free(h_A);
+    free(h_B);
+    free(h_C);
+
+#ifdef NVIDIA
+    status = cublasFree(d_A);
+    if (status != CUBLAS_STATUS_SUCCESS) {
+        fprintf (stderr, "!!!! memory free error (A)\n");
+        return EXIT_FAILURE;
+    }
+    status = cublasFree(d_B);
+    if (status != CUBLAS_STATUS_SUCCESS) {
+        fprintf (stderr, "!!!! memory free error (B)\n");
+        return EXIT_FAILURE;
+    }
+    status = cublasFree(d_C);
+    if (status != CUBLAS_STATUS_SUCCESS) {
+        fprintf (stderr, "!!!! memory free error (C)\n");
+        return EXIT_FAILURE;
+    }
+
+    /* Shutdown */
+    status = cublasShutdown();
+    if (status != CUBLAS_STATUS_SUCCESS) {
+        fprintf (stderr, "!!!! shutdown error (A)\n");
+        return EXIT_FAILURE;
+    }
+#endif
+    //    if (argc <= 1 || strcmp(argv[1], "-noprompt")) {
+    //        printf("\nPress ENTER to exit...\n");
+    //        getchar();
+    //    }
+    return EXIT_SUCCESS;
+}



From nouiz at mail.berlios.de  Mon Apr 23 20:54:12 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 23 Apr 2007 20:54:12 +0200
Subject: [Plearn-commits] r6929 - trunk
Message-ID: <200704231854.l3NIsClQ013832@sheep.berlios.de>

Author: nouiz
Date: 2007-04-23 20:54:11 +0200 (Mon, 23 Apr 2007)
New Revision: 6929

Modified:
   trunk/pymake.config.model
Log:
Added an compile option that compile with the BLAS version that is recommended at lisa. This will automatically load the best GOTO blas lib at execution. So we can compile on a computer and execute it on another one and we will have the best version. We can't mix x86 and x86_64 computer.


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2007-04-23 15:59:36 UTC (rev 6928)
+++ trunk/pymake.config.model	2007-04-23 18:54:11 UTC (rev 6929)
@@ -419,7 +419,7 @@
   [ 'python23', 'python24', 'python25' ],
   
   [ 'blas', 'p3blas','p4blas','athlonblas','pentiumblas', 'mammouthblas',
-    'noblas', 'veclib', 'scs', 'goto' ],
+    'noblas', 'veclib', 'scs', 'goto', 'lisa' ],
   
   [ 'logging=dbg', 'logging=mand', 'logging=imp', 'logging=normal', 'logging=extreme' ]
 ]
@@ -690,6 +690,14 @@
               linkeroptions = '-L' + libdir +'goto -llapack -lgoto -lgfortran'
               )
 
+## We must link again the static version of lapack as the dynamic version is linked again the default version of blas
+## and we don't want to link again it. Also, we must remove fonction from lapack as some of them are also in GOTO
+pymakeOption( name = 'lisa',
+              description = 'compilation and linking using GOTO for BLAS',
+              cpp_definitions = ['USE_BLAS_SPECIALISATIONS'],
+              linkeroptions = '-L' + libdir +'goto /u/lisa/local/'+target_platform+'/lib/lapack_for_goto.a  -lgoto -lpthread -lgfortran -lg2c'
+              )
+
 cpp_variables += ['USE_BLAS_SPECIALISATIONS']
 
 



From tihocan at mail.berlios.de  Tue Apr 24 16:45:25 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 24 Apr 2007 16:45:25 +0200
Subject: [Plearn-commits] r6930 - trunk/plearn/vmat
Message-ID: <200704241445.l3OEjPRB013314@sheep.berlios.de>

Author: tihocan
Date: 2007-04-24 16:45:24 +0200 (Tue, 24 Apr 2007)
New Revision: 6930

Modified:
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
Modified the getExamples(..) method so that it can get examples in a 'circular' fashion, by picking the samples at the beginning of the VMat if the subset exceeds the VMat's length

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2007-04-23 18:54:11 UTC (rev 6929)
+++ trunk/plearn/vmat/VMatrix.cc	2007-04-24 14:45:24 UTC (rev 6930)
@@ -515,7 +515,7 @@
 // getExamples //
 /////////////////
 void VMatrix::getExamples(int i_start, int length, Mat& inputs, Mat& targets,
-                          Vec& weights, Mat* extras)
+                          Vec& weights, Mat* extras, bool allow_circular)
 {
     inputs.resize(length, inputsize());
     targets.resize(length, targetsize());
@@ -523,13 +523,19 @@
     if (extras)
         extras->resize(length, extrasize());
     Vec input, target, extra;
+    int total_length = this->length();
+    PLASSERT( i_start < total_length );
     for (int k = 0; k < length; k++) {
         input = inputs(k);
         target = targets(k);
-        getExample(i_start + k, input, target, weights[k]);
+        int idx = i_start + k;
+        if (allow_circular)
+            idx %= total_length;
+        PLASSERT( idx >= 0 && idx < total_length );
+        getExample(idx, input, target, weights[k]);
         if (extras) {
             extra = (*extras)(k);
-            getExtra(i_start + k, extra);
+            getExtra(idx, extra);
         }
     }
 }

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2007-04-23 18:54:11 UTC (rev 6929)
+++ trunk/plearn/vmat/VMatrix.h	2007-04-24 14:45:24 UTC (rev 6930)
@@ -439,8 +439,13 @@
     //! Obtain a subset of 'length' examples, starting from 'i_start'.
     //! The 'extra' matrix is provided as a pointer so that it can be omitted
     //! without significant overhead.
+    //! If the 'allow_circular' boolean parameter is set to 'true', then one
+    //! may ask for a subset that goes beyond this VMat's length: in such a
+    //! case, the rest of the subset will be filled with data found at the
+    //! beginning of this VMat.
     void getExamples(int i_start, int length, Mat& inputs, Mat& targets,
-                     Vec& weights, Mat* extra = NULL);
+                     Vec& weights, Mat* extra = NULL,
+                     bool allow_circular = false);
 
     /**
      *  Complements the getExample method, fetching the the extrasize_ "extra"



From tihocan at mail.berlios.de  Tue Apr 24 16:54:17 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 24 Apr 2007 16:54:17 +0200
Subject: [Plearn-commits] r6931 - trunk/plearn_learners/online
Message-ID: <200704241454.l3OEsHsb013888@sheep.berlios.de>

Author: tihocan
Date: 2007-04-24 16:54:16 +0200 (Tue, 24 Apr 2007)
New Revision: 6931

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
Log:
Various bug fixes for mini-batch setting

Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-04-24 14:45:24 UTC (rev 6930)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-04-24 14:54:16 UTC (rev 6931)
@@ -449,8 +449,6 @@
 /////////////////////////////////
 void DeepBeliefNet::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    // TODO Add missing fields.
-
     inherited::makeDeepCopyFromShallowCopy(copies);
 
     deepCopyField(training_schedule,        copies);
@@ -464,9 +462,13 @@
     deepCopyField(classification_cost,      copies);
     deepCopyField(joint_layer,              copies);
     deepCopyField(activation_gradients,     copies);
+    deepCopyField(activations_gradients,    copies);
     deepCopyField(expectation_gradients,    copies);
+    deepCopyField(expectations_gradients,   copies);
     deepCopyField(final_cost_input,         copies);
+    deepCopyField(final_cost_inputs,        copies);
     deepCopyField(final_cost_value,         copies);
+    deepCopyField(final_cost_values,        copies);
     deepCopyField(final_cost_output,        copies);
     deepCopyField(class_output,             copies);
     deepCopyField(class_gradient,           copies);
@@ -475,8 +477,11 @@
     deepCopyField(final_cost_gradients,     copies);
     deepCopyField(save_layer_activation,    copies);
     deepCopyField(save_layer_expectation,   copies);
-    deepCopyField(pos_down_val,          copies);
-    deepCopyField(pos_up_val,            copies);
+    deepCopyField(pos_down_val,             copies);
+    deepCopyField(pos_up_val,               copies);
+    deepCopyField(pos_down_vals,            copies);
+    deepCopyField(pos_up_vals,              copies);
+    deepCopyField(optimized_costs,          copies);
     deepCopyField(final_cost_indices,       copies);
     deepCopyField(partial_cost_indices,     copies);
 }
@@ -546,6 +551,7 @@
         }
         if (final_cost)
             final_cost_gradients.resize(minibatch_size, final_cost->input_size);
+        optimized_costs.resize(minibatch_size);
     }
 
     layers[n_layers-1]->random_gen = random_gen;
@@ -666,7 +672,7 @@
                     int sample_start = stage % nsamples;
                     if (batch_size > 1) {
                         train_set->getExamples(sample_start, minibatch_size,
-                                inputs, targets, weights);
+                                inputs, targets, weights, NULL, true);
                         greedyStep( inputs, targets, i );
                     } else {
                         train_set->getExample(sample_start, input, target, weight);
@@ -726,6 +732,8 @@
 
         // TODO Do we really want to systematically compute this reconstruction
         // error?
+
+        if (batch_size == 1) {
         for(int train_index = 0 ; train_index < nsamples ; train_index++)
         {
 
@@ -756,6 +764,10 @@
         }
 
         train_recons_error /= nsamples ;
+        } else {
+            // Currently do not compute reconstruction error with mini-batches.
+            train_recons_error = MISSING_VALUE;
+        }
 
 
         /***** fine-tuning by gradient descent *****/
@@ -792,7 +804,7 @@
 
                 if (minibatch_size > 1) {
                     train_set->getExamples(sample_start, minibatch_size, inputs,
-                            targets, weights);
+                            targets, weights, NULL, true);
                     fineTuningStep(inputs, targets, train_costs_m);
                 } else {
                     train_set->getExample( sample_start, input, target, weight );
@@ -1341,9 +1353,6 @@
     }
 }
 
-////////////////////
-// fineTuningStep //
-////////////////////
 void DeepBeliefNet::fineTuningStep(const Mat& inputs, const Mat& targets,
                                    Mat& train_costs )
 {
@@ -1368,11 +1377,14 @@
 
         if( final_module )
         {
+            final_cost_inputs.resize(minibatch_size,
+                                     final_module->output_size);
             final_module->fprop( layers[ n_layers-1 ]->getExpectations(),
                                  final_cost_inputs );
             final_cost->fprop( final_cost_inputs, targets, final_cost_values );
 
-            Mat optimized_costs = final_cost_values.column(0);
+            // TODO This extra memory copy is annoying: how can we avoid it?
+            optimized_costs << final_cost_values.column(0);
             final_cost->bpropUpdate( final_cost_inputs, targets,
                                      optimized_costs,
                                      final_cost_gradients );
@@ -1386,7 +1398,7 @@
             final_cost->fprop( layers[ n_layers-1 ]->getExpectations(), targets,
                                final_cost_values );
 
-            Mat optimized_costs = final_cost_values.column(0);
+            optimized_costs << final_cost_values.column(0);
             final_cost->bpropUpdate( layers[ n_layers-1 ]->getExpectations(),
                                      targets, optimized_costs,
                                      expectations_gradients[ n_layers-1 ] );
@@ -1415,7 +1427,8 @@
 
     if( use_classification_cost )
     {
-        PLASSERT_MSG(batch_size == 1, "Not implemented for mini-batches");
+        PLERROR("DeepBeliefNet::fineTuningStep - Not implemented for "
+                "mini-batches");
         /*
         classification_module->fprop( layers[ n_layers-2 ]->expectation,
                                       class_output );

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2007-04-24 14:45:24 UTC (rev 6930)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2007-04-24 14:54:16 UTC (rev 6931)
@@ -309,6 +309,9 @@
     mutable Mat pos_down_vals;
     mutable Mat pos_up_vals;
 
+    //! Used to store the costs optimized by the final cost module.
+    Vec optimized_costs;
+
     //! Keeps the index of the NLL cost in train_costs
     int nll_cost_index;
 



From tihocan at mail.berlios.de  Tue Apr 24 17:00:43 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 24 Apr 2007 17:00:43 +0200
Subject: [Plearn-commits] r6932 - trunk/plearn_learners/online
Message-ID: <200704241500.l3OF0hGp014404@sheep.berlios.de>

Author: tihocan
Date: 2007-04-24 17:00:43 +0200 (Tue, 24 Apr 2007)
New Revision: 6932

Modified:
   trunk/plearn_learners/online/CostModule.cc
   trunk/plearn_learners/online/CostModule.h
Log:
Added new mini-batch methods and implemented deepCopy

Modified: trunk/plearn_learners/online/CostModule.cc
===================================================================
--- trunk/plearn_learners/online/CostModule.cc	2007-04-24 14:54:16 UTC (rev 6931)
+++ trunk/plearn_learners/online/CostModule.cc	2007-04-24 15:00:43 UTC (rev 6932)
@@ -83,12 +83,25 @@
 }
 
 
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
 void CostModule::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(tmp_costs,                            copies);
+    deepCopyField(tmp_input_and_target,                 copies);
+    deepCopyField(tmp_input_and_target_gradient,        copies);
+    deepCopyField(tmp_input_and_target_diag_hessian,    copies);
+    deepCopyField(tmp_costs_mat,                        copies);
+    deepCopyField(tmp_input_gradients,                  copies);
 }
 
 
+///////////
+// fprop //
+///////////
 void CostModule::fprop(const Vec& input, const Vec& target, Vec& cost) const
 {
     PLERROR("CostModule::fprop(const Vec& input, const Vec& target, Vec& cost)"
@@ -96,14 +109,38 @@
             "is not implemented. You have to implement it in your class.\n");
 }
 
-//! keeps only the first cost
+void CostModule::fprop(const Mat& inputs, const Mat& targets, Mat& costs) const
+{
+    //PLWARNING("CostModule::fprop - Not implemented for class %s",
+    //       classname().c_str());
+    // Default (possibly inefficient) implementation.
+    costs.resize(inputs.length(), output_size);
+    Vec input, target, cost;
+    for (int i = 0; i < inputs.length(); i++) {
+        input = inputs(i);
+        target = targets(i);
+        cost = costs(i);
+        this->fprop(input, target, cost);
+    }
+}
+
 void CostModule::fprop(const Vec& input, const Vec& target, real& cost) const
 {
+    // Keep only the first cost.
     fprop( input, target, tmp_costs );
-
     cost = tmp_costs[0];
 }
 
+void CostModule::fprop(const Mat& inputs, const Mat& targets, Vec& costs)
+{
+    //PLWARNING("In CostModule::fprop - Using default (possibly inefficient) "
+    //        "implementation for class %s", classname().c_str());
+    // Keep only the first cost.
+    tmp_costs_mat.resize(inputs.length(), output_size);
+    fprop(inputs, targets, tmp_costs_mat);
+    costs << tmp_costs_mat.column(0);
+}
+
 //! for compatibility with OnlineLearningModule interface
 void CostModule::fprop(const Vec& input_and_target, Vec& output) const
 {
@@ -114,6 +151,9 @@
 }
 
 
+/////////////////
+// bpropUpdate //
+/////////////////
 void CostModule::bpropUpdate(const Vec& input, const Vec& target, real cost,
                              Vec& input_gradient, bool accumulate)
 {
@@ -147,6 +187,14 @@
     bpropUpdate( input, target, cost, tmp_input_gradient );
 }
 
+void CostModule::bpropUpdate(const Mat& inputs, const Mat& targets,
+        const Vec& costs)
+{
+    PLWARNING("In CostModule::bpropUpdate - Using default (possibly "
+        "inefficient) version for class %s", classname().c_str());
+    bpropUpdate( inputs, targets, costs, tmp_input_gradients );
+}
+
 void CostModule::bpropUpdate(const Vec& input_and_target, const Vec& output,
                              Vec& input_and_target_gradient,
                              const Vec& output_gradient,

Modified: trunk/plearn_learners/online/CostModule.h
===================================================================
--- trunk/plearn_learners/online/CostModule.h	2007-04-24 14:54:16 UTC (rev 6931)
+++ trunk/plearn_learners/online/CostModule.h	2007-04-24 15:00:43 UTC (rev 6932)
@@ -71,15 +71,15 @@
     //! given the input and target, compute the main output (cost)
     virtual void fprop(const Vec& input, const Vec& target, real& cost ) const;
 
+    //! Mini-batch version.
+    virtual void fprop(const Mat& inputs, const Mat& targets, Vec& costs );
 
     //! this version allows for several costs
     virtual void fprop(const Vec& input, const Vec& target, Vec& cost ) const;
 
-    // TODO Document and implement (sub-classes too).
-    virtual void fprop(const Mat& inputs, const Mat& targets, Mat& costs ) const
-    {
-        PLASSERT( false );
-    }
+    //! Mini-batch version with several costs..
+    virtual void fprop(const Mat& inputs, const Mat& targets, Mat& costs)
+        const;
 
     //! this version is provided for compatibility with the parent class
     virtual void fprop(const Vec& input_and_target, Vec& output) const;
@@ -88,9 +88,21 @@
     virtual void bpropUpdate(const Vec& input, const Vec& target, real cost,
                              Vec& input_gradient, bool accumulate=false);
 
+    //! Adapt based on the mini-batch cost gradient, and obtain the mini-batch
+    //! input gradient.
+    virtual void bpropUpdate(const Mat& inputs, const Mat& targets,
+            const Vec& costs, Mat& input_gradients, bool accumulate = false)
+    {
+        PLERROR("bpropUpdate on mini-batches not implemented in class %s",
+                classname().c_str());
+    }
+
     //! Without the input gradient
     virtual void bpropUpdate(const Vec& input, const Vec& target, real cost );
 
+    virtual void bpropUpdate(const Mat& inputs, const Mat& targets,
+            const Vec& costs);
+
     //! this version is provided for compatibility with the parent class.
     virtual void bpropUpdate(const Vec& input_and_target, const Vec& output,
                              Vec& input_and_target_gradient,
@@ -151,6 +163,8 @@
     mutable Vec tmp_input_and_target;
     mutable Vec tmp_input_and_target_gradient;
     mutable Vec tmp_input_and_target_diag_hessian;
+    Mat tmp_costs_mat;
+    Mat tmp_input_gradients;
 
 private:
     //#####  Private Member Functions  ########################################



From tihocan at mail.berlios.de  Tue Apr 24 17:06:52 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 24 Apr 2007 17:06:52 +0200
Subject: [Plearn-commits] r6933 - trunk/plearn_learners/online
Message-ID: <200704241506.l3OF6qkt014768@sheep.berlios.de>

Author: tihocan
Date: 2007-04-24 17:06:51 +0200 (Tue, 24 Apr 2007)
New Revision: 6933

Modified:
   trunk/plearn_learners/online/ClassErrorCostModule.cc
   trunk/plearn_learners/online/ClassErrorCostModule.h
   trunk/plearn_learners/online/CombiningCostsModule.cc
   trunk/plearn_learners/online/CombiningCostsModule.h
   trunk/plearn_learners/online/NLLCostModule.cc
   trunk/plearn_learners/online/NLLCostModule.h
   trunk/plearn_learners/online/OnlineLearningModule.cc
   trunk/plearn_learners/online/RBMBinomialLayer.cc
   trunk/plearn_learners/online/RBMBinomialLayer.h
   trunk/plearn_learners/online/RBMConnection.cc
   trunk/plearn_learners/online/RBMConnection.h
   trunk/plearn_learners/online/RBMConv2DConnection.h
   trunk/plearn_learners/online/RBMGaussianLayer.h
   trunk/plearn_learners/online/RBMLayer.cc
   trunk/plearn_learners/online/RBMLayer.h
   trunk/plearn_learners/online/RBMMatrixConnection.cc
   trunk/plearn_learners/online/RBMMatrixConnection.h
   trunk/plearn_learners/online/RBMMixedConnection.h
   trunk/plearn_learners/online/RBMMixedLayer.cc
   trunk/plearn_learners/online/RBMMixedLayer.h
   trunk/plearn_learners/online/RBMMultinomialLayer.cc
   trunk/plearn_learners/online/RBMMultinomialLayer.h
   trunk/plearn_learners/online/RBMTruncExpLayer.h
Log:
More work towards mini-batches (big commit to fix compilation issue in previous commit)

Modified: trunk/plearn_learners/online/ClassErrorCostModule.cc
===================================================================
--- trunk/plearn_learners/online/ClassErrorCostModule.cc	2007-04-24 15:00:43 UTC (rev 6932)
+++ trunk/plearn_learners/online/ClassErrorCostModule.cc	2007-04-24 15:06:51 UTC (rev 6933)
@@ -113,6 +113,13 @@
         cost = ( argmax(input) == int(round(target[0])) ) ? 0. : 1.;
 }
 
+void ClassErrorCostModule::fprop(const Mat& inputs, const Mat& targets,
+                                 Mat& costs) const
+{
+    for (int i = 0; i < inputs.length(); i++)
+        fprop(inputs(i), targets(i), costs(i, 0));
+}
+
 /////////////////
 // bpropUpdate //
 /////////////////

Modified: trunk/plearn_learners/online/ClassErrorCostModule.h
===================================================================
--- trunk/plearn_learners/online/ClassErrorCostModule.h	2007-04-24 15:00:43 UTC (rev 6932)
+++ trunk/plearn_learners/online/ClassErrorCostModule.h	2007-04-24 15:06:51 UTC (rev 6933)
@@ -69,6 +69,10 @@
     //! (possibly resize it appropriately)
     virtual void fprop(const Vec& input, const Vec& target, Vec& cost) const;
 
+    //! Overridden from parent class.
+    virtual void fprop(const Mat& inputs, const Mat& targets, Mat& costs)
+        const;
+
     //! Given the input and the target, compute only the first cost
     //! (of which we will compute the gradient)
     virtual void fprop(const Vec& input, const Vec& target, real& cost) const;
@@ -76,7 +80,7 @@
     //! Nothing to do
     virtual void bpropUpdate(const Vec& input, const Vec& target, real cost);
 
-    /* Default implementation in super class raises a PLERROR
+    /*
     //! No differentiable, so no gradient to backprop!
     virtual void bpropUpdate(const Vec& input, const Vec& target, real cost,
                              Vec& input_gradient);
@@ -85,6 +89,10 @@
     //! Nothing to do
     virtual void bbpropUpdate(const Vec& input, const Vec& target, real cost);
 
+    virtual void bpropUpdate(const Mat& inputs, const Mat& targets,
+            const Vec& costs)
+    {}
+
     /* Optional
        N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
        RAISES A PLERROR.

Modified: trunk/plearn_learners/online/CombiningCostsModule.cc
===================================================================
--- trunk/plearn_learners/online/CombiningCostsModule.cc	2007-04-24 15:00:43 UTC (rev 6932)
+++ trunk/plearn_learners/online/CombiningCostsModule.cc	2007-04-24 15:06:51 UTC (rev 6933)
@@ -129,6 +129,9 @@
     output_size = n_sub_costs+1;
 }
 
+///////////
+// build //
+///////////
 void CombiningCostsModule::build()
 {
     inherited::build();
@@ -136,16 +139,25 @@
 }
 
 
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
 void CombiningCostsModule::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
-    deepCopyField(sub_costs, copies);
-    deepCopyField(cost_weights, copies);
-    deepCopyField(sub_costs_values, copies);
+    deepCopyField(sub_costs,            copies);
+    deepCopyField(cost_weights,         copies);
+    deepCopyField(sub_costs_values,     copies);
+    deepCopyField(sub_costs_mbatch_values, copies);
+    deepCopyField(partial_gradient,     copies);
+    deepCopyField(partial_diag_hessian, copies);
 }
 
 
+///////////
+// fprop //
+///////////
 void CombiningCostsModule::fprop(const Vec& input, const Vec& target,
                                  Vec& cost) const
 {
@@ -160,6 +172,34 @@
     cost.subVec( 1, n_sub_costs ) << sub_costs_values;
 }
 
+void CombiningCostsModule::fprop(const Mat& inputs, const Mat& targets,
+                                 Mat& costs) const
+{
+    PLASSERT( inputs.width() == input_size );
+    PLASSERT( targets.width() == target_size );
+    costs.resize(inputs.length(), output_size);
+
+    Mat final_cost = costs.column(0);
+    final_cost.fill(0);
+    Mat other_costs = costs.subMatColumns(1, n_sub_costs);
+    sub_costs_mbatch_values.resize(n_sub_costs, inputs.length());
+    for( int i=0 ; i<n_sub_costs ; i++ ) {
+        Vec sub_costs_i = sub_costs_mbatch_values(i);
+        sub_costs[i]->fprop(inputs, targets, sub_costs_i);
+        Mat first_sub_cost = sub_costs_i.toMat(sub_costs_i.length(), 1);
+
+        // final_cost += weight_i * cost_i
+        multiplyAcc(final_cost, first_sub_cost, cost_weights[i]);
+
+        // Fill the rest of the costs matrix.
+        other_costs.column(i) << first_sub_cost;
+    }
+}
+
+
+/////////////////
+// bpropUpdate //
+/////////////////
 void CombiningCostsModule::bpropUpdate(const Vec& input, const Vec& target,
                                        real cost, Vec& input_gradient,
                                        bool accumulate)
@@ -202,6 +242,53 @@
     }
 }
 
+void CombiningCostsModule::bpropUpdate(const Mat& inputs, const Mat& targets,
+        const Vec& costs, Mat& input_gradients, bool accumulate)
+{
+    PLASSERT( inputs.width() == input_size );
+    PLASSERT( targets.width() == target_size );
+
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradients.width() == input_size &&
+                      input_gradients.length() == inputs.length(),
+                      "Cannot resize input_gradients and accumulate into it" );
+    }
+    else
+    {
+        input_gradients.resize(inputs.length(), input_size );
+        input_gradients.clear();
+    }
+
+
+    Vec sub;
+    for( int i=0 ; i<n_sub_costs ; i++ )
+    {
+        sub = sub_costs_mbatch_values(i);
+        if( cost_weights[i] == 0. )
+        {
+            // Do not compute input_gradients.
+            sub_costs[i]->bpropUpdate( inputs, targets, sub );
+        }
+        else if( cost_weights[i] == 1. )
+        {
+            // Accumulate directly into input_gradients.
+
+            sub_costs[i]->bpropUpdate( inputs, targets, sub, input_gradients,
+                    true );
+        }
+        else
+        {
+            // Put the result into partial_gradients, then accumulate into
+            // input_gradients with the appropriate weight.
+            sub_costs[i]->bpropUpdate( inputs, targets, sub, partial_gradients,
+                    false);
+            multiplyAcc( input_gradients, partial_gradients, cost_weights[i] );
+        }
+    }
+}
+
+
 void CombiningCostsModule::bpropUpdate(const Vec& input, const Vec& target,
                                        real cost)
 {

Modified: trunk/plearn_learners/online/CombiningCostsModule.h
===================================================================
--- trunk/plearn_learners/online/CombiningCostsModule.h	2007-04-24 15:00:43 UTC (rev 6932)
+++ trunk/plearn_learners/online/CombiningCostsModule.h	2007-04-24 15:06:51 UTC (rev 6933)
@@ -78,11 +78,19 @@
     //! given the input and target, compute the cost
     virtual void fprop(const Vec& input, const Vec& target, Vec& cost) const;
 
+    //! Overridden from parent class.
+    virtual void fprop(const Mat& inputs, const Mat& targets, Mat& costs)
+        const;
+
     //! Adapt based on the output gradient: this method should only
     //! be called just after a corresponding fprop.
     virtual void bpropUpdate(const Vec& input, const Vec& target, real cost,
                              Vec& input_gradient, bool accumulate=false);
 
+    //! Overridden.
+    virtual void bpropUpdate(const Mat& inputs, const Mat& targets,
+            const Vec& costs, Mat& input_gradients, bool accumulate = false);
+
     //! Calls this method on the sub_costs
     virtual void bpropUpdate(const Vec& input, const Vec& target, real cost);
 
@@ -145,9 +153,16 @@
     //! Stores the output values of the sub_costs
     mutable Vec sub_costs_values;
 
+    //! Stores mini-batch outputs values of sub costs.
+    mutable Mat sub_costs_mbatch_values;
+
     //! Stores intermediate values of the input gradient
     mutable Vec partial_gradient;
 
+    //! Used to store intermediate values of input gradient in mini-batch
+    //! setting.
+    Mat partial_gradients;
+
     //! Stores intermediate values of the input diagonal of Hessian
     mutable Vec partial_diag_hessian;
 };

Modified: trunk/plearn_learners/online/NLLCostModule.cc
===================================================================
--- trunk/plearn_learners/online/NLLCostModule.cc	2007-04-24 15:00:43 UTC (rev 6932)
+++ trunk/plearn_learners/online/NLLCostModule.cc	2007-04-24 15:06:51 UTC (rev 6933)
@@ -84,6 +84,9 @@
 }
 
 
+///////////
+// fprop //
+///////////
 void NLLCostModule::fprop(const Vec& input, const Vec& target, Vec& cost) const
 {
     PLASSERT( input.size() == input_size );
@@ -94,6 +97,9 @@
     cost[0] = -pl_log( input[ the_target ] );
 }
 
+/////////////////
+// bpropUpdate //
+/////////////////
 void NLLCostModule::bpropUpdate(const Vec& input, const Vec& target, real cost,
                                 Vec& input_gradient, bool accumulate)
 {
@@ -115,9 +121,36 @@
     // input_gradient[ i ] = 0 if i != t,
     // input_gradient[ t ] = -1/x[t]
     input_gradient[ the_target ] = - 1. / input[ the_target ];
+    // TODO Is that a bug with accumulate?
 }
 
+void NLLCostModule::bpropUpdate(const Mat& inputs, const Mat& targets,
+        const Vec& costs, Mat& input_gradients, bool accumulate)
+{
+    PLASSERT( inputs.width() == input_size );
+    PLASSERT( targets.width() == target_size );
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradients.width() == input_size &&
+                input_gradients.length() == inputs.length(),
+                "Cannot resize input_gradients and accumulate into it" );
+    }
+    else
+    {
+        input_gradients.resize(inputs.length(), input_size );
+        input_gradients.clear();
+    }
+
+    // input_gradient[ i ] = 0 if i != t,
+    // input_gradient[ t ] = -1/x[t]
+    for (int i = 0; i < inputs.length(); i++) {
+        int the_target = (int) round( targets(i, 0) );
+        input_gradients(i, the_target) = - 1. / inputs(i, the_target);
+        // TODO Is that a bug with accumulate?
+    }
+}
+
 void NLLCostModule::bbpropUpdate(const Vec& input, const Vec& target,
                                  real cost,
                                  Vec& input_gradient, Vec& input_diag_hessian,

Modified: trunk/plearn_learners/online/NLLCostModule.h
===================================================================
--- trunk/plearn_learners/online/NLLCostModule.h	2007-04-24 15:00:43 UTC (rev 6932)
+++ trunk/plearn_learners/online/NLLCostModule.h	2007-04-24 15:06:51 UTC (rev 6933)
@@ -74,10 +74,13 @@
     virtual void bpropUpdate(const Vec& input, const Vec& target, real cost,
                              Vec& input_gradient, bool accumulate=false);
 
+    //! Overridden.
+    virtual void bpropUpdate(const Mat& inputs, const Mat& targets,
+            const Vec& costs, Mat& input_gradients, bool accumulate = false);
+
     //! Does nothing
     virtual void bpropUpdate(const Vec& input, const Vec& target, real cost)
-    {
-    }
+    {}
 
     //! Similar to bpropUpdate, but adapt based also on the estimation
     //! of the diagonal of the Hessian matrix, and propagates this back.
@@ -87,8 +90,7 @@
 
     //! Does nothing
     virtual void bbpropUpdate(const Vec& input, const Vec& target, real cost)
-    {
-    }
+    {}
 
     //! Overridden to do nothing (in particular, no warning).
     virtual void setLearningRate(real dynamic_learning_rate) {}

Modified: trunk/plearn_learners/online/OnlineLearningModule.cc
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.cc	2007-04-24 15:00:43 UTC (rev 6932)
+++ trunk/plearn_learners/online/OnlineLearningModule.cc	2007-04-24 15:06:51 UTC (rev 6933)
@@ -64,9 +64,12 @@
 {
 }
 
-//! default inefficient implementation of mini-batch fprop
+///////////
+// fprop //
+///////////
 void OnlineLearningModule::fprop(const Mat& input, Mat& output) const
 {
+    // Default (inefficient) implementation of mini-batch fprop.
     int n=input.length();
 #ifdef BOUNDCHECK
     if (n!=output.length())
@@ -80,6 +83,9 @@
     }
 }
 
+/////////////////
+// bpropUpdate //
+/////////////////
 void OnlineLearningModule::bpropUpdate(const Vec& input, const Vec& output,
                                        Vec& input_gradient,
                                        const Vec& output_gradient,
@@ -87,8 +93,8 @@
 {
     PLERROR("In OnlineLearningModule.cc: method 'bpropUpdate' not"
             " implemented.\n"
-            "Please implement it in your derived class or don't call"
-            " bpropUpdate.\n");
+            "Please implement it in your derived class (%s) or do not call"
+            " bpropUpdate.", classname().c_str());
 }
 
 void OnlineLearningModule::bpropUpdate(const Vec& input, const Vec& output,

Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-04-24 15:00:43 UTC (rev 6932)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-04-24 15:06:51 UTC (rev 6933)
@@ -146,6 +146,9 @@
         output[i] = sigmoid( -input[i] - rbm_bias[i]);
 }
 
+/////////////////
+// bpropUpdate //
+/////////////////
 void RBMBinomialLayer::bpropUpdate(const Vec& input, const Vec& output,
                                    Vec& input_gradient,
                                    const Vec& output_gradient,
@@ -191,6 +194,57 @@
     }
 }
 
+void RBMBinomialLayer::bpropUpdate(const Mat& inputs, const Mat& outputs,
+        Mat& input_gradients,
+        const Mat& output_gradients,
+        bool accumulate)
+{
+    PLASSERT( inputs.width() == size );
+    PLASSERT( outputs.width() == size );
+    PLASSERT( output_gradients.width() == size );
+
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradients.width() == size &&
+                input_gradients.length() == inputs.length(),
+                "Cannot resize input_gradients and accumulate into it" );
+    }
+    else
+    {
+        input_gradients.resize(inputs.length(), size);
+        input_gradients.fill(0);
+    }
+
+    if( momentum != 0. )
+        bias_inc.resize( size );
+
+    for( int i=0 ; i<size ; i++ )
+    {
+        for (int j = 0; j < inputs.length(); j++) {
+            real output_i = outputs(j, i);
+            real in_grad_i = -output_i * (1-output_i) * output_gradients(j, i);
+            input_gradients(j, i) += in_grad_i;
+
+            if( momentum == 0. )
+            {
+                // update the bias: bias -= learning_rate * input_gradient
+                bias[i] -= learning_rate * in_grad_i;
+            }
+            else
+            {
+                PLERROR("In RBMBinomialLayer:bpropUpdate - Not implemented for "
+                        "momentum with mini-batches");
+                // The update rule becomes:
+                // bias_inc = momentum * bias_inc - learning_rate * input_gradient
+                // bias += bias_inc
+                bias_inc[i] = momentum * bias_inc[i] - learning_rate * in_grad_i;
+                bias[i] += bias_inc[i];
+            }
+        }
+    }
+}
+
+
 //! TODO: add "accumulate" here
 void RBMBinomialLayer::bpropUpdate(const Vec& input, const Vec& rbm_bias,
                                    const Vec& output,

Modified: trunk/plearn_learners/online/RBMBinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.h	2007-04-24 15:00:43 UTC (rev 6932)
+++ trunk/plearn_learners/online/RBMBinomialLayer.h	2007-04-24 15:06:51 UTC (rev 6933)
@@ -97,6 +97,12 @@
                              Vec& input_gradient, Vec& rbm_bias_gradient,
                              const Vec& output_gradient) ;
 
+    //! Back-propagate the output gradient to the input, and update parameters.
+    virtual void bpropUpdate(const Mat& inputs, const Mat& outputs,
+                             Mat& input_gradients,
+                             const Mat& output_gradients,
+                             bool accumulate = false);
+
     //! Computes the negative log-likelihood of target given the
     //! internal activations of the layer
     virtual real fpropNLL(const Vec& target);

Modified: trunk/plearn_learners/online/RBMConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMConnection.cc	2007-04-24 15:00:43 UTC (rev 6932)
+++ trunk/plearn_learners/online/RBMConnection.cc	2007-04-24 15:06:51 UTC (rev 6933)
@@ -229,6 +229,17 @@
     update();
 }
 
+void RBMConnection::update( const Mat& pos_down_values,
+                            const Mat& pos_up_values,
+                            const Mat& neg_down_values,
+                            const Mat& neg_up_values)
+{
+    // Not-so-efficient implementation.
+    accumulatePosStats( pos_down_values, pos_up_values );
+    accumulateNegStats( neg_down_values, neg_up_values );
+    update();
+}
+
 ///////////
 // fprop //
 ///////////

Modified: trunk/plearn_learners/online/RBMConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMConnection.h	2007-04-24 15:00:43 UTC (rev 6932)
+++ trunk/plearn_learners/online/RBMConnection.h	2007-04-24 15:06:51 UTC (rev 6933)
@@ -115,14 +115,19 @@
     //! afterwards.
     virtual void setAsDownInputs( const Mat& inputs ) const;
 
-    //! Accumulates positive phase statistics to *_pos_stats
+    //! Accumulates positive phase statistics to *_pos_stats.
     virtual void accumulatePosStats( const Vec& down_values,
                                      const Vec& up_values ) = 0;
 
-    //! Accumulates negative phase statistics to *_neg_stats
+    virtual void accumulatePosStats( const Mat& down_values,
+                                     const Mat& up_values ) = 0;
+
+    //! Accumulates negative phase statistics to *_neg_stats.
     virtual void accumulateNegStats( const Vec& down_values,
                                      const Vec& up_values ) = 0;
 
+    virtual void accumulateNegStats( const Mat& down_values,
+                                     const Mat& up_values ) = 0;
 
     //! Updates parameters according to contrastive divergence gradient
     virtual void update() = 0;
@@ -134,14 +139,12 @@
                          const Vec& neg_down_values,
                          const Vec& neg_up_values);
 
-    // TODO Implement (in sub-classes too).
+    //! Updates parameters according to contrastive divergence gradient,
+    //! not using the statistics but explicit matrix values.
     virtual void update( const Mat& pos_down_values,
                          const Mat& pos_up_values,
                          const Mat& neg_down_values,
-                         const Mat& neg_up_values)
-    {
-        PLASSERT( false );
-    }
+                         const Mat& neg_up_values);
 
     //! Clear all information accumulated during stats
     virtual void clearStats() = 0;

Modified: trunk/plearn_learners/online/RBMConv2DConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMConv2DConnection.h	2007-04-24 15:00:43 UTC (rev 6932)
+++ trunk/plearn_learners/online/RBMConv2DConnection.h	2007-04-24 15:06:51 UTC (rev 6933)
@@ -110,10 +110,22 @@
     virtual void accumulatePosStats( const Vec& down_values,
                                      const Vec& up_values );
 
+    virtual void accumulatePosStats( const Mat& down_values,
+                                     const Mat& up_values )
+    {
+        PLASSERT_MSG( false, "Not implemented" );
+    }
+
     //! Accumulates negative phase statistics to *_neg_stats
     virtual void accumulateNegStats( const Vec& down_values,
                                      const Vec& up_values );
 
+    virtual void accumulateNegStats( const Mat& down_values,
+                                     const Mat& up_values )
+    {
+        PLASSERT_MSG( false, "Not implemented" );
+    }
+
     //! Updates parameters according to contrastive divergence gradient
     virtual void update();
 
@@ -124,6 +136,15 @@
                          const Vec& neg_down_values,
                          const Vec& neg_up_values );
 
+    //! Not implemented.
+    virtual void update( const Mat& pos_down_values,
+                         const Mat& pos_up_values,
+                         const Mat& neg_down_values,
+                         const Mat& neg_up_values)
+    {
+        PLASSERT_MSG( false, "Not implemented" );
+    }
+
     //! Clear all information accumulated during stats
     virtual void clearStats();
 

Modified: trunk/plearn_learners/online/RBMGaussianLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.h	2007-04-24 15:00:43 UTC (rev 6932)
+++ trunk/plearn_learners/online/RBMGaussianLayer.h	2007-04-24 15:06:51 UTC (rev 6933)
@@ -95,6 +95,15 @@
                              Vec& input_gradient, const Vec& output_gradient,
                              bool accumulate=false);
 
+    //! Back-propagate the output gradient to the input, and update parameters.
+    virtual void bpropUpdate(const Mat& inputs, const Mat& outputs,
+                             Mat& input_gradients,
+                             const Mat& output_gradients,
+                             bool accumulate = false)
+    {
+        PLASSERT_MSG(false, "Not implemented");
+    }
+
     //! Accumulates positive phase statistics
     virtual void accumulatePosStats( const Vec& pos_values );
 
@@ -107,6 +116,12 @@
     //! Update parameters according to one pair of vectors
     virtual void update( const Vec& pos_values, const Vec& neg_values );
 
+    //! Not implemented.
+    virtual void update( const Mat& pos_values, const Mat& neg_values )
+    {
+        PLASSERT_MSG(false, "Not implemented");
+    }
+
     //! resets activations, sample, expectation and sigma fields
     virtual void reset();
 

Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2007-04-24 15:00:43 UTC (rev 6932)
+++ trunk/plearn_learners/online/RBMLayer.cc	2007-04-24 15:06:51 UTC (rev 6933)
@@ -120,6 +120,9 @@
                     "output_size = size");
 }
 
+////////////
+// build_ //
+////////////
 void RBMLayer::build_()
 {
     if( size <= 0 )
@@ -139,6 +142,9 @@
     bias_neg_stats.resize( size );
 }
 
+///////////
+// build //
+///////////
 void RBMLayer::build()
 {
     inherited::build();
@@ -146,6 +152,9 @@
 }
 
 
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
 void RBMLayer::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
@@ -156,10 +165,11 @@
     deepCopyField(sample,         copies);
     deepCopyField(samples,        copies);
     deepCopyField(expectation,    copies);
-    deepCopyField(expectations,   copies);
     deepCopyField(bias_pos_stats, copies);
     deepCopyField(bias_neg_stats, copies);
     deepCopyField(bias_inc,       copies);
+    deepCopyField(ones,           copies);
+    deepCopyField(expectations,   copies);
 }
 
 
@@ -174,6 +184,9 @@
 }
 
 
+///////////////////////
+// getUnitActivation //
+///////////////////////
 void RBMLayer::getUnitActivation( int i, PP<RBMConnection> rbmc, int offset )
 {
     Vec act = activation.subVec(i,1);
@@ -313,9 +326,6 @@
     clearStats();
 }
 
-////////////
-// update //
-////////////
 void RBMLayer::update( const Vec& pos_values, const Vec& neg_values)
 {
     // bias -= learning_rate * (pos_values - neg_values)
@@ -340,6 +350,38 @@
     }
 }
 
+void RBMLayer::update( const Mat& pos_values, const Mat& neg_values)
+{
+    // bias -= learning_rate * (pos_values - neg_values)
+
+    if (ones.length() < pos_values.length()) {
+        ones.resize(pos_values.length());
+        ones.fill(1);
+    } else if (ones.length() > pos_values.length())
+        // No need to fill with ones since we are only shrinking the vector.
+        ones.resize(pos_values.length());
+
+
+    if( momentum == 0. )
+    {
+        productScaleAcc(bias, pos_values, true, ones, -learning_rate, 1);
+        productScaleAcc(bias, neg_values, true, ones,  learning_rate, 1);
+    }
+    else
+    {
+        PLERROR("RBMLayer::update - Not implemented yet");
+        /*
+        bias_inc.resize( size );
+        real* binc = bias_inc.data();
+        for( int i=0 ; i<size ; i++ )
+        {
+            binc[i] = momentum*binc[i] + learning_rate*( nv[i] - pv[i] );
+            b[i] += binc[i];
+        }
+        */
+    }
+}
+
 ////////////////
 // setAllBias //
 ////////////////

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2007-04-24 15:00:43 UTC (rev 6932)
+++ trunk/plearn_learners/online/RBMLayer.h	2007-04-24 15:06:51 UTC (rev 6933)
@@ -157,12 +157,11 @@
                              const Vec& output_gradient,
                              bool accumulate=false) = 0 ;
 
+    //! Back-propagate the output gradient to the input, and update parameters.
     virtual void bpropUpdate(const Mat& inputs, const Mat& outputs,
                              Mat& input_gradients,
                              const Mat& output_gradients,
-                             bool accumulate=false) {
-        PLASSERT( false );
-    }
+                             bool accumulate=false) = 0;
 
     //! back-propagates the output gradient to the input and the bias
     virtual void bpropUpdate(const Vec& input, const Vec& rbm_bias,
@@ -190,10 +189,8 @@
     //! Update parameters according to one pair of vectors
     virtual void update( const Vec& pos_values, const Vec& neg_values );
 
-    // TODO Implement (in sub-classes too).
-    virtual void update( const Mat& pos_values, const Mat& neg_values ) {
-        PLASSERT( false );
-    }
+    //! Update parameters according to one pair of matrices.
+    virtual void update( const Mat& pos_values, const Mat& neg_values );
 
     //! resets activations, sample and expectation fields
     virtual void reset();
@@ -239,6 +236,9 @@
     //! Stores the momentum of the gradient
     Vec bias_inc;
 
+    //! A vector containing only ones, used to compute efficiently mini-batch
+    //! updates.
+    Vec ones;
 
     //! Count of positive examples
     int pos_count;

Modified: trunk/plearn_learners/online/RBMMatrixConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.cc	2007-04-24 15:00:43 UTC (rev 6932)
+++ trunk/plearn_learners/online/RBMMatrixConnection.cc	2007-04-24 15:06:51 UTC (rev 6933)
@@ -117,6 +117,9 @@
     pos_count++;
 }
 
+////////////////////////
+// accumulateNegStats //
+////////////////////////
 void RBMMatrixConnection::accumulateNegStats( const Vec& down_values,
                                               const Vec& up_values )
 {
@@ -126,6 +129,9 @@
     neg_count++;
 }
 
+////////////
+// update //
+////////////
 void RBMMatrixConnection::update()
 {
     // updates parameters
@@ -230,6 +236,54 @@
     }
 }
 
+void RBMMatrixConnection::update( const Mat& pos_down_values, // v_0
+                                  const Mat& pos_up_values,   // h_0
+                                  const Mat& neg_down_values, // v_1
+                                  const Mat& neg_up_values )  // h_1
+{
+    // weights -= learning_rate * ( h_0 v_0' - h_1 v_1' );
+    // or:
+    // weights[i][j] += learning_rate * (h_1[i] v_1[j] - h_0[i] v_0[j]);
+
+    PLASSERT( pos_up_values.width() == weights.length() );
+    PLASSERT( neg_up_values.width() == weights.length() );
+    PLASSERT( pos_down_values.width() == weights.width() );
+    PLASSERT( neg_down_values.width() == weights.width() );
+
+    if( momentum == 0. )
+    {
+        productScaleAcc(weights, pos_up_values, true, pos_down_values, false,
+                -learning_rate, 1);
+
+        productScaleAcc(weights, neg_up_values, true, neg_down_values, false,
+                learning_rate, 1);
+    }
+    else
+    {
+        PLERROR("RBMMatrixConnection::update - Not implemented");
+        /*
+        // ensure that weights_inc has the right size
+        weights_inc.resize( l, w );
+
+        // The update rule becomes:
+        // weights_inc = momentum * weights_inc
+        //               - learning_rate * ( h_0 v_0' - h_1 v_1' );
+        // weights += weights_inc;
+
+        real* winc_i = weights_inc.data();
+        int winc_mod = weights_inc.mod();
+        for( int i=0 ; i<l ; i++, w_i += w_mod, winc_i += winc_mod,
+                             puv_i++, nuv_i++ )
+            for( int j=0 ; j<w ; j++ )
+            {
+                winc_i[j] = momentum * winc_i[j]
+                    + learning_rate * (*nuv_i * ndv[j] - *puv_i * pdv[j]);
+                w_i[j] += winc_i[j];
+            }
+         */
+    }
+}
+
 ////////////////
 // clearStats //
 ////////////////
@@ -306,13 +360,13 @@
         PLASSERT( start+length <= down_size );
         // activations(k, i-start) += sum_j weights(j,i) inputs_mat(k, j)
         if( accumulate )
-            transposeProductAcc( activations,
-                                 weights.subMatColumns(start,length),
-                                 inputs_mat );
+            productAcc(activations,
+                    inputs_mat,
+                    weights.subMatColumns(start,length) );
         else
-            transposeProduct( activations,
-                              weights.subMatColumns(start,length),
-                              inputs_mat );
+            product(activations,
+                    inputs_mat,
+                    weights.subMatColumns(start,length) );
     }
 }
 

Modified: trunk/plearn_learners/online/RBMMatrixConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.h	2007-04-24 15:00:43 UTC (rev 6932)
+++ trunk/plearn_learners/online/RBMMatrixConnection.h	2007-04-24 15:06:51 UTC (rev 6933)
@@ -85,10 +85,22 @@
     virtual void accumulatePosStats( const Vec& down_values,
                                      const Vec& up_values );
 
+    virtual void accumulatePosStats( const Mat& down_values,
+                                     const Mat& up_values )
+    {
+        PLASSERT_MSG( false, "Not implemented" );
+    }
+
     //! Accumulates negative phase statistics to *_neg_stats
     virtual void accumulateNegStats( const Vec& down_values,
                                      const Vec& up_values );
 
+    virtual void accumulateNegStats( const Mat& down_values,
+                                     const Mat& up_values )
+    {
+        PLASSERT_MSG( false, "Not implemented" );
+    }
+
     //! Updates parameters according to contrastive divergence gradient
     virtual void update();
 
@@ -99,6 +111,12 @@
                          const Vec& neg_down_values,
                          const Vec& neg_up_values );
 
+    //! Not implemented.
+    virtual void update( const Mat& pos_down_values,
+                         const Mat& pos_up_values,
+                         const Mat& neg_down_values,
+                         const Mat& neg_up_values);
+
     //! Clear all information accumulated during stats
     virtual void clearStats();
 

Modified: trunk/plearn_learners/online/RBMMixedConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMMixedConnection.h	2007-04-24 15:00:43 UTC (rev 6932)
+++ trunk/plearn_learners/online/RBMMixedConnection.h	2007-04-24 15:06:51 UTC (rev 6933)
@@ -95,10 +95,22 @@
     virtual void accumulatePosStats( const Vec& down_values,
                                      const Vec& up_values );
 
+    virtual void accumulatePosStats( const Mat& down_values,
+                                     const Mat& up_values )
+    {
+        PLASSERT_MSG( false, "Not implemented" );
+    }
+
     //! Accumulates negative phase statistics to *_neg_stats
     virtual void accumulateNegStats( const Vec& down_values,
                                      const Vec& up_values );
 
+    virtual void accumulateNegStats( const Mat& down_values,
+                                     const Mat& up_values )
+    {
+        PLASSERT_MSG( false, "Not implemented" );
+    }
+
     //! Updates parameters according to contrastive divergence gradient
     virtual void update();
 
@@ -108,6 +120,16 @@
                          const Vec& pos_up_values,
                          const Vec& neg_down_values,
                          const Vec& neg_up_values );
+
+    //! Not implemented.
+    virtual void update( const Mat& pos_down_values,
+                         const Mat& pos_up_values,
+                         const Mat& neg_down_values,
+                         const Mat& neg_up_values)
+    {
+        PLASSERT_MSG( false, "Not implemented" );
+    }
+
     //! Clear all information accumulated during stats
     virtual void clearStats();
 

Modified: trunk/plearn_learners/online/RBMMixedLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.cc	2007-04-24 15:00:43 UTC (rev 6932)
+++ trunk/plearn_learners/online/RBMMixedLayer.cc	2007-04-24 15:06:51 UTC (rev 6933)
@@ -61,6 +61,9 @@
 }
 
 
+/////////////////////
+// setLearningRate //
+/////////////////////
 void RBMMixedLayer::setLearningRate( real the_learning_rate )
 {
     inherited::setLearningRate( the_learning_rate );
@@ -69,6 +72,9 @@
         sub_layers[i]->setLearningRate( the_learning_rate );
 }
 
+/////////////////
+// setMomentum //
+/////////////////
 void RBMMixedLayer::setMomentum( real the_momentum )
 {
     inherited::setMomentum( the_momentum );
@@ -111,6 +117,9 @@
         sub_layers[i]->generateSample();
 }
 
+////////////////////////
+// computeExpectation //
+////////////////////////
 void RBMMixedLayer::computeExpectation()
 {
     if( expectation_is_up_to_date )
@@ -159,6 +168,9 @@
 }
 
 
+/////////////////
+// bpropUpdate //
+/////////////////
 void RBMMixedLayer::bpropUpdate( const Vec& input, const Vec& output,
                                  Vec& input_gradient,
                                  const Vec& output_gradient,
@@ -175,6 +187,7 @@
     }
     else
         input_gradient.resize( size );
+    // TODO Should we clear the input gradient here?
 
     for( int i=0 ; i<n_layers ; i++ )
     {
@@ -191,6 +204,42 @@
     }
 }
 
+void RBMMixedLayer::bpropUpdate(const Mat& inputs, const Mat& outputs,
+        Mat& input_gradients,
+        const Mat& output_gradients,
+        bool accumulate)
+{
+    PLASSERT( inputs.width() == size );
+    PLASSERT( outputs.width() == size );
+    PLASSERT( output_gradients.width() == size );
+
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradients.width() == size &&
+                input_gradients.length() == inputs.length(),
+                "Cannot resize input_gradients and accumulate into it" );
+    }
+    else
+        input_gradients.resize(inputs.length(), size);
+    // TODO Should we clear the input gradient here?
+
+    for( int i=0 ; i<n_layers ; i++ )
+    {
+        int begin = init_positions[i];
+        int size_i = sub_layers[i]->size;
+        Mat sub_inputs = inputs.subMatColumns( begin, size_i );
+        Mat sub_outputs = outputs.subMatColumns( begin, size_i );
+        Mat sub_input_gradients =
+            input_gradients.subMatColumns( begin, size_i );
+        Mat sub_output_gradients =
+            output_gradients.subMatColumns( begin, size_i );
+
+        sub_layers[i]->bpropUpdate( sub_inputs, sub_outputs,
+                sub_input_gradients, sub_output_gradients,
+                accumulate );
+    }
+}
+
 void RBMMixedLayer::bpropUpdate(const Vec& input, const Vec& rbm_bias,
                                 const Vec& output,
                                 Vec& input_gradient, Vec& rbm_bias_gradient,

Modified: trunk/plearn_learners/online/RBMMixedLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.h	2007-04-24 15:00:43 UTC (rev 6932)
+++ trunk/plearn_learners/online/RBMMixedLayer.h	2007-04-24 15:06:51 UTC (rev 6933)
@@ -111,6 +111,12 @@
                              Vec& input_gradient, const Vec& output_gradient,
                              bool accumulate=false);
 
+    //! Back-propagate the output gradient to the input, and update parameters.
+    virtual void bpropUpdate(const Mat& inputs, const Mat& outputs,
+                             Mat& input_gradients,
+                             const Mat& output_gradients,
+                             bool accumulate = false);
+
     //! back-propagates the output gradient to the input and the bias
     virtual void bpropUpdate(const Vec& input, const Vec& rbm_bias,
                              const Vec& output,
@@ -137,6 +143,12 @@
     //! Update parameters according to one pair of vectors
     virtual void update( const Vec& pos_values, const Vec& neg_values );
 
+    //! Not implemented.
+    virtual void update( const Mat& pos_values, const Mat& neg_values )
+    {
+        PLASSERT_MSG(false, "Not implemented");
+    }
+
     //! resets activations, sample and expectation fields
     virtual void reset();
 

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-04-24 15:00:43 UTC (rev 6932)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-04-24 15:06:51 UTC (rev 6933)
@@ -114,6 +114,9 @@
     softmaxMinus( input+bias, output );
 }
 
+///////////
+// fprop //
+///////////
 void RBMMultinomialLayer::fprop( const Vec& input, const Vec& rbm_bias,
                                  Vec& output ) const
 {
@@ -125,6 +128,9 @@
     softmaxMinus( input+rbm_bias, output );
 }
 
+/////////////////
+// bpropUpdate //
+/////////////////
 void RBMMultinomialLayer::bpropUpdate(const Vec& input, const Vec& output,
                                       Vec& input_gradient,
                                       const Vec& output_gradient,
@@ -178,6 +184,66 @@
     }
 }
 
+void RBMMultinomialLayer::bpropUpdate(const Mat& inputs, const Mat& outputs,
+        Mat& input_gradients,
+        const Mat& output_gradients,
+        bool accumulate)
+{
+    PLASSERT( inputs.width() == size );
+    PLASSERT( outputs.width() == size );
+    PLASSERT( output_gradients.width() == size );
+
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradients.width() == size &&
+                input_gradients.length() == inputs.length(),
+                "Cannot resize input_gradient and accumulate into it." );
+    }
+    else
+    {
+        input_gradients.resize(inputs.length(), size);
+        input_gradients.fill(0);
+    }
+
+    PLERROR("In RBMMultinomialLayer::bpropUpdate - Not yet fully implemented "
+            "for mini-batches");
+
+    /*
+
+    if( momentum != 0. )
+        bias_inc.resize( size );
+
+    // input_gradient[i] =
+    //      (output_gradient . output - output_gradient[i] ) output[i]
+    real outg_dot_out = dot( output_gradient, output );
+    real* out = output.data();
+    real* outg = output_gradient.data();
+    real* ing = input_gradient.data();
+    real* b = bias.data();
+    real* binc = momentum==0?0:bias_inc.data();
+
+    for( int i=0 ; i<size ; i++ )
+    {
+        real ing_i = (outg_dot_out - outg[i]) * out[i];
+        ing[i] += ing_i;
+
+        if( momentum == 0. )
+        {
+            // update the bias: bias -= learning_rate * input_gradient
+            b[i] -= learning_rate * ing_i;
+        }
+        else
+        {
+            // The update rule becomes:
+            // bias_inc = momentum * bias_inc - learning_rate * input_gradient
+            // bias += bias_inc
+            binc[i] = momentum * binc[i] - learning_rate * ing_i;
+            b[i] += binc[i];
+        }
+    }
+    */
+}
+
 //! TODO: add "accumulate" here
 void RBMMultinomialLayer::bpropUpdate(const Vec& input, const Vec& rbm_bias,
                                       const Vec& output,
@@ -204,6 +270,9 @@
     rbm_bias_gradient << input_gradient;
 }
 
+//////////////
+// fpropNLL //
+//////////////
 real RBMMultinomialLayer::fpropNLL(const Vec& target)
 {
     computeExpectation();

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.h	2007-04-24 15:00:43 UTC (rev 6932)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.h	2007-04-24 15:06:51 UTC (rev 6933)
@@ -93,6 +93,12 @@
                              Vec& input_gradient, const Vec& output_gradient,
                              bool accumulate=false);
 
+    //! Back-propagate the output gradient to the input, and update parameters.
+    virtual void bpropUpdate(const Mat& inputs, const Mat& outputs,
+                             Mat& input_gradients,
+                             const Mat& output_gradients,
+                             bool accumulate = false);
+
     //! back-propagates the output gradient to the input and the bias
     virtual void bpropUpdate(const Vec& input, const Vec& rbm_bias,
                              const Vec& output,

Modified: trunk/plearn_learners/online/RBMTruncExpLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMTruncExpLayer.h	2007-04-24 15:00:43 UTC (rev 6932)
+++ trunk/plearn_learners/online/RBMTruncExpLayer.h	2007-04-24 15:06:51 UTC (rev 6933)
@@ -90,6 +90,15 @@
                              Vec& input_gradient, const Vec& output_gradient,
                              bool accumulate=false);
 
+    //! Back-propagate the output gradient to the input, and update parameters.
+    virtual void bpropUpdate(const Mat& inputs, const Mat& outputs,
+                             Mat& input_gradients,
+                             const Mat& output_gradients,
+                             bool accumulate = false)
+    {
+        PLASSERT_MSG(false, "Not implemented");
+    }
+
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.



From tihocan at mail.berlios.de  Tue Apr 24 17:13:14 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 24 Apr 2007 17:13:14 +0200
Subject: [Plearn-commits] r6934 - trunk/plearn_learners/online
Message-ID: <200704241513.l3OFDEcH015324@sheep.berlios.de>

Author: tihocan
Date: 2007-04-24 17:13:14 +0200 (Tue, 24 Apr 2007)
New Revision: 6934

Modified:
   trunk/plearn_learners/online/CombiningCostsModule.cc
Log:
Added missing deep copy statements due to previous commit

Modified: trunk/plearn_learners/online/CombiningCostsModule.cc
===================================================================
--- trunk/plearn_learners/online/CombiningCostsModule.cc	2007-04-24 15:06:51 UTC (rev 6933)
+++ trunk/plearn_learners/online/CombiningCostsModule.cc	2007-04-24 15:13:14 UTC (rev 6934)
@@ -146,12 +146,13 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
-    deepCopyField(sub_costs,            copies);
-    deepCopyField(cost_weights,         copies);
-    deepCopyField(sub_costs_values,     copies);
-    deepCopyField(sub_costs_mbatch_values, copies);
-    deepCopyField(partial_gradient,     copies);
-    deepCopyField(partial_diag_hessian, copies);
+    deepCopyField(sub_costs,                copies);
+    deepCopyField(cost_weights,             copies);
+    deepCopyField(sub_costs_values,         copies);
+    deepCopyField(sub_costs_mbatch_values,  copies);
+    deepCopyField(partial_gradient,         copies);
+    deepCopyField(partial_gradients,        copies);
+    deepCopyField(partial_diag_hessian,     copies);
 }
 
 



From yoshua at mail.berlios.de  Tue Apr 24 19:38:10 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Tue, 24 Apr 2007 19:38:10 +0200
Subject: [Plearn-commits] r6935 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200704241738.l3OHcAiC004731@sheep.berlios.de>

Author: yoshua
Date: 2007-04-24 19:38:09 +0200 (Tue, 24 Apr 2007)
New Revision: 6935

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
Log:
Ajout de features (pas encore toutes implantees) dans NatGradNNet


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-04-24 15:13:14 UTC (rev 6934)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-04-24 17:38:09 UTC (rev 6935)
@@ -63,6 +63,13 @@
       output_layer_lrate_scale(1),
       minibatch_size(1),
       output_type("NLL"),
+      input_size_lrate_normalization_power(0),
+      lrate_scale_factor(3),
+      lrate_scale_factor_max_power(0),
+      lrate_scale_factor_min_power(0),
+      self_adjusted_scaling_and_bias(false),
+      target_mean_activation(-4), // 
+      target_stdev_activation(3), // 2.5% of the time we are above 1
       verbosity(0),
       n_layers(-1),
       cumulative_training_time(0)
@@ -100,10 +107,15 @@
 
     declareOption(ol, "layer_params", &NatGradNNet::layer_params,
                   OptionBase::learntoption,
-                  "Training parameters for each layer, organized as follows: layer_params[i] \n"
+                  "Parameters used while training, for each layer, organized as follows: layer_params[i] \n"
                   "is a matrix of dimension layer_sizes[i+1] x (layer_sizes[i]+1)\n"
                   "containing the neuron biases in its first column.\n");
 
+    declareOption(ol, "activations_scaling", &NatGradNNet::activations_scaling,
+                  OptionBase::learntoption,
+                  "Scaling coefficients for each neuron of each layer, if self_adjusted_scaling_and_bias:\n"
+                  " output = tanh(activations_scaling[layer][neuron] * (biases[layer][neuron] + weights[layer]*input[layer-1])\n");
+
     declareOption(ol, "layer_mparams", &NatGradNNet::layer_mparams,
                   OptionBase::learntoption,
                   "Test parameters for each layer, organized like layer_params.\n"
@@ -180,6 +192,60 @@
                   "type of output cost: 'NLL' for classification problems,\n"
                   "or 'MSE' for regression.\n");
 
+    declareOption(ol, "input_size_lrate_normalization_power", 
+                  &NatGradNNet::input_size_lrate_normalization_power, 
+                  OptionBase::buildoption,
+                  "Scale the learning rate neuron-wise (or layer-wise actually, here):\n"
+                  "-1 scales by 1 / ||x||^2, where x is the 1-extended input vector of the neuron\n"
+                  "0 does not scale the learning rate\n"
+                  "1 scales it by 1 / the nb of inputs of the neuron\n"
+                  "2 scales it by 1 / sqrt(the nb of inputs of the neuron), etc.\n");
+
+    declareOption(ol, "lrate_scale_factor",
+                  &NatGradNNet::lrate_scale_factor,
+                  OptionBase::buildoption,
+                  "scale the learning rate in different neurons by a factor\n"
+                  "taken randomly as follows: choose integer n uniformly between\n"
+                  "lrate_scale_factor_min_power and lrate_scale_factor_max_power\n"
+                  "inclusively, and then scale learning rate by lrate_scale_factor^n.\n");
+
+    declareOption(ol, "lrate_scale_factor_max_power",
+                  &NatGradNNet::lrate_scale_factor_max_power,
+                  OptionBase::buildoption,
+                  "See help on lrate_scale_factor\n");
+
+    declareOption(ol, "lrate_scale_factor_min_power",
+                  &NatGradNNet::lrate_scale_factor_min_power,
+                  OptionBase::buildoption,
+                  "See help on lrate_scale_factor\n");
+
+    declareOption(ol, "self_adjusted_scaling_and_bias",
+                  &NatGradNNet::self_adjusted_scaling_and_bias,
+                  OptionBase::buildoption,
+                  "If true, let each neuron self-adjust its bias and scaling factor\n"
+                  "of its activations so that the mean and standard deviation of the\n"
+                  "activations reach the target_mean_activation and target_stdev_activation.\n"
+                  "The activations mean and variance are estimated by a moving average with\n"
+                  "coefficient given by activations_statistics_moving_average_coefficient\n");
+
+    declareOption(ol, "target_mean_activation",
+                  &NatGradNNet::target_mean_activation,
+                  OptionBase::buildoption,
+                  "See help on self_adjusted_scaling_and_bias\n");
+
+    declareOption(ol, "target_stdev_activation",
+                  &NatGradNNet::target_stdev_activation,
+                  OptionBase::buildoption,
+                  "See help on self_adjusted_scaling_and_bias\n");
+
+    declareOption(ol, "activation_statistics_moving_average_coefficient",
+                  &NatGradNNet::activation_statistics_moving_average_coefficient,
+                  OptionBase::buildoption,
+                  "The activations mean and variance used for self_adjusted_scaling_and_bias\n"
+                  "are estimated by a moving average with this coefficient:\n"
+                  "   xbar <-- coefficient * xbar + (1-coefficient) x\n"
+                  "where x could be the activation or its square\n");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -216,7 +282,11 @@
     layer_params_delta.resize(n_layers-1);
     layer_params_gradient.resize(n_layers-1);
     biases.resize(n_layers-1);
+    activations_scaling.resize(n_layers-1);
     weights.resize(n_layers-1);
+    mweights.resize(n_layers-1);
+    mean_activations.resize(n_layers-1);
+    var_activations.resize(n_layers-1);
     int n_neurons=0;
     int n_params=0;
     for (int i=0;i<n_layers-1;i++)
@@ -238,8 +308,12 @@
         layer_mparams[i]=all_mparams.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
         biases[i]=layer_params[i].subMatColumns(0,1);
         weights[i]=layer_params[i].subMatColumns(1,layer_sizes[i]); // weights[0] from layer 0 to layer 1
+        mweights[i]=layer_mparams[i].subMatColumns(1,layer_sizes[i]); // weights[0] from layer 0 to layer 1
+        activations_scaling[i].resize(layer_sizes[i+1]);
         layer_params_gradient[i]=all_params_gradient.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
         layer_params_delta[i]=all_params_delta.subVec(p,np);
+        mean_activations[i].resize(layer_sizes[i+1]);
+        var_activations[i].resize(layer_sizes[i+1]);
         for (int j=0;j<layer_sizes[i+1];j++,k++)
         {
             neuron_params[k]=all_params.subVec(p,1+layer_sizes[i]);
@@ -297,6 +371,10 @@
     deepCopyField(hidden_layer_sizes, copies);
     deepCopyField(layer_params, copies);
     deepCopyField(layer_mparams, copies);
+    deepCopyField(biases, copies);
+    deepCopyField(weights, copies);
+    deepCopyField(mweights, copies);
+    deepCopyField(activations_scaling, copies);
     deepCopyField(neurons_natgrad_template, copies);
     deepCopyField(neurons_natgrad_per_layer, copies);
     deepCopyField(params_natgrad_template, copies);
@@ -341,6 +419,9 @@
         real delta = 1/sqrt(real(layer_sizes[i]));
         random_gen->fill_random_uniform(weights[i],-delta,delta);
         biases[i].clear();
+        activations_scaling[i].fill(1.0);
+        mean_activations[i].clear();
+        var_activations[i].fill(1.0);
     }
     stage = 0;
     cumulative_training_time=0;
@@ -433,8 +514,28 @@
         //      (minibatch_size x layer_size[i])
 
         Mat previous_neurons_gradient = neuron_gradients_per_layer[i-1];
+        Mat next_neurons_gradient = neuron_gradients_per_layer[i];
         Mat previous_neurons_output = neuron_outputs_per_layer[i-1];
         real layer_lrate_factor = (i==n_layers-1)?output_layer_lrate_scale:1;
+        if (self_adjusted_scaling_and_bias && i+1<n_layers-1)
+            for (int k=0;k<minibatch_size;k++)
+            {
+                Vec g=next_neurons_gradient(k);
+                g*=activations_scaling[i]; // pass gradient through scaling
+            }
+        if (input_size_lrate_normalization_power==-1)
+            layer_lrate_factor /= (sumsquare(neuron_extended_outputs_per_layer[i-1])/minibatch_size);
+        else if (input_size_lrate_normalization_power==-2)
+            layer_lrate_factor /= sqrt(sumsquare(neuron_extended_outputs_per_layer[i-1])/minibatch_size);
+        else if (input_size_lrate_normalization_power!=0)
+        {
+            int fan_in = neuron_extended_outputs_per_layer[i-1].length();
+            if (input_size_lrate_normalization_power==1)
+                layer_lrate_factor/=fan_in;
+            else if (input_size_lrate_normalization_power==2)
+                layer_lrate_factor/=sqrt(real(fan_in));
+            else layer_lrate_factor/=pow(fan_in,1.0/input_size_lrate_normalization_power);
+        }
         // optionally correct the gradient on neurons using their covariance
         if (neurons_natgrad_template && neurons_natgrad_per_layer[i])
         {
@@ -442,7 +543,7 @@
             tmp.resize(layer_sizes[i]);
             for (int k=0;k<minibatch_size;k++)
             {
-                Vec g_k = neuron_gradients_per_layer[i](k);
+                Vec g_k = next_neurons_gradient(k);
                 (*neurons_natgrad_per_layer[i])(t-minibatch_size+1+k,g_k,tmp);
                 g_k << tmp;
             }
@@ -450,7 +551,7 @@
         if (i>1) // compute gradient on previous layer
         {
             // propagate gradients
-            productScaleAcc(previous_neurons_gradient,neuron_gradients_per_layer[i],false,
+            productScaleAcc(previous_neurons_gradient,next_neurons_gradient,false,
                             weights[i-1],false,1,0);
             // propagate through tanh non-linearity
             for (int j=0;j<previous_neurons_gradient.length();j++)
@@ -464,14 +565,15 @@
         // compute gradient on parameters, possibly update them
         if (full_natgrad || params_natgrad_template) 
         {
-            productScaleAcc(layer_params_gradient[i-1],neuron_gradients_per_layer[i],true,
+            productScaleAcc(layer_params_gradient[i-1],next_neurons_gradient,true,
                             neuron_extended_outputs_per_layer[i-1],false,1,0);
             layer_params_gradient[i-1] *= 1.0/minibatch_size; // use the MEAN gradient
         } else // just regular stochastic gradient
             // compute gradient on weights and update them in one go (more efficient)
-            productScaleAcc(layer_params[i-1],neuron_gradients_per_layer[i],true,
+            // mean gradient has less variance, can afford larger learning rate
+            productScaleAcc(layer_params[i-1],next_neurons_gradient,true,
                             neuron_extended_outputs_per_layer[i-1],false,
-                            -layer_lrate_factor*lrate/minibatch_size,1); // mean gradient, has less variance, can afford larger learning rate
+                            -layer_lrate_factor*lrate/minibatch_size,1);
     }
     if (full_natgrad)
     {
@@ -505,7 +607,8 @@
     PLASSERT_MSG(n_examples<=minibatch_size,"NatGradNNet::fpropNet: nb input vectors treated should be <= minibatch_size\n");
     for (int i=0;i<n_layers-1;i++)
     {
-        Mat prev_layer = neuron_extended_outputs_per_layer[i];
+        Mat prev_layer = self_adjusted_scaling_and_bias?
+            neuron_outputs_per_layer[i]:neuron_extended_outputs_per_layer[i];
         Mat next_layer = neuron_outputs_per_layer[i+1];
         if (n_examples!=minibatch_size)
         {
@@ -513,16 +616,49 @@
             next_layer = next_layer.subMatRows(0,n_examples);
         }
         // try to use BLAS for the expensive operation
-        productScaleAcc(next_layer, prev_layer, false, 
-                        (during_training || params_averaging_coeff==1.0)?
-                        layer_params[i]:layer_mparams[i], 
-                        true, 1, 0);
+        if (self_adjusted_scaling_and_bias && i+1<n_layers-1)
+            productScaleAcc(next_layer, prev_layer, false, 
+                            (during_training || params_averaging_coeff==1.0)?
+                            weights[i]:mweights[i], 
+                            true, 1, 0);
+        else
+            productScaleAcc(next_layer, prev_layer, false, 
+                            (during_training || params_averaging_coeff==1.0)?
+                            layer_params[i]:layer_mparams[i], 
+                            true, 1, 0);
         // compute layer's output non-linearity
         if (i+1<n_layers-1)
             for (int k=0;k<n_examples;k++)
             {
                 Vec L=next_layer(k);
-                compute_tanh(L,L);
+                if (self_adjusted_scaling_and_bias)
+                {
+                    real* m=mean_activations[i].data();
+                    real* v=var_activations[i].data();
+                    real* a=L.data();
+                    real* s=activations_scaling[i].data();
+                    real* b=biases[i].data(); // biases[i] is a 1-column matrix
+                    int bmod = biases[i].mod();
+                    for (int j=0;j<layer_sizes[i+1];j++,b+=bmod,m++,v++,a++,s++)
+                    {
+                        if (during_training)
+                        {
+                            real diff = *a - *m;
+                            *v = (1-activation_statistics_moving_average_coefficient) * *v
+                                + activation_statistics_moving_average_coefficient * diff*diff;
+                            *m = (1-activation_statistics_moving_average_coefficient) * *m
+                                + activation_statistics_moving_average_coefficient * *a;
+                            *b = target_mean_activation - *m;
+                            if (*v<1e6)
+                                *s = target_stdev_activation/sqrt(*v);
+                            else
+                                PLWARNING("NatGradNNet::fpropNet: activation variance >= 1e6!\n");
+                        }
+                        *a = tanh((*a + *b) * *s);
+                    }
+                }
+                else
+                    compute_tanh(L,L);
             }
         else if (output_type=="NLL")
             for (int k=0;k<n_examples;k++)

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-04-24 15:13:14 UTC (rev 6934)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-04-24 17:38:09 UTC (rev 6935)
@@ -103,6 +103,30 @@
     //! type of output cost: "NLL" for classification problems, "MSE" for regression
     string output_type;
 
+    //! 0 does not scale the learning rate
+    //! 1 scales it by 1 / the nb of inputs of the neuron
+    //! 2 scales it by 1 / sqrt(the nb of inputs of the neuron)
+    //! etc.
+    real input_size_lrate_normalization_power;
+
+    //! scale the learning rate in different neurons by a factor
+    //! taken randomly as follows: choose integer n uniformly between 
+    //! lrate_scale_factor_min_power and lrate_scale_factor_max_power
+    //! inclusively, and then scale learning rate by lrate_scale_factor^n.
+    real lrate_scale_factor;
+    int lrate_scale_factor_max_power;
+    int lrate_scale_factor_min_power;
+
+    //! Let each neuron self-adjust its bias and scaling factor of its activations
+    //! so that the mean and standard deviation of the activations reach 
+    //! the target_mean_activation and target_stdev_activation.
+    bool self_adjusted_scaling_and_bias;
+    real target_mean_activation;
+    real target_stdev_activation;
+    // the mean and variance of the activations is estimated by a moving
+    // average with this coefficient (near 0 for very slow averaging)
+    real activation_statistics_moving_average_coefficient;
+
     int verbosity;
 
 public:
@@ -193,8 +217,10 @@
 
     //! pointers into the layer_params
     TVec<Mat> biases;
-    TVec<Mat> weights;
-
+    TVec<Mat> weights,mweights;
+    TVec<Vec> activations_scaling; // output = tanh(activations_scaling[layer][neuron] * (biases[layer][neuron] + weights[layer]*input[layer-1])
+    TVec<Vec> mean_activations;
+    TVec<Vec> var_activations;
     real cumulative_training_time;
 
 protected:



From tihocan at mail.berlios.de  Tue Apr 24 19:59:47 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 24 Apr 2007 19:59:47 +0200
Subject: [Plearn-commits] r6936 - trunk/plearn_learners/online
Message-ID: <200704241759.l3OHxlht006096@sheep.berlios.de>

Author: tihocan
Date: 2007-04-24 19:59:46 +0200 (Tue, 24 Apr 2007)
New Revision: 6936

Modified:
   trunk/plearn_learners/online/RBMMixedLayer.cc
Log:
Added comment

Modified: trunk/plearn_learners/online/RBMMixedLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.cc	2007-04-24 17:38:09 UTC (rev 6935)
+++ trunk/plearn_learners/online/RBMMixedLayer.cc	2007-04-24 17:59:46 UTC (rev 6936)
@@ -186,8 +186,9 @@
                       "Cannot resize input_gradient AND accumulate into it" );
     }
     else
+        // Note that, by construction of 'size', the whole gradient vector
+        // should be cleared in the calls to sub_layers->bpropUpdate(..) below.
         input_gradient.resize( size );
-    // TODO Should we clear the input gradient here?
 
     for( int i=0 ; i<n_layers ; i++ )
     {
@@ -220,8 +221,9 @@
                 "Cannot resize input_gradients and accumulate into it" );
     }
     else
+        // Note that, by construction of 'size', the whole gradient vector
+        // should be cleared in the calls to sub_layers->bpropUpdate(..) below.
         input_gradients.resize(inputs.length(), size);
-    // TODO Should we clear the input gradient here?
 
     for( int i=0 ; i<n_layers ; i++ )
     {



From tihocan at mail.berlios.de  Tue Apr 24 20:00:38 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 24 Apr 2007 20:00:38 +0200
Subject: [Plearn-commits] r6937 - trunk/plearn_learners/online
Message-ID: <200704241800.l3OI0cOj006252@sheep.berlios.de>

Author: tihocan
Date: 2007-04-24 20:00:38 +0200 (Tue, 24 Apr 2007)
New Revision: 6937

Modified:
   trunk/plearn_learners/online/NLLCostModule.cc
Log:
Fixed bug in bpropUpdate when 'accumulate' is true

Modified: trunk/plearn_learners/online/NLLCostModule.cc
===================================================================
--- trunk/plearn_learners/online/NLLCostModule.cc	2007-04-24 17:59:46 UTC (rev 6936)
+++ trunk/plearn_learners/online/NLLCostModule.cc	2007-04-24 18:00:38 UTC (rev 6937)
@@ -120,8 +120,7 @@
     int the_target = (int) round( target[0] );
     // input_gradient[ i ] = 0 if i != t,
     // input_gradient[ t ] = -1/x[t]
-    input_gradient[ the_target ] = - 1. / input[ the_target ];
-    // TODO Is that a bug with accumulate?
+    input_gradient[ the_target ] -= 1. / input[ the_target ];
 }
 
 void NLLCostModule::bpropUpdate(const Mat& inputs, const Mat& targets,
@@ -146,8 +145,7 @@
     // input_gradient[ t ] = -1/x[t]
     for (int i = 0; i < inputs.length(); i++) {
         int the_target = (int) round( targets(i, 0) );
-        input_gradients(i, the_target) = - 1. / inputs(i, the_target);
-        // TODO Is that a bug with accumulate?
+        input_gradients(i, the_target) -= 1. / inputs(i, the_target);
     }
 }
 



From nouiz at mail.berlios.de  Tue Apr 24 21:17:09 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 24 Apr 2007 21:17:09 +0200
Subject: [Plearn-commits] r6938 - trunk/speedtest
Message-ID: <200704241917.l3OJH90W013405@sheep.berlios.de>

Author: nouiz
Date: 2007-04-24 21:17:07 +0200 (Tue, 24 Apr 2007)
New Revision: 6938

Modified:
   trunk/speedtest/xgemm.c
Log:
Allow matrix size where M!=N!=K for nvidia as cublas use colomn major matrix and C use row major matrix.
Allow compare with a reference matrix, when the matrix size is not square


Modified: trunk/speedtest/xgemm.c
===================================================================
--- trunk/speedtest/xgemm.c	2007-04-24 18:00:38 UTC (rev 6937)
+++ trunk/speedtest/xgemm.c	2007-04-24 19:17:07 UTC (rev 6938)
@@ -22,23 +22,25 @@
 //#include <iostream>
 //using namespace std;
 
+#if defined(CXGEMM) || defined(COMPARE)
 /* Host implementation of a simple version of sgemm */
-static void simple_xgemm(int n, real alpha, const real *A, const real *B,
-                         real beta, real *C)
+static void c_xgemm(int M, int N, int K, const real alpha, const real *A, 
+		  const real *B, const real beta, real *C)
 {
-    int i;
-    int j;
-    int k;
-    for (i = 0; i < n; ++i) {
-        for (j = 0; j < n; ++j) {
-            float prod = 0;
-            for (k = 0; k < n; ++k) {
-                prod += A[k * n + i] * B[j * n + k];
-            }
-            C[j * n + i] = alpha * prod + beta * C[j * n + i];
-        }
+  int i;
+  int j;
+  int k;
+  for (i = 0; i < M; ++i) {
+    for (j = 0; j < N; ++j) {
+      float prod = 0;
+      for (k = 0; k < K; ++k) {
+	prod += A[i * K + k] * B[k * N + j];
+      }
+      C[i * N + j] = alpha * prod + beta * C[i * N + j];
     }
+  }
 }
+#endif
 /* Main */
 int main(int argc, char** argv)
 {    
@@ -139,7 +141,7 @@
 #ifdef COMPARE
     /* Performs operation using plain C code */
     for (int i=0;i<NBITER;i++)
-      simple_xgemm(N, alpha, h_A, h_B, beta, h_C);
+      c_xgemm(M,N,K, alpha, h_A, h_B, beta, h_C);
     h_C_ref = h_C;
     /* Allocate host memory for reading back the result from device memory */
     h_C = (float*)malloc(NC * sizeof(h_C[0]));
@@ -151,7 +153,9 @@
 #ifdef NVIDIA
     /* Performs operation using cublas */
     for (int i=0;i<NBITER;i++)
-      cublasSgemm('n', 'n', M, N, K, alpha, d_A, M, d_B, K, beta, d_C, K);
+      //We must Change the order of the parameter as cublas take
+      //matrix as colomn major and C matrix is row major
+      cublasSgemm('n', 'n', N, M, K, alpha, d_B, N, d_A, K, beta, d_C, N);
 
     status = cublasGetError();
     if (status != CUBLAS_STATUS_SUCCESS) {
@@ -166,10 +170,10 @@
     }
 #elif defined( CXGEMM )
     for (int i=0;i<NBITER;i++)
-      simple_xgemm(N, alpha, h_A, h_B, beta, h_C);
+      c_xgemm(M,N,K, alpha, h_A, h_B, beta, h_C);
 #else
     for (int i=0;i<NBITER;i++)
-      cblas_xgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, M,N,K, 1.0, h_A, K, h_B, N, 0.0, h_C, N);
+      cblas_xgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, M,N,K, alpha, h_A, K, h_B, N, beta, h_C, N);
 #endif
 #ifdef COMPARE
     /* Check result against reference */



From yoshua at mail.berlios.de  Wed Apr 25 15:03:43 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Wed, 25 Apr 2007 15:03:43 +0200
Subject: [Plearn-commits] r6939 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200704251303.l3PD3hTv027585@sheep.berlios.de>

Author: yoshua
Date: 2007-04-25 15:03:43 +0200 (Wed, 25 Apr 2007)
New Revision: 6939

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
Log:
Debugging the self_adjusted_bias_and_scaling


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-04-24 19:17:07 UTC (rev 6938)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-04-25 13:03:43 UTC (rev 6939)
@@ -521,7 +521,7 @@
             for (int k=0;k<minibatch_size;k++)
             {
                 Vec g=next_neurons_gradient(k);
-                g*=activations_scaling[i]; // pass gradient through scaling
+                g*=activations_scaling[i-1]; // pass gradient through scaling
             }
         if (input_size_lrate_normalization_power==-1)
             layer_lrate_factor /= (sumsquare(neuron_extended_outputs_per_layer[i-1])/minibatch_size);
@@ -607,7 +607,7 @@
     PLASSERT_MSG(n_examples<=minibatch_size,"NatGradNNet::fpropNet: nb input vectors treated should be <= minibatch_size\n");
     for (int i=0;i<n_layers-1;i++)
     {
-        Mat prev_layer = self_adjusted_scaling_and_bias?
+        Mat prev_layer = (self_adjusted_scaling_and_bias && i+1<n_layers-1)?
             neuron_outputs_per_layer[i]:neuron_extended_outputs_per_layer[i];
         Mat next_layer = neuron_outputs_per_layer[i+1];
         if (n_examples!=minibatch_size)



From tihocan at mail.berlios.de  Wed Apr 25 17:00:43 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 25 Apr 2007 17:00:43 +0200
Subject: [Plearn-commits] r6940 - trunk/plearn_learners/online
Message-ID: <200704251500.l3PF0htg006685@sheep.berlios.de>

Author: tihocan
Date: 2007-04-25 17:00:42 +0200 (Wed, 25 Apr 2007)
New Revision: 6940

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
Log:
Added hack to allow to use the minibatch setting even with a size of 1, for debugging purpose (disabled by default)

Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-04-25 13:03:43 UTC (rev 6939)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-04-25 15:00:42 UTC (rev 6940)
@@ -42,6 +42,8 @@
 
 #include "DeepBeliefNet.h"
 
+#define minibatch_hack 0 // Do we force the minibatch setting? (debug hack)
+
 namespace PLearn {
 using namespace std;
 
@@ -670,7 +672,7 @@
                 // Do a step every 'minibatch_size' examples.
                 if (stage % minibatch_size == 0) {
                     int sample_start = stage % nsamples;
-                    if (batch_size > 1) {
+                    if (batch_size > 1 || minibatch_hack) {
                         train_set->getExamples(sample_start, minibatch_size,
                                 inputs, targets, weights, NULL, true);
                         greedyStep( inputs, targets, i );
@@ -802,7 +804,7 @@
                     setLearningRate( grad_learning_rate
                             / (1. + grad_decrease_ct * (stage - init_stage) ) );
 
-                if (minibatch_size > 1) {
+                if (minibatch_size > 1 || minibatch_hack) {
                     train_set->getExamples(sample_start, minibatch_size, inputs,
                             targets, weights, NULL, true);
                     fineTuningStep(inputs, targets, train_costs_m);
@@ -811,7 +813,7 @@
                     fineTuningStep( input, target, train_costs );
                 }
                 if (update_stats)
-                    if (minibatch_size > 1)
+                    if (minibatch_size > 1 || minibatch_hack)
                         for (int k = 0; k < minibatch_size; k++)
                             train_stats->update(train_costs_m(k));
                     else
@@ -898,22 +900,6 @@
     {
         if( final_module )
         {
-            /*
-            if (minibatch_size > 1) {
-                final_module->fprop( layers[ n_layers-1 ]->getExpectations(),
-                        final_cost_inputs );
-                final_cost->fprop( final_cost_inputs, targets,
-                        final_cost_values );
-                final_cost->bpropUpdate(final_cost_inputs, targets,
-                        final_cost_values.column(0),
-                        final_cost_gradients );
-
-                final_module->bpropUpdate(
-                        layers[ n_layers-1 ]->getExpectations(),
-                        final_cost_inputs,
-                        expectations_gradients[ n_layers-1 ],
-                        final_cost_gradients, true );
-            } else {*/
                 final_module->fprop( layers[ n_layers-1 ]->expectation,
                         final_cost_input );
                 final_cost->fprop( final_cost_input, target,
@@ -927,20 +913,9 @@
                         final_cost_input,
                         expectation_gradients[ n_layers-1 ],
                         final_cost_gradient, true );
-            //}
         }
         else
         {
-            /*
-            if (minibatch_size > 1) {
-                final_cost->fprop( layers[ n_layers-1 ]->getExpectations(),
-                        targets,
-                        final_cost_values );
-                final_cost->bpropUpdate( layers[ n_layers-1 ]->getExpectations(),
-                        targets, final_cost_values.column(0),
-                        expectations_gradients[n_layers-1],
-                        true);
-            } else {*/
                 final_cost->fprop( layers[ n_layers-1 ]->expectation,
                         target,
                         final_cost_value );
@@ -948,7 +923,6 @@
                         target, final_cost_value[0],
                         expectation_gradients[n_layers-1],
                         true);
-            //}
         }
 
         for (int j=0;j<final_cost_indices.length();j++)
@@ -1481,7 +1455,7 @@
     const PP<RBMLayer>& up_layer,
     bool nofprop)
 {
-    bool mbatch = minibatch_size > 1;
+    bool mbatch = minibatch_size > 1 || minibatch_hack;
 
     // positive phase
     if (!nofprop)



From manzagop at mail.berlios.de  Wed Apr 25 23:28:59 2007
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Wed, 25 Apr 2007 23:28:59 +0200
Subject: [Plearn-commits] r6941 - in trunk/plearn_learners_experimental: .
	netflix
Message-ID: <200704252128.l3PLSx67015809@sheep.berlios.de>

Author: manzagop
Date: 2007-04-25 23:28:58 +0200 (Wed, 25 Apr 2007)
New Revision: 6941

Added:
   trunk/plearn_learners_experimental/netflix/
   trunk/plearn_learners_experimental/netflix/NxProfileLearner.cc
   trunk/plearn_learners_experimental/netflix/NxProfileLearner.h
Log:
Implements an idea for netflix. See the twiki section Neurones/DeepNetsRecommandationSystems about this.


Added: trunk/plearn_learners_experimental/netflix/NxProfileLearner.cc
===================================================================
--- trunk/plearn_learners_experimental/netflix/NxProfileLearner.cc	2007-04-25 15:00:42 UTC (rev 6940)
+++ trunk/plearn_learners_experimental/netflix/NxProfileLearner.cc	2007-04-25 21:28:58 UTC (rev 6941)
@@ -0,0 +1,297 @@
+// -*- C++ -*-
+
+// NxProfileLearner.cc
+//
+// Copyright (C) 2007 Pierre-Antoine Manzagol
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pierre-Antoine Manzagol
+
+/*! \file NxProfileLearner.cc */
+
+
+#include "NxProfileLearner.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    NxProfileLearner,
+    "ONE LINE DESCRIPTION",
+    "MULTI-LINE \nHELP");
+
+NxProfileLearner::NxProfileLearner()    :   profile_dim(1),
+                                            slr(0.0),
+                                            dc(0.0),
+                                            n_films(17770),
+                                            n_users(480189)
+/* ### Initialize all fields to their default value here */
+{
+    // ...
+
+    // ### You may (or not) want to call build_() to finish building the object
+    // ### (doing so assumes the parent classes' build_() have been called too
+    // ### in the parent classes' constructors, something that you must ensure)
+
+    // ### If this learner needs to generate random numbers, uncomment the
+    // ### line below to enable the use of the inherited PRandom object.
+    // random_gen = new PRandom();
+    if( !random_gen)
+        random_gen = new PRandom;
+}
+
+void NxProfileLearner::declareOptions(OptionList& ol)
+{
+    // ### Declare all of this object's options here.
+    // ### For the "flags" of each option, you should typically specify
+    // ### one of OptionBase::buildoption, OptionBase::learntoption or
+    // ### OptionBase::tuningoption. If you don't provide one of these three,
+    // ### this option will be ignored when loading values from a script.
+    // ### You can also combine flags, for example with OptionBase::nosave:
+    // ### (OptionBase::buildoption | OptionBase::nosave)
+
+    declareOption(ol, "profile_dim", &NxProfileLearner::profile_dim,
+                  OptionBase::buildoption,
+                  "Dimension of the profiles to learn.");
+    declareOption(ol, "slr", &NxProfileLearner::slr,
+                  OptionBase::buildoption,
+                  "Starting learning rate.");
+    declareOption(ol, "dc", &NxProfileLearner::dc,
+                  OptionBase::buildoption,
+                  "Learning rate decrease constant.");
+
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void NxProfileLearner::build_()
+{
+    // ### This method should do the real building of the object,
+    // ### according to set 'options', in *any* situation.
+    // ### Typical situations include:
+    // ###  - Initial building of an object from a few user-specified options
+    // ###  - Building of a "reloaded" object: i.e. from the complete set of
+    // ###    all serialised options.
+    // ###  - Updating or "re-building" of an object after a few "tuning"
+    // ###    options have been modified.
+    // ### You should assume that the parent class' build_() has already been
+    // ### called.
+
+    if( !train_set )
+        return;
+
+    cout << "build()" << endl;
+
+    f_profiles.resize(n_films, profile_dim);     //! matrix of film profiles (n_films*profile_dim)
+    u_profiles.resize(n_users, profile_dim);     //! matrix of user profiles (n_users*profile_dim)
+
+    forget();
+}
+
+// ### Nothing to add here, simply calls build_
+void NxProfileLearner::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void NxProfileLearner::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(f_profiles, copies);
+    deepCopyField(u_profiles, copies);
+
+    // ### Remove this line when you have fully implemented this method.
+    //PLERROR("NxProfileLearner::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
+}
+
+
+int NxProfileLearner::outputsize() const
+{
+    // Compute and return the size of this learner's output (which typically
+    // may depend on its inputsize(), targetsize() and set options).
+    return 1;
+}
+
+void NxProfileLearner::forget()
+{
+    //! (Re-)initialize the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!)
+    /*!
+      A typical forget() method should do the following:
+      - call inherited::forget() to initialize its random number generator
+        with the 'seed' option
+      - initialize the learner's parameters, using this random generator
+      - stage = 0
+    */
+    inherited::forget();
+
+    cout << "forget" << endl;
+
+    //real delta = 1/sqrt(real(layer_sizes[i]));
+    real delta =  1.0/sqrt(real(profile_dim));;
+    random_gen->fill_random_uniform(u_profiles,-delta,delta);
+    random_gen->fill_random_uniform(f_profiles,-delta,delta);
+    stage = 0;
+
+}
+
+void NxProfileLearner::train()
+{
+    // The role of the train method is to bring the learner up to
+    // stage==nstages, updating train_stats with training costs measured
+    // on-line in the process.
+
+    static Vec input;  // static so we don't reallocate memory each time...
+    static Vec target; // (but be careful that static means shared!)
+    input.resize(inputsize());    // the train_set's inputsize()
+    target.resize(targetsize());  // the train_set's targetsize()
+    real weight, error, lr;
+
+    // This generic PLearner method does a number of standard stuff useful for
+    // (almost) any learner, and return 'false' if no training should take
+    // place. See PLearner.h for more details.
+    if (!initTrain())
+        return;
+
+    int nsamples = train_set->length();
+    // clear statistics of previous epoch
+    train_stats->forget();
+
+    PP<ProgressBar> pb;
+    if( report_progress && stage < nstages )
+        pb = new ProgressBar( "Training "+classname(), nstages-stage);
+
+    while(stage<nstages)
+    {
+        train_set->getExample(stage%nsamples, input, target, weight);
+
+        PLASSERT( (input[0]>=0) && (input[0]<n_films) && (input[1]>=0) && (input[1]<n_users) );
+
+        // save a function call by not using the functions
+        // We're using squared error cost, but dropping the 2 and taking the
+        // negative already
+        error = target[0] - dot( f_profiles((int)input[0]), u_profiles((int)input[1]) );
+
+    /*cout << " f " << filmProfileID << " " << f_profiles(filmProfileID) << endl;
+    cout << " u " << userProfileID << " " << u_profiles(userProfileID) << endl;
+    cout << "error " << error << endl;*/
+
+        lr = slr/(1.0 + stage*dc);
+        // Not quite exact. Should do exact (copy the f_profiles entry)? 
+        // Or perhaps alternate based on stage parity?
+        f_profiles((int)input[0]) += lr * error * u_profiles((int)input[1]);
+        u_profiles((int)input[1]) += lr * error * f_profiles((int)input[0]);
+
+        // TODO add regularization
+/*        real delta_L2 = learning_rate * L2_penalty_factor;
+        if( delta_L2 > 1 )
+            PLWARNING("GradNNetLayerModule::bpropUpdate:\n"
+                      "learning rate = %f is too large!\n", learning_rate);
+
+          if( delta_L2 > 0. )
+                w_[j] *= (1 - delta_L2);
+
+
+*/
+        //train_stats->update(train_costs)
+        ++stage;
+        if( pb )
+            pb->update( stage );
+    }
+
+    train_stats->finalize(); // finalize statistics for this epoch
+
+}
+
+// Compute the output from the input.
+void NxProfileLearner::computeOutput(const Vec& input, Vec& output) const
+{
+    int nout = outputsize();
+    output.resize(nout);
+
+    PLASSERT( (input[0]>=0) && (input[0]<n_films) && (input[1]>=0) && (input[1]<n_users) );
+
+    output[0] = dot( f_profiles((int)input[0]), u_profiles((int)input[1]) );
+
+/*    cout << " f " << filmProfileID << " " << f_profiles(filmProfileID) << endl;
+    cout << " u " << userProfileID << " " << u_profiles(userProfileID) << endl;
+    cout << "output[0] " << output[0];*/
+}
+
+// Compute the costs from *already* computed output.
+void NxProfileLearner::computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                           const Vec& target, Vec& costs) const
+{
+    real error = target[0] - output[0];
+    // the 16 is to put the error on the 1-5 rating basis
+    costs[0] = 16.0 * error * error;
+//cout << " error " << error << " cost[0] " << costs[0] << endl;
+}
+
+TVec<string> NxProfileLearner::getTestCostNames() const
+{
+    // Return the names of the costs computed by computeCostsFromOutputs
+    // (these may or may not be exactly the same as what's returned by
+    // getTrainCostNames).
+    return getTrainCostNames();
+}
+
+TVec<string> NxProfileLearner::getTrainCostNames() const
+{
+    // Return the names of the objective costs that the train method computes
+    // and for which it updates the VecStatsCollector train_stats
+    // (these may or may not be exactly the same as what's returned by
+    // getTestCostNames).
+    TVec<string> costs;
+    costs.resize(1);
+    costs[0]="MSE";
+    return costs;
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners_experimental/netflix/NxProfileLearner.h
===================================================================
--- trunk/plearn_learners_experimental/netflix/NxProfileLearner.h	2007-04-25 15:00:42 UTC (rev 6940)
+++ trunk/plearn_learners_experimental/netflix/NxProfileLearner.h	2007-04-25 21:28:58 UTC (rev 6941)
@@ -0,0 +1,207 @@
+// -*- C++ -*-
+
+// NxProfileLearner.h
+//
+// Copyright (C) 2007 Pierre-Antoine Manzagol
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pierre-Antoine Manzagol
+
+/*! \file NxProfileLearner.h */
+
+
+#ifndef NxProfileLearner_INC
+#define NxProfileLearner_INC
+
+#include <plearn_learners/generic/PLearner.h>
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Think about sorting the profiles by frequency to save access time.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class NxProfileLearner : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+
+    int profile_dim;    //! dimension of the profiles to be learnt
+    real slr;           //! start learning rate
+    real dc;            //! learning rate decrease constant
+
+    //#####  Public NOT Options  ##############################################
+
+    int n_films;
+    int n_users;
+
+    //map<int, int> u_map;  //! userID to userProfileID
+    // We are already using modified IDs... see about this
+    //map<int, int> f_map;  //! filmID to filmProfileID
+
+    // The rest of the private stuff goes here
+    Mat f_profiles;     //! matrix of film profiles (n_films*profile_dim)
+    Mat u_profiles;     //! matrix of user profiles (n_users*profile_dim)
+
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    NxProfileLearner();
+
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Returns the size of this learner's output, (which typically
+    //! may depend on its inputsize(), targetsize() and set options).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual int outputsize() const;
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training costs
+    //! measured on-line in the process.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void train();
+
+    //! Computes the output from the input.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeOutput(const Vec& input, Vec& output) const;
+
+    //! Computes the costs from already computed output.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                         const Vec& target, Vec& costs) const;
+
+    //! Returns the names of the costs computed by computeCostsFromOutpus (and
+    //! thus the test method).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec<std::string> getTestCostNames() const;
+
+    //! Returns the names of the objective costs that the train method computes
+    //! and  for which it updates the VecStatsCollector train_stats.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec<std::string> getTrainCostNames() const;
+
+
+    // *** SUBCLASS WRITING: ***
+    // While in general not necessary, in case of particular needs
+    // (efficiency concerns for ex) you may also want to overload
+    // some of the following methods:
+    // virtual void computeOutputAndCosts(const Vec& input, const Vec& target,
+    //                                    Vec& output, Vec& costs) const;
+    // virtual void computeCostsOnly(const Vec& input, const Vec& target,
+    //                               Vec& costs) const;
+    // virtual void test(VMat testset, PP<VecStatsCollector> test_stats,
+    //                   VMat testoutputs=0, VMat testcosts=0) const;
+    // virtual int nTestCosts() const;
+    // virtual int nTrainCosts() const;
+    // virtual void resetInternalState();
+    // virtual bool isStatefulLearner() const;
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(NxProfileLearner);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(NxProfileLearner);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From manzagop at mail.berlios.de  Thu Apr 26 01:06:24 2007
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Thu, 26 Apr 2007 01:06:24 +0200
Subject: [Plearn-commits] r6942 - trunk/plearn_learners_experimental/netflix
Message-ID: <200704252306.l3PN6Oct013890@sheep.berlios.de>

Author: manzagop
Date: 2007-04-26 01:06:23 +0200 (Thu, 26 Apr 2007)
New Revision: 6942

Modified:
   trunk/plearn_learners_experimental/netflix/NxProfileLearner.cc
Log:
Changed the significance of 'stage'. Using it in the sense of 'seeing an example' was impracticle, as the netflix dataset has 100M examples: 100 epochs means we explode an int's capacity. Stage is now taken to mean 'epoch'.


Modified: trunk/plearn_learners_experimental/netflix/NxProfileLearner.cc
===================================================================
--- trunk/plearn_learners_experimental/netflix/NxProfileLearner.cc	2007-04-25 21:28:58 UTC (rev 6941)
+++ trunk/plearn_learners_experimental/netflix/NxProfileLearner.cc	2007-04-25 23:06:23 UTC (rev 6942)
@@ -189,49 +189,52 @@
     // clear statistics of previous epoch
     train_stats->forget();
 
-    PP<ProgressBar> pb;
-    if( report_progress && stage < nstages )
-        pb = new ProgressBar( "Training "+classname(), nstages-stage);
 
     while(stage<nstages)
     {
-        train_set->getExample(stage%nsamples, input, target, weight);
+        PP<ProgressBar> pb;
+        if( report_progress )
+            pb = new ProgressBar( "Training "+classname(), nsamples);
+        
+        lr = slr/(1.0 + stage*dc);
 
-        PLASSERT( (input[0]>=0) && (input[0]<n_films) && (input[1]>=0) && (input[1]<n_users) );
+        for(int i=0; i<nsamples; i++)   {
+            train_set->getExample(i, input, target, weight);
 
-        // save a function call by not using the functions
-        // We're using squared error cost, but dropping the 2 and taking the
-        // negative already
-        error = target[0] - dot( f_profiles((int)input[0]), u_profiles((int)input[1]) );
+            PLASSERT( (input[0]>=0) && (input[0]<n_films) && (input[1]>=0) && (input[1]<n_users) );
 
-    /*cout << " f " << filmProfileID << " " << f_profiles(filmProfileID) << endl;
-    cout << " u " << userProfileID << " " << u_profiles(userProfileID) << endl;
-    cout << "error " << error << endl;*/
+            // save a function call by not using the functions
+            // We're using squared error cost, but dropping the 2 and taking the
+            // negative already
+            error = target[0] - dot( f_profiles((int)input[0]), u_profiles((int)input[1]) );
 
-        lr = slr/(1.0 + stage*dc);
-        // Not quite exact. Should do exact (copy the f_profiles entry)? 
-        // Or perhaps alternate based on stage parity?
-        f_profiles((int)input[0]) += lr * error * u_profiles((int)input[1]);
-        u_profiles((int)input[1]) += lr * error * f_profiles((int)input[0]);
+        /*cout << " f " << filmProfileID << " " << f_profiles(filmProfileID) << endl;
+        cout << " u " << userProfileID << " " << u_profiles(userProfileID) << endl;
+        cout << "error " << error << endl;*/
 
-        // TODO add regularization
-/*        real delta_L2 = learning_rate * L2_penalty_factor;
-        if( delta_L2 > 1 )
-            PLWARNING("GradNNetLayerModule::bpropUpdate:\n"
+            // Not quite exact. Should do exact (copy the f_profiles entry)? 
+            // Or perhaps alternate based on stage parity?
+            f_profiles((int)input[0]) += lr * error * u_profiles((int)input[1]);
+            u_profiles((int)input[1]) += lr * error * f_profiles((int)input[0]);
+
+            // TODO add regularization
+    /*        real delta_L2 = learning_rate * L2_penalty_factor;
+            if( delta_L2 > 1 )
+                PLWARNING("GradNNetLayerModule::bpropUpdate:\n"
                       "learning rate = %f is too large!\n", learning_rate);
 
-          if( delta_L2 > 0. )
-                w_[j] *= (1 - delta_L2);
+              if( delta_L2 > 0. )
+                    w_[j] *= (1 - delta_L2);
+*/
+            //train_stats->update(train_costs)
+            if( pb )
+                pb->update(i);
 
-
-*/
-        //train_stats->update(train_costs)
+        }
         ++stage;
-        if( pb )
-            pb->update( stage );
+        train_stats->finalize(); // finalize statistics for this epoch
     }
 
-    train_stats->finalize(); // finalize statistics for this epoch
 
 }
 



From dorionc at mail.berlios.de  Thu Apr 26 01:22:26 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Thu, 26 Apr 2007 01:22:26 +0200
Subject: [Plearn-commits] r6943 - trunk/plearn/math
Message-ID: <200704252322.l3PNMQTu009637@sheep.berlios.de>

Author: dorionc
Date: 2007-04-26 01:22:25 +0200 (Thu, 26 Apr 2007)
New Revision: 6943

Modified:
   trunk/plearn/math/VecStatsCollector.cc
   trunk/plearn/math/VecStatsCollector.h
Log:
Added an option to the window mechanism as to how to deal with updates which
contain NaNs


Modified: trunk/plearn/math/VecStatsCollector.cc
===================================================================
--- trunk/plearn/math/VecStatsCollector.cc	2007-04-25 23:06:23 UTC (rev 6942)
+++ trunk/plearn/math/VecStatsCollector.cc	2007-04-25 23:22:25 UTC (rev 6943)
@@ -52,6 +52,7 @@
       compute_covariance(false),
       epsilon(0.0),
       m_window(-1),
+      m_window_nan_code(0),
       no_removal_warnings(false), // Window mechanism
       sum_non_missing_weights(0),
       sum_non_missing_square_weights(0)
@@ -93,13 +94,27 @@
     declareOption(
         ol, "window", &VecStatsCollector::m_window,
         OptionBase::buildoption,
-        "If positive, the window restricts the stats computed by this"
+        "If positive, the window restricts the stats computed by this\n"
         "VecStatsCollector to the last 'window' observations. This uses the\n"
         "VecStatsCollector::remove_observation mechanism.\n"
         "Default: -1 (all observations are considered);\n"
         " -2 means all observations kept in an ObservationWindow\n");
-
+    
     declareOption(
+        ol, "window_nan_code", &VecStatsCollector::m_window_nan_code,
+        OptionBase::buildoption,
+        "How to deal with update vectors containing NaNs with respect to the\n"
+        "window mechanism.\n"
+        "\n"
+        " 0 - Do not check for NaNs (all updates are accounted in the window)\n"
+        " 1 - If *all* entries of the update vector are NaNs, do not account for\n"
+        "     that observation in the window.\n"
+        " 2 - If *any* entries of the update vector are NaNs, do not account for\n"
+        "     that observation in the window.\n"
+        "\n"
+        " Default: 0" );
+ 
+    declareOption(
         ol, "no_removal_warnings", &VecStatsCollector::no_removal_warnings,
         OptionBase::buildoption,
         "If the remove_observation mecanism is used and the removed\n"
@@ -284,7 +299,7 @@
     }
     
     // Window mechanism
-    if (m_window > 0 || m_window == -2)
+    if ( (m_window > 0 || m_window == -2) && shouldUpdateWindow(x) )
     {
         tuple<Vec, real> outdated = m_observation_window->update(x, weight);
         Vec& obs = get<0>(outdated);
@@ -294,6 +309,25 @@
     }
 }
 
+bool VecStatsCollector::shouldUpdateWindow(const Vec& x)
+{
+    // Avoid dealing with missings if not necessary
+    if ( m_window_nan_code > 0 )
+    {
+        int count = 0;
+        Vec::iterator it = x.begin();
+        Vec::iterator itend = x.end();
+        for(; it!=itend; ++it)
+            if(is_missing(*it))
+                count++;
+        
+        if ( (m_window_nan_code == 1 && count == x.length())
+             || (m_window_nan_code == 2 && count > 0) )
+            return false;
+    }
+    return true;
+}
+    
 ////////////////////////
 // remove_observation //
 ////////////////////////
@@ -375,6 +409,8 @@
 {
     if(!m_observation_window && (m_window > 0 || m_window == -2))
         m_observation_window = new ObservationWindow(m_window);
+    if( m_window_nan_code < 0 || m_window_nan_code > 2 )
+        PLERROR("The 'window_nan_code' option can only take values 0, 1 or 2.");
 }
 
 void VecStatsCollector::build()

Modified: trunk/plearn/math/VecStatsCollector.h
===================================================================
--- trunk/plearn/math/VecStatsCollector.h	2007-04-25 23:06:23 UTC (rev 6942)
+++ trunk/plearn/math/VecStatsCollector.h	2007-04-25 23:22:25 UTC (rev 6943)
@@ -84,6 +84,20 @@
     int m_window;
 
     /**
+     * How to deal with update vectors containing NaNs with respect to the
+     * window mechanism.
+     *
+     *  0 - Do not check for NaNs (all updates are accounted in the window)
+     *  1 - If *all* entries of the update vector are NaNs, do not account for
+     *      that observation in the window.
+     *  2 - If *any* entries of the update vector are NaNs, do not account for
+     *      that observation in the window.
+     *
+     *  Default: 0
+     */
+    int m_window_nan_code;
+    
+    /**
      * If the remove_observation mecanism is used and the removed
      * value is equal to one of first_, last_, min_ or max_, the default
      * behavior is to warn the user.
@@ -133,6 +147,9 @@
     //! The weight applies to all elements of x
     virtual void update(const Vec& x, real weight = 1.0);
 
+    //! Handling m_window_nan_code 
+    bool shouldUpdateWindow(const Vec& x);
+
     /*! 
      * Update statistics as if the vectorial observation x
      * was removed of the observation sequence.



From dorionc at mail.berlios.de  Thu Apr 26 02:44:51 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Thu, 26 Apr 2007 02:44:51 +0200
Subject: [Plearn-commits] r6944 - in trunk/plearn/opt/test/.pytest:
	PL_ConjGradientRosenbrock100/expected_results
	PL_ConjGradientRosenbrock2/expected_results
Message-ID: <200704260044.l3Q0ipNw019984@sheep.berlios.de>

Author: dorionc
Date: 2007-04-26 02:44:50 +0200 (Thu, 26 Apr 2007)
New Revision: 6944

Modified:
   trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock100/expected_results/ConjRosenbrock.psave
   trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock100/expected_results/RUN.log
   trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock2/expected_results/ConjRosenbrock.psave
   trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock2/expected_results/RUN.log
Log:
Updated results - serialization change in VecStatsCollector

Modified: trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock100/expected_results/ConjRosenbrock.psave
===================================================================
--- trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock100/expected_results/ConjRosenbrock.psave	2007-04-25 23:22:25 UTC (rev 6943)
+++ trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock100/expected_results/ConjRosenbrock.psave	2007-04-26 00:44:50 UTC (rev 6944)
@@ -9,6 +9,8 @@
 max_extrapolate = 3 ;
 max_eval_per_line_search = 20 ;
 slope_ratio = 10 ;
+minibatch_n_samples = 0 ;
+minibatch_n_line_searches = 3 ;
 nstages = 12  )
 ;
 D = 100 ;

Modified: trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock100/expected_results/RUN.log
===================================================================
--- trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock100/expected_results/RUN.log	2007-04-25 23:22:25 UTC (rev 6943)
+++ trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock100/expected_results/RUN.log	2007-04-26 00:44:50 UTC (rev 6944)
@@ -23,20 +23,20 @@
 angle : nan
 [ConjGradientOptimizer] Stage 0: 97.9894768544881885
 [ConjGradientOptimizer] Stage 1: 97.7218140342201451
-[ConjGradientOptimizer] Stage 2: 97.5964371877169583
-[ConjGradientOptimizer] Stage 3: 97.0150733297190015
-[ConjGradientOptimizer] Stage 4: 96.3406815074285845
-[ConjGradientOptimizer] Stage 5: 96.048840297136536
-[ConjGradientOptimizer] Stage 6: 95.3425240586193183
-[ConjGradientOptimizer] Stage 7: 95.0901929093151921
-[ConjGradientOptimizer] Stage 8: 94.789149254668942
-[ConjGradientOptimizer] Stage 9: 94.3880969305764808
-[ConjGradientOptimizer] Stage 10: 94.2032421985915107
-[ConjGradientOptimizer] Stage 11: 94.0699882228350504
+[ConjGradientOptimizer] Stage 2: 97.5964371877169441
+[ConjGradientOptimizer] Stage 3: 97.0150733297189873
+[ConjGradientOptimizer] Stage 4: 96.3406815074287977
+[ConjGradientOptimizer] Stage 5: 96.0488402971366213
+[ConjGradientOptimizer] Stage 6: 95.3425240586180109
+[ConjGradientOptimizer] Stage 7: 95.0901929093150358
+[ConjGradientOptimizer] Stage 8: 94.7891492546678052
+[ConjGradientOptimizer] Stage 9: 94.3880969305755428
+[ConjGradientOptimizer] Stage 10: 94.2032421985906723
+[ConjGradientOptimizer] Stage 11: 94.0699882228292523
 
 After optimization:
-inputs = 0.894919261690432233 0.812719803688390474 0.691262540185474172 0.479006601645307117 0.238513468194269856 0.0570540312296109325 0.011749065275106222 0.0109118700027758201 0.0109092123796657534 0.0109092092874087162 0.0109092092862516695 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.010909209286251!
 5446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515446 0.0109092092862515429 0.0109092092862!
 514943 0.0109092092862437245 0.010909209285987093 0.0109092093!
 20261506
1 0.0109092111322282269 0.0109091469760126888 0.0109062720560183391 0.0109672261114316938 0.0116566063972916457 0.000441438946423713431 0.00565243781348718794 
-output = 94.0699882228350504 
+inputs = 0.894919261690449663 0.812719803689734399 0.691262540187082664 0.479006601647567642 0.238513468196373202 0.057054031229707397 0.0117490652751253959 0.0109118700028127368 0.0109092123797027742 0.0109092092874457387 0.0109092092862886902 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.010909209286288!
 5653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862885653 0.0109092092862!
 88515 0.0109092092862807469 0.010909209286024112 0.01090920932!
 02985269
 0.0109092111322652459 0.0109091469760497183 0.010906272056055228 0.0109672261114673771 0.0116566063974134163 0.00044143894579159028 0.00565243781386380161 
+output = 94.0699882228292523 
 
 Optimization stats collector: VecStatsCollector(
 maxnvalues = 0;
@@ -45,17 +45,18 @@
 compute_covariance = 0;
 epsilon = 0;
 window = -1;
+window_nan_code = 0;
 no_removal_warnings = 0;
 stats = # samples: 12
 # missing: 0
-mean: 95.8829597321096543
-stddev: 1.43759313057391425
-stderr: 0.414997390461003091
-min: 94.0699882228350504
+mean: 95.8829597321088301
+stddev: 1.43759313057489235
+stderr: 0.414997390461285476
+min: 94.0699882228292523
 max: 97.9894768544881885
 
 first: 97.9894768544881885
-last:  94.0699882228350504
+last:  94.0699882228292523
 
 counts size: 0
  ;

Modified: trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock2/expected_results/ConjRosenbrock.psave
===================================================================
--- trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock2/expected_results/ConjRosenbrock.psave	2007-04-25 23:22:25 UTC (rev 6943)
+++ trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock2/expected_results/ConjRosenbrock.psave	2007-04-26 00:44:50 UTC (rev 6944)
@@ -9,6 +9,8 @@
 max_extrapolate = 3 ;
 max_eval_per_line_search = 20 ;
 slope_ratio = 10 ;
+minibatch_n_samples = 0 ;
+minibatch_n_line_searches = 3 ;
 nstages = 12  )
 ;
 D = 2 ;

Modified: trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock2/expected_results/RUN.log
===================================================================
--- trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock2/expected_results/RUN.log	2007-04-25 23:22:25 UTC (rev 6943)
+++ trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock2/expected_results/RUN.log	2007-04-26 00:44:50 UTC (rev 6944)
@@ -23,20 +23,20 @@
 angle : nan
 [ConjGradientOptimizer] Stage 0: 0.771109685362030506
 [ConjGradientOptimizer] Stage 1: 0.623820670484137363
-[ConjGradientOptimizer] Stage 2: 0.450241455686290259
-[ConjGradientOptimizer] Stage 3: 0.324894116831040636
-[ConjGradientOptimizer] Stage 4: 0.289080509880981829
-[ConjGradientOptimizer] Stage 5: 0.134528951900989568
-[ConjGradientOptimizer] Stage 6: 0.0451997258669274427
-[ConjGradientOptimizer] Stage 7: 0.0432489603494031899
-[ConjGradientOptimizer] Stage 8: 0.0243882970263747424
-[ConjGradientOptimizer] Stage 9: 0.00964802003379824739
-[ConjGradientOptimizer] Stage 10: 0.00349181359415567678
-[ConjGradientOptimizer] Stage 11: 0.000747066325395875615
+[ConjGradientOptimizer] Stage 2: 0.450241455686290704
+[ConjGradientOptimizer] Stage 3: 0.324894116831040858
+[ConjGradientOptimizer] Stage 4: 0.289080509880982384
+[ConjGradientOptimizer] Stage 5: 0.134528951900985683
+[ConjGradientOptimizer] Stage 6: 0.0451997258669295521
+[ConjGradientOptimizer] Stage 7: 0.0432489603494071728
+[ConjGradientOptimizer] Stage 8: 0.0243882970263967144
+[ConjGradientOptimizer] Stage 9: 0.0096480200338116464
+[ConjGradientOptimizer] Stage 10: 0.00349181359417502458
+[ConjGradientOptimizer] Stage 11: 0.000747066325399959479
 
 After optimization:
-inputs = 0.973055345897248625 0.947295530117985396 
-output = 0.000747066325395875615 
+inputs = 0.973055345897172463 0.947295530117836959 
+output = 0.000747066325399959479 
 
 Optimization stats collector: VecStatsCollector(
 maxnvalues = 0;
@@ -45,17 +45,18 @@
 compute_covariance = 0;
 epsilon = 0;
 window = -1;
+window_nan_code = 0;
 no_removal_warnings = 0;
 stats = # samples: 12
 # missing: 0
-mean: 0.226699939445127147
-stddev: 0.266560802017495546
-stderr: 0.0769494754001017883
-min: 0.000747066325395875615
+mean: 0.226699939445132337
+stddev: 0.266560802017491161
+stderr: 0.0769494754001005254
+min: 0.000747066325399959479
 max: 0.771109685362030506
 
 first: 0.771109685362030506
-last:  0.000747066325395875615
+last:  0.000747066325399959479
 
 counts size: 0
  ;



From chapados at mail.berlios.de  Fri Apr 27 04:20:08 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Fri, 27 Apr 2007 04:20:08 +0200
Subject: [Plearn-commits] r6945 - trunk/plearn/ker
Message-ID: <200704270220.l3R2K8hk031234@sheep.berlios.de>

Author: chapados
Date: 2007-04-27 04:20:07 +0200 (Fri, 27 Apr 2007)
New Revision: 6945

Modified:
   trunk/plearn/ker/IIDNoiseKernel.cc
   trunk/plearn/ker/IIDNoiseKernel.h
   trunk/plearn/ker/Kernel.h
   trunk/plearn/ker/RationalQuadraticARDKernel.cc
   trunk/plearn/ker/RationalQuadraticARDKernel.h
   trunk/plearn/ker/SummationKernel.cc
   trunk/plearn/ker/SummationKernel.h
Log:
* Made the functions evaluate_all_i_x and evaluate_all_x_i virtual in Kernel

* Overrides in some GaussianProcess-related kernels for performance



Modified: trunk/plearn/ker/IIDNoiseKernel.cc
===================================================================
--- trunk/plearn/ker/IIDNoiseKernel.cc	2007-04-26 00:44:50 UTC (rev 6944)
+++ trunk/plearn/ker/IIDNoiseKernel.cc	2007-04-27 02:20:07 UTC (rev 6945)
@@ -134,13 +134,31 @@
 
 real IIDNoiseKernel::evaluate_i_x(int i, const Vec& x, real) const
 {
-    // Noise component is ZERO between a test and any train example
+    // Noise component is ZERO between a test and any train example.
+    // Just compute the Kronecker part is not necessarily zero
     Vec* train_row = dataRow(i);
     PLASSERT( train_row );
     return softplus(m_isp_kronecker_sigma) * inherited::evaluate(*train_row, x);
 }
 
 
+//#####  evaluate_all_i_x  ####################################################
+
+void IIDNoiseKernel::evaluate_all_i_x(const Vec& x, const Vec& k_xi_x,
+                                      real, int istart) const
+{
+    // Noise component is ZERO between a test and any train example
+    k_xi_x.fill(0.0);
+    real kronecker_sigma = softplus(m_isp_kronecker_sigma);
+    int i_max = min(istart + k_xi_x.size(), data->length());
+    int j = 0;
+    for (int i=istart ; i<i_max ; ++i, ++j) {
+        Vec* train_row = dataRow(i);
+        k_xi_x[j] = kronecker_sigma * inherited::evaluate(*train_row, x);
+    }
+}
+
+
 //#####  computeGramMatrix  ###################################################
 
 void IIDNoiseKernel::computeGramMatrix(Mat K) const

Modified: trunk/plearn/ker/IIDNoiseKernel.h
===================================================================
--- trunk/plearn/ker/IIDNoiseKernel.h	2007-04-26 00:44:50 UTC (rev 6944)
+++ trunk/plearn/ker/IIDNoiseKernel.h	2007-04-27 02:20:07 UTC (rev 6945)
@@ -104,6 +104,11 @@
     //! Always zero by independence
     virtual real evaluate_i_x(int i, const Vec& x, real) const;
     
+    //! Fill k_xi_x with K(x_i, x), for all i from
+    //! istart to istart + k_xi_x.length() - 1.
+    virtual void evaluate_all_i_x(const Vec& x, const Vec& k_xi_x,
+                                  real squared_norm_of_x=-1, int istart = 0) const;
+
     //! Compute the Gram Matrix.  Note that this version DOES NOT CACHE
     //! the results, since it is usually called by derived classes.
     virtual void computeGramMatrix(Mat K) const;

Modified: trunk/plearn/ker/Kernel.h
===================================================================
--- trunk/plearn/ker/Kernel.h	2007-04-26 00:44:50 UTC (rev 6944)
+++ trunk/plearn/ker/Kernel.h	2007-04-27 02:20:07 UTC (rev 6945)
@@ -174,6 +174,14 @@
     virtual void setParameters(Vec paramvec); //!<  default version produces an error
     virtual Vec getParameters() const; //!<  default version returns an empty Vec
 
+    //! Fill k_xi_x with K(x_i, x), for all i from istart to istart + k_xi_x.length() - 1.
+    virtual void evaluate_all_i_x(const Vec& x, const Vec& k_xi_x,
+                                  real squared_norm_of_x=-1, int istart = 0) const;
+
+    //! Fill k_x_xi with K(x, x_i), for all i from istart to istart + k_x_xi.length() - 1.
+    virtual void evaluate_all_x_i(const Vec& x, const Vec& k_x_xi,
+                                  real squared_norm_of_x=-1, int istart = 0) const;
+
     //!  ** Subclasses should NOT override the following methods. The default versions are fine. **
 
     void apply(VMat m1, VMat m2, Mat& result) const; //!<  result(i,j) = K(m1(i),m2(j))
@@ -181,12 +189,6 @@
     void apply(VMat m, const Vec& x, Vec& result) const; //!<  result[i]=K(m[i],x)
     void apply(Vec x, VMat m, Vec& result) const; //!<  result[i]=K(x,m[i])
 
-    //! Fill k_xi_x with K(x_i, x), for all i from istart to istart + k_xi_x.length() - 1.
-    void evaluate_all_i_x(const Vec& x, const Vec& k_xi_x, real squared_norm_of_x=-1, int istart = 0) const;
-
-    //! Fill k_x_xi with K(x, x_i), for all i from istart to istart + k_x_xi.length() - 1.
-    void evaluate_all_x_i(const Vec& x, const Vec& k_x_xi, real squared_norm_of_x=-1, int istart = 0) const;
-
     inline real operator()(const Vec& x1, const Vec& x2) const
     {
         return evaluate(x1,x2);

Modified: trunk/plearn/ker/RationalQuadraticARDKernel.cc
===================================================================
--- trunk/plearn/ker/RationalQuadraticARDKernel.cc	2007-04-26 00:44:50 UTC (rev 6944)
+++ trunk/plearn/ker/RationalQuadraticARDKernel.cc	2007-04-27 02:20:07 UTC (rev 6945)
@@ -146,14 +146,12 @@
     real sf         = softplus(m_isp_signal_sigma);
     real alpha      = softplus(m_isp_alpha);
     real sum_wt     = 0.0;
-    real sum_sqdiff = 0.0;
     
     if (m_isp_input_sigma.size() > 0) {
         const real* pinpsig = m_isp_input_sigma.data();
         for (int i=0, n=x1.size() ; i<n ; ++i) {
             real diff   = *px1++ - *px2++;
             real sqdiff = diff * diff;
-            sum_sqdiff += sqdiff;
             sum_wt     += sqdiff / softplus(m_isp_global_sigma + *pinpsig++);
         }
     }
@@ -162,7 +160,6 @@
         for (int i=0, n=x1.size() ; i<n ; ++i) {
             real diff   = *px1++ - *px2++;
             real sqdiff = diff * diff;
-            sum_sqdiff += sqdiff;
             sum_wt     += sqdiff / global_sigma;
         }
     }
@@ -172,6 +169,52 @@
 }
 
 
+//#####  evaluate_all_i_x  ####################################################
+
+void RationalQuadraticARDKernel::evaluate_all_i_x(const Vec& x1, const Vec& k_xi_x,
+                                                  real , int istart) const
+{
+    if (x1.size() == 0) {
+        k_xi_x.fill(0.0);
+        return;
+    }
+ 
+    // Precompute some terms
+    real sf    = softplus(m_isp_signal_sigma);
+    real alpha = softplus(m_isp_alpha);
+    m_input_sigma.resize(dataInputsize());
+    for (int i=0, n=m_input_sigma.size() ; i<n ; ++i)
+        m_input_sigma[i] = softplus(m_isp_global_sigma + m_isp_input_sigma[i]);
+    
+    const real* px1_start = x1.data();
+    const real* pinpsig_start = m_input_sigma.data();
+    int i_max = min(istart + k_xi_x.size(), data->length());
+    int j = 0;
+    for (int i=istart ; i<i_max ; ++i, ++j) {
+        Vec* train_row = dataRow(i);
+        const real* px2 = train_row->data();
+        const real* px1 = px1_start;
+    
+        real gating_term = inherited::evaluate(x1,*train_row);
+        if (fast_is_equal(gating_term, 0.0)) {
+            k_xi_x[j] = 0.0;
+            continue;
+        }
+    
+        real sum_wt     = 0.0;
+        const real* pinpsig = pinpsig_start;
+        for (int i=0, n=x1.size() ; i<n ; ++i) {
+            real diff   = *px1++ - *px2++;
+            real sqdiff = diff * diff;
+            sum_wt     += sqdiff / *pinpsig++;
+        }
+
+        // Gate by Kronecker term
+        k_xi_x[j] = sf * pow(1 + sum_wt / (real(2.)*alpha), -alpha) * gating_term;
+    }
+}
+
+
 //#####  computeGramMatrix  ###################################################
 
 void RationalQuadraticARDKernel::computeGramMatrix(Mat K) const

Modified: trunk/plearn/ker/RationalQuadraticARDKernel.h
===================================================================
--- trunk/plearn/ker/RationalQuadraticARDKernel.h	2007-04-26 00:44:50 UTC (rev 6944)
+++ trunk/plearn/ker/RationalQuadraticARDKernel.h	2007-04-27 02:20:07 UTC (rev 6945)
@@ -102,6 +102,11 @@
     //! Compute K(x1,x2).
     virtual real evaluate(const Vec& x1, const Vec& x2) const;
 
+    //! Fill k_xi_x with K(x_i, x), for all i from
+    //! istart to istart + k_xi_x.length() - 1.
+    virtual void evaluate_all_i_x(const Vec& x, const Vec& k_xi_x,
+                                  real squared_norm_of_x=-1, int istart = 0) const;
+
     //! Compute entire Gram matrix
     virtual void computeGramMatrix(Mat K) const;
 

Modified: trunk/plearn/ker/SummationKernel.cc
===================================================================
--- trunk/plearn/ker/SummationKernel.cc	2007-04-26 00:44:50 UTC (rev 6944)
+++ trunk/plearn/ker/SummationKernel.cc	2007-04-27 02:20:07 UTC (rev 6945)
@@ -195,6 +195,27 @@
 }
 
 
+//#####  evaluate_all_i_x  ####################################################
+
+void SummationKernel::evaluate_all_i_x(const Vec& x, const Vec& k_xi_x,
+                                       real sq_norm_of_x, int istart) const
+{
+    k_xi_x.fill(0.0);
+    m_eval_buf.resize(k_xi_x.size());
+    bool split_inputs = m_input_indexes.size() > 0;
+    for (int i=0, n=m_terms.size() ; i<n ; ++i) {
+        // Note: if we slice x, we cannot rely on sq_norm_of_x any more...
+        if (split_inputs && m_input_indexes[i].size() > 0) {
+            selectElements(x, m_input_indexes[i], m_input_buf1[i]);
+            m_terms[i]->evaluate_all_i_x(m_input_buf1[i], m_eval_buf, -1, istart);
+        }
+        else
+            m_terms[i]->evaluate_all_i_x(x, m_eval_buf, sq_norm_of_x, istart);
+
+        k_xi_x += m_eval_buf;
+    }
+}
+
 //#####  computeGramMatrix  ###################################################
 
 void SummationKernel::computeGramMatrix(Mat K) const

Modified: trunk/plearn/ker/SummationKernel.h
===================================================================
--- trunk/plearn/ker/SummationKernel.h	2007-04-26 00:44:50 UTC (rev 6944)
+++ trunk/plearn/ker/SummationKernel.h	2007-04-27 02:20:07 UTC (rev 6945)
@@ -96,6 +96,11 @@
     //! Evaluate a test example x against a train example given by its index
     virtual real evaluate_i_x(int i, const Vec& x, real) const;
     
+    //! Fill k_xi_x with K(x_i, x), for all i from
+    //! istart to istart + k_xi_x.length() - 1.
+    virtual void evaluate_all_i_x(const Vec& x, const Vec& k_xi_x,
+                                  real squared_norm_of_x=-1, int istart = 0) const;
+
     //! Compute the Gram Matrix by calling subkernels computeGramMatrix
     virtual void computeGramMatrix(Mat K) const;
     
@@ -121,6 +126,9 @@
     TVec<Vec> m_input_buf1;
     TVec<Vec> m_input_buf2;
 
+    //! Temporary buffer for kernel evaluation on all training dataset
+    mutable Vec m_eval_buf;
+    
     //! Temporary buffer for Gram matrix accumulation
     mutable Mat m_gram_buf;
 



From chrish at mail.berlios.de  Fri Apr 27 20:39:49 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Fri, 27 Apr 2007 20:39:49 +0200
Subject: [Plearn-commits] r6946 - trunk/python_modules/plearn/vmat
Message-ID: <200704271839.l3RIdnY6028591@sheep.berlios.de>

Author: chrish
Date: 2007-04-27 20:39:48 +0200 (Fri, 27 Apr 2007)
New Revision: 6946

Modified:
   trunk/python_modules/plearn/vmat/readAMat.py
Log:
Handle tabs, etc. correctly (i.e. treat all whitespace the same way).


Modified: trunk/python_modules/plearn/vmat/readAMat.py
===================================================================
--- trunk/python_modules/plearn/vmat/readAMat.py	2007-04-27 02:20:07 UTC (rev 6945)
+++ trunk/python_modules/plearn/vmat/readAMat.py	2007-04-27 18:39:48 UTC (rev 6946)
@@ -57,14 +57,14 @@
     a = []
     for line in f:
         if line.startswith("#size:"):
-            (length,width) = line[6:].strip().split(" ")
+            (length,width) = line[6:].strip().split()
 
         elif line.startswith("#:"):
-            fieldnames = line[2:].strip().split(" ")
+            fieldnames = line[2:].strip().split()
             pass
 
         else:
-            row = [ safefloat(x) for x in line.strip().split(" ") ]
+            row = [ safefloat(x) for x in line.strip().split() ]
             a.append(row)
 
     f.close()



From larocheh at mail.berlios.de  Fri Apr 27 23:00:58 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 27 Apr 2007 23:00:58 +0200
Subject: [Plearn-commits] r6947 - trunk/plearn_learners/online
Message-ID: <200704272100.l3RL0wHe006309@sheep.berlios.de>

Author: larocheh
Date: 2007-04-27 23:00:57 +0200 (Fri, 27 Apr 2007)
New Revision: 6947

Modified:
   trunk/plearn_learners/online/RBMLayer.cc
   trunk/plearn_learners/online/RBMLayer.h
Log:
Added a function to update the bias according to a gradient provided as input...


Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2007-04-27 18:39:48 UTC (rev 6946)
+++ trunk/plearn_learners/online/RBMLayer.cc	2007-04-27 21:00:57 UTC (rev 6947)
@@ -67,6 +67,7 @@
     activation.clear();
     sample.clear();
     expectation.clear();
+    bias_inc.clear();
     expectation_is_up_to_date = false;
     expectations_are_up_to_date = false;
 }
@@ -326,6 +327,30 @@
     clearStats();
 }
 
+void RBMLayer::update( const Vec& grad )
+{   
+    real* b = bias.data();
+    real* gb = grad.data();
+    real* binc = momentum==0?0:bias_inc.data();
+
+    for( int i=0 ; i<size ; i++ )
+    {
+        if( momentum == 0. )
+        {
+            // update the bias: bias -= learning_rate * input_gradient
+            b[i] -= learning_rate * gb[i];
+        }
+        else
+        {
+            // The update rule becomes:
+            // bias_inc = momentum * bias_inc - learning_rate * input_gradient
+            // bias += bias_inc
+            binc[i] = momentum * binc[i] - learning_rate * gb[i];
+            b[i] += binc[i];
+        }
+    }
+}
+
 void RBMLayer::update( const Vec& pos_values, const Vec& neg_values)
 {
     // bias -= learning_rate * (pos_values - neg_values)

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2007-04-27 18:39:48 UTC (rev 6946)
+++ trunk/plearn_learners/online/RBMLayer.h	2007-04-27 21:00:57 UTC (rev 6947)
@@ -186,6 +186,9 @@
     //! Update parameters according to accumulated statistics
     virtual void update();
 
+    //! Updates parameters according to the given gradient
+    virtual void update( const Vec& grad );
+
     //! Update parameters according to one pair of vectors
     virtual void update( const Vec& pos_values, const Vec& neg_values );
 



From larocheh at mail.berlios.de  Fri Apr 27 23:02:21 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 27 Apr 2007 23:02:21 +0200
Subject: [Plearn-commits] r6948 - trunk/plearn_learners/online
Message-ID: <200704272102.l3RL2L1k006417@sheep.berlios.de>

Author: larocheh
Date: 2007-04-27 23:02:20 +0200 (Fri, 27 Apr 2007)
New Revision: 6948

Modified:
   trunk/plearn_learners/online/ClassErrorCostModule.h
   trunk/plearn_learners/online/CombiningCostsModule.cc
   trunk/plearn_learners/online/CombiningCostsModule.h
Log:
Added the setLearningRate(...) definitions...


Modified: trunk/plearn_learners/online/ClassErrorCostModule.h
===================================================================
--- trunk/plearn_learners/online/ClassErrorCostModule.h	2007-04-27 21:00:57 UTC (rev 6947)
+++ trunk/plearn_learners/online/ClassErrorCostModule.h	2007-04-27 21:02:20 UTC (rev 6948)
@@ -106,6 +106,8 @@
     //! build().
     virtual void forget();
 
+    //! Does nothing (there isn't a learning rate in this class)
+    virtual void setLearningRate(real dynamic_learning_rate) {}
 
     //! In case bpropUpdate does not do anything, make it known
     virtual bool bpropDoesNothing();

Modified: trunk/plearn_learners/online/CombiningCostsModule.cc
===================================================================
--- trunk/plearn_learners/online/CombiningCostsModule.cc	2007-04-27 21:00:57 UTC (rev 6947)
+++ trunk/plearn_learners/online/CombiningCostsModule.cc	2007-04-27 21:02:20 UTC (rev 6948)
@@ -372,6 +372,13 @@
         sub_costs[i]->forget();
 }
 
+//! Sets the sub_costs' learning rates
+void CombiningCostsModule::setLearningRate(real dynamic_learning_rate)
+{
+    for( int i=0 ; i<n_sub_costs ; i++ )
+        sub_costs[i]->setLearningRate(dynamic_learning_rate);
+}
+
 //! reset the parameters to the state they would be BEFORE starting training.
 //! Note that this method is necessarily called from build().
 void CombiningCostsModule::finalize()

Modified: trunk/plearn_learners/online/CombiningCostsModule.h
===================================================================
--- trunk/plearn_learners/online/CombiningCostsModule.h	2007-04-27 21:00:57 UTC (rev 6947)
+++ trunk/plearn_learners/online/CombiningCostsModule.h	2007-04-27 21:02:20 UTC (rev 6948)
@@ -103,14 +103,14 @@
     //! Calls this method on the sub_costs
     virtual void bbpropUpdate(const Vec& input, const Vec& target, real cost);
 
-    //! Overridden to do nothing (no warning message in particular).
-    virtual void setLearningRate(real dynamic_learning_rate) {}
-
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from
     //! build().
     virtual void forget();
 
+    //! Sets the sub_costs' learning rates
+    virtual void setLearningRate(real dynamic_learning_rate);
+
     //! optionally perform some processing after training, or after a
     //! series of fprop/bpropUpdate calls to prepare the model for truly
     //! out-of-sample operation.



From larocheh at mail.berlios.de  Fri Apr 27 23:09:54 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 27 Apr 2007 23:09:54 +0200
Subject: [Plearn-commits] r6949 - trunk/plearn_learners/online
Message-ID: <200704272109.l3RL9sZ9006788@sheep.berlios.de>

Author: larocheh
Date: 2007-04-27 23:09:52 +0200 (Fri, 27 Apr 2007)
New Revision: 6949

Added:
   trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc
   trunk/plearn_learners/online/RBMMatrixTransposeConnection.h
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
   trunk/plearn_learners/online/StackedAutoassociatorsNet.h
Log:
Code for Stacked Autoassociators neural network...


Added: trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc	2007-04-27 21:02:20 UTC (rev 6948)
+++ trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc	2007-04-27 21:09:52 UTC (rev 6949)
@@ -0,0 +1,398 @@
+// -*- C++ -*-
+
+// RBMMatrixTransposeConnection.cc
+//
+// Copyright (C) 2007 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file RBMMatrixTransposeConnection.cc */
+
+#include "RBMMatrixTransposeConnection.h"
+#include <plearn/math/TMat_maths.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    RBMMatrixTransposeConnection,
+    "RBMConnection which uses the tranpose of some other "
+    "RBMMatrixConnection's weights",
+    "");
+
+RBMMatrixTransposeConnection::RBMMatrixTransposeConnection( 
+    PP<RBMMatrixConnection> the_rbm_matrix_connection,
+    real the_learning_rate ) :
+    inherited(the_learning_rate), 
+    rbm_matrix_connection(the_rbm_matrix_connection)
+{}
+
+void RBMMatrixTransposeConnection::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "rbm_matrix_connection", 
+                  &RBMMatrixTransposeConnection::rbm_matrix_connection,
+                  OptionBase::buildoption,
+                  "RBMMatrixConnection from which the weights are taken");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+
+    redeclareOption(ol, "up_size", &RBMConnection::up_size,
+                    OptionBase::learntoption,
+                    "Is set to rbm_matrix_connection->down_size.");
+    redeclareOption(ol, "down_size", &RBMConnection::down_size,
+                    OptionBase::learntoption,
+                    "Is set to rbm_matrix_connection->up_size.");
+}
+
+void RBMMatrixTransposeConnection::build_()
+{
+    
+    if( !rbm_matrix_connection )
+        PLERROR("In RBMMatrixTransposeConnection::build_(): "
+            "rbm_matrix_connection needs to be provided.");
+
+
+    rbm_matrix_connection->build();
+    weights = rbm_matrix_connection->weights;
+    down_size = rbm_matrix_connection->up_size;
+    up_size = rbm_matrix_connection->down_size;
+
+    weights_pos_stats.resize( down_size, up_size );
+    weights_neg_stats.resize( down_size, up_size );
+
+    if( momentum != 0. )
+        weights_inc.resize( down_size, up_size );
+
+    clearStats();
+}
+
+void RBMMatrixTransposeConnection::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void RBMMatrixTransposeConnection::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(weights, copies);
+    deepCopyField(rbm_matrix_connection, copies);
+    deepCopyField(weights_pos_stats, copies);
+    deepCopyField(weights_neg_stats, copies);
+    deepCopyField(weights_inc, copies);
+}
+
+void RBMMatrixTransposeConnection::accumulatePosStats( const Vec& down_values,
+                                              const Vec& up_values )
+{
+    // weights_pos_stats += down_values * up_values'
+    externalProductAcc( weights_pos_stats, down_values, up_values );
+
+    pos_count++;
+}
+
+void RBMMatrixTransposeConnection::accumulateNegStats( const Vec& down_values,
+                                              const Vec& up_values )
+{
+    // weights_neg_stats += down_values * up_values'
+    externalProductAcc( weights_neg_stats, down_values, up_values );
+
+    neg_count++;
+}
+
+void RBMMatrixTransposeConnection::update()
+{
+    // updates parameters
+    //weights -= learning_rate * (weights_pos_stats/pos_count
+    //                              - weights_neg_stats/neg_count)
+    real pos_factor = -learning_rate / pos_count;
+    real neg_factor = learning_rate / neg_count;
+
+    int l = weights.length();
+    int w = weights.width();
+
+    real* w_i = weights.data();
+    real* wps_i = weights_pos_stats.data();
+    real* wns_i = weights_neg_stats.data();
+    int w_mod = weights.mod();
+    int wps_mod = weights_pos_stats.mod();
+    int wns_mod = weights_neg_stats.mod();
+
+    if( momentum == 0. )
+    {
+        // no need to use weights_inc
+        for( int i=0 ; i<l ; i++, w_i+=w_mod, wps_i+=wps_mod, wns_i+=wns_mod )
+            for( int j=0 ; j<w ; j++ )
+                w_i[j] += pos_factor * wps_i[j] + neg_factor * wns_i[j];
+    }
+    else
+    {
+        // ensure that weights_inc has the right size
+        weights_inc.resize( l, w );
+
+        // The update rule becomes:
+        // weights_inc = momentum * weights_inc
+        //               - learning_rate * (weights_pos_stats/pos_count
+        //                                  - weights_neg_stats/neg_count);
+        // weights += weights_inc;
+        real* winc_i = weights_inc.data();
+        int winc_mod = weights_inc.mod();
+        for( int i=0 ; i<l ; i++, w_i += w_mod, wps_i += wps_mod,
+                             wns_i += wns_mod, winc_i += winc_mod )
+            for( int j=0 ; j<w ; j++ )
+            {
+                winc_i[j] = momentum * winc_i[j]
+                    + pos_factor * wps_i[j] + neg_factor * wns_i[j];
+                w_i[j] += winc_i[j];
+            }
+    }
+
+    clearStats();
+}
+
+// Instead of using the statistics, we assume we have only one markov chain
+// runned and we update the parameters from the first 4 values of the chain
+void RBMMatrixTransposeConnection::update( const Vec& pos_down_values, // v_0
+                                  const Vec& pos_up_values,   // h_0
+                                  const Vec& neg_down_values, // v_1
+                                  const Vec& neg_up_values )  // h_1
+{
+    // weights -= learning_rate * ( h_0 v_0' - h_1 v_1' );
+    // or:
+    // weights[i][j] += learning_rate * (h_1[i] v_1[j] - h_0[i] v_0[j]);
+
+    int l = weights.length();
+    int w = weights.width();
+    PLASSERT( pos_up_values.length() == l );
+    PLASSERT( neg_up_values.length() == l );
+    PLASSERT( pos_down_values.length() == w );
+    PLASSERT( neg_down_values.length() == w );
+
+    real* w_i = weights.data();
+    real* puv_i = pos_up_values.data();
+    real* nuv_i = neg_up_values.data();
+    real* pdv = pos_down_values.data();
+    real* ndv = neg_down_values.data();
+    int w_mod = weights.mod();
+
+    if( momentum == 0. )
+    {
+        for( int i=0 ; i<l ; i++, w_i += w_mod, puv_i++, nuv_i++ )
+            for( int j=0 ; j<w ; j++ )
+                w_i[j] += learning_rate * (*nuv_i * ndv[j] - *puv_i * pdv[j]);
+    }
+    else
+    {
+        // ensure that weights_inc has the right size
+        weights_inc.resize( l, w );
+
+        // The update rule becomes:
+        // weights_inc = momentum * weights_inc
+        //               - learning_rate * ( h_0 v_0' - h_1 v_1' );
+        // weights += weights_inc;
+
+        real* winc_i = weights_inc.data();
+        int winc_mod = weights_inc.mod();
+        for( int i=0 ; i<l ; i++, w_i += w_mod, winc_i += winc_mod,
+                             puv_i++, nuv_i++ )
+            for( int j=0 ; j<w ; j++ )
+            {
+                winc_i[j] = momentum * winc_i[j]
+                    + learning_rate * (*nuv_i * ndv[j] - *puv_i * pdv[j]);
+                w_i[j] += winc_i[j];
+            }
+    }
+}
+
+void RBMMatrixTransposeConnection::clearStats()
+{
+    weights_pos_stats.clear();
+    weights_neg_stats.clear();
+
+    pos_count = 0;
+    neg_count = 0;
+}
+
+void RBMMatrixTransposeConnection::computeProduct( int start, int length,
+                                          const Vec& activations,
+                                          bool accumulate ) const
+{
+    PLASSERT( activations.length() == length );
+    if( going_up )
+    {
+        PLASSERT( start+length <= up_size );
+        // activations[i-start] += sum_j weights(i,j) input_vec[j]
+
+        if( accumulate )
+            transposeProductAcc( activations,
+                                 weights.subMatColumns(start,length),
+                                 input_vec );
+        else
+            transposeProduct( activations,
+                              weights.subMatColumns(start,length),
+                              input_vec );
+    }
+    else
+    {
+        PLASSERT( start+length <= down_size );
+        // activations[i-start] += sum_j weights(j,i) input_vec[j]
+        if( accumulate )
+            productAcc( activations,
+                        weights.subMatRows(start,length),
+                        input_vec );
+        else
+            product( activations,
+                     weights.subMatRows(start,length),
+                     input_vec );
+    }
+}
+
+void RBMMatrixTransposeConnection::computeProducts(int start, int length,
+                                          Mat& activations,
+                                          bool accumulate ) const
+{
+    activations.resize(inputs_mat.length(), length);
+    if( going_up )
+    {
+        PLASSERT( start+length <= up_size );
+        // activations(k, i-start) += sum_j weights(i,j) inputs_mat(k, j)
+
+        if( accumulate )
+            productAcc(activations,
+                    inputs_mat,
+                    weights.subMatColumns(start,length));
+        else
+            product(activations,
+                    inputs_mat,
+                    weights.subMatColumns(start,length));
+    }
+    else
+    {
+        PLASSERT( start+length <= down_size );
+        // activations(k, i-start) += sum_j weights(j,i) inputs_mat(k, j)
+        if( accumulate )
+            productTransposeAcc(activations,
+                    inputs_mat,
+                    weights.subMatRows(start,length) );
+        else
+            productTranspose(activations,
+                    inputs_mat,
+                    weights.subMatRows(start,length) );
+    }
+}
+
+//! this version allows to obtain the input gradient as well
+void RBMMatrixTransposeConnection::bpropUpdate(const Vec& input, 
+                                               const Vec& output,
+                                               Vec& input_gradient,
+                                               const Vec& output_gradient,
+                                               bool accumulate)
+{
+    PLASSERT( input.size() == down_size );
+    PLASSERT( output.size() == up_size );
+    PLASSERT( output_gradient.size() == up_size );
+
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == down_size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+
+        // input_gradient += weights' * output_gradient
+        productAcc( input_gradient, weights, output_gradient );
+    }
+    else
+    {
+        input_gradient.resize( down_size );
+        
+        // input_gradient = weights' * output_gradient
+        product( input_gradient, weights, output_gradient );
+    }
+    
+    // weights -= learning_rate * output_gradient * input'
+    externalProductScaleAcc( weights, input, output_gradient, -learning_rate );
+}
+
+//! reset the parameters to the state they would be BEFORE starting training.
+//! Note that this method is necessarily called from build().
+void RBMMatrixTransposeConnection::forget()
+{
+    rbm_matrix_connection->forget();
+    clearStats();
+}
+
+
+/* THIS METHOD IS OPTIONAL
+//! reset the parameters to the state they would be BEFORE starting training.
+//! Note that this method is necessarily called from build().
+//! THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS DOES NOT DO
+//! ANYTHING.
+void RBMMatrixTransposeConnection::finalize()
+{
+}
+*/
+
+//! return the number of parameters
+int RBMMatrixTransposeConnection::nParameters() const
+{
+    return weights.size();
+}
+
+//! Make the parameters data be sub-vectors of the given global_parameters.
+//! The argument should have size >= nParameters. The result is a Vec
+//! that starts just after this object's parameters end, i.e.
+//!    result = global_parameters.subVec(nParameters(),global_parameters.size()-nParameters());
+//! This allows to easily chain calls of this method on multiple RBMParameters.
+Vec RBMMatrixTransposeConnection::makeParametersPointHere(const Vec& global_parameters)
+{
+    Vec ret = rbm_matrix_connection->makeParametersPointHere(global_parameters);
+    weights = rbm_matrix_connection->weights;
+    return ret;
+}
+
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/RBMMatrixTransposeConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMMatrixTransposeConnection.h	2007-04-27 21:02:20 UTC (rev 6948)
+++ trunk/plearn_learners/online/RBMMatrixTransposeConnection.h	2007-04-27 21:09:52 UTC (rev 6949)
@@ -0,0 +1,221 @@
+// -*- C++ -*-
+
+// RBMMatrixTransposeConnection.h
+//
+// Copyright (C) 2007 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file RBMMatrixTransposeConnection.h */
+
+
+#ifndef RBMMatrixTransposeConnection_INC
+#define RBMMatrixTransposeConnection_INC
+
+#include <plearn_learners/online/RBMConnection.h>
+#include <plearn_learners/online/RBMMatrixConnection.h>
+
+namespace PLearn {
+using namespace std;
+
+
+/**
+ * RBMConnection which uses the tranpose of some other RBMMatrixConnection's weights
+ */
+class RBMMatrixTransposeConnection: public RBMConnection
+{
+    typedef RBMConnection inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //#####  Learned Options  #################################################
+
+    //! Matrix containing unit-to-unit weights (output_size \times input_size)
+    Mat weights;
+
+    //! RBMMatrixConnection from which the weights are taken
+    PP<RBMMatrixConnection> rbm_matrix_connection;
+
+    //#####  Not Options  #####################################################
+
+    //! Accumulates positive contribution to the weights' gradient
+    Mat weights_pos_stats;
+
+    //! Accumulates negative contribution to the weights' gradient
+    Mat weights_neg_stats;
+
+    //! Used if momentum != 0.
+    Mat weights_inc;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    RBMMatrixTransposeConnection( 
+        PP<RBMMatrixConnection> the_rbm_matrix_connection = 0,
+        real the_learning_rate=0 );
+
+    // Your other public member functions go here
+
+    //! Accumulates positive phase statistics to *_pos_stats
+    virtual void accumulatePosStats( const Vec& down_values,
+                                     const Vec& up_values );
+
+    virtual void accumulatePosStats( const Mat& down_values,
+                                     const Mat& up_values )
+    {
+        PLASSERT_MSG( false, "Not implemented" );
+    }
+
+    //! Accumulates negative phase statistics to *_neg_stats
+    virtual void accumulateNegStats( const Vec& down_values,
+                                     const Vec& up_values );
+
+    virtual void accumulateNegStats( const Mat& down_values,
+                                     const Mat& up_values )
+    {
+        PLASSERT_MSG( false, "Not implemented" );
+    }
+
+    //! Updates parameters according to contrastive divergence gradient
+    virtual void update();
+
+    //! Updates parameters according to contrastive divergence gradient,
+    //! not using the statistics but the explicit values passed
+    virtual void update( const Vec& pos_down_values,
+                         const Vec& pos_up_values,
+                         const Vec& neg_down_values,
+                         const Vec& neg_up_values );
+
+    //! Clear all information accumulated during stats
+    virtual void clearStats();
+
+    //! Computes the vectors of activation of "length" units,
+    //! starting from "start", and stores (or add) them into "activations".
+    //! "start" indexes an up unit if "going_up", else a down unit.
+    virtual void computeProduct( int start, int length,
+                                 const Vec& activations,
+                                 bool accumulate=false ) const;
+
+    //! Same as 'computeProduct' but for mini-batches.
+    virtual void computeProducts(int start, int length,
+                                 Mat& activations,
+                                 bool accumulate=false ) const;
+
+    //! Adapt based on the output gradient: this method should only
+    //! be called just after a corresponding fprop; it should be
+    //! called with the same arguments as fprop for the first two arguments
+    //! (and output should not have been modified since then).
+    //! Since sub-classes are supposed to learn ONLINE, the object
+    //! is 'ready-to-be-used' just after any bpropUpdate.
+    //! N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+    //! JUST CALLS
+    //!     bpropUpdate(input, output, input_gradient, output_gradient)
+    //! AND IGNORES INPUT GRADIENT.
+    // virtual void bpropUpdate(const Vec& input, const Vec& output,
+    //                          const Vec& output_gradient);
+
+    //! this version allows to obtain the input gradient as well
+    //! N.B. THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
+    virtual void bpropUpdate(const Vec& input, const Vec& output,
+                             Vec& input_gradient,
+                             const Vec& output_gradient,
+                             bool accumulate=false);
+
+    //! reset the parameters to the state they would be BEFORE starting
+    //! training.  Note that this method is necessarily called from
+    //! build().
+    virtual void forget();
+
+    //! optionally perform some processing after training, or after a
+    //! series of fprop/bpropUpdate calls to prepare the model for truly
+    //! out-of-sample operation.  THE DEFAULT IMPLEMENTATION PROVIDED IN
+    //! THE SUPER-CLASS DOES NOT DO ANYTHING.
+    // virtual void finalize();
+
+    //! return the number of parameters
+    virtual int nParameters() const;
+
+    //! Make the parameters data be sub-vectors of the given global_parameters.
+    //! The argument should have size >= nParameters. The result is a Vec
+    //! that starts just after this object's parameters end, i.e.
+    //!    result = global_parameters.subVec(nParameters(),global_parameters.size()-nParameters());
+    //! This allows to easily chain calls of this method on multiple RBMParameters.
+    virtual Vec makeParametersPointHere(const Vec& global_parameters);
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(RBMMatrixTransposeConnection);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Member Functions  ######################################
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(RBMMatrixTransposeConnection);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2007-04-27 21:02:20 UTC (rev 6948)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2007-04-27 21:09:52 UTC (rev 6949)
@@ -0,0 +1,854 @@
+// -*- C++ -*-
+
+// StackedAutoassociatorsNet.cc
+//
+// Copyright (C) 2007 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file StackedAutoassociatorsNet.cc */
+
+
+#define PL_LOG_MODULE_NAME "StackedAutoassociatorsNet"
+#include <plearn/io/pl_log.h>
+
+#include "StackedAutoassociatorsNet.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    StackedAutoassociatorsNet,
+    "Neural net, trained layer-wise in a greedy fashion using autoassociators",
+    "It is highly inspired by the DeepBeliefNet class, and can use the\n"
+    "same RBMLayer and RBMConnection components.\n"
+    );
+
+StackedAutoassociatorsNet::StackedAutoassociatorsNet() :
+    greedy_learning_rate( 0. ),
+    greedy_decrease_ct( 0. ),
+    fine_tuning_learning_rate( 0. ),
+    fine_tuning_decrease_ct( 0. ),
+    l1_neuron_decay( 0. ),
+    compute_all_test_costs( false ),
+    n_layers( 0 ),
+    currently_trained_layer( 0 ),
+    final_module_has_learning_rate( false ),
+    final_cost_has_learning_rate( false )
+{
+    random_gen = new PRandom( seed_ );
+    nstages = 0;
+}
+
+void StackedAutoassociatorsNet::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "greedy_learning_rate", 
+                  &StackedAutoassociatorsNet::greedy_learning_rate,
+                  OptionBase::buildoption,
+                  "The learning rate used during the autoassociator "
+                  "gradient descent training");
+
+    declareOption(ol, "greedy_decrease_ct", 
+                  &StackedAutoassociatorsNet::greedy_decrease_ct,
+                  OptionBase::buildoption,
+                  "The decrease constant of the learning rate used during "
+                  "the autoassociator\n"
+                  "gradient descent training. When a hidden layer has finished "
+                  "its training,\n"
+                  "the learning rate is reset to it's initial value.\n");
+
+    declareOption(ol, "fine_tuning_learning_rate", 
+                  &StackedAutoassociatorsNet::fine_tuning_learning_rate,
+                  OptionBase::buildoption,
+                  "The learning rate used during the fine tuning gradient descent");
+
+    declareOption(ol, "fine_tuning_decrease_ct", 
+                  &StackedAutoassociatorsNet::fine_tuning_decrease_ct,
+                  OptionBase::buildoption,
+                  "The decrease constant of the learning rate used during "
+                  "fine tuning\n"
+                  "gradient descent.\n");
+
+    declareOption(ol, "l1_neuron_decay", 
+                  &StackedAutoassociatorsNet::l1_neuron_decay,
+                  OptionBase::buildoption,
+                  " L1 penalty weight on the hidden layers, to encourage "
+                  "sparsity during\n"
+                  "the greedy unsupervised phases.\n");
+
+    declareOption(ol, "training_schedule", 
+                  &StackedAutoassociatorsNet::training_schedule,
+                  OptionBase::buildoption,
+                  "Number of examples to use during each phase of learning:\n"
+                  "first the greedy phases, and then the gradient descent.\n"
+                  "Unlike for DeepBeliefNet, these numbers should not be\n"
+                  "cumulative. They correspond to the number of seen training\n"
+                  "examples for each phase.\n"
+        );
+
+    declareOption(ol, "layers", &StackedAutoassociatorsNet::layers,
+                  OptionBase::buildoption,
+                  "The layers of units in the network. The first element\n"
+                  "of this vector should be the input layer and the\n"
+                  "subsequent elements should be the hidden layers. The\n"
+                  "should not be included in this layer.\n");
+
+    declareOption(ol, "connections", &StackedAutoassociatorsNet::connections,
+                  OptionBase::buildoption,
+                  "The weights of the connections between the layers");
+
+    declareOption(ol, "reconstruction_connections", 
+                  &StackedAutoassociatorsNet::reconstruction_connections,
+                  OptionBase::buildoption,
+                  "The weights of the reconstruction connections between the "
+                  "layers");
+
+    declareOption(ol, "final_module", &StackedAutoassociatorsNet::final_module,
+                  OptionBase::buildoption,
+                  "Module that takes as input the output of the last layer\n"
+                  "(layers[n_layers-1), and feeds its output to final_cost\n"
+                  "which defines the fine-tuning criteria.\n"
+                 );
+
+    declareOption(ol, "final_cost", &StackedAutoassociatorsNet::final_cost,
+                  OptionBase::buildoption,
+                  "The cost function to be applied on top of the neural network\n"
+                  "(i.e. at the output of final_module). Its gradients will be \n"
+                  "backpropagated to final_module and then backpropagated to\n"
+                  "the layers.\n"
+                  );
+
+    declareOption(ol, "partial_costs", &StackedAutoassociatorsNet::partial_costs,
+                  OptionBase::buildoption,
+                  "Corresponding additional supervised cost function to be "
+                  "applied on \n"
+                  "top of each hidden layer during the autoassociator "
+                  "training stages. \n"
+                  "The gradient for these costs are not backpropagated to "
+                  "previous layers.\n"
+        );
+
+    declareOption(ol, "partial_costs_weights", 
+                  &StackedAutoassociatorsNet::partial_costs_weights,
+                  OptionBase::buildoption,
+                  "Relative weights of the partial costs. If not defined,\n"
+                  "weights of 1 will be assumed for all partial costs.\n"
+        );
+
+    declareOption(ol, "compute_all_test_costs", 
+                  &StackedAutoassociatorsNet::compute_all_test_costs,
+                  OptionBase::buildoption,
+                  "Indication that, at test time, all costs for all layers \n"
+                  "(up to the currently trained layer) should be computed.\n"
+        );
+
+    declareOption(ol, "greedy_stages", 
+                  &StackedAutoassociatorsNet::greedy_stages,
+                  OptionBase::learntoption,
+                  "Number of training samples seen in the different greedy "
+                  "phases.\n"
+        );
+
+    declareOption(ol, "n_layers", &StackedAutoassociatorsNet::n_layers,
+                  OptionBase::learntoption,
+                  "Number of layers");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void StackedAutoassociatorsNet::build_()
+{
+    // ### This method should do the real building of the object,
+    // ### according to set 'options', in *any* situation.
+    // ### Typical situations include:
+    // ###  - Initial building of an object from a few user-specified options
+    // ###  - Building of a "reloaded" object: i.e. from the complete set of
+    // ###    all serialised options.
+    // ###  - Updating or "re-building" of an object after a few "tuning"
+    // ###    options have been modified.
+    // ### You should assume that the parent class' build_() has already been
+    // ### called.
+
+    MODULE_LOG << "build_() called" << endl;
+
+    if(train_set)
+    {
+        // Initialize some learnt variables
+        n_layers = layers.length();
+        
+        if( weightsize_ > 0 )
+            PLERROR("StackedAutoassociatorsNet::build_() - \n"
+                    "usage of weighted samples (weight size > 0) is not\n"
+                    "implemented yet.\n");
+
+        if( training_schedule.length() != n_layers-1 )        
+            PLERROR("StackedAutoassociatorsNet::build_() - \n"
+                    "training_schedule should have %d elements.\n",
+                    n_layers-1);
+        
+        if( partial_costs && partial_costs.length() != n_layers-1 )
+            PLERROR("StackedAutoassociatorsNet::build_() - \n"
+                    "partial_costs should have %d elements.\n",
+                    n_layers-1);
+
+        if( partial_costs && partial_costs_weights &&
+            partial_costs_weights.length() != n_layers-1 )
+            PLERROR("StackedAutoassociatorsNet::build_() - \n"
+                    "partial_costs_weights should have %d elements.\n",
+                    n_layers-1);
+
+        if(greedy_stages.length() == 0)
+        {
+            greedy_stages.resize(n_layers-1);
+            greedy_stages.clear();
+        }
+
+        if(stage > 0)
+            currently_trained_layer = n_layers;
+        else
+        {            
+            currently_trained_layer = n_layers-1;
+            while(currently_trained_layer>1
+                  && greedy_stages[currently_trained_layer-1] <= 0)
+                currently_trained_layer--;
+        }
+
+        build_layers_and_connections();
+        build_costs();
+    }
+}
+
+void StackedAutoassociatorsNet::build_layers_and_connections()
+{
+    MODULE_LOG << "build_layers_and_connections() called" << endl;
+
+    if( connections.length() != n_layers-1 )
+        PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() - \n"
+                "there should be %d connections.\n",
+                n_layers-1);
+
+    if(layers[0]->size != inputsize_)
+        PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() - \n"
+                "layers[0] should have a size of %d.\n",
+                inputsize_);
+    
+    activations.resize( n_layers );
+    expectations.resize( n_layers );
+    activation_gradients.resize( n_layers );
+    expectation_gradients.resize( n_layers );
+
+    for( int i=0 ; i<n_layers-1 ; i++ )
+    {
+        if( layers[i]->size != connections[i]->down_size )
+            PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() "
+                    "- \n"
+                    "connections[%i] should have a down_size of %d.\n",
+                    i, layers[i]->size);
+
+        if( connections[i]->up_size != layers[i+1]->size )
+            PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() "
+                    "- \n"
+                    "connections[%i] should have a up_size of %d.\n",
+                    i, layers[i+1]->size);
+
+        if( layers[i+1]->size != reconstruction_connections[i]->down_size )
+            PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() "
+                    "- \n"
+                    "recontruction_connections[%i] should have a down_size of "
+                    "%d.\n",
+                    i, layers[i+1]->size);
+
+        if( reconstruction_connections[i]->up_size != layers[i]->size )
+            PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() "
+                    "- \n"
+                    "recontruction_connections[%i] should have a up_size of "
+                    "%d.\n",
+                    i, layers[i]->size);
+
+        layers[i]->random_gen = random_gen;
+        layers[i]->build();
+
+        connections[i]->random_gen = random_gen;
+        connections[i]->build();
+
+        reconstruction_connections[i]->random_gen = random_gen;
+        reconstruction_connections[i]->build();
+
+        activations[i].resize( layers[i]->size );
+        expectations[i].resize( layers[i]->size );
+        activation_gradients[i].resize( layers[i]->size );
+        expectation_gradients[i].resize( layers[i]->size );
+    }
+    layers[n_layers-1]->random_gen = random_gen;
+    activations[n_layers-1].resize( layers[n_layers-1]->size );
+    expectations[n_layers-1].resize( layers[n_layers-1]->size );
+    activation_gradients[n_layers-1].resize( layers[n_layers-1]->size );
+    expectation_gradients[n_layers-1].resize( layers[n_layers-1]->size );
+}
+
+void StackedAutoassociatorsNet::build_costs()
+{
+    MODULE_LOG << "build_final_cost() called" << endl;
+
+    final_cost_gradient.resize( final_cost->input_size );
+    final_cost->setLearningRate( fine_tuning_learning_rate );
+
+    if( !final_module )
+        PLERROR("StackedAutoassociatorsNet::build_costs() - \n"
+                "final_module should be provided.\n");
+    
+    if( layers[n_layers-1]->size != final_module->input_size )
+        PLERROR("StackedAutoassociatorsNet::build_costs() - \n"
+                "final_module should have an input_size of %d.\n", 
+                layers[n_layers-1]->size);
+    
+    if( final_module->output_size != final_cost->input_size )
+        PLERROR("StackedAutoassociatorsNet::build_costs() - \n"
+                "final_module should have an output_size of %d.\n", 
+                final_module->input_size);
+
+    final_module->setLearningRate( fine_tuning_learning_rate );
+
+    if(targetsize_ != 1)
+        PLERROR("StackedAutoassociatorsNet::build_costs() - \n"
+                "target size of %d is not supported.\n", targetsize_);
+    
+    if(partial_costs)
+    {
+        partial_costs_positions.resize(partial_costs.length());
+        partial_costs_positions.clear();
+        for(int i=0; i<partial_costs.length(); i++)
+        {
+            if(!partial_costs[i])
+                PLERROR("StackedAutoassociatorsNet::build_final_cost() - \n"
+                        "partial_costs[%i] should be provided.\n",i);
+            if( layers[i+1]->size != partial_costs[i]->input_size )
+                PLERROR("StackedAutoassociatorsNet::build_costs() - \n"
+                        "partial_costs[%i] should have an input_size of %d.\n", 
+                        i,layers[i+1]->size);
+            if(i==0)
+                partial_costs_positions[i] = n_layers-1;
+            else
+                partial_costs_positions[i] = partial_costs_positions[i-1]
+                    + partial_costs[i-1]->name().length();
+        }
+    }
+}
+
+// ### Nothing to add here, simply calls build_
+void StackedAutoassociatorsNet::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void StackedAutoassociatorsNet::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // deepCopyField(, copies);
+
+    deepCopyField(training_schedule, copies);
+    deepCopyField(layers, copies);
+    deepCopyField(connections, copies);
+    deepCopyField(reconstruction_connections, copies);
+    deepCopyField(final_module, copies);
+    deepCopyField(final_cost, copies);
+    deepCopyField(partial_costs, copies);
+    deepCopyField(partial_costs_weights, copies);
+    deepCopyField(activations, copies);
+    deepCopyField(expectations, copies);
+    deepCopyField(activation_gradients, copies);
+    deepCopyField(expectation_gradients, copies);
+    deepCopyField(reconstruction_activations, copies);
+    deepCopyField(reconstruction_expectations, copies);
+    deepCopyField(reconstruction_activation_gradients, copies);
+    deepCopyField(reconstruction_expectation_gradients, copies);
+    deepCopyField(partial_costs_positions, copies);
+    deepCopyField(partial_cost_value, copies);
+    deepCopyField(final_cost_input, copies);
+    deepCopyField(final_cost_value, copies);
+    deepCopyField(final_cost_gradient, copies);
+    deepCopyField(greedy_stages, copies);
+}
+
+
+int StackedAutoassociatorsNet::outputsize() const
+{
+    if(currently_trained_layer < n_layers)
+        return layers[currently_trained_layer]->size;
+    return final_module->output_size;
+}
+
+void StackedAutoassociatorsNet::forget()
+{
+    //! (Re-)initialize the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!)
+    /*!
+      A typical forget() method should do the following:
+      - call inherited::forget() to initialize its random number generator
+        with the 'seed' option
+      - initialize the learner's parameters, using this random generator
+      - stage = 0
+    */
+    inherited::forget();
+
+    for( int i=0 ; i<n_layers-1 ; i++ )
+    {
+        connections[i]->forget();
+        reconstruction_connections[i]->forget();
+    }
+    
+    final_module->forget();
+    final_cost->forget();
+
+    for( int i=0 ; i<partial_costs.length() ; i++ )
+        if( partial_costs[i] )
+            partial_costs[i]->forget();
+
+    stage = 0;
+    greedy_stages.clear();
+}
+
+void StackedAutoassociatorsNet::train()
+{
+    MODULE_LOG << "train() called " << endl;
+    MODULE_LOG << "  training_schedule = " << training_schedule << endl;
+
+    Vec input( inputsize() );
+    Vec target( targetsize() );
+    real weight; // unused
+
+    TVec<string> train_cost_names = getTrainCostNames() ;
+    Vec train_costs( train_cost_names.length() );
+    train_costs.fill(MISSING_VALUE) ;
+
+    int nsamples = train_set->length();
+    int sample;
+
+    PP<ProgressBar> pb;
+
+    // clear stats of previous epoch
+    train_stats->forget();
+
+    real lr = 0;
+    int init_stage;
+
+    /***** initial greedy training *****/
+    for( int i=0 ; i<n_layers-1 ; i++ )
+    {
+        MODULE_LOG << "Training connection weights between layers " << i
+            << " and " << i+1 << endl;
+
+        int end_stage = training_schedule[i];
+        int* this_stage = greedy_stages.subVec(i,1).data();
+        init_stage = *this_stage;
+
+        MODULE_LOG << "  stage = " << *this_stage << endl;
+        MODULE_LOG << "  end_stage = " << end_stage << endl;
+        MODULE_LOG << "  greedy_learning_rate = " << greedy_learning_rate << endl;
+
+        if( report_progress && *this_stage < end_stage )
+            pb = new ProgressBar( "Training layer "+tostring(i)
+                                  +" of "+classname(),
+                                  end_stage - init_stage );
+
+        train_costs.fill(MISSING_VALUE);
+        lr = greedy_learning_rate;
+        layers[i]->setLearningRate( lr );
+        connections[i]->setLearningRate( lr );
+        reconstruction_connections[i]->setLearningRate( lr );
+        layers[i+1]->setLearningRate( lr );
+
+        reconstruction_activations.resize(layers[i+1]->size);
+        reconstruction_expectations.resize(layers[i+1]->size);
+        reconstruction_activation_gradients.resize(layers[i+1]->size);
+        reconstruction_expectation_gradients.resize(layers[i+1]->size);
+
+        for( ; *this_stage<end_stage ; (*this_stage)++ )
+        {
+            if( !fast_exact_is_equal( greedy_decrease_ct , 0 ) )
+            {
+                lr = greedy_learning_rate/(1 + greedy_decrease_ct 
+                                           * (*this_stage)); 
+                layers[i]->setLearningRate( lr );
+                connections[i]->setLearningRate( lr );
+                reconstruction_connections[i]->setLearningRate( lr );
+                layers[i+1]->setLearningRate( lr );                
+            }
+
+            sample = *this_stage % nsamples;
+            train_set->getExample(sample, input, target, weight);
+            greedyStep( input, target, i, train_costs );
+            train_stats->update( train_costs );
+
+            if( pb )
+                pb->update( *this_stage - init_stage + 1 );
+        }
+    }
+
+    /***** fine-tuning by gradient descent *****/
+    if( stage < nstages )
+    {
+
+        MODULE_LOG << "Fine-tuning all parameters, by gradient descent" << endl;
+        MODULE_LOG << "  stage = " << stage << endl;
+        MODULE_LOG << "  nstages = " << nstages << endl;
+        MODULE_LOG << "  fine_tuning_learning_rate = " << 
+            fine_tuning_learning_rate << endl;
+
+        init_stage = stage;
+        if( report_progress && stage < nstages )
+            pb = new ProgressBar( "Fine-tuning parameters of all layers of "
+                                  + classname(),
+                                  nstages - init_stage );
+
+        setLearningRate( fine_tuning_learning_rate );
+        train_costs.fill(MISSING_VALUE);
+        for( ; stage<nstages ; stage++ )
+        {
+            sample = stage % nsamples;
+            if( !fast_exact_is_equal( fine_tuning_decrease_ct, 0. ) )
+                setLearningRate( fine_tuning_learning_rate
+                                 / (1. + fine_tuning_decrease_ct * stage ) );
+
+            train_set->getExample( sample, input, target, weight );
+            fineTuningStep( input, target, train_costs );
+            train_stats->update( train_costs );
+
+            if( pb )
+                pb->update( stage - init_stage + 1 );
+        }
+    }
+    
+    train_stats->finalize();
+        
+    // Update currently_trained_layer
+    if(stage > 0)
+        currently_trained_layer = n_layers;
+    else
+    {            
+        currently_trained_layer = n_layers-1;
+        while(currently_trained_layer>1 
+              && greedy_stages[currently_trained_layer-1] <= 0)
+            currently_trained_layer--;
+    }
+}
+
+void StackedAutoassociatorsNet::greedyStep( const Vec& input, const Vec& target, int index, Vec train_costs )
+{
+    PLASSERT( index < n_layers );
+
+    expectations[0] << input;
+    for( int i=0 ; i<index + 1; i++ )
+    {
+        connections[i]->fprop( expectations[i], activations[i+1] );
+        layers[i+1]->fprop(activations[i+1],expectations[i+1]);
+    }
+
+    if( partial_costs && partial_costs[ index ] )
+    {
+        partial_costs[ index ]->fprop( expectations[ index + 1],
+                                       target, partial_cost_value );
+
+        // Update partial cost (might contain some weights for example)
+        partial_costs[ index ]->bpropUpdate( expectations[ index + 1 ],
+                                             target, partial_cost_value,
+                                             expectation_gradients[ index + 1 ]
+                                             );
+
+        train_costs.subVec(partial_costs_positions[index],
+                           partial_cost_value.length()) << partial_cost_value;
+
+        if( !fast_exact_is_equal( partial_costs_weights.length(), 0 ) )
+            expectation_gradients[ index + 1 ] *= partial_costs_weights[index];
+
+        // Update hidden layer bias and weights
+        layers[ index+1 ]->bpropUpdate( activations[ index + 1 ],
+                                        expectations[ index + 1 ],
+                                        activation_gradients[ index + 1 ],
+                                        expectation_gradients[ index + 1 ] );
+
+        connections[ index ]->bpropUpdate( expectations[ index ],
+                                           activations[ index + 1 ],
+                                           expectation_gradients[ index ],
+                                           activation_gradients[ index + 1 ] );
+    }
+
+    reconstruction_connections[ index ]->fprop( expectations[ index + 1],
+                                                reconstruction_activations);
+    layers[ index ]->fprop( reconstruction_activations,
+                            layers[ index ]->expectation);
+    
+    layers[ index ]->expectation_is_up_to_date = true;
+    train_costs[index] = layers[ index ]->fpropNLL(expectations[index]);
+
+    layers[ index ]->bpropNLL(expectations[index], train_costs[index],
+                                  reconstruction_activation_gradients);
+
+    layers[ index ]->update(reconstruction_activation_gradients);
+
+    // // This is a bad update! Propagates gradient through sigmoid again!
+    // layers[ index ]->bpropUpdate( reconstruction_activations, 
+    //                                   layers[ index ]->expectation,
+    //                                   reconstruction_activation_gradients,
+    //                                   reconstruction_expectation_gradients);
+
+    reconstruction_connections[ index ]->bpropUpdate( 
+        expectations[ index + 1], 
+        reconstruction_activations, 
+        reconstruction_expectation_gradients, //reused
+        reconstruction_activation_gradients);
+
+    if(!fast_exact_is_equal(l1_neuron_decay,0))
+    {
+        // Compute L1 penalty gradient on neurons
+        real* hid = expectations[ index + 1 ].data();
+        real* grad = reconstruction_expectation_gradients.data();
+        int len = expectations[ index + 1 ].length();
+        for(int i=0; i<len; i++)
+        {
+            if(*hid > 0)
+                *grad -= l1_neuron_decay;
+            else if(*hid < 0)
+                *grad += l1_neuron_decay;
+            hid++;
+            grad++;
+        }
+    }
+
+    // Update hidden layer bias and weights
+    layers[ index+1 ]->bpropUpdate( activations[ index + 1 ],
+                                    expectations[ index + 1 ],
+                                    reconstruction_activation_gradients, // reused
+                                    reconstruction_expectation_gradients);    
+
+    connections[ index ]->bpropUpdate( 
+        expectations[ index ],
+        activations[ index + 1 ],
+        reconstruction_expectation_gradients, //reused
+        reconstruction_activation_gradients);
+
+}
+
+void StackedAutoassociatorsNet::fineTuningStep( const Vec& input, const Vec& target,
+                                    Vec& train_costs )
+{
+    // fprop
+    expectations[0] << input;
+    for( int i=0 ; i<n_layers-1; i++ )
+    {
+        connections[i]->fprop( expectations[i], activations[i+1] );
+        layers[i+1]->fprop(activations[i+1],expectations[i+1]);
+    }
+
+    final_module->fprop( expectations[ n_layers-1 ],
+                         final_cost_input );
+    final_cost->fprop( final_cost_input, target, final_cost_value );
+
+    train_costs.subVec(train_costs.length()-final_cost_value.length(),
+                       final_cost_value.length()) <<
+        final_cost_value;
+
+    final_cost->bpropUpdate( final_cost_input, target,
+                             final_cost_value[0],
+                             final_cost_gradient );
+    final_module->bpropUpdate( expectations[ n_layers-1 ],
+                               final_cost_input,
+                               expectation_gradients[ n_layers-1 ],
+                               final_cost_gradient );
+
+    for( int i=n_layers-1 ; i>0 ; i-- )
+    {
+        layers[i]->bpropUpdate( activations[i],
+                                expectations[i],
+                                activation_gradients[i],
+                                expectation_gradients[i] );
+
+        connections[i-1]->bpropUpdate( expectations[i-1],
+                                       activations[i],
+                                       expectation_gradients[i-1],
+                                       activation_gradients[i] );
+    }
+}
+
+void StackedAutoassociatorsNet::computeOutput(const Vec& input, Vec& output) const
+{
+    // fprop
+
+    expectations[0] << input;
+
+    for(int i=0 ; i<currently_trained_layer-1 ; i++ )
+    {
+        connections[i]->fprop( expectations[i], activations[i+1] );
+        layers[i+1]->fprop(activations[i+1],expectations[i+1]);
+    }
+
+    if( currently_trained_layer<n_layers )
+    {
+        connections[currently_trained_layer-1]->fprop( 
+            expectations[currently_trained_layer-1], 
+            activations[currently_trained_layer] );
+        layers[currently_trained_layer]->fprop(
+            activations[currently_trained_layer],
+            output);
+    }
+    else        
+        final_module->fprop( expectations[ currently_trained_layer - 1],
+                             output );
+}
+
+void StackedAutoassociatorsNet::computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                           const Vec& target, Vec& costs) const
+{
+    //Assumes that computeOutput has been called
+
+    costs.resize( getTestCostNames().length() );
+    costs.fill( MISSING_VALUE );
+
+    if(compute_all_test_costs)
+    {
+        for(int i=0; i<currently_trained_layer-1; i++)
+        {
+            reconstruction_connections[ i ]->fprop( expectations[ i+1 ],
+                                                    reconstruction_activations);
+            layers[ i ]->fprop( reconstruction_activations,
+                                    layers[ i ]->expectation);
+            
+            layers[ i ]->expectation_is_up_to_date = true;
+            costs[i] = layers[ i ]->fpropNLL(expectations[ i ]);
+            
+            if( partial_costs && partial_costs[i])
+            {
+                partial_costs[ i ]->fprop( expectations[ i + 1],
+                                           target, partial_cost_value );
+                costs.subVec(partial_costs_positions[i],
+                             partial_cost_value.length()) << 
+                    partial_cost_value;
+            }
+        }
+    }
+
+    if( currently_trained_layer<n_layers )
+    {
+        reconstruction_connections[ currently_trained_layer-1 ]->fprop( 
+            output,
+            reconstruction_activations);
+        layers[ currently_trained_layer-1 ]->fprop( 
+            reconstruction_activations,
+            layers[ currently_trained_layer-1 ]->expectation);
+        
+        layers[ currently_trained_layer-1 ]->expectation_is_up_to_date = true;
+        costs[ currently_trained_layer-1 ] = 
+            layers[ currently_trained_layer-1 ]->fpropNLL(
+                expectations[ currently_trained_layer-1 ]);
+
+        if( partial_costs && partial_costs[ currently_trained_layer-1 ] )
+        {
+            partial_costs[ currently_trained_layer-1 ]->fprop( 
+                output,
+                target, partial_cost_value );
+            costs.subVec(partial_costs_positions[currently_trained_layer-1],
+                         partial_cost_value.length()) << partial_cost_value;
+        }
+    }
+    else
+    {
+        final_cost->fprop( output, target, final_cost_value );        
+        costs.subVec(costs.length()-final_cost_value.length(),
+                     final_cost_value.length()) <<
+            final_cost_value;
+    }
+}
+
+TVec<string> StackedAutoassociatorsNet::getTestCostNames() const
+{
+    // Return the names of the costs computed by computeCostsFromOutputs
+    // (these may or may not be exactly the same as what's returned by
+    // getTrainCostNames).
+
+    TVec<string> cost_names(0);
+
+    for( int i=0; i<layers.size()-1; i++)
+        cost_names.push_back("reconstruction_error_" + tostring(i+1));
+    
+    for( int i=0 ; i<partial_costs.size() ; i++ )
+    {
+        TVec<string> cost_names = partial_costs[i]->name();
+        for(int j=0; j<cost_names.length(); j++)
+            cost_names.push_back("partial_cost_" + tostring(i+1) + "_" + 
+                cost_names[j]);
+    }
+
+    cost_names.append( final_cost->name() );
+
+    return cost_names;
+}
+
+TVec<string> StackedAutoassociatorsNet::getTrainCostNames() const
+{
+    return getTestCostNames() ;    
+}
+
+
+//#####  Helper functions  ##################################################
+
+void StackedAutoassociatorsNet::setLearningRate( real the_learning_rate )
+{
+    for( int i=0 ; i<n_layers-1 ; i++ )
+    {
+        layers[i]->setLearningRate( the_learning_rate );
+        connections[i]->setLearningRate( the_learning_rate );
+    }
+    layers[n_layers-1]->setLearningRate( the_learning_rate );
+
+    final_cost->setLearningRate( fine_tuning_learning_rate );
+    final_module->setLearningRate( fine_tuning_learning_rate );
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/StackedAutoassociatorsNet.h
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2007-04-27 21:02:20 UTC (rev 6948)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2007-04-27 21:09:52 UTC (rev 6949)
@@ -0,0 +1,299 @@
+// -*- C++ -*-
+
+// StackedAutoassociatorsNet.h
+//
+// Copyright (C) 2007 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file StackedAutoassociatorsNet.h */
+
+
+#ifndef StackedAutoassociatorsNet_INC
+#define StackedAutoassociatorsNet_INC
+
+#include <plearn_learners/generic/PLearner.h>
+#include <plearn_learners/online/OnlineLearningModule.h>
+#include <plearn_learners/online/CostModule.h>
+#include <plearn_learners/online/NLLCostModule.h>
+#include <plearn_learners/online/RBMClassificationModule.h>
+#include <plearn_learners/online/RBMLayer.h>
+#include <plearn_learners/online/RBMMixedLayer.h>
+#include <plearn_learners/online/RBMConnection.h>
+#include <plearn/misc/PTimer.h>
+
+namespace PLearn {
+
+/**
+ * Neural net, trained layer-wise in a greedy fashion using autoassociators.
+ * It is highly inspired by the DeepBeliefNet class, and can use use the
+ * same RBMLayer and RBMConnection components.
+ *
+ * TODO: - code globally online version (can't use hyperlearner, 
+ *         because of copies in earlystopping oracle and testing after change...)
+ *       - make sure fpropNLL only uses the expectation field in RBMLayer objects
+ */
+class StackedAutoassociatorsNet : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! The learning rate used during the autoassociator gradient descent training
+    real greedy_learning_rate;
+
+    //! The decrease constant of the learning rate used during the autoassociator
+    //! gradient descent training. When a hidden layer has finished its training,
+    //! the learning rate is reset to it's initial value.
+    real greedy_decrease_ct;
+
+    //! The learning rate used during the fine tuning gradient descent
+    real fine_tuning_learning_rate;
+
+    //! The decrease constant of the learning rate used during fine tuning
+    //! gradient descent
+    real fine_tuning_decrease_ct;
+
+    //! L1 penalty weight on the hidden layers, to encourage sparsity during
+    //! the greedy unsupervised phases
+    real l1_neuron_decay;
+
+    //! Number of examples to use during each phase of learning:
+    //! first the greedy phases, and then the gradient descent.
+    //! Unlike for DeepBeliefNet, these numbers should not be
+    //! cumulative. They correspond to the number of seen training
+    //! examples for each phase.
+    TVec<int> training_schedule;
+
+    //! The layers of units in the network
+    TVec< PP<RBMLayer> > layers;
+
+    //! The weights of the connections between the layers
+    TVec< PP<RBMConnection> > connections;
+
+    //! The weights of the reconstruction connections between the layers
+    TVec< PP<RBMConnection> > reconstruction_connections;
+
+    //! Module that takes as input the output of the last layer
+    //! (layers[n_layers-1), and feeds its output to final_cost
+    //! which defines the fine-tuning criteria.
+    PP<OnlineLearningModule> final_module;
+
+    //! The cost function to be applied on top of the neural network
+    //! (i.e. at the output of final_module). Its gradients will be 
+    //! backpropagated to final_module and then backpropagated to
+    //! the layers.
+    PP<CostModule> final_cost;
+
+    //! Corresponding additional supervised cost function to be applied on 
+    //! top of each hidden layer during the autoassociator training stages. 
+    //! The gradient for these costs are not backpropagated to previous layers.
+    TVec< PP<CostModule> > partial_costs;
+
+    //! Relative weights of the partial costs. If not defined,
+    //! weights of 1 will be assumed for all partial costs.
+    Vec partial_costs_weights;
+
+    //! Indication that, at test time, all costs for all
+    //! layers (up to the currently trained layer) should be computed.
+    bool compute_all_test_costs;
+
+    //#####  Public Learnt Options  ###########################################
+
+    //! Number of layers
+    int n_layers;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    StackedAutoassociatorsNet();
+
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Returns the size of this learner's output, (which typically
+    //! may depend on its inputsize(), targetsize() and set options).
+    virtual int outputsize() const;
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training costs
+    //! measured on-line in the process.
+    virtual void train();
+
+    //! Computes the output from the input.
+    virtual void computeOutput(const Vec& input, Vec& output) const;
+
+    //! Computes the costs from already computed output.
+    virtual void computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                         const Vec& target, Vec& costs) const;
+
+    //! Returns the names of the costs computed by computeCostsFromOutpus (and
+    //! thus the test method).
+    virtual TVec<std::string> getTestCostNames() const;
+
+    //! Returns the names of the objective costs that the train method computes
+    //! and  for which it updates the VecStatsCollector train_stats.
+    virtual TVec<std::string> getTrainCostNames() const;
+
+
+    void greedyStep( const Vec& input, const Vec& target, int index, 
+                     Vec train_costs );
+
+    void fineTuningStep( const Vec& input, const Vec& target,
+                         Vec& train_costs );
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(StackedAutoassociatorsNet);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Not Options  #####################################################
+
+    //! Stores the activations of the input and hidden layers
+    //! (at the input of the layers)
+    mutable TVec<Vec> activations;
+
+    //! Stores the expectations of the input and hidden layers
+    //! (at the output of the layers)
+    mutable TVec<Vec> expectations;
+
+    //! Stores the gradient of the cost wrt the activations of 
+    //! the input and hidden layers
+    //! (at the input of the layers)
+    mutable TVec<Vec> activation_gradients;
+
+    //! Stores the gradient of the cost wrt the expectations of 
+    //! the input and hidden layers
+    //! (at the output of the layers)
+    mutable TVec<Vec> expectation_gradients;
+
+    //! Reconstruction activations
+    mutable Vec reconstruction_activations;
+    
+    //! Reconstruction expectations
+    mutable Vec reconstruction_expectations;
+    
+    //! Reconstruction activations
+    mutable Vec reconstruction_activation_gradients;
+    
+    //! Reconstruction expectations
+    mutable Vec reconstruction_expectation_gradients;
+
+    //! Position in the total cost vector of the different partial costs
+    mutable TVec<int> partial_costs_positions;
+    
+    //! Cost value of partial_costs
+    mutable Vec partial_cost_value;
+
+    //! Input of the final_cost
+    mutable Vec final_cost_input;
+
+    //! Cost value of final_cost
+    mutable Vec final_cost_value;
+
+    //! Stores the gradient of the cost at the input of final_cost
+    mutable Vec final_cost_gradient;
+
+    //! Stages of the different greedy phases
+    TVec<int> greedy_stages;
+
+    //! Currently trained layer (1 means the first hidden layer,
+    //! n_layers means the output layer)
+    int currently_trained_layer;
+
+    //! Indication whether final_module has learning rate
+    bool final_module_has_learning_rate;
+    
+    //! Indication whether final_cost has learning rate
+    bool final_cost_has_learning_rate;
+    
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+    void build_layers_and_connections();
+
+    void build_classification_cost();
+
+    void build_costs();
+
+    void setLearningRate( real the_learning_rate );
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here    
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(StackedAutoassociatorsNet);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From larocheh at mail.berlios.de  Fri Apr 27 23:19:13 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 27 Apr 2007 23:19:13 +0200
Subject: [Plearn-commits] r6950 - trunk/commands
Message-ID: <200704272119.l3RLJDRk007510@sheep.berlios.de>

Author: larocheh
Date: 2007-04-27 23:19:13 +0200 (Fri, 27 Apr 2007)
New Revision: 6950

Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
Added the StackedAutoassociatorsNet and RBMMatrixTransposeConnection classes...




Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2007-04-27 21:09:52 UTC (rev 6949)
+++ trunk/commands/plearn_noblas_inc.h	2007-04-27 21:19:13 UTC (rev 6950)
@@ -209,12 +209,14 @@
 #include <plearn_learners/online/RBMGaussianLayer.h>
 #include <plearn_learners/online/RBMLayer.h>
 #include <plearn_learners/online/RBMMatrixConnection.h>
+#include <plearn_learners/online/RBMMatrixTransposeConnection.h>
 #include <plearn_learners/online/RBMMixedConnection.h>
 #include <plearn_learners/online/RBMMixedLayer.h>
 #include <plearn_learners/online/RBMMultinomialLayer.h>
 #include <plearn_learners/online/RBMTruncExpLayer.h>
 #include <plearn_learners/online/SoftmaxModule.h>
 #include <plearn_learners/online/SquaredErrorCostModule.h>
+#include <plearn_learners/online/StackedAutoassociatorsNet.h>
 #include <plearn_learners/online/Subsampling2DModule.h>
 #include <plearn_learners/online/Supersampling2DModule.h>
 #include <plearn_learners/online/TanhModule.h>



From yoshua at mail.berlios.de  Sat Apr 28 17:14:29 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Sat, 28 Apr 2007 17:14:29 +0200
Subject: [Plearn-commits] r6951 - in trunk:
	plearn_learners/generic/EXPERIMENTAL plearn_learners/online scripts
Message-ID: <200704281514.l3SFET9n019593@sheep.berlios.de>

Author: yoshua
Date: 2007-04-28 17:14:28 +0200 (Sat, 28 Apr 2007)
New Revision: 6951

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/RBMMatrixConnection.cc
   trunk/scripts/collectres
Log:


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-04-27 21:19:13 UTC (rev 6950)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-04-28 15:14:28 UTC (rev 6951)
@@ -649,10 +649,18 @@
                             *m = (1-activation_statistics_moving_average_coefficient) * *m
                                 + activation_statistics_moving_average_coefficient * *a;
                             *b = target_mean_activation - *m;
-                            if (*v<1e6)
+                            if (*v<100*target_stdev_activation*target_stdev_activation)
                                 *s = target_stdev_activation/sqrt(*v);
-                            else
-                                PLWARNING("NatGradNNet::fpropNet: activation variance >= 1e6!\n");
+                            else // rescale the weights and the statistics for that neuron
+                            {
+                                real rescale_factor = target_stdev_activation/sqrt(*v);
+                                Vec w = weights[i](j);
+                                w *= rescale_factor;
+                                *b *= rescale_factor;
+                                *s = 1;
+                                *m *= rescale_factor;
+                                *v *= rescale_factor*rescale_factor;
+                            }
                         }
                         *a = tanh((*a + *b) * *s);
                     }

Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-04-27 21:19:13 UTC (rev 6950)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-04-28 15:14:28 UTC (rev 6951)
@@ -1526,7 +1526,7 @@
         // update
         down_layer->update( pos_down_vals, neg_down_vals );
         connection->update( pos_down_vals, pos_up_vals,
-                neg_down_vals, neg_up_vals );
+                            neg_down_vals, neg_up_vals );
         up_layer->update( pos_up_vals, neg_up_vals );
     } else {
         // accumulate negative stats

Modified: trunk/plearn_learners/online/RBMMatrixConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.cc	2007-04-27 21:19:13 UTC (rev 6950)
+++ trunk/plearn_learners/online/RBMMatrixConnection.cc	2007-04-28 15:14:28 UTC (rev 6951)
@@ -260,7 +260,7 @@
     }
     else
     {
-        PLERROR("RBMMatrixConnection::update - Not implemented");
+        PLERROR("RBMMatrixConnection::update minibatch with momentum - Not implemented");
         /*
         // ensure that weights_inc has the right size
         weights_inc.resize( l, w );

Modified: trunk/scripts/collectres
===================================================================
--- trunk/scripts/collectres	2007-04-27 21:19:13 UTC (rev 6950)
+++ trunk/scripts/collectres	2007-04-28 15:14:28 UTC (rev 6951)
@@ -56,6 +56,9 @@
 from plearn.vmat.PMat import *
 from numarray import *
 
+# should probably be an option
+separator = "_"
+
 def selectres(loc_specs,a):
   res = []
   loc_mode = loc_specs[0]
@@ -124,7 +127,7 @@
     f.write("\n")
 
 def distinct_experiment_names(filenames):
-  splitted_filenames = map(lambda fname: string.split(fname.replace("/","-"),"-"), filenames)
+  splitted_filenames = map(lambda fname: string.split(fname.replace("/",separator),separator), filenames)
   distinct_names = []
   for splitted_filename in splitted_filenames:
     different_parts = []
@@ -136,7 +139,7 @@
           break
       if not everywhere:
         different_parts.append(part)
-    distinct_names.append(string.join(different_parts,"-"))
+    distinct_names.append(string.join(different_parts,separator))
   return distinct_names
 
 def outputres(f,mode,speclist,results):



From yoshua at mail.berlios.de  Mon Apr 30 05:30:04 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Mon, 30 Apr 2007 05:30:04 +0200
Subject: [Plearn-commits] r6952 - trunk/plearn_learners/online
Message-ID: <200704300330.l3U3U4gp012557@sheep.berlios.de>

Author: yoshua
Date: 2007-04-30 05:30:03 +0200 (Mon, 30 Apr 2007)
New Revision: 6952

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
Log:
Beginning work to add side Gibbs chain for negative phase.


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-04-28 15:14:28 UTC (rev 6951)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-04-30 03:30:03 UTC (rev 6952)
@@ -67,6 +67,8 @@
     reconstruct_layerwise( false ),
     n_layers( 0 ),
     online ( false ),
+    background_gibbs_update_ratio(0),
+    gibbs_chain_statistics_forgetting_factor(0.999),
     minibatch_size(0),
     final_module_has_learning_rate( false ),
     final_cost_has_learning_rate( false ),
@@ -208,6 +210,21 @@
                   "If true then all unsupervised training stages (as well as\n"
                   "the fine-tuning stage) are done simultaneously.\n");
 
+    declareOption(ol, "background_gibbs_update_ratio", &DeepBeliefNet::background_gibbs_update_ratio,
+                  OptionBase::buildoption,
+                  "Coefficient between 0 and 1. If non-zero, run a background Gibbs chain and use\n"
+                  "the visible-hidden statistics to contribute in the negative phase update\n"
+                  "(in proportion background_gibbs_update_ratio wrt the contrastive divergence\n"
+                  "negative phase statistics). If = 1, then do not perform any contrastive\n"
+                  "divergence negative phase (use only the Gibbs chain statistics).\n");
+
+    declareOption(ol, "gibbs_chain_statistics_forgetting_factor", 
+                  &DeepBeliefNet::gibbs_chain_statistics_forgetting_factor,
+                  OptionBase::buildoption,
+                  "Negative chain statistics are forgotten at this rate (a value of 0\n"
+                  "would only use the current sample, a value of .99 would use 1% of\n"
+                  "the current sample and 99% of the old statistics).\n");
+
     declareOption(ol, "top_layer_joint_cd", &DeepBeliefNet::top_layer_joint_cd,
                   OptionBase::buildoption,
                   "Wether we do a step of joint contrastive divergence on"
@@ -296,6 +313,8 @@
     activations_gradients.resize( n_layers );
     expectation_gradients.resize( n_layers );
     expectations_gradients.resize( n_layers );
+    gibbs_up_state.resize( n_layers-1 );
+    gibbs_down_state.resize( n_layers-1 );
 
     for( int i=0 ; i<n_layers-1 ; i++ )
     {
@@ -483,6 +502,8 @@
     deepCopyField(pos_up_val,               copies);
     deepCopyField(pos_down_vals,            copies);
     deepCopyField(pos_up_vals,              copies);
+    deepCopyField(gibbs_up_state,           copies);
+    deepCopyField(gibbs_down_state,           copies);
     deepCopyField(optimized_costs,          copies);
     deepCopyField(final_cost_indices,       copies);
     deepCopyField(partial_cost_indices,     copies);
@@ -550,6 +571,12 @@
         for (int i = 0 ; i < n_layers; i++) {
             activations_gradients[i].resize(minibatch_size, layers[i]->size);
             expectation_gradients[i].resize(minibatch_size, layers[i]->size);
+
+            if (background_gibbs_update_ratio>0)
+            {
+                gibbs_up_state[i].resize(minibatch_size, layers[i]->size);
+                gibbs_down_state[i].resize(minibatch_size, layers[i]->size);
+            }
         }
         if (final_cost)
             final_cost_gradients.resize(minibatch_size, final_cost->input_size);
@@ -1472,8 +1499,6 @@
     }
 
     if (mbatch) {
-        up_layer->generateSamples();
-
         // accumulate positive stats using the expectation
         // we deep-copy because the value will change during negative phase
         pos_down_vals.resize(minibatch_size, down_layer->size);
@@ -1483,13 +1508,36 @@
         pos_up_vals << up_layer->getExpectations();
 
         // down propagation, starting from a sample of up_layer
-        connection->setAsUpInputs( up_layer->samples );
+        if (background_gibbs_update_ratio<1) 
+        {
+            // then do some contrastive divergence, o/w only background Gibbs
+            up_layer->generateSamples();
+            connection->setAsUpInputs( up_layer->samples );
+            down_layer->getAllActivations( connection, 0, true );
+            down_layer->generateSamples();
+            // negative phase
+            connection->setAsDownInputs( down_layer->samples );
+            up_layer->getAllActivations( connection, 0, mbatch );
+            up_layer->computeExpectations();
 
-        down_layer->getAllActivations( connection, 0, true );
+            // accumulate negative stats
+            // no need to deep-copy because the values won't change before update
+            Mat neg_down_vals = down_layer->samples;
+            Mat neg_up_vals = up_layer->getExpectations();
 
-        down_layer->generateSamples();
-        // negative phase
-        connection->setAsDownInputs( down_layer->samples );
+            // update
+            if (background_gibbs_update_ratio==0)
+            {
+                down_layer->update( pos_down_vals, neg_down_vals );
+                connection->update( pos_down_vals, pos_up_vals,
+                                    neg_down_vals, neg_up_vals );
+                up_layer->update( pos_up_vals, neg_up_vals );
+            }
+        }
+        // 
+        if (background_gibbs_update_ratio>0) 
+        {
+        }
     } else {
         up_layer->generateSample();
 
@@ -1509,28 +1557,10 @@
         down_layer->generateSample();
         // negative phase
         connection->setAsDownInput( down_layer->sample );
-    }
-
-    up_layer->getAllActivations( connection, 0, mbatch );
-    if (mbatch)
-        up_layer->computeExpectations();
-    else
+        up_layer->getAllActivations( connection, 0, mbatch );
         up_layer->computeExpectation();
-
-    if (mbatch) {
         // accumulate negative stats
         // no need to deep-copy because the values won't change before update
-        Mat neg_down_vals = down_layer->samples;
-        Mat neg_up_vals = up_layer->getExpectations();
-
-        // update
-        down_layer->update( pos_down_vals, neg_down_vals );
-        connection->update( pos_down_vals, pos_up_vals,
-                            neg_down_vals, neg_up_vals );
-        up_layer->update( pos_up_vals, neg_up_vals );
-    } else {
-        // accumulate negative stats
-        // no need to deep-copy because the values won't change before update
         Vec neg_down_val = down_layer->sample;
         Vec neg_up_val = up_layer->expectation;
 

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2007-04-28 15:14:28 UTC (rev 6951)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2007-04-30 03:30:03 UTC (rev 6952)
@@ -140,6 +140,17 @@
     //! whether to do things by stages, including fine-tuning, or on-line
     bool online;
 
+    // Coefficient between 0 and 1. If non-zero, run a background Gibbs chain and use 
+    // the visible-hidden statistics to contribute in the negative phase update
+    // (in proportion background_gibbs_update_ratio wrt the contrastive divergence
+    // negative phase statistics). If = 1, then do not perform any contrastive
+    // divergence negative phase (use only the Gibbs chain statistics).
+    real background_gibbs_update_ratio;
+    // negative chain statistics are forgotten at this rate (a value of 0
+    // would only use the current sample, a value of .99 would use 1% of
+    // the new sample and 99% of the old statistics).
+    real gibbs_chain_statistics_forgetting_factor;
+
     //! Wether we do a step of joint contrastive divergence on top-layer
     //! Only used if online for the moment
     bool top_layer_joint_cd;
@@ -308,6 +319,10 @@
     mutable Vec pos_up_val;
     mutable Mat pos_down_vals;
     mutable Mat pos_up_vals;
+    
+    //! Store the state of the Gibbs chain for each RBM
+    mutable TVec<Mat> gibbs_up_state;
+    mutable TVec<Mat> gibbs_down_state;
 
     //! Used to store the costs optimized by the final cost module.
     Vec optimized_costs;



From nouiz at mail.berlios.de  Mon Apr 30 21:05:41 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 30 Apr 2007 21:05:41 +0200
Subject: [Plearn-commits] r6953 - trunk/speedtest
Message-ID: <200704301905.l3UJ5f96010245@sheep.berlios.de>

Author: nouiz
Date: 2007-04-30 21:05:41 +0200 (Mon, 30 Apr 2007)
New Revision: 6953

Added:
   trunk/speedtest/xgemv.c
Log:
Added test programm to compare the execution speed of sgemv of blas library


Added: trunk/speedtest/xgemv.c
===================================================================
--- trunk/speedtest/xgemv.c	2007-04-30 03:30:03 UTC (rev 6952)
+++ trunk/speedtest/xgemv.c	2007-04-30 19:05:41 UTC (rev 6953)
@@ -0,0 +1,252 @@
+
+/* Includes, system */
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <math.h>
+
+#ifdef NVIDIA
+  /* Includes, cuda */
+  #include <cublas.h>
+#else
+  /* Includes, cblas */
+  #include <gsl/gsl_cblas.h>
+#endif
+#ifdef DOUBLE
+  typedef double real;
+  #define cblas_xgemv cblas_dgemv
+#else
+  typedef float real;
+  #define cblas_xgemv cblas_sgemv
+#endif
+//#include <iostream>
+//using namespace std;
+
+#if defined(CXGEMV) || defined(COMPARE)
+/* Host implementation of a simple version of sgemm */
+static void c_xgemv(int M_, int N_, const real alpha, const real *A, 
+		  const real *X, const real beta, real *Y)
+{
+  int M=M_,N=1,K=N_;
+  int i;
+  int j;
+  int k;
+  for (i = 0; i < M; ++i) {
+    for (j = 0; j < N; ++j) {
+      float prod = 0;
+      for (k = 0; k < K; ++k) {
+	prod += A[i * K + k] * X[k * N + j];
+      }
+      Y[i * N + j] = alpha * prod + beta * Y[i * N + j];
+    }
+  }
+}
+#endif
+/* Main */
+int main(int argc, char** argv)
+{    
+  if (argc!=4){ 
+    fprintf (stderr, "Usage: %s <sizeM> <sizeN> <Nb iter>\n",argv[0]); 
+    exit(0); 
+  } 
+  const int M=strtol(argv[1],0,0);
+  const int N=strtol(argv[2],0,0);
+  const int NBITER=strtol(argv[3],0,0);
+  const int NA= M * N;
+  const int NX= N;
+  const int NY= N;
+  real* h_A;
+  real* h_X;
+  real* h_Y;
+  const real alpha = 1.0f;
+  const real beta = 0.0f;
+#ifdef NVIDIA
+  cublasStatus status;
+  real* d_A = 0;
+  real* d_X = 0;
+  real* d_Y = 0;
+#endif
+
+#ifdef COMPARE
+  real* h_Y_ref;
+  real error_norm;
+  real ref_norm;
+  real diff;
+#endif
+
+    /* Allocate host memory for the matrices */
+    h_A = (real*)malloc(NA * sizeof(h_A[0]));
+    if (h_A == 0) {
+        fprintf (stderr, "!!!! host memory allocation error (A)\n");
+        return EXIT_FAILURE;
+    }
+    h_X = (real*)malloc(NX * sizeof(h_X[0]));
+    if (h_X == 0) {
+        fprintf (stderr, "!!!! host memory allocation error (X)\n");
+        return EXIT_FAILURE;
+    }
+    h_Y = (real*)malloc(NY * sizeof(h_Y[0]));
+    if (h_Y == 0) {
+        fprintf (stderr, "!!!! host memory allocation error (Y)\n");
+        return EXIT_FAILURE;
+    }
+
+    for (int i = 0; i < NA; ++i) h_A[i] = M_PI+(real)i;
+    for (int i = 0; i < NX; ++i) h_X[i] = M_PI+(real)i;
+
+#ifdef NVIDIA
+    /* Initialize CUBLAS */
+    status = cublasInit();
+    if (status != CUBLAS_STATUS_SUCCESS) {
+        fprintf (stderr, "!!!! CUBLAS initialization error\n");
+        return EXIT_FAILURE;
+    }
+    /* Allocate device memory for the matrices */
+    status = cublasAlloc(NA, sizeof(d_A[0]), (void**)&d_A);
+    if (status != CUBLAS_STATUS_SUCCESS) {
+        fprintf (stderr, "!!!! device memory allocation error (A)\n");
+        return EXIT_FAILURE;
+    }
+    status = cublasAlloc(NX, sizeof(d_X[0]), (void**)&d_X);
+    if (status != CUBLAS_STATUS_SUCCESS) {
+        fprintf (stderr, "!!!! device memory allocation error (X)\n");
+        return EXIT_FAILURE;
+    }
+    status = cublasAlloc(NY, sizeof(d_Y[0]), (void**)&d_Y);
+    if (status != CUBLAS_STATUS_SUCCESS) {
+        fprintf (stderr, "!!!! device memory allocation error (Y)\n");
+        return EXIT_FAILURE;
+    }
+
+    /* Initialize the device matrices with the host matrices */
+    status = cublasSetVector(NA, sizeof(h_A[0]), h_A, 1, d_A, 1);
+    if (status != CUBLAS_STATUS_SUCCESS) {
+        fprintf (stderr, "!!!! device access error (write A)\n");
+        return EXIT_FAILURE;
+    }
+    status = cublasSetVector(NX, sizeof(h_X[0]), h_X, 1, d_X, 1);
+    if (status != CUBLAS_STATUS_SUCCESS) {
+        fprintf (stderr, "!!!! device access error (write X)\n");
+        return EXIT_FAILURE;
+    }
+    status = cublasSetVector(NY, sizeof(h_Y[0]), h_Y, 1, d_Y, 1);
+    if (status != CUBLAS_STATUS_SUCCESS) {
+        fprintf (stderr, "!!!! device access error (write C)\n");
+        return EXIT_FAILURE;
+    }
+
+    /* Clear last error */
+    cublasGetError();
+#endif
+#ifdef COMPARE
+    /* Performs operation using plain C code */
+    for (int i=0;i<NBITER;i++)
+      c_xgemv(M,N, alpha, h_A, h_X, beta, h_Y);
+    h_Y_ref = h_Y;
+    /* Allocate host memory for reading back the result from device memory */
+    h_Y = (float*)malloc(NY * sizeof(h_Y[0]));
+    if (h_Y == 0) {
+        fprintf (stderr, "!!!! host memory allocation error (C)\n");
+        return EXIT_FAILURE;
+    }
+#endif
+#ifdef NVIDIA
+    /* Performs operation using cublas */
+    for (int i=0;i<NBITER;i++)
+      //We must Change the order of the parameter as cublas take
+      //matrix as colomn major and C matrix is row major?????
+      cublasSgemv('n', 'n', N, M, alpha, d_X, N, d_A, K, beta, d_Y, N);
+
+    status = cublasGetError();
+    if (status != CUBLAS_STATUS_SUCCESS) {
+        fprintf (stderr, "!!!! kernel execution error.\n");
+        return EXIT_FAILURE;
+    }
+    /* Read the result back */
+    status = cublasGetVector(NY, sizeof(h_Y[0]), d_Y, 1, h_Y, 1);
+    if (status != CUBLAS_STATUS_SUCCESS) {
+        fprintf (stderr, "!!!! device access error (read C)\n");
+        return EXIT_FAILURE;
+    }
+#elif defined( CXGEMV )
+    for (int i=0;i<NBITER;i++)
+      c_xgemv(M,N, alpha, h_A, h_X, beta, h_Y);
+#else
+    for (int i=0;i<NBITER;i++)
+      cblas_xgemv(CblasRowMajor, CblasNoTrans, M,N, alpha, h_A, N, h_X, 1, beta, h_Y, 1);
+#endif
+    /*    for (int i = 0; i < NA; ++i) printf("%f,",h_A[i]);
+    printf(")\n");
+    for (int i = 0; i < NX; ++i) printf("%f,",h_X[i]);
+    printf(")\n");
+    for (int i = 0; i < NY; ++i) printf("%f,",h_Y[i]);
+    printf(")\n");*/
+#ifdef COMPARE
+    /*    for (int i = 0; i < NY; ++i) printf("%f,",h_Y_ref[i]);
+	  printf(")\n");*/
+    /* Check result against reference */
+    error_norm = 0;
+    ref_norm = 0;
+    for (int i = 0; i < NY; ++i) {
+        diff = h_Y_ref[i] - h_Y[i];
+        error_norm += diff * diff;
+        ref_norm += h_Y_ref[i] * h_Y_ref[i];
+    }
+    error_norm = (float)sqrt((double)error_norm);
+    ref_norm = (float)sqrt((double)ref_norm);
+    if (fabs(ref_norm) < 1e-7) {
+        fprintf (stderr, "!!!! reference norm is 0\n");
+        return EXIT_FAILURE;
+    }
+    printf( "Test %s\n", (error_norm / ref_norm < 1e-6f) ? "PASSED" : "FAILED");
+#endif
+
+    /*    printf("Matrix A:\n");
+    for(int i=0;i<NA;i++)
+      printf("%f,",h_A[i]);
+    printf("\nArray X:\n");
+    for(int i=0;i<NX;i++)
+      printf("%f,",h_X[i]);
+    printf("\nArray Y:\n");
+    for(int i=0;i<NY;i++)
+      printf("%f,",h_Y[i]);
+    printf("\n");
+    */
+    /* Memory clean up */
+    free(h_A);
+    free(h_X);
+    free(h_Y);
+#ifdef COMPARE
+    free(h_Y_ref);
+#endif
+#ifdef NVIDIA
+    status = cublasFree(d_A);
+    if (status != CUBLAS_STATUS_SUCCESS) {
+        fprintf (stderr, "!!!! memory free error (A)\n");
+        return EXIT_FAILURE;
+    }
+    status = cublasFree(d_B);
+    if (status != CUBLAS_STATUS_SUCCESS) {
+        fprintf (stderr, "!!!! memory free error (B)\n");
+        return EXIT_FAILURE;
+    }
+    status = cublasFree(d_C);
+    if (status != CUBLAS_STATUS_SUCCESS) {
+        fprintf (stderr, "!!!! memory free error (C)\n");
+        return EXIT_FAILURE;
+    }
+
+    /* Shutdown */
+    status = cublasShutdown();
+    if (status != CUBLAS_STATUS_SUCCESS) {
+        fprintf (stderr, "!!!! shutdown error (A)\n");
+        return EXIT_FAILURE;
+    }
+#endif
+    //    if (argc <= 1 || strcmp(argv[1], "-noprompt")) {
+    //        printf("\nPress ENTER to exit...\n");
+    //        getchar();
+    //    }
+
+    return EXIT_SUCCESS;
+}



From manzagop at mail.berlios.de  Mon Apr 30 23:07:13 2007
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Mon, 30 Apr 2007 23:07:13 +0200
Subject: [Plearn-commits] r6954 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200704302107.l3UL7DPA017638@sheep.berlios.de>

Author: manzagop
Date: 2007-04-30 23:07:12 +0200 (Mon, 30 Apr 2007)
New Revision: 6954

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
Log:
Modified the class so it allows for different natural gradient blocks on the first layer: parameters linked to the same input (instead of neuron) are grouped together. This was done by adding the params_natgrad_per_input_template option representing the NatGradEstimator to be applied to those groups. If present, it overrides params_natgrad_template for the first layer weights. 

Some changes to variable names were made:
* neuron_params... now called group_params, since we don't only have groups of parameters based on neurons, but also inputs.

In terms of implementation, if params_natgrad_per_input_template is present:
* biases, weights, mweights and layer_params (and _gradients and _delta) are held tranposed for the first layer.
* this affects the fprop and bprop ( in the onlineStep)

Also added commented out code for computing the correlation of the gradients.




Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-04-30 19:05:41 UTC (rev 6953)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-04-30 21:07:12 UTC (rev 6954)
@@ -71,6 +71,8 @@
       target_mean_activation(-4), // 
       target_stdev_activation(3), // 2.5% of the time we are above 1
       verbosity(0),
+      //corr_profiling_start(0), 
+      //corr_profiling_end(0),
       n_layers(-1),
       cumulative_training_time(0)
 {
@@ -172,12 +174,20 @@
                   "It is replicated in the params_natgrad vector, for each neuron\n"
                   "If not provided, then the neuron-specific natural gradient estimator is not used.\n");
 
-    declareOption(ol, "params_natgrad_per_neuron", 
-                  &NatGradNNet::params_natgrad_per_neuron,
-                  OptionBase::learntoption,
-                  "Vector of NatGradEstimator objects for the gradient of the parameters inside each neuron\n"
-                  "They are copies of the params_natgrad_template provided by the user\n");
+    declareOption(ol, "params_natgrad_per_input_template",
+                  &NatGradNNet::params_natgrad_per_input_template,
+                  OptionBase::buildoption,
+                  "Optional template NatGradEstimator object for the gradient of the parameters of the first layer\n"
+                  "grouped based upon their input. It is replicated in the params_natgrad_per_group vector, for each group.\n"
+                  "If provided, overides the params_natgrad_template for the parameters of the first layer.\n");
 
+    declareOption(ol, "params_natgrad_per_group", 
+                    &NatGradNNet::params_natgrad_per_group,
+                    OptionBase::learntoption,
+                    "Vector of NatGradEstimator objects for the gradient inside groups of parameters.\n"
+                    "They are copies of the params_natgrad_template and params_natgrad_per_input_template\n"
+                    "templates provided by the user.\n");
+
     declareOption(ol, "full_natgrad", &NatGradNNet::full_natgrad,
                   OptionBase::buildoption,
                   "NatGradEstimator for all the parameter gradients simultaneously.\n"
@@ -185,7 +195,6 @@
                   "is provided. If none of the NatGradEstimators is provided, then\n"
                   "regular stochastic gradient is performed.\n");
 
-
     declareOption(ol, "output_type", 
                   &NatGradNNet::output_type,
                   OptionBase::buildoption,
@@ -246,6 +255,18 @@
                   "   xbar <-- coefficient * xbar + (1-coefficient) x\n"
                   "where x could be the activation or its square\n");
 
+    //declareOption(ol, "corr_profiling_start",
+    //              &NatGradNNet::corr_profiling_start,
+    //              OptionBase::buildoption,
+    //              "Stage to start the profiling of the gradients' and the\n"
+    //              "natural gradients' correlation.\n");
+
+    //declareOption(ol, "corr_profiling_end",
+    //              &NatGradNNet::corr_profiling_end,
+    //              OptionBase::buildoption,
+    //              "Stage to end the profiling of the gradients' and the\n"
+    //              "natural gradients' correlations.\n");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -298,35 +319,70 @@
     all_mparams.resize(n_params);
     all_params_gradient.resize(n_params);
     all_params_delta.resize(n_params);
-    neuron_params.resize(n_neurons);
-    neuron_params_delta.resize(n_neurons);
-    neuron_params_gradient.resize(n_neurons);
+
+    // depending on how parameters are grouped on the first layer
+    int n_groups = params_natgrad_per_input_template ? (n_neurons-layer_sizes[1]+layer_sizes[0]+1) : n_neurons;
+    group_params.resize(n_groups);
+    group_params_delta.resize(n_groups);
+    group_params_gradient.resize(n_groups);
+
     for (int i=0,k=0,p=0;i<n_layers-1;i++)
     {
         int np=layer_sizes[i+1]*(1+layer_sizes[i]);
-        layer_params[i]=all_params.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
-        layer_mparams[i]=all_mparams.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
-        biases[i]=layer_params[i].subMatColumns(0,1);
-        weights[i]=layer_params[i].subMatColumns(1,layer_sizes[i]); // weights[0] from layer 0 to layer 1
-        mweights[i]=layer_mparams[i].subMatColumns(1,layer_sizes[i]); // weights[0] from layer 0 to layer 1
+        // First layer has natural gradient applied on groups of parameters
+        // linked to the same input -> parameters must be stored TRANSPOSED!
+        if( i==0 && params_natgrad_per_input_template ) {
+            layer_params[i]=all_params.subVec(p,np).toMat(layer_sizes[i]+1,layer_sizes[i+1]);
+            layer_mparams[i]=all_mparams.subVec(p,np).toMat(layer_sizes[i]+1,layer_sizes[i+1]);
+            biases[i]=layer_params[i].subMatRows(0,1);
+            weights[i]=layer_params[i].subMatRows(1,layer_sizes[i]); //weights[0] from layer 0 to layer 1
+            mweights[i]=layer_mparams[i].subMatRows(1,layer_sizes[i]); //weights[0] from layer 0 to layer 1
+            layer_params_gradient[i]=all_params_gradient.subVec(p,np).toMat(layer_sizes[i]+1,layer_sizes[i+1]);
+            layer_params_delta[i]=all_params_delta.subVec(p,np);
+            for (int j=0;j<layer_sizes[i]+1;j++,k++)   // include a bias input 
+            {
+                group_params[k]=all_params.subVec(p,layer_sizes[i+1]);
+                group_params_delta[k]=all_params_delta.subVec(p,layer_sizes[i+1]);
+                group_params_gradient[k]=all_params_gradient.subVec(p,layer_sizes[i+1]);
+                p+=layer_sizes[i+1];
+            }
+        // Usual parameter storage
+        }   else    {
+            layer_params[i]=all_params.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
+            layer_mparams[i]=all_mparams.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
+            biases[i]=layer_params[i].subMatColumns(0,1);
+            weights[i]=layer_params[i].subMatColumns(1,layer_sizes[i]); // weights[0] from layer 0 to layer 1
+            mweights[i]=layer_mparams[i].subMatColumns(1,layer_sizes[i]); // weights[0] from layer 0 to layer 1
+            layer_params_gradient[i]=all_params_gradient.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
+            layer_params_delta[i]=all_params_delta.subVec(p,np);
+            for (int j=0;j<layer_sizes[i+1];j++,k++)
+            {
+                group_params[k]=all_params.subVec(p,1+layer_sizes[i]);
+                group_params_delta[k]=all_params_delta.subVec(p,1+layer_sizes[i]);
+                group_params_gradient[k]=all_params_gradient.subVec(p,1+layer_sizes[i]);
+                p+=1+layer_sizes[i];
+            }
+        }
         activations_scaling[i].resize(layer_sizes[i+1]);
-        layer_params_gradient[i]=all_params_gradient.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
-        layer_params_delta[i]=all_params_delta.subVec(p,np);
         mean_activations[i].resize(layer_sizes[i+1]);
         var_activations[i].resize(layer_sizes[i+1]);
-        for (int j=0;j<layer_sizes[i+1];j++,k++)
-        {
-            neuron_params[k]=all_params.subVec(p,1+layer_sizes[i]);
-            neuron_params_delta[k]=all_params_delta.subVec(p,1+layer_sizes[i]);
-            neuron_params_gradient[k]=all_params_gradient.subVec(p,1+layer_sizes[i]);
-            p+=1+layer_sizes[i];
-        }
     }
-    if (params_natgrad_template)
+    if (params_natgrad_template || params_natgrad_per_input_template)
     {
-        params_natgrad_per_neuron.resize(n_neurons);
-        for (int i=0;i<n_neurons;i++)
-            params_natgrad_per_neuron[i] = PLearn::deepCopy(params_natgrad_template);
+        int n_input_groups=0;
+        int n_neuron_groups=0;
+        if(params_natgrad_template)
+            n_neuron_groups = n_neurons;
+        if( params_natgrad_per_input_template ) {
+            n_input_groups = layer_sizes[0]+1;
+            if(params_natgrad_template) // override first layer groups if present
+                n_neuron_groups -= layer_sizes[1];
+        }
+        params_natgrad_per_group.resize(n_input_groups+n_neuron_groups);
+        for (int i=0;i<n_input_groups;i++)
+            params_natgrad_per_group[i] = PLearn::deepCopy(params_natgrad_per_input_template);
+        for (int i=n_input_groups; i<n_input_groups+n_neuron_groups;i++)
+            params_natgrad_per_group[i] = PLearn::deepCopy(params_natgrad_template);
     }
     if (neurons_natgrad_template && neurons_natgrad_per_layer.length()==0)
     {
@@ -354,6 +410,31 @@
     train_costs.resize(minibatch_size,train_cost_names.length()-2 );
 
     Profiler::activate();
+
+    // Gradient correlation profiling
+    //if( corr_profiling_start != corr_profiling_end )  {
+    //    PLASSERT( (0<=corr_profiling_start) && (corr_profiling_start<corr_profiling_end) );
+    //    cout << "n_params " << n_params << endl;
+    //    // Build the names.
+    //    stringstream ss_suffix;
+    //    for (int i=0;i<n_layers;i++)    {
+    //        ss_suffix << "_" << layer_sizes[i];
+    //    }
+    //    ss_suffix << "_stages_" << corr_profiling_start << "_" << corr_profiling_end;
+    //    string str_gc_name = "gCcorr" + ss_suffix.str();
+    //    string str_ngc_name;
+    //    if( full_natgrad )  {
+    //        str_ngc_name = "ngCcorr_full" + ss_suffix.str();
+    //    }   else if (params_natgrad_template)   {
+    //        str_ngc_name = "ngCcorr_params" + ss_suffix.str();
+    //    }
+    //    // Build the profilers.
+    //    g_corrprof = new CorrelationProfiler( n_params, str_gc_name);
+    //    g_corrprof->build();
+    //    ng_corrprof = new CorrelationProfiler( n_params, str_ngc_name);
+    //    ng_corrprof->build();
+    //}
+
 }
 
 // ### Nothing to add here, simply calls build_
@@ -378,7 +459,8 @@
     deepCopyField(neurons_natgrad_template, copies);
     deepCopyField(neurons_natgrad_per_layer, copies);
     deepCopyField(params_natgrad_template, copies);
-    deepCopyField(params_natgrad_per_neuron, copies);
+    deepCopyField(params_natgrad_per_input_template, copies);
+    deepCopyField(params_natgrad_per_group, copies);
     deepCopyField(full_natgrad, copies);
     deepCopyField(layer_sizes, copies);
     deepCopyField(targets, copies);
@@ -393,9 +475,9 @@
     deepCopyField(neuron_gradients, copies);
     deepCopyField(neuron_gradients_per_layer, copies);
     deepCopyField(all_params_delta, copies);
-    deepCopyField(neuron_params, copies);
-    deepCopyField(neuron_params_gradient, copies);
-    deepCopyField(neuron_params_delta, copies);
+    deepCopyField(group_params, copies);
+    deepCopyField(group_params_gradient, copies);
+    deepCopyField(group_params_delta, copies);
     deepCopyField(layer_params_delta, copies);
 /*
     deepCopyField(, copies);
@@ -431,6 +513,7 @@
 
 void NatGradNNet::train()
 {
+
     if (inputsize_<0)
         build();
 
@@ -498,6 +581,14 @@
     costs_plus_time[train_costs.width()+1] = cumulative_training_time;
     train_stats->update( costs_plus_time );
     train_stats->finalize(); // finalize statistics for this epoch
+
+    // profiling gradient correlation
+    //if( g_corrprof )    {
+    //    PLASSERT( corr_profiling_end <= nstages );
+    //    g_corrprof->printAndReset();
+    //    ng_corrprof->printAndReset();
+    //}
+
 }
 
 void NatGradNNet::onlineStep(int t, const Mat& targets,
@@ -563,9 +654,16 @@
             }
         }
         // compute gradient on parameters, possibly update them
-        if (full_natgrad || params_natgrad_template) 
+        if (full_natgrad || params_natgrad_template || params_natgrad_per_input_template) 
         {
-            productScaleAcc(layer_params_gradient[i-1],next_neurons_gradient,true,
+//alternate
+            if( params_natgrad_per_input_template && i==1 ) // parameters are transposed
+                productScaleAcc(layer_params_gradient[i-1],
+                            neuron_extended_outputs_per_layer[i-1], true,
+                            next_neurons_gradient, false, 
+                            1, 0);                          
+            else
+                productScaleAcc(layer_params_gradient[i-1],next_neurons_gradient,true,
                             neuron_extended_outputs_per_layer[i-1],false,1,0);
             layer_params_gradient[i-1] *= 1.0/minibatch_size; // use the MEAN gradient
         } else // just regular stochastic gradient
@@ -579,19 +677,33 @@
     {
         (*full_natgrad)(t/minibatch_size,all_params_gradient,all_params_delta); // compute update direction by natural gradient
         if (output_layer_lrate_scale!=1.0)
-            layer_params_delta[n_layers-2] *= output_layer_lrate_scale; // scale output layer's learning rate 
+            layer_params_delta[n_layers-2] *= output_layer_lrate_scale; // scale output layer's learning rate
         multiplyAcc(all_params,all_params_delta,-lrate); // update
-    } else if (params_natgrad_template)
+        // Hack to apply batch gradient even in this case (used for profiling
+        // the gradient correlations)
+        //if (output_layer_lrate_scale!=1.0)
+        //      layer_params_gradient[n_layers-2] *= output_layer_lrate_scale; // scale output layer's learning rate
+        //  multiplyAcc(all_params,all_params_gradient,-lrate); // update
+
+    } else if (params_natgrad_template || params_natgrad_per_input_template)
     {
-        for (int i=0;i<params_natgrad_per_neuron.length();i++)
+        for (int i=0;i<params_natgrad_per_group.length();i++)
         {
-            NatGradEstimator& neuron_natgrad = *(params_natgrad_per_neuron[i]);
-            neuron_natgrad(t/minibatch_size,neuron_params_gradient[i],neuron_params_delta[i]); // compute update direction by natural gradient
+            NatGradEstimator& neuron_natgrad = *(params_natgrad_per_group[i]);
+            neuron_natgrad(t/minibatch_size,group_params_gradient[i],group_params_delta[i]); // compute update direction by natural gradient
         }
+//alternate
         if (output_layer_lrate_scale!=1.0)
             layer_params_delta[n_layers-2] *= output_layer_lrate_scale; // scale output layer's learning rate 
         multiplyAcc(all_params,all_params_delta,-lrate); // update
     }
+
+    // profiling gradient correlation
+    //if( (t>=corr_profiling_start) && (t<=corr_profiling_end) && g_corrprof )    {
+    //    (*g_corrprof)(all_params_gradient);
+    //    (*ng_corrprof)(all_params_delta);
+    //}
+
 }
 
 void NatGradNNet::computeOutput(const Vec& input, Vec& output) const
@@ -615,17 +727,23 @@
             prev_layer = prev_layer.subMatRows(0,n_examples);
             next_layer = next_layer.subMatRows(0,n_examples);
         }
+//alternate
+        // Are the input weights transposed? (because of ...)
+        bool tw = true;
+        if( params_natgrad_per_input_template && i==0 )
+            tw = false;
+
         // try to use BLAS for the expensive operation
         if (self_adjusted_scaling_and_bias && i+1<n_layers-1)
             productScaleAcc(next_layer, prev_layer, false, 
                             (during_training || params_averaging_coeff==1.0)?
                             weights[i]:mweights[i], 
-                            true, 1, 0);
+                            tw, 1, 0);
         else
             productScaleAcc(next_layer, prev_layer, false, 
                             (during_training || params_averaging_coeff==1.0)?
                             layer_params[i]:layer_mparams[i], 
-                            true, 1, 0);
+                            tw, 1, 0);
         // compute layer's output non-linearity
         if (i+1<n_layers-1)
             for (int k=0;k<n_examples;k++)

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-04-30 19:05:41 UTC (rev 6953)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-04-30 21:07:12 UTC (rev 6954)
@@ -43,6 +43,7 @@
 #include <plearn_learners/generic/PLearner.h>
 #include <plearn_learners/generic/NatGradEstimator.h>
 #include <plearn/sys/Profiler.h>
+//#include "CorrelationProfiler.h"
 
 namespace PLearn {
 
@@ -92,9 +93,18 @@
     //! natural gradient estimator for the parameters within each neuron
     //! (if 0 then do not correct the gradient on each neuron weight)
     PP<NatGradEstimator> params_natgrad_template;
-    //! the above template is used the user to specifiy all the elements of the vector below
-    TVec<PP<NatGradEstimator> > params_natgrad_per_neuron;
+    //! natural gradient estimator solely for the parameters of the first
+    //! layer. If present, performs over groups of parameters related to the
+    //! same input (this includes the additional bias input).
+    //! Has precedence over params_natgrad_template, ie if present, there is
+    //! no natural gradient performed on the groups of a neuron's parameters:
+    //! params_natgrad_template is not applied for the first hidden layer's
+    //! parameters). 
+    PP<NatGradEstimator> params_natgrad_per_input_template;
 
+    //! the above templates are used by the user to specifiy all the elements of the vector below
+    TVec<PP<NatGradEstimator> > params_natgrad_per_group;
+
     //! optionally, if neurons_natgrad==0 and params_natgrad_template==0, one can
     //! have regular stochastic gradient descent, or full-covariance natural gradient
     //! using the natural gradient estimator below
@@ -129,6 +139,9 @@
 
     int verbosity;
 
+    //! Stages for profiling the correlation between the gradients' elements
+    //int corr_profiling_start, corr_profiling_end;
+
 public:
     //#####  Public Member Functions  #########################################
 
@@ -259,9 +272,9 @@
     Vec all_mparams; // mean parameters (moving-averaged over past values)
     TVec<Mat> layer_params_gradient;
     TVec<Vec> layer_params_delta;
-    TVec<Vec> neuron_params; // params of each neuron (pointing in all_params)
-    TVec<Vec> neuron_params_delta; // params_delta of each neuron (pointing in all_params_delta)
-    TVec<Vec> neuron_params_gradient; // params_delta of each neuron (pointing in all_params_gradient)
+    TVec<Vec> group_params; // params of each group (pointing in all_params)
+    TVec<Vec> group_params_delta; // params_delta of each group (pointing in all_params_delta)
+    TVec<Vec> group_params_gradient; // params_delta of each group (pointing in all_params_gradient)
     Mat neuron_gradients; // one row per example of a minibatch, has concatenation of layer 0, layer 1, ... gradients.
     TVec<Mat> neuron_gradients_per_layer; // pointing into neuron_gradients (one row per example of a minibatch)
     mutable TVec<Mat> neuron_outputs_per_layer;  // same structure
@@ -269,6 +282,8 @@
     Mat targets; // one target row per example in a minibatch
     Vec example_weights; // one element per example in a minibatch
     Mat train_costs; // one row per example in a minibatch
+
+    //PP<CorrelationProfiler> g_corrprof, ng_corrprof;    // for optional gradient correlation profiling
 };
 
 // Declares a few other classes and functions related to this class



From manzagop at mail.berlios.de  Mon Apr 30 23:10:11 2007
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Mon, 30 Apr 2007 23:10:11 +0200
Subject: [Plearn-commits] r6955 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200704302110.l3ULABAw017790@sheep.berlios.de>

Author: manzagop
Date: 2007-04-30 23:10:10 +0200 (Mon, 30 Apr 2007)
New Revision: 6955

Added:
   trunk/plearn_learners/generic/EXPERIMENTAL/CorrelationProfiler.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/CorrelationProfiler.h
Log:
Helper class for computing gradient correlations.



Added: trunk/plearn_learners/generic/EXPERIMENTAL/CorrelationProfiler.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/CorrelationProfiler.cc	2007-04-30 21:07:12 UTC (rev 6954)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/CorrelationProfiler.cc	2007-04-30 21:10:10 UTC (rev 6955)
@@ -0,0 +1,204 @@
+// -*- C++ -*-
+
+// CorrelationProfiler.cc
+//
+// Copyright (C) 2007 Pierre-Antoine Manzagol
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pierre-Antoine Manzagol
+
+/*! \file CorrelationProfiler.cc */
+
+
+#include "CorrelationProfiler.h"
+#include <plearn/math/TMat_maths.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    CorrelationProfiler,
+    "Object used to profile the correlation between the elements of a vector",
+    "\n"
+    );
+
+CorrelationProfiler::CorrelationProfiler()      :   its_dim(0), 
+                                                    its_name("noname")
+    /* ### Initialize all fields to their default value */
+{
+    // ...
+
+    // ### You may (or not) want to call build_() to finish building the object
+    // ### (doing so assumes the parent classes' build_() have been called too
+    // ### in the parent classes' constructors, something that you must ensure)
+}
+
+
+CorrelationProfiler::CorrelationProfiler(int dim, string name)    :  its_dim(dim),
+                                                its_name(name)
+{
+}
+
+
+void CorrelationProfiler::reset()   
+{
+    A.clear();
+    sum_v.clear();
+    n = 0;
+}
+
+// Accumulates statistics for computing the correlation
+void CorrelationProfiler::operator()(Vec& v)
+{
+    PLASSERT( A.length() == v.length() );
+    externalProductAcc(A, v, v);
+    sum_v += v;
+    n++;
+}
+
+
+void CorrelationProfiler::printAndReset()
+{
+
+    cout << its_name << " - correlation based on " << n << " samples." << endl;
+
+    // *** Get mean
+    sum_v /= n;
+
+    // *** Divide by n-1 to get non-centered covariance
+    A /= (real)(n-1);
+
+    // *** Get the centered covariance
+    externalProductScaleAcc(A, sum_v, sum_v, -1.0);
+
+    // *** Get correlation by dividing by the product of the standard deviations
+    Vec diagA = diag( A );
+    // TODO Not very efficient. Isn't there a lapack function for this. 
+    for(int i=0; i<A.length(); i++)   {
+        for(int j=0; j<i; j++)   {
+            A(i,j) /= sqrt( diagA[i]*diagA[j] );
+            A(j,i) = A(i,j);
+        }
+        // the diagonal
+        A(i,i) = 1.0;
+    }
+
+    // *** Open file, print correlation and close.
+    // TODO check opening
+    string file_name;
+    file_name = its_name + ".txt";
+    ofstream fd;
+    fd.open( file_name.c_str() );
+    A.print(fd);
+    fd.close();
+
+    // *** Reset
+    reset();
+}
+
+
+// ### Nothing to add here, simply calls build_
+void CorrelationProfiler::build()
+{
+    inherited::build();
+    build_();
+}
+
+void CorrelationProfiler::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### Call deepCopyField on all "pointer-like" fields
+    // ### that you wish to be deepCopied rather than
+    // ### shallow-copied.
+    // ### ex:
+    deepCopyField(A, copies);
+    deepCopyField(sum_v, copies);
+
+    // ### Remove this line when you have fully implemented this method.
+    //PLERROR("CorrelationProfiler::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
+}
+
+void CorrelationProfiler::declareOptions(OptionList& ol)
+{
+    // ### Declare all of this object's options here.
+    // ### For the "flags" of each option, you should typically specify
+    // ### one of OptionBase::buildoption, OptionBase::learntoption or
+    // ### OptionBase::tuningoption. If you don't provide one of these three,
+    // ### this option will be ignored when loading values from a script.
+    // ### You can also combine flags, for example with OptionBase::nosave:
+    // ### (OptionBase::buildoption | OptionBase::nosave)
+
+    declareOption(ol, "dimension", &CorrelationProfiler::its_dim,
+                   OptionBase::buildoption,
+                   "Dimension of the vector whose correlantion we want (integer).");
+    declareOption(ol, "name", &CorrelationProfiler::its_name,
+                   OptionBase::buildoption,
+                   "Name of the vector whose correlantion we want (string)."
+                    "Used in determining the output file's name in"
+                    "printAndReset().");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void CorrelationProfiler::build_()
+{
+    // ### This method should do the real building of the object,
+    // ### according to set 'options', in *any* situation.
+    // ### Typical situations include:
+    // ###  - Initial building of an object from a few user-specified options
+    // ###  - Building of a "reloaded" object: i.e. from the complete set of
+    // ###    all serialised options.
+    // ###  - Updating or "re-building" of an object after a few "tuning"
+    // ###    options have been modified.
+    // ### You should assume that the parent class' build_() has already been
+    // ### called.
+    PLASSERT( its_dim > 0 );
+    A.resize(its_dim, its_dim);
+    sum_v.resize(its_dim);
+    reset();
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/generic/EXPERIMENTAL/CorrelationProfiler.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/CorrelationProfiler.h	2007-04-30 21:07:12 UTC (rev 6954)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/CorrelationProfiler.h	2007-04-30 21:10:10 UTC (rev 6955)
@@ -0,0 +1,140 @@
+// -*- C++ -*-
+
+// CorrelationProfiler.h
+//
+// Copyright (C) 2007 Pierre-Antoine Manzagol
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pierre-Antoine Manzagol
+
+/*! \file CorrelationProfiler.h */
+
+
+#ifndef CorrelationProfiler_INC
+#define CorrelationProfiler_INC
+
+#include <plearn/base/Object.h>
+
+namespace PLearn {
+
+/**
+ * Used to profile the correlation between the elements of a vector. 
+ *
+ * @todo 
+ * @deprecated 
+ */
+class CorrelationProfiler : public Object
+{
+    typedef Object inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+    int its_dim;
+    string its_name;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    CorrelationProfiler();
+    CorrelationProfiler(int dim, string name);
+
+    // Your other public member functions go here
+    void reset();
+    void operator()(Vec& v);
+    void printAndReset();
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(CorrelationProfiler);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+    int n;  // number of vectors used in computing A
+    Mat A;  // sum of the vv'
+    Vec sum_v;  // sum of the v
+    
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(CorrelationProfiler);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From yoshua at mail.berlios.de  Mon Apr 30 23:59:37 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Mon, 30 Apr 2007 23:59:37 +0200
Subject: [Plearn-commits] r6956 - trunk/plearn_learners/online
Message-ID: <200704302159.l3ULxbFn020005@sheep.berlios.de>

Author: yoshua
Date: 2007-04-30 23:59:37 +0200 (Mon, 30 Apr 2007)
New Revision: 6956

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
   trunk/plearn_learners/online/RBMConnection.cc
   trunk/plearn_learners/online/RBMConnection.h
   trunk/plearn_learners/online/RBMLayer.cc
   trunk/plearn_learners/online/RBMLayer.h
Log:
Added code in DeepBeliefNet and headers in RBM{Connection,Layer} to handle
the background Gibbs chain, but the updates in RBM{Connection,Layer} are still to do.


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-04-30 21:10:10 UTC (rev 6955)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-04-30 21:59:37 UTC (rev 6956)
@@ -239,6 +239,10 @@
                   OptionBase::learntoption,
                   "Size of a mini-batch.");
 
+    declareOption(ol, "gibbs_down_state", &DeepBeliefNet::gibbs_down_state,
+                  OptionBase::learntoption,
+                  "State of visible units of RBMs at each layer in background Gibbs chain.");
+
     /*
     declareOption(ol, "n_final_costs", &DeepBeliefNet::n_final_costs,
                   OptionBase::learntoption,
@@ -313,7 +317,6 @@
     activations_gradients.resize( n_layers );
     expectation_gradients.resize( n_layers );
     expectations_gradients.resize( n_layers );
-    gibbs_up_state.resize( n_layers-1 );
     gibbs_down_state.resize( n_layers-1 );
 
     for( int i=0 ; i<n_layers-1 ; i++ )
@@ -500,10 +503,9 @@
     deepCopyField(save_layer_expectation,   copies);
     deepCopyField(pos_down_val,             copies);
     deepCopyField(pos_up_val,               copies);
-    deepCopyField(pos_down_vals,            copies);
-    deepCopyField(pos_up_vals,              copies);
-    deepCopyField(gibbs_up_state,           copies);
-    deepCopyField(gibbs_down_state,           copies);
+    deepCopyField(cd_neg_up_vals,           copies);
+    deepCopyField(cd_neg_down_vals,         copies);
+    deepCopyField(gibbs_down_state,         copies);
     deepCopyField(optimized_costs,          copies);
     deepCopyField(final_cost_indices,       copies);
     deepCopyField(partial_cost_indices,     copies);
@@ -573,10 +575,7 @@
             expectation_gradients[i].resize(minibatch_size, layers[i]->size);
 
             if (background_gibbs_update_ratio>0)
-            {
-                gibbs_up_state[i].resize(minibatch_size, layers[i]->size);
                 gibbs_down_state[i].resize(minibatch_size, layers[i]->size);
-            }
         }
         if (final_cost)
             final_cost_gradients.resize(minibatch_size, final_cost->input_size);
@@ -1021,7 +1020,7 @@
             contrastiveDivergenceStep(
                 get_pointer(joint_layer),
                 get_pointer(classification_module->joint_connection),
-                layers[ n_layers-1 ] );
+                layers[ n_layers-1 ], n_layers-2);
 
             layers[ n_layers-2 ]->activation << save_layer_activation;
             layers[ n_layers-2 ]->expectation << save_layer_expectation;
@@ -1063,7 +1062,7 @@
         contrastiveDivergenceStep( layers[ i ],
                                    connections[ i ],
                                    layers[ i+1 ] ,
-                                   true);
+                                   i, true);
         if( i > 0 )
         {
             layers[i]->activation << save_layer_activation;
@@ -1123,7 +1122,7 @@
     contrastiveDivergenceStep( layers[ index ],
                                connections[ index ],
                                layers[ index+1 ],
-                               true );
+                               index, true);
 }
 
 /////////////////
@@ -1179,7 +1178,7 @@
     contrastiveDivergenceStep( layers[ index ],
                                connections[ index ],
                                layers[ index+1 ],
-                               true );
+                               index, true);
 }
 
 /////////////////////
@@ -1242,7 +1241,7 @@
     contrastiveDivergenceStep(
         get_pointer( joint_layer ),
         get_pointer( classification_module->joint_connection ),
-        layers[ n_layers-1 ] );
+        layers[ n_layers-1 ], n_layers-2);
 }
 
 ////////////////////
@@ -1480,7 +1479,7 @@
     const PP<RBMLayer>& down_layer,
     const PP<RBMConnection>& connection,
     const PP<RBMLayer>& up_layer,
-    bool nofprop)
+    int layer_index, bool nofprop)
 {
     bool mbatch = minibatch_size > 1 || minibatch_hack;
 
@@ -1509,8 +1508,8 @@
 
         // down propagation, starting from a sample of up_layer
         if (background_gibbs_update_ratio<1) 
+            // then do some contrastive divergence, o/w only background Gibbs
         {
-            // then do some contrastive divergence, o/w only background Gibbs
             up_layer->generateSamples();
             connection->setAsUpInputs( up_layer->samples );
             down_layer->getAllActivations( connection, 0, true );
@@ -1525,18 +1524,68 @@
             Mat neg_down_vals = down_layer->samples;
             Mat neg_up_vals = up_layer->getExpectations();
 
-            // update
             if (background_gibbs_update_ratio==0)
+            // update here only if there is ONLY contrastive divergence
             {
                 down_layer->update( pos_down_vals, neg_down_vals );
                 connection->update( pos_down_vals, pos_up_vals,
                                     neg_down_vals, neg_up_vals );
                 up_layer->update( pos_up_vals, neg_up_vals );
             }
+            else
+            {
+                connection->accumulatePosStats(pos_down_vals,pos_up_vals);
+                cd_neg_down_vals.resize(minibatch_size, down_layer->size);
+                cd_neg_up_vals.resize(minibatch_size, up_layer->size);
+                cd_neg_down_vals << neg_down_vals;
+                cd_neg_up_vals << neg_up_vals;
+            }
         }
         // 
         if (background_gibbs_update_ratio>0) 
         {
+            Mat down_state = gibbs_down_state[layer_index];
+
+            // sample up state given down state
+            connection->setAsDownInputs(down_state);
+            up_layer->getAllActivations(connection, 0, true);
+            up_layer->computeExpectations();
+            up_layer->generateSamples();
+
+            // update using the down_state and up_layer->expectations for moving average in negative phase
+            // (and optionally 
+            if (background_gibbs_update_ratio<1)
+            {
+                down_layer->updateCDandGibbs(pos_down_vals,cd_neg_down_vals,
+                                             down_state,
+                                             background_gibbs_update_ratio,
+                                             gibbs_chain_statistics_forgetting_factor);
+                connection->updateCDandGibbs(pos_down_vals,pos_up_vals,
+                                             cd_neg_down_vals, cd_neg_up_vals,
+                                             down_state,
+                                             up_layer->getExpectations(),
+                                             background_gibbs_update_ratio,
+                                             gibbs_chain_statistics_forgetting_factor);
+                up_layer->updateCDandGibbs(pos_up_vals,cd_neg_up_vals,
+                                           up_layer->getExpectations(),
+                                           background_gibbs_update_ratio,
+                                           gibbs_chain_statistics_forgetting_factor);
+            }
+            else
+            {
+                down_layer->updateGibbs(pos_down_vals,down_state,
+                                        gibbs_chain_statistics_forgetting_factor);
+                connection->updateGibbs(pos_down_vals,pos_up_vals,down_state,
+                                        up_layer->getExpectations(),
+                                        gibbs_chain_statistics_forgetting_factor);
+                up_layer->updateGibbs(pos_up_vals,up_layer->getExpectations(),
+                                        gibbs_chain_statistics_forgetting_factor);
+            }
+            // sample down state given up state, to prepare for next time
+            connection->setAsUpInputs(up_layer->samples);
+            down_layer->getAllActivations(connection, 0, true);
+            down_layer->generateSamples();
+            down_state << down_layer->samples;
         }
     } else {
         up_layer->generateSample();

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2007-04-30 21:10:10 UTC (rev 6955)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2007-04-30 21:59:37 UTC (rev 6956)
@@ -235,6 +235,7 @@
     void contrastiveDivergenceStep( const PP<RBMLayer>& down_layer,
                                     const PP<RBMConnection>& connection,
                                     const PP<RBMLayer>& up_layer,
+                                    int layer_index,
                                     bool nofprop=false);
 
 
@@ -319,9 +320,10 @@
     mutable Vec pos_up_val;
     mutable Mat pos_down_vals;
     mutable Mat pos_up_vals;
+    mutable Mat cd_neg_down_vals;
+    mutable Mat cd_neg_up_vals;
     
     //! Store the state of the Gibbs chain for each RBM
-    mutable TVec<Mat> gibbs_up_state;
     mutable TVec<Mat> gibbs_down_state;
 
     //! Used to store the costs optimized by the final cost module.

Modified: trunk/plearn_learners/online/RBMConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMConnection.cc	2007-04-30 21:10:10 UTC (rev 6955)
+++ trunk/plearn_learners/online/RBMConnection.cc	2007-04-30 21:59:37 UTC (rev 6956)
@@ -240,6 +240,38 @@
     update();
 }
 
+// neg_stats <-- gibbs_chain_statistics_forgetting_factor * neg_stats
+//              +(1-gibbs_chain_statistics_forgetting_factor)
+//               * gibbs_neg_up_values'*gibbs_neg_down_values
+// delta w = lrate * ( pos_up_values'*pos_down_values
+//                  - ( background_gibbs_update_ratio*neg_stats
+//                     +(1-background_gibbs_update_ratio)
+//                      * cd_neg_up_values'*cd_neg_down_values))
+void RBMConnection::updateCDandGibbs( const Mat& pos_down_values,
+                                      const Mat& pos_up_values,
+                                      const Mat& cd_neg_down_values,
+                                      const Mat& cd_neg_up_values,
+                                      const Mat& gibbs_neg_down_values,
+                                      const Mat& gibbs_neg_up_values,
+                                      real background_gibbs_update_ratio,
+                                      real gibbs_chain_statistics_forgetting_factor)
+{
+    PLASSERT_MSG(false, "Not implemented by subclass!");
+}
+
+// neg_stats <-- gibbs_chain_statistics_forgetting_factor * neg_stats
+//              +(1-gibbs_chain_statistics_forgetting_factor)
+//               * gibbs_neg_up_values'*gibbs_neg_down_values
+// delta w = lrate * ( pos_up_values'*pos_down_values - neg_stats )
+void RBMConnection::updateGibbs( const Mat& pos_down_values,
+                                 const Mat& pos_up_values,
+                                 const Mat& gibbs_neg_down_values,
+                                 const Mat& gibbs_neg_up_values,
+                                 real gibbs_chain_statistics_forgetting_factor)
+{
+    PLASSERT_MSG(false, "Not implemented by subclass!");
+}
+
 ///////////
 // fprop //
 ///////////

Modified: trunk/plearn_learners/online/RBMConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMConnection.h	2007-04-30 21:10:10 UTC (rev 6955)
+++ trunk/plearn_learners/online/RBMConnection.h	2007-04-30 21:59:37 UTC (rev 6956)
@@ -146,6 +146,32 @@
                          const Mat& neg_down_values,
                          const Mat& neg_up_values);
 
+    // neg_stats <-- gibbs_chain_statistics_forgetting_factor * neg_stats
+    //              +(1-gibbs_chain_statistics_forgetting_factor)
+    //               * gibbs_neg_up_values'*gibbs_neg_down_values
+    // delta w = lrate * ( pos_up_values'*pos_down_values
+    //                  - ( background_gibbs_update_ratio*neg_stats
+    //                     +(1-background_gibbs_update_ratio)
+    //                      * cd_neg_up_values'*cd_neg_down_values))
+    virtual void updateCDandGibbs( const Mat& pos_down_values,
+                                   const Mat& pos_up_values,
+                                   const Mat& cd_neg_down_values,
+                                   const Mat& cd_neg_up_values,
+                                   const Mat& gibbs_neg_down_values,
+                                   const Mat& gibbs_neg_up_values,
+                                   real background_gibbs_update_ratio,
+                                   real gibbs_chain_statistics_forgetting_factor);
+
+    // neg_stats <-- gibbs_chain_statistics_forgetting_factor * neg_stats
+    //              +(1-gibbs_chain_statistics_forgetting_factor)
+    //               * gibbs_neg_up_values'*gibbs_neg_down_values
+    // delta w = lrate * ( pos_up_values'*pos_down_values - neg_stats )
+    virtual void updateGibbs( const Mat& pos_down_values,
+                              const Mat& pos_up_values,
+                              const Mat& gibbs_neg_down_values,
+                              const Mat& gibbs_neg_up_values,
+                              real gibbs_chain_statistics_forgetting_factor);
+
     //! Clear all information accumulated during stats
     virtual void clearStats() = 0;
 

Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2007-04-30 21:10:10 UTC (rev 6955)
+++ trunk/plearn_learners/online/RBMLayer.cc	2007-04-30 21:59:37 UTC (rev 6956)
@@ -407,6 +407,33 @@
     }
 }
 
+// neg_stats <-- gibbs_chain_statistics_forgetting_factor * neg_stats
+//              +(1-gibbs_chain_statistics_forgetting_factor)
+//               * gibbs_neg_values
+// delta w = lrate * ( pos_values
+//                  - ( background_gibbs_update_ratio*neg_stats
+//                     +(1-background_gibbs_update_ratio)
+//                      * cd_neg_values ) )
+void RBMLayer::updateCDandGibbs( const Mat& pos_values,
+                                 const Mat& cd_neg_values,
+                                 const Mat& gibbs_neg_values,
+                                 real background_gibbs_update_ratio,
+                                 real gibbs_chain_statistics_forgetting_factor)
+{
+    PLASSERT_MSG(false, "Not implemented by subclass!");
+}
+
+// neg_stats <-- gibbs_chain_statistics_forgetting_factor * neg_stats
+//              +(1-gibbs_chain_statistics_forgetting_factor)
+//               * gibbs_neg_values
+// delta w = lrate * ( pos_values - neg_stats )
+void RBMLayer::updateGibbs( const Mat& pos_values,
+                            const Mat& gibbs_neg_values,
+                            real gibbs_chain_statistics_forgetting_factor)
+{
+    PLASSERT_MSG(false, "Not implemented by subclass!");
+}
+
 ////////////////
 // setAllBias //
 ////////////////

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2007-04-30 21:10:10 UTC (rev 6955)
+++ trunk/plearn_learners/online/RBMLayer.h	2007-04-30 21:59:37 UTC (rev 6956)
@@ -195,6 +195,28 @@
     //! Update parameters according to one pair of matrices.
     virtual void update( const Mat& pos_values, const Mat& neg_values );
 
+    // neg_stats <-- gibbs_chain_statistics_forgetting_factor * neg_stats
+    //              +(1-gibbs_chain_statistics_forgetting_factor)
+    //               * gibbs_neg_values
+    // delta w = lrate * ( pos_values
+    //                  - ( background_gibbs_update_ratio*neg_stats
+    //                     +(1-background_gibbs_update_ratio)
+    //                      * cd_neg_values ) )
+    virtual void updateCDandGibbs( const Mat& pos_values,
+                                   const Mat& cd_neg_values,
+                                   const Mat& gibbs_neg_values,
+                                   real background_gibbs_update_ratio,
+                                   real gibbs_chain_statistics_forgetting_factor);
+
+    // neg_stats <-- gibbs_chain_statistics_forgetting_factor * neg_stats
+    //              +(1-gibbs_chain_statistics_forgetting_factor)
+    //               * gibbs_neg_values
+    // delta w = lrate * ( pos_values - neg_stats )
+    virtual void updateGibbs( const Mat& pos_values,
+                              const Mat& gibbs_neg_values,
+                              real gibbs_chain_statistics_forgetting_factor);
+
+
     //! resets activations, sample and expectation fields
     virtual void reset();
 



