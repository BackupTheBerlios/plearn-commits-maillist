From dorionc at mail.berlios.de  Thu Feb  1 17:39:11 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Thu, 1 Feb 2007 17:39:11 +0100
Subject: [Plearn-commits] r6619 - trunk/plearn/misc
Message-ID: <200702011639.l11GdBqt011916@sheep.berlios.de>

Author: dorionc
Date: 2007-02-01 17:39:10 +0100 (Thu, 01 Feb 2007)
New Revision: 6619

Modified:
   trunk/plearn/misc/Calendar.cc
   trunk/plearn/misc/Calendar.h
Log:
Added an accessor for the ctime corresponding to the last day of month...


Modified: trunk/plearn/misc/Calendar.cc
===================================================================
--- trunk/plearn/misc/Calendar.cc	2007-01-26 14:26:12 UTC (rev 6618)
+++ trunk/plearn/misc/Calendar.cc	2007-02-01 16:39:10 UTC (rev 6619)
@@ -247,7 +247,23 @@
     return last_calendar_time_;
 }
 
+CTime Calendar::getLastDayOfMonth(const PDate& day_of_the_month) const
+{
+    PDate last_day_of_month = day_of_the_month.lastDateOfMonth();
+    PLASSERT(day_of_the_month.month==last_day_of_month.month);
 
+    JTime jtime_ldom = last_day_of_month.toJulianDay();
+    CTime ctime_ldom = getCalendarTime(jtime_ldom);
+
+    // Was first day of the following month returned?
+    if ( timestamps_[ctime_ldom] > jtime_ldom )
+        ctime_ldom--;
+
+    PLASSERT( ctime_ldom >= 0 && timestamps_[ctime_ldom] <= jtime_ldom );
+    return ctime_ldom;
+}
+
+
 CTime Calendar::convertCalendarTime(const Calendar& source_calendar,
                                     const Calendar& dest_calendar,
                                     CTime source_time,

Modified: trunk/plearn/misc/Calendar.h
===================================================================
--- trunk/plearn/misc/Calendar.h	2007-01-26 14:26:12 UTC (rev 6618)
+++ trunk/plearn/misc/Calendar.h	2007-02-01 16:39:10 UTC (rev 6619)
@@ -49,6 +49,7 @@
 #include <limits.h>
 
 #include <plearn/base/Object.h>
+#include <plearn/base/PDate.h>
 #include "PRange.h"
 
 namespace PLearn {
@@ -229,6 +230,12 @@
     CTime getCalendarTime(JTime julian_time, bool use_lower_bound = true) const;
 
     /**
+     * Similar to a call to getCalendarTime but ensures that the ctime returned
+     * actually is in the same month than the argument provided
+     */
+    CTime getLastDayOfMonth(const PDate& day_of_the_month) const;
+    
+    /**
      *  Returns true iff julian_time is a valid timestamp. If specified,
      *  argument calendar_time will be filled with the right value if true and
      *  will be left unchanged if not.



From dorionc at mail.berlios.de  Thu Feb  1 18:06:54 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Thu, 1 Feb 2007 18:06:54 +0100
Subject: [Plearn-commits] r6620 - trunk/python_modules/plearn/math/stats
Message-ID: <200702011706.l11H6rUQ013841@sheep.berlios.de>

Author: dorionc
Date: 2007-02-01 18:06:53 +0100 (Thu, 01 Feb 2007)
New Revision: 6620

Modified:
   trunk/python_modules/plearn/math/stats/pytest.config
Log:
CVXOPT is not installed at LISA yet...


Modified: trunk/python_modules/plearn/math/stats/pytest.config
===================================================================
--- trunk/python_modules/plearn/math/stats/pytest.config	2007-02-01 16:39:10 UTC (rev 6619)
+++ trunk/python_modules/plearn/math/stats/pytest.config	2007-02-01 17:06:53 UTC (rev 6620)
@@ -96,22 +96,22 @@
     name = "PL_cvx_numarray_matrix_conversions",
     description = "",
     category = "General",
-    program = Program(name = "python"),
+    program = Program( name = "python" ),
     arguments = "-c 'from plearn.math.stats.cvx_utils import *; test_numarray_cvx_conversions()'",
     resources = [ ],
     precision = 1e-06,
     pfileprg = None,
-    disabled = False
+    disabled = True
     )
 
 Test(
     name = "PL_cvx_numpy_matrix_conversions",
     description = "",
     category = "General",
-    program = Program(name = "python"),
+    program = Program( name = "python" ),
     arguments = "-c 'from plearn.math.stats.cvx_utils import *; test_numpy_cvx_conversions()'",
     resources = [ ],
     precision = 1e-06,
     pfileprg = None,
-    disabled = False
+    disabled = True
     )



From dorionc at mail.berlios.de  Thu Feb  1 20:37:01 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Thu, 1 Feb 2007 20:37:01 +0100
Subject: [Plearn-commits] r6621 - trunk/python_modules/plearn/pytest
Message-ID: <200702011937.l11Jb1Cl001715@sheep.berlios.de>

Author: dorionc
Date: 2007-02-01 20:37:00 +0100 (Thu, 01 Feb 2007)
New Revision: 6621

Modified:
   trunk/python_modules/plearn/pytest/modes.py
Log:
Meld mode renamed to diff -- Now uses kdiff3.

Modified: trunk/python_modules/plearn/pytest/modes.py
===================================================================
--- trunk/python_modules/plearn/pytest/modes.py	2007-02-01 17:06:53 UTC (rev 6620)
+++ trunk/python_modules/plearn/pytest/modes.py	2007-02-01 19:37:00 UTC (rev 6621)
@@ -296,13 +296,13 @@
             except KeyError:
                 logging.critical("No test named %s found."%options.test_name)
 
-class meld(locate):
-    """Starts meld to compare the expected/run results directory trees.
+class diff(locate):
+    """Starts kdiff3 to compare the expected/run results directory trees.
 
-    Usage: pytest meld <test_name>
+    Usage: pytest diff <test_name>
     """
     def __init__(self, targets, options):
-        super(meld, self).__init__(targets, options)
+        super(diff, self).__init__(targets, options)
         if isinstance(self._listed, tuple):
             moresh.pushd()
             dirc = self._listed[0]
@@ -324,7 +324,8 @@
 
         expected = test.resultsDirectory(EXPECTED_RESULTS)
         run_results = test.resultsDirectory(RUN_RESULTS)
-        os.system("meld %s %s"%(expected, run_results))
+        os.system("kdiff3 %s %s"%(expected, run_results))
+        #os.system("meld %s %s"%(expected, run_results))
         
 class prune( PyTestMode ):
     """Removes all pytest directories within given test directories."""    



From dorionc at mail.berlios.de  Thu Feb  1 20:37:48 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Thu, 1 Feb 2007 20:37:48 +0100
Subject: [Plearn-commits] r6622 - trunk/python_modules/plearn/vmat
Message-ID: <200702011937.l11JbmP4001786@sheep.berlios.de>

Author: dorionc
Date: 2007-02-01 20:37:47 +0100 (Thu, 01 Feb 2007)
New Revision: 6622

Modified:
   trunk/python_modules/plearn/vmat/PMat.py
Log:
Removed annoying print

Modified: trunk/python_modules/plearn/vmat/PMat.py
===================================================================
--- trunk/python_modules/plearn/vmat/PMat.py	2007-02-01 19:37:00 UTC (rev 6621)
+++ trunk/python_modules/plearn/vmat/PMat.py	2007-02-01 19:37:47 UTC (rev 6622)
@@ -40,7 +40,7 @@
     if isinstance( cols, int ):
         indices = [ cols ]
     elif isinstance( cols, slice ):
-        print cols
+        #print cols
         indices = range( *cols.indices(cols.stop) )
     else:
         indices = list( cols )            



From dorionc at mail.berlios.de  Thu Feb  1 20:38:22 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Thu, 1 Feb 2007 20:38:22 +0100
Subject: [Plearn-commits] r6623 - in trunk/python_modules/plearn: math report
Message-ID: <200702011938.l11JcMdH001929@sheep.berlios.de>

Author: dorionc
Date: 2007-02-01 20:38:22 +0100 (Thu, 01 Feb 2007)
New Revision: 6623

Modified:
   trunk/python_modules/plearn/math/statistical_tools.py
   trunk/python_modules/plearn/report/formatter.py
   trunk/python_modules/plearn/report/graphical_tools.py
Log:
Minor improvements


Modified: trunk/python_modules/plearn/math/statistical_tools.py
===================================================================
--- trunk/python_modules/plearn/math/statistical_tools.py	2007-02-01 19:37:47 UTC (rev 6622)
+++ trunk/python_modules/plearn/math/statistical_tools.py	2007-02-01 19:38:22 UTC (rev 6623)
@@ -250,8 +250,7 @@
     for stat,pvalue in stats:
         assert pvalue >= 0.0 and pvalue <= 1.0, "Invalid probability %.3f"%pvalue
         pair = stat, report_significance(pvalue, "%.3f")
-        formatted.append(stacked_pair(pair, '%.2f', r'\text{\scriptsize (%s)}'))
-        #    r"$\underset{\text{(%s)}}{%.2f}$"%(pair[1], pair[0]) )
+        formatted.append(stacked_pair(pair, '%.3f', r'\text{\scriptsize (%s)}'))
     if len(formatted)==1:
         return formatted[0]
     return formatted

Modified: trunk/python_modules/plearn/report/formatter.py
===================================================================
--- trunk/python_modules/plearn/report/formatter.py	2007-02-01 19:37:47 UTC (rev 6622)
+++ trunk/python_modules/plearn/report/formatter.py	2007-02-01 19:38:22 UTC (rev 6623)
@@ -63,7 +63,7 @@
 def latexTable(table, headers=[], 
                align="", super_headers=[],
                padding=0.5, vpadding=0.0, caption="", label="",
-               fontsize="", landscape=False, writer=DEFAULT_WRITER):
+               fontsize="", landscape=False, targetpos="", writer=DEFAULT_WRITER):
     lwriter = lambda line : writer("%s\n"%line)
     if align:
         assert len(align)==len(table[0]), \
@@ -86,7 +86,7 @@
     if landscape:
         lwriter(r"\begin{landscape}")
         lwriter(r"\addtocounter{@inlandscapetable}{1}")
-    lwriter("\\begin{table}")
+    lwriter("\\begin{table}%s"%targetpos)
     
     lwriter("\\begin{center}")
     if fontsize:
@@ -133,6 +133,10 @@
             handling_multicol.append(elem)
     writer('&'.join(handling_multicol) + r"\\" + "\n")
 
+def vpaddingLine(vpadding, length):
+    vpad = r"\raisebox{%.3fcm}{\rule{0pt}{%.3fcm}}"%(-0.5*vpadding, vpadding)
+    return [vpad]+[""]*(length-1)
+
 def strictlyUpperTriangularTable(table, headers=[], format="%s"):
     """Returns a table of strings and modified headers suitable for latex/twikiTable.
 

Modified: trunk/python_modules/plearn/report/graphical_tools.py
===================================================================
--- trunk/python_modules/plearn/report/graphical_tools.py	2007-02-01 19:37:47 UTC (rev 6622)
+++ trunk/python_modules/plearn/report/graphical_tools.py	2007-02-01 19:38:22 UTC (rev 6623)
@@ -79,8 +79,11 @@
         self.max = max(limits[1], self.max)    
 
 class FigureWrapper(object):
+    instances = []
+
     def __init__(self, figsize=(12,10)):
         self.figure = getNewFigure(figsize)
+        self.instances.append(self)
 
     def addAxes(self, rect, *args, **kwargs):
         axes = self.figure.add_axes(rect, *args, **kwargs)
@@ -106,6 +109,12 @@
             for label in axes.get_yticklabels():
                 label.set_fontproperties(fp)            
 
+    def publishAll(FigureWrapper, ext='pdf', fno_start=1):
+        for fno, figure in enumerate(FigureWrapper.instances):
+            figure.publish('figure%d.%s'%(fno_start+fno,ext))
+        FigureWrapper.instances = []
+    publishAll = classmethod(publishAll)
+    
 class TwoFramesFigure(FigureWrapper):
     def __init__(self,
                  urect = [LEFT, 0.525, WIDTH, 0.375],
@@ -114,11 +123,11 @@
 
         self.urect = urect
         self.upperAxes = self.figure.add_axes(urect)
-        print self.upperAxes.get_frame()
+        #print self.upperAxes.get_frame()
 
         self.lrect = lrect
         self.lowerAxes = self.figure.add_axes(lrect)
-        print self.lowerAxes.get_frame()
+        #print self.lowerAxes.get_frame()
 
 #3 frames: impact          = self.addAxes(getWideRect(0.075, 0.250))
 #3 frames: positive_impact = self.addAxes(getWideRect(0.375, 0.250))



From dorionc at mail.berlios.de  Thu Feb  1 20:54:13 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Thu, 1 Feb 2007 20:54:13 +0100
Subject: [Plearn-commits] r6624 -
	trunk/python_modules/plearn/pytest/.pytest/PL_PyTestCore/expected_results
Message-ID: <200702011954.l11JsDSx003497@sheep.berlios.de>

Author: dorionc
Date: 2007-02-01 20:54:13 +0100 (Thu, 01 Feb 2007)
New Revision: 6624

Modified:
   trunk/python_modules/plearn/pytest/.pytest/PL_PyTestCore/expected_results/RUN.log
Log:
New test results (renamed meld to diff)

Modified: trunk/python_modules/plearn/pytest/.pytest/PL_PyTestCore/expected_results/RUN.log
===================================================================
--- trunk/python_modules/plearn/pytest/.pytest/PL_PyTestCore/expected_results/RUN.log	2007-02-01 19:38:22 UTC (rev 6623)
+++ trunk/python_modules/plearn/pytest/.pytest/PL_PyTestCore/expected_results/RUN.log	2007-02-01 19:54:13 UTC (rev 6624)
@@ -19,6 +19,10 @@
 #
 
 #
+#  pytest diff -h
+#
+
+#
 #  pytest disable -h
 #
 
@@ -39,10 +43,6 @@
 #
 
 #
-#  pytest meld -h
-#
-
-#
 #  pytest prune -h
 #
 



From chrish at mail.berlios.de  Thu Feb  1 22:18:10 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Thu, 1 Feb 2007 22:18:10 +0100
Subject: [Plearn-commits] r6625 - trunk/plearn/vmat
Message-ID: <200702012118.l11LIAvk008902@sheep.berlios.de>

Author: chrish
Date: 2007-02-01 22:18:09 +0100 (Thu, 01 Feb 2007)
New Revision: 6625

Modified:
   trunk/plearn/vmat/VMatLanguage.cc
Log:
Readability cleanup to VMatLanguage while bug hunting.
* Remove dead, commented-out code. That's what we have a version control
  system for.
* Use 'const' in int, real, etc. variables that are not meant to change.
* Add whitespace.
* Remove double { usage.


Modified: trunk/plearn/vmat/VMatLanguage.cc
===================================================================
--- trunk/plearn/vmat/VMatLanguage.cc	2007-02-01 19:54:13 UTC (rev 6624)
+++ trunk/plearn/vmat/VMatLanguage.cc	2007-02-01 21:18:09 UTC (rev 6625)
@@ -249,6 +249,9 @@
                   "The output fieldnames produced by the program");
     declareOption(ol, "vmsource", &VMatLanguage::vmsource, OptionBase::learntoption,
                   "The VMat that was set by setSource");
+    // XXX This is a duplicate of the already defined "srcfieldnames" option,
+    // but with OptionBase::learntoption instead of buildoption. Which one is
+    // right?
     declareOption(ol, "srcfieldnames", &VMatLanguage::srcfieldnames, OptionBase::learntoption,
                   "The fieldnames that were set by setSourceFieldNames");
     declareOption(ol, "program", &VMatLanguage::program, OptionBase::learntoption,
@@ -288,63 +291,64 @@
 void VMatLanguage::preprocess(PStream& in, map<string, string>& defines,
                               string& processed_sourcecode, vector<string>& fieldnames)
 {
-    // pout << defines << endl;
     string token;
     size_t spos;
-    map<string,string>::iterator pos;
-    while(in)
+    map<string, string>::iterator pos;
+    while (in)
     {
         in >> token;
-        pos=defines.find(token);
+        pos = defines.find(token);
 
-        // are we sitting on a mapping declaration?
-        if(token[0]=='{')
+        // Are we sitting on a mapping declaration?
+        if (token[0] == '{')
         {
-            //skip mapping to avoid brackets conflicts with fieldcopy macro syntax
-            char car;
-            processed_sourcecode+=token;
-            // if the token is only a part of the mapping...
-            if(token.find("}")==string::npos)
+            // Skip mapping to avoid brackets conflicts with fieldcopy macro
+            // syntax
+            processed_sourcecode += token;
+            // If the token is only a part of the mapping...
+            if (token.find("}") == string::npos)
             {
-                // just eat till the end of the mapping
-                while((car=in.get())!='}' && !in.eof())
-                    processed_sourcecode+=car;
-                processed_sourcecode+="}";
+                char car;
+                // Just eat till the end of the mapping
+                while ((car = in.get()) != '}' && !in.eof())
+                    processed_sourcecode += car;
+                processed_sourcecode += "}";
             }
         }
-        // did we find a fieldName declaration?
-        // format is either :myField or :myField:a:b
-        else if(token[0]==':')
+        // Did we find a fieldName declaration? format is either :myField or
+        // :myField:a:b
+        else if (token[0] == ':')
         {
-            if(isBlank(token.substr(1)))
+            if (isBlank(token.substr(1)))
                 PLERROR("Found a ':' with no fieldname. Do not put a whitespace after the ':'");
-            vector<string> parts=split(token,":");
-            if(parts.size()==3)
+            vector<string> parts = split(token, ":");
+            if (parts.size() == 3)
             {
-                int a=toint(parts[1]);
-                int b=0;
-                // let the chance for the second interval boundary to be a "DEFINE"
-                // this is used with onehot and @myfield.ranges10.nbins
+                int a = toint(parts[1]);
+                int b = 0;
+                // Let the chance for the second interval boundary to be a "DEFINE".
+                // This is used with onehot and @myfield.ranges10.nbins
                 // ie: @myfield.onehot10 :myfieldonehot:0:@myfield.ranges10.nbins
-                if(pl_isnumber(parts[2]))
-                    b=toint(parts[2]);
+                if (pl_isnumber(parts[2]))
+                    b = toint(parts[2]);
                 else
                 {
-                    if(defines.find(parts[2])!=defines.end())
-                        b=toint(defines[parts[2]]);
+                    if (defines.find(parts[2]) != defines.end())
+                        b = toint(defines[parts[2]]);
                     else
-                        PLERROR("found a undefined non-numeric boundary in multifield declaration : '%s'",parts[2].c_str());
+                        PLERROR("found a undefined non-numeric boundary in multifield declaration : '%s'",
+                                parts[2].c_str());
                 }
 
-                for(int i=a;i<=b;i++)
-                    fieldnames.push_back(parts[0]+tostring(i));
+                for (int i = a; i <= b; i++)
+                    fieldnames.push_back(parts[0] + tostring(i));
             }
-            else if (parts.size()==1)
+            else if (parts.size() == 1)
                 fieldnames.push_back(token.substr(1));
             else PLERROR("Strange fieldname format (multiple declaration format is :label:0:10");
         }
         // Did we find a fieldcopy macro?
-        else if(token[0]=='[')
+        else if (token[0] == '[')
         {
             if (token[token.size() - 1] != ']') {
                 // First read until the brackets are closed.
@@ -366,40 +370,51 @@
                 if (code_to_perform)
                     performed_code = parts[2];
 
-                int a=-1,b=-1;
+                int a=-1;
+                int b=-1;
 
-                if(parts[0][0]=='@')
+                if (parts[0][0] == '@')
                 {
-                    for(int i=0;i<srcfieldnames.length();i++)
-                        if(srcfieldnames[i]==astr){a=i;break;}
+                    for (int i = 0; i < srcfieldnames.length(); i++)
+                        if (srcfieldnames[i] == astr)
+                        {
+                            a = i;
+                            break;
+                        }
                 }
-                else if(parts[0][0]=='%')
-                    a=toint(parts[0].substr(1));
+                else if (parts[0][0] == '%')
+                    a = toint(parts[0].substr(1));
                 else if (parts[0] == "END")
                     // Keyword indicating we go till the end.
                     a = srcfieldnames.length() - 1;
-                else PLERROR("fieldcopy macro syntax is : [start:end] EG: [@year:%6]. 'end' must be after 'start'.. OR [field] to copy a single field");
+                else
+                    PLERROR("fieldcopy macro syntax is : [start:end] EG: [@year:%6]. 'end' must be after 'start'.. OR [field] to copy a single field");
 
-                if(parts[1][0]=='@')
+                if (parts[1][0] == '@')
                 {
-                    for(int i=0;i<srcfieldnames.length();i++)
-                        if(srcfieldnames[i]==bstr){b=i;break;}
+                    for (int i = 0; i < srcfieldnames.length(); i++)
+                        if (srcfieldnames[i] == bstr)
+                        {
+                            b = i;
+                            break;
+                        }
                 }
-                else if(parts[1][0]=='%')
-                    b=toint(parts[1].substr(1));
+                else if (parts[1][0] == '%')
+                    b = toint(parts[1].substr(1));
                 else if (parts[1] == "END")
                     // Keyword indicating we go till the end.
                     b = srcfieldnames.length() - 1;
-                else PLERROR("fieldcopy macro syntax is : [start:end] EG: [@year:%6]. 'end' must be after 'start'.. OR [field] to copy a single field");
+                else
+                    PLERROR("fieldcopy macro syntax is : [start:end] EG: [@year:%6]. 'end' must be after 'start'.. OR [field] to copy a single field");
 
-                if(a>b)
+                if (a > b)
                     PLERROR("In copyfield macro, you have specified a start field that is after the end field. Eg : [%10:%5]");
-                if(a==-1)
-                    PLERROR("In copyfield macro, unknown field : '%s'",astr.c_str());
-                if(b==-1)
-                    PLERROR("In copyfield macro, unknown field : '%s'",astr.c_str());
+                if (a == -1)
+                    PLERROR("In copyfield macro, unknown field : '%s'", astr.c_str());
+                if (b == -1)
+                    PLERROR("In copyfield macro, unknown field : '%s'", astr.c_str());
 
-                for(int i=a;i<=b;i++)
+                for (int i = a; i <= b; i++)
                 {
                     processed_sourcecode+=string("%")+tostring(i)+ " ";
                     if (code_to_perform)
@@ -410,7 +425,7 @@
                     fieldnames.push_back(srcfieldnames[i]);
                 }
             }
-            else if(parts.size()==1)
+            else if (parts.size() == 1)
                 // fieldcopy macro type is [field]
             {
                 bool ignore_if_missing = false;
@@ -420,22 +435,28 @@
                     parts[0] = parts[0].substr(0, parts[0].size()-1);
                 }
                 
-                string astr=parts[0].substr(1);
-                int a=-1;
-                if(parts[0][0]=='@')
+                string astr = parts[0].substr(1);
+                int a = -1;
+                if (parts[0][0] == '@')
                 {
-                    for(int i=0;i<srcfieldnames.length();i++)
-                        if(srcfieldnames[i]==astr){a=i;break;}
+                    for (int i = 0;i < srcfieldnames.length(); i++)
+                        if (srcfieldnames[i] == astr)
+                        {
+                            a = i;
+                            break;
+                        }
                 }
-                else if(parts[0][0]=='%')
-                    a=toint(parts[0].substr(1));
-                else PLERROR("fieldcopy macro syntax is : [start:end] EG: [@year:%6]. 'end' must be after 'start'.. OR [field] to copy a single field");
+                else if (parts[0][0] == '%')
+                    a = toint(parts[0].substr(1));
+                else
+                    PLERROR("fieldcopy macro syntax is : [start:end] EG: [@year:%6]. 'end' must be after 'start'.. OR [field] to copy a single field");
+
                 if (a == -1) {
                     if (!ignore_if_missing)
                         PLERROR("In copyfield macro, unknown field :%s",astr.c_str());
                 }
                 else {
-                    processed_sourcecode+=string("%")+tostring(a)+ " ";
+                    processed_sourcecode += string("%") + tostring(a) + " ";
                     if (a >= srcfieldnames.length())
                         PLERROR("In VMatLanguage::preprocess - Asked field number %d, but there "
                                 "are only %d fields available", a, srcfieldnames.length());
@@ -446,81 +467,92 @@
         }
 
         // did we find a comment?
-        else if(token[0]=='#')
+        else if (token[0] == '#')
             skipRestOfLine(in);
 
         // include declaration
-        else if(token=="INCLUDE")
+        else if (token=="INCLUDE")
         {
             in >> token;
-            // Try to be intelligent and find out if the file belongs directly to another .?mat (the case of a
-            // stats file for example) and warn if the file is out of date
+            // Try to be intelligent and find out if the file belongs directly
+            // to another .?mat (the case of a stats file for example) and warn
+            // if the file is out of date
 
-            // Mhhh.. is this still pertinent? This "stats" and "bins" thing is semi-standard I think
+            // Mhhh.. is this still pertinent? This "stats" and "bins" thing is
+            // semi-standard I think
             size_t idx_meta  =  token.find(".metadata");
             size_t idx_stats =  token.find("stats.");
             size_t idx_bins  =  token.find("bins.");
-            if(idx_meta!=string::npos && (idx_stats!=string::npos || idx_bins!=string::npos))
+            if (idx_meta != string::npos && (idx_stats != string::npos || idx_bins != string::npos))
             {
-                string file=token.substr(0,idx_meta);
-                if(getDataSetDate(file) > mtime(token))
-                    PLWARNING("File %s seems out of date with parent matrix %s",token.c_str(),file.c_str());
+                string file = token.substr(0, idx_meta);
+                if (getDataSetDate(file) > mtime(token))
+                    PLWARNING("File %s seems out of date with parent matrix %s",
+                              token.c_str(), file.c_str());
             }
 
             PStream incfile = openFile(token, PStream::raw_ascii, "r");
             // process recursively this included file
             // **POSSIBLE DRAWBACK : defines done in this file will be used in the next recursion level
-            preprocess(incfile,defines, processed_sourcecode,fieldnames);
+            preprocess(incfile, defines, processed_sourcecode, fieldnames);
 	
         }
         // define declaration
-        else if(token=="DEFINE")
+        else if (token == "DEFINE")
         {
             in >> token;
             string str_buf;
             in.getline(str_buf);
             defines[token.c_str()] = str_buf;
         }
-        else if(pos!=defines.end())
+        else if (pos != defines.end())
         {
-            // the token is a macro (define) so we process it recursively until it's stable
-            // (necessary since the define macro can use defines recursively)
-            string oldstr=pos->second,newstr;
-            bool unstable=true;
-            while(unstable)
+            // the token is a macro (define) so we process it recursively until
+            // it's stable (necessary since the define macro can use defines
+            // recursively)
+            string oldstr = pos->second;
+            string newstr;
+            bool unstable = true;
+            while (unstable)
             {
                 PStream strm = openString(oldstr, PStream::raw_ascii);
-                newstr="";
-                preprocess(strm,defines,newstr,fieldnames);
-                if(removeblanks(oldstr)==removeblanks(newstr))
-                    unstable=false;
-                oldstr=newstr;
+                newstr = "";
+                preprocess(strm, defines, newstr, fieldnames);
+                if (removeblanks(oldstr) == removeblanks(newstr))
+                    unstable = false;
+                oldstr = newstr;
             }
-            processed_sourcecode+=newstr + " ";
+            processed_sourcecode += newstr + " ";
         }
-        // did we find a reference to a string value of a VMatrix that has overloaded getStringVal(..) e.g.:StrTableVMatrix
-        // In VPL, you can push on the stack the value of a string according to the string map of a particular column
-        // e.g. : to push value of string "WBush" from field MostSuspectAmericanPresidents, write @MostSuspectsAmericanPresidents."WBush"
-        else if ((token[0]=='@' || token[0]=='%') && token[token.length()-1]=='"' && (spos=token.find(".\""))!=string::npos)
-
+        // Did we find a reference to a string value of a VMatrix that has
+        // overloaded getStringVal(..) e.g.:StrTableVMatrix? In VPL, you can
+        // push on the stack the value of a string according to the string map
+        // of a particular column e.g. : to push value of string "WBush" from
+        // field MostSuspectAmericanPresidents, write
+        // @MostSuspectsAmericanPresidents."WBush"
+        else if ((token[0]=='@' || token[0]=='%') &&
+                 token[token.length()-1]=='"' &&
+                 (spos=token.find(".\""))!=string::npos)
         {
-            string colname=token.substr(1,spos-1);
-            string str=token.substr(spos+2,token.length()-spos-3);
-            // do we have a named field reference?
-            if(token[0]=='@')
+            string colname = token.substr(1, spos - 1);
+            const string str = token.substr(spos + 2, token.length() - spos - 3);
+            // Do we have a named field reference?
+            if (token[0]=='@')
             {
-                pos=defines.find(string("@")+colname);
-                if(pos==defines.end())
+                pos = defines.find(string("@") + colname);
+                if (pos == defines.end())
                     PLERROR("unknown field : '%s'",colname.c_str());
-                colname=pos->second.substr(1);
+                colname = pos->second.substr(1);
             }
-            int colnum=toint(colname);
-            real r=vmsource->getStringVal(colnum,str);
-            if(is_missing(r))
-                PLERROR("String '%s' is not a known string for the field '%s'", str.c_str(), token.c_str());
+            const int colnum = toint(colname);
+            const real r = vmsource->getStringVal(colnum, str);
+            if (is_missing(r))
+                PLERROR("String '%s' is not a known string for the field '%s'",
+                        str.c_str(), token.c_str());
             processed_sourcecode+=tostring(r)+" ";
         }
-        else processed_sourcecode+=token + " ";
+        else
+            processed_sourcecode += token + " ";
     }
 }
 
@@ -868,28 +900,32 @@
 
 void VMatLanguage::run(const Vec& srcvec, const Vec& result, int rowindex) const
 {
-    if(program.length()==0 && sourcecode!="")
+    if (program.length() == 0 && sourcecode != "")
     {
         TVec<string> outnames;
         const_cast<VMatLanguage*>(this)->compileString(sourcecode, outnames);
     }
-    real a,b,c;
-    if(srcvec.length()!=srcfieldnames.length())
+    
+    
+    if (srcvec.length()!=srcfieldnames.length())
         PLERROR("In VMatLanguage::run, srcvec should have length %d, not %d.",srcfieldnames.length(),srcvec.length());
+    
     pstack.resize(0);
     TVec<int>::iterator pptr = program.begin();
-    TVec<int>::iterator pptrend = program.end();
+    const TVec<int>::iterator pptrend = program.end();
     real* pfieldvalues = srcvec.data();
-    while(pptr!=pptrend)
+    real a,b,c;    
+
+    while (pptr != pptrend)
     {
-        int op = *pptr++;
+        const int op = *pptr++;
         switch(op)
         {
         case 0: // insertconstant
             pstack.push(*((float*)pptr++));
             break;
         case 1: // getfieldval
-            // Question: why is the next PLERROR line commented? Is if made
+            // XXX Question: why is the next PLERROR line commented? Is if made
             // obsolete by another bound check earlier in the code? Or is it
             // temporarily disabled? If the former, please *delete* the line,
             // together with this comment. If the latter, please reenable the
@@ -917,12 +953,12 @@
             break;
         case 6: // onehot
         {
-            int nclasses = int(pstack.pop());
-            int index = int(pstack.pop());
-            for(int i=0; i<nclasses; i++)
-                pstack.push(i==index ?1 :0);
+            const int nclasses = int(pstack.pop());
+            const int index = int(pstack.pop());
+            for (int i = 0; i < nclasses; i++)
+                pstack.push(i == index ? 1 : 0);
+            break;
         }
-        break;
         case 7: // +
             b = pstack.pop();
             a = pstack.pop();
@@ -1056,8 +1092,8 @@
             pstack.push(d.year);
             pstack.push(d.month);
             pstack.push(d.day);
+            break;
         }
-        break;
         case 36: //todate
             c = pstack.pop();
             b = pstack.pop();
@@ -1149,32 +1185,26 @@
         }
         case 51: // get
         {
-            {
-                int i = int(pstack.pop());
-                if(i>=0)
-                    pstack.push(pstack[i]);
-                else
-                    pstack.push(pstack.length()+i);
-            }
+            const int i = int(pstack.pop());
+            if (i >= 0)
+                pstack.push(pstack[i]);
+            else
+                pstack.push(pstack.length() + i);
             break;
         }
         case 52: // memput
         {
-            {
-                int i = int(pstack.pop());
-                a = pstack.pop();
-                if(mem.size()<i+1)
-                    mem.resize(i+1);
-                mem[i] = a;
-            }
+            const int i = int(pstack.pop());
+            a = pstack.pop();
+            if (mem.size()<i+1)
+                mem.resize(i+1);
+            mem[i] = a;
             break;
         }
         case 53: // memget
         {
-            {
-                int i = int(pstack.pop());
-                pstack.push(mem[i]);
-            }
+            const int i = int(pstack.pop());
+            pstack.push(mem[i]);
             break;
         }
         case 54: // neg
@@ -1199,7 +1229,7 @@
             break;
         case 59: // nextincal
         {
-            string cal_name = tostring(pstack.pop());
+            const string cal_name = tostring(pstack.pop());
             PDate d = float_to_date(pstack.pop());
             JTime date = d.toJulianDay();
             const Calendar* cal = Calendar::getGlobalCalendar(cal_name);
@@ -1219,7 +1249,7 @@
         }
         case 60: // previncal
         {
-            string cal_name = tostring(pstack.pop());
+            const string cal_name = tostring(pstack.pop());
             PDate d = float_to_date(pstack.pop());
             JTime date = d.toJulianDay();
             const Calendar* cal = Calendar::getGlobalCalendar(cal_name);
@@ -1239,13 +1269,12 @@
         }
         case 61: // gausshot
         {
-            real sigma = pstack.pop();
-            int nclasses = int(pstack.pop());
-            int index = int(pstack.pop());
-            for(int i=0; i<nclasses; i++) {
-                real diff_index = i-index;
-                real value = exp(- diff_index*diff_index / sigma);
-                pstack.push(value);
+            const real sigma = pstack.pop();
+            const int nclasses = int(pstack.pop());
+            const int index = int(pstack.pop());
+            for (int i = 0; i < nclasses; i++) {
+                const real diff_index = i - index;
+                pstack.push(exp(- diff_index*diff_index / sigma));
             }
             break;
         }
@@ -1261,10 +1290,10 @@
         }
         case 64: // varproduct
         {
-            real num_vars_real = pstack.pop();
+            const real num_vars_real = pstack.pop();
             if (num_vars_real <= 0)
                 PLERROR("VMatLanguage: varproduct: num_vars must be a strictly positive number.");
-            int num_vars = (int)num_vars_real;
+            const int num_vars = (int)num_vars_real;
             if (num_vars != num_vars_real)
                 PLERROR("VMatLanguage: varproduct: num_vars must be an integer.");
             TVec<Vec> vars(num_vars);
@@ -1306,8 +1335,8 @@
         }
         case 65: // thermometer
         {
-            int nclasses = int(pstack.pop());
-            int index = int(pstack.pop());
+            const int nclasses = int(pstack.pop());
+            const int index = int(pstack.pop());
             for (int i = 0; i < nclasses; i++)
                 pstack.push(i > index ? 1 : 0);
             
@@ -1318,16 +1347,14 @@
                          tostring(op));
         }
     }
-    // copy to result vec.
-    //for(int i=0;i<pstack.size();i++)
-    //  cout<<pstack[i]<<" ";
-    //cout<<endl;
+
     if (pstack.length() > result.length())
         PLERROR("Parsing VMatLanguage: left with %d too many items on the stack!",
                 pstack.length()-result.length());
     if (pstack.length() < result.length())
         PLERROR("Parsing VMatLanguage: left with %d missing items on the stack!",
                 result.length()-pstack.length());
+
     pstack >> result;
 }
 
@@ -1348,10 +1375,6 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
-    // ### Call deepCopyField on all "pointer-like" fields
-    // ### that you wish to be deepCopied rather than
-    // ### shallow-copied.
-
     deepCopyField(vmsource, copies);
     deepCopyField(srcfieldnames, copies);
     deepCopyField(outputfieldnames, copies);



From lamblin at mail.berlios.de  Thu Feb  1 22:41:08 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Thu, 1 Feb 2007 22:41:08 +0100
Subject: [Plearn-commits] r6626 - trunk/plearn_learners/online
Message-ID: <200702012141.l11Lf8bj010675@sheep.berlios.de>

Author: lamblin
Date: 2007-02-01 22:41:08 +0100 (Thu, 01 Feb 2007)
New Revision: 6626

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
Log:
Fix index bug in partial_costs


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-01 21:18:09 UTC (rev 6625)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-01 21:41:08 UTC (rev 6626)
@@ -62,8 +62,8 @@
     final_cost_has_learning_rate( false ),
     nll_cost_index( -1 ),
     class_cost_index( -1 ),
-    recons_cost_index( -1 ),
-    final_cost_index( -1 )
+    final_cost_index( -1 ),
+    recons_cost_index( -1 )
 
 {
     random_gen = new PRandom();
@@ -691,12 +691,12 @@
 
         // Backward pass
         real cost;
-        partial_costs[ index+1 ]->fprop( layers[ index+1 ]->expectation,
-                                         target, cost );
+        partial_costs[ index ]->fprop( layers[ index+1 ]->expectation,
+                                       target, cost );
 
-        partial_costs[ index+1 ]->bpropUpdate( layers[ index+1 ]->expectation,
-                                               target, cost,
-                                               expectation_gradients[ index+1 ]
+        partial_costs[ index ]->bpropUpdate( layers[ index+1 ]->expectation,
+                                             target, cost,
+                                             expectation_gradients[ index+1 ]
                                              );
 
         layers[ index+1 ]->bpropUpdate( layers[ index+1 ]->activation,
@@ -738,7 +738,7 @@
     fill_one_hot( joint_exp.subVec( layers[ n_layers-2 ]->size, n_classes ),
                   (int) round(target[0]), 0., 1. );
 
-    if( partial_costs && partial_costs[ n_layers-1 ] )
+    if( partial_costs && partial_costs[ n_layers-2 ] )
     {
         // Deterministic forward pass
         classification_module->joint_connection->setAsDownInput(
@@ -754,10 +754,10 @@
 
         // Backward pass
         real cost;
-        partial_costs[ n_layers-1 ]->fprop( layers[ n_layers-1 ]->expectation,
+        partial_costs[ n_layers-2 ]->fprop( layers[ n_layers-1 ]->expectation,
                                             target, cost );
 
-        partial_costs[ n_layers-1 ]->bpropUpdate(
+        partial_costs[ n_layers-2 ]->bpropUpdate(
             layers[ n_layers-1 ]->expectation, target, cost,
             expectation_gradients[ n_layers-1 ] );
 



From chrish at mail.berlios.de  Thu Feb  1 23:09:54 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Thu, 1 Feb 2007 23:09:54 +0100
Subject: [Plearn-commits] r6627 - trunk/plearn_learners/testers
Message-ID: <200702012209.l11M9sFh013482@sheep.berlios.de>

Author: chrish
Date: 2007-02-01 23:09:53 +0100 (Thu, 01 Feb 2007)
New Revision: 6627

Modified:
   trunk/plearn_learners/testers/PTester.cc
Log:
Save testset as a .vmat, not a .psave (because it is one).

Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2007-02-01 21:41:08 UTC (rev 6626)
+++ trunk/plearn_learners/testers/PTester.cc	2007-02-01 22:09:53 UTC (rev 6627)
@@ -589,7 +589,7 @@
                 PP<VecStatsCollector> test_stats = stcol[setnum];
                 const string setname = "test" + tostring(setnum);
                 if (is_splitdir && save_data_sets)
-                    PLearn::save(splitdir / (setname + "_set.psave"), testset);
+                    PLearn::save(splitdir / (setname + "_set.vmat"), testset);
 
                 // QUESTION Why is this done so late? Can't it be moved
                 // somewhere earlier? At least before the save_data_sets?



From dorionc at mail.berlios.de  Fri Feb  2 04:30:24 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Fri, 2 Feb 2007 04:30:24 +0100
Subject: [Plearn-commits] r6628 - trunk/plearn/misc
Message-ID: <200702020330.l123UOqE020212@sheep.berlios.de>

Author: dorionc
Date: 2007-02-02 04:30:24 +0100 (Fri, 02 Feb 2007)
New Revision: 6628

Modified:
   trunk/plearn/misc/Calendar.cc
   trunk/plearn/misc/Calendar.h
Log:
Bug fix in getLastDayOfMonth


Modified: trunk/plearn/misc/Calendar.cc
===================================================================
--- trunk/plearn/misc/Calendar.cc	2007-02-01 22:09:53 UTC (rev 6627)
+++ trunk/plearn/misc/Calendar.cc	2007-02-02 03:30:24 UTC (rev 6628)
@@ -256,10 +256,12 @@
     CTime ctime_ldom = getCalendarTime(jtime_ldom);
 
     // Was first day of the following month returned?
-    if ( timestamps_[ctime_ldom] > jtime_ldom )
+    if ( ctime_ldom > 0 && timestamps_[ctime_ldom] > jtime_ldom )
+    {
         ctime_ldom--;
+        PLASSERT( timestamps_[ctime_ldom] <= jtime_ldom );
+    }
 
-    PLASSERT( ctime_ldom >= 0 && timestamps_[ctime_ldom] <= jtime_ldom );
     return ctime_ldom;
 }
 

Modified: trunk/plearn/misc/Calendar.h
===================================================================
--- trunk/plearn/misc/Calendar.h	2007-02-01 22:09:53 UTC (rev 6627)
+++ trunk/plearn/misc/Calendar.h	2007-02-02 03:30:24 UTC (rev 6628)
@@ -230,8 +230,9 @@
     CTime getCalendarTime(JTime julian_time, bool use_lower_bound = true) const;
 
     /**
-     * Similar to a call to getCalendarTime but ensures that the ctime returned
-     * actually is in the same month than the argument provided
+     * Given a PDate D, returns the last calendar day in the same month than
+     * D. Note that the first or last date of the calendar are returned
+     * whenever the given month is out from the calendar boundaries...
      */
     CTime getLastDayOfMonth(const PDate& day_of_the_month) const;
     



From dorionc at mail.berlios.de  Fri Feb  2 15:20:32 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Fri, 2 Feb 2007 15:20:32 +0100
Subject: [Plearn-commits] r6629 - trunk/python_modules/plearn/pytest
Message-ID: <200702021420.l12EKW1t008342@sheep.berlios.de>

Author: dorionc
Date: 2007-02-02 15:20:31 +0100 (Fri, 02 Feb 2007)
New Revision: 6629

Modified:
   trunk/python_modules/plearn/pytest/modes.py
Log:


Modified: trunk/python_modules/plearn/pytest/modes.py
===================================================================
--- trunk/python_modules/plearn/pytest/modes.py	2007-02-02 03:30:24 UTC (rev 6628)
+++ trunk/python_modules/plearn/pytest/modes.py	2007-02-02 14:20:31 UTC (rev 6629)
@@ -324,7 +324,7 @@
 
         expected = test.resultsDirectory(EXPECTED_RESULTS)
         run_results = test.resultsDirectory(RUN_RESULTS)
-        os.system("kdiff3 %s %s"%(expected, run_results))
+        os.system("kdiff3 %s %s >& /dev/null"%(expected, run_results))
         #os.system("meld %s %s"%(expected, run_results))
         
 class prune( PyTestMode ):



From lamblin at mail.berlios.de  Sat Feb  3 03:02:09 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Sat, 3 Feb 2007 03:02:09 +0100
Subject: [Plearn-commits] r6630 - trunk/plearn_learners/online
Message-ID: <200702030202.l13229wU030426@sheep.berlios.de>

Author: lamblin
Date: 2007-02-03 03:02:08 +0100 (Sat, 03 Feb 2007)
New Revision: 6630

Modified:
   trunk/plearn_learners/online/RBMTruncExpLayer.cc
Log:
Fix bad formula for gradient bprop
Fix several numerical instabilities:
    - in generateSample(), by using logadd
    - in generateSample(), fprop(), computeExpectation() and
bpropUpdate() by approximating the transfer function by its Taylor
expansion around 0.



Modified: trunk/plearn_learners/online/RBMTruncExpLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMTruncExpLayer.cc	2007-02-02 14:20:31 UTC (rev 6629)
+++ trunk/plearn_learners/online/RBMTruncExpLayer.cc	2007-02-03 02:02:08 UTC (rev 6630)
@@ -102,7 +102,13 @@
     {
         real s = random_gen->uniform_sample();
         real a_i = activation[i];
-        sample[i] = - pl_log( 1. - s*( 1 - exp(-a_i) ) ) / a_i;
+
+        // Polynomial approximation to avoid numerical instability if a ~ 0
+        // C^{-1}(s) ~ s + (-s + s^2)/2 * a + O(a^2)
+        if( fabs( a_i ) <= 1e-5 )
+            sample[i] = s + a_i*( s*(-1 + s)/2 );
+        else
+            sample[i] = - logadd( pl_log( 1-s ), pl_log(s) - a_i );
     }
 }
 
@@ -118,7 +124,13 @@
     for( int i=0 ; i<size ; i++ )
     {
         real a_i = activation[i];
-        expectation[i] = 1/(1-exp(a_i)) + 1/a_i;
+
+        // Polynomial approximation to avoid numerical instability
+        // f(a) = 1/2 - a/12 + a^3/720 + O(a^5)
+        if( fabs( a_i ) <= 0.01 )
+            expectation[i] = 0.5 - a_i*(1./12. + a_i*a_i/720.);
+        else
+            expectation[i] = 1/(1-exp(a_i)) + 1/a_i;
     }
 
     expectation_is_up_to_date = true;
@@ -132,8 +144,15 @@
 
     for( int i=0 ; i<size ; i++ )
     {
-        real x_i = input[i] + bias[i];
-        output[i] = 1/(1-exp(x_i)) + 1/x_i;
+        real a_i = input[i] + bias[i];
+
+        // Polynomial approximation to avoid numerical instability
+        // f(a) = 1/(1-exp(a) + 1/a
+        // f(a) = 1/2 - a/12 + a^3/720 + O(a^5)
+        if( fabs( a_i ) <= 0.01 )
+            output[i] = 0.5 - a_i*(1./12. +a_i*a_i/720.);
+        else
+            output[i] = 1/(1-exp(a_i)) + 1/a_i;
     }
 }
 
@@ -150,9 +169,21 @@
 
     for( int i=0 ; i<size ; i++ )
     {
-        real a_i = input[i];
-        real ea_i = exp( a_i );
-        input_gradient[i] = ea_i/( (1 - ea_i) * (1 - ea_i) ) + 1/(a_i * a_i);
+        real a_i = input[i] + bias[i];
+
+        // Polynomial approximation to avoid numerical instability
+        // df/da = -1/12 + a^2/240 + O(a^4)
+        if( fabs( a_i ) <= 0.01 )
+        {
+            input_gradient[i] = output_gradient[i] * (
+                -1./12. + a_i * a_i / 240. );
+        }
+        else
+        {
+            real ea_i = exp( a_i );
+            input_gradient[i] = output_gradient[i] * (
+                ea_i/( (1 - ea_i) * (1 - ea_i) ) + 1/(a_i * a_i) );
+        }
     }
 
     if( momentum == 0. )



From lamblin at mail.berlios.de  Sat Feb  3 03:45:34 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Sat, 3 Feb 2007 03:45:34 +0100
Subject: [Plearn-commits] r6631 - trunk/plearn_learners/online
Message-ID: <200702030245.l132jYTI032655@sheep.berlios.de>

Author: lamblin
Date: 2007-02-03 03:45:34 +0100 (Sat, 03 Feb 2007)
New Revision: 6631

Modified:
   trunk/plearn_learners/online/RBMTruncExpLayer.cc
Log:
Fix in last commit. Thanks Hugo for pointing it to me :)


Modified: trunk/plearn_learners/online/RBMTruncExpLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMTruncExpLayer.cc	2007-02-03 02:02:08 UTC (rev 6630)
+++ trunk/plearn_learners/online/RBMTruncExpLayer.cc	2007-02-03 02:45:34 UTC (rev 6631)
@@ -108,7 +108,7 @@
         if( fabs( a_i ) <= 1e-5 )
             sample[i] = s + a_i*( s*(-1 + s)/2 );
         else
-            sample[i] = - logadd( pl_log( 1-s ), pl_log(s) - a_i );
+            sample[i] = - logadd( pl_log( 1-s ), pl_log(s) - a_i ) / a_i;
     }
 }
 



From tihocan at mail.berlios.de  Tue Feb  6 17:09:21 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 6 Feb 2007 17:09:21 +0100
Subject: [Plearn-commits] r6632 - trunk/plearn/vmat
Message-ID: <200702061609.l16G9LGc025223@sheep.berlios.de>

Author: tihocan
Date: 2007-02-06 17:09:21 +0100 (Tue, 06 Feb 2007)
New Revision: 6632

Modified:
   trunk/plearn/vmat/VMatLanguage.cc
Log:
It seems 'srcfieldnames' should be a learnt option, since usually a VMatLanguage object will associated to a VMat that will provide it with the field names (just like 'vmsource' is a learnt option)

Modified: trunk/plearn/vmat/VMatLanguage.cc
===================================================================
--- trunk/plearn/vmat/VMatLanguage.cc	2007-02-03 02:45:34 UTC (rev 6631)
+++ trunk/plearn/vmat/VMatLanguage.cc	2007-02-06 16:09:21 UTC (rev 6632)
@@ -243,15 +243,10 @@
 {
     declareOption(ol, "sourcecode", &VMatLanguage::sourcecode, OptionBase::buildoption,
                   "The VPL sourcecode of the program.");
-    declareOption(ol, "srcfieldnames", &VMatLanguage::srcfieldnames, OptionBase::buildoption,
-                  "The fieldnames that were set by setSourceFieldNames");
     declareOption(ol, "outputfieldnames", &VMatLanguage::outputfieldnames, OptionBase::learntoption,
                   "The output fieldnames produced by the program");
     declareOption(ol, "vmsource", &VMatLanguage::vmsource, OptionBase::learntoption,
                   "The VMat that was set by setSource");
-    // XXX This is a duplicate of the already defined "srcfieldnames" option,
-    // but with OptionBase::learntoption instead of buildoption. Which one is
-    // right?
     declareOption(ol, "srcfieldnames", &VMatLanguage::srcfieldnames, OptionBase::learntoption,
                   "The fieldnames that were set by setSourceFieldNames");
     declareOption(ol, "program", &VMatLanguage::program, OptionBase::learntoption,



From lamblin at mail.berlios.de  Tue Feb  6 21:26:36 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 6 Feb 2007 21:26:36 +0100
Subject: [Plearn-commits] r6633 - trunk/scripts/Skeletons
Message-ID: <200702062026.l16KQaLr022189@sheep.berlios.de>

Author: lamblin
Date: 2007-02-06 21:26:35 +0100 (Tue, 06 Feb 2007)
New Revision: 6633

Modified:
   trunk/scripts/Skeletons/CostModule.cc
Log:
Small comment fix.


Modified: trunk/scripts/Skeletons/CostModule.cc
===================================================================
--- trunk/scripts/Skeletons/CostModule.cc	2007-02-06 16:09:21 UTC (rev 6632)
+++ trunk/scripts/Skeletons/CostModule.cc	2007-02-06 20:26:35 UTC (rev 6633)
@@ -102,9 +102,9 @@
 }
 */
 
-/////////////////
-// bpropUpdate //
-/////////////////
+//////////////////
+// bbpropUpdate //
+//////////////////
 /* THIS METHOD IS OPTIONAL
 void DERIVEDCLASS::bbpropUpdate(const Vec& input, const Vec& target, real cost,
                                 Vec& input_gradient, Vec& input_diag_hessian)



From lamblin at mail.berlios.de  Tue Feb  6 21:42:41 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 6 Feb 2007 21:42:41 +0100
Subject: [Plearn-commits] r6634 - trunk/plearn_learners/online
Message-ID: <200702062042.l16KgfHX024134@sheep.berlios.de>

Author: lamblin
Date: 2007-02-06 21:42:40 +0100 (Tue, 06 Feb 2007)
New Revision: 6634

Modified:
   trunk/plearn_learners/online/GradNNetLayerModule.cc
Log:
Resize output in fprop()


Modified: trunk/plearn_learners/online/GradNNetLayerModule.cc
===================================================================
--- trunk/plearn_learners/online/GradNNetLayerModule.cc	2007-02-06 20:26:35 UTC (rev 6633)
+++ trunk/plearn_learners/online/GradNNetLayerModule.cc	2007-02-06 20:42:40 UTC (rev 6634)
@@ -71,6 +71,7 @@
 void GradNNetLayerModule::fprop(const Vec& input, Vec& output) const
 {
     int in_size = input.size();
+    output.resize( output_size );
 
     // size check
     if( in_size != input_size )



From lamblin at mail.berlios.de  Tue Feb  6 21:46:18 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 6 Feb 2007 21:46:18 +0100
Subject: [Plearn-commits] r6635 - in trunk: commands plearn_learners/online
Message-ID: <200702062046.l16KkI2a024375@sheep.berlios.de>

Author: lamblin
Date: 2007-02-06 21:46:17 +0100 (Tue, 06 Feb 2007)
New Revision: 6635

Added:
   trunk/plearn_learners/online/ClassErrorCostModule.cc
   trunk/plearn_learners/online/ClassErrorCostModule.h
Modified:
   trunk/commands/plearn_noblas_inc.h
   trunk/plearn_learners/online/NLLCostModule.cc
Log:
Add new cost module, computing multiclass classification error


Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2007-02-06 20:42:40 UTC (rev 6634)
+++ trunk/commands/plearn_noblas_inc.h	2007-02-06 20:46:17 UTC (rev 6635)
@@ -185,6 +185,7 @@
 
 // Online
 #include <plearn_learners/online/BackConvolution2DModule.h>
+#include <plearn_learners/online/ClassErrorCostModule.h>
 #include <plearn_learners/online/CombiningCostsModule.h>
 #include <plearn_learners/online/Convolution2DModule.h>
 #include <plearn_learners/online/CostModule.h>

Added: trunk/plearn_learners/online/ClassErrorCostModule.cc
===================================================================
--- trunk/plearn_learners/online/ClassErrorCostModule.cc	2007-02-06 20:42:40 UTC (rev 6634)
+++ trunk/plearn_learners/online/ClassErrorCostModule.cc	2007-02-06 20:46:17 UTC (rev 6635)
@@ -0,0 +1,164 @@
+// -*- C++ -*-
+
+// ClassErrorCostModule.cc
+//
+// Copyright (C) 2007 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file ClassErrorCostModule.cc */
+
+
+
+#include "ClassErrorCostModule.h"
+#include <plearn/math/TMat_maths.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    ClassErrorCostModule,
+    "Multiclass classification error",
+    "If input_size > 1, outputs 0 if target == argmax(input), 1 else\n"
+    "If input_size == 1, outputs 0 if target is the closest integer to\n"
+    "input[0], 1 else.\n"
+    "There is no gradient to compute (it returns an error if you try), so if\n"
+    "you use this module inside a CombiningCostsModule, put its weight to 0.\n"
+    );
+
+ClassErrorCostModule::ClassErrorCostModule()
+{
+    output_size = 1;
+    target_size = 1;
+}
+
+void ClassErrorCostModule::declareOptions(OptionList& ol)
+{
+    // declareOption(ol, "myoption", &ClassErrorCostModule::myoption,
+    //               OptionBase::buildoption,
+    //               "Help text describing this option");
+    // ...
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void ClassErrorCostModule::build_()
+{
+    PLASSERT( output_size == 1 );
+    PLASSERT( target_size == 1 );
+}
+
+void ClassErrorCostModule::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void ClassErrorCostModule::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // deepCopyField(trainvec, copies);
+}
+
+///////////
+// fprop //
+///////////
+void ClassErrorCostModule::fprop(const Vec& input, const Vec& target,
+                                 Vec& cost) const
+{
+    cost.resize( output_size );
+    fprop( input, target, cost[0] );
+}
+
+void ClassErrorCostModule::fprop(const Vec& input, const Vec& target,
+                                 real& cost) const
+{
+    PLASSERT( input.size() == input_size );
+    PLASSERT( target.size() == target_size );
+
+    if( input_size == 1 ) // is target[0] the closest integer to input[0]?
+        cost = ( round(input[0]) == round(target[0]) ) ? 0. : 1.;
+    else // is target[0] equals to argmax(input)?
+        cost = ( argmax(input) == int(round(target[0])) ) ? 0. : 1.;
+}
+
+/////////////////
+// bpropUpdate //
+/////////////////
+/*
+void ClassErrorCostModule::bpropUpdate(const Vec& input, const Vec& target,
+                                       real cost,
+                                       Vec& input_gradient)
+{
+}
+*/
+
+////////////
+// forget //
+////////////
+void ClassErrorCostModule::forget()
+{
+}
+
+//////////
+// name //
+//////////
+TVec<string> ClassErrorCostModule::name()
+{
+    return TVec<string>(1, "class_error");
+}
+
+//////////////////////
+// bpropDoesNothing //
+//////////////////////
+bool ClassErrorCostModule::bpropDoesNothing()
+{
+    return true;
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/ClassErrorCostModule.h
===================================================================
--- trunk/plearn_learners/online/ClassErrorCostModule.h	2007-02-06 20:42:40 UTC (rev 6634)
+++ trunk/plearn_learners/online/ClassErrorCostModule.h	2007-02-06 20:46:17 UTC (rev 6635)
@@ -0,0 +1,160 @@
+// -*- C++ -*-
+
+// ClassErrorCostModule.h
+//
+// Copyright (C) 2007 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file ClassErrorCostModule.h */
+
+
+#ifndef ClassErrorCostModule_INC
+#define ClassErrorCostModule_INC
+
+#include <plearn_learners/online/CostModule.h>
+
+namespace PLearn {
+
+/**
+ * Multiclass classification error.
+ * If input_size > 1, outputs 0 if target == argmax(input), 1 else
+ * If input_size == 1, outputs 0 if target is the closest integer to
+ * input[0], 1 else.
+ * There is no gradient to compute (it returns an error if you try), so if you
+ * use this module inside a CombiningCostsModule, put its weight to 0.
+ */
+class ClassErrorCostModule : public CostModule
+{
+    typedef CostModule inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    ClassErrorCostModule();
+
+    // Your other public member functions go here
+
+    //! Given the input and the target, compute a vector of costs
+    //! (possibly resize it appropriately)
+    virtual void fprop(const Vec& input, const Vec& target, Vec& cost) const;
+
+    //! Given the input and the target, compute only the first cost
+    //! (of which we will compute the gradient)
+    virtual void fprop(const Vec& input, const Vec& target, real& cost) const;
+
+    /* Default implementation in super class raises a PLERROR
+    //! No differentiable, so no gradient to backprop!
+    virtual void bpropUpdate(const Vec& input, const Vec& target, real cost,
+                             Vec& input_gradient);
+    */
+
+    /* Optional
+       N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+       RAISES A PLERROR.
+    //! No differentiable, so no gradient to backprop!
+    virtual void bbpropUpdate(const Vec& input, const Vec& target, real cost,
+                              Vec& input_gradient, Vec& input_diag_hessian);
+    */
+
+    //! Reset the parameters to the state they would be BEFORE starting
+    //! training.  Note that this method is necessarily called from
+    //! build().
+    virtual void forget();
+
+
+    //! In case bpropUpdate does not do anything, make it known
+    virtual bool bpropDoesNothing();
+
+    //! Indicates the name of the computed costs
+    virtual TVec<string> name();
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(ClassErrorCostModule);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(ClassErrorCostModule);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Modified: trunk/plearn_learners/online/NLLCostModule.cc
===================================================================
--- trunk/plearn_learners/online/NLLCostModule.cc	2007-02-06 20:42:40 UTC (rev 6634)
+++ trunk/plearn_learners/online/NLLCostModule.cc	2007-02-06 20:46:17 UTC (rev 6635)
@@ -102,9 +102,9 @@
     input_gradient.resize( input_size );
 
     int the_target = (int) round( target[0] );
-    input_gradient.clear();
     // input_gradient[ i ] = 0 if i != t,
     // input_gradient[ t ] = -1/x[t]
+    input_gradient.clear();
     input_gradient[ the_target ] = - 1. / input[ the_target ];
 }
 
@@ -117,10 +117,10 @@
 
     int the_target = (int) round( target[0] );
     real input_gradient_t = input_gradient[ the_target ];
+    // input_diag_hessian[ i ] = 0 if i!=t
+    // input_diag_hessian[ t ] = 1/(x[t])^2
     input_diag_hessian.resize( input_size );
     input_diag_hessian.clear();
-    // input_diag_hessian[ i ] = 0 if i!=t
-    // input_diag_hessian[ t ] = 1/(x[t])^2
     input_diag_hessian[ the_target ] = input_gradient_t * input_gradient_t;
 }
 



From lamblin at mail.berlios.de  Wed Feb  7 22:08:51 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 7 Feb 2007 22:08:51 +0100
Subject: [Plearn-commits] r6636 - trunk/plearn_learners/online
Message-ID: <200702072108.l17L8pwE000391@sheep.berlios.de>

Author: lamblin
Date: 2007-02-07 22:08:50 +0100 (Wed, 07 Feb 2007)
New Revision: 6636

Modified:
   trunk/plearn_learners/online/GradNNetLayerModule.cc
   trunk/plearn_learners/online/GradNNetLayerModule.h
Log:
Don't copy the input at each fprop() or bpropUpdate() anymore
Does the updates due to weight decay in the same loop, in bpropUpdate()


Modified: trunk/plearn_learners/online/GradNNetLayerModule.cc
===================================================================
--- trunk/plearn_learners/online/GradNNetLayerModule.cc	2007-02-06 20:46:17 UTC (rev 6635)
+++ trunk/plearn_learners/online/GradNNetLayerModule.cc	2007-02-07 21:08:50 UTC (rev 6636)
@@ -48,18 +48,19 @@
 
 PLEARN_IMPLEMENT_OBJECT(
     GradNNetLayerModule,
-    "Neural Network layer, using stochastic gradient to update neuron weights",
-    "       Output = weights * (1,Input)\n"
-    "Weights are updated by online gradient with learning rate possibly decreasing\n"
-    "in 1/(1 + n_updates_done_up_to_now * decrease_constant).\n"
-    "An L1 and/or L2 regularization penalty can be added to push weights to 0.\n"
+    "Affine transformation module, with stochastic gradient descent updates",
+    "Neural Network layer, using stochastic gradient to update neuron weights\n"
+    "       Output = weights * Input + bias\n"
+    "Weights and bias are updated by online gradient descent, with learning\n"
+    "rate possibly decreasing in 1/(1 + n_updates_done * decrease_constant).\n"
+    "An L1 and L2 regularization penalty can be added to push weights to 0.\n"
     "Weights can be initialized to 0, to a given initial matrix, or randomly\n"
     "from a uniform distribution.\n"
     );
 
 GradNNetLayerModule::GradNNetLayerModule():
     start_learning_rate( .001 ),
-    decrease_constant( 0 ),
+    decrease_constant( 0. ),
     init_weights_random_scale( 1. ),
     L1_penalty_factor( 0. ),
     L2_penalty_factor( 0. ),
@@ -70,246 +71,178 @@
 // Applies linear transformation
 void GradNNetLayerModule::fprop(const Vec& input, Vec& output) const
 {
-    int in_size = input.size();
+    PLASSERT_MSG( input.size() == input_size,
+                  "input.size() should be equal to this->input_size" );
+
     output.resize( output_size );
 
-    // size check
-    if( in_size != input_size )
-    {
-        PLERROR("GradNNetLayerModule::fprop: 'input.size()' should be equal\n"
-                " to 'input_size' (%i != %i)\n", in_size, input_size);
-    }
-
-    bias_input.subVec( 1, input_size ) << input;
-
-    product( output, weights, bias_input );
-
+    for( int i=0 ; i<output_size ; i++ )
+        output[i] = dot( weights(i), input ) + bias[i];
 }
 
+// We are not using blas routines anymore, because we would iterate several
+// times over the weight matrix.
 void GradNNetLayerModule::bpropUpdate(const Vec& input, const Vec& output,
                                       const Vec& output_gradient)
 {
-    int in_size = input.size();
-    int out_size = output.size();
-    int og_size = output_gradient.size();
+    PLASSERT_MSG( input.size() == input_size,
+                  "input.size() should be equal to this->input_size" );
+    PLASSERT_MSG( output.size() == output_size,
+                  "output.size() should be equal to this->output_size" );
+    PLASSERT_MSG( output_gradient.size() == output_size,
+                  "output_gradient.size() should be equal to this->output_size"
+                );
 
-    // size check
-    if( in_size != input_size )
+    learning_rate = start_learning_rate / (1+decrease_constant*step_number);
+
+    for( int i=0; i<output_size; i++ )
     {
-        PLERROR("GradNNetLayerModule::bpropUpdate: 'input.size()' should be"
-                " equal\n"
-                " to 'input_size' (%i != %i)\n", in_size, input_size);
-    }
-    if( out_size != output_size )
-    {
-        PLERROR("GradNNetLayerModule::bpropUpdate: 'output.size()' should be"
-                " equal\n"
-                " to 'output_size' (%i != %i)\n", out_size, output_size);
-    }
-    if( og_size != output_size )
-    {
-        PLERROR("GradNNetLayerModule::bpropUpdate: 'output_gradient.size()'"
-                " should\n"
-                " be equal to 'output_size' (%i != %i)\n",
-                og_size, output_size);
-    }
+        real og_i = output_gradient[i];
+        real* w_ = weights[i];
 
-    learning_rate = start_learning_rate / ( 1+decrease_constant*step_number);
+        real delta_L1 = learning_rate * L1_penalty_factor;
+        real delta_L2 = learning_rate * L2_penalty_factor;
+        if( delta_L2 > 1 )
+            PLWARNING("GradNNetLayerModule::bpropUpdate:\n"
+                      "learning rate = %f is too large!\n", learning_rate);
 
-    bias_input.subVec( 1,input_size ) << input;
+        bias[i] -= learning_rate * og_i;
 
-    externalProductScaleAcc( weights, output_gradient, bias_input,
-                             -learning_rate );
-
-    if (L1_penalty_factor!=0)
-    {
-        real delta = learning_rate * L1_penalty_factor;
-        for (int i=0;i<output_size;i++)
+        for( int j=0; j<input_size; j++ )
         {
-            real* Wi = weights[i]+1; // don't apply penalty on bias
-            for (int j=0;j<input_size;j++)
+            w_[j] -= learning_rate * input[j] * og_i;
+
+            if( delta_L1 > 0. )
             {
-                real Wij =  Wi[j];
-                if (Wij>delta)
-                    Wi[j] -=delta;
-                else if (Wij<-delta)
-                    Wi[j] +=delta;
+                if( w_[j] > delta_L1 )
+                    w_[j] -= delta_L1;
+                else if( w_[j] < -delta_L1 )
+                    w_[j] += delta_L1;
                 else
-                    Wi[j]=0;
+                    w_[j] = 0.;
             }
+
+            if( delta_L2 > 0. )
+                w_[j] *= (1 - delta_L2);
         }
     }
-    if (L2_penalty_factor!=0)
-    {
-        real delta = learning_rate*L2_penalty_factor;
-        if (delta>1)
-            PLWARNING("GradNNetLayerModule::bpropUpdate: learning rate = %f is too large!",learning_rate);
-        weights.subMatColumns(1,input_size) *= 1 - delta; // no weight decay on the bias
-    }
-
     step_number++;
-
 }
 
 
 // Simply updates and propagates back gradient
-// PA - the original version of this function propagated the error to the inputs,
-// then called the above bpropUpdate() - this proved inefficient as the weight
-// matrix had to be iterated over twice.
-// However, since we're not using blas anymore, the speedup is only when
-// compiled in optimized mode (ie debug is much slower).
 void GradNNetLayerModule::bpropUpdate(const Vec& input, const Vec& output,
                                       Vec& input_gradient,
                                       const Vec& output_gradient)
 {
+    PLASSERT_MSG( input.size() == input_size,
+                  "input.size() should be equal to this->input_size" );
+    PLASSERT_MSG( output.size() == output_size,
+                  "output.size() should be equal to this->output_size" );
+    PLASSERT_MSG( output_gradient.size() == output_size,
+                  "output_gradient.size() should be equal to this->output_size"
+                );
 
-#ifdef BOUNDCHECK
-    int in_size = input.size();
-    int out_size = output.size();
-    int og_size = output_gradient.size();
-
-    // size check
-    if( in_size != input_size )
-    {
-        PLERROR("GradNNetLayerModule::bpropUpdate: 'input.size()' should be"
-                " equal\n"
-                " to 'input_size' (%i != %i)\n", in_size, input_size);
-    }
-    if( out_size != output_size )
-    {
-        PLERROR("GradNNetLayerModule::bpropUpdate: 'output.size()' should be"
-                " equal\n"
-                " to 'output_size' (%i != %i)\n", out_size, output_size);
-    }
-    if( og_size != output_size )
-    {
-        PLERROR("GradNNetLayerModule::bpropUpdate: 'output_gradient.size()'"
-                " should\n"
-                " be equal to 'output_size' (%i != %i)\n",
-                og_size, output_size);
-    }
-#endif
-
     input_gradient.resize( input_size );
     input_gradient.clear();
 
-    learning_rate = start_learning_rate / ( 1+decrease_constant*step_number);
+    learning_rate = start_learning_rate / (1+decrease_constant*step_number);
 
-    bias_input.subVec( 1,input_size ) << input;
-
-    for(int i=0; i<output_size; i++)  {
-
+    for( int i=0; i<output_size; i++ )
+    {
         real og_i = output_gradient[i];
         real* w_ = weights[i];
 
-        w_[0] -= learning_rate * og_i; // * bias_input[0], ie 1
-        for(int j=0, jj=1; j<input_size; j++, jj++) {
-            input_gradient[j] += w_[jj] * og_i;
-            w_[jj] -= learning_rate * bias_input[jj] * og_i;
-        }
-    }
+        real delta_L1 = learning_rate * L1_penalty_factor;
+        real delta_L2 = learning_rate * L2_penalty_factor;
+        if( delta_L2 > 1 )
+            PLWARNING("GradNNetLayerModule::bpropUpdate:\n"
+                      "learning rate = %f is too large!\n", learning_rate);
 
-    // TODO When weight decays, combine penalties using similar loop as above!
+        bias[i] -= learning_rate * og_i;
 
-    if (L1_penalty_factor!=0)
-    {
-        real delta = learning_rate * L1_penalty_factor;
-        for (int i=0;i<output_size;i++)
+        for( int j=0; j<input_size; j++ )
         {
-            real* Wi = weights[i]+1; // don't apply penalty on bias
-            for (int j=0;j<input_size;j++)
+            input_gradient[j] += w_[j] * og_i;
+            w_[j] -= learning_rate * input[j] * og_i;
+
+            if( delta_L1 > 0. )
             {
-                real Wij =  Wi[j];
-                if (Wij>delta)
-                    Wi[j] -=delta;
-                else if (Wij<-delta)
-                    Wi[j] +=delta;
+                if( w_[j] > delta_L1 )
+                    w_[j] -= delta_L1;
+                else if( w_[j] < -delta_L1 )
+                    w_[j] += delta_L1;
                 else
-                    Wi[j]=0;
+                    w_[j] = 0.;
             }
+
+            if( delta_L2 > 0. )
+                w_[j] *= (1 - delta_L2);
         }
     }
-    if (L2_penalty_factor!=0)
-    {
-        real delta = learning_rate*L2_penalty_factor;
-        if (delta>1)
-            PLWARNING("GradNNetLayerModule::bpropUpdate: learning rate = %f is too large!",learning_rate);
-        weights.subMatColumns(1,input_size) *= 1 - delta; // no weight decay on the bias
-    }
-
     step_number++;
-
-
-
-    ///***OLD VERSION***
-    // compute input_gradient from initial weights
-    //input_gradient.resize( input_size );
-    //transposeProduct( input_gradient,
-    //                  weights.subMatColumns(1,input_size), output_gradient );
-    // do the update (and size check)
-    //bpropUpdate( input, output, output_gradient );
-    ///***OLD VERSION - END***
-
 }
 
-
-
 // Update
 void GradNNetLayerModule::bbpropUpdate(const Vec& input, const Vec& output,
                                        const Vec& output_gradient,
                                        const Vec& output_diag_hessian)
 {
-    int odh_size = output_diag_hessian.size();
-    if( odh_size != output_size )
-    {
-        PLERROR("GradNNetLayerModule::bbpropUpdate:"
-                " 'output_diag_hessian.size()'\n"
-                " should be equal to 'output_size' (%i != %i)\n",
-                odh_size, output_size);
-    }
-
+    PLASSERT_MSG( output_diag_hessian.size() == output_size,
+                  "output_diag_hessian.size() should be equal to"
+                  " this->output_size" );
     bpropUpdate( input, output, output_gradient );
-
 }
 
+/* This implementation is incorrect. Let the PLERROR defined in parent version
 // Propagates back output_gradient and output_diag_hessian
 void GradNNetLayerModule::bbpropUpdate(const Vec& input, const Vec& output,
-                              Vec&  input_gradient,
-                              const Vec& output_gradient,
-                              Vec&  input_diag_hessian,
-                              const Vec& output_diag_hessian)
+                                       Vec&  input_gradient,
+                                       const Vec& output_gradient,
+                                       Vec&  input_diag_hessian,
+                                       const Vec& output_diag_hessian)
 {
     bpropUpdate( input, output, input_gradient, output_gradient );
 }
+*/
 
-
-// Nothing to forget
+// Forget the bias and reinitialize the weights
 void GradNNetLayerModule::forget()
 {
-    weights.resize( output_size, 1+input_size );
+    bias.resize( output_size );
+    if( init_bias.size() > 0 )
+    {
+        if( init_bias.size() != output_size )
+            PLERROR( "init_bias (%d) should have length equal to output_size (%d)",
+                     init_bias.size(), output_size );
+        bias << init_bias;
+    }
+    else
+        bias.clear();
 
-    if( init_weights.size() !=0 )
+    weights.resize( output_size, input_size );
+    if( init_weights.size() > 0 )
+    {
+        if( weights.length() != output_size || weights.width() != input_size )
+            PLERROR( "weights (%d,%d) should have size equal to (output_size, input_size) (%d,%d)",
+                     weights.length(), weights.width(),
+                     output_size, input_size );
+
         weights << init_weights;
-    else if (init_weights_random_scale!=0)
+    }
+    else if(init_weights_random_scale != 0. )
     {
         real r = init_weights_random_scale / input_size;
-        random_gen->fill_random_uniform(weights.subMatColumns(1,input_size),
-                                        -r,r);
-        weights.subMatColumns(0,1).fill(0);
+        random_gen->fill_random_uniform(weights, -r, r);
     }
     else
-        weights.fill(0);
+        weights.clear();
 
     learning_rate = start_learning_rate;
     step_number = 0;
-
-    bias_input.resize( 1+input_size );
-    bias_input[0] = 1;
-
 }
 
 
-// ### Nothing to add here, simply calls build_
 void GradNNetLayerModule::build()
 {
     inherited::build();
@@ -321,8 +254,9 @@
     inherited::makeDeepCopyFromShallowCopy(copies);
 
     deepCopyField(init_weights, copies);
+    deepCopyField(init_bias, copies);
     deepCopyField(weights, copies);
-    deepCopyField(bias_input, copies);
+    deepCopyField(bias, copies);
 }
 
 void GradNNetLayerModule::declareOptions(OptionList& ol)
@@ -339,23 +273,29 @@
 
     declareOption(ol, "init_weights", &GradNNetLayerModule::init_weights,
                   OptionBase::buildoption,
-                  "Optional initial weights of the neurons (bias on first column,\n"
-                  "one row per neuron. If not provided then weights are initialized\n"
-                  "according to a uniform distribution (see init_weights_random_scale)\n"
-                  "and biases are initialized to 0.\n");
+                  "Optional initial weights of the neurons (one row per neuron).\n"
+                  "If not provided then weights are initialized according to a uniform\n"
+                  "distribution (see init_weights_random_scale)\n");
 
-    declareOption(ol, "init_weights_random_scale", &GradNNetLayerModule::init_weights_random_scale,
+    declareOption(ol, "init_bias", &GradNNetLayerModule::init_bias,
                   OptionBase::buildoption,
+                  "Optional initial bias of the neurons. If not provided, they are set to 0.\n");
+
+    declareOption(ol, "init_weights_random_scale",
+                  &GradNNetLayerModule::init_weights_random_scale,
+                  OptionBase::buildoption,
                   "If init_weights is not provided, the weights are initialized randomly\n"
                   "from a uniform in [-r,r], with r = init_weights_random_scale/input_size.\n"
                   "To clear the weights initially, just set this option to 0.");
 
-    declareOption(ol, "L1_penalty_factor", &GradNNetLayerModule::L1_penalty_factor,
+    declareOption(ol, "L1_penalty_factor",
+                  &GradNNetLayerModule::L1_penalty_factor,
                   OptionBase::buildoption,
                   "Optional (default=0) factor of L1 regularization term, i.e.\n"
                   "minimize L1_penalty_factor * sum_{ij} |weights(i,j)| during training.\n");
 
-    declareOption(ol, "L2_penalty_factor", &GradNNetLayerModule::L2_penalty_factor,
+    declareOption(ol, "L2_penalty_factor",
+                  &GradNNetLayerModule::L2_penalty_factor,
                   OptionBase::buildoption,
                   "Optional (default=0) factor of L2 regularization term, i.e.\n"
                   "minimize 0.5 * L2_penalty_factor * sum_{ij} weights(i,j)^2 during training.\n");
@@ -363,9 +303,12 @@
 
     declareOption(ol, "weights", &GradNNetLayerModule::weights,
                   OptionBase::learntoption,
-                  "Input weights of the neurons (bias on first column,"
-                  " one row per neuron" );
+                  "Input weights of the neurons (one row per neuron)");
 
+    declareOption(ol, "bias", &GradNNetLayerModule::bias,
+                  OptionBase::learntoption,
+                  "Bias of the neurons");
+
     inherited::declareOptions(ol);
 }
 
@@ -382,9 +325,12 @@
     if (init_weights.size()==0 && init_weights_random_scale!=0 && !random_gen)
         random_gen = new PRandom();
 
-    if( weights.length() != output_size || weights.width() != 1+input_size )
+    if( weights.length() != output_size
+        || weights.width() != input_size
+        || bias.size() != output_size )
+    {
         forget();
-
+    }
 }
 
 

Modified: trunk/plearn_learners/online/GradNNetLayerModule.h
===================================================================
--- trunk/plearn_learners/online/GradNNetLayerModule.h	2007-02-06 20:46:17 UTC (rev 6635)
+++ trunk/plearn_learners/online/GradNNetLayerModule.h	2007-02-07 21:08:50 UTC (rev 6636)
@@ -51,8 +51,15 @@
 namespace PLearn {
 
 /**
- * This class
+ * Affine transformation module, with stochastic gradient descent updates.
  *
+ * Neural Network layer, using stochastic gradient to update neuron weights,
+ *      Output = weights * Input + bias
+ * Weights and bias are updated by online gradient descent, with learning
+ * rate possibly decreasing in 1/(1 + n_updates_done * decrease_constant).
+ * An L1 and L2 regularization penalty can be added to push weights to 0.
+ * Weights can be initialized to 0, to a given initial matrix, or randomly
+ * from a uniform distribution.
  *
  */
 class GradNNetLayerModule : public OnlineLearningModule
@@ -69,10 +76,12 @@
     //! where t is the number of updates since the beginning
     real decrease_constant;
 
-    //! Optional initial weights of the neurons (bias on first column,
-    //! one row per neuron.
+    //! Optional initial weights of the neurons (one row per neuron).
     Mat init_weights;
 
+    //! Optional initial bias of the neurons.
+    Vec init_bias;
+
     //! If init_weights is not provided, the weights are initialized randomly
     //! from a uniform in [-r,r], with r = init_weights_random_scale/input_size
     real init_weights_random_scale;
@@ -83,9 +92,12 @@
     //! Optional (default=0) factor of L2 regularization term
     real L2_penalty_factor;
 
-    //! The weights, one neuron per line, bias first
+    //! The weights, one neuron per line
     Mat weights;
 
+    //! The bias
+    Vec bias;
+
 public:
     //#####  Public Member Functions  #########################################
 
@@ -123,7 +135,6 @@
     virtual void build();
 
     //! Transforms a shallow copy into a deep copy
-    // (PLEASE IMPLEMENT IN .cc)
     virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
 protected:
@@ -147,8 +158,6 @@
     // The rest of the private stuff goes here
     real learning_rate;
     int step_number;
-    mutable Vec bias_input;
-
 };
 
 // Declares a few other classes and functions related to this class



From lamblin at mail.berlios.de  Wed Feb  7 22:14:36 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 7 Feb 2007 22:14:36 +0100
Subject: [Plearn-commits] r6637 - trunk/plearn_learners/online
Message-ID: <200702072114.l17LEa0x000844@sheep.berlios.de>

Author: lamblin
Date: 2007-02-07 22:14:36 +0100 (Wed, 07 Feb 2007)
New Revision: 6637

Modified:
   trunk/plearn_learners/online/GradNNetLayerModule.h
Log:
Fix linking bug


Modified: trunk/plearn_learners/online/GradNNetLayerModule.h
===================================================================
--- trunk/plearn_learners/online/GradNNetLayerModule.h	2007-02-07 21:08:50 UTC (rev 6636)
+++ trunk/plearn_learners/online/GradNNetLayerModule.h	2007-02-07 21:14:36 UTC (rev 6637)
@@ -118,11 +118,13 @@
                               const Vec& output_gradient,
                               const Vec& output_diag_hessian);
 
+    /* Bad implementation. Let the parent call PLERROR.
     virtual void bbpropUpdate(const Vec& input, const Vec& output,
                               Vec& input_gradient,
                               const Vec& output_gradient,
                               Vec& input_diag_hessian,
                               const Vec& output_diag_hessian);
+    */
 
     virtual void forget();
 



From chrish at mail.berlios.de  Wed Feb  7 23:37:12 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Wed, 7 Feb 2007 23:37:12 +0100
Subject: [Plearn-commits] r6638 - trunk/plearn_learners/testers
Message-ID: <200702072237.l17MbCUA004871@sheep.berlios.de>

Author: chrish
Date: 2007-02-07 23:37:11 +0100 (Wed, 07 Feb 2007)
New Revision: 6638

Modified:
   trunk/plearn_learners/testers/PTester.cc
Log:
* Also save training set using .vmat extension instead of .psave
* Update documentation.


Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2007-02-07 21:14:36 UTC (rev 6637)
+++ trunk/plearn_learners/testers/PTester.cc	2007-02-07 22:37:11 UTC (rev 6638)
@@ -190,7 +190,7 @@
 
     declareOption(
         ol, "save_data_sets", &PTester::save_data_sets, OptionBase::buildoption,
-        "If true, the data set generated for split #k will be saved as Split#k/training_set.psave Split#k/test1_set.psave ...");
+        "If true, the data set generated for split #k will be saved as Split#k/training_set.vmat Split#k/test1_set.vmat ...");
 
     declareOption(
         ol, "save_test_outputs", &PTester::save_test_outputs, OptionBase::buildoption,
@@ -536,7 +536,7 @@
         if (should_train) {
             VMat trainset = dsets[0];
             if (is_splitdir && save_data_sets)
-                PLearn::save(splitdir / "training_set.psave", trainset);
+                PLearn::save(splitdir / "training_set.vmat", trainset);
             
             if (provide_learner_expdir)
             {



From tihocan at mail.berlios.de  Thu Feb  8 21:31:30 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 8 Feb 2007 21:31:30 +0100
Subject: [Plearn-commits] r6639 - trunk/plearn_learners/classifiers
Message-ID: <200702082031.l18KVUH2032211@sheep.berlios.de>

Author: tihocan
Date: 2007-02-08 21:31:30 +0100 (Thu, 08 Feb 2007)
New Revision: 6639

Modified:
   trunk/plearn_learners/classifiers/ClassifierFromDensity.cc
   trunk/plearn_learners/classifiers/ClassifierFromDensity.h
Log:
Fixed bug where loaded objects had their estimators erased at build time, because of the 'copy_estimator' option being true. This fix should be compatible with the goal of the 'copy_estimator' option that was added to avoid a bug in HyperLearner

Modified: trunk/plearn_learners/classifiers/ClassifierFromDensity.cc
===================================================================
--- trunk/plearn_learners/classifiers/ClassifierFromDensity.cc	2007-02-07 22:37:11 UTC (rev 6638)
+++ trunk/plearn_learners/classifiers/ClassifierFromDensity.cc	2007-02-08 20:31:30 UTC (rev 6639)
@@ -54,7 +54,7 @@
     : nclasses(-1),
       output_log_probabilities(false),
       normalize_probabilities(true),
-      copy_estimator(false)
+      using_template_estimator(false)
 {}
 
 PLEARN_IMPLEMENT_OBJECT(
@@ -97,8 +97,8 @@
     declareOption(ol, "log_priors", &ClassifierFromDensity::log_priors, OptionBase::learntoption,
                   "The log of the class prior probabilities");
 
-    declareOption(ol, "copy_estimator", &ClassifierFromDensity::copy_estimator, OptionBase::learntoption,
-                  "Indication that, at build time, the estimators for all classes are a copy of the first one");
+    declareOption(ol, "using_template_estimator", &ClassifierFromDensity::using_template_estimator, OptionBase::learntoption,
+                  "Indication that the estimators were originally obtained from one single template");
 
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
@@ -118,9 +118,9 @@
 ////////////
 void ClassifierFromDensity::build_()
 {
-    if(estimators.size()==1 || copy_estimator)
+    if(estimators.size()==1)
     {
-        copy_estimator = true;
+        using_template_estimator = true;
         estimators.resize(nclasses);
         for(int i=1; i<nclasses; i++)
             estimators[i] = PLearn::deepCopy(estimators[0]);
@@ -154,8 +154,12 @@
 void ClassifierFromDensity::forget()
 {
     stage=0;
+    if (using_template_estimator)
+        estimators.resize(1);
     for(int c=0; c<estimators.length(); c++)
         estimators[c]->forget();
+    if (using_template_estimator)
+        build(); // Ensure the first estimator is duplicated.
 }
 
 ///////////

Modified: trunk/plearn_learners/classifiers/ClassifierFromDensity.h
===================================================================
--- trunk/plearn_learners/classifiers/ClassifierFromDensity.h	2007-02-07 22:37:11 UTC (rev 6638)
+++ trunk/plearn_learners/classifiers/ClassifierFromDensity.h	2007-02-08 20:31:30 UTC (rev 6639)
@@ -66,7 +66,7 @@
     bool output_log_probabilities;
     bool normalize_probabilities;
     Vec use_these_priors;
-    bool copy_estimator;
+    bool using_template_estimator;
     // ****************
     // * Constructors *
     // ****************



From chapados at mail.berlios.de  Fri Feb  9 18:53:39 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Fri, 9 Feb 2007 18:53:39 +0100
Subject: [Plearn-commits] r6640 - trunk/plearn/var
Message-ID: <200702091753.l19Hrdqw015274@sheep.berlios.de>

Author: chapados
Date: 2007-02-09 18:53:36 +0100 (Fri, 09 Feb 2007)
New Revision: 6640

Modified:
   trunk/plearn/var/NegLogPoissonVariable.cc
Log:
Added robustness checks to NegLogPoissonVariable

Modified: trunk/plearn/var/NegLogPoissonVariable.cc
===================================================================
--- trunk/plearn/var/NegLogPoissonVariable.cc	2007-02-08 20:31:30 UTC (rev 6639)
+++ trunk/plearn/var/NegLogPoissonVariable.cc	2007-02-09 17:53:36 UTC (rev 6640)
@@ -99,16 +99,24 @@
         real weight = 1;
         if (varray.size()>2)
             weight = varray[2]->valuedata[i];
-        real log_fact_target = pl_gammln(target+1);
+
+        static real EPSILON = 1e-8;          // Regularization
+        real log_fact_target = pl_gammln(max(target, EPSILON)+1);
 //        cost += exp(output) * weight - (output + pl_log(weight) ) * target + log_fact_target;
-        cost += exp(output) - (output + pl_log(weight)) * target / weight + log_fact_target / weight;
+
+        if (weight > 0)
+            cost += exp(output) - (output + pl_log(weight)) * target / weight +
+                    log_fact_target / weight;
     }
+    if (is_missing(cost))
+        PLERROR("NegLogPoissonVariable::fprop: encountered NaN cost");
+    
     valuedata[0] = cost;
 }
 
 void NegLogPoissonVariable::bprop()
 {
-    real gr = *gradientdata;
+    real gr = gradient[0];
     for (int i=0; i<varray[0]->size(); i++)
     {
         real output = varray[0]->valuedata[i];
@@ -116,7 +124,9 @@
         real weight = 1;
         if (varray.size()>2)
             weight = varray[2]->valuedata[i];
-        varray[0]->gradientdata[i] += gr* ( exp(output) - target / weight);
+
+        if (weight > 0)
+            varray[0]->gradientdata[i] += gr * ( exp(output) - target / weight);
 //        varray[0]->gradientdata[i] += gr* ( exp(output) * weight - target);
     }
 }



From lamblin at mail.berlios.de  Fri Feb  9 22:06:30 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 9 Feb 2007 22:06:30 +0100
Subject: [Plearn-commits] r6641 - trunk/plearn_learners/online
Message-ID: <200702092106.l19L6U7G028948@sheep.berlios.de>

Author: lamblin
Date: 2007-02-09 22:06:30 +0100 (Fri, 09 Feb 2007)
New Revision: 6641

Modified:
   trunk/plearn_learners/online/CombiningCostsModule.cc
Log:
Bugfix: now clear input_gradient vector before accumulating in it...


Modified: trunk/plearn_learners/online/CombiningCostsModule.cc
===================================================================
--- trunk/plearn_learners/online/CombiningCostsModule.cc	2007-02-09 17:53:36 UTC (rev 6640)
+++ trunk/plearn_learners/online/CombiningCostsModule.cc	2007-02-09 21:06:30 UTC (rev 6641)
@@ -97,10 +97,10 @@
                  "should be equal to n_sub_costs (%d != %d).\n",
                  cost_weights.length(), n_sub_costs );
 
+    sub_costs_values.resize( n_sub_costs );
     output_size = n_sub_costs+1;
 }
 
-// ### Nothing to add here, simply calls build_
 void CombiningCostsModule::build()
 {
     inherited::build();
@@ -138,6 +138,7 @@
     PLASSERT( input.size() == input_size );
     PLASSERT( target.size() == target_size );
     input_gradient.resize( input_size );
+    input_gradient.clear();
 
     Vec partial_gradient;
     for( int i=0 ; i<n_sub_costs ; i++ )
@@ -171,7 +172,9 @@
     PLASSERT( input.size() == input_size );
     PLASSERT( target.size() == target_size );
     input_gradient.resize( input_size );
+    input_gradient.clear();
     input_diag_hessian.resize( input_size );
+    input_diag_hessian.clear();
 
     Vec partial_gradient;
     Vec partial_diag_hessian;
@@ -196,7 +199,7 @@
 }
 
 
-//! reset the parameters to the state they would be BEFORE starting training.
+//! Reset the parameters to the state they would be BEFORE starting training.
 //! Note that this method is necessarily called from build().
 void CombiningCostsModule::forget()
 {



From lamblin at mail.berlios.de  Fri Feb  9 22:13:08 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 9 Feb 2007 22:13:08 +0100
Subject: [Plearn-commits] r6642 - trunk/plearn_learners/online
Message-ID: <200702092113.l19LD8rZ029290@sheep.berlios.de>

Author: lamblin
Date: 2007-02-09 22:13:08 +0100 (Fri, 09 Feb 2007)
New Revision: 6642

Modified:
   trunk/plearn_learners/online/ClassErrorCostModule.cc
   trunk/plearn_learners/online/ClassErrorCostModule.h
Log:
Allows call to the bpropUpdate that only does the "update" part
(especially for use with CombiningCostsModule).


Modified: trunk/plearn_learners/online/ClassErrorCostModule.cc
===================================================================
--- trunk/plearn_learners/online/ClassErrorCostModule.cc	2007-02-09 21:06:30 UTC (rev 6641)
+++ trunk/plearn_learners/online/ClassErrorCostModule.cc	2007-02-09 21:13:08 UTC (rev 6642)
@@ -116,14 +116,36 @@
 /////////////////
 // bpropUpdate //
 /////////////////
-/*
 void ClassErrorCostModule::bpropUpdate(const Vec& input, const Vec& target,
+                                       real cost)
+{
+}
+
+/* Not supposed to happen
+void ClassErrorCostModule::bpropUpdate(const Vec& input, const Vec& target,
                                        real cost,
                                        Vec& input_gradient)
 {
 }
 */
 
+//////////////////
+// bbpropUpdate //
+//////////////////
+void ClassErrorCostModule::bbpropUpdate(const Vec& input, const Vec& target,
+                                        real cost)
+{
+}
+
+/* Not supposed to happen
+void ClassErrorCostModule::bbpropUpdate(const Vec& input, const Vec& target,
+                                       real cost,
+                                       Vec& input_gradient,
+                                       Vec& input_diag_hessian)
+{
+}
+*/
+
 ////////////
 // forget //
 ////////////

Modified: trunk/plearn_learners/online/ClassErrorCostModule.h
===================================================================
--- trunk/plearn_learners/online/ClassErrorCostModule.h	2007-02-09 21:06:30 UTC (rev 6641)
+++ trunk/plearn_learners/online/ClassErrorCostModule.h	2007-02-09 21:13:08 UTC (rev 6642)
@@ -59,19 +59,12 @@
 public:
     //#####  Public Build Options  ############################################
 
-    //! ### declare public option fields (such as build options) here
-    //! Start your comments with Doxygen-compatible comments such as //!
-
 public:
     //#####  Public Member Functions  #########################################
 
     //! Default constructor
-    // ### Make sure the implementation in the .cc
-    // ### initializes all fields to reasonable default values.
     ClassErrorCostModule();
 
-    // Your other public member functions go here
-
     //! Given the input and the target, compute a vector of costs
     //! (possibly resize it appropriately)
     virtual void fprop(const Vec& input, const Vec& target, Vec& cost) const;
@@ -80,12 +73,18 @@
     //! (of which we will compute the gradient)
     virtual void fprop(const Vec& input, const Vec& target, real& cost) const;
 
+    //! Nothing to do
+    virtual void bpropUpdate(const Vec& input, const Vec& target, real cost);
+
     /* Default implementation in super class raises a PLERROR
     //! No differentiable, so no gradient to backprop!
     virtual void bpropUpdate(const Vec& input, const Vec& target, real cost,
                              Vec& input_gradient);
     */
 
+    //! Nothing to do
+    virtual void bbpropUpdate(const Vec& input, const Vec& target, real cost);
+
     /* Optional
        N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
        RAISES A PLERROR.
@@ -110,8 +109,6 @@
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.
-    // ### If your class is not instantiatable (it has pure virtual methods)
-    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
     PLEARN_DECLARE_OBJECT(ClassErrorCostModule);
 
     // Simply calls inherited::build() then build_()
@@ -135,8 +132,6 @@
 
 private:
     //#####  Private Data Members  ############################################
-
-    // The rest of the private stuff goes here
 };
 
 // Declares a few other classes and functions related to this class



From lamblin at mail.berlios.de  Fri Feb  9 22:26:31 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 9 Feb 2007 22:26:31 +0100
Subject: [Plearn-commits] r6643 - trunk/plearn_learners/online
Message-ID: <200702092126.l19LQVmC029689@sheep.berlios.de>

Author: lamblin
Date: 2007-02-09 22:26:30 +0100 (Fri, 09 Feb 2007)
New Revision: 6643

Modified:
   trunk/plearn_learners/online/GradNNetLayerModule.cc
Log:
Avoid recomputing learning_rate*output_grad[i] n times during the
weights' update.


Modified: trunk/plearn_learners/online/GradNNetLayerModule.cc
===================================================================
--- trunk/plearn_learners/online/GradNNetLayerModule.cc	2007-02-09 21:13:08 UTC (rev 6642)
+++ trunk/plearn_learners/online/GradNNetLayerModule.cc	2007-02-09 21:26:30 UTC (rev 6643)
@@ -106,11 +106,12 @@
             PLWARNING("GradNNetLayerModule::bpropUpdate:\n"
                       "learning rate = %f is too large!\n", learning_rate);
 
-        bias[i] -= learning_rate * og_i;
+        real lr_og_i = learning_rate * og_i;
+        bias[i] -= lr_og_i;
 
         for( int j=0; j<input_size; j++ )
         {
-            w_[j] -= learning_rate * input[j] * og_i;
+            w_[j] -= input[j] * lr_og_i;
 
             if( delta_L1 > 0. )
             {
@@ -159,12 +160,13 @@
             PLWARNING("GradNNetLayerModule::bpropUpdate:\n"
                       "learning rate = %f is too large!\n", learning_rate);
 
-        bias[i] -= learning_rate * og_i;
+        real lr_og_i = learning_rate * og_i;
+        bias[i] -= lr_og_i;
 
         for( int j=0; j<input_size; j++ )
         {
             input_gradient[j] += w_[j] * og_i;
-            w_[j] -= learning_rate * input[j] * og_i;
+            w_[j] -= input[j] * lr_og_i;
 
             if( delta_L1 > 0. )
             {



From plearner at mail.berlios.de  Sat Feb 10 00:09:23 2007
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Sat, 10 Feb 2007 00:09:23 +0100
Subject: [Plearn-commits] r6644 - in trunk/plearn: opt var
Message-ID: <200702092309.l19N9NSk001825@sheep.berlios.de>

Author: plearner
Date: 2007-02-10 00:09:22 +0100 (Sat, 10 Feb 2007)
New Revision: 6644

Modified:
   trunk/plearn/opt/ConjGradientOptimizer.cc
   trunk/plearn/opt/ConjGradientOptimizer.h
   trunk/plearn/var/SumOfVariable.cc
   trunk/plearn/var/SumOfVariable.h
Log:
Initial version of minibatch capability for ConjGradientOptimizer



Modified: trunk/plearn/opt/ConjGradientOptimizer.cc
===================================================================
--- trunk/plearn/opt/ConjGradientOptimizer.cc	2007-02-09 21:26:30 UTC (rev 6643)
+++ trunk/plearn/opt/ConjGradientOptimizer.cc	2007-02-09 23:09:22 UTC (rev 6644)
@@ -42,6 +42,7 @@
 
 #include "ConjGradientOptimizer.h"
 #include <plearn/io/pl_log.h>
+#include <plearn/var/SumOfVariable.h>
 
 namespace PLearn {
 using namespace std;
@@ -59,6 +60,9 @@
       max_eval_per_line_search(20),
       no_negative_gamma(true),
       verbosity(0),
+      minibatch_n_samples(0),
+      minibatch_n_line_searches(3),
+      minibatch_curpos(0),
       line_search_failed(false),
       line_search_succeeded(false)
 { }
@@ -175,6 +179,20 @@
         OptionBase::buildoption, 
         "Maximum slope ratio.");
 
+    declareOption(
+        ol, "minibatch_n_samples", &ConjGradientOptimizer::minibatch_n_samples,
+        OptionBase::buildoption, 
+        "If >0 we'll do minibatch. In minibatch mode, weight updates are based on \n"
+        "cost and gradients computed on a subset of the whole training set, made \n"
+        "of minibatch_n_samples consecutive samples. Each such subset will be used \n"
+        "to perform minibatch_n_line_searches line searches before moving to the \n"
+        "next minibatch subset.\n");
+
+    declareOption(
+        ol, "minibatch_n_line_searches", &ConjGradientOptimizer::minibatch_n_line_searches,
+        OptionBase::buildoption, 
+        "How many line searches to perform with each minibatch subset.");
+
     inherited::declareOptions(ol);
 }
 
@@ -424,6 +442,25 @@
 bool ConjGradientOptimizer::optimizeN(VecStatsCollector& stats_coll) {
     int stage_max = stage + nstages; // The stage to reach.
 
+    SumOfVariable* sumofvar = 0;
+    int trainsetlength = -1;
+    int minibatch_n_line_searches_left = minibatch_n_line_searches;
+    if(minibatch_n_samples>0)
+    {
+        sumofvar = dynamic_cast<SumOfVariable*>((Variable*)cost);
+        if(sumofvar)
+        {
+            trainsetlength = sumofvar->getDataSet()->length();
+            sumofvar->setSampleRange(minibatch_curpos, minibatch_n_samples, true);
+        }
+        else
+        {
+            PLWARNING("In ConjGradientOptimizer, minibatch_n_samples>0 but can't "
+                      "do minibatch since cost does not seem to be a SumOfVariable "
+                      " (the only type of variable for which minibatch is supported)");
+        }
+    }
+    
     if (stage == 0)
     {
         computeOppositeGradient(current_opp_gradient);
@@ -444,8 +481,19 @@
     }
 
     for (; !early_stop && stage<stage_max; stage++) {
+
+        if(sumofvar && minibatch_n_line_searches_left==0)
+        {
+            minibatch_curpos = (minibatch_curpos+minibatch_n_samples)%trainsetlength;
+            sumofvar->setSampleRange(minibatch_curpos, minibatch_n_samples, true);
+            minibatch_n_line_searches_left = minibatch_n_line_searches;            
+        }
+
         // Make a line search along the current search direction.
         early_stop = !lineSearch();
+        if(sumofvar) // we're doing minibatch
+            --minibatch_n_line_searches_left;
+            
         // Ensure 'delta' contains the opposite gradient at the new point
         // reached after the line search.
         // Also update 'current_cost'.
@@ -490,6 +538,7 @@
     inherited::reset();
     line_search_failed = false;
     line_search_succeeded = false;
+    minibatch_curpos = 0;
 }
 
 ///////////////////////////

Modified: trunk/plearn/opt/ConjGradientOptimizer.h
===================================================================
--- trunk/plearn/opt/ConjGradientOptimizer.h	2007-02-09 21:26:30 UTC (rev 6643)
+++ trunk/plearn/opt/ConjGradientOptimizer.h	2007-02-09 23:09:22 UTC (rev 6644)
@@ -66,8 +66,13 @@
     bool no_negative_gamma;
     int verbosity;
 
+    int minibatch_n_samples;
+    int minibatch_n_line_searches;
+
 protected:
-  
+
+    int minibatch_curpos; // current starting sample position in dataset for minibatch 
+
     //! Bracket limit.
     real bracket_limit;
 

Modified: trunk/plearn/var/SumOfVariable.cc
===================================================================
--- trunk/plearn/var/SumOfVariable.cc	2007-02-09 21:26:30 UTC (rev 6643)
+++ trunk/plearn/var/SumOfVariable.cc	2007-02-09 23:09:22 UTC (rev 6644)
@@ -69,7 +69,7 @@
     : inherited(nonInputParentsOfPath(the_f->inputs,the_f->outputs), 
                 the_f->outputs[0]->length(), 
                 the_f->outputs[0]->width()),
-      distr(the_distr), f(the_f), nsamples(the_nsamples), curpos(0),
+      distr(the_distr), f(the_f), nsamples(the_nsamples), curpos(0), loop(false),
       //input_value(the_distr->inputsize()+the_distr->targetsize()+the_distr->weightsize()), 
       //input_gradient(the_distr->inputsize()+the_distr->targetsize()+the_distr->weightsize()), 
       input_value(the_distr->width()),
@@ -90,7 +90,8 @@
 void
 SumOfVariable::build_()
 {
-    if (f && distr) {
+    if (f && distr) 
+    {
         input_value.resize(distr->inputsize() + distr->targetsize() + distr->weightsize());
         input_gradient.resize(distr->inputsize() + distr->targetsize() + distr->weightsize());
         if(f->outputs.size() != 1)
@@ -116,6 +117,17 @@
                   "all rows of the matrix.");
     declareOption(ol, "curpos", &SumOfVariable::curpos, OptionBase::buildoption,
                   "Current position (row) in the VMatrix we are summing over.");
+    declareOption(ol, "loop", &SumOfVariable::loop, OptionBase::buildoption,
+                  "If true, every propagation operation, before returning,\n"
+                  "will set back curpos to the value it had when entering\n"
+                  "the call. So curpos will be left unchanged by the call.\n"
+                  "This behavior corresponds to propagation operations \n"
+                  "always summing over the same nsamples (in range \n"
+                  "curpos, ..., curpos+nsamples-1) \n"
+                  "If loop is false however, any propagation call will \n"
+                  "move curpos by nsamples, thus a subsequent propagation \n"
+                  "call will sum over the *next* nsamples (which will correspond \n"
+                  "to the same saples only if nsamples == distr.length()).");
     inherited::declareOptions(ol);
 }
 
@@ -140,6 +152,8 @@
 
 void SumOfVariable::fprop()
 {
+    int orig_curpos = curpos;
+
     f->recomputeParents();
 
     if(nsamples==1)
@@ -186,6 +200,9 @@
         }
 #endif
     }
+
+    if(loop)
+        curpos = orig_curpos;
 }
 
 
@@ -195,8 +212,9 @@
 
 void SumOfVariable::fbprop()
 {
-    f->recomputeParents();
-  
+    f->recomputeParents();  
+    int orig_curpos = curpos;
+
     if(nsamples==1)
     {
         input_value.resize(distr->width());
@@ -254,6 +272,10 @@
         }
 #endif
     }
+
+    if(loop)
+        curpos = orig_curpos;
+
 }
 
 
@@ -280,6 +302,8 @@
 
 void SumOfVariable::rfprop()
 {
+    int orig_curpos = curpos;
+
     if (rValue.length()==0) resizeRValue();
     // TODO... (we will need a rfprop() in Func)
   
@@ -320,6 +344,11 @@
 //      }
 //  #endif
 //    }
+
+
+    if(loop)
+        curpos = orig_curpos;
+
 }
 
 
@@ -348,9 +377,9 @@
             curpos = 0;
         f->fproppath.printInfo(print_gradient);
     }
-    cout << info() << " : " << getName() << " = " << value;
+    pout << info() << " : " << getName() << " = " << value;
     if (print_gradient) cout << " gradient=" << gradient;
-    cout << endl; 
+    pout << endl; 
 }
 
 

Modified: trunk/plearn/var/SumOfVariable.h
===================================================================
--- trunk/plearn/var/SumOfVariable.h	2007-02-09 21:26:30 UTC (rev 6643)
+++ trunk/plearn/var/SumOfVariable.h	2007-02-09 23:09:22 UTC (rev 6644)
@@ -63,22 +63,34 @@
 {
     typedef NaryVariable inherited;
 
+// protected:
 public:
-    //protected:
     VMat distr;
     Func f;
-    int nsamples;
-    int curpos; //!<  current pos in VMat 
+
+    int nsamples; //!< number of consecutive samples from the dataset distr
+                  //!< that every propagation operation will use
+
+    int curpos;   //!< position of current sample in dataset distr 
+
+    bool loop;    //!< if true, every propagation operation, before returning,
+                  //!< will set back curpos to the value it had when entering
+                  //!< the call. So that curpos will be unchanged by the call.
+
     // To avoid allocation/deallocations in fprop/bprop
     Vec input_value;
     Vec input_gradient;
     Vec output_value;
     //! Indication that sizefprop should be used on f
     bool do_sizeprop;
+
+    int beginpos;
+    int endpos;
     
 public:
     //!  protected default constructor for persistence
-    SumOfVariable() : distr(), f(), nsamples(), curpos() {}
+    SumOfVariable() : distr(), f(), nsamples(0), curpos(0), loop(false) {}
+
     //!  Sum_{inputs \in distr} f(inputs)
     SumOfVariable(VMat the_distr, Func the_f, int the_nsamples=-1, bool the_do_resizeprop=false);
     
@@ -94,7 +106,46 @@
     virtual void fbprop();
     virtual void symbolicBprop();
     virtual void rfprop();
-    
+
+    VMat getDataSet() const
+    { return distr; }
+
+    void setDataSet(VMat dset)
+    {
+        if(distr.isNotNull() && distr.length()==nsamples)
+            nsamples = -1;
+        
+        distr = dset;
+        if(nsamples == -1)
+            nsamples = distr->length();
+
+        curpos = 0;
+    }
+
+    void setCurrentSamplePos(int pos)
+    { curpos = pos; }
+
+    int getCurrentSamplePos() const
+    { return curpos; }
+
+    //! This allows to control over which part of the dataset
+    //! the next propagation operation(s) will sum.
+    /** The call sets the curpos, nsamples and loop options.  Thus the next
+        propagation call will start at sample curpos=startpos and sum over
+        nsamples=n consecutive samples.  If loop (assigned the value do_loop)
+        is true, then curpos will be left unchanged by propagation calls, which
+        will thus always sum over the same nsamples samples.  If loop is
+        false however, any propagation call will move curpos by nsamples, thus
+        a subsequent propagation call will sum over the *next* nsamples (which
+        will correspond to the same smaples only if nsamples == distr.length()) **/
+
+    void setSampleRange(int startpos, int n, bool do_loop)
+    {
+        curpos = startpos;
+        nsamples = n;
+        loop = do_loop;
+    }
+
     void printInfo(bool print_gradient);
 
 protected:



From plearner at mail.berlios.de  Sat Feb 10 00:10:19 2007
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Sat, 10 Feb 2007 00:10:19 +0100
Subject: [Plearn-commits] r6645 -
	trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results
Message-ID: <200702092310.l19NAJpZ001889@sheep.berlios.de>

Author: plearner
Date: 2007-02-10 00:10:19 +0100 (Sat, 10 Feb 2007)
New Revision: 6645

Modified:
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
Log:
Test result fix due to new options in ConjGradOptimizer (for the minibatch case)


Modified: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2007-02-09 23:09:22 UTC (rev 6644)
+++ trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2007-02-09 23:10:19 UTC (rev 6645)
@@ -4,11 +4,11 @@
 !R 0 
 !R 1 GaussianProcessRegressor(
 kernel = *1 ->RationalQuadraticARDKernel(
-log_alpha = 1.045475 ;
-log_signal_sigma = 1.788859 ;
+log_alpha = 1.0454759217536389 ;
+log_signal_sigma = 1.78885962610151594 ;
 log_global_sigma = 0 ;
-log_input_sigma = 1 [ 1.605025 ] ;
-log_noise_sigma = -1.084960 ;
+log_input_sigma = 1 [ 1.60502563724931702 ] ;
+log_noise_sigma = -1.08496085286010091 ;
 is_symmetric = 1 ;
 report_progress = 0 ;
 specify_dataset = *0 ;
@@ -19,35 +19,37 @@
 weight_decay = 0 ;
 include_bias = 1 ;
 compute_confidence = 1 ;
-confidence_epsilon = 1.000000e-08 ;
+confidence_epsilon = 1.00000000000000002e-08 ;
 hyperparameters = 3 [ ("log_signal_sigma" , "0.0" )("log_noise_sigma" , "0.0" )("log_alpha" , "0.0" )] ;
 ARD_hyperprefix_initval = ("log_input_sigma" , "0.0" );
 optimizer = *2 ->ConjGradientOptimizer(
 verbosity = 1 ;
 expected_red = 1 ;
 no_negative_gamma = 1 ;
-sigma = 0.100000 ;
-rho = 0.050000 ;
-constrain_limit = 0.100000 ;
+sigma = 0.100000000000000006 ;
+rho = 0.0500000000000000028 ;
+constrain_limit = 0.100000000000000006 ;
 max_extrapolate = 3 ;
 max_eval_per_line_search = 20 ;
 slope_ratio = 10 ;
+minibatch_n_samples = 0 ;
+minibatch_n_line_searches = 3 ;
 nstages = 1  )
 ;
 alpha = 5  1  [ 
--1.448018 	
--1.758700 	
-2.977907 	
-0.366193 	
--0.252937 	
+-1.44801819162290024 	
+-1.75870011045946795 	
+2.97790716927227539 	
+0.366193225658293153 	
+-0.252937646496436019 	
 ]
 ;
 gram_inverse = 5  5  [ 
-2.375045 	0.318202 	-2.795014 	0.183693 	-0.015760 	
-0.318202 	2.916507 	-3.005313 	-0.300764 	0.023724 	
--2.795014 	-3.005313 	5.750029 	0.045140 	-0.001069 	
-0.183693 	-0.300764 	0.045140 	0.105748 	-0.013296 	
--0.015760 	0.023724 	-0.001069 	-0.013296 	0.029873 	
+2.3750455877628629 	0.318202839946681582 	-2.79501466140384469 	0.183693865043761062 	-0.0157606149450654383 	
+0.318202839946681137 	2.9165070423025945 	-3.00531385230735149 	-0.300764945104235126 	0.0237240821333623043 	
+-2.79501466140384469 	-3.00531385230735149 	5.75002906618107712 	0.0451407476338803876 	-0.00106974531844977044 	
+0.183693865043760979 	-0.300764945104235071 	0.0451407476338803806 	0.105748934100125966 	-0.0132961075182882887 	
+-0.0157606149450654383 	0.0237240821333623009 	-0.00106974531844977066 	-0.0132961075182882853 	0.0298730636742724452 	
 ]
 ;
 target_mean = 1 [ 10 ] ;
@@ -85,18 +87,18 @@
 nservers = 0 ;
 save_trainingset_prefix = ""  )
 
-!R 1 1 [ 15.000000 ] 
-!R 1 1 [ 14.402740 ] 
+!R 1 1 [ 15.0000000000000071 ] 
+!R 1 1 [ 14.4027409228245133 ] 
 !R 2 4  1  [ 
-13.500058 	
-14.425999 	
-15.000000 	
-14.402740 	
+13.5000588445143688 	
+14.425999686387911 	
+15.0000000000000071 	
+14.4027409228245133 	
 ]
 1 [ 4  4  [ 
-0.495755 	0.296446 	-1.492139e-13 	-0.618033 	
-0.296446 	0.372151 	-1.207922e-13 	-0.472278 	
--6.394884e-14 	-2.060573e-13 	9.999907e-09 	-1.065814e-13 	
--0.618033 	-0.472278 	-5.684341e-14 	2.538947 	
+0.495755881017678368 	0.296446868109264017 	-1.49213974509621039e-13 	-0.618033363536593328 	
+0.296446868109470074 	0.372151242547516026 	-1.20792265079217032e-13 	-0.47227821255051694 	
+-6.39488462184090167e-14 	-2.06057393370429054e-13 	9.9999076294443514e-09 	-1.06581410364015028e-13 	
+-0.618033363536468983 	-0.472278212550524046 	-5.68434188608080149e-14 	2.53894720061439383 	
 ]
 ] 



From chapados at mail.berlios.de  Sun Feb 11 01:18:57 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Sun, 11 Feb 2007 01:18:57 +0100
Subject: [Plearn-commits] r6646 - trunk/plearn/math
Message-ID: <200702110018.l1B0Ivw2000626@sheep.berlios.de>

Author: chapados
Date: 2007-02-11 01:18:56 +0100 (Sun, 11 Feb 2007)
New Revision: 6646

Modified:
   trunk/plearn/math/lapack_proto.h
   trunk/plearn/math/plapack.cc
   trunk/plearn/math/plapack.h
Log:
Added lapack interface for cholesky decomposition and solving a linear system through the cholesky

Modified: trunk/plearn/math/lapack_proto.h
===================================================================
--- trunk/plearn/math/lapack_proto.h	2007-02-09 23:10:19 UTC (rev 6645)
+++ trunk/plearn/math/lapack_proto.h	2007-02-11 00:18:56 UTC (rev 6646)
@@ -77,6 +77,13 @@
     void ssygvx_(int* ITYPE, char* JOBZ, char* RANGE, char* UPLO, int* N, float* A, int* LDA, float* B, int* LDB, float* VL, float* VU, int* IL, int* IU, float* ABSTOL, int* M, float* W, float* Z, int* LDZ, float* WORK, int* LWORK, int* IWORK, int* IFAIL, int* INFO);
     void dsygvx_(int* ITYPE, char* JOBZ, char* RANGE, char* UPLO, int* N, double* A, int* LDA, double* B, int* LDB, double* VL, double* VU, int* IL, int* IU, double* ABSTOL, int* M, double* W, double* Z, int* LDZ, double* WORK, int* LWORK, int* IWORK, int* IFAIL, int* INFO);
 
+    // Cholesky Decomposition
+    void spotrf_(char* UPLO, int* N, float*  A, int* LDA, int* INFO);
+    void dpotrf_(char* UPLO, int* N, double* A, int* LDA, int* INFO);
+
+    // Solve linear system given Cholesky Decomposition
+    void spotrs_(char* UPLO, int* N, int* NRHS, float*  A, int* LDA, float*  B, int* LDB, int* INFO);
+    void dpotrs_(char* UPLO, int* N, int* NRHS, double* A, int* LDA, double* B, int* LDB, int* INFO);
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn/math/plapack.cc
===================================================================
--- trunk/plearn/math/plapack.cc	2007-02-09 23:10:19 UTC (rev 6645)
+++ trunk/plearn/math/plapack.cc	2007-02-11 00:18:56 UTC (rev 6646)
@@ -417,7 +417,124 @@
 }
 
 
+//#####  lapackCholeskyDecompositionInPlace  ##################################
 
+void lapackCholeskyDecompositionInPlace(Mat& A, char uplo)
+{
+    if (A.width() == 0 || A.length() == 0)
+        return;
+    if (A.mod() != A.width())
+        PLERROR("lapackCholeskyDecompositionInPlace: matrix mod (%d) must equal "
+                "its width (%d)", A.mod(), A.width());
+    if (A.width() != A.length())
+        PLERROR("lapackCholeskyDecompositionInPlace: matrix width (%d) and height (%d) "
+                "must be equal", A.width(), A.length());
+
+    char lapack_uplo;
+    switch (uplo) {
+    case 'L':
+    case 'l':
+        lapack_uplo = 'U';
+        break;
+
+    case 'U':
+    case 'u':
+        lapack_uplo = 'L';
+        break;
+
+    default:
+        PLERROR("lapackCholeskyDecompositionInPlace: unrecognized character '%c' for "
+                "argument 'uplo'; valid characters are 'U' and 'L'", uplo);
+    }
+
+    real* data = A.data();
+    int N = A.width();
+    int INFO;
+
+    // call LAPACK
+    lapack_Xpotrf_(&lapack_uplo, &N, data, &N, &INFO);
+
+    if (INFO == 0)
+        return;                              // all successful
+    else if (INFO < 0)
+        PLERROR("lapackCholeskyDecompositionInPlace: implementation error; argument %d "
+                "to xPOTRF had an illegal value", -INFO);
+    else
+        PLERROR("lapackCholeskyDecompositionInPlace: error in decomposition; "
+                "leading minor of order %d is not positive definite, "
+                "and the factorization could not be completed.", INFO);
+}
+
+
+//#####  lapackCholeskySolveInPlace  ##########################################
+
+void lapackCholeskySolveInPlace(Mat& A, Mat& B, bool B_is_column_major, char uplo)
+{
+    if (A.width() == 0 || A.length() == 0 || B.width() == 0 || B.length() == 0)
+        return;
+    if (A.mod() != A.width())
+        PLERROR("lapackCholeskySolveInPlace: matrix A mod (%d) must equal "
+                "its width (%d)", A.mod(), A.width());
+    if (B.mod() != B.width())
+        PLERROR("lapackCholeskySolveInPlace: matrix B mod (%d) must equal "
+                "its width (%d)", B.mod(), B.width());
+    if (A.width() != A.length())
+        PLERROR("lapackCholeskySolveInPlace: matrix width (%d) and height (%d) "
+                "must be equal", A.width(), A.length());
+    if ((! B_is_column_major && B.length() != A.length()) ||
+        (  B_is_column_major && B.width()  != A.length()) )
+        PLERROR("lapackCholeskySolveInPlace: matrix B length (%d) is "
+                "incompatible with the dimensions of A (%d)",
+                (B_is_column_major? B.width() : B.length()), A.length());
+
+    char lapack_uplo;
+    switch (uplo) {
+    case 'L':
+    case 'l':
+        lapack_uplo = 'U';
+        break;
+
+    case 'U':
+    case 'u':
+        lapack_uplo = 'L';
+        break;
+
+    default:
+        PLERROR("lapackCholeskySolveInPlace: unrecognized character '%c' for "
+                "argument 'uplo'; valid characters are 'U' and 'L'", uplo);
+    }
+
+    // If B is not column-major, transpose it
+    Mat lapack_B;
+    if (! B_is_column_major)
+        lapack_B = transpose(B);
+    else
+        lapack_B = B;
+
+    // Prepare for call to LAPACK
+    int N    = A.width();
+    int NRHS = lapack_B.length();   // Don't forget it's transposed for lapack
+    int LDA  = A.length();
+    int LDB  = lapack_B.width();
+    int INFO;
+    real* A_data = A.data();
+    real* B_data = lapack_B.data();
+
+    // Call LAPACK
+    lapack_Xpotrs_(&lapack_uplo, &N, &NRHS, A_data, &LDA, B_data, &LDB, &INFO);
+
+    if (INFO < 0)
+        PLERROR("lapackCholeskySolvePlace: implementation error; argument %d "
+                "to xPOTRS had an illegal value", -INFO);
+    PLASSERT( INFO == 0 );
+
+    // If B was not originally column-major, transpose back result from LAPACK
+    if (! B_is_column_major)
+        transpose(lapack_B, B);
+}
+
+
+
 Mat multivariate_normal(const Vec& mu, const Mat& A, int N)
 {
     Vec e_values;

Modified: trunk/plearn/math/plapack.h
===================================================================
--- trunk/plearn/math/plapack.h	2007-02-09 23:10:19 UTC (rev 6645)
+++ trunk/plearn/math/plapack.h	2007-02-11 00:18:56 UTC (rev 6646)
@@ -76,7 +76,19 @@
 inline void lapack_Xsygvx_(int* ITYPE, char* JOBZ, char* RANGE, char* UPLO, int* N, float* A, int* LDA, float* B, int* LDB, float* VL, float* VU, int* IL, int* IU, float* ABSTOL, int* M, float* W, float* Z, int* LDZ, float* WORK, int* LWORK, int* IWORK, int* IFAIL, int* INFO)
 { ssygvx_(ITYPE, JOBZ, RANGE, UPLO, N, A, LDA, B, LDB, VL, VU, IL, IU, ABSTOL, M, W, Z, LDZ, WORK, LWORK, IWORK, IFAIL, INFO); }
 
+inline void lapack_Xpotrf_(char* UPLO, int* N, float* A, int* LDA, int* INFO)
+{ spotrf_(UPLO, N, A, LDA, INFO); }
 
+inline void lapack_Xpotrf_(char* UPLO, int* N, double* A, int* LDA, int* INFO)
+{ dpotrf_(UPLO, N, A, LDA, INFO); }
+
+inline void lapack_Xpotrs_(char* UPLO, int* N, int* NRHS, float*  A, int* LDA, float*  B, int* LDB, int* INFO)
+{ spotrs_(UPLO, N, NRHS, A, LDA, B, LDB, INFO); }
+
+inline void lapack_Xpotrs_(char* UPLO, int* N, int* NRHS, double* A, int* LDA, double* B, int* LDB, int* INFO)
+{ dpotrs_(UPLO, N, NRHS, A, LDA, B, LDB, INFO); }
+
+
 //!  Computes the eigenvalues and eigenvectors of a symmetric (NxN) matrix A.
 //!  BEWARE: The content of A is destroyed by the call.
 //!  NOTE: you may wish to use the simpler call eigenVecOfSymmMat
@@ -619,6 +631,36 @@
 Vec constrainedLinearRegression(const Mat& Xt, const Vec& Y, real lambda=0.);
 
 
+/**
+ *  Call LAPACK to perform in-place Cholesky Decomposition of a square
+ *  SYMMETRIC matrix A.  Note that the matrix mod must equal its width in the
+ *  current implementation.  The argument uplo is a single character, which is
+ *  either 'L' if the lower-triangle should be considered (and the returned
+ *  cholesky is L * L') or 'U' if the upper-triangle should be considered (and
+ *  the returned cholesky is U' * U).  [[Implementation note: in the call to
+ *  LAPACK, we swap those letters in order to reflect the row-ordering
+ *  differences between PLearn and LAPACK.]]
+ */
+void lapackCholeskyDecompositionInPlace(Mat& A, char uplo='L');
+
+/**
+ *  Call LAPACK to solve in-place a linear system given its previously-computed
+ *  Cholesky decomposition.  The argument B contains the matrix of
+ *  right-hand-sides.  Since LAPACK is column-major, one can specify if 'B' is
+ *  ALREADY in column-major to avoid a transpose (which would otherwise be
+ *  performed automatically by this function).  The argument uplo is a single
+ *  character, which is either 'L' if the lower-triangle should be considered
+ *  (and the returned cholesky is L * L') or 'U' if the upper-triangle should
+ *  be considered (and the returned cholesky is U' * U).  [[Implementation
+ *  note: in the call to LAPACK, we swap those letters in order to reflect the
+ *  row-ordering differences between PLearn and LAPACK.]]
+ *
+ *  On return, B contains the solution matrix.  A is not modified and can be
+ *  reused in further calls to lapackCholeskySolveInPlace.
+ */
+void lapackCholeskySolveInPlace(Mat& A, Mat& B, bool B_is_column_major=false,
+                                char uplo='L');
+
 //! Compute the generalization error estimator called Generalized Cross-Validation (Craven & Wahba 1979),
 //! and the corresponding ridge regression weights in
 //!    min ||Y - X*W'||^2 + weight_decay ||W||^2.



From chapados at mail.berlios.de  Sun Feb 11 01:19:42 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Sun, 11 Feb 2007 01:19:42 +0100
Subject: [Plearn-commits] r6647 - trunk/plearn/var
Message-ID: <200702110019.l1B0Jgsa000819@sheep.berlios.de>

Author: chapados
Date: 2007-02-11 01:19:42 +0100 (Sun, 11 Feb 2007)
New Revision: 6647

Modified:
   trunk/plearn/var/GaussianProcessNLLVariable.cc
Log:
Use LAPACK if available (for the cholesky)

Modified: trunk/plearn/var/GaussianProcessNLLVariable.cc
===================================================================
--- trunk/plearn/var/GaussianProcessNLLVariable.cc	2007-02-11 00:18:56 UTC (rev 6646)
+++ trunk/plearn/var/GaussianProcessNLLVariable.cc	2007-02-11 00:19:42 UTC (rev 6647)
@@ -39,6 +39,10 @@
 #include <plearn/math/TMat_maths.h>
 #include "GaussianProcessNLLVariable.h"
 
+#ifdef USE_BLAS_SPECIALISATIONS
+#include <plearn/math/plapack.h>
+#endif
+
 namespace PLearn {
 using namespace std;
 
@@ -205,8 +209,9 @@
 }
 
 
-//#####  fbpropFragments  #####################################################
+//#####  fbpropFragments (NO LAPACK)  #########################################
 
+#ifndef USE_BLAS_SPECIALISATIONS
 void GaussianProcessNLLVariable::fbpropFragments(
     Kernel* kernel, real noise, const Mat& inputs, const Mat& targets,
     bool compute_inverse, Mat& gram, Mat& L, Mat& alpha, Mat& inv,
@@ -246,7 +251,55 @@
         alpha = alpha.subMatColumns(0, targetsize);
     }
 }
+#endif
 
+//#####  fbpropFragments (LAPACK)  ############################################
+
+#ifdef USE_BLAS_SPECIALISATIONS
+void GaussianProcessNLLVariable::fbpropFragments(
+    Kernel* kernel, real noise, const Mat& inputs, const Mat& targets,
+    bool compute_inverse, Mat& gram, Mat& L, Mat& alpha, Mat& inv,
+    Vec& tmp_chol, Mat& tmp_rhs)
+{
+    PLASSERT( kernel );
+    PLASSERT( inputs.length() == targets.length() );
+    const int trainlength = inputs.length();
+    const int targetsize  = targets.width();
+    
+    // The RHS matrix (when solving the linear system Gram*Params=RHS) is made
+    // up of two parts: the regression targets themselves, and the identity
+    // matrix if we requested them (for confidence intervals).  After solving
+    // the linear system, set the gram-inverse appropriately.
+    int rhs_width = targetsize + (compute_inverse? trainlength : 0);
+    tmp_rhs.resize(trainlength, rhs_width);
+    tmp_rhs.subMatColumns(0, targetsize) << targets;
+    if (compute_inverse) {
+        Mat rhs_identity = tmp_rhs.subMatColumns(targetsize, trainlength);
+        identityMatrix(rhs_identity);
+    }
+
+    // Compute Gram Matrix and add weight decay to diagonal
+    kernel->setDataForKernelMatrix(inputs);
+    gram.resize(trainlength, trainlength);
+    kernel->computeGramMatrix(gram);
+    addToDiagonal(gram, noise);
+
+    // Compute Cholesky decomposition and solve the linear system.  LAPACK
+    // solves in-place, but luckily we don't need either the Gram and RHS
+    // matrices after solving.  Note that for now we don't bother to create an
+    // appropriately transposed RHS (will come later).
+    lapackCholeskyDecompositionInPlace(gram);
+    lapackCholeskySolveInPlace(gram, tmp_rhs);
+    alpha = tmp_rhs;                         // LAPACK solves in-place
+    L     = gram;                            // LAPACK solves in-place
+    
+    if (compute_inverse) {
+        inv   = alpha.subMatColumns(targetsize, trainlength);
+        alpha = alpha.subMatColumns(0, targetsize);
+    }
+}
+#endif
+
 } // end of namespace PLearn
 
 



From lamblin at mail.berlios.de  Mon Feb 12 21:20:25 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Mon, 12 Feb 2007 21:20:25 +0100
Subject: [Plearn-commits] r6648 - trunk/plearn_learners/online
Message-ID: <200702122020.l1CKKPIh019725@sheep.berlios.de>

Author: lamblin
Date: 2007-02-12 21:20:24 +0100 (Mon, 12 Feb 2007)
New Revision: 6648

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
Log:
- comment out grad_weight_decay option, not implemented yet
- initialize random number generator with seed_



Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-11 00:19:42 UTC (rev 6647)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-12 20:20:24 UTC (rev 6648)
@@ -66,7 +66,7 @@
     recons_cost_index( -1 )
 
 {
-    random_gen = new PRandom();
+    random_gen = new PRandom( seed_ );
 }
 
 void DeepBeliefNet::declareOptions(OptionList& ol)
@@ -81,9 +81,11 @@
                   "The decrease constant of the learning rate used during"
                   " gradient descent");
 
+    /* NOT IMPLEMENTED YET
     declareOption(ol, "grad_weight_decay", &DeepBeliefNet::grad_weight_decay,
                   OptionBase::buildoption,
                   "The weight decay used during the gradient descent");
+    */
 
     declareOption(ol, "n_classes", &DeepBeliefNet::n_classes,
                   OptionBase::buildoption,

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2007-02-11 00:19:42 UTC (rev 6647)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2007-02-12 20:20:24 UTC (rev 6648)
@@ -73,8 +73,10 @@
     //! The decrease constant of the learning rate used during gradient descent
     real grad_decrease_ct;
 
+    /* NOT IMPLEMENTED YET
     //! The weight decay used during the gradient descent
     real grad_weight_decay;
+    */
 
     //! Number of classes in the training set
     //!   - 0 means we are doing regression,



From lamblin at mail.berlios.de  Mon Feb 12 21:23:04 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Mon, 12 Feb 2007 21:23:04 +0100
Subject: [Plearn-commits] r6649 - trunk/plearn_learners/online
Message-ID: <200702122023.l1CKN4ej019913@sheep.berlios.de>

Author: lamblin
Date: 2007-02-12 21:23:04 +0100 (Mon, 12 Feb 2007)
New Revision: 6649

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
Log:
I forgot to save before committing


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-12 20:20:24 UTC (rev 6648)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-12 20:23:04 UTC (rev 6649)
@@ -55,7 +55,7 @@
     cd_learning_rate( 0. ),
     grad_learning_rate( 0. ),
     grad_decrease_ct( 0. ),
-    grad_weight_decay( 0. ),
+    // grad_weight_decay( 0. ),
     use_classification_cost( true ),
     n_layers( 0 ),
     final_module_has_learning_rate( false ),



From lamblin at mail.berlios.de  Mon Feb 12 21:25:03 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Mon, 12 Feb 2007 21:25:03 +0100
Subject: [Plearn-commits] r6650 - trunk/plearn_learners/online
Message-ID: <200702122025.l1CKP3V6020117@sheep.berlios.de>

Author: lamblin
Date: 2007-02-12 21:25:03 +0100 (Mon, 12 Feb 2007)
New Revision: 6650

Modified:
   trunk/plearn_learners/online/Subsampling2DModule.cc
Log:
Fix build


Modified: trunk/plearn_learners/online/Subsampling2DModule.cc
===================================================================
--- trunk/plearn_learners/online/Subsampling2DModule.cc	2007-02-12 20:23:04 UTC (rev 6649)
+++ trunk/plearn_learners/online/Subsampling2DModule.cc	2007-02-12 20:25:03 UTC (rev 6650)
@@ -187,11 +187,12 @@
 
     // Build the learntoptions from the buildoptions
     input_images_size = input_images_length * input_images_width;
-    input_size = n_input_images * input_size;
+    input_size = n_input_images * input_images_size;
 
     output_images_length = input_images_length / kernel_length;
     output_images_width = input_images_width / kernel_width;
     output_images_size = output_images_length * output_images_width;
+    output_size = n_input_images * output_images_size;
 
     kernel_size = kernel_length * kernel_width;
 
@@ -203,6 +204,7 @@
 
     input_images.resize(n_input_images);
     output_images.resize(n_input_images);
+    kernel.resize(kernel_length, kernel_width);
     input_gradients.resize(n_input_images);
     output_gradients.resize(n_input_images);
     kernel_gradient.resize(kernel_length, kernel_width);



From chapados at mail.berlios.de  Tue Feb 13 01:21:28 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Tue, 13 Feb 2007 01:21:28 +0100
Subject: [Plearn-commits] r6651 - trunk/plearn/math
Message-ID: <200702130021.l1D0LS1e014629@sheep.berlios.de>

Author: chapados
Date: 2007-02-13 01:21:27 +0100 (Tue, 13 Feb 2007)
New Revision: 6651

Modified:
   trunk/plearn/math/ObservationWindow.cc
   trunk/plearn/math/VecStatsCollector.cc
Log:
Bug fixes in deep-copy

Modified: trunk/plearn/math/ObservationWindow.cc
===================================================================
--- trunk/plearn/math/ObservationWindow.cc	2007-02-12 20:25:03 UTC (rev 6650)
+++ trunk/plearn/math/ObservationWindow.cc	2007-02-13 00:21:27 UTC (rev 6651)
@@ -135,8 +135,8 @@
   
     //!  Otherwise call the copy constructor to obtain a copy
     ObservationWindow* deep_copy = new ObservationWindow(*this);
-    deepCopyField(this->m_observations, copies);
-    deepCopyField(this->m_obs_weights,  copies);
+    deepCopyField(deep_copy->m_observations, copies);
+    deepCopyField(deep_copy->m_obs_weights,  copies);
 
     //!  Put the copy in the map
     copies[this] = deep_copy;

Modified: trunk/plearn/math/VecStatsCollector.cc
===================================================================
--- trunk/plearn/math/VecStatsCollector.cc	2007-02-12 20:25:03 UTC (rev 6650)
+++ trunk/plearn/math/VecStatsCollector.cc	2007-02-13 00:21:27 UTC (rev 6651)
@@ -621,7 +621,7 @@
 
 void VecStatsCollector::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    Object::makeDeepCopyFromShallowCopy(copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(fieldnames,               copies);
     deepCopyField(stats,                    copies);
     deepCopyField(cov,                      copies);



From chapados at mail.berlios.de  Tue Feb 13 17:37:10 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Tue, 13 Feb 2007 17:37:10 +0100
Subject: [Plearn-commits] r6652 - trunk/plearn/math
Message-ID: <200702131637.l1DGbAgh029756@sheep.berlios.de>

Author: chapados
Date: 2007-02-13 17:37:10 +0100 (Tue, 13 Feb 2007)
New Revision: 6652

Modified:
   trunk/plearn/math/TVec_decl.h
Log:
Added method subVecSelf

Modified: trunk/plearn/math/TVec_decl.h
===================================================================
--- trunk/plearn/math/TVec_decl.h	2007-02-13 00:21:27 UTC (rev 6651)
+++ trunk/plearn/math/TVec_decl.h	2007-02-13 16:37:10 UTC (rev 6652)
@@ -326,6 +326,22 @@
         return subv;
     }
 
+    /**
+     *  Modify the current Vec to point to a subset of itself.  This is useful
+     *  if we are handed a 'large' Vec, but only want a subset without incurring
+     *  the performance hit of a temporary object.
+     */
+    void subVecSelf(int newstart, int newlength)
+    {
+#ifdef BOUNDCHECK
+        if(newstart+newlength>length() || newlength<0)
+            PLERROR("TVec::subVecSelf(int newstart, int newlength) OUT OF BOUNDS OR <0 length()"
+                    " length()=%d; newstart=%d; newlength=%d.", length(), newstart, newlength);
+#endif
+        length_ = newlength;
+        offset_ += newstart;
+    }
+
     /*! ************
       Deep copying
       ************



From chapados at mail.berlios.de  Tue Feb 13 17:37:53 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Tue, 13 Feb 2007 17:37:53 +0100
Subject: [Plearn-commits] r6653 - trunk/plearn/ker
Message-ID: <200702131637.l1DGbriO029862@sheep.berlios.de>

Author: chapados
Date: 2007-02-13 17:37:52 +0100 (Tue, 13 Feb 2007)
New Revision: 6653

Modified:
   trunk/plearn/ker/IIDNoiseKernel.cc
   trunk/plearn/ker/IIDNoiseKernel.h
   trunk/plearn/ker/Kernel.cc
   trunk/plearn/ker/Kernel.h
   trunk/plearn/ker/RationalQuadraticARDKernel.cc
   trunk/plearn/ker/RationalQuadraticARDKernel.h
Log:
More efficient derivatives (analytic) of the Gram matrix

Modified: trunk/plearn/ker/IIDNoiseKernel.cc
===================================================================
--- trunk/plearn/ker/IIDNoiseKernel.cc	2007-02-13 16:37:10 UTC (rev 6652)
+++ trunk/plearn/ker/IIDNoiseKernel.cc	2007-02-13 16:37:52 UTC (rev 6653)
@@ -36,6 +36,7 @@
 
 /*! \file IIDNoiseKernel.cc */
 
+#include <plearn/base/lexical_cast.h>
 #include <plearn/math/TMat_maths.h>
 #include "IIDNoiseKernel.h"
 
@@ -54,6 +55,18 @@
     "where delta_x,y is the Kronecker delta function, and sn2 is the exp of\n"
     "twice the 'log_noise_sigma' option.\n"
     "\n"
+    "In addition to comparing the complete x and y vectors, this kernel allows\n"
+    "adding a Kronecker delta when there is a match in only ONE DIMENSION.  This\n"
+    "may be generalized in the future to allow match according to a subset of\n"
+    "the input variables (but is not currently done for performance reasons).\n"
+    "With these terms, the kernel function takes the form:\n"
+    "\n"
+    "  k(x,y) = delta_x,y * sn2 + \\sum_i delta_x[kr(i)],y[kr(i)] * ks2[i]\n"
+    "\n"
+    "where kr(i) is the i-th element of 'kronecker_indexes' (representing an\n"
+    "index into the input vectors), and ks2[i] is the exp of twice the value of\n"
+    "the i-th element of the 'log_kronecker_sigma' option.\n"
+    "\n"
     "Note that to make its operations more robust when used with unconstrained\n"
     "optimization of hyperparameters, all hyperparameters of this kernel are\n"
     "specified in the log-domain.\n"
@@ -70,10 +83,21 @@
 void IIDNoiseKernel::declareOptions(OptionList& ol)
 {
     declareOption(
-        ol, "log_noise_sigma",
-        &IIDNoiseKernel::m_log_noise_sigma,
+        ol, "log_noise_sigma", &IIDNoiseKernel::m_log_noise_sigma,
         OptionBase::buildoption,
         "Log of the global noise variance.  Default value=0.0");
+
+    declareOption(
+        ol, "kronecker_indexes", &IIDNoiseKernel::m_kronecker_indexes,
+        OptionBase::buildoption,
+        "Element index in the input vectors that should be subject to additional\n"
+        "Kronecker delta terms");
+
+    declareOption(
+        ol, "log_kronecker_sigma", &IIDNoiseKernel::m_log_kronecker_sigma,
+        OptionBase::buildoption,
+        "Log of the noise variance terms for the Kronecker deltas associated\n"
+        "with kronecker_indexes");
     
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
@@ -93,17 +117,37 @@
 //#####  build_  ##############################################################
 
 void IIDNoiseKernel::build_()
-{ }
+{
+    if (m_kronecker_indexes.size() != m_log_kronecker_sigma.size())
+        PLERROR("IIDNoiseKernel::build_: size of 'kronecker_indexes' (%d) "
+                "does not match that of 'log_kronecker_sigma' (%d)",
+                m_kronecker_indexes.size(), m_log_kronecker_sigma.size());
+}
 
 
 //#####  evaluate  ############################################################
 
 real IIDNoiseKernel::evaluate(const Vec& x1, const Vec& x2) const
 {
-    if (fast_is_equal(powdistance(x1,x2,2), 0.0))
-        return exp(2*m_log_noise_sigma);
-    else
-        return 0.0;
+    // if (fast_is_equal(powdistance(x1,x2,2), 0.0))
+    //     return exp(2*m_log_noise_sigma);
+    // else
+    //     return 0.0;
+
+    real value = 0.0;
+    if (x1 == x2)
+        value += exp(2*m_log_noise_sigma);
+
+    const int n = m_kronecker_indexes.size();
+    if (n > 0) {
+        int*  cur_index = m_kronecker_indexes.data();
+        real* cur_sigma = m_log_kronecker_sigma.data();
+
+        for (int i=0 ; i<n ; ++i, ++cur_index, ++cur_sigma)
+            if (fast_is_equal(x1[*cur_index], x2[*cur_index]))
+                value += exp(2 * *cur_sigma);
+    }
+    return value;
 }
 
 
@@ -112,6 +156,9 @@
 void IIDNoiseKernel::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(m_kronecker_indexes,   copies);
+    deepCopyField(m_log_kronecker_sigma, copies);
 }
 
 
@@ -121,6 +168,7 @@
                                                  real epsilon) const
 {
     static const string LNS("log_noise_sigma");
+    static const string LKS("log_kronecker_sigma[");
     if (kernel_param == LNS) {
         if (!data)
             PLERROR("Kernel::computeGramMatrixDerivative should be called only after "
@@ -137,10 +185,63 @@
         for (int i=0 ; i<W ; ++i)
             KD(i,i) = deriv;
     }
+    else if (string_begins_with(kernel_param, LKS) &&
+             kernel_param[kernel_param.size()-1] == ']')
+    {
+        int arg = tolong(kernel_param.substr(
+                             LKS.size(), kernel_param.size() - LKS.size() - 1));
+        PLASSERT( arg < m_kronecker_indexes.size() );
+
+        computeGramMatrixDerivNV<
+            IIDNoiseKernel, &IIDNoiseKernel::derivKronecker>(KD, this, arg);
+        
+        // int W = nExamples();
+        // KD.resize(W,W);
+        // real deriv = 2*exp(2*m_log_kronecker_sigma[arg]);
+        // int index  = m_kronecker_indexes[arg];
+        // 
+        // Vec row_i;
+        // Vec row_j;
+        // int m = KD.mod();
+        // real* KDi;                           // Start of row i
+        // real* KDji;                          // Start of column i
+        // for (int i=0 ; i<W ; ++i) {
+        //     KDi = KD[i];
+        //     KDji = &KD[0][i];
+        //     dataRow(i, row_i);
+        //     real row_i_index = row_i[index];
+        //     for (int j=0 ; j<=i ; ++j, KDji += m) {
+        //         dataRow(j, row_j);
+        //         real KDij;
+        //         if (fast_is_equal(row_i_index, row_j[index]))
+        //             KDij = deriv;
+        //         else
+        //             KDij = 0.0;
+        // 
+        //         *KDi++ = KDij;
+        //         if (j < i)
+        //             *KDji = KDij;
+        //     }
+        // }
+    }
     else
         inherited::computeGramMatrixDerivative(KD, kernel_param, epsilon);
 }
 
+
+//#####  derivKronecker  ######################################################
+
+real IIDNoiseKernel::derivKronecker(const Vec& row_i, const Vec& row_j, real K,
+                                    int arg) const
+{
+    int index = m_kronecker_indexes[arg];
+    if (fast_is_equal(row_i[index], row_j[index]))
+        return 2*exp(2*m_log_kronecker_sigma[arg]);
+    else
+        return 0.0;
+}
+
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/ker/IIDNoiseKernel.h
===================================================================
--- trunk/plearn/ker/IIDNoiseKernel.h	2007-02-13 16:37:10 UTC (rev 6652)
+++ trunk/plearn/ker/IIDNoiseKernel.h	2007-02-13 16:37:52 UTC (rev 6653)
@@ -40,7 +40,7 @@
 #ifndef IIDNoiseKernel_INC
 #define IIDNoiseKernel_INC
 
-#include <plearn/ker/Kernel.h>
+#include <plearn/ker/MemoryCachedKernel.h>
 
 namespace PLearn {
 
@@ -56,19 +56,39 @@
  *  where delta_x,y is the Kronecker delta function, and sn2 is the exp of
  *  twice the 'log_noise_sigma' option.
  *
+ *  In addition to comparing the complete x and y vectors, this kernel allows
+ *  adding a Kronecker delta when there is a match in only ONE DIMENSION.  This
+ *  may be generalized in the future to allow match according to a subset of
+ *  the input variables (but is not currently done for performance reasons).
+ *  With these terms, the kernel function takes the form:
+ *
+ *    k(x,y) = delta_x,y * sn2 + \sum_i delta_x[kr(i)],y[kr(i)] * ks2[i]
+ *
+ *  where kr(i) is the i-th element of 'kronecker_indexes' (representing an
+ *  index into the input vectors), and ks2[i] is the exp of twice the value of
+ *  the i-th element of the 'log_kronecker_sigma' option.
+ *
  *  Note that to make its operations more robust when used with unconstrained
  *  optimization of hyperparameters, all hyperparameters of this kernel are
  *  specified in the log-domain.
  */
-class IIDNoiseKernel : public Kernel
+class IIDNoiseKernel : public MemoryCachedKernel
 {
-    typedef Kernel inherited;
+    typedef MemoryCachedKernel inherited;
 
 public:
     //#####  Public Build Options  ############################################
 
     //! Log of the global noise variance.  Default value=0.0
     real m_log_noise_sigma;
+
+    //! Element index in the input vectors that should be subject to additional
+    //! Kronecker delta terms
+    TVec<int> m_kronecker_indexes;
+
+    //! Log of the noise variance terms for the Kronecker deltas associated
+    //! with kronecker_indexes
+    Vec m_log_kronecker_sigma;
     
 public:
     //#####  Public Member Functions  #########################################
@@ -102,6 +122,9 @@
     //! Declares the class options.
     static void declareOptions(OptionList& ol);
 
+    //! Derivative function with respect to kronecker_indexes[arg] hyperparameter
+    real derivKronecker(const Vec& row_i, const Vec& row_j, real K, int arg) const;
+    
 private:
     //! This does the actual building.
     void build_();

Modified: trunk/plearn/ker/Kernel.cc
===================================================================
--- trunk/plearn/ker/Kernel.cc	2007-02-13 16:37:10 UTC (rev 6652)
+++ trunk/plearn/ker/Kernel.cc	2007-02-13 16:37:52 UTC (rev 6653)
@@ -326,6 +326,10 @@
         K << gram_matrix;
         return;
     }
+    if (K.length() != data.length() || K.width() != data.length())
+        PLERROR("Kernel::computeGramMatrix: the argument matrix K should be\n"
+                "of size %d x %d (currently of size %d x %d)",
+                data.length(), data.length(), K.length(), K.width());
     int l=data->length();
     int m=K.mod();
     PP<ProgressBar> pb;

Modified: trunk/plearn/ker/Kernel.h
===================================================================
--- trunk/plearn/ker/Kernel.h	2007-02-13 16:37:10 UTC (rev 6652)
+++ trunk/plearn/ker/Kernel.h	2007-02-13 16:37:52 UTC (rev 6653)
@@ -88,7 +88,7 @@
 
     PLEARN_DECLARE_ABSTRACT_OBJECT(Kernel);
 
-    //!  ** Subclasses must overload this method **
+    //!  ** Subclasses must override this method **
     virtual real evaluate(const Vec& x1, const Vec& x2) const = 0; //!<  returns K(x1,x2) 
 
     //!  ** Subclasses may override these methods to provide efficient kernel matrix access **
@@ -169,12 +169,12 @@
     virtual void computeGramMatrixDerivative(Mat& KD, const string& kernel_param,
                                              real epsilon=1e-6) const;
     
-    //!  ** Subclasses may overload these methods ** 
+    //!  ** Subclasses may override these methods ** 
     //!  They provide a generic way to set and retrieve kernel parameters
     virtual void setParameters(Vec paramvec); //!<  default version produces an error
     virtual Vec getParameters() const; //!<  default version returns an empty Vec
 
-    //!  ** Subclasses should NOT overload the following methods. The default versions are fine. **
+    //!  ** Subclasses should NOT override the following methods. The default versions are fine. **
 
     void apply(VMat m1, VMat m2, Mat& result) const; //!<  result(i,j) = K(m1(i),m2(j))
     Mat apply(VMat m1, VMat m2) const; //!<  same as above, but returns the result mat instead

Modified: trunk/plearn/ker/RationalQuadraticARDKernel.cc
===================================================================
--- trunk/plearn/ker/RationalQuadraticARDKernel.cc	2007-02-13 16:37:10 UTC (rev 6652)
+++ trunk/plearn/ker/RationalQuadraticARDKernel.cc	2007-02-13 16:37:52 UTC (rev 6653)
@@ -51,11 +51,11 @@
     "Similar to C.E. Rasmussen's GPML code (see http://www.gaussianprocess.org),\n"
     "this kernel is specified as:\n"
     "\n"
-    "  k(x,y) = sf2 * [1 + (sum_i (x_i - y_i)^2 / w_i)/(2*alpha)]^(-alpha) + delta_x,y*sn2\n"
+    "  k(x,y) = sf2 * [1 + (sum_i (x_i - y_i)^2 / w_i)/(2*alpha)]^(-alpha) + k_iid(x,y)\n"
     "\n"
-    "where sf2 is the exp of twice the 'log_signal_sigma' option, sn2 is the\n"
-    "exp of twice the 'log_noise_sigma' option (added only if x==y), and w_i\n"
-    "is exp(2*log_global_sigma + 2*log_input_sigma[i]).\n"
+    "where sf2 is the exp of twice the 'log_signal_sigma' option, w_i is\n"
+    "exp(2*log_global_sigma + 2*log_input_sigma[i]), and k_iid(x,y) is the\n"
+    "result of IIDNoiseKernel kernel evaluation.\n"
     "\n"
     "Note that to make its operations more robust when used with unconstrained\n"
     "optimizaiton of hyperparameters, all hyperparameters of this kernel are\n"
@@ -100,6 +100,14 @@
 { }
 
 
+//#####  makeDeepCopyFromShallowCopy  #########################################
+
+void RationalQuadraticARDKernel::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+}
+
+
 //#####  evaluate  ############################################################
 
 real RationalQuadraticARDKernel::evaluate(const Vec& x1, const Vec& x2) const
@@ -108,7 +116,7 @@
     PLASSERT( !m_log_input_sigma.size() || x1.size() == m_log_input_sigma.size() );
 
     if (x1.size() == 0)
-        return exp(2*m_log_signal_sigma) + exp(2*m_log_noise_sigma);
+        return exp(2*m_log_signal_sigma) + inherited::evaluate(x1,x2);
     
     const real* px1 = x1.data();
     const real* px2 = x2.data();
@@ -136,21 +144,131 @@
         }
     }
 
-    // We add a noise variance only if x and y are equal (within machine tolerance)
-    real noise_cov = 0.0;
-    if (is_equal(sum_sqdiff, 0))
-        noise_cov = exp(2*m_log_noise_sigma);
+    // We add the noise covariance as well
+    real noise_cov = inherited::evaluate(x1,x2);
     return sf2 * pow(1 + sum_wt / (2.*alpha), -alpha) + noise_cov;
 }
 
 
-//#####  makeDeepCopyFromShallowCopy  #########################################
+//#####  computeGramMatrix  ###################################################
 
-void RationalQuadraticARDKernel::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+void RationalQuadraticARDKernel::computeGramMatrix(Mat K) const
 {
-    inherited::makeDeepCopyFromShallowCopy(copies);
+    computeGramMatrixNV(K, this);
 }
 
+
+//#####  computeGramMatrixDerivative  #########################################
+
+void RationalQuadraticARDKernel::computeGramMatrixDerivative(
+    Mat& KD, const string& kernel_param, real epsilon) const
+{
+    static const string LSS("log_sigmal_sigma");
+    static const string LGS("log_global_sigma");
+    static const string LIS("log_input_sigma[");
+    static const string LAL("log_alpha");
+
+    if (kernel_param == LSS) {
+        computeGramMatrixDerivNV<
+            RationalQuadraticARDKernel,
+            &RationalQuadraticARDKernel::derivLogSignalSigma>(KD, this, -1);
+    }
+    else if (kernel_param == LGS) {
+        computeGramMatrixDerivNV<
+            RationalQuadraticARDKernel,
+            &RationalQuadraticARDKernel::derivLogGlobalSigma>(KD, this, -1);
+    }
+    else if (string_begins_with(kernel_param, LIS) &&
+             kernel_param[kernel_param.size()-1] == ']')
+    {
+        int arg = tolong(kernel_param.substr(
+                             LIS.size(), kernel_param.size() - LIS.size() - 1));
+        PLASSERT( arg < m_log_input_sigma.size() );
+
+        computeGramMatrixDerivNV<
+            RationalQuadraticARDKernel,
+            &RationalQuadraticARDKernel::derivLogInputSigma>(KD, this, arg);
+    }
+    else if (kernel_param == LAL) {
+        computeGramMatrixDerivNV<
+            RationalQuadraticARDKernel,
+            &RationalQuadraticARDKernel::derivLogAlpha>(KD, this, -1);
+    }
+    else
+        inherited::computeGramMatrixDerivative(KD, kernel_param, epsilon);
+
+    // Compare against finite differences
+    // Mat KD1;
+    // Kernel::computeGramMatrixDerivative(KD1, kernel_param, epsilon);
+    // cerr << "Kernel hyperparameter: " << kernel_param << endl;
+    // cerr << "Analytic derivative (1st row):" << endl
+    //      << KD(0) << endl
+    //      << "Finite differences:" << endl
+    //      << KD1(0) << endl;
+}
+
+
+//#####  derivLogSignalSigma  #################################################
+
+real RationalQuadraticARDKernel::derivLogSignalSigma(
+    const Vec& row_i, const Vec& row_j, real K, int arg) const
+{
+    real noise = inherited::evaluate(row_i, row_j);
+    return 2*(K-noise);
+}
+
+
+//#####  derivLogGlobalSigma  #################################################
+
+real RationalQuadraticARDKernel::derivLogGlobalSigma(
+    const Vec& row_i, const Vec& row_j, real K, int arg) const
+{
+    // The rational quadratic gives us:
+    //     K = exp(2*s)*k^(-alpha).
+    // Rederive the value of k
+    real alpha = exp(m_log_alpha);
+    real noise = inherited::evaluate(row_i, row_j);
+    K -= noise;
+    real k     = exp(- (pl_log(K) - 2*m_log_signal_sigma) / alpha);
+    real inner = - (k - 1) * alpha;
+    return -0.5 * (K / k) * inner;
+}
+
+
+//#####  derivLogInputSigma  ##################################################
+
+real RationalQuadraticARDKernel::derivLogInputSigma(
+    const Vec& row_i, const Vec& row_j, real K, int arg) const
+{
+    // The rational quadratic gives us:
+    //     K = exp(2*s)*k^(-alpha).
+    // Rederive the value of k
+    real alpha   = exp(m_log_alpha);
+    real noise   = inherited::evaluate(row_i, row_j);
+    K -= noise;
+    real k       = exp(- (pl_log(K) - 2*m_log_signal_sigma) / alpha);
+    real diff    = row_i[arg] - row_j[arg];
+    real sq_diff = diff * diff;
+    return (K / k) * exp(-2 * (m_log_global_sigma + m_log_input_sigma[arg])) * sq_diff;
+}
+
+
+//#####  derivLogAlpha  #######################################################
+
+real RationalQuadraticARDKernel::derivLogAlpha(
+    const Vec& row_i, const Vec& row_j, real K, int arg) const
+{
+    real alpha = exp(m_log_alpha);
+    real noise = inherited::evaluate(row_i, row_j);
+    K -= noise;
+    real k     = exp(- (pl_log(K) - 2*m_log_signal_sigma) / alpha);
+    real left  = - alpha * pl_log(k);
+    real num   = (k - 1) * 2 * alpha;
+    real denum = 2 * k;
+    return K * (left + num / denum);
+}
+
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/ker/RationalQuadraticARDKernel.h
===================================================================
--- trunk/plearn/ker/RationalQuadraticARDKernel.h	2007-02-13 16:37:10 UTC (rev 6652)
+++ trunk/plearn/ker/RationalQuadraticARDKernel.h	2007-02-13 16:37:52 UTC (rev 6653)
@@ -54,11 +54,11 @@
  *  Similar to C.E. Rasmussen's GPML code (see http://www.gaussianprocess.org),
  *  this kernel is specified as:
  *
- *    k(x,y) = sf2 * [1 + (sum_i (x_i - y_i)^2 / w_i)/(2*alpha)]^(-alpha) + delta_x,y*sn2
+ *    k(x,y) = sf2 * [1 + (sum_i (x_i - y_i)^2 / w_i)/(2*alpha)]^(-alpha) + k_iid(x,y)
  *
- *  where sf2 is the exp of twice the 'log_signal_sigma' option, sn2 is the
- *  exp of twice the 'log_noise_sigma' option (added only if x==y), and w_i
- *  is exp(2*log_global_sigma + 2*log_input_sigma[i]).
+ *  where sf2 is the exp of twice the 'log_signal_sigma' option, w_i is
+ *  exp(2*log_global_sigma + 2*log_input_sigma[i]), and k_iid(x,y) is the
+ *  result of IIDNoiseKernel kernel evaluation.
  *
  *  Note that to make its operations more robust when used with unconstrained
  *  optimizaiton of hyperparameters, all hyperparameters of this kernel are
@@ -87,7 +87,16 @@
     //! Compute K(x1,x2).
     virtual real evaluate(const Vec& x1, const Vec& x2) const;
 
+    //! Compute entire Gram matrix
+    virtual void computeGramMatrix(Mat K) const;
 
+    //! Compute the derivative of the Gram matrix with respect to one of the
+    //! kernel's parameters.  Analytic derivatives are implemented for this
+    //! kernel.
+    virtual void computeGramMatrixDerivative(Mat& KD, const string& kernel_param,
+                                             real epsilon=1e-6) const;
+    
+
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.
@@ -103,6 +112,18 @@
     //! Declares the class options.
     static void declareOptions(OptionList& ol);
 
+    //! Derivative function with respect to log_signal_sigma
+    real derivLogSignalSigma(const Vec& row_i, const Vec& row_j, real K, int arg) const;
+
+    //! Derivative function with respect to log_global_sigma
+    real derivLogGlobalSigma(const Vec& row_i, const Vec& row_j, real K, int arg) const;
+    
+    //! Derivative function with respect to log_input_sigma[arg]
+    real derivLogInputSigma(const Vec& row_i, const Vec& row_j, real K, int arg) const;
+    
+    //! Derivative function with respect to log_alpha
+    real derivLogAlpha(const Vec& row_i, const Vec& row_j, real K, int arg) const;
+    
 private:
     //! This does the actual building.
     void build_();



From chrish at mail.berlios.de  Tue Feb 13 17:46:03 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Tue, 13 Feb 2007 17:46:03 +0100
Subject: [Plearn-commits] r6654 - trunk/python_modules/plearn/math
Message-ID: <200702131646.l1DGk3S2030412@sheep.berlios.de>

Author: chrish
Date: 2007-02-13 17:46:03 +0100 (Tue, 13 Feb 2007)
New Revision: 6654

Modified:
   trunk/python_modules/plearn/math/StatsCollector.py
Log:
* Add comments about suspicious behavior of ARGMIN and ARGMAX computations.


Modified: trunk/python_modules/plearn/math/StatsCollector.py
===================================================================
--- trunk/python_modules/plearn/math/StatsCollector.py	2007-02-13 16:37:52 UTC (rev 6653)
+++ trunk/python_modules/plearn/math/StatsCollector.py	2007-02-13 16:46:03 UTC (rev 6654)
@@ -141,6 +141,8 @@
         ma_min     = ma[ma_argmin, range(width)]
         min_newpos = argmin(array([self.min, ma_min]), 0).astype('Bool')
         self.min[min_newpos]    = ma_min[min_newpos]
+        # XXX Argmin computation needs to be revised! Does not work, at least
+        # when passing array of shape (1,1).
         self.argmin[min_newpos] = ma_argmin[min_newpos] + initial_n[min_newpos]
 
         ## Update (arg)max / make sure old argmax is kept if not updated
@@ -148,6 +150,9 @@
         ma_max     = ma[ma_argmax, range(width)]
         max_newpos = argmax(array([self.max, ma_max]), 0).astype('Bool')
         self.max[max_newpos]    = ma_max[max_newpos]
+        # XXX Argmax computation needs to be revised! Does not work, at least
+        # when passing array of shape (1,1). Also, is the use of min_newpos
+        # correct?
         self.argmax[max_newpos] = ma_argmax[max_newpos] + initial_n[min_newpos]
 
 



From chapados at mail.berlios.de  Tue Feb 13 17:57:58 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Tue, 13 Feb 2007 17:57:58 +0100
Subject: [Plearn-commits] r6655 - trunk/plearn/ker
Message-ID: <200702131657.l1DGvwHv031508@sheep.berlios.de>

Author: chapados
Date: 2007-02-13 17:57:57 +0100 (Tue, 13 Feb 2007)
New Revision: 6655

Added:
   trunk/plearn/ker/MemoryCachedKernel.cc
   trunk/plearn/ker/MemoryCachedKernel.h
Log:
Kernel that implements some caching utilities to avoid virtual calls

Added: trunk/plearn/ker/MemoryCachedKernel.cc
===================================================================
--- trunk/plearn/ker/MemoryCachedKernel.cc	2007-02-13 16:46:03 UTC (rev 6654)
+++ trunk/plearn/ker/MemoryCachedKernel.cc	2007-02-13 16:57:57 UTC (rev 6655)
@@ -0,0 +1,160 @@
+// -*- C++ -*-
+
+// MemoryCachedKernel.cc
+//
+// Copyright (C) 2007 Nicolas Chapados
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Nicolas Chapados
+
+/*! \file MemoryCachedKernel.cc */
+
+
+#include "MemoryCachedKernel.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_ABSTRACT_OBJECT(
+    MemoryCachedKernel,
+    "Provide some memory-management utilities for kernels.",
+    "This class is intended as a base class to provide some memory management\n"
+    "utilities for the data-matrix set with setDataForKernelMatrix function.  In\n"
+    "particular, it provides a single (inline, non-virtual) function to access a\n"
+    "given input vector of the data matrix.  If the data VMatrix passed to\n"
+    "setDataForKernelMatrix is within a certain size threshold, the VMatrix is\n"
+    "converted to a Mat and cached to memory (without requiring additional space\n"
+    "if the VMatrix is actually a MemoryVMatrix), and all further element access\n"
+    "are done without requiring virtual function calls.\n"
+    "\n"
+    "IMPORTANT NOTE: the 'cache_gram_matrix' option is enabled automatically by\n"
+    "default for this class.  This makes the computation of the Gram matrix\n"
+    "derivatives (with respect to kernel hyperparameters) quite faster in many\n"
+    "cases.  If you really don't want this caching to occur, just set it\n"
+    "explicitly to false.\n"
+    "\n"
+    "This class also provides utility functions to derived classes to compute\n"
+    "the Gram matrix and its derivative (with respect to kernel hyperparameters)\n"
+    "without requiring virtual function calls in data access or evaluation\n"
+    "function.\n"
+    );
+
+
+//#####  MemoryCachedKernel::MemoryCachedKernel  ##############################
+
+MemoryCachedKernel::MemoryCachedKernel()
+    : m_cache_threshold(1000000)
+{
+    cache_gram_matrix = true;
+}
+
+
+//#####  declareOptions  ######################################################
+
+void MemoryCachedKernel::declareOptions(OptionList& ol)
+{
+    declareOption(
+        ol, "cache_threshold", &MemoryCachedKernel::m_cache_threshold,
+        OptionBase::buildoption,
+        "Threshold on the number of elements to cache the data VMatrix into a\n"
+        "real matrix.  Above this threshold, the VMatrix is left as-is, and\n"
+        "element access remains virtual.  (Default value = 1000000)\n");
+    
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+
+//#####  build  ###############################################################
+
+void MemoryCachedKernel::build()
+{
+    // ### Nothing to add here, simply calls build_
+    inherited::build();
+    build_();
+}
+
+
+//#####  build_  ##############################################################
+
+void MemoryCachedKernel::build_()
+{ }
+
+
+//#####  makeDeepCopyFromShallowCopy  #########################################
+
+void MemoryCachedKernel::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(m_data_cache, copies);
+}
+
+
+//#####  setDataForKernelMatrix  ##############################################
+
+void MemoryCachedKernel::setDataForKernelMatrix(VMat the_data)
+{
+    inherited::setDataForKernelMatrix(the_data);
+
+    if (the_data.width() * the_data.length() <= m_cache_threshold &&
+        the_data.isNotNull())
+    {
+        m_data_cache = the_data.toMat();
+    }
+    else
+        m_data_cache = Mat();
+}
+
+
+//#####  addDataForKernelMatrix  ##############################################
+
+void MemoryCachedKernel::addDataForKernelMatrix(const Vec& newrow)
+{
+    inherited::addDataForKernelMatrix(newrow);
+
+    if (m_data_cache.isNotNull())
+        m_data_cache.appendRow(newrow);
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/ker/MemoryCachedKernel.h
===================================================================
--- trunk/plearn/ker/MemoryCachedKernel.h	2007-02-13 16:46:03 UTC (rev 6654)
+++ trunk/plearn/ker/MemoryCachedKernel.h	2007-02-13 16:57:57 UTC (rev 6655)
@@ -0,0 +1,318 @@
+// -*- C++ -*-
+
+// MemoryCachedKernel.h
+//
+// Copyright (C) 2007 Nicolas Chapados
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Nicolas Chapados
+
+/*! \file MemoryCachedKernel.h */
+
+
+#ifndef MemoryCachedKernel_INC
+#define MemoryCachedKernel_INC
+
+#include <plearn/ker/Kernel.h>
+
+namespace PLearn {
+
+/**
+ *  Provide some memory-management utilities for kernels.
+ *
+ *  This class is intended as a base class to provide some memory management
+ *  utilities for the data-matrix set with setDataForKernelMatrix function.  In
+ *  particular, it provides a single (inline, non-virtual) function to access a
+ *  given input vector of the data matrix.  If the data VMatrix passed to
+ *  setDataForKernelMatrix is within a certain size threshold, the VMatrix is
+ *  converted to a Mat and cached to memory (without requiring additional space
+ *  if the VMatrix is actually a MemoryVMatrix), and all further element access
+ *  are done without requiring virtual function calls.
+ *
+ *  IMPORTANT NOTE: the 'cache_gram_matrix' option is enabled automatically by
+ *  default for this class.  This makes the computation of the Gram matrix
+ *  derivatives (with respect to kernel hyperparameters) quite faster in many
+ *  cases.  If you really don't want this caching to occur, just set it
+ *  explicitly to false.
+ *
+ *  This class also provides utility functions to derived classes to compute
+ *  the Gram matrix and its derivative (with respect to kernel hyperparameters)
+ *  without requiring virtual function calls in data access or evaluation
+ *  function.
+ */
+class MemoryCachedKernel : public Kernel
+{
+    typedef Kernel inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    /**
+     *  Threshold on the number of elements to cache the data VMatrix into a
+     *  real matrix.  Above this threshold, the VMatrix is left as-is, and
+     *  element access remains virtual.  (Default value = 1000000)
+     */
+    int m_cache_threshold;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    MemoryCachedKernel();
+
+
+    //#####  Kernel Member Functions  #########################################
+
+    //! Optionally cache the data to a real Mat if its number of elements lies
+    //! within the threshold.
+    virtual void setDataForKernelMatrix(VMat the_data);
+
+    //! Update the cache if a new row is added to the data
+    virtual void addDataForKernelMatrix(const Vec& newRow);
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_ABSTRACT_OBJECT(MemoryCachedKernel);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+    /**
+     *  Interface for derived classes: access row i of the data matrix.  Note:
+     *  the contents of the Vec SHOULD ABSOLUTELY NOT BE MODIFIED after calling
+     *  this function.  For performance, the Vec may not contain a copy of the
+     *  input vector, but may point to the original data
+     */
+    inline void dataRow(int i, Vec& row) const;
+
+    /**
+     *  Interface to ease derived-class implementation of computeGramMatrix
+     *  that avoids virtual function calls in kernel evaluation.  The
+     *  computeGramMatrixNV function should be called directly by the
+     *  implementation of computeGramMatrix in a derived class, passing the
+     *  name of the derived class as a template argument.
+     */
+    template <class DerivedClass>
+    void computeGramMatrixNV(Mat K, const DerivedClass* This) const;
+
+    /**
+     *  Interface to ease derived-class implementation of
+     *  computeGramMatrixDerivative, that avoids virtual function calls as much
+     *  as possible.  This template is instantiated with a member function
+     *  pointer in the derived class to compute the actual element-wise
+     *  derivative (with respect to some kernel hyperparameter, depending on
+     *  which a different member pointer is passed).  Both GCC 3.3.6 and 4.0.3
+     *  (which I tested on) generate very efficient code to call a member
+     *  function passed as a template argument within a loop [although the
+     *  generated code looks very different in both cases].
+     *
+     *  The member function is called with the following arguments:
+     *
+     *  - x1  : element at row i of the data matrix
+     *  - x2  : element at row j of the data matrix
+     *  - K   : kernel value for (x1,x2); obtained from cache if available
+     *  - arg : integer argument passed to the function; may be used to index
+     *          into a vector of hyperparameters
+     *
+     *  The last argument to computeGramMatrixDerivNV,
+     *  'derivative_func_requires_K', specifies whether the derivativeFunc
+     *  requires the value of K in order to compute the derivative.  Passing
+     *  the value 'false' can avoid unnecessary kernel computations in cases
+     *  where the Gram matrix is not cached.  In this case, the derivativeFunc
+     *  is called with a MISSING_VALUE for its argument K.
+     */
+    template <class DerivedClass,
+              real (DerivedClass::*derivativeFunc)(const Vec&, const Vec&, real, int) const>
+    void computeGramMatrixDerivNV(Mat& KD, const DerivedClass* This, int arg,
+                                  bool derivative_func_requires_K = true) const;
+    
+    
+private:
+    //! This does the actual building.
+    void build_();
+
+private:
+    //! In-memory cache of the data matrix
+    Mat m_data_cache;
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(MemoryCachedKernel);
+
+
+//#####  dataRow  #############################################################
+
+inline void MemoryCachedKernel::dataRow(int i, Vec& row) const
+{
+    if (m_data_cache.isNotNull()) {
+        row = m_data_cache(i);
+        row.subVecSelf(0, dataInputsize());
+    }
+    else {
+        row.resize(dataInputsize());
+        data->getSubRow(i, 0, row);
+    }
+}
+
+
+//#####  computeGramMatrixNV  #################################################
+
+template <class DerivedClass>
+void MemoryCachedKernel::computeGramMatrixNV(Mat K, const DerivedClass* This) const
+{
+    if (!data)
+        PLERROR("Kernel::computeGramMatrix: setDataForKernelMatrix not yet called");
+    if (!is_symmetric)
+        PLERROR("Kernel::computeGramMatrix: not supported for non-symmetric kernels");
+    if (K.length() != data.length() || K.width() != data.length())
+        PLERROR("Kernel::computeGramMatrix: the argument matrix K should be\n"
+                "of size %d x %d (currently of size %d x %d)",
+                data.length(), data.length(), K.length(), K.width());
+    if (cache_gram_matrix && gram_matrix_is_cached) {
+        K << gram_matrix;
+        return;
+    }
+                
+    int l=data->length();
+    int m=K.mod();
+    PP<ProgressBar> pb;
+    int count = 0;
+    if (report_progress)
+        pb = new ProgressBar("Computing Gram matrix for " + classname(),
+                             (l * (l + 1)) / 2);
+
+    Vec row_i, row_j;
+    real Kij;
+    real* Ki;
+    real* Kji;
+    for (int i=0 ; i<l ; ++i) {
+        Ki = K[i];
+        Kji = &K[0][i];
+        dataRow(i, row_i);
+        for (int j=0; j<=i; ++j, Kji += m) {
+            dataRow(j, row_j);
+            Kij = This->DerivedClass::evaluate(row_i, row_j);
+            *Ki++ = Kij;
+            if (j<i)
+                *Kji = Kij;
+        }
+        if (report_progress) {
+            count += i + 1;
+            PLASSERT( pb );
+            pb->update(count);
+        }
+    }
+    if (cache_gram_matrix) {
+        gram_matrix.resize(l,l);
+        gram_matrix << K;
+        gram_matrix_is_cached = true;
+    }
+}
+
+
+//#####  computeGramMatrixDerivNV  ############################################
+
+template <class DerivedClass,
+          real (DerivedClass::*derivativeFunc)(const Vec&, const Vec&, real, int) const>
+void MemoryCachedKernel::computeGramMatrixDerivNV(Mat& KD, const DerivedClass* This,
+                                                  int arg, bool require_K) const
+{
+    if (!data)
+        PLERROR("Kernel::computeGramMatrixDerivative: "
+                "setDataForKernelMatrix not yet called");
+    if (!is_symmetric)
+        PLERROR("Kernel::computeGramMatrixDerivative: "
+                "not supported for non-symmetric kernels");
+
+    int W = nExamples();
+    KD.resize(W,W);
+    int m=KD.mod();
+    
+    Vec row_i, row_j;
+    real KDij;
+    real* KDi;
+    real* KDji;
+    real  K  = MISSING_VALUE;
+    real* Ki = 0;                       // Current row of kernel matrix, if cached
+
+    for (int i=0 ; i<W ; ++i) {
+        KDi  = KD[i];
+        KDji = &KD[0][i];
+        dataRow(i, row_i);
+        if (gram_matrix_is_cached)
+            Ki = gram_matrix[i];
+        
+        for (int j=0 ; j <= i ; ++j, KDji += m) {
+            dataRow(j, row_j);
+
+            // Access the current kernel value depending on whether it's cached
+            if (Ki)
+                K = *Ki++;
+            else if (require_K)
+                K = This->DerivedClass::evaluate(row_i, row_j);
+
+            // Compute and store the derivative
+            KDij   = (This->*derivativeFunc)(row_i, row_j, K, arg);
+            *KDi++ = KDij;
+            if (j < i)
+                *KDji = KDij;
+        }
+    }
+}
+
+
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From chrish at mail.berlios.de  Tue Feb 13 18:09:35 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Tue, 13 Feb 2007 18:09:35 +0100
Subject: [Plearn-commits] r6656 - trunk/plearn_learners/generic
Message-ID: <200702131709.l1DH9ZuC032514@sheep.berlios.de>

Author: chrish
Date: 2007-02-13 18:09:34 +0100 (Tue, 13 Feb 2007)
New Revision: 6656

Modified:
   trunk/plearn_learners/generic/VPLProcessor.cc
Log:
* Fix more VPLProcessor bogosity: when using input_prg, only pass 
  the *input* column names to the VMatLanguage instance responsible for running
  the input program. That way, the VPLProcessor can process inputs with, say,
  different sets of targets.
* Add @todo comments on other methods that look like they might also be
  problematic.
* Fix comments and add whitespace to improve readability.


Modified: trunk/plearn_learners/generic/VPLProcessor.cc
===================================================================
--- trunk/plearn_learners/generic/VPLProcessor.cc	2007-02-13 16:57:57 UTC (rev 6655)
+++ trunk/plearn_learners/generic/VPLProcessor.cc	2007-02-13 17:09:34 UTC (rev 6656)
@@ -124,9 +124,9 @@
 
 void VPLProcessor::build_()
 {
-    if(train_set.isNull() && (orig_inputsize>0 || orig_targetsize>0) ) // we're probably reloading a saved VPLProcessor
+    // We're probably reloading a saved VPLProcessor
+    if(train_set.isNull() && (orig_inputsize>0 || orig_targetsize>0) )
         initializeInputPrograms();
-
 }
 
 void VPLProcessor::build()
@@ -136,16 +136,12 @@
 }
 
 
+/// @todo Why is only input_prg deep-copied?
 void VPLProcessor::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
-    // ### Call deepCopyField on all "pointer-like" fields 
-    // ### that you wish to be deepCopied rather than 
-    // ### shallow-copied.
-
     input_prg_.makeDeepCopyFromShallowCopy(copies);
- 
 
     deepCopyField(input_prg_fieldnames, copies);
     deepCopyField(processed_input, copies);
@@ -154,7 +150,7 @@
 
 int VPLProcessor::outputsize() const
 {
-    if(!input_prg.empty())
+    if (!input_prg.empty())
         return input_prg_fieldnames.length();
 
     return inputsize();
@@ -165,13 +161,15 @@
     inherited::forget();
     stage = 0;
 }
-    
+
+/// @todo Why is this called initializeInputPrograms() and not
+/// initializePrograms()? Why is there no initialization done for
+/// fitlering_prg, target_prg, weight_prg and extra_prg?
 void VPLProcessor::initializeInputPrograms()
 {
-    if(!input_prg.empty())
+    if (!input_prg.empty())
     {
-        //input_prg_.setSourceFieldNames(orig_fieldnames.subVec(0,orig_inputsize));
-        input_prg_.setSourceFieldNames(orig_fieldnames);
+        input_prg_.setSourceFieldNames(orig_fieldnames.subVec(0,orig_inputsize));
         input_prg_.compileString(input_prg, input_prg_fieldnames);
     }
     else
@@ -194,10 +192,11 @@
 
     VMat filtered_trainset = training_set;
     PPath filtered_trainset_metadatadir = getExperimentDirectory() / "filtered_train_set.metadata";
-    if(!filtering_prg.empty())
+    if (!filtering_prg.empty())
         filtered_trainset = new FilteredVMatrix(training_set, filtering_prg, filtered_trainset_metadatadir, verbosity>1,
                                                 use_filtering_prg_for_repeat, repeat_id_field_name, repeat_count_field_name);
 
+    // XXX The next line does nothing!
     VMat processed_trainset = new ProcessingVMatrix(filtered_trainset, input_prg, target_prg, weight_prg, extra_prg);
     inherited::setTrainingSet(training_set, call_forget); // will call forget if needed
 }
@@ -206,7 +205,7 @@
 {
     VMat filtered_dataset = dataset;
     PPath filtered_dataset_metadatadir = getExperimentDirectory() / "filtered_dataset.metadata";
-    if(!filtering_prg.empty())
+    if (!filtering_prg.empty())
         filtered_dataset = new FilteredVMatrix(dataset, filtering_prg, filtered_dataset_metadatadir, verbosity>1,
                                                use_filtering_prg_for_repeat, repeat_id_field_name, repeat_count_field_name);
 
@@ -230,12 +229,14 @@
     
     string processing_target_prg = target_prg;
     if (processing_target_prg.empty() && dataset->targetsize() > 0) {
-        processing_target_prg = "[%" + tostring(start_of_targets) + ":%" + tostring(start_of_weights-1) + "]";
+        processing_target_prg = "[%" + tostring(start_of_targets) + ":%" +
+            tostring(start_of_weights-1) + "]";
     }
 
     string processing_weight_prg = weight_prg;
     if (processing_weight_prg.empty() && dataset->weightsize() > 0) {
-        processing_weight_prg = "[%" + tostring(start_of_weights) + ":%" + tostring(start_of_extras-1) + "]";
+        processing_weight_prg = "[%" + tostring(start_of_weights) + ":%" +
+            tostring(start_of_extras-1) + "]";
     }
 
     string processing_extras_prg = extra_prg;
@@ -251,8 +252,8 @@
 void VPLProcessor::computeOutput(const Vec& input, Vec& output) const
 {
     output.resize(outputsize());
-    Vec newinput= input;
-    if(!input_prg.empty())
+    Vec newinput = input;
+    if (!input_prg.empty())
     {
         processed_input.resize(input_prg_fieldnames.length());
         input_prg_.run(input, processed_input);
@@ -273,19 +274,21 @@
 }
 
 void VPLProcessor::computeCostsFromOutputs(const Vec& input, const Vec& output, 
-                                                     const Vec& target, Vec& costs) const
+                                           const Vec& target, Vec& costs) const
 { 
     Vec nonconst_output = output; // to make the constipated compiler happy
     computeOutputAndCosts(input, target, nonconst_output, costs); 
 }
 
+/// @todo What is that outpuy_prg_ doing commented out? Is the fieldnames for
+/// the input_prg really the right thing to return for *output* names?
 TVec<string> VPLProcessor::getOutputNames() const
 {
-    if(!input_prg.empty())//output_prg_)
+    if (!input_prg.empty())//output_prg_)
         return input_prg_fieldnames;
 
     VMat trainset= getTrainingSet();
-    if(trainset==0)
+    if (trainset==0)
         PLERROR("in VPLProcessor::getOutputNames: no train set specified yet.");
 
     return trainset->inputFieldNames();



From chapados at mail.berlios.de  Tue Feb 13 19:44:33 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Tue, 13 Feb 2007 19:44:33 +0100
Subject: [Plearn-commits] r6657 -
	trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results
Message-ID: <200702131844.l1DIiX9r016590@sheep.berlios.de>

Author: chapados
Date: 2007-02-13 19:44:32 +0100 (Tue, 13 Feb 2007)
New Revision: 6657

Modified:
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
Log:
New test results following analytic derivatives (very slight numerical changes)

Modified: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2007-02-13 17:09:34 UTC (rev 6656)
+++ trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2007-02-13 18:44:32 UTC (rev 6657)
@@ -4,15 +4,20 @@
 !R 0 
 !R 1 GaussianProcessRegressor(
 kernel = *1 ->RationalQuadraticARDKernel(
-log_alpha = 1.0454759217536389 ;
-log_signal_sigma = 1.78885962610151594 ;
+log_alpha = 1.04547586264741277 ;
+log_signal_sigma = 1.78885963703595285 ;
 log_global_sigma = 0 ;
-log_input_sigma = 1 [ 1.60502563724931702 ] ;
-log_noise_sigma = -1.08496085286010091 ;
+log_input_sigma = 1 [ 1.60502566389644263 ] ;
+log_noise_sigma = -1.08496084417144156 ;
+kronecker_indexes = []
+;
+log_kronecker_sigma = []
+;
+cache_threshold = 1000000 ;
 is_symmetric = 1 ;
 report_progress = 0 ;
 specify_dataset = *0 ;
-cache_gram_matrix = 0 ;
+cache_gram_matrix = 1 ;
 data_inputsize = 1 ;
 n_examples = 5  )
 ;
@@ -37,19 +42,19 @@
 nstages = 1  )
 ;
 alpha = 5  1  [ 
--1.44801819162290024 	
--1.75870011045946795 	
-2.97790716927227539 	
-0.366193225658293153 	
--0.252937646496436019 	
+-1.44801815670686862 	
+-1.75870009663165083 	
+2.97790711086592319 	
+0.366193237472126842 	
+-0.252937647804560684 	
 ]
 ;
 gram_inverse = 5  5  [ 
-2.3750455877628629 	0.318202839946681582 	-2.79501466140384469 	0.183693865043761062 	-0.0157606149450654383 	
-0.318202839946681137 	2.9165070423025945 	-3.00531385230735149 	-0.300764945104235126 	0.0237240821333623043 	
--2.79501466140384469 	-3.00531385230735149 	5.75002906618107712 	0.0451407476338803876 	-0.00106974531844977044 	
-0.183693865043760979 	-0.300764945104235071 	0.0451407476338803806 	0.105748934100125966 	-0.0132961075182882887 	
--0.0157606149450654383 	0.0237240821333623009 	-0.00106974531844977066 	-0.0132961075182882853 	0.0298730636742724452 	
+2.37504560445266355 	0.318202770254115641 	-2.79501461899015302 	0.183693875492433906 	-0.0157606163667142161 	
+0.318202770254115308 	2.91650706588821063 	-3.00531379641274965 	-0.300764955773723752 	0.0237240838912132698 	
+-2.79501461899015258 	-3.00531379641274965 	5.75002897142370362 	0.0451407441539973669 	-0.00106974501214030199 	
+0.183693875492433878 	-0.300764955773723752 	0.0451407441539973669 	0.105748937745051619 	-0.0132961086237992979 	
+-0.0157606163667142196 	0.0237240838912132768 	-0.00106974501214030242 	-0.0132961086237992996 	0.0298730633663767363 	
 ]
 ;
 target_mean = 1 [ 10 ] ;
@@ -87,18 +92,18 @@
 nservers = 0 ;
 save_trainingset_prefix = ""  )
 
-!R 1 1 [ 15.0000000000000071 ] 
-!R 1 1 [ 14.4027409228245133 ] 
+!R 1 1 [ 15 ] 
+!R 1 1 [ 14.4027409350913036 ] 
 !R 2 4  1  [ 
-13.5000588445143688 	
-14.425999686387911 	
-15.0000000000000071 	
-14.4027409228245133 	
+13.5000588443909812 	
+14.4259996839628197 	
+15 	
+14.4027409350913036 	
 ]
 1 [ 4  4  [ 
-0.495755881017678368 	0.296446868109264017 	-1.49213974509621039e-13 	-0.618033363536593328 	
-0.296446868109470074 	0.372151242547516026 	-1.20792265079217032e-13 	-0.47227821255051694 	
--6.39488462184090167e-14 	-2.06057393370429054e-13 	9.9999076294443514e-09 	-1.06581410364015028e-13 	
--0.618033363536468983 	-0.472278212550524046 	-5.68434188608080149e-14 	2.53894720061439383 	
+0.495755869965555596 	0.296446857385134876 	5.68434188608080149e-14 	-0.61803333064101551 	
+0.296446857385205931 	0.372151235349497844 	5.68434188608080149e-14 	-0.472278178376861035 	
+1.27897692436818033e-13 	6.39488462184090167e-14 	1.00000497379915034e-08 	6.39488462184090167e-14 	
+-0.618033330640987089 	-0.472278178376878799 	4.26325641456060112e-14 	2.53894706592205122 	
 ]
 ] 



From chapados at mail.berlios.de  Tue Feb 13 23:21:04 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Tue, 13 Feb 2007 23:21:04 +0100
Subject: [Plearn-commits] r6658 - trunk/plearn/math
Message-ID: <200702132221.l1DML4Ee001775@sheep.berlios.de>

Author: chapados
Date: 2007-02-13 23:21:03 +0100 (Tue, 13 Feb 2007)
New Revision: 6658

Modified:
   trunk/plearn/math/TMat_decl.h
   trunk/plearn/math/TMat_impl.h
Log:
Partially out-of-line TMat::resize in order to ensure that the common case is indeed inlined (and does not overflow the compiler overflow threshold)

Modified: trunk/plearn/math/TMat_decl.h
===================================================================
--- trunk/plearn/math/TMat_decl.h	2007-02-13 18:44:32 UTC (rev 6657)
+++ trunk/plearn/math/TMat_decl.h	2007-02-13 22:21:03 UTC (rev 6658)
@@ -180,96 +180,46 @@
     TMatColRowsIterator<T> col_end(int column);
   
 
-    /*!     Resizes the matrix to a new length() and width()
-      Notice that the previous structure of the data in the matrix
-      is not preserved if you increase the width() beyond mod().
-      The underlying storage is never shrunk, and it is grown only if necessary.
-      When grown, it is grown with extra entries to anticipate further resizes.
-      If preserve_content is true then a change of mod_ triggers a COPY
-      of the old entries so that their old value remains accessed at the same indices.
-    */
+    /**
+     *  Resizes the matrix to a new length() and width().
+     *
+     *  Note that the previous structure of the data in the matrix is not
+     *  preserved if you increase the width() beyond mod().  The underlying
+     *  storage is never shrunk, and it is grown only if necessary.  When
+     *  grown, it is grown with extra entries to anticipate further resizes.
+     *  If preserve_content is true then a change of mod_ triggers a COPY of
+     *  the old entries so that their old value remains accessed at the same
+     *  indices.
+     *
+     *  This function is split into several parts: a `small' one that handles
+     *  the common cases, and a few `auxiliary' ones that perform the
+     *  heavy-lifting.  The small one is easily inlined, whereas having one
+     *  `large' resize() function would overflow the compiler inlining
+     *  threshold, yielding no inlining whatsoever.
+     */
     void resize(int new_length, int new_width, int extra=0, bool preserve_content=false)
     {
 #ifdef BOUNDCHECK
-        if(new_length<0 || new_width<0)
-            PLERROR("IN TMat::resize(int new_length, int new_width)\nInvalid arguments (<0)");
+        resizeBoundCheck(new_length, new_width);
 #endif
-        if (new_length==length_ && new_width==width_) return;
-        if(storage.isNull())
+        if (new_length==length_ && new_width==width_)
+            return;
+        else if(storage.isNull())
         {
             offset_ = 0;
             length_ = new_length;
-            width_ = new_width;
-            mod_ = new_width;
-            storage = new Storage<T>(length()*mod());
+            width_  = new_width;
+            mod_    = new_width;
+            storage = new Storage<T>(length()*mod() + extra);
         }
         else
         {
-            int usage=storage->usage();
-            if(usage>1 && new_width > mod()-offset_%mod())
-                PLERROR("IN TMat::resize(int new_length, int new_width) - For safety "
-                        "reasons, increasing the width() beyond mod()-offset_ modulo "
-                        "mod() is not allowed when the storage is shared with others");
-            if (preserve_content && size()>0)
-            {
-                int new_size = new_length*MAX(mod(),new_width);
-                int new_offset = usage>1?offset_:0;
-                if(new_size>storage->length() || new_width>mod())
-                {
-                    int extracols=0, extrarows=0;
-                    if (extra>min(new_width,new_length))
-                    {
-                        // if width has increased, bet that it will increase again in the future,
-                        // similarly for length,  so allocate the extra as extra mod
-                        float l=float(length_), l1=float(new_length),
-							  w=float(width_),  w1=float(new_width),
-							  x=float(extra);
-                        // Solve the following equations to apportion the extra 
-                        // while keeping the same percentage increase in width and length:
-                        //   Solve[{x+w1*l1==w2*l2,(w2/w1 - 1)/(l2/l1 - 1) == (w1/w - 1)/(l1/l - 1)},{w2,l2}]
-                        // This is a quadratic system which has two solutions: {w2a,l2a} and {w2b,l2b}:
-                        float w2a = 
-                            w1*(-1 - l1/(l - l1) + w1/w + (l1*w1)/(l*w - l1*w) + 
-                                (2*l*(-w + w1)*x)/
-                                (2*l*l1*w*w1 - l1*l1*w*w1 - l*l1*w1*w1 + 
-                                 sqrt(square(l1*l1*w*w1 - l*l1*w1*w1) + 
-                                      4*l*(l - l1)*l1*w*(w - w1)*w1*(l1*w1 + x))));
-                        float l2a = -(-l1*l1*w*w1 + l*l1*w1*w1 + 
-                                      sqrt(square(l1*l1*w*w1 - l*l1*w1*w1) + 
-                                           4*l*(l - l1)*l1*w*(w - w1)*w1*(l1*w1 + x)))/(2*l*(w - w1)*w1);
-                        float w2b =w1*(-1 - l1/(l - l1) + w1/w + (l1*w1)/(l*w - l1*w) - 
-                                       (2*l*(-w + w1)*x)/
-                                       (-2*l*l1*w*w1 + l1*l1*w*w1 + l*l1*w1*w1 + 
-                                        sqrt(square(l1*l1*w*w1 - l*l1*w1*w1) + 
-                                             4*l*(l - l1)*l1*w*(w - w1)*w1*(l1*w1 + x))));
-                        float l2b = (l1*l1*w*w1 - l*l1*w1*w1 + 
-                                     sqrt(square(l1*l1*w*w1 - l*l1*w1*w1) + 
-                                          4*l*(l - l1)*l1*w*(w - w1)*w1*(l1*w1 + x)))/(2*l*(w - w1)*w1);
-
-                        // pick one that is feasible and maximizes the mod
-                        if (w2b>w2a && w2b>w1 && l2b>l1)
-                        {
-                            extracols=int(ceil(w2b-w1));
-                            extrarows=int(ceil(l2b-l1));
-                        } else if (w2a>w1 && l2a>l1)
-                        {
-                            extrarows=int(ceil(l2a-l1));
-                            extracols=int(ceil(w2a-w1));
-                        } else // no valid solution to the system of equation, use a heuristic
-                        {
-                            extracols = max(0,int(ceil(sqrt(real(extra))/new_length)));
-                            extrarows = max(0,int((extra+l1*w1)/(w1+extracols) - l1));
-                        }
-
-                    }
-                    storage->resizeMat(new_length,new_width,extrarows,extracols,
-                                       new_offset,mod_,length_,width_,offset_);
-                    mod_ = new_width + extracols;
-                }
-                offset_ = new_offset;
-            }
-            else
-            {
+            int usage = storage->usage();
+            if (usage > 1 && new_width > mod()-offset_%mod())
+                resizeModError();
+            else if (preserve_content && size() > 0)
+                resizePreserve(new_length, new_width, extra);
+            else {
                 // 'new_size' takes into account the ABSOLUTELY REQUIRED size
                 // to hold the elements of the matrix.  We only resize the
                 // underlying storage when the latter is not big enough to hold
@@ -972,6 +922,15 @@
         input(in); 
     }
 
+protected:
+    //! Utility function to resize a matrix while preserving contents
+    void resizePreserve(int new_length, int new_width, int extra=0);
+
+    //! Perform bound-checking on resize
+    inline void resizeBoundCheck(int new_length, int new_width);
+
+    //! Report PLERROR if we resize changing the mod with usage > 1
+    void resizeModError();
 };
 
 typedef TMat<real> Mat;

Modified: trunk/plearn/math/TMat_impl.h
===================================================================
--- trunk/plearn/math/TMat_impl.h	2007-02-13 18:44:32 UTC (rev 6657)
+++ trunk/plearn/math/TMat_impl.h	2007-02-13 22:21:03 UTC (rev 6658)
@@ -334,6 +334,82 @@
     }
 }
 
+template <class T>
+void TMat<T>::resizePreserve(int new_length, int new_width, int extra)
+{
+    int usage      = storage->usage();
+    int new_size   = new_length*MAX(mod(),new_width);
+    int new_offset = usage>1?offset_:0;
+    if (new_size>storage->length() || new_width>mod())
+    {
+        int extracols=0, extrarows=0;
+        if (extra>min(new_width,new_length))
+        {
+            // if width has increased, bet that it will increase again in the future,
+            // similarly for length,  so allocate the extra as extra mod
+            float l=float(length_), l1=float(new_length),
+                w=float(width_),  w1=float(new_width),
+                x=float(extra);
+            // Solve the following equations to apportion the extra 
+            // while keeping the same percentage increase in width and length:
+            //   Solve[{x+w1*l1==w2*l2,(w2/w1 - 1)/(l2/l1 - 1) == (w1/w - 1)/(l1/l - 1)},{w2,l2}]
+            // This is a quadratic system which has two solutions: {w2a,l2a} and {w2b,l2b}:
+            float w2a = 
+                w1*(-1 - l1/(l - l1) + w1/w + (l1*w1)/(l*w - l1*w) + 
+                    (2*l*(-w + w1)*x)/
+                    (2*l*l1*w*w1 - l1*l1*w*w1 - l*l1*w1*w1 + 
+                     sqrt(square(l1*l1*w*w1 - l*l1*w1*w1) + 
+                          4*l*(l - l1)*l1*w*(w - w1)*w1*(l1*w1 + x))));
+            float l2a = -(-l1*l1*w*w1 + l*l1*w1*w1 + 
+                          sqrt(square(l1*l1*w*w1 - l*l1*w1*w1) + 
+                               4*l*(l - l1)*l1*w*(w - w1)*w1*(l1*w1 + x)))/(2*l*(w - w1)*w1);
+            float w2b =w1*(-1 - l1/(l - l1) + w1/w + (l1*w1)/(l*w - l1*w) - 
+                           (2*l*(-w + w1)*x)/
+                           (-2*l*l1*w*w1 + l1*l1*w*w1 + l*l1*w1*w1 + 
+                            sqrt(square(l1*l1*w*w1 - l*l1*w1*w1) + 
+                                 4*l*(l - l1)*l1*w*(w - w1)*w1*(l1*w1 + x))));
+            float l2b = (l1*l1*w*w1 - l*l1*w1*w1 + 
+                         sqrt(square(l1*l1*w*w1 - l*l1*w1*w1) + 
+                              4*l*(l - l1)*l1*w*(w - w1)*w1*(l1*w1 + x)))/(2*l*(w - w1)*w1);
+
+            // pick one that is feasible and maximizes the mod
+            if (w2b>w2a && w2b>w1 && l2b>l1) {
+                extracols=int(ceil(w2b-w1));
+                extrarows=int(ceil(l2b-l1));
+            }
+            else if (w2a>w1 && l2a>l1) {
+                extrarows=int(ceil(l2a-l1));
+                extracols=int(ceil(w2a-w1));
+            }
+            else { // no valid solution to the system of equation, use a heuristic
+                extracols = max(0,int(ceil(sqrt(real(extra))/new_length)));
+                extrarows = max(0,int((extra+l1*w1)/(w1+extracols) - l1));
+            }
+
+        }
+        storage->resizeMat(new_length,new_width,extrarows,extracols,
+                           new_offset,mod_,length_,width_,offset_);
+        mod_ = new_width + extracols;
+    }
+    offset_ = new_offset;
+}
+
+template <class T>
+inline void TMat<T>::resizeBoundCheck(int new_length, int new_width)
+{
+    if(new_length<0 || new_width<0)
+        PLERROR("IN TMat::resize(int new_length, int new_width)\nInvalid arguments (<0)");
+}
+
+template <class T>
+void TMat<T>::resizeModError()
+{
+    PLERROR("IN TMat::resize(int new_length, int new_width) - For safety "
+            "reasons, increasing the width() beyond mod()-offset_ modulo "
+            "mod() is not allowed when the storage is shared with others");
+}
+
+
 // Deep copying
 
 template<class T>



From chapados at mail.berlios.de  Tue Feb 13 23:21:25 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Tue, 13 Feb 2007 23:21:25 +0100
Subject: [Plearn-commits] r6659 - trunk/plearn/var
Message-ID: <200702132221.l1DMLPDF001825@sheep.berlios.de>

Author: chapados
Date: 2007-02-13 23:21:25 +0100 (Tue, 13 Feb 2007)
New Revision: 6659

Modified:
   trunk/plearn/var/GaussianProcessNLLVariable.cc
Log:
Faster version of bprop

Modified: trunk/plearn/var/GaussianProcessNLLVariable.cc
===================================================================
--- trunk/plearn/var/GaussianProcessNLLVariable.cc	2007-02-13 22:21:03 UTC (rev 6658)
+++ trunk/plearn/var/GaussianProcessNLLVariable.cc	2007-02-13 22:21:25 UTC (rev 6659)
@@ -192,16 +192,34 @@
         m_kernel->computeGramMatrixDerivative(m_gram_derivative,
                                               m_hyperparam_names[j]);
         for (int i=0, n=m_alpha.width() ; i<n ; ++i) {
-            Mat curalpha = m_alpha.column(i);
-            real curtrace = 0;
+            Mat curalpha_mat = m_alpha.column(i);
+            int curalpha_mod = curalpha_mat.mod();
+            real* curalpha   = curalpha_mat[0];
+            real  curtrace   = 0.0;
 
             // Sum over all rows and columns of matrix
-            for (int row=0, nrows=m_inverse_gram.length() ; row<nrows ; ++row)
-                for (int col=0, ncols=m_inverse_gram.width() ; col<ncols ; ++col)
+            real* curalpha_row = curalpha;
+            for (int row=0, nrows=m_inverse_gram.length()
+                     ; row<nrows ; ++row, curalpha_row += curalpha_mod)
+            {
+                real* p_inverse_gram    = m_inverse_gram[row];
+                real* p_gram_derivative = m_gram_derivative[row];
+                real  curalpha_row      = curalpha[row * curalpha_mod];
+                real* curalpha_col      = curalpha;
+
+                for (int col=0, ncols=m_inverse_gram.width()
+                         ; col<ncols ; ++col, curalpha_col += curalpha_mod)
+                {
                     curtrace +=
-                        (m_inverse_gram(row,col) - curalpha(row,0)*curalpha(col,0))
-                        * m_gram_derivative(row,col);
+                        (*p_inverse_gram++ - curalpha_row * *curalpha_col)
+                        * *p_gram_derivative++;
 
+                    // curtrace +=
+                    //     (m_inverse_gram(row,col) - curalpha(row,0)*curalpha(col,0))
+                    //     * m_gram_derivative(row,col);
+                }
+            }
+
             dnll_dj += curtrace / 2.0;
         }
         m_hyperparam_vars[j]->gradient[0] += dnll_dj * gradient[0];



From chapados at mail.berlios.de  Thu Feb 15 01:53:13 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Thu, 15 Feb 2007 01:53:13 +0100
Subject: [Plearn-commits] r6660 - trunk/plearn/ker
Message-ID: <200702150053.l1F0rD8V003300@sheep.berlios.de>

Author: chapados
Date: 2007-02-15 01:53:12 +0100 (Thu, 15 Feb 2007)
New Revision: 6660

Modified:
   trunk/plearn/ker/ARDBaseKernel.cc
   trunk/plearn/ker/ARDBaseKernel.h
   trunk/plearn/ker/IIDNoiseKernel.cc
   trunk/plearn/ker/IIDNoiseKernel.h
   trunk/plearn/ker/Kernel.cc
   trunk/plearn/ker/MemoryCachedKernel.cc
   trunk/plearn/ker/MemoryCachedKernel.h
   trunk/plearn/ker/RationalQuadraticARDKernel.cc
   trunk/plearn/ker/RationalQuadraticARDKernel.h
Log:
Some massive performance improvements in GaussianProcessRegressor, esp. RationalQuadraticARDKernel; some experiments run 20 times faster than last week.

Modified: trunk/plearn/ker/ARDBaseKernel.cc
===================================================================
--- trunk/plearn/ker/ARDBaseKernel.cc	2007-02-13 22:21:25 UTC (rev 6659)
+++ trunk/plearn/ker/ARDBaseKernel.cc	2007-02-15 00:53:12 UTC (rev 6660)
@@ -109,6 +109,7 @@
     inherited::makeDeepCopyFromShallowCopy(copies);
 
     deepCopyField(m_log_input_sigma, copies);
+    deepCopyField(m_input_sigma,     copies);
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn/ker/ARDBaseKernel.h
===================================================================
--- trunk/plearn/ker/ARDBaseKernel.h	2007-02-13 22:21:25 UTC (rev 6659)
+++ trunk/plearn/ker/ARDBaseKernel.h	2007-02-15 00:53:12 UTC (rev 6660)
@@ -102,6 +102,10 @@
     //! Declares the class options.
     static void declareOptions(OptionList& ol);
 
+    //! Buffer for final input sigma (add both global and per-input terms);
+    //! Can be used by derived class.
+    mutable Vec m_input_sigma;
+    
 private:
     //! This does the actual building.
     void build_();

Modified: trunk/plearn/ker/IIDNoiseKernel.cc
===================================================================
--- trunk/plearn/ker/IIDNoiseKernel.cc	2007-02-13 22:21:25 UTC (rev 6659)
+++ trunk/plearn/ker/IIDNoiseKernel.cc	2007-02-15 00:53:12 UTC (rev 6660)
@@ -151,6 +151,74 @@
 }
 
 
+//#####  computeGramMatrix  ###################################################
+
+void IIDNoiseKernel::computeGramMatrix(Mat K) const
+{
+    if (!data)
+        PLERROR("Kernel::computeGramMatrix: setDataForKernelMatrix not yet called");
+    if (!is_symmetric)
+        PLERROR("Kernel::computeGramMatrix: not supported for non-symmetric kernels");
+    if (K.length() != data.length() || K.width() != data.length())
+        PLERROR("Kernel::computeGramMatrix: the argument matrix K should be\n"
+                "of size %d x %d (currently of size %d x %d)",
+                data.length(), data.length(), K.length(), K.width());
+                
+    PLASSERT( K.size() == 0 || m_data_cache.size() > 0 );  // Ensure data cached OK
+
+    // Precompute some terms
+    real noise_sigma  = exp(2 * m_log_noise_sigma);
+    m_kronecker_sigma.resize(m_log_kronecker_sigma.size());
+    m_kronecker_sigma << m_log_kronecker_sigma;
+    m_kronecker_sigma *= 2.0;
+    exp(m_kronecker_sigma, m_kronecker_sigma);
+
+    // Prepare kronecker iteration
+    int   kronecker_num     = m_kronecker_indexes.size();
+    int*  kronecker_indexes = m_kronecker_indexes.data();
+    real* kronecker_sigma   = m_kronecker_sigma.data();
+
+    // Compute Gram Matrix
+    int  l = data->length();
+    int  m = K.mod();
+    int  cache_mod = m_data_cache.mod();
+
+    real *data_start = &m_data_cache(0,0);
+    real Kij;
+    real *Ki, *Kji;
+    real *xi = data_start;
+    
+    for (int i=0 ; i<l ; ++i, xi += cache_mod) {
+        Ki  = K[i];
+        Kji = &K[0][i];
+        real *xj = data_start;
+
+        for (int j=0; j<=i; ++j, Kji += m, xj += cache_mod) {
+            // Kernel evaluation per se
+            if (i == j)
+                Kij = noise_sigma;
+            else
+                Kij = 0.0;
+
+            // Kronecker terms
+            if (kronecker_num > 0) {
+                int*  cur_index = kronecker_indexes;
+                real* cur_sigma = kronecker_sigma;
+                
+                for (int k=0 ; k<kronecker_num ; ++k, ++cur_index, ++cur_sigma)
+                    if (fast_is_equal(xi[*cur_index], xj[*cur_index]))
+                        Kij += *cur_sigma;
+            }
+            
+            // Fill upper triangle if not on diagonal
+            *Ki++ = Kij;
+            if (j < i)
+                *Kji = Kij;
+        }
+    }
+}
+
+
 //#####  makeDeepCopyFromShallowCopy  #########################################
 
 void IIDNoiseKernel::makeDeepCopyFromShallowCopy(CopiesMap& copies)
@@ -159,6 +227,7 @@
 
     deepCopyField(m_kronecker_indexes,   copies);
     deepCopyField(m_log_kronecker_sigma, copies);
+    deepCopyField(m_kronecker_sigma,     copies);
 }
 
 
@@ -231,10 +300,11 @@
 
 //#####  derivKronecker  ######################################################
 
-real IIDNoiseKernel::derivKronecker(const Vec& row_i, const Vec& row_j, real K,
-                                    int arg) const
+real IIDNoiseKernel::derivKronecker(int i, int j, int arg, real K) const
 {
-    int index = m_kronecker_indexes[arg];
+    int index  = m_kronecker_indexes[arg];
+    Vec& row_i = *dataRow(i);
+    Vec& row_j = *dataRow(j);
     if (fast_is_equal(row_i[index], row_j[index]))
         return 2*exp(2*m_log_kronecker_sigma[arg]);
     else

Modified: trunk/plearn/ker/IIDNoiseKernel.h
===================================================================
--- trunk/plearn/ker/IIDNoiseKernel.h	2007-02-13 22:21:25 UTC (rev 6659)
+++ trunk/plearn/ker/IIDNoiseKernel.h	2007-02-15 00:53:12 UTC (rev 6660)
@@ -102,6 +102,10 @@
     //! Compute K(x1,x2).
     virtual real evaluate(const Vec& x1, const Vec& x2) const;
 
+    //! Compute the Gram Matrix.  Note that this version DOES NOT CACHE
+    //! the results, since it is usually called by derived classes.
+    virtual void computeGramMatrix(Mat K) const;
+    
     //! Directly compute the derivative with respect to hyperparameters
     //! (Faster than finite differences...)
     virtual void computeGramMatrixDerivative(Mat& KD, const string& kernel_param,
@@ -123,7 +127,11 @@
     static void declareOptions(OptionList& ol);
 
     //! Derivative function with respect to kronecker_indexes[arg] hyperparameter
-    real derivKronecker(const Vec& row_i, const Vec& row_j, real K, int arg) const;
+    real derivKronecker(int i, int j, int arg, real K) const;
+
+protected:
+    //! Buffer for exponential of m_log_kronecker_sigma
+    mutable Vec m_kronecker_sigma;
     
 private:
     //! This does the actual building.

Modified: trunk/plearn/ker/Kernel.cc
===================================================================
--- trunk/plearn/ker/Kernel.cc	2007-02-13 22:21:25 UTC (rev 6659)
+++ trunk/plearn/ker/Kernel.cc	2007-02-15 00:53:12 UTC (rev 6660)
@@ -40,7 +40,10 @@
  * This file is part of the PLearn library.
  ******************************************************* */
 
+#define PL_LOG_MODULE_NAME "Kernel"
+
 #include "Kernel.h"
+#include <plearn/io/pl_log.h>
 #include <plearn/base/lexical_cast.h>
 #include <plearn/base/tostring.h>
 #include <plearn/base/ProgressBar.h>
@@ -447,6 +450,10 @@
 void Kernel::computeGramMatrixDerivative(Mat& KD, const string& kernel_param,
                                          real epsilon) const
 {
+    MODULE_LOG << "Computing Gram matrix derivative by finite differences "
+               << "for hyper-parameter '" << kernel_param << "'"
+               << endl;
+    
     // This function is conceptually const, but the evaluation by finite
     // differences in a generic way requires some change-options, which
     // formally require a const-away cast.

Modified: trunk/plearn/ker/MemoryCachedKernel.cc
===================================================================
--- trunk/plearn/ker/MemoryCachedKernel.cc	2007-02-13 22:21:25 UTC (rev 6659)
+++ trunk/plearn/ker/MemoryCachedKernel.cc	2007-02-15 00:53:12 UTC (rev 6660)
@@ -128,9 +128,17 @@
         the_data.isNotNull())
     {
         m_data_cache = the_data.toMat();
+
+        // Update row cache
+        const int N = m_data_cache.length();
+        m_row_cache.resize(N);
+        for (int i=0 ; i<N ; ++i)
+            dataRow(i, m_row_cache[i]);
     }
-    else
+    else {
         m_data_cache = Mat();
+        m_row_cache.resize(0);
+    }
 }
 
 
@@ -140,8 +148,15 @@
 {
     inherited::addDataForKernelMatrix(newrow);
 
-    if (m_data_cache.isNotNull())
+    if (m_data_cache.isNotNull()) {
+        const int OLD_N = m_data_cache.length();
+        PLASSERT( m_data_cache.length() == m_row_cache.size() );
         m_data_cache.appendRow(newrow);
+
+        // Update row cache
+        m_row_cache.push_back(Vec());
+        dataRow(OLD_N, m_row_cache[OLD_N]);
+    }
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn/ker/MemoryCachedKernel.h
===================================================================
--- trunk/plearn/ker/MemoryCachedKernel.h	2007-02-13 22:21:25 UTC (rev 6659)
+++ trunk/plearn/ker/MemoryCachedKernel.h	2007-02-15 00:53:12 UTC (rev 6660)
@@ -97,7 +97,10 @@
     //! Update the cache if a new row is added to the data
     virtual void addDataForKernelMatrix(const Vec& newRow);
 
+    //! Return true if the cache is active after setting some data
+    bool dataCached() const { return m_data_cache.size() > 0; }
 
+
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.
@@ -124,6 +127,13 @@
     inline void dataRow(int i, Vec& row) const;
 
     /**
+     *  Interface for derived classes: access row i of the data matrix and
+     *  return it as a POINTER to a Vec.  NOTE: this version ASSUMES that the
+     *  cache exists.  You can verify this with the dataCached() function.
+     */
+    inline Vec* dataRow(int i) const;
+
+    /**
      *  Interface to ease derived-class implementation of computeGramMatrix
      *  that avoids virtual function calls in kernel evaluation.  The
      *  computeGramMatrixNV function should be called directly by the
@@ -146,11 +156,11 @@
      *
      *  The member function is called with the following arguments:
      *
-     *  - x1  : element at row i of the data matrix
-     *  - x2  : element at row j of the data matrix
-     *  - K   : kernel value for (x1,x2); obtained from cache if available
+     *  - i   : current row i of the data matrix
+     *  - j   : current row j of the data matrix
      *  - arg : integer argument passed to the function; may be used to index
      *          into a vector of hyperparameters
+     *  - K   : kernel value for (x1,x2); obtained from cache if available
      *
      *  The last argument to computeGramMatrixDerivNV,
      *  'derivative_func_requires_K', specifies whether the derivativeFunc
@@ -160,7 +170,7 @@
      *  is called with a MISSING_VALUE for its argument K.
      */
     template <class DerivedClass,
-              real (DerivedClass::*derivativeFunc)(const Vec&, const Vec&, real, int) const>
+              real (DerivedClass::*derivativeFunc)(int, int, int, real) const>
     void computeGramMatrixDerivNV(Mat& KD, const DerivedClass* This, int arg,
                                   bool derivative_func_requires_K = true) const;
     
@@ -169,9 +179,13 @@
     //! This does the actual building.
     void build_();
 
-private:
+protected:
     //! In-memory cache of the data matrix
     Mat m_data_cache;
+
+    //! Cache of vectors for each row of the data matrix; this avoids
+    //! reconstructing a Vec each time we want to access a row.
+    TVec<Vec> m_row_cache;
 };
 
 // Declares a few other classes and functions related to this class
@@ -192,7 +206,14 @@
     }
 }
 
+inline Vec* MemoryCachedKernel::dataRow(int i) const
+{
+    // Note: ASSUME that the cache exists; will boundcheck in dbg/safeopt if
+    // not.
+    return &m_row_cache[i];
+}
 
+
 //#####  computeGramMatrixNV  #################################################
 
 template <class DerivedClass>
@@ -251,7 +272,7 @@
 //#####  computeGramMatrixDerivNV  ############################################
 
 template <class DerivedClass,
-          real (DerivedClass::*derivativeFunc)(const Vec&, const Vec&, real, int) const>
+          real (DerivedClass::*derivativeFunc)(int, int, int, real) const>
 void MemoryCachedKernel::computeGramMatrixDerivNV(Mat& KD, const DerivedClass* This,
                                                   int arg, bool require_K) const
 {
@@ -266,7 +287,6 @@
     KD.resize(W,W);
     int m=KD.mod();
     
-    Vec row_i, row_j;
     real KDij;
     real* KDi;
     real* KDji;
@@ -276,21 +296,21 @@
     for (int i=0 ; i<W ; ++i) {
         KDi  = KD[i];
         KDji = &KD[0][i];
-        dataRow(i, row_i);
         if (gram_matrix_is_cached)
             Ki = gram_matrix[i];
         
         for (int j=0 ; j <= i ; ++j, KDji += m) {
-            dataRow(j, row_j);
-
             // Access the current kernel value depending on whether it's cached
             if (Ki)
                 K = *Ki++;
-            else if (require_K)
+            else if (require_K) {
+                Vec& row_i = *dataRow(i);
+                Vec& row_j = *dataRow(j);
                 K = This->DerivedClass::evaluate(row_i, row_j);
+            }
 
             // Compute and store the derivative
-            KDij   = (This->*derivativeFunc)(row_i, row_j, K, arg);
+            KDij   = (This->*derivativeFunc)(i, j, arg, K);
             *KDi++ = KDij;
             if (j < i)
                 *KDji = KDij;

Modified: trunk/plearn/ker/RationalQuadraticARDKernel.cc
===================================================================
--- trunk/plearn/ker/RationalQuadraticARDKernel.cc	2007-02-13 22:21:25 UTC (rev 6659)
+++ trunk/plearn/ker/RationalQuadraticARDKernel.cc	2007-02-15 00:53:12 UTC (rev 6660)
@@ -105,6 +105,8 @@
 void RationalQuadraticARDKernel::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(m_noise_gram_cache, copies);
 }
 
 
@@ -154,7 +156,80 @@
 
 void RationalQuadraticARDKernel::computeGramMatrix(Mat K) const
 {
-    computeGramMatrixNV(K, this);
+    PLASSERT( !m_log_input_sigma.size() || dataInputsize() == m_log_input_sigma.size() );
+    PLASSERT( K.size() == 0 || m_data_cache.size() > 0 );  // Ensure data cached OK
+
+    // Compute IID noise gram matrix and save it
+    inherited::computeGramMatrix(K);
+    m_noise_gram_cache.resize(K.length(), K.width());
+    m_noise_gram_cache << K;
+
+    // Precompute some terms
+    real sf2   = exp(2*m_log_signal_sigma);
+    real alpha = exp(m_log_alpha);
+    m_input_sigma.resize(dataInputsize());
+    m_input_sigma.fill(m_log_global_sigma);
+    if (m_log_input_sigma.size() > 0)
+        m_input_sigma += m_log_input_sigma;
+    m_input_sigma *= 2.0;
+    exp(m_input_sigma, m_input_sigma);
+    
+    // Compute Gram Matrix
+    int  l = data->length();
+    int  m = K.mod();
+    int  n = dataInputsize();
+    int  cache_mod = m_data_cache.mod();
+
+    real *data_start = &m_data_cache(0,0);
+    real Kij;
+    real *Ki, *Kji, *x1, *x2;
+    real *input_sigma_data = m_input_sigma.data();
+    real *xi = data_start;
+    
+    for (int i=0 ; i<l ; ++i, xi += cache_mod) {
+        Ki  = K[i];
+        Kji = &K[0][i];
+        real *xj = data_start;
+
+        for (int j=0; j<=i; ++j, Kji += m, xj += cache_mod) {
+            // Kernel evaluation per se
+            x1 = xi;
+            x2 = xj;
+            real* p_inpsigma = input_sigma_data;
+            real sum_wt = 0.0;
+            int k = n;
+
+            // Use Duff's device to unroll the following loop:
+            //     while (k--) {
+            //         real diff = *x1++ - *x2++;
+            //         sum_wt += (diff * diff) / *p_inpsigma++;
+            //     }
+            real diff;
+            switch (k % 8) {
+            case 0: do { diff = *x1++ - *x2++; sum_wt += (diff*diff) / *p_inpsigma++;
+            case 7:      diff = *x1++ - *x2++; sum_wt += (diff*diff) / *p_inpsigma++;
+            case 6:      diff = *x1++ - *x2++; sum_wt += (diff*diff) / *p_inpsigma++;
+            case 5:      diff = *x1++ - *x2++; sum_wt += (diff*diff) / *p_inpsigma++;
+            case 4:      diff = *x1++ - *x2++; sum_wt += (diff*diff) / *p_inpsigma++;
+            case 3:      diff = *x1++ - *x2++; sum_wt += (diff*diff) / *p_inpsigma++;
+            case 2:      diff = *x1++ - *x2++; sum_wt += (diff*diff) / *p_inpsigma++;
+            case 1:      diff = *x1++ - *x2++; sum_wt += (diff*diff) / *p_inpsigma++;
+                       } while((k -= 8) > 0);
+            }
+            
+            Kij = sf2 * pow(1 + sum_wt / (2.*alpha), -alpha);
+            
+            // Update kernel matrix (already pre-filled with IID noise terms)
+            *Ki++ += Kij;
+            if (j < i)
+                *Kji += Kij;
+        }
+    }
+    if (cache_gram_matrix) {
+        gram_matrix.resize(l,l);
+        gram_matrix << K;
+        gram_matrix_is_cached = true;
+    }
 }
 
 
@@ -163,7 +238,7 @@
 void RationalQuadraticARDKernel::computeGramMatrixDerivative(
     Mat& KD, const string& kernel_param, real epsilon) const
 {
-    static const string LSS("log_sigmal_sigma");
+    static const string LSS("log_signal_sigma");
     static const string LGS("log_global_sigma");
     static const string LIS("log_input_sigma[");
     static const string LAL("log_alpha");
@@ -185,9 +260,11 @@
                              LIS.size(), kernel_param.size() - LIS.size() - 1));
         PLASSERT( arg < m_log_input_sigma.size() );
 
-        computeGramMatrixDerivNV<
-            RationalQuadraticARDKernel,
-            &RationalQuadraticARDKernel::derivLogInputSigma>(KD, this, arg);
+        computeGramMatrixDerivLogInputSigma(KD, arg);
+
+        // computeGramMatrixDerivNV<
+        //     RationalQuadraticARDKernel,
+        //     &RationalQuadraticARDKernel::derivLogInputSigma>(KD, this, arg);
     }
     else if (kernel_param == LAL) {
         computeGramMatrixDerivNV<
@@ -210,24 +287,22 @@
 
 //#####  derivLogSignalSigma  #################################################
 
-real RationalQuadraticARDKernel::derivLogSignalSigma(
-    const Vec& row_i, const Vec& row_j, real K, int arg) const
+real RationalQuadraticARDKernel::derivLogSignalSigma(int i, int j, int arg, real K) const
 {
-    real noise = inherited::evaluate(row_i, row_j);
+    real noise = m_noise_gram_cache(i,j);
     return 2*(K-noise);
 }
 
 
 //#####  derivLogGlobalSigma  #################################################
 
-real RationalQuadraticARDKernel::derivLogGlobalSigma(
-    const Vec& row_i, const Vec& row_j, real K, int arg) const
+real RationalQuadraticARDKernel::derivLogGlobalSigma(int i, int j, int arg, real K) const
 {
     // The rational quadratic gives us:
     //     K = exp(2*s)*k^(-alpha).
     // Rederive the value of k
     real alpha = exp(m_log_alpha);
-    real noise = inherited::evaluate(row_i, row_j);
+    real noise = m_noise_gram_cache(i,j);
     K -= noise;
     real k     = exp(- (pl_log(K) - 2*m_log_signal_sigma) / alpha);
     real inner = - (k - 1) * alpha;
@@ -237,14 +312,18 @@
 
 //#####  derivLogInputSigma  ##################################################
 
-real RationalQuadraticARDKernel::derivLogInputSigma(
-    const Vec& row_i, const Vec& row_j, real K, int arg) const
+// This function computes the derivative element-wise.  The function actually
+// used now is computeGramMatrixDerivLogInputSigma, which computes the whole
+// matrix much faster.
+real RationalQuadraticARDKernel::derivLogInputSigma(int i, int j, int arg, real K) const
 {
     // The rational quadratic gives us:
     //     K = exp(2*s)*k^(-alpha).
     // Rederive the value of k
+    Vec& row_i   = *dataRow(i);
+    Vec& row_j   = *dataRow(j);
     real alpha   = exp(m_log_alpha);
-    real noise   = inherited::evaluate(row_i, row_j);
+    real noise   = m_noise_gram_cache(i,j);
     K -= noise;
     real k       = exp(- (pl_log(K) - 2*m_log_signal_sigma) / alpha);
     real diff    = row_i[arg] - row_j[arg];
@@ -255,11 +334,10 @@
 
 //#####  derivLogAlpha  #######################################################
 
-real RationalQuadraticARDKernel::derivLogAlpha(
-    const Vec& row_i, const Vec& row_j, real K, int arg) const
+real RationalQuadraticARDKernel::derivLogAlpha(int i, int j, int arg, real K) const
 {
     real alpha = exp(m_log_alpha);
-    real noise = inherited::evaluate(row_i, row_j);
+    real noise = m_noise_gram_cache(i,j);
     K -= noise;
     real k     = exp(- (pl_log(K) - 2*m_log_signal_sigma) / alpha);
     real left  = - alpha * pl_log(k);
@@ -269,6 +347,68 @@
 }
 
 
+//#####  computeGramMatrixDerivLogInputSigma  #################################
+
+void RationalQuadraticARDKernel::computeGramMatrixDerivLogInputSigma(Mat& KD,
+                                                                     int arg) const
+{
+    // Precompute some terms
+    real alpha = exp(m_log_alpha);
+    real twice_log_signal_sigma = 2.*m_log_signal_sigma;
+    
+    // Compute Gram Matrix derivative w.r.t. log_input_sigma[arg]
+    int  l = data->length();
+    int  k_mod     = gram_matrix.mod();
+    int  cache_mod = m_data_cache.mod();
+
+    // Variables that walk over the pre-computed kernel (K) and data matrices
+    real *input_sigma_data = m_input_sigma.data();
+    real *data_start = &m_data_cache(0,0);
+    real *xi = data_start;                   // Iterator on data rows
+    real *Ki = &gram_matrix(0,0);            // Current row of kernel matrix
+    real *Kij;                               // Current element of kernel matrix
+
+    // Variables that walk over the noise cache
+    real *noise_start_row = m_noise_gram_cache.data();
+    real *cur_noise;                         // Current element of noise matrix
+    int  noise_mod = m_noise_gram_cache.mod();
+    
+    // Variables that walk over the kernel derivative matrix (KD)
+    KD.resize(l,l);
+    real* KDi = KD.data();                   // Start of row i
+    real* KDj = KD.data();                   // Start of column j
+    real* KDij;                              // Current element on row i
+    real* KDji;                              // Current element on column j
+    int   KD_mod = KD.mod();
+
+    // Iterate on rows of derivative matrix
+    for (int i=0 ; i<l ; ++i, xi += cache_mod, Ki += k_mod,
+             KDi += KD_mod, ++KDj, noise_start_row += noise_mod)
+    {
+        Kij  = Ki;
+        KDij = KDi;
+        KDji = KDj;
+        real *xj  = data_start;              // Inner iterator on data rows
+        cur_noise = noise_start_row;
+
+        // Iterate on columns of derivative matrix
+        for (int j=0 ; j <= i
+                 ; ++j, ++Kij, KDji+=KD_mod, xj += cache_mod, ++cur_noise)
+        {
+            real K       = *Kij - *cur_noise;
+            real k       = exp(- (pl_log(K) - twice_log_signal_sigma) / alpha);
+            real diff    = xi[arg] - xj[arg];
+            real sq_diff = diff * diff;
+            real KD_cur  = (K / k) * sq_diff / input_sigma_data[arg];
+            
+            // Set into derivative matrix
+            *KDij++ = KD_cur;
+            if (j < i)
+                *KDji = KD_cur;
+        }
+    }
+}
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/ker/RationalQuadraticARDKernel.h
===================================================================
--- trunk/plearn/ker/RationalQuadraticARDKernel.h	2007-02-13 22:21:25 UTC (rev 6659)
+++ trunk/plearn/ker/RationalQuadraticARDKernel.h	2007-02-15 00:53:12 UTC (rev 6660)
@@ -113,17 +113,24 @@
     static void declareOptions(OptionList& ol);
 
     //! Derivative function with respect to log_signal_sigma
-    real derivLogSignalSigma(const Vec& row_i, const Vec& row_j, real K, int arg) const;
+    real derivLogSignalSigma(int i, int j, int arg, real K) const;
 
     //! Derivative function with respect to log_global_sigma
-    real derivLogGlobalSigma(const Vec& row_i, const Vec& row_j, real K, int arg) const;
+    real derivLogGlobalSigma(int i, int j, int arg, real K) const;
     
     //! Derivative function with respect to log_input_sigma[arg]
-    real derivLogInputSigma(const Vec& row_i, const Vec& row_j, real K, int arg) const;
+    real derivLogInputSigma(int i, int j, int arg, real K) const;
     
     //! Derivative function with respect to log_alpha
-    real derivLogAlpha(const Vec& row_i, const Vec& row_j, real K, int arg) const;
+    real derivLogAlpha(int i, int j, int arg, real K) const;
+
+    // Compute derivative w.r.t. log_input_sigma[arg] for WHOLE MATRIX
+    void computeGramMatrixDerivLogInputSigma(Mat& KD, int arg) const;
     
+protected:
+    //! Cached version of IID noise gram matrix
+    mutable Mat m_noise_gram_cache;
+
 private:
     //! This does the actual building.
     void build_();



From chapados at mail.berlios.de  Thu Feb 15 04:15:56 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Thu, 15 Feb 2007 04:15:56 +0100
Subject: [Plearn-commits] r6661 - trunk/plearn/math
Message-ID: <200702150315.l1F3Fuw4013057@sheep.berlios.de>

Author: chapados
Date: 2007-02-15 04:15:54 +0100 (Thu, 15 Feb 2007)
New Revision: 6661

Modified:
   trunk/plearn/math/TMat_maths_impl.h
Log:
Fixed a lot of accesses to a vec data() when called with an empty vec -- but must still fix much more

Modified: trunk/plearn/math/TMat_maths_impl.h
===================================================================
--- trunk/plearn/math/TMat_maths_impl.h	2007-02-15 00:53:12 UTC (rev 6660)
+++ trunk/plearn/math/TMat_maths_impl.h	2007-02-15 03:15:54 UTC (rev 6661)
@@ -228,9 +228,11 @@
 //! computes y <- exp(x)
 template <class T> 
 void exp(const TVec<T>& x, TVec<T>& y)
-{  
+{
     y.resize(x.length());
     int n = x.length();
+    if (!n)
+        return;
     T* xp = x.data();
     T* yp = y.data();
     while(n--)
@@ -241,6 +243,8 @@
 template<class T> 
 T sumsquare(const TVec<T>& x)
 {
+    if (x.length() == 0)
+        return T(0);
     T* v = x.data();
     T res = square(v[0]);
     int l = x.length();
@@ -253,6 +257,8 @@
 template<class T> 
 T sumabs(const TVec<T>& x)
 {
+    if (x.length() == 0)
+        return T(0);
     T* v = x.data();
     T res = (T)(fabs((real)v[0]));
     int l = x.length();
@@ -265,6 +271,8 @@
 template<class T>
 void squareElements(const TVec<T>& x)
 {
+    if (x.length() == 0)
+        return;
     T* ptr = x.data();
     int l = x.length();
     while(l--)
@@ -278,7 +286,8 @@
 template<class T>
 void squareElements(const TMat<T>& m)
 {
-    if (m.size()==0) return;
+    if (m.size()==0)
+        return;
     if(m.isCompact()) {
         typename TMat<T>::compact_iterator it = m.compact_begin();
         typename TMat<T>::compact_iterator itend = m.compact_end();
@@ -296,7 +305,8 @@
 template<class T> 
 T sumsquare(const TMat<T>& m)
 {  
-    if (m.size()==0) return 0;
+    if (m.size()==0)
+        return T(0);
     if(m.isCompact())
     {
         typename TMat<T>::compact_iterator it = m.compact_begin();
@@ -324,7 +334,8 @@
 template<class T> 
 T sumabs(const TMat<T>& m)
 {  
-    if (m.size()==0) return 0;
+    if (m.size()==0)
+        return T(0);
     if(m.isCompact())
     {
         typename TMat<T>::compact_iterator it = m.compact_begin();
@@ -382,6 +393,8 @@
     int n=source1.length();
     if (n!=destination.length())
         destination.resize(n);
+    if (!n)
+        return;
     T* s1=source1.data();
     T* d=destination.data();
     for (int i=0;i<n;i++)
@@ -397,6 +410,8 @@
 T sum(const TVec<T>& vec, bool ignore_missing)
 {
     double res = 0.0;
+    if (vec.size() == 0)
+        return res;
     T* v = vec.data();
     for(int i=0; i<vec.length(); i++)
     {
@@ -412,6 +427,8 @@
 T sum(const TVec<T>& vec)
 {
     T res = T(0);
+    if (vec.size() == 0)
+        return res;
     T* v = vec.data();
     for(int i=0; i<vec.length(); i++)
         res += v[i];
@@ -425,6 +442,8 @@
 T sum_of_log(const TVec<T>& vec)
 {
     double res = 0.0;
+    if (vec.size() == 0)
+        return res;
     T* v = vec.data();
     for(int i=0; i<vec.length(); i++)
         res += pl_log(v[i]);
@@ -435,6 +454,8 @@
 T product(const TVec<T>& vec)
 {
     double res = 1.0;
+    if (vec.size() == 0)
+        return res;
     T* v = vec.data();
     for(int i=0; i<vec.length(); i++)
         res *= v[i];
@@ -468,6 +489,8 @@
     if(vec.length()==0)
         PLERROR("IN T mean(const TVec<T>& vec) vec has zero length");
 #endif
+    if (vec.size() == 0)
+        return MISSING_VALUE;
     double res = 0.0;
     int n = 0;
     T* v = vec.data();
@@ -478,10 +501,12 @@
             res += v[i];
             n++;
         }
-        else if (!ignore_missing) return MISSING_VALUE;
+        else if (!ignore_missing)
+            return MISSING_VALUE;
     }
 
-    if (n == 0) return MISSING_VALUE;
+    if (n == 0)
+        return MISSING_VALUE;
     return T(res/double(n));
 }
 
@@ -492,6 +517,8 @@
     if(vec.length()==0)
         PLERROR("IN T mean(const TVec<T>& vec) vec has zero length");
 #endif
+    if (vec.size() == 0)
+        return MISSING_VALUE;
     double res = 0.0;
     int n = 0;
     T* v = vec.data();
@@ -502,10 +529,12 @@
             res += 1.0/v[i];
             n++;
         }
-        else if (!ignore_missing) return MISSING_VALUE;
+        else if (!ignore_missing)
+            return MISSING_VALUE;
     }
 
-    if (n == 0) return MISSING_VALUE;
+    if (n == 0)
+        return MISSING_VALUE;
     return T(double(n)/res);
 }
 



From chapados at mail.berlios.de  Thu Feb 15 04:16:33 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Thu, 15 Feb 2007 04:16:33 +0100
Subject: [Plearn-commits] r6662 - trunk/plearn/ker
Message-ID: <200702150316.l1F3GXtn013253@sheep.berlios.de>

Author: chapados
Date: 2007-02-15 04:16:33 +0100 (Thu, 15 Feb 2007)
New Revision: 6662

Modified:
   trunk/plearn/ker/IIDNoiseKernel.cc
Log:
Minor bugfix when no kronecker term is used

Modified: trunk/plearn/ker/IIDNoiseKernel.cc
===================================================================
--- trunk/plearn/ker/IIDNoiseKernel.cc	2007-02-15 03:15:54 UTC (rev 6661)
+++ trunk/plearn/ker/IIDNoiseKernel.cc	2007-02-15 03:16:33 UTC (rev 6662)
@@ -175,8 +175,10 @@
 
     // Prepare kronecker iteration
     int   kronecker_num     = m_kronecker_indexes.size();
-    int*  kronecker_indexes = m_kronecker_indexes.data();
-    real* kronecker_sigma   = m_kronecker_sigma.data();
+    int*  kronecker_indexes = ( kronecker_num > 0?
+                                m_kronecker_indexes.data() : 0 );
+    real* kronecker_sigma   = ( kronecker_num > 0?
+                                m_kronecker_sigma.data() : 0 );
 
     // Compute Gram Matrix
     int  l = data->length();



From larocheh at mail.berlios.de  Thu Feb 15 21:12:07 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 15 Feb 2007 21:12:07 +0100
Subject: [Plearn-commits] r6663 - trunk/doc
Message-ID: <200702152012.l1FKC7Ie011327@sheep.berlios.de>

Author: larocheh
Date: 2007-02-15 21:12:04 +0100 (Thu, 15 Feb 2007)
New Revision: 6663

Modified:
   trunk/doc/programmers_guide.tex
Log:
Added some stuff in the TVec/TMat section


Modified: trunk/doc/programmers_guide.tex
===================================================================
--- trunk/doc/programmers_guide.tex	2007-02-15 03:16:33 UTC (rev 6662)
+++ trunk/doc/programmers_guide.tex	2007-02-15 20:12:04 UTC (rev 6663)
@@ -46,6 +46,7 @@
 
 Copyright \copyright\ 1998-2002 Pascal Vincent, Yoshua Bengio \\
 Copyright \copyright\ 2004 Martin Monperrus \\
+Copyright \copyright\ 2007 Hugo Larochelle \\
 
 Permission is granted to copy and distribute this document in any medium,
 with or without modification, provided that the following conditions are
@@ -180,11 +181,27 @@
 
 \section{PLearn for Matrix-Vectors Operations}
 
-PLearn can be used as a kind of Matlab. The notation are more
-difficult but the efficiency is uncomparable.
+PLearn has its own vector and matrix data structures.
+The files \texttt{PLearn/plearn/math/TVec\_\{decl,math\}.h}
+contain the declaration and implementation of the
+vector class template\texttt{TVec}, and the matrix
+class template \texttt{TMat} can be found
+in files \texttt{PLearn/plearn/math/TMat\_\{decl,math\}.h}.
 
-\subsection{Creating}
+\subsection{Creation and Basic Manipulations}
 
+The PLearn vector and matrix data structures
+are easy to instantiate and
+support many useful basic operations, such
+as subvector and submatrix access.
+
+Here is a concrete example of how to use
+these data structures.
+The data type \texttt{Vec} and \texttt{Mat} refer to the \texttt{TVec<real>}
+and \texttt{TMat<real>} classes, where \texttt{real} is a macro
+corresponding either to \texttt{double} or \texttt{float}, depending
+on the compilation options used.
+
 \begin{verbatim}
 #include <plearn/math/TMat_maths.h>
 using namespace PLearn;
@@ -196,21 +213,89 @@
     // Please don't use double nor float
 
     real a=15;
-    cout<<a<<endl;
+    cout<<"real a=15;"<<endl;
+    cout<<"%> a="<<a<<endl;
+    cout<<endl;
 
-    // Vector creation: method 1
+    // Vector creation
     Vec b(3);
     b[0] = 2;
     b[1] = 42;
     b[2] = 21;
-    cout<<b<<endl;
+    cout<<"Vec b(3);"<<endl;
+    cout<<"b[0] = 2;"<<endl;
+    cout<<"b[1] = 42;"<<endl;
+    cout<<"b[2] = 21;"<<endl;
+    cout<<"%> b="<<b<<endl;
+    cout<<"%> b.length()="<<b.length()<<endl;
+    cout<<endl;
 
-    // Vector creation: method 2
-    Vec b2(3);
-    b2<<"5 4.2 -2";
-    cout<<b2<<endl;
+    // Vector manipulations:
 
-    // Matrix creation : method 1
+    // Subvector access of the last two elements (not a copy!!!)
+    Vec b3 = b.subVec(1,2);
+    cout<<"Vec b3 = b.subVec(1,2);"<<endl;
+    cout<<"%> b3="<<b3<<endl;
+    cout<<endl;
+
+    // Concatenation
+    Vec b4 = concat(b,b);
+    cout<<"Vec b4 = concat(b,b);"<<endl;
+    cout<<"%> b4="<<b4<<endl;
+    cout<<endl;
+
+    // Note: "=" operator does not copy!!!
+    Vec b5 = b4;
+    b5[1] = 100000;
+    cout<<"Vec b5 = b4;"<<endl;
+    cout<<"b5[1] = 100000;"<<endl;
+    cout<<"%> b4="<<b4<<endl;
+    cout<<"%> b5="<<b5<<endl;
+    cout<<endl;
+
+    // Copy
+    b5 = b4.copy();
+    b5[1]=100001;
+    cout<<"b5 = b4.copy();"<<endl;
+    cout<<"b5[0]=100001;"<<endl;
+    cout<<"%> b4="<<b4<<endl;
+    cout<<"%> b5="<<b5<<endl;
+    cout<<endl;
+
+    // Fill in one element
+    Vec b6(b4.length());
+    b6.fill(3);
+    cout<<"Vec b6(b4.length());"<<endl;
+    cout<<"b6.fill(3);"<<endl;
+    cout<<"%> b6="<<b6<<endl;
+    cout<<endl;
+
+    // Fill in elements of another vector
+    b6 << b4;
+    cout<<"b6 << b4;"<<endl;
+    cout<<"%> b6="<<b6<<endl;
+    cout<<endl;
+
+    // Clear
+    b6.clear();
+    cout<<"b6.clear();"<<endl;
+    cout<<"%> b6="<<b6<<endl;
+    cout<<endl;
+
+    // Resize
+    b4.resize(7);
+    b4[6] = 6;
+    cout<<"b4.resize(7);"<<endl;
+    cout<<"b4[6] = 6;"<<endl;
+    cout<<"%> b4="<<b4<<endl;
+    cout<<endl;
+    b4.resize(4);
+    cout<<"b4.resize(4);"<<endl;
+    cout<<"%> b4="<<b4<<endl;
+    cout<<endl;
+
+
+    // Matrix creation : 
     Mat c(3,2);
     c(1,1)=1.1;
     c(1,0)=4;
@@ -218,55 +303,330 @@
     c(0,1)=-73.2;
     c(0,0)=78;
     c(2,1)=5.32e-2;
-    cout<<c<<endl;
+    cout<<"Mat c(3,2);"<<endl;
+    cout<<"c(1,1)=1.1;"<<endl;
+    cout<<"c(1,0)=4;"<<endl;
+    cout<<"c(2,0)=5;"<<endl;
+    cout<<"c(0,1)=-73.2;"<<endl;
+    cout<<"c(0,0)=78;"<<endl;
+    cout<<"c(2,1)=5.32e-2;"<<endl;
+    cout<<"%> c=\n"<<c<<endl;
+    cout<<"%> c.length()="<<c.length()<<endl;
+    cout<<"%> c.width()="<<c.width()<<endl;
+    cout<<endl;
 
-    // Matrix creation : method 2
-    Mat c2(3,2);
-    c2<<"2 4 2.5e-1 6 1e3 3.1"; // read matrix row by matrix row 
-    cout<<c2<<endl;
+    // Matrix manipulation:
+   
+    // Submatrix access (not a copy!!!)...
 
+    // ... of the last two rows and first column
+    Mat c3 = c.subMat(1,0,2,1);
+    cout<<"Mat c3 = c.subMat(1,0,2,1);"<<endl;
+    cout<<"%> c3=\n"<<c3<<endl;
+    cout<<endl;
+
+    // ... of the second column
+    Mat c4 = c.column(1);
+    cout<<"Mat c4 = c.column(1);"<<endl;
+    cout<<"%> c4=\n"<<c4<<endl;
+    cout<<endl;
+
+    // ... of the third row
+    Mat c5 = c.row(2);
+    cout<<"Mat c5 = c.row(2);"<<endl;
+    cout<<"%> c5=\n"<<c5<<endl;
+    cout<<endl;
+
+    // .. of the third row, as a vector
+    Vec b7 = c(2);
+    cout<<"Vec b7 = c(2);"<<endl;
+    cout<<"%> b7="<<b7<<endl;
+    cout<<endl;
+
+    // Note: "=" operator does not copy!!!
+    Mat c6 = c;
+    c6(1,1) = 100000;
+    cout<<"Mat c6 = c;"<<endl;
+    cout<<"c6(1,1) = 100000;"<<endl;
+    cout<<"%> c=\n"<<c<<endl;
+    cout<<"%> c6=\n"<<c6<<endl;
+    cout<<endl;
+
+    // Copy
+    c6 = c.copy();
+    c6(1,1) = 100001;
+    cout<<"c6 = c.copy();"<<endl;
+    cout<<"c6(1,1) = 100001;"<<endl;
+    cout<<"%> c=\n"<<c<<endl;
+    cout<<"%> c6=\n"<<c6<<endl;
+    cout<<endl;
+
+    // Fill in one element
+    Mat c7(c.length(),c.width());
+    c7.fill(3);
+    cout<<"Mat c7(c.length(),c.width());"<<endl;
+    cout<<"c7.fill(3);"<<endl;
+    cout<<"%> c7=\n"<<c7<<endl;
+    cout<<endl;
+
+    // Fill in elements of another matrix
+    c7 << c;
+    cout<<"c7 << c;"<<endl;
+    cout<<"%> c7=\n"<<c7<<endl;
+    cout<<endl;
+
+    // Fill in a row of another matrix
+    c7(2) << c(1);
+    cout<<"c7(2) << c(1);"<<endl;
+    cout<<"%> c7=\n"<<c7<<endl;
+    cout<<endl;
+
+    // Clear
+    c7.clear();
+    cout<<"c7.clear();"<<endl;
+    cout<<"%> c7=\n"<<c7<<endl;
+    cout<<endl;
+
+    // Resize
+    c7.resize(4,4);
+    c7.subMat(0,2,3,2)<<c.subMat(0,0,3,2);
+    c7(3,0)=0.01;
+    c7(3,1)=0.02;
+    c7(3,2)=0.03;
+    c7(3,3)=0.04;
+    cout<<"c7.resize(4,4);"<<endl;
+    cout<<"c7.subMat(0,2,3,2)<<c.subMat(0,0,3,2);"<<endl;
+    cout<<"c7(3,0)=0.01;"<<endl;
+    cout<<"c7(3,1)=0.02;"<<endl;
+    cout<<"c7(3,2)=0.03;"<<endl;
+    cout<<"c7(3,3)=0.04;"<<endl;
+    cout<<"%> c7=\n"<<c7<<endl;
+    cout<<endl;
+    c7.resize(2,3);
+    cout<<"c7.resize(2,3);"<<endl;
+    cout<<"%> c7=\n"<<c7<<endl;
+    cout<<endl;
+
     return 0;
 }
+
 \end{verbatim}
 
-\subsection{Manipulating}
-All the corresponding methods are in TMat\_maths\_impl.h (or should be). As of 
-date, they are not documented, so you have to have a bath in the file (methods 
-should be commented). A short example:
+The output of this code example is the following:
 
 \begin{verbatim}
-#include <plearn/math/TMat_maths.h>
-using namespace PLearn;
+real a=15;
+%> a=15
 
-int main(int argc, char** argv)
-{
-    // vector creation
-    Vec b(3);
-    b[0] = 2;
-    b[1] = 42;
-    b[2] = 21;
-    cout<<b<<endl;
+Vec b(3);
+b[0] = 2;
+b[1] = 42;
+b[2] = 21;
+%> b=2           42          21
+%> b.length()=3
 
-    // matrix creation
-    Mat c(3,2);
-    c(1,1)=1.0;
-    c(1,0)=4.0;
-    c(2,0)=5.0;
-    c(0,1)=73.0;
-    c(0,0)=78.0;
-    c(2,1)=5.0;
+Vec b3 = b.subVec(1,2);
+%> b3=42          21
 
+Vec b4 = concat(b,b);
+%> b4=2           42          21          2           42          21
 
-    Vec d(2);
-    transposeProductAcc(d,c,b);
-    cout<<d<<endl;
-    return 0;
-}
+Vec b5 = b4;
+b5[1] = 100000;
+%> b4=2           100000      21          2           42          21
+%> b5=2           100000      21          2           42          21
+
+b5 = b4.copy();
+b5[0]=100001;
+%> b4=2           100000      21          2           42          21
+%> b5=2           100001      21          2           42          21
+
+Vec b6(b4.length());
+b6.fill(3);
+%> b6=3           3           3           3           3           3
+
+b6 << b4;
+%> b6=2           100000      21          2           42          21
+
+b6.clear();
+%> b6=0           0           0           0           0           0
+
+b4.resize(7);
+b4[6] = 6;
+%> b4=2           100000      21          2           42          21          6
+
+b4.resize(4);
+%> b4=2           100000      21          2
+
+Mat c(3,2);
+c(1,1)=1.1;
+c(1,0)=4;
+c(2,0)=5;
+c(0,1)=-73.2;
+c(0,0)=78;
+c(2,1)=5.32e-2;
+%> c=
+78          -73.2
+4           1.1
+5           0.0532
+
+%> c.length()=3
+%> c.width()=2
+
+Mat c3 = c.subMat(1,0,2,1);
+%> c3=
+4
+5
+
+
+Mat c4 = c.column(1);
+%> c4=
+-73.2
+1.1
+0.0532
+
+
+Mat c5 = c.row(2);
+%> c5=
+5           0.0532
+
+
+Vec b7 = c(2);
+%> b7=5           0.0532
+
+Mat c6 = c;
+c6(1,1) = 100000;
+%> c=
+78          -73.2
+4           100000
+5           0.0532
+
+%> c6=
+78          -73.2
+4           100000
+5           0.0532
+
+
+c6 = c.copy();
+c6(1,1) = 100001;
+%> c=
+78          -73.2
+4           100000
+5           0.0532
+
+%> c6=
+78          -73.2
+4           100001
+5           0.0532
+
+
+Mat c7(c.length(),c.width());
+c7.fill(3);
+%> c7=
+3           3
+3           3
+3           3
+
+
+c7 << c;
+%> c7=
+78          -73.2
+4           100000
+5           0.0532
+
+
+c7(2) << c(1);
+%> c7=
+78          -73.2
+4           100000
+4           100000
+
+
+c7.clear();
+%> c7=
+0           0
+0           0
+0           0
+
+
+c7.resize(4,4);
+c7.subMat(0,2,3,2)<<c.subMat(0,0,3,2);
+c7(3,0)=0.01;
+c7(3,1)=0.02;
+c7(3,2)=0.03;
+c7(3,3)=0.04;
+%> c7=
+0           0           78          -73.2
+0           0           4           100000
+0           0           5           0.0532
+0.01        0.02        0.03        0.04
+
+
+c7.resize(2,3);
+%> c7=
+0           0           78
+0           0           4
+
 \end{verbatim}
 
+For other useful methods for \texttt{TVec} and
+\texttt{TMat} and more details on their implementation,
+see files \texttt{PLearn/plearn/math/TVec\_\{decl,math\}.h}
+and \texttt{PLearn/plearn/math/TMat\_\{decl,math\}.h}
+
+\subsection{Mathematical Manipulations}
+
+Though you might want to implement certain mathematical functions or operators
+yourself, many mathematical manipulations for \texttt{TVec}
+and \texttt{TMat} are already implemented in PLearn. 
+
+In \texttt{PLearn/plearn/math/TMat\_maths\_impl.h},
+many mathematical operators, such as  \texttt{+}, \texttt{-}, \texttt{*}, 
+\texttt{/}, \texttt{+=}, \texttt{-=}, \texttt{*=} and \texttt{/=} are already overloaded.
+When using \texttt{+}, \texttt{-}, \texttt{*} or
+\texttt{/}, a new vector/matrix is created as the result of the operation,
+and when using \texttt{+=}, \texttt{-=}, \texttt{*=}, \texttt{/=}, the operand on
+the left is modified and no object is created. Also, many vector/matrix products
+are implemented. Given the vector $x$ and $y$ and the matrices $A$, $B$ and $C$:
+
+\begin{itemize}
+\item \texttt{dot(x,y)} computes $x' y$
+\item \texttt{product(y,A,x)} computes $y$ such that $A x = y$
+\item \texttt{transposeProduct(y,A,x)} computes $y$ such that $A' x = y$
+\item \texttt{product(C,A,B)} computes $C$ such that $A B = C$
+\item \texttt{transposeProduct(C,A,B)} computes $C$ such that $A' B = C$
+\item \texttt{externalProduct(A,x,y)} computes $A$ such that $x y' = A$
+\end{itemize}
+
+Moreover, the functions \texttt{productAcc}, \texttt{transposeProductAcc} and \texttt{externalProductAcc}
+perform the same operations but accumulate the result of the computations in 
+the modified data structure instead of overwriting what it initially contained.
+For example, the computation of $A x + A y = z$ can be done by the following
+calls: \texttt{product(z,A,x)} followed by \texttt{productAcc(z,A,y)}.
+
+Many other standard functions can be found in \texttt{PLearn/plearn/math/TMat\_maths\_impl.h}. 
+The most popular are probably \texttt{sign}, \texttt{max}, 
+\texttt{argmax}, \texttt{min}, \texttt{argmin},
+\texttt{softmax}, \texttt{exp}, \texttt{abs}, \texttt{log}, 
+\texttt{logadd}, \texttt{sqrt}, \texttt{sigmoid} and \texttt{tanh}.
+
+When considering to implement a given mathematical function on
+vectors and matrices in PLearn, some time can be saved by first 
+looking in \texttt{PLearn/plearn/math/TMat\_maths\_impl.h} 
+in order to verify whether it has already been implemented.
+
+In \texttt{PLearn/plearn/math/TMat\_maths\_specialisation.h}, optimized
+versions of vector/matrix operators for specific data types and relying on the BLAS library
+can be found.
+Also, in \texttt{PLearn/plearn/math/plapack.h}, other specialized functions for vectors
+and matrices (matrix inverse, eigenvalue and singular value decomposition, linear system
+solver, etc.) relying on the LAPACK library can also be found.
+
+
 \subsection{Loading and saving}
-You can load and save a Mat with the following code (note that it needs an 
-include of VMat.h):
+You can load and save a Mat with the following code (VMat.h
+must be included):
+
 \begin{verbatim}
 
 #include <plearn/math/TMat_maths.h>



From larocheh at mail.berlios.de  Thu Feb 15 23:01:23 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 15 Feb 2007 23:01:23 +0100
Subject: [Plearn-commits] r6664 - trunk/doc
Message-ID: <200702152201.l1FM1NDg017585@sheep.berlios.de>

Author: larocheh
Date: 2007-02-15 23:01:23 +0100 (Thu, 15 Feb 2007)
New Revision: 6664

Modified:
   trunk/doc/programmers_guide.tex
Log:
Corrected some stuff in TMat/TVec part...


Modified: trunk/doc/programmers_guide.tex
===================================================================
--- trunk/doc/programmers_guide.tex	2007-02-15 20:12:04 UTC (rev 6663)
+++ trunk/doc/programmers_guide.tex	2007-02-15 22:01:23 UTC (rev 6664)
@@ -213,86 +213,85 @@
     // Please don't use double nor float
 
     real a=15;
-    cout<<"real a=15;"<<endl;
-    cout<<"%> a="<<a<<endl;
-    cout<<endl;
+    cout<<"a="<<a<<endl;
+    // Output:
+    // a=15
 
     // Vector creation
     Vec b(3);
     b[0] = 2;
     b[1] = 42;
     b[2] = 21;
-    cout<<"Vec b(3);"<<endl;
-    cout<<"b[0] = 2;"<<endl;
-    cout<<"b[1] = 42;"<<endl;
-    cout<<"b[2] = 21;"<<endl;
-    cout<<"%> b="<<b<<endl;
-    cout<<"%> b.length()="<<b.length()<<endl;
-    cout<<endl;
+    cout<<"b="<<b<<endl;
+    cout<<"b.length()="<<b.length()<<endl;
+    // Output:
+    // b=2           42          21
+    // b.length()=3
 
     // Vector manipulations:
 
     // Subvector access of the last two elements (not a copy!!!)
     Vec b3 = b.subVec(1,2);
-    cout<<"Vec b3 = b.subVec(1,2);"<<endl;
-    cout<<"%> b3="<<b3<<endl;
-    cout<<endl;
+    cout<<"b3="<<b3<<endl;
+    // Output:
+    // b3=42          21
 
     // Concatenation
     Vec b4 = concat(b,b);
-    cout<<"Vec b4 = concat(b,b);"<<endl;
-    cout<<"%> b4="<<b4<<endl;
-    cout<<endl;
+    cout<<"b4="<<b4<<endl;
+    // Output:
+    // b4=2           42          21          2           42          21
 
     // Note: "=" operator does not copy!!!
     Vec b5 = b4;
     b5[1] = 100000;
-    cout<<"Vec b5 = b4;"<<endl;
-    cout<<"b5[1] = 100000;"<<endl;
-    cout<<"%> b4="<<b4<<endl;
-    cout<<"%> b5="<<b5<<endl;
-    cout<<endl;
+    cout<<"b4="<<b4<<endl;
+    cout<<"b5="<<b5<<endl;
+    // Output:
+    // b4=2           100000      21          2           42          21
+    // b5=2           100000      21          2           42          21
+    
 
     // Copy
     b5 = b4.copy();
-    b5[1]=100001;
-    cout<<"b5 = b4.copy();"<<endl;
-    cout<<"b5[0]=100001;"<<endl;
-    cout<<"%> b4="<<b4<<endl;
-    cout<<"%> b5="<<b5<<endl;
-    cout<<endl;
+    b5[1]=1;
+    cout<<"b4="<<b4<<endl;
+    cout<<"b5="<<b5<<endl;
+    // Output:
+    // b4=2           100000      21          2           42          21
+    // b5=2           1      21          2           42          21
 
+
     // Fill in one element
     Vec b6(b4.length());
     b6.fill(3);
-    cout<<"Vec b6(b4.length());"<<endl;
-    cout<<"b6.fill(3);"<<endl;
-    cout<<"%> b6="<<b6<<endl;
-    cout<<endl;
+    cout<<"b6="<<b6<<endl;
+    // Output:
+    // b6=3           3           3           3           3           3
 
     // Fill in elements of another vector
     b6 << b4;
-    cout<<"b6 << b4;"<<endl;
-    cout<<"%> b6="<<b6<<endl;
-    cout<<endl;
+    cout<<"b6="<<b6<<endl;
+    // Output:
+    // b6=2           100000      21          2           42          21
 
     // Clear
     b6.clear();
-    cout<<"b6.clear();"<<endl;
-    cout<<"%> b6="<<b6<<endl;
-    cout<<endl;
+    cout<<"b6="<<b6<<endl;
+    // Output:
+    // b6=0           0           0           0           0           0
 
     // Resize
     b4.resize(7);
     b4[6] = 6;
-    cout<<"b4.resize(7);"<<endl;
-    cout<<"b4[6] = 6;"<<endl;
-    cout<<"%> b4="<<b4<<endl;
-    cout<<endl;
+    cout<<"b4="<<b4<<endl;
+    // Output:
+    // b4=2           100000      21          2           42          21          6
+    
     b4.resize(4);
-    cout<<"b4.resize(4);"<<endl;
-    cout<<"%> b4="<<b4<<endl;
-    cout<<endl;
+    cout<<"b4="<<b4<<endl;
+    // Output:
+    // b4=2           100000      21          2
 
 
     // Matrix creation : 
@@ -303,17 +302,17 @@
     c(0,1)=-73.2;
     c(0,0)=78;
     c(2,1)=5.32e-2;
-    cout<<"Mat c(3,2);"<<endl;
-    cout<<"c(1,1)=1.1;"<<endl;
-    cout<<"c(1,0)=4;"<<endl;
-    cout<<"c(2,0)=5;"<<endl;
-    cout<<"c(0,1)=-73.2;"<<endl;
-    cout<<"c(0,0)=78;"<<endl;
-    cout<<"c(2,1)=5.32e-2;"<<endl;
-    cout<<"%> c=\n"<<c<<endl;
-    cout<<"%> c.length()="<<c.length()<<endl;
-    cout<<"%> c.width()="<<c.width()<<endl;
-    cout<<endl;
+    cout<<"c=\n"<<c<<endl;
+    cout<<"c.length()="<<c.length()<<endl;
+    cout<<"c.width()="<<c.width()<<endl;
+    // Output:
+    // c=
+    // 78          -73.2
+    // 4           1.1
+    // 5           0.0532
+    //
+    // c.length()=3
+    // c.width()=2
 
     // Matrix manipulation:
    
@@ -321,71 +320,102 @@
 
     // ... of the last two rows and first column
     Mat c3 = c.subMat(1,0,2,1);
-    cout<<"Mat c3 = c.subMat(1,0,2,1);"<<endl;
-    cout<<"%> c3=\n"<<c3<<endl;
-    cout<<endl;
+    cout<<"c3=\n"<<c3<<endl;
+    // Output:
+    // c3=
+    // 4
+    // 5
 
     // ... of the second column
     Mat c4 = c.column(1);
-    cout<<"Mat c4 = c.column(1);"<<endl;
-    cout<<"%> c4=\n"<<c4<<endl;
-    cout<<endl;
+    cout<<"c4=\n"<<c4<<endl;
+    // Output:
+    // c4=
+    // -73.2
+    // 1.1
+    // 0.0532
 
     // ... of the third row
     Mat c5 = c.row(2);
-    cout<<"Mat c5 = c.row(2);"<<endl;
-    cout<<"%> c5=\n"<<c5<<endl;
-    cout<<endl;
+    cout<<"c5=\n"<<c5<<endl;
+    // Output:
+    // c5=
+    // 5           0.0532
 
     // .. of the third row, as a vector
     Vec b7 = c(2);
-    cout<<"Vec b7 = c(2);"<<endl;
-    cout<<"%> b7="<<b7<<endl;
-    cout<<endl;
+    cout<<"b7="<<b7<<endl;
+    // Output:
+    // b7=5           0.0532
 
     // Note: "=" operator does not copy!!!
     Mat c6 = c;
     c6(1,1) = 100000;
-    cout<<"Mat c6 = c;"<<endl;
-    cout<<"c6(1,1) = 100000;"<<endl;
-    cout<<"%> c=\n"<<c<<endl;
-    cout<<"%> c6=\n"<<c6<<endl;
-    cout<<endl;
+    cout<<"c=\n"<<c<<endl;
+    cout<<"c6=\n"<<c6<<endl;
+    // Output:
+    // c=
+    // 78          -73.2
+    // 4           100000
+    // 5           0.0532
+    //
+    // c6=
+    // 78          -73.2
+    // 4           100000
+    // 5           0.0532
 
     // Copy
     c6 = c.copy();
-    c6(1,1) = 100001;
-    cout<<"c6 = c.copy();"<<endl;
-    cout<<"c6(1,1) = 100001;"<<endl;
-    cout<<"%> c=\n"<<c<<endl;
-    cout<<"%> c6=\n"<<c6<<endl;
-    cout<<endl;
+    c6(1,1) = 1;
+    cout<<"c=\n"<<c<<endl;
+    cout<<"c6=\n"<<c6<<endl;
+    // Output:
+    // c=
+    // 78          -73.2
+    // 4           100000
+    // 5           0.0532
+    //
+    // c6=
+    // 78          -73.2
+    // 4           1
+    // 5           0.0532
 
     // Fill in one element
     Mat c7(c.length(),c.width());
     c7.fill(3);
-    cout<<"Mat c7(c.length(),c.width());"<<endl;
-    cout<<"c7.fill(3);"<<endl;
-    cout<<"%> c7=\n"<<c7<<endl;
-    cout<<endl;
+    cout<<"c7=\n"<<c7<<endl;
+    // Output:
+    // c7=
+    // 3           3
+    // 3           3
+    // 3           3
 
     // Fill in elements of another matrix
     c7 << c;
-    cout<<"c7 << c;"<<endl;
-    cout<<"%> c7=\n"<<c7<<endl;
-    cout<<endl;
+    cout<<"c7=\n"<<c7<<endl;
+    // Output:
+    // c7=
+    // 78          -73.2
+    // 4           100000
+    // 5           0.0532
 
     // Fill in a row of another matrix
     c7(2) << c(1);
-    cout<<"c7(2) << c(1);"<<endl;
-    cout<<"%> c7=\n"<<c7<<endl;
-    cout<<endl;
+    cout<<"c7=\n"<<c7<<endl;
+    // Output:
+    // c7=
+    // 78          -73.2
+    // 4           100000
+    // 5           0.0532
 
     // Clear
     c7.clear();
-    cout<<"c7.clear();"<<endl;
-    cout<<"%> c7=\n"<<c7<<endl;
-    cout<<endl;
+    cout<<"c7=\n"<<c7<<endl;
+    // Output:
+    // c7=
+    // 0           0
+    // 0           0
+    // 0           0
 
     // Resize
     c7.resize(4,4);
@@ -394,181 +424,28 @@
     c7(3,1)=0.02;
     c7(3,2)=0.03;
     c7(3,3)=0.04;
-    cout<<"c7.resize(4,4);"<<endl;
-    cout<<"c7.subMat(0,2,3,2)<<c.subMat(0,0,3,2);"<<endl;
-    cout<<"c7(3,0)=0.01;"<<endl;
-    cout<<"c7(3,1)=0.02;"<<endl;
-    cout<<"c7(3,2)=0.03;"<<endl;
-    cout<<"c7(3,3)=0.04;"<<endl;
-    cout<<"%> c7=\n"<<c7<<endl;
-    cout<<endl;
+    cout<<"c7=\n"<<c7<<endl;
+    // Output:
+    // c7=
+    // 0           0           78          -73.2
+    // 0           0           4           100000
+    // 0           0           5           0.0532
+    // 0.01        0.02        0.03        0.04
+
+
     c7.resize(2,3);
-    cout<<"c7.resize(2,3);"<<endl;
-    cout<<"%> c7=\n"<<c7<<endl;
-    cout<<endl;
+    cout<<"c7=\n"<<c7<<endl;
+    // Output:
+    // c7=
+    // 0           0           78
+    // 0           0           4
 
     return 0;
 }
 
 \end{verbatim}
 
-The output of this code example is the following:
 
-\begin{verbatim}
-real a=15;
-%> a=15
-
-Vec b(3);
-b[0] = 2;
-b[1] = 42;
-b[2] = 21;
-%> b=2           42          21
-%> b.length()=3
-
-Vec b3 = b.subVec(1,2);
-%> b3=42          21
-
-Vec b4 = concat(b,b);
-%> b4=2           42          21          2           42          21
-
-Vec b5 = b4;
-b5[1] = 100000;
-%> b4=2           100000      21          2           42          21
-%> b5=2           100000      21          2           42          21
-
-b5 = b4.copy();
-b5[0]=100001;
-%> b4=2           100000      21          2           42          21
-%> b5=2           100001      21          2           42          21
-
-Vec b6(b4.length());
-b6.fill(3);
-%> b6=3           3           3           3           3           3
-
-b6 << b4;
-%> b6=2           100000      21          2           42          21
-
-b6.clear();
-%> b6=0           0           0           0           0           0
-
-b4.resize(7);
-b4[6] = 6;
-%> b4=2           100000      21          2           42          21          6
-
-b4.resize(4);
-%> b4=2           100000      21          2
-
-Mat c(3,2);
-c(1,1)=1.1;
-c(1,0)=4;
-c(2,0)=5;
-c(0,1)=-73.2;
-c(0,0)=78;
-c(2,1)=5.32e-2;
-%> c=
-78          -73.2
-4           1.1
-5           0.0532
-
-%> c.length()=3
-%> c.width()=2
-
-Mat c3 = c.subMat(1,0,2,1);
-%> c3=
-4
-5
-
-
-Mat c4 = c.column(1);
-%> c4=
--73.2
-1.1
-0.0532
-
-
-Mat c5 = c.row(2);
-%> c5=
-5           0.0532
-
-
-Vec b7 = c(2);
-%> b7=5           0.0532
-
-Mat c6 = c;
-c6(1,1) = 100000;
-%> c=
-78          -73.2
-4           100000
-5           0.0532
-
-%> c6=
-78          -73.2
-4           100000
-5           0.0532
-
-
-c6 = c.copy();
-c6(1,1) = 100001;
-%> c=
-78          -73.2
-4           100000
-5           0.0532
-
-%> c6=
-78          -73.2
-4           100001
-5           0.0532
-
-
-Mat c7(c.length(),c.width());
-c7.fill(3);
-%> c7=
-3           3
-3           3
-3           3
-
-
-c7 << c;
-%> c7=
-78          -73.2
-4           100000
-5           0.0532
-
-
-c7(2) << c(1);
-%> c7=
-78          -73.2
-4           100000
-4           100000
-
-
-c7.clear();
-%> c7=
-0           0
-0           0
-0           0
-
-
-c7.resize(4,4);
-c7.subMat(0,2,3,2)<<c.subMat(0,0,3,2);
-c7(3,0)=0.01;
-c7(3,1)=0.02;
-c7(3,2)=0.03;
-c7(3,3)=0.04;
-%> c7=
-0           0           78          -73.2
-0           0           4           100000
-0           0           5           0.0532
-0.01        0.02        0.03        0.04
-
-
-c7.resize(2,3);
-%> c7=
-0           0           78
-0           0           4
-
-\end{verbatim}
-
 For other useful methods for \texttt{TVec} and
 \texttt{TMat} and more details on their implementation,
 see files \texttt{PLearn/plearn/math/TVec\_\{decl,math\}.h}
@@ -1303,6 +1180,7 @@
 
 \subsection{\label{plearner_vmat}Datasets}
 
+TODO
 
 \subsection{Testing phase}
 TODO



From larocheh at mail.berlios.de  Fri Feb 16 00:29:16 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 16 Feb 2007 00:29:16 +0100
Subject: [Plearn-commits] r6665 - trunk/doc
Message-ID: <200702152329.l1FNTGUm021651@sheep.berlios.de>

Author: larocheh
Date: 2007-02-16 00:29:15 +0100 (Fri, 16 Feb 2007)
New Revision: 6665

Modified:
   trunk/doc/programmers_guide.tex
Log:
Filled in the Dataset section..


Modified: trunk/doc/programmers_guide.tex
===================================================================
--- trunk/doc/programmers_guide.tex	2007-02-15 22:01:23 UTC (rev 6664)
+++ trunk/doc/programmers_guide.tex	2007-02-15 23:29:15 UTC (rev 6665)
@@ -1180,8 +1180,60 @@
 
 \subsection{\label{plearner_vmat}Datasets}
 
-TODO
+In PLearn, the data structure for datasets (training, validation
+and test sets) correspond to the \texttt{VMatrix} class. 
+Conceptually, it corresponds
+to a matrix where each row is a sample
+(training or test example). The length of the \texttt{VMatrix}, that is
+the number of samples it contains, is given by the
+\texttt{length()} method. 
 
+A row is divided in three parts called the input,
+target and weight parts. Their respective sizes are given
+by the methods \texttt{inputsize()}, \texttt{targetsize()}
+and \texttt{weightsize()} of the \texttt{VMatrix} object. 
+The total width of
+the \texttt{VMatrix} is given by the method \texttt{width()} 
+and should be equal to the sum of the input, target
+and weight part sizes. The weight size should also
+be either 1 or 0, that is a sample either
+has a weight or does not.
+
+There are two ways of accessing a sample of a \texttt{VMatrix}.
+The method {\tt getRow(i, row)} can be called, where {\tt i}
+is the index of the row and {\tt row} is a {\tt Vec} which
+will be filled with the input, target and weight parts
+of the $i^{th}$ sample. The length of {\tt row}
+should already be set to the width of the {\texttt VMatrix}.
+
+Another possibility is to call
+{\tt getExample(i, input, target, weight)}, where
+{\tt input} and {\tt target} are {\tt Vec} objects and
+will be filled with the input and target parts of
+the $i^{th}$ sample. The {\tt weight} variable is a reference
+to a {\tt real}, which will be equal to the weight
+of the $i^{th}$ sample. The {\tt input} and {\tt target}
+vectors do not have to be sized according to the
+{\tt VMatrix} input and target part sizes. They
+will be resized appropriately by {\tt VMatrix}
+(this is possible since they are passed as
+references to the method).
+
+Usually, instead of manipulating {\tt VMatrix} objects,
+you will have access to a {\tt VMat} object, which
+can be seen as a pointer to a {\tt VMatrix} object
+and should be used as such. The {\tt VMat} class
+inherits from the class {\tt PP<VMatrix>}, that
+is the class of smart pointers for {\tt VMatrix} objects.
+To know more about smart pointers, see section \ref{PP}.
+
+The {\tt VMatrix} object is implemented in files
+{\tt PLearn/plearn/vmat/VMatrix.\{cc,h\}}, where
+you will find many more methods for this class.
+However, the ones that we described in this
+section will usually be sufficient for the
+implementation of a {\tt PLearner}.
+
 \subsection{Testing phase}
 TODO
 



From chapados at mail.berlios.de  Fri Feb 16 01:04:51 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Fri, 16 Feb 2007 01:04:51 +0100
Subject: [Plearn-commits] r6666 - in trunk/plearn: ker var
Message-ID: <200702160004.l1G04pMk011668@sheep.berlios.de>

Author: chapados
Date: 2007-02-16 01:04:50 +0100 (Fri, 16 Feb 2007)
New Revision: 6666

Modified:
   trunk/plearn/ker/IIDNoiseKernel.cc
   trunk/plearn/ker/IIDNoiseKernel.h
   trunk/plearn/ker/MemoryCachedKernel.h
   trunk/plearn/ker/RationalQuadraticARDKernel.cc
   trunk/plearn/ker/RationalQuadraticARDKernel.h
   trunk/plearn/var/GaussianProcessNLLVariable.cc
   trunk/plearn/var/GaussianProcessNLLVariable.h
Log:
New optimizations to gaussian processes computations

Modified: trunk/plearn/ker/IIDNoiseKernel.cc
===================================================================
--- trunk/plearn/ker/IIDNoiseKernel.cc	2007-02-15 23:29:15 UTC (rev 6665)
+++ trunk/plearn/ker/IIDNoiseKernel.cc	2007-02-16 00:04:50 UTC (rev 6666)
@@ -214,8 +214,8 @@
             
             // Fill upper triangle if not on diagonal
             *Ki++ = Kij;
-            if (j < i)
-                *Kji = Kij;
+            // if (j < i)
+            //     *Kji = Kij;
         }
     }
 }
@@ -263,9 +263,11 @@
                              LKS.size(), kernel_param.size() - LKS.size() - 1));
         PLASSERT( arg < m_kronecker_indexes.size() );
 
-        computeGramMatrixDerivNV<
-            IIDNoiseKernel, &IIDNoiseKernel::derivKronecker>(KD, this, arg);
+        computeGramMatrixDerivKronecker(KD, arg);
         
+        // computeGramMatrixDerivNV<
+        //     IIDNoiseKernel, &IIDNoiseKernel::derivKronecker>(KD, this, arg);
+        
         // int W = nExamples();
         // KD.resize(W,W);
         // real deriv = 2*exp(2*m_log_kronecker_sigma[arg]);
@@ -314,6 +316,46 @@
 }
 
 
+//#####  computeGramMatrixDerivKronecker  #####################################
+
+void IIDNoiseKernel::computeGramMatrixDerivKronecker(Mat& KD, int arg) const
+{
+    // Precompute some terms
+    real kronecker_sigma_arg = 2. * exp(2. * m_log_kronecker_sigma[arg]);
+    int index = m_kronecker_indexes[arg];
+    
+    // Compute Gram Matrix derivative w.r.t. log_kronecker_sigma[arg]
+    int  l = data->length();
+
+    // Variables that walk over the data matrix
+    int  cache_mod = m_data_cache.mod();
+    real *data_start = &m_data_cache(0,0);
+    real *xi = data_start+index;             // Iterator on data rows
+
+    // Variables that walk over the kernel derivative matrix (KD)
+    KD.resize(l,l);
+    real* KDi = KD.data();                   // Start of row i
+    real* KDij;                              // Current element on row i
+    int   KD_mod = KD.mod();
+
+    // Iterate on rows of derivative matrix
+    for (int i=0 ; i<l ; ++i, xi += cache_mod, KDi += KD_mod)
+    {
+        KDij = KDi;
+        real xi_cur = *xi;
+        real *xj  = data_start+index;        // Inner iterator on data rows
+
+        // Iterate on columns of derivative matrix
+        for (int j=0 ; j <= i ; ++j, xj += cache_mod)
+        {
+            // Set into derivative matrix
+            *KDij++ = fast_is_equal(xi_cur, *xj)? kronecker_sigma_arg : 0.0;
+        }
+    }
+}
+
+
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/ker/IIDNoiseKernel.h
===================================================================
--- trunk/plearn/ker/IIDNoiseKernel.h	2007-02-15 23:29:15 UTC (rev 6665)
+++ trunk/plearn/ker/IIDNoiseKernel.h	2007-02-16 00:04:50 UTC (rev 6666)
@@ -129,6 +129,9 @@
     //! Derivative function with respect to kronecker_indexes[arg] hyperparameter
     real derivKronecker(int i, int j, int arg, real K) const;
 
+    //! Derivative w.r.t kronecker_indexes[arg] for WHOLE MATRIX
+    void computeGramMatrixDerivKronecker(Mat& KD, int arg) const;
+    
 protected:
     //! Buffer for exponential of m_log_kronecker_sigma
     mutable Vec m_kronecker_sigma;

Modified: trunk/plearn/ker/MemoryCachedKernel.h
===================================================================
--- trunk/plearn/ker/MemoryCachedKernel.h	2007-02-15 23:29:15 UTC (rev 6665)
+++ trunk/plearn/ker/MemoryCachedKernel.h	2007-02-16 00:04:50 UTC (rev 6666)
@@ -285,21 +285,18 @@
 
     int W = nExamples();
     KD.resize(W,W);
-    int m=KD.mod();
     
     real KDij;
     real* KDi;
-    real* KDji;
     real  K  = MISSING_VALUE;
     real* Ki = 0;                       // Current row of kernel matrix, if cached
 
     for (int i=0 ; i<W ; ++i) {
         KDi  = KD[i];
-        KDji = &KD[0][i];
         if (gram_matrix_is_cached)
             Ki = gram_matrix[i];
         
-        for (int j=0 ; j <= i ; ++j, KDji += m) {
+        for (int j=0 ; j <= i ; ++j) {
             // Access the current kernel value depending on whether it's cached
             if (Ki)
                 K = *Ki++;
@@ -312,8 +309,6 @@
             // Compute and store the derivative
             KDij   = (This->*derivativeFunc)(i, j, arg, K);
             *KDi++ = KDij;
-            if (j < i)
-                *KDji = KDij;
         }
     }
 }

Modified: trunk/plearn/ker/RationalQuadraticARDKernel.cc
===================================================================
--- trunk/plearn/ker/RationalQuadraticARDKernel.cc	2007-02-15 23:29:15 UTC (rev 6665)
+++ trunk/plearn/ker/RationalQuadraticARDKernel.cc	2007-02-16 00:04:50 UTC (rev 6666)
@@ -106,7 +106,8 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
-    deepCopyField(m_noise_gram_cache, copies);
+    deepCopyField(m_noise_gram_cache,        copies);
+    deepCopyField(m_pow_minus_alpha_minus_1, copies);
 }
 
 
@@ -173,6 +174,11 @@
         m_input_sigma += m_log_input_sigma;
     m_input_sigma *= 2.0;
     exp(m_input_sigma, m_input_sigma);
+
+    // Prepare the cache for the pow terms
+    m_pow_minus_alpha_minus_1.resize(K.length(), K.width());
+    int   pow_cache_mod = m_pow_minus_alpha_minus_1.mod();
+    real* pow_cache_row = m_pow_minus_alpha_minus_1.data();
     
     // Compute Gram Matrix
     int  l = data->length();
@@ -181,23 +187,25 @@
     int  cache_mod = m_data_cache.mod();
 
     real *data_start = &m_data_cache(0,0);
-    real Kij;
-    real *Ki, *Kji, *x1, *x2;
+    real *Ki = K[0];                         // Start of current row
+    real *Kij;                               // Current element along row
     real *input_sigma_data = m_input_sigma.data();
     real *xi = data_start;
     
-    for (int i=0 ; i<l ; ++i, xi += cache_mod) {
-        Ki  = K[i];
-        Kji = &K[0][i];
+    for (int i=0 ; i<l
+             ; ++i, xi += cache_mod, pow_cache_row+=pow_cache_mod, Ki+=m)
+    {
+        Kij = Ki;
         real *xj = data_start;
+        real *pow_cache_cur = pow_cache_row;
 
-        for (int j=0; j<=i; ++j, Kji += m, xj += cache_mod) {
+        for (int j=0; j<=i; ++j, xj += cache_mod) {
             // Kernel evaluation per se
-            x1 = xi;
-            x2 = xj;
-            real* p_inpsigma = input_sigma_data;
+            real *x1 = xi;
+            real *x2 = xj;
+            real *p_inpsigma = input_sigma_data;
             real sum_wt = 0.0;
-            int k = n;
+            int  k = n;
 
             // Use Duff's device to unroll the following loop:
             //     while (k--) {
@@ -216,13 +224,14 @@
             case 1:      diff = *x1++ - *x2++; sum_wt += (diff*diff) / *p_inpsigma++;
                        } while((k -= 8) > 0);
             }
+
+            real inner_pow   = 1 + sum_wt / (2.*alpha);
+            real pow_alpha   = pow(inner_pow, -alpha);
+            real Kij_cur     = sf2 * pow_alpha;
+            *pow_cache_cur++ = Kij_cur / inner_pow;
             
-            Kij = sf2 * pow(1 + sum_wt / (2.*alpha), -alpha);
-            
             // Update kernel matrix (already pre-filled with IID noise terms)
-            *Ki++ += Kij;
-            if (j < i)
-                *Kji += Kij;
+            *Kij++ += Kij_cur;
         }
     }
     if (cache_gram_matrix) {
@@ -267,9 +276,7 @@
         //     &RationalQuadraticARDKernel::derivLogInputSigma>(KD, this, arg);
     }
     else if (kernel_param == LAL) {
-        computeGramMatrixDerivNV<
-            RationalQuadraticARDKernel,
-            &RationalQuadraticARDKernel::derivLogAlpha>(KD, this, -1);
+        computeGramMatrixDerivLogAlpha(KD);
     }
     else
         inherited::computeGramMatrixDerivative(KD, kernel_param, epsilon);
@@ -322,13 +329,10 @@
     // Rederive the value of k
     Vec& row_i   = *dataRow(i);
     Vec& row_j   = *dataRow(j);
-    real alpha   = exp(m_log_alpha);
-    real noise   = m_noise_gram_cache(i,j);
-    K -= noise;
-    real k       = exp(- (pl_log(K) - 2*m_log_signal_sigma) / alpha);
+    real K_over_k= m_pow_minus_alpha_minus_1(i,j);
     real diff    = row_i[arg] - row_j[arg];
     real sq_diff = diff * diff;
-    return (K / k) * exp(-2 * (m_log_global_sigma + m_log_input_sigma[arg])) * sq_diff;
+    return K_over_k * exp(-2 * (m_log_global_sigma + m_log_input_sigma[arg])) * sq_diff;
 }
 
 
@@ -353,62 +357,109 @@
                                                                      int arg) const
 {
     // Precompute some terms
-    real alpha = exp(m_log_alpha);
-    real twice_log_signal_sigma = 2.*m_log_signal_sigma;
+    real input_sigma_arg = m_input_sigma[arg];
     
     // Compute Gram Matrix derivative w.r.t. log_input_sigma[arg]
     int  l = data->length();
-    int  k_mod     = gram_matrix.mod();
+
+    // Variables that walk over the data matrix
     int  cache_mod = m_data_cache.mod();
+    real *data_start = &m_data_cache(0,0);
+    real *xi = data_start+arg;               // Iterator on data rows
 
-    // Variables that walk over the pre-computed kernel (K) and data matrices
-    real *input_sigma_data = m_input_sigma.data();
-    real *data_start = &m_data_cache(0,0);
-    real *xi = data_start;                   // Iterator on data rows
+    // Variables that walk over the pow cache
+    int   pow_cache_mod = m_pow_minus_alpha_minus_1.mod();
+    real *pow_cache_row = m_pow_minus_alpha_minus_1.data();
+    real *pow_cache_cur;
+    
+    // Variables that walk over the kernel derivative matrix (KD)
+    KD.resize(l,l);
+    real* KDi = KD.data();                   // Start of row i
+    real* KDij;                              // Current element on row i
+    int   KD_mod = KD.mod();
+
+    // Iterate on rows of derivative matrix
+    for (int i=0 ; i<l ; ++i, xi += cache_mod, KDi += KD_mod,
+             pow_cache_row += pow_cache_mod)
+    {
+        KDij = KDi;
+        real *xj  = data_start+arg;           // Inner iterator on data rows
+        pow_cache_cur = pow_cache_row;
+
+        // Iterate on columns of derivative matrix
+        for (int j=0 ; j <= i
+                 ; ++j, xj += cache_mod, ++pow_cache_cur)
+        {
+            real diff    = *xi - *xj;
+            real sq_diff = diff * diff;
+            real KD_cur  = *pow_cache_cur * sq_diff / input_sigma_arg;
+
+            // Set into derivative matrix
+            *KDij++ = KD_cur;
+        }
+    }
+}
+
+
+//#####  computeGramMatrixDerivLogAlpha  ######################################
+
+void RationalQuadraticARDKernel::computeGramMatrixDerivLogAlpha(Mat& KD) const
+{
+    // Precompute some terms
+    real alpha = exp(m_log_alpha);
+    
+    // Compute Gram Matrix derivative w.r.t. log_alpha
+    int  l = data->length();
+    int  k_mod     = gram_matrix.mod();
+
+    // Variables that walk over the pre-computed kernel matrix (K) 
     real *Ki = &gram_matrix(0,0);            // Current row of kernel matrix
     real *Kij;                               // Current element of kernel matrix
 
+    // Variables that walk over the pow cache
+    int   pow_cache_mod = m_pow_minus_alpha_minus_1.mod();
+    real *pow_cache_row = m_pow_minus_alpha_minus_1.data();
+    real *pow_cache_cur;
+
     // Variables that walk over the noise cache
-    real *noise_start_row = m_noise_gram_cache.data();
-    real *cur_noise;                         // Current element of noise matrix
-    int  noise_mod = m_noise_gram_cache.mod();
+    int   noise_cache_mod = m_noise_gram_cache.mod();
+    real *noise_cache_row = m_noise_gram_cache[0];
+    real *noise_cache_cur;
     
     // Variables that walk over the kernel derivative matrix (KD)
     KD.resize(l,l);
     real* KDi = KD.data();                   // Start of row i
-    real* KDj = KD.data();                   // Start of column j
     real* KDij;                              // Current element on row i
-    real* KDji;                              // Current element on column j
     int   KD_mod = KD.mod();
 
     // Iterate on rows of derivative matrix
-    for (int i=0 ; i<l ; ++i, xi += cache_mod, Ki += k_mod,
-             KDi += KD_mod, ++KDj, noise_start_row += noise_mod)
+    for (int i=0 ; i<l ; ++i, Ki += k_mod,
+             KDi += KD_mod, pow_cache_row += pow_cache_mod,
+             noise_cache_row += noise_cache_mod)
     {
         Kij  = Ki;
         KDij = KDi;
-        KDji = KDj;
-        real *xj  = data_start;              // Inner iterator on data rows
-        cur_noise = noise_start_row;
+        pow_cache_cur   = pow_cache_row;
+        noise_cache_cur = noise_cache_row;
 
         // Iterate on columns of derivative matrix
         for (int j=0 ; j <= i
-                 ; ++j, ++Kij, KDji+=KD_mod, xj += cache_mod, ++cur_noise)
+                 ; ++j, ++Kij, ++noise_cache_cur, ++pow_cache_cur)
         {
-            real K       = *Kij - *cur_noise;
-            real k       = exp(- (pl_log(K) - twice_log_signal_sigma) / alpha);
-            real diff    = xi[arg] - xj[arg];
-            real sq_diff = diff * diff;
-            real KD_cur  = (K / k) * sq_diff / input_sigma_data[arg];
+            real K      = *Kij - *noise_cache_cur;
+            real k      = K / *pow_cache_cur;
+            real left   = -alpha * pl_log(k);
+            real num    = (k - 1) * 2. * alpha;
+            real denum  = 2. * k;
+            real KD_cur = K * (left + num / denum);
             
             // Set into derivative matrix
             *KDij++ = KD_cur;
-            if (j < i)
-                *KDji = KD_cur;
         }
     }
 }
 
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/ker/RationalQuadraticARDKernel.h
===================================================================
--- trunk/plearn/ker/RationalQuadraticARDKernel.h	2007-02-15 23:29:15 UTC (rev 6665)
+++ trunk/plearn/ker/RationalQuadraticARDKernel.h	2007-02-16 00:04:50 UTC (rev 6666)
@@ -127,10 +127,19 @@
     // Compute derivative w.r.t. log_input_sigma[arg] for WHOLE MATRIX
     void computeGramMatrixDerivLogInputSigma(Mat& KD, int arg) const;
     
+    // Compute derivative w.r.t. log_alpha for WHOLE MATRIX
+    void computeGramMatrixDerivLogAlpha(Mat& KD) const;
+    
 protected:
     //! Cached version of IID noise gram matrix
     mutable Mat m_noise_gram_cache;
 
+    /**
+     *  Cached version of the K / k terms, useful for computing derivatives
+     *      pow(1 + sum_wt / (2*alpha), -alpha-1)
+     */
+    mutable Mat m_pow_minus_alpha_minus_1;
+
 private:
     //! This does the actual building.
     void build_();

Modified: trunk/plearn/var/GaussianProcessNLLVariable.cc
===================================================================
--- trunk/plearn/var/GaussianProcessNLLVariable.cc	2007-02-15 23:29:15 UTC (rev 6665)
+++ trunk/plearn/var/GaussianProcessNLLVariable.cc	2007-02-16 00:04:50 UTC (rev 6666)
@@ -117,7 +117,8 @@
     deepCopyField(m_gram,            copies);
     deepCopyField(m_gram_derivative, copies);
     deepCopyField(m_cholesky_gram,   copies);
-    deepCopyField(m_alpha,           copies);
+    deepCopyField(m_alpha_t,         copies);
+    deepCopyField(m_alpha_buf,       copies);
     deepCopyField(m_inverse_gram,    copies);
     deepCopyField(m_cholesky_gram,   copies);
 }
@@ -136,22 +137,34 @@
 }
 
 
+//#####  alpha  ###############################################################
+
+const Mat& GaussianProcessNLLVariable::alpha() const
+{
+    m_alpha_buf.resize(m_alpha_t.width(), m_alpha_t.length());
+    transpose(m_alpha_t, m_alpha_buf);
+    return m_alpha_buf;
+}
+    
+
 //#####  fprop  ###############################################################
 
 // ### computes value from varray values
 void GaussianProcessNLLVariable::fprop()
 {
     fbpropFragments(m_kernel, m_noise, m_inputs, m_targets, m_allow_bprop,
-                    m_gram, m_cholesky_gram, m_alpha, m_inverse_gram,
+                    m_gram, m_cholesky_gram, m_alpha_t, m_inverse_gram,
                     m_cholesky_tmp, m_rhs_tmp);
 
     // Assuming y is a column vector...  For multivariate targets, we
     // separately dot each column of the targets with corresponding columns of
     // alpha, and add as many of the other two terms as there are variables
     //
-    // 0.5 * y'*alpha + sum(log(diag(L))) + 0.5*n*log(2*pi)
-    const int n = m_alpha.length();
-    const int m = m_alpha.width();
+    //     0.5 * y'*alpha + sum(log(diag(L))) + 0.5*n*log(2*pi)
+    //
+    // Don't forget that alpha_t is transposed
+    const int n = m_alpha_t.width();
+    const int m = m_alpha_t.length();
 
     real logdet_log2pi = 0;
     for (int i=0 ; i<n ; ++i)
@@ -160,7 +173,7 @@
     
     real nll = 0;
     for (int i=0 ; i<m ; ++i)
-        nll += 0.5*dot(m_targets.column(i), m_alpha.column(i)) + logdet_log2pi;
+        nll += 0.5*dot(m_targets.column(i), m_alpha_t.row(i)) + logdet_log2pi;
     value[0] = nll;
 }
 
@@ -174,7 +187,7 @@
                   "GaussianProcessNLLVariable must be constructed with the option "
                   "'will_bprop'=True in order to call bprop" );
     PLASSERT( m_hyperparam_names.size() == m_hyperparam_vars.size() );
-    PLASSERT( m_alpha.length() == m_inverse_gram.width() );
+    PLASSERT( m_alpha_t.width() == m_inverse_gram.width() );
     PLASSERT( m_inverse_gram.width() == m_inverse_gram.length() );
     PLASSERT( m_kernel );
     
@@ -187,40 +200,44 @@
     // Since both the first term inside the trace and the derivative of the
     // gram matrix are symmetric square matrices, the trace is efficiently
     // computed as the sum of the elementwise product of those matrices.
+    //
+    // Don't forget that m_alpha_t is transposed.
     for (int j=0, m=m_hyperparam_names.size() ; j<m ; ++j) {
         real dnll_dj = 0;
         m_kernel->computeGramMatrixDerivative(m_gram_derivative,
                                               m_hyperparam_names[j]);
-        for (int i=0, n=m_alpha.width() ; i<n ; ++i) {
-            Mat curalpha_mat = m_alpha.column(i);
-            int curalpha_mod = curalpha_mat.mod();
-            real* curalpha   = curalpha_mat[0];
-            real  curtrace   = 0.0;
+        for (int i=0, n=m_alpha_t.length() ; i<n ; ++i) {
+            real* curalpha = m_alpha_t[i];
+            real  cur_trace = 0.0;
 
             // Sum over all rows and columns of matrix
             real* curalpha_row = curalpha;
             for (int row=0, nrows=m_inverse_gram.length()
-                     ; row<nrows ; ++row, curalpha_row += curalpha_mod)
+                     ; row<nrows ; ++row, ++curalpha_row)
             {
-                real* p_inverse_gram    = m_inverse_gram[row];
-                real* p_gram_derivative = m_gram_derivative[row];
-                real  curalpha_row      = curalpha[row * curalpha_mod];
-                real* curalpha_col      = curalpha;
+                real* p_inverse_gram     = m_inverse_gram[row];
+                real* p_gram_derivative  = m_gram_derivative[row];
+                real  curalpha_row_value = *curalpha_row;
+                real* curalpha_col       = curalpha;
+                real  row_trace          = 0.0;
 
-                for (int col=0, ncols=m_inverse_gram.width()
-                         ; col<ncols ; ++col, curalpha_col += curalpha_mod)
+                for (int col=0 ; col <= row ; ++col, ++curalpha_col)
                 {
-                    curtrace +=
-                        (*p_inverse_gram++ - curalpha_row * *curalpha_col)
+                    if (col == row)
+                        row_trace *= 2.;
+                    
+                    row_trace +=
+                        (*p_inverse_gram++ - curalpha_row_value * *curalpha_col)
                         * *p_gram_derivative++;
 
                     // curtrace +=
                     //     (m_inverse_gram(row,col) - curalpha(row,0)*curalpha(col,0))
                     //     * m_gram_derivative(row,col);
                 }
+                cur_trace += row_trace;
             }
 
-            dnll_dj += curtrace / 2.0;
+            dnll_dj += cur_trace / 2.0;
         }
         m_hyperparam_vars[j]->gradient[0] += dnll_dj * gradient[0];
     }
@@ -232,7 +249,7 @@
 #ifndef USE_BLAS_SPECIALISATIONS
 void GaussianProcessNLLVariable::fbpropFragments(
     Kernel* kernel, real noise, const Mat& inputs, const Mat& targets,
-    bool compute_inverse, Mat& gram, Mat& L, Mat& alpha, Mat& inv,
+    bool compute_inverse, Mat& gram, Mat& L, Mat& alpha_t, Mat& inv,
     Vec& tmp_chol, Mat& tmp_rhs)
 {
     PLASSERT( kernel );
@@ -259,15 +276,20 @@
     addToDiagonal(gram, noise);
 
     // Compute Cholesky decomposition and solve the linear system
-    alpha.resize(trainlength, rhs_width);
+    alpha_t.resize(trainlength, rhs_width);
     L.resize(trainlength, trainlength);
     tmp_chol.resize(trainlength);
-    solveLinearSystemByCholesky(gram, tmp_rhs, alpha, &L, &tmp_chol);
-    
+    solveLinearSystemByCholesky(gram, tmp_rhs, alpha_t, &L, &tmp_chol);
+
+    // Must return transpose here since the code has been modified to work with
+    // a transposed alpha, to better interface with lapack (much faster in the
+    // latter case to avoid superfluous transposes).
     if (compute_inverse) {
-        inv   = alpha.subMatColumns(targetsize, trainlength);
-        alpha = alpha.subMatColumns(0, targetsize);
+        inv     = alpha_t.subMatColumns(targetsize, trainlength);
+        alpha_t = transpose(alpha_t.subMatColumns(0, targetsize));
     }
+    else
+        alpha_t = transpose(alpha_t);
 }
 #endif
 
@@ -276,7 +298,7 @@
 #ifdef USE_BLAS_SPECIALISATIONS
 void GaussianProcessNLLVariable::fbpropFragments(
     Kernel* kernel, real noise, const Mat& inputs, const Mat& targets,
-    bool compute_inverse, Mat& gram, Mat& L, Mat& alpha, Mat& inv,
+    bool compute_inverse, Mat& gram, Mat& L, Mat& alpha_t, Mat& inv,
     Vec& tmp_chol, Mat& tmp_rhs)
 {
     PLASSERT( kernel );
@@ -287,12 +309,14 @@
     // The RHS matrix (when solving the linear system Gram*Params=RHS) is made
     // up of two parts: the regression targets themselves, and the identity
     // matrix if we requested them (for confidence intervals).  After solving
-    // the linear system, set the gram-inverse appropriately.
+    // the linear system, set the gram-inverse appropriately.  To interface
+    // nicely with LAPACK, we store this in a transposed format.
     int rhs_width = targetsize + (compute_inverse? trainlength : 0);
-    tmp_rhs.resize(trainlength, rhs_width);
-    tmp_rhs.subMatColumns(0, targetsize) << targets;
+    tmp_rhs.resize(rhs_width, trainlength);
+    Mat targets_submat = tmp_rhs.subMatRows(0, targetsize);
+    transpose(targets, targets_submat);
     if (compute_inverse) {
-        Mat rhs_identity = tmp_rhs.subMatColumns(targetsize, trainlength);
+        Mat rhs_identity = tmp_rhs.subMatRows(targetsize, trainlength);
         identityMatrix(rhs_identity);
     }
 
@@ -307,13 +331,13 @@
     // matrices after solving.  Note that for now we don't bother to create an
     // appropriately transposed RHS (will come later).
     lapackCholeskyDecompositionInPlace(gram);
-    lapackCholeskySolveInPlace(gram, tmp_rhs);
-    alpha = tmp_rhs;                         // LAPACK solves in-place
-    L     = gram;                            // LAPACK solves in-place
+    lapackCholeskySolveInPlace(gram, tmp_rhs, true /* column-major */);
+    alpha_t = tmp_rhs;                         // LAPACK solves in-place
+    L       = gram;                            // LAPACK solves in-place
     
     if (compute_inverse) {
-        inv   = alpha.subMatColumns(targetsize, trainlength);
-        alpha = alpha.subMatColumns(0, targetsize);
+        inv     = alpha_t.subMatRows(targetsize, trainlength);
+        alpha_t = alpha_t.subMatRows(0, targetsize);
     }
 }
 #endif

Modified: trunk/plearn/var/GaussianProcessNLLVariable.h
===================================================================
--- trunk/plearn/var/GaussianProcessNLLVariable.h	2007-02-15 23:29:15 UTC (rev 6665)
+++ trunk/plearn/var/GaussianProcessNLLVariable.h	2007-02-16 00:04:50 UTC (rev 6666)
@@ -129,7 +129,7 @@
                                 Vec& tmpch, Mat& tmprhs);
 
     //! Accessor to the last computed 'alpha' matrix in an fprop
-    const Mat& alpha() const { return m_alpha; }
+    const Mat& alpha() const;
 
     //! Accessor to the last computed gram matrix in an fprop
     const Mat& gram() const { return m_gram; }
@@ -183,9 +183,14 @@
     //! Holds the Cholesky decomposition of m_gram
     Mat m_cholesky_gram;
 
-    //! Solution of the linear system gram*alpha = targets
-    Mat m_alpha;
+    //! Solution of the linear system gram*alpha = targets.  This is actually
+    //! stored as a transpose to interface better with lapack.
+    Mat m_alpha_t;
 
+    //! Temporary buffer to hold the transpose of m_alpha_t; used for the
+    //! alpha() accessor and outside-world interface
+    mutable Mat m_alpha_buf;
+    
     //! Inverse of the Gram matrix
     Mat m_inverse_gram;
     



From dorionc at mail.berlios.de  Fri Feb 16 17:00:09 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Fri, 16 Feb 2007 17:00:09 +0100
Subject: [Plearn-commits] r6667 - in trunk/python_modules/plearn: parallel
	report utilities
Message-ID: <200702161600.l1GG09Rg011629@sheep.berlios.de>

Author: dorionc
Date: 2007-02-16 17:00:08 +0100 (Fri, 16 Feb 2007)
New Revision: 6667

Modified:
   trunk/python_modules/plearn/parallel/dispatch.py
   trunk/python_modules/plearn/report/formatter.py
   trunk/python_modules/plearn/report/graphical_tools.py
   trunk/python_modules/plearn/utilities/pldatetime.py
Log:
- Added FIGSIZE support in FigureWrapper
- Added a PDF creator
- Minor bug fix in dispatch when no machines are currently available


Modified: trunk/python_modules/plearn/parallel/dispatch.py
===================================================================
--- trunk/python_modules/plearn/parallel/dispatch.py	2007-02-16 00:04:50 UTC (rev 6666)
+++ trunk/python_modules/plearn/parallel/dispatch.py	2007-02-16 16:00:08 UTC (rev 6667)
@@ -310,9 +310,10 @@
         except StopIteration:
             cls._available_machines = None
             if new_loop:
-                raise EmptyMachineListError
-            else:
-                return cls.nextAvailableMachine()
+                time.sleep(3)                
+                #raise EmptyMachineListError 
+            #else:
+            return cls.nextAvailableMachine()
     nextAvailableMachine = classmethod(nextAvailableMachine)
 
     #

Modified: trunk/python_modules/plearn/report/formatter.py
===================================================================
--- trunk/python_modules/plearn/report/formatter.py	2007-02-16 00:04:50 UTC (rev 6666)
+++ trunk/python_modules/plearn/report/formatter.py	2007-02-16 16:00:08 UTC (rev 6667)
@@ -30,7 +30,7 @@
 #  library, go to the PLearn Web site at www.plearn.org
 
 # Author: Christian Dorion
-import sys
+import os, sys
 from plearn.pyplearn.plearn_repr import plearn_repr
 
 DEFAULT_WRITER = sys.stdout.write
@@ -208,3 +208,62 @@
 
     return merged_table, merged_headers
         
+#####  PDF creator  #########################################################
+
+TEX_BEGIN = r"""
+%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
+%% Automatically generated by the 'mytex' program
+%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
+
+\documentclass[11pt]{article}
+\usepackage{apstat_article_style}
+\usepackage{mcgill_cover_classical}
+
+\usepackage{lscape}
+
+\include{MathStuff}
+\include{LabelsAndRefs}
+\newcommand{\prgname}[1]{\texttt{#1}}
+\newcommand{\citeP}[1]{(\citeNP{#1})} %% Since I overwrite \cite...
+\renewcommand{\cite}[1]{\citeN{#1}}
+
+%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
+
+\renewcommand{\tabcaption}[1]{%
+  \refstepcounter{table}%
+  {\raggedright\hspace{0pt}\small\it 
+      \captionheadfont\caparrowup%
+      \tablename~\mbox{\arabic{table}}.~%
+      \captionbodyfont{#1}%
+  }}%
+
+%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
+
+\begin{document}
+"""
+
+TEX_END = r"""
+
+%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
+%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
+\cleardoublepage
+\end{document}
+"""
+
+def createPDF(file_name, content):
+    assert file_name.endswith('.tex')    
+    if isinstance(content, list):
+        content = '\n'.join(content)
+    
+    tex_file = open(file_name, 'w')
+    tex_file.write(TEX_BEGIN)
+    tex_file.write(content)
+    tex_file.write(TEX_END)
+    tex_file.close()
+
+    pdf_name = file_name.replace(".tex", '.pdf')
+    os.system("rm -f %s"%pdf_name)
+    os.system("pdflatex -interaction=nonstopmode %s >& /dev/null"%file_name)
+    assert os.path.exists(pdf_name), "PDF could not be created!"
+    os.system("pdflatex %s >& /dev/null"%file_name)
+    os.system("pdflatex %s >& /dev/null"%file_name)

Modified: trunk/python_modules/plearn/report/graphical_tools.py
===================================================================
--- trunk/python_modules/plearn/report/graphical_tools.py	2007-02-16 00:04:50 UTC (rev 6666)
+++ trunk/python_modules/plearn/report/graphical_tools.py	2007-02-16 16:00:08 UTC (rev 6667)
@@ -33,15 +33,16 @@
 import pylab, os
 from plearn.report import GRID_COL, FONTSIZE, LEGEND_FONTPROP, TICK_LABEL_FONTPROP
 
-LEFT, WIDTH = 0.125, 0.8
+LEFT,   WIDTH  = 0.125, 0.800
+BOTTOM, HEIGHT = 0.100, 0.800
 LINE_COLORS = [ '#660033', 'b', 'r', 'k', "#CDBE70",
                 "#FF8C69", "#65754D", "#4d6575", "#754d65" ]
 
-__figure_counter = 0
+_figure_counter = 0
 def getNewFigure(figsize=(12,10)):
-    global __figure_counter
-    __figure_counter += 1
-    return pylab.figure(__figure_counter, figsize=figsize)
+    global _figure_counter
+    _figure_counter += 1
+    return pylab.figure(_figure_counter, figsize=figsize)
 
 def getBounds(frame):
     return [ frame.get_x(), frame.get_y(), frame.get_width(), frame.get_height() ]
@@ -52,6 +53,26 @@
 def plotZeroLine(axes, color='#666666'):
     axes.plot(axes.get_xlim(), [0,0], color=color)
 
+def same_xlim(*ax_list):
+    m, M = float('inf'), -float('inf')
+    for axes in ax_list:
+        xlim = axes.get_xlim()
+        m, M = min(m, xlim[0]), max(M, xlim[1]), 
+
+    for axes in ax_list:
+        axes.set_xlim(m, M)
+    return m, M
+
+def same_ylim(*ax_list):
+    m, M = float('inf'), -float('inf')
+    for axes in ax_list:
+        ylim = axes.get_ylim()
+        m, M = min(m, ylim[0]), max(M, ylim[1]), 
+
+    for axes in ax_list:
+        axes.set_ylim(m, M)
+    return m, M
+
 def setLegend(axes, legend_map, sorted_keys=None, loc=0):
     if not sorted_keys:
         sorted_keys = legend_map.keys(); sorted_keys.sort()
@@ -79,10 +100,13 @@
         self.max = max(limits[1], self.max)    
 
 class FigureWrapper(object):
+    FIGSIZE = (12,10)
     instances = []
-
-    def __init__(self, figsize=(12,10)):
+    
+    def __init__(self, figsize=None):
+        if figsize is None: figsize = self.FIGSIZE
         self.figure = getNewFigure(figsize)
+        self.figno  = _figure_counter  
         self.instances.append(self)
 
     def addAxes(self, rect, *args, **kwargs):

Modified: trunk/python_modules/plearn/utilities/pldatetime.py
===================================================================
--- trunk/python_modules/plearn/utilities/pldatetime.py	2007-02-16 00:04:50 UTC (rev 6666)
+++ trunk/python_modules/plearn/utilities/pldatetime.py	2007-02-16 16:00:08 UTC (rev 6667)
@@ -10,7 +10,6 @@
     cyymm,dd = divmod(cyymmdd,100)
     cyy,mm   = divmod(cyymm,100)
     yyyy = 1900+cyy
-
     return date(yyyy,mm,dd)
 
 def yyyymmdd_to_date(x):    



From dorionc at mail.berlios.de  Fri Feb 16 17:01:32 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Fri, 16 Feb 2007 17:01:32 +0100
Subject: [Plearn-commits] r6668 - in trunk/plearn: base sys
Message-ID: <200702161601.l1GG1W33011739@sheep.berlios.de>

Author: dorionc
Date: 2007-02-16 17:01:31 +0100 (Fri, 16 Feb 2007)
New Revision: 6668

Modified:
   trunk/plearn/base/stringutils.cc
   trunk/plearn/base/stringutils.h
   trunk/plearn/sys/Popen.cc
Log:
Improved on the naive quoting of arguments in Popen::lauch. It caused
problems when arguments where already containing single quotes...


Modified: trunk/plearn/base/stringutils.cc
===================================================================
--- trunk/plearn/base/stringutils.cc	2007-02-16 16:00:08 UTC (rev 6667)
+++ trunk/plearn/base/stringutils.cc	2007-02-16 16:01:31 UTC (rev 6668)
@@ -196,7 +196,26 @@
     }
     return s;
 }
-    
+
+//! Quote the provided string 's'
+string quote_string(const string& s)
+{
+    string quoted(s);
+        
+    // Escape the existing quotes
+    string::size_type pos = quoted.find("\"");
+    while ( pos != quoted.npos )
+    {
+        quoted.insert(pos, "\\");
+        pos = quoted.find("\"", pos+2); // +2 since the inserted char...
+    }
+
+    // Quote the string
+    quoted.insert(0, "\"");
+    quoted.insert(quoted.size(), "\"");
+    return quoted;
+}
+
 string remove_trailing_slash(const string& s)
 {
     string::size_type pos = s.length();

Modified: trunk/plearn/base/stringutils.h
===================================================================
--- trunk/plearn/base/stringutils.h	2007-02-16 16:00:08 UTC (rev 6667)
+++ trunk/plearn/base/stringutils.h	2007-02-16 16:01:31 UTC (rev 6668)
@@ -93,6 +93,9 @@
 //!  if there is none, return the string unmodified
 string removequotes(const string& s);
 
+//! Quote the provided string 's'
+string quote_string(const string& s);
+
 //!  convert a string to all lowercase
 string lowerstring(const string& s);
   

Modified: trunk/plearn/sys/Popen.cc
===================================================================
--- trunk/plearn/sys/Popen.cc	2007-02-16 16:00:08 UTC (rev 6667)
+++ trunk/plearn/sys/Popen.cc	2007-02-16 16:01:31 UTC (rev 6668)
@@ -91,11 +91,7 @@
     string concatenated_args = program;
     for (vector<string>::const_iterator it = arguments.begin(); it != arguments.end();
          ++it)
-    {
-        concatenated_args += " '";
-        concatenated_args += *it;
-        concatenated_args += "'";
-    }
+        concatenated_args += " "+quote_string(*it);
     args[2] = concatenated_args.c_str();
     args[3] = 0;
   



From chrish at mail.berlios.de  Fri Feb 16 22:03:27 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Fri, 16 Feb 2007 22:03:27 +0100
Subject: [Plearn-commits] r6669 - trunk/plearn/vmat
Message-ID: <200702162103.l1GL3RQr013004@sheep.berlios.de>

Author: chrish
Date: 2007-02-16 22:03:27 +0100 (Fri, 16 Feb 2007)
New Revision: 6669

Modified:
   trunk/plearn/vmat/VMatLanguage.cc
Log:
Rewrite second half of variable product VPL implementation to make it work.


Modified: trunk/plearn/vmat/VMatLanguage.cc
===================================================================
--- trunk/plearn/vmat/VMatLanguage.cc	2007-02-16 16:01:31 UTC (rev 6668)
+++ trunk/plearn/vmat/VMatLanguage.cc	2007-02-16 21:03:27 UTC (rev 6669)
@@ -1310,18 +1310,28 @@
             }
 
             Vec res(result_size, 1);
-            int step_size = result_size;
+            int length_of_run = 1;
+            int gap_between_runs = 1;
 
             // Accumulate variable product into "res" variable.
-            for (int var_index = 0; var_index < num_vars; var_index++) {
-                step_size /= vars[var_index].size();
-                for (int var_data_index = 0; var_data_index < vars[var_index].size(); var_data_index++) {
-                    for (int dest_index = 0; dest_index < result_size; dest_index += step_size) {
-                        res[dest_index] *= vars[var_index][var_data_index];
+            for (int var_index = num_vars - 1; var_index >= 0; var_index--) {
+                const int current_var_size = vars[var_index].size();
+                gap_between_runs *= current_var_size;
+                int start_dest_index = 0;
+
+                for (int var_data_index = 0; var_data_index < current_var_size; var_data_index++) {
+                    int dest_index = start_dest_index;
+                    while (dest_index < result_size) {
+                        int start_of_run = dest_index;
+                        while (dest_index - start_of_run < length_of_run) {
+                            res[dest_index++] *= vars[var_index][var_data_index];
+                        }
+                        dest_index += gap_between_runs - 1;
                     }
+                    start_dest_index += length_of_run;
                 }
+                length_of_run = gap_between_runs;
             }
-            PLASSERT(step_size == 1);
 
             // Put the result onto the stack
             for (int i = 0; i < result_size; i++)



From dorionc at mail.berlios.de  Tue Feb 20 15:55:12 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Tue, 20 Feb 2007 15:55:12 +0100
Subject: [Plearn-commits] r6670 - trunk/python_modules/plearn/parallel
Message-ID: <200702201455.l1KEtClC030290@sheep.berlios.de>

Author: dorionc
Date: 2007-02-20 15:55:10 +0100 (Tue, 20 Feb 2007)
New Revision: 6670

Modified:
   trunk/python_modules/plearn/parallel/dispatch.py
Log:
- Rolled back to the old delay version which is way faster when all machines
are quite busy...
- Improved the way to deal with busy machines...


Modified: trunk/python_modules/plearn/parallel/dispatch.py
===================================================================
--- trunk/python_modules/plearn/parallel/dispatch.py	2007-02-16 21:03:27 UTC (rev 6669)
+++ trunk/python_modules/plearn/parallel/dispatch.py	2007-02-20 14:55:10 UTC (rev 6670)
@@ -208,7 +208,7 @@
     
     def __init__(self, argv):
         self.argv = argv
-
+        
     def launch(self, wait=False):
         """Launch process on an available machine"""
         try:
@@ -310,9 +310,7 @@
         except StopIteration:
             cls._available_machines = None
             if new_loop:
-                time.sleep(3)                
-                #raise EmptyMachineListError 
-            #else:
+                time.sleep(SLEEP_TIME)
             return cls.nextAvailableMachine()
     nextAvailableMachine = classmethod(nextAvailableMachine)
 
@@ -324,9 +322,8 @@
         # Get the first available machine
         self.host = self.nextAvailableMachine()
         actual_command = ' '.join(['cd', os.getcwd(), ';', 'nice'] + self.argv)
-        actual_command = actual_command.replace("'", "\'")
-        actual_command = actual_command.replace('"', '\"')
-        return "ssh %s %s '%s'"%(self.host, self.Xopt, actual_command)
+        actual_command = actual_command.replace('"', r'\"')
+        return 'ssh %s %s "%s"'%(self.host, self.Xopt, actual_command)
 
     def getLogFileBaseName(self):
         return "ssh-%s-pid=%d"%(self.host, self.process.pid)
@@ -412,6 +409,7 @@
             self.expdir_root = os.getcwd()
         self.__check_constant_args()
 
+        # If oracles were already provided, the ctor can lauch the tasks...
         if oracles:                    
             self.start( *oracles )
 
@@ -516,13 +514,18 @@
             # # Module function defined above
             # launch_task( prepend+_quoted(arguments) )
             assert Task is not None
-            task = Task(prepend+_quoted(arguments)+[";", "echo", "'Task Done.'"])
+            ##TBM?: task = Task(prepend+_quoted(arguments)+[";", "echo", "'Task Done.'"])
             if self.delay:
-                logging.info('Delayed: %s'%task.getLaunchCommand())
+                ##TBM?: 
+                cmd = ' '.join(prepend+_quoted(arguments))
+                logging.info('Delayed: %s'%cmd)
+                ##TBM?: logging.info('Delayed: %s'%task.getLaunchCommand())
                 delayed += 1
-                task.free() # Since it won't be launch, free the resources...
-                continue            
-
+                ##TBM?: task.free() # Since it won't be launch, free the resources...
+                continue
+            ##TBM?: 
+            task = Task(prepend+_quoted(arguments)+[";", "echo", "'Task Done.'"])
+            
             task.launch()
             if Task.count( ) == self.max_nmachines:
                 logging.info( "+++ Using %d machines or more; waiting..."



From chapados at mail.berlios.de  Tue Feb 20 20:23:41 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Tue, 20 Feb 2007 20:23:41 +0100
Subject: [Plearn-commits] r6671 - trunk/plearn/io
Message-ID: <200702201923.l1KJNfq5026677@sheep.berlios.de>

Author: chapados
Date: 2007-02-20 20:23:40 +0100 (Tue, 20 Feb 2007)
New Revision: 6671

Modified:
   trunk/plearn/io/PStream.cc
   trunk/plearn/io/PStream.h
Log:
Added possibility to override formatting of floats and doubles in PStream

Modified: trunk/plearn/io/PStream.cc
===================================================================
--- trunk/plearn/io/PStream.cc	2007-02-20 14:55:10 UTC (rev 6670)
+++ trunk/plearn/io/PStream.cc	2007-02-20 19:23:40 UTC (rev 6671)
@@ -50,6 +50,10 @@
 namespace PLearn {
 using namespace std;
 
+// Default format string for floats and doubles
+const char* PStream::format_float_default  = "%.8g";
+const char* PStream::format_double_default = "%.18g";
+
 // Initialization for pin, pout, ...
 
 PStream& get_pnull()
@@ -170,14 +174,20 @@
     :inherited(0),
      inmode(plearn_ascii), 
      outmode(plearn_ascii), 
-     implicit_storage(true), compression_mode(compr_none)
+     implicit_storage(true),
+     compression_mode(compr_none),
+     format_float (format_float_default),
+     format_double(format_double_default)
 {}
 
 PStream::PStream(streambuftype* sb)
     :inherited(sb),
      inmode(plearn_ascii), 
      outmode(plearn_ascii), 
-     implicit_storage(true), compression_mode(compr_none)
+     implicit_storage(true),
+     compression_mode(compr_none),
+     format_float (format_float_default),
+     format_double(format_double_default)
 {}
 
 
@@ -186,7 +196,10 @@
     :inherited(new StdPStreamBuf(pin_,own_pin_)),
      inmode(plearn_ascii), 
      outmode(plearn_ascii),
-     implicit_storage(true), compression_mode(compr_none)
+     implicit_storage(true),
+     compression_mode(compr_none),
+     format_float (format_float_default),
+     format_double(format_double_default)
 {}
 //! ctor. from an ostream (O)
 
@@ -194,7 +207,10 @@
     :inherited(new StdPStreamBuf(pout_,own_pout_)),
      inmode(plearn_ascii), 
      outmode(plearn_ascii),
-     implicit_storage(true), compression_mode(compr_none)
+     implicit_storage(true),
+     compression_mode(compr_none),
+     format_float (format_float_default),
+     format_double(format_double_default)
 {}
 
 //! ctor. from an iostream (IO)
@@ -202,7 +218,10 @@
     :inherited(new StdPStreamBuf(pios_,own_pios_)),
      inmode(plearn_ascii), 
      outmode(plearn_ascii),
-     implicit_storage(true), compression_mode(compr_none)
+     implicit_storage(true),
+     compression_mode(compr_none),
+     format_float (format_float_default),
+     format_double(format_double_default)
 {}
 
 //! ctor. from an istream and an ostream (IO)
@@ -210,13 +229,15 @@
     :inherited(new StdPStreamBuf(pin_,pout_,own_pin_,own_pout_)),
      inmode(plearn_ascii), 
      outmode(plearn_ascii),
-     implicit_storage(true), compression_mode(compr_none)
+     implicit_storage(true),
+     compression_mode(compr_none),
+     format_float (format_float_default),
+     format_double(format_double_default)
 {}
 
 //! dtor.
 PStream::~PStream()
-{
-}
+{ }
 
 PStream::mode_t PStream::switchToPLearnOutMode() 
 { 
@@ -524,7 +545,7 @@
     }
     else
     {
-        snprintf(tmpbuf, sizeof(tmpbuf), "%.8g", x);
+        snprintf(tmpbuf, sizeof(tmpbuf), format_float, x);
         write(tmpbuf, streamsize(strlen(tmpbuf)));
     }
 }
@@ -541,7 +562,7 @@
     }
     else
     {
-        snprintf(tmpbuf, sizeof(tmpbuf), "%.18g", x);
+        snprintf(tmpbuf, sizeof(tmpbuf), format_double, x);
         write(tmpbuf, streamsize(strlen(tmpbuf)));
     }
 }

Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2007-02-20 14:55:10 UTC (rev 6670)
+++ trunk/plearn/io/PStream.h	2007-02-20 19:23:40 UTC (rev 6671)
@@ -71,7 +71,6 @@
  */
 
 class PStream : public PP<PStreamBuf>
-
 {
 public:
     //! typedef's for PStream manipulators
@@ -91,6 +90,11 @@
     typedef ios ios_base;
 #endif
 
+    /**
+     *  plearn_ascii and plearn_binary are used on output to determine in which
+     *  format to write stuff.  On input however, they are equivalent, as the
+     *  right format is automatically detected.
+     */
     enum mode_t 
     {
         plearn_ascii,    //!< PLearn ascii serialization format (can be mixed with plearn_binary)
@@ -100,8 +104,6 @@
         pretty_ascii     //!< Ascii pretty print (in particular for Vec and Mat, formatted output without size info)
     };
   
-    //! plearn_ascii and plearn_binary are used on output to determine in which format to write stuff.
-    //! On input however, they are equivalent, as the right format is automatically detected.
 
     //! Compression mode (mostly used by binary serialization of sequences of floats or doubles, such as TMat<real>)
     //! (Used on output only; autodetect on read).
@@ -121,8 +123,21 @@
     map<void *, unsigned int> copies_map_out; //!< copies map for output
 
 private:
+    //! Buffer for some formatting operations
     static char tmpbuf[100];
-  
+
+    //! Current format string for floats
+    const char* format_float;
+
+    //! Current format string for doubles
+    const char* format_double;
+
+    //! Default format string for floats
+    static const char* format_float_default;
+
+    //! Default format string for doubles
+    static const char* format_double_default;
+    
 public:
     //! If true, then Mat and Vec will be serialized with their elements in place,
     //! If false, they will have an explicit pointer to a storage
@@ -229,8 +244,20 @@
     // This operator is required for compilation under Visual C++.
     bool operator !() const
     { return !good(); }
-  
-    /******
+
+    /**
+     *  Getters/Setters for the printf format strings for writeAsciiNum for
+     *  float and double.  By default, this is "%.8g" for float, and "%.18g"
+     *  for double.  NOTE: these strings must remain accessible for the
+     *  lifetime of the PStream.  In particular, they (generally) should not
+     *  come from a string c_str().
+     */
+    const char* getFloatFormat()  const { return format_float;  }
+    const char* getDoubleFormat() const { return format_double; }
+    void setFloatFormat(const char* f)  { format_float = f;  }
+    void setDoubleFormat(const char* f) { format_double = f; }
+    
+    /**
      * The folowing methods are 'forwarded' from {i|o}stream.
      */
     inline int get() 



From chapados at mail.berlios.de  Wed Feb 21 00:18:39 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Wed, 21 Feb 2007 00:18:39 +0100
Subject: [Plearn-commits] r6672 - in trunk: plearn/var
	plearn_learners/regressors
Message-ID: <200702202318.l1KNIdQh009305@sheep.berlios.de>

Author: chapados
Date: 2007-02-21 00:18:38 +0100 (Wed, 21 Feb 2007)
New Revision: 6672

Modified:
   trunk/plearn/var/GaussianProcessNLLVariable.cc
   trunk/plearn/var/GaussianProcessNLLVariable.h
   trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
   trunk/plearn_learners/regressors/GaussianProcessRegressor.h
Log:
Option to save the gram matrix for debugging

Modified: trunk/plearn/var/GaussianProcessNLLVariable.cc
===================================================================
--- trunk/plearn/var/GaussianProcessNLLVariable.cc	2007-02-20 19:23:40 UTC (rev 6671)
+++ trunk/plearn/var/GaussianProcessNLLVariable.cc	2007-02-20 23:18:38 UTC (rev 6672)
@@ -37,6 +37,7 @@
 /*! \file GaussianProcessNLLVariable.cc */
 
 #include <plearn/math/TMat_maths.h>
+#include <plearn/io/MatIO.h>
 #include "GaussianProcessNLLVariable.h"
 
 #ifdef USE_BLAS_SPECIALISATIONS
@@ -67,7 +68,8 @@
     );
 
 GaussianProcessNLLVariable::GaussianProcessNLLVariable()
-    : m_kernel(0),
+    : m_save_gram_matrix(0),
+      m_kernel(0),
       m_noise(0),
       m_allow_bprop(true)
 { }
@@ -76,8 +78,10 @@
 GaussianProcessNLLVariable::GaussianProcessNLLVariable(
     Kernel* kernel, real noise, Mat inputs, Mat targets,
     const TVec<string>& hyperparam_names, const VarArray& hyperparam_vars,
-    bool allow_bprop)
+    bool allow_bprop, bool save_gram_matrix, PPath expdir)
     : inherited(hyperparam_vars, 1, 1),
+      m_save_gram_matrix(save_gram_matrix),
+      m_expdir(expdir),
       m_kernel(kernel),
       m_noise(noise),
       m_inputs(inputs),
@@ -125,7 +129,11 @@
 
 void GaussianProcessNLLVariable::declareOptions(OptionList& ol)
 {
-    // (no additional options)
+    declareOption(
+        ol, "save_gram_matrix", &GaussianProcessNLLVariable::m_save_gram_matrix,
+        OptionBase::buildoption,
+        "If true, the Gram matrix is saved before undergoing Cholesky\n"
+        "decomposition; useful for debugging if the matrix is quasi-singular.");
     
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
@@ -153,6 +161,7 @@
 void GaussianProcessNLLVariable::fprop()
 {
     fbpropFragments(m_kernel, m_noise, m_inputs, m_targets, m_allow_bprop,
+                    m_save_gram_matrix, m_expdir,
                     m_gram, m_cholesky_gram, m_alpha_t, m_inverse_gram,
                     m_cholesky_tmp, m_rhs_tmp);
 
@@ -249,7 +258,8 @@
 #ifndef USE_BLAS_SPECIALISATIONS
 void GaussianProcessNLLVariable::fbpropFragments(
     Kernel* kernel, real noise, const Mat& inputs, const Mat& targets,
-    bool compute_inverse, Mat& gram, Mat& L, Mat& alpha_t, Mat& inv,
+    bool compute_inverse, bool save_gram_matrix, const PPath& expdir,
+    Mat& gram, Mat& L, Mat& alpha_t, Mat& inv,
     Vec& tmp_chol, Mat& tmp_rhs)
 {
     PLASSERT( kernel );
@@ -275,6 +285,14 @@
     kernel->computeGramMatrix(gram);
     addToDiagonal(gram, noise);
 
+    // Save the Gram matrix if requested
+    if (save_gram_matrix) {
+        static int counter = 1;
+        string filename = expdir / ("gram_matrix_" +
+                                    tostring(counter++) + ".pmat");
+        savePMat(filename, gram);
+    }
+
     // Compute Cholesky decomposition and solve the linear system
     alpha_t.resize(trainlength, rhs_width);
     L.resize(trainlength, trainlength);
@@ -298,7 +316,8 @@
 #ifdef USE_BLAS_SPECIALISATIONS
 void GaussianProcessNLLVariable::fbpropFragments(
     Kernel* kernel, real noise, const Mat& inputs, const Mat& targets,
-    bool compute_inverse, Mat& gram, Mat& L, Mat& alpha_t, Mat& inv,
+    bool compute_inverse, bool save_gram_matrix, const PPath& expdir,
+    Mat& gram, Mat& L, Mat& alpha_t, Mat& inv,
     Vec& tmp_chol, Mat& tmp_rhs)
 {
     PLASSERT( kernel );
@@ -326,6 +345,14 @@
     kernel->computeGramMatrix(gram);
     addToDiagonal(gram, noise);
 
+    // Save the Gram matrix if requested
+    if (save_gram_matrix) {
+        static int counter = 1;
+        string filename = expdir / ("gram_matrix_" +
+                                    tostring(counter++) + ".pmat");
+        savePMat(filename, gram);
+    }
+
     // Compute Cholesky decomposition and solve the linear system.  LAPACK
     // solves in-place, but luckily we don't need either the Gram and RHS
     // matrices after solving.  Note that for now we don't bother to create an

Modified: trunk/plearn/var/GaussianProcessNLLVariable.h
===================================================================
--- trunk/plearn/var/GaussianProcessNLLVariable.h	2007-02-20 19:23:40 UTC (rev 6671)
+++ trunk/plearn/var/GaussianProcessNLLVariable.h	2007-02-20 23:18:38 UTC (rev 6672)
@@ -72,7 +72,12 @@
 public:
     //#####  Public Build Options  ############################################
 
-    // (no options)
+    //! If true, the Gram matrix is saved before undergoing Cholesky
+    //! decomposition; useful for debugging if the matrix is quasi-singular.
+    bool m_save_gram_matrix;
+
+    //! Expdir where to save the Gram Matrix, if 'save_gram_matrix' requested.
+    PPath m_expdir;
     
 public:
     //#####  Public Member Functions  #########################################
@@ -98,7 +103,9 @@
                                Mat inputs, Mat targets,
                                const TVec<string>& hyperparam_names,
                                const VarArray& hyperparam_vars,
-                               bool allow_bprop = true);
+                               bool allow_bprop = true,
+                               bool save_gram_matrix = false,
+                               PPath expdir = "");
 
     
     //#####  PLearn::Variable methods #########################################
@@ -115,7 +122,9 @@
      *  @param[in]  noise:    observation noise to add to the diagonal Gram matrix
      *  @param[in]  inputs:   matrix of training inputs
      *  @param[in]  targets:  matrix of training targets (may be multivariate)
-     *  @param[in]  compute_inverse: whether to compute inverse of Gram matrix
+     *  @param[in]  compute_inverse:  whether to compute inverse of Gram matrix
+     *  @param[in]  save_gram_matrix: whether to save the computed Gram matrix
+     *  @param[in]  expdir:           if saving Gram matrix, where to save it
      *  @param[out] gram:     The kernel (Gram) matrix
      *  @param[out] L:        Cholesky decomposition of the Gram matrix
      *  @param[out] alpha:    Solution to the linear system gram*alpha = targets
@@ -125,6 +134,7 @@
      */
     static void fbpropFragments(Kernel* kernel, real noise, const Mat& inputs,
                                 const Mat& targets, bool compute_inverse,
+                                bool save_gram_matrix, const PPath& expdir,
                                 Mat& gram, Mat& L, Mat& alpha, Mat& inv,
                                 Vec& tmpch, Mat& tmprhs);
 

Modified: trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2007-02-20 19:23:40 UTC (rev 6671)
+++ trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2007-02-20 23:18:38 UTC (rev 6672)
@@ -107,7 +107,8 @@
     : m_weight_decay(0.0),
       m_include_bias(true),
       m_compute_confidence(false),
-      m_confidence_epsilon(1e-8)
+      m_confidence_epsilon(1e-8),
+      m_save_gram_matrix(false)
 { }
 
 
@@ -181,7 +182,15 @@
         "Specification of the optimizer to use for train-time hyperparameter\n"
         "optimization.  A ConjGradientOptimizer should be an adequate choice.\n");
 
+    declareOption(
+        ol, "save_gram_matrix", &GaussianProcessRegressor::m_save_gram_matrix,
+        OptionBase::buildoption,
+        "If true, the Gram matrix is saved before undergoing Cholesky each\n"
+        "decomposition; useful for debugging if the matrix is quasi-singular.\n"
+        "It is saved in the current expdir under the names 'gram_matrix_N.pmat'\n"
+        "where N is an increasing counter.\n");
 
+
     //#####  Learnt Options  ##################################################
 
     declareOption(
@@ -564,7 +573,8 @@
     {
         return new GaussianProcessNLLVariable(
             m_kernel, m_weight_decay, inputs, targets,
-            TVec<string>(), VarArray(), m_compute_confidence);
+            TVec<string>(), VarArray(), m_compute_confidence,
+            m_save_gram_matrix, getExperimentDirectory());
     }
 
     // Otherwise create Vars that wrap each hyperparameter
@@ -600,7 +610,7 @@
     // Create the cost-function variable
     PP<GaussianProcessNLLVariable> nll = new GaussianProcessNLLVariable(
         m_kernel, m_weight_decay, inputs, targets, hyperparam_names,
-        hyperparam_vars, true);
+        hyperparam_vars, true, m_save_gram_matrix, getExperimentDirectory());
     nll->setName("GaussianProcessNLLVariable");
 
     // Some logging about the initial values

Modified: trunk/plearn_learners/regressors/GaussianProcessRegressor.h
===================================================================
--- trunk/plearn_learners/regressors/GaussianProcessRegressor.h	2007-02-20 19:23:40 UTC (rev 6671)
+++ trunk/plearn_learners/regressors/GaussianProcessRegressor.h	2007-02-20 23:18:38 UTC (rev 6672)
@@ -171,7 +171,15 @@
      */
     PP<Optimizer> m_optimizer;
 
+    /**
+     *  If true, the Gram matrix is saved before undergoing Cholesky each
+     *  decomposition; useful for debugging if the matrix is quasi-singular.
+     *  It is saved in the current expdir under the names 'gram_matrix_N.pmat'
+     *  where N is an increasing counter.
+     */
+    bool m_save_gram_matrix;
 
+
 public:
     //#####  Public Member Functions  #########################################
 



From chapados at mail.berlios.de  Wed Feb 21 00:19:23 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Wed, 21 Feb 2007 00:19:23 +0100
Subject: [Plearn-commits] r6673 - trunk/plearn/math
Message-ID: <200702202319.l1KNJNLi009361@sheep.berlios.de>

Author: chapados
Date: 2007-02-21 00:19:23 +0100 (Wed, 21 Feb 2007)
New Revision: 6673

Modified:
   trunk/plearn/math/lapack_proto.h
   trunk/plearn/math/plapack.h
Log:
Some more lapack prototypes

Modified: trunk/plearn/math/lapack_proto.h
===================================================================
--- trunk/plearn/math/lapack_proto.h	2007-02-20 23:18:38 UTC (rev 6672)
+++ trunk/plearn/math/lapack_proto.h	2007-02-20 23:19:23 UTC (rev 6673)
@@ -84,6 +84,17 @@
     // Solve linear system given Cholesky Decomposition
     void spotrs_(char* UPLO, int* N, int* NRHS, float*  A, int* LDA, float*  B, int* LDB, int* INFO);
     void dpotrs_(char* UPLO, int* N, int* NRHS, double* A, int* LDA, double* B, int* LDB, int* INFO);
+
+    // Expert driver for factorising and solving through Cholesky (and estimate
+    // condition number, equilibrate, etc.)
+    void sposvx_(char*   FACT, char* UPLO,  int*    N,     int*    NRHS, float* A,  int* LDA,
+                 float*  AF,   int*  LDAF,  char*   EQUED, float*  S,    float* B,  int* LDB,
+                 float*  X,    int*  LDX,   float*  RCOND, float*  FERR, float* BERR,
+                 float*  WORK, int*  IWORK, int*    INFO);
+    void dposvx_(char*   FACT, char* UPLO,  int*    N,     int*    NRHS, double* A, int* LDA,
+                 double* AF,   int*  LDAF,  char*   EQUED, double* S,    double* B, int* LDB,
+                 double* X,    int*  LDX,   double* RCOND, double* FERR, double* BERR,
+                 double* WORK, int*  IWORK, int*    INFO);
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn/math/plapack.h
===================================================================
--- trunk/plearn/math/plapack.h	2007-02-20 23:18:38 UTC (rev 6672)
+++ trunk/plearn/math/plapack.h	2007-02-20 23:19:23 UTC (rev 6673)
@@ -76,18 +76,40 @@
 inline void lapack_Xsygvx_(int* ITYPE, char* JOBZ, char* RANGE, char* UPLO, int* N, float* A, int* LDA, float* B, int* LDB, float* VL, float* VU, int* IL, int* IU, float* ABSTOL, int* M, float* W, float* Z, int* LDZ, float* WORK, int* LWORK, int* IWORK, int* IFAIL, int* INFO)
 { ssygvx_(ITYPE, JOBZ, RANGE, UPLO, N, A, LDA, B, LDB, VL, VU, IL, IU, ABSTOL, M, W, Z, LDZ, WORK, LWORK, IWORK, IFAIL, INFO); }
 
+// Cholesky decomposition
 inline void lapack_Xpotrf_(char* UPLO, int* N, float* A, int* LDA, int* INFO)
 { spotrf_(UPLO, N, A, LDA, INFO); }
 
 inline void lapack_Xpotrf_(char* UPLO, int* N, double* A, int* LDA, int* INFO)
 { dpotrf_(UPLO, N, A, LDA, INFO); }
 
+// Solve linear system from Cholesky
 inline void lapack_Xpotrs_(char* UPLO, int* N, int* NRHS, float*  A, int* LDA, float*  B, int* LDB, int* INFO)
 { spotrs_(UPLO, N, NRHS, A, LDA, B, LDB, INFO); }
 
 inline void lapack_Xpotrs_(char* UPLO, int* N, int* NRHS, double* A, int* LDA, double* B, int* LDB, int* INFO)
 { dpotrs_(UPLO, N, NRHS, A, LDA, B, LDB, INFO); }
 
+// Expert driver for factorising and solving through Cholesky (and estimate
+// condition number, equilibrate, etc.)
+inline void lapack_Xposvx_(
+    char*   FACT, char* UPLO,  int*    N,     int*    NRHS, float* A,  int* LDA,
+    float*  AF,   int*  LDAF,  char*   EQUED, float*  S,    float* B,  int* LDB,
+    float*  X,    int*  LDX,   float*  RCOND, float*  FERR, float* BERR,
+    float*  WORK, int*  IWORK, int*    INFO)
+{
+    sposvx_(FACT, UPLO, N, NRHS, A,     LDA,  AF,   LDAF, EQUED, S,
+            B,    LDB,  X, LDX,  RCOND, FERR, BERR, WORK, IWORK, INFO);
+}
+inline void lapack_Xposvx_(
+    char*   FACT, char* UPLO,  int*    N,     int*    NRHS, double* A, int* LDA,
+    double* AF,   int*  LDAF,  char*   EQUED, double* S,    double* B, int* LDB,
+    double* X,    int*  LDX,   double* RCOND, double* FERR, double* BERR,
+    double* WORK, int*  IWORK, int*    INFO)
+{
+    dposvx_(FACT, UPLO, N, NRHS, A,     LDA,  AF,   LDAF, EQUED, S,
+            B,    LDB,  X, LDX,  RCOND, FERR, BERR, WORK, IWORK, INFO);
+}
 
 //!  Computes the eigenvalues and eigenvectors of a symmetric (NxN) matrix A.
 //!  BEWARE: The content of A is destroyed by the call.



From dorionc at mail.berlios.de  Wed Feb 21 06:43:25 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Wed, 21 Feb 2007 06:43:25 +0100
Subject: [Plearn-commits] r6674 - trunk/python_modules/plearn/pyplearn
Message-ID: <200702210543.l1L5hPZV005873@sheep.berlios.de>

Author: dorionc
Date: 2007-02-21 06:43:23 +0100 (Wed, 21 Feb 2007)
New Revision: 6674

Modified:
   trunk/python_modules/plearn/pyplearn/plargs.py
Log:
Fixing a minor bug in plnamespace.inherit()


Modified: trunk/python_modules/plearn/pyplearn/plargs.py
===================================================================
--- trunk/python_modules/plearn/pyplearn/plargs.py	2007-02-20 23:19:23 UTC (rev 6673)
+++ trunk/python_modules/plearn/pyplearn/plargs.py	2007-02-21 05:43:23 UTC (rev 6674)
@@ -896,7 +896,7 @@
         """A deep-copy driven inheritance-like mechanism.
 
         In the context of plnamespace, usual (Python) inheritance is not
-        satisfatory. Indeed, the options defined in some base class will be
+        satisfactory. Indeed, the options defined in some base class will be
         shared among subclasses. However, when one would want to subclass a
         plnamespace, it is more likely the he want the 'subclass' to have
         options 'of the same name' than the ones in the base-class but
@@ -919,9 +919,14 @@
             def __new__(metacls, clsname, bases, dic):
                 # Do not use plopt.optdict: the documentation, choices and other
                 # property would be lost...
-                optdict = dict([ (
-                    opt.getName(), opt) for opt in plopt.iterator(namespace) ])
-                dic.update( copy.deepcopy(optdict) )
+                for opt in plopt.iterator(namespace):
+                    inh_opt = copy.deepcopy(opt)
+                    inh_opt.set( opt.get() )
+                    dic[inh_opt.getName()] = inh_opt
+                    
+                #OLD: optdict = dict([ (
+                #OLD:     opt.getName(), opt) for opt in plopt.iterator(namespace) ])
+                #OLD: dic.update( copy.deepcopy(optdict) )
                 cls = META.__new__(metacls, clsname, bases, dic)
                 return cls        
         return __metaclass__



From chapados at mail.berlios.de  Wed Feb 21 16:49:27 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Wed, 21 Feb 2007 16:49:27 +0100
Subject: [Plearn-commits] r6675 - in trunk: plearn/var
	plearn_learners/regressors
Message-ID: <200702211549.l1LFnR7c005068@sheep.berlios.de>

Author: chapados
Date: 2007-02-21 16:49:27 +0100 (Wed, 21 Feb 2007)
New Revision: 6675

Modified:
   trunk/plearn/var/GaussianProcessNLLVariable.cc
   trunk/plearn/var/GaussianProcessNLLVariable.h
   trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
   trunk/plearn_learners/regressors/GaussianProcessRegressor.h
Log:
Bug fixes in gaussian processes: kernel hyperparameters were not always updated following changes to the bound ObjectOptionVariable

Modified: trunk/plearn/var/GaussianProcessNLLVariable.cc
===================================================================
--- trunk/plearn/var/GaussianProcessNLLVariable.cc	2007-02-21 05:43:23 UTC (rev 6674)
+++ trunk/plearn/var/GaussianProcessNLLVariable.cc	2007-02-21 15:49:27 UTC (rev 6675)
@@ -36,6 +36,10 @@
 
 /*! \file GaussianProcessNLLVariable.cc */
 
+#define PL_LOG_MODULE_NAME "GaussianProcessNLLVariable"
+
+// From PLearn
+#include <plearn/io/pl_log.h>
 #include <plearn/math/TMat_maths.h>
 #include <plearn/io/MatIO.h>
 #include "GaussianProcessNLLVariable.h"
@@ -160,6 +164,12 @@
 // ### computes value from varray values
 void GaussianProcessNLLVariable::fprop()
 {
+    logVarray(m_hyperparam_vars, "FProp current hyperparameters:", true);
+
+    // Ensure that the current hyperparameter variable values are propagated
+    // into kernel options
+    m_hyperparam_vars.fprop();
+    
     fbpropFragments(m_kernel, m_noise, m_inputs, m_targets, m_allow_bprop,
                     m_save_gram_matrix, m_expdir,
                     m_gram, m_cholesky_gram, m_alpha_t, m_inverse_gram,
@@ -293,6 +303,12 @@
         savePMat(filename, gram);
     }
 
+    // Dump a fragment of the Gram Matrix to the debug log
+    DBG_MODULE_LOG << "Gram fragment: "
+                   << gram(0,0) << ' '
+                   << gram(1,0) << ' '
+                   << gram(1,1) << endl;
+
     // Compute Cholesky decomposition and solve the linear system
     alpha_t.resize(trainlength, rhs_width);
     L.resize(trainlength, trainlength);
@@ -353,6 +369,12 @@
         savePMat(filename, gram);
     }
 
+    // Dump a fragment of the Gram Matrix to the debug log
+    DBG_MODULE_LOG << "Gram fragment: "
+                   << gram(0,0) << ' '
+                   << gram(1,0) << ' '
+                   << gram(1,1) << endl;
+
     // Compute Cholesky decomposition and solve the linear system.  LAPACK
     // solves in-place, but luckily we don't need either the Gram and RHS
     // matrices after solving.  Note that for now we don't bother to create an
@@ -369,6 +391,26 @@
 }
 #endif
 
+
+//#####  logVarray  ###########################################################
+
+void GaussianProcessNLLVariable::logVarray(const VarArray& varr,
+                                           const string& title, bool debug)
+{
+    string entry = title + '\n';
+    for (int i=0, n=varr.size() ; i<n ; ++i) {
+        entry += right(varr[i]->getName(), 35) + ": " + tostring(varr[i]->value[0]);
+        if (i < n-1)
+            entry += '\n';
+    }
+    if (debug) {
+        DBG_MODULE_LOG << entry << endl;
+    }
+    else {
+        MODULE_LOG << entry << endl;
+    }
+}
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/var/GaussianProcessNLLVariable.h
===================================================================
--- trunk/plearn/var/GaussianProcessNLLVariable.h	2007-02-21 05:43:23 UTC (rev 6674)
+++ trunk/plearn/var/GaussianProcessNLLVariable.h	2007-02-21 15:49:27 UTC (rev 6675)
@@ -147,6 +147,10 @@
     //! Accessor to the last computed gram matrix inverse in an fprop
     const Mat& gramInverse() const { return m_inverse_gram; }
 
+    /// Minor utility function to dump the contents of a varray to a log
+    static void logVarray(const VarArray& varr, const string& title="",
+                          bool debug=false);
+    
 
     //#####  PLearn::Object Protocol  #########################################
 

Modified: trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2007-02-21 05:43:23 UTC (rev 6674)
+++ trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2007-02-21 15:49:27 UTC (rev 6675)
@@ -346,15 +346,22 @@
     }
 
     // Optimize hyperparameters
-    PP<GaussianProcessNLLVariable> nll = hyperOptimize(m_training_inputs, targets);
+    VarArray hyperparam_vars;
+    PP<GaussianProcessNLLVariable> nll = hyperOptimize(m_training_inputs, targets,
+                                                       hyperparam_vars);
     PLASSERT( nll );
     
-    // Compute parameters
+    // Compute parameters.  Be careful to also propagate through the
+    // hyperparameter variables to ensure the latest values are correctly set
+    // into their respective kernels.
+    hyperparam_vars.fprop();
     nll->fprop();
     m_alpha = nll->alpha();
     m_gram_inverse = nll->gramInverse();
 
     // Compute train MSE, as 1/(2N) * dot(z,z), with z=K*alpha - y
+    // *** FIXME: MSE IS NOT COMPUTED CORRECTLY SINCE GRAM MATRIX IS
+    // OVERWRITTEN BY CHOLESKY DECOMPOSITION ***
     Mat residuals(m_alpha.length(), m_alpha.width());
     product(residuals, nll->gram(), m_alpha);
     residuals -= targets;
@@ -564,7 +571,8 @@
 //#####  hyperOptimize  #######################################################
 
 PP<GaussianProcessNLLVariable>
-GaussianProcessRegressor::hyperOptimize(const Mat& inputs, const Mat& targets)
+GaussianProcessRegressor::hyperOptimize(const Mat& inputs, const Mat& targets,
+                                        VarArray& hyperparam_vars)
 {
     // If there are no hyperparameters or optimizer, just create a simple
     // variable and return it right away.
@@ -581,7 +589,7 @@
     const int numhyper  = m_hyperparameters.size();
     const int numinputs = ( ! m_ARD_hyperprefix_initval.first.empty() ?
                             inputsize() : 0 );
-    VarArray     hyperparam_vars (numhyper + numinputs);
+    hyperparam_vars = VarArray(numhyper + numinputs);
     TVec<string> hyperparam_names(numhyper + numinputs);
     int i;
     for (i=0 ; i<numhyper ; ++i) {
@@ -614,7 +622,8 @@
     nll->setName("GaussianProcessNLLVariable");
 
     // Some logging about the initial values
-    logVarray(hyperparam_vars, "Hyperparameter initial values:");
+    GaussianProcessNLLVariable::logVarray(hyperparam_vars,
+                                          "Hyperparameter initial values:");
     
     // And optimize for nstages
     m_optimizer->setToOptimize(hyperparam_vars, (Variable*)nll);
@@ -639,25 +648,11 @@
     pb = 0;                                  // Finish progress bar right now
 
     // Some logging about the final values
-    logVarray(hyperparam_vars, "Hyperparameter final values:");
+    GaussianProcessNLLVariable::logVarray(hyperparam_vars,
+                                          "Hyperparameter final values:");
     return nll;
 }
 
-
-//#####  logVarray  ###########################################################
-
-void GaussianProcessRegressor::logVarray(const VarArray& varr,
-                                         const string& title)
-{
-    string entry = title + '\n';
-    for (int i=0, n=varr.size() ; i<n ; ++i) {
-        entry += right(varr[i]->getName(), 35) + ": " + tostring(varr[i]->value[0]);
-        if (i < n-1)
-            entry += '\n';
-    }
-    MODULE_LOG << entry << endl; 
-}
-
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/regressors/GaussianProcessRegressor.h
===================================================================
--- trunk/plearn_learners/regressors/GaussianProcessRegressor.h	2007-02-21 05:43:23 UTC (rev 6674)
+++ trunk/plearn_learners/regressors/GaussianProcessRegressor.h	2007-02-21 15:49:27 UTC (rev 6675)
@@ -257,12 +257,9 @@
     /// Optimize the hyperparameters if any.  Return a Variable on which
     /// train() carries out a final fprop for obtaining the final trained
     /// learner parameters.
-    PP<GaussianProcessNLLVariable> hyperOptimize(const Mat& inputs,
-                                                 const Mat& targets);
+    PP<GaussianProcessNLLVariable> hyperOptimize(
+        const Mat& inputs, const Mat& targets, VarArray& hyperparam_vars);
 
-    /// Minor utility function to dump the contents of a varray to a log
-    static void logVarray(const VarArray& varr, const string& title="");
-    
 protected:
     //#####  Protected Options  ###############################################
 



From saintmlx at mail.berlios.de  Thu Feb 22 22:05:56 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 22 Feb 2007 22:05:56 +0100
Subject: [Plearn-commits] r6676 - in trunk: commands/PLearnCommands
	examples/Demo/Tasks/2d_classif/Datasets/moons.amat.metadata
	plearn/base plearn/io plearn/math plearn/misc plearn/sys
	plearn/vmat plearn_learners/generic plearn_learners/testers
	python_modules/plearn/io python_modules/plearn/parallel
	python_modules/plearn/utilities scripts
Message-ID: <200702222105.l1ML5uGJ020153@sheep.berlios.de>

Author: saintmlx
Date: 2007-02-22 22:05:50 +0100 (Thu, 22 Feb 2007)
New Revision: 6676

Added:
   trunk/commands/PLearnCommands/TestClientCommand.cc
   trunk/commands/PLearnCommands/TestClientCommand.h
   trunk/plearn/io/ServerLogStreamBuf.cc
   trunk/plearn/io/ServerLogStreamBuf.h
   trunk/scripts/xdispatch
Modified:
   trunk/commands/PLearnCommands/ServerCommand.cc
   trunk/commands/PLearnCommands/plearn_main.cc
   trunk/examples/Demo/Tasks/2d_classif/Datasets/moons.amat.metadata/stats.psave
   trunk/plearn/base/ProgressBar.cc
   trunk/plearn/base/ProgressBar.h
   trunk/plearn/io/PStream.cc
   trunk/plearn/io/PStreamBuf.cc
   trunk/plearn/io/PStreamBuf.h
   trunk/plearn/io/Poll.cc
   trunk/plearn/io/Poll.h
   trunk/plearn/io/PrPStreamBuf.cc
   trunk/plearn/io/openSocket.cc
   trunk/plearn/io/pl_log.cc
   trunk/plearn/io/pl_log.h
   trunk/plearn/math/TMat_maths_impl.h
   trunk/plearn/misc/PLearnServer.cc
   trunk/plearn/misc/PLearnServer.h
   trunk/plearn/misc/PLearnService.cc
   trunk/plearn/misc/PLearnService.h
   trunk/plearn/misc/RemotePLearnServer.cc
   trunk/plearn/misc/RemotePLearnServer.h
   trunk/plearn/sys/Popen.cc
   trunk/plearn/sys/Popen.h
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn_learners/generic/NNet.cc
   trunk/plearn_learners/testers/PTester.cc
   trunk/plearn_learners/testers/PTester.h
   trunk/python_modules/plearn/io/server.py
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/python_modules/plearn/parallel/dispatch.py
   trunk/python_modules/plearn/parallel/utils.py
   trunk/python_modules/plearn/utilities/progress.py
Log:
- added progress and log messages for remote servers
- PTester can perform splits in parallel on several servers



Modified: trunk/commands/PLearnCommands/ServerCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/ServerCommand.cc	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/commands/PLearnCommands/ServerCommand.cc	2007-02-22 21:05:50 UTC (rev 6676)
@@ -113,11 +113,22 @@
             myhostname = buf;
 #endif
 
+        PRNetAddr assigned_addr;
+        PR_InitializeNetAddr(PR_IpAddrAny, PR_htons(0), &assigned_addr);
+        st= PR_GetSockName(sock, &assigned_addr);
+        if(port==0 && st==PR_SUCCESS)
+            port= PR_ntohs(assigned_addr.inet.port);
+
         pout << "PLEARN_SERVER_TCP " << myhostname << " " << port << " " << mypid << endl;
         NORMAL_LOG << "PLEARN_SERVER STARTING IN TCP MODE ON "  << myhostname << ", PORT " << port << ", PID " << mypid << endl;
-      
+
         for (bool running = true; running; ) {
             NORMAL_LOG << "\nPLEARN_SERVER WAITING FOR CONNECTION"  << endl;
+            
+            ///***///***
+            pout << " JBL " << endl;
+            ///***///***
+
             st = PR_Listen(sock,0);
             if(st!=PR_SUCCESS)
                 PLERROR("serverCommand: listen on socket failed");
@@ -131,6 +142,10 @@
 
             PLearnServer server(io);
             running = server.run();
+            io.flush();
+            if (PR_Close(fd) != PR_SUCCESS)
+                PLERROR("ServerCommand: couldn't close client socket from %s!", buf);
+
         }
         NORMAL_LOG << "PLEARN_SERVER CLOSING SOCKET" << endl;
         if (PR_Shutdown(sock, PR_SHUTDOWN_BOTH) != PR_SUCCESS)

Added: trunk/commands/PLearnCommands/TestClientCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/TestClientCommand.cc	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/commands/PLearnCommands/TestClientCommand.cc	2007-02-22 21:05:50 UTC (rev 6676)
@@ -0,0 +1,193 @@
+// -*- C++ -*-
+
+// TestClientCommand.cc
+//
+// Copyright (C) 2007 Xavier Saint-Mleux, Apstat Technologies, inc.
+// 
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Xavier Saint-Mleux
+
+/*! \file TestClientCommand.cc */
+
+
+#include "TestClientCommand.h"
+#include <plearn/misc/PLearnService.h>
+#include <plearn/io/pl_log.h>
+#include <plearn/vmat/MemoryVMatrix.h>
+#include <plearn/base/PDateTime.h>
+#include <plearn/base/ProgressBar.h>
+
+namespace PLearn {
+using namespace std;
+
+//! This allows to register the 'TestClientCommand' command in the command registry
+PLearnCommandRegistry TestClientCommand::reg_(new TestClientCommand);
+
+TestClientCommand::TestClientCommand():
+    PLearnCommand("testclient",
+
+                  "Launches plearn in test client mode",
+
+                  "testclient\n"
+                  "  Launches plearn in test client mode \n"
+                  " \n"
+        )
+{}
+
+
+//! The actual implementation of the 'TestClientCommand' command 
+void TestClientCommand::run(const vector<string>& args)
+{
+
+    cout << "test client run" << endl;
+    
+
+    PLearnService& ps(PLearnService::instance()); 
+  
+    int ns= ps.availableServers();
+    TVec<PP<RemotePLearnServer> > ss= ps.reserveServers(ns);
+
+
+    cout << "test client has servers" << endl;
+
+    TVec<int> objids(ns);
+
+    for(int i= 0; i < ns; ++i)
+    {
+        //ss[i]->newObjectAsync(1, string("MemoryVMatrix(width= 5000, length= 7000)"));
+        //ss[i]->newObjectAsync(string("MemoryVMatrix(width= 5000, length= 7000)"));
+        objids[i]= ss[i]->newObject(string("MemoryVMatrix(width= 5000, length= 7000)"));
+        cerr << "NEW OBJ " << i << '-' << objids[i] << endl;
+    }
+/*
+    for(int i= 0; i < ns; ++i)
+    {
+        int j= ps.watchServers(ss, PLearnService::log_callback, PLearnService::progress_callback);
+        cout << "result 1 from " << j << " at " << PDateTime::currentLocalTime() << endl;
+        ss[j]->getResults(objids[j]);
+    }
+*/
+    for(int i= 0; i < ns; ++i)
+        ss[i]->callMethod(objids[i], "fill", 3.21*static_cast<real>(i+1));
+
+    for(int i= 0; i < ns; ++i)
+    {
+/*
+        int j= ps.watchServers(ss, PLearnService::log_callback, PLearnService::progress_callback);
+        cout << "result 2 from " << j << " at " << PDateTime::currentLocalTime() << endl;
+        ss[j]->getResults();
+*/
+        ps.waitForResult()->getResults();
+    }
+
+    cout << "before getRow" << endl;
+
+    for(int i= 0; i < ns; ++i)
+        ss[i]->callMethod(objids[i], "getRow", 3);
+
+    cout << "after getRow" << endl;
+
+    for(int i= 0; i < ns; ++i)
+    {
+        cout << "bws " << i << endl;
+
+        int j= ps.watchServers(ss, PLearnService::log_callback, PLearnService::progress_callback);
+        cout << "result 3 from " << j << " at " << PDateTime::currentLocalTime() << endl;
+        Vec vv;
+
+        cout << "getting it" << endl;
+
+        ss[j]->getResults(vv);
+
+        cout << "got it" << endl;
+
+        pout << '\t' << vv[0] << endl;
+    }
+
+    cout << "before call dot" << endl;
+
+    for(int i= 0; i < ns; ++i)
+        ss[i]->callMethod(objids[i], "dot", 1, 3, 4500);
+
+    cout << "after call dot" << endl;
+
+    for(int i= 0; i < ns; ++i)
+    {
+        int j= ps.watchServers(ss, PLearnService::log_callback, PLearnService::progress_callback);
+        cout << "result 4 from " << j << " at " << PDateTime::currentLocalTime() << endl;
+        real x;
+        ss[j]->getResults(x);
+        cout << endl << '=' << x << endl;
+    }
+
+    cout << "before call save" << endl;
+
+    for(int i= 0; i < ns; ++i)
+        ss[i]->callMethod(objids[i], "savePMAT", string("/home/saintmlx/testclient/mat")+tostring(i)+string(".pmat"));
+
+    cout << "after call save" << endl;
+
+    for(int i= 0; i < ns; ++i)
+    {
+        int j= ps.watchServers(ss, PLearnService::log_callback, PLearnService::progress_callback);
+        cout << "result 5 from " << j << " at " << PDateTime::currentLocalTime() << endl;
+        ss[j]->getResults();
+    }
+
+    cout << "before delete" << endl;
+
+    for(int i= 0; i < ns; ++i)
+        ss[i]->deleteObjectAsync(objids[i]);
+
+    cout << "after delete" << endl;
+
+    for(int i= 0; i < ns; ++i)
+    {
+        int j= ps.watchServers(ss, PLearnService::log_callback, PLearnService::progress_callback);
+        cout << "result 6 from " << j << " at " << PDateTime::currentLocalTime() << endl;
+        ss[j]->getResults();
+    }
+
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/commands/PLearnCommands/TestClientCommand.h
===================================================================
--- trunk/commands/PLearnCommands/TestClientCommand.h	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/commands/PLearnCommands/TestClientCommand.h	2007-02-22 21:05:50 UTC (rev 6676)
@@ -0,0 +1,74 @@
+// -*- C++ -*-
+
+// TestClientCommand.h
+//
+// Copyright (C) 2007 Xavier Saint-Mleux, Apstat Technologies, inc.
+// 
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Xavier Saint-Mleux
+
+/*! \file TestClientCommand.h */
+
+
+#ifndef TestClientCommand_INC
+#define TestClientCommand_INC
+
+#include <commands/PLearnCommands/PLearnCommand.h>
+#include <commands/PLearnCommands/PLearnCommandRegistry.h>
+
+namespace PLearn {
+
+class TestClientCommand: public PLearnCommand
+{
+public:
+    TestClientCommand();                    
+    virtual void run(const std::vector<std::string>& args);
+
+protected:
+    static PLearnCommandRegistry reg_;
+};
+
+  
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Modified: trunk/commands/PLearnCommands/plearn_main.cc
===================================================================
--- trunk/commands/PLearnCommands/plearn_main.cc	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/commands/PLearnCommands/plearn_main.cc	2007-02-22 21:05:50 UTC (rev 6676)
@@ -158,6 +158,8 @@
         verbosity_value =
             PL_Log::vlevelFromString( command_line[verbosity_value_pos] );
     }
+    // set verbosity level now so that it is valid for the rest of global_options
+    PL_Log::instance().verbosity( verbosity_value );
 
     // Option to enable logging for the specified modules, specified as
     // --enable-logging module1,module2,module3,... i.e. as a comma-separated
@@ -235,7 +237,6 @@
         }
     command_line.resize( cleaned ); // Truncating the end of the vector.
   
-    PL_Log::instance().verbosity( verbosity_value );
     if (no_version_pos == -1)
         output_version( );
 

Modified: trunk/examples/Demo/Tasks/2d_classif/Datasets/moons.amat.metadata/stats.psave
===================================================================
--- trunk/examples/Demo/Tasks/2d_classif/Datasets/moons.amat.metadata/stats.psave	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/examples/Demo/Tasks/2d_classif/Datasets/moons.amat.metadata/stats.psave	2007-02-22 21:05:50 UTC (rev 6676)
@@ -5,8 +5,8 @@
 nnonmissing_ = 54 ;
 sumsquarew_ = 54 ;
 sum_ = 38.4679999999999964 ;
-sumsquare_ = 35.5002080000000078 ;
-sumcube_ = 35.9551203199999918 ;
+sumsquare_ = 35.5002080000000007 ;
+sumcube_ = 35.9551203199999989 ;
 sumfourth_ = 38.3519593203199989 ;
 min_ = -0.855999999999999983 ;
 max_ = 0.508000000000000007 ;

Modified: trunk/plearn/base/ProgressBar.cc
===================================================================
--- trunk/plearn/base/ProgressBar.cc	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/plearn/base/ProgressBar.cc	2007-02-22 21:05:50 UTC (rev 6676)
@@ -3,6 +3,7 @@
 // PLearn (A C++ Machine Learning Library)
 // Copyright (C) 1998 Pascal Vincent
 // Copyright (C) 1999-2002 Pascal Vincent, Yoshua Bengio and University of Montreal
+// Copyright (C) 2007 Xavier Saint-Mleux, ApSTAT Technologies inc.
 //
 
 // Redistribution and use in source and binary forms, with or without
@@ -172,6 +173,91 @@
 }
 
 
+//*************************/
+// RemoteProgressBarPlugin
+
+RemoteProgressBarPlugin::RemoteProgressBarPlugin(ostream& _out, unsigned int nticks_)
+    :TextProgressBarPlugin(_out), nticks(nticks_)
+{}
+
+RemoteProgressBarPlugin::RemoteProgressBarPlugin(PStream& _out, unsigned int nticks_)
+    :TextProgressBarPlugin(_out), nticks(nticks_)
+{}
+
+void RemoteProgressBarPlugin::addProgressBar(ProgressBar* pb)
+{ printTitle(pb); }
+
+void RemoteProgressBarPlugin::update(ProgressBar* pb, unsigned long newpos)
+{
+    // this handles the case where we reuse the same progress bar
+    if(newpos < pb->currentpos)
+    {
+        pb->currentpos=0;
+        printTitle(pb);
+    }
+    if(0 <  (int( newpos / (double(pb->maxpos) / nticks) ) -
+             int(round( pb->currentpos / (double(pb->maxpos) / nticks) ))))
+    {
+        out.write("*PU ");
+        out << reinterpret_cast<unsigned int>(pb) << newpos << endl;
+        pb->currentpos = newpos;
+    }
+}
+
+void RemoteProgressBarPlugin::printTitle(ProgressBar* pb)
+{
+    string fulltitle = string(" ") + pb->title + " (" + tostring(pb->maxpos) + ") ";
+    out.write("*PA ");
+    out << reinterpret_cast<unsigned int>(pb) << pb->maxpos << fulltitle << endl;
+}
+
+void RemoteProgressBarPlugin::killProgressBar(ProgressBar* pb)
+{
+    out.write("*PK ");
+    out << reinterpret_cast<unsigned int>(pb) << endl;
+}
+
+//*************************/
+// LineOutputProgressBarPlugin
+
+LineOutputProgressBarPlugin::LineOutputProgressBarPlugin(ostream& _out, unsigned int nticks_)
+    :TextProgressBarPlugin(_out), nticks(nticks_)
+{}
+
+LineOutputProgressBarPlugin::LineOutputProgressBarPlugin(PStream& _out, unsigned int nticks_)
+    :TextProgressBarPlugin(_out), nticks(nticks_)
+{}
+
+void LineOutputProgressBarPlugin::addProgressBar(ProgressBar* pb)
+{ out << "In progress: " << pbInfo(pb) << endl; }
+
+void LineOutputProgressBarPlugin::update(ProgressBar* pb, unsigned long newpos)
+{
+    // this handles the case where we reuse the same progress bar
+    if(newpos < pb->currentpos)
+    {
+        pb->currentpos= newpos;
+        out << "In progress: ";//to be continued...
+    }
+    else if(0 <  (int( newpos / (double(pb->maxpos) / nticks) ) -
+             int(round( pb->currentpos / (double(pb->maxpos) / nticks) ))))
+        pb->currentpos= newpos;
+    out << pbInfo(pb) << endl;
+}
+
+void LineOutputProgressBarPlugin::killProgressBar(ProgressBar* pb)
+{ out << pbInfo(pb) << " Finished" << endl; }
+
+string LineOutputProgressBarPlugin::pbInfo(ProgressBar* pb)
+{
+    unsigned int curpos= pb->currentpos,
+                 maxpos= pb->maxpos;
+    return string("[") + pb->title + "] " 
+        + tostring(curpos) + '/' + tostring(maxpos)
+        + " (" + tostring(static_cast<double>(curpos)*100.0 / static_cast<double>(maxpos)) + "%)";
+}
+
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/base/ProgressBar.h
===================================================================
--- trunk/plearn/base/ProgressBar.h	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/plearn/base/ProgressBar.h	2007-02-22 21:05:50 UTC (rev 6676)
@@ -3,6 +3,7 @@
 // PLearn (A C++ Machine Learning Library)
 // Copyright (C) 1998 Pascal Vincent
 // Copyright (C) 1999-2002 Pascal Vincent, Yoshua Bengio and University of Montreal
+// Copyright (C) 2007 Xavier Saint-Mleux, ApSTAT Technologies inc.
 //
 
 // Redistribution and use in source and binary forms, with or without
@@ -77,6 +78,7 @@
 //! Simple plugin for displaying text progress bar
 class TextProgressBarPlugin : public ProgressBarPlugin
 {
+protected:
     PStream out;
 public:
     virtual void addProgressBar(ProgressBar * pb);
@@ -89,7 +91,43 @@
     static int width;
 };
 
+//! Similar to TextProgressBarPlugin with a different output format 
+//! so that remote servers can update progress bars on a client.
+class RemoteProgressBarPlugin : public TextProgressBarPlugin
+{
+public:
+    virtual void addProgressBar(ProgressBar* pb);
+    virtual void update(ProgressBar* pb, unsigned long newpos);
 
+    RemoteProgressBarPlugin(ostream& _out, unsigned int nticks_= 20);
+    RemoteProgressBarPlugin(PStream& _out, unsigned int nticks_= 20);
+
+    virtual void killProgressBar(ProgressBar* pb);
+
+protected:
+    void printTitle(ProgressBar* pb);
+    unsigned int nticks;
+};
+
+//! Similar to TextProgressBarPlugin with a different output format 
+//! so that updates appear on different lines of output.
+//! (for logging or when multiple progress bars are used simultaneously)
+class LineOutputProgressBarPlugin : public TextProgressBarPlugin
+{
+public:
+    virtual void addProgressBar(ProgressBar* pb);
+    virtual void update(ProgressBar* pb, unsigned long newpos);
+
+    LineOutputProgressBarPlugin(ostream& _out, unsigned int nticks_= 100);
+    LineOutputProgressBarPlugin(PStream& _out, unsigned int nticks_= 100);
+
+    virtual void killProgressBar(ProgressBar* pb);
+
+protected:
+    static string pbInfo(ProgressBar* pb);
+    unsigned int nticks;
+};
+
 //! Simpler plugin that doesn't display a progress bar at all.  Useful to
 //! disable progress bars for operations that are known to be short.
 //! Use it as follows:

Modified: trunk/plearn/io/PStream.cc
===================================================================
--- trunk/plearn/io/PStream.cc	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/plearn/io/PStream.cc	2007-02-22 21:05:50 UTC (rev 6676)
@@ -40,6 +40,7 @@
 #include <plearn/math/pl_math.h>
 #include <nspr/prio.h>
 #include <ctype.h>
+#include <plearn/io/pl_log.h>
 
 
 // This is probably an ugly hack to get it to work under Visual Studio.
@@ -174,20 +175,20 @@
     :inherited(0),
      inmode(plearn_ascii), 
      outmode(plearn_ascii), 
+     format_float (format_float_default),
+     format_double(format_double_default),
      implicit_storage(true),
-     compression_mode(compr_none),
-     format_float (format_float_default),
-     format_double(format_double_default)
+     compression_mode(compr_none)
 {}
 
 PStream::PStream(streambuftype* sb)
     :inherited(sb),
      inmode(plearn_ascii), 
      outmode(plearn_ascii), 
+     format_float (format_float_default),
+     format_double(format_double_default),
      implicit_storage(true),
-     compression_mode(compr_none),
-     format_float (format_float_default),
-     format_double(format_double_default)
+     compression_mode(compr_none)
 {}
 
 
@@ -196,10 +197,10 @@
     :inherited(new StdPStreamBuf(pin_,own_pin_)),
      inmode(plearn_ascii), 
      outmode(plearn_ascii),
+     format_float (format_float_default),
+     format_double(format_double_default),
      implicit_storage(true),
-     compression_mode(compr_none),
-     format_float (format_float_default),
-     format_double(format_double_default)
+     compression_mode(compr_none)
 {}
 //! ctor. from an ostream (O)
 
@@ -207,10 +208,10 @@
     :inherited(new StdPStreamBuf(pout_,own_pout_)),
      inmode(plearn_ascii), 
      outmode(plearn_ascii),
+     format_float (format_float_default),
+     format_double(format_double_default),
      implicit_storage(true),
-     compression_mode(compr_none),
-     format_float (format_float_default),
-     format_double(format_double_default)
+     compression_mode(compr_none)
 {}
 
 //! ctor. from an iostream (IO)
@@ -218,10 +219,10 @@
     :inherited(new StdPStreamBuf(pios_,own_pios_)),
      inmode(plearn_ascii), 
      outmode(plearn_ascii),
+     format_float (format_float_default),
+     format_double(format_double_default),
      implicit_storage(true),
-     compression_mode(compr_none),
-     format_float (format_float_default),
-     format_double(format_double_default)
+     compression_mode(compr_none)
 {}
 
 //! ctor. from an istream and an ostream (IO)
@@ -229,10 +230,10 @@
     :inherited(new StdPStreamBuf(pin_,pout_,own_pin_,own_pout_)),
      inmode(plearn_ascii), 
      outmode(plearn_ascii),
+     format_float (format_float_default),
+     format_double(format_double_default),
      implicit_storage(true),
-     compression_mode(compr_none),
-     format_float (format_float_default),
-     format_double(format_double_default)
+     compression_mode(compr_none)
 {}
 
 //! dtor.
@@ -1991,7 +1992,7 @@
 
 //! The binread_ for float and double are special
 
-    void binread_(PStream& in, double* x, unsigned int n, unsigned char typecode)
+void binread_(PStream& in, double* x, unsigned int n, unsigned char typecode)
 { 
     if(typecode==TypeTraits<double>::little_endian_typecode())
     {

Modified: trunk/plearn/io/PStreamBuf.cc
===================================================================
--- trunk/plearn/io/PStreamBuf.cc	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/plearn/io/PStreamBuf.cc	2007-02-22 21:05:50 UTC (rev 6676)
@@ -38,6 +38,7 @@
 
 /*! \file PStreamBuf.cc */
 #include "PStreamBuf.h"
+#include <plearn/io/pl_log.h>
 
 #define PSTREAMBUF_NO_GET (-1000)
 
@@ -125,6 +126,7 @@
 #endif
 
     inbuf_p = inbuf + ungetsize;
+    inbuf_end= inbuf_p; //buf empty until read_ finished
     streamsize n = read_(inbuf_p, inbuf_chunksize);
     inbuf_end = inbuf_p + n;
     return n;
@@ -152,25 +154,40 @@
     if(nleft) // need some more ?
     {
         if(nleft>=inbuf_chunksize) // large block: read it directly
-            nleft -= read_(p,nleft);
+        {
+            streamsize nr= read_(p,nleft);
+            nleft-= nr;
+            p+= nr;
+            while(nleft > 0 && nr > 0) // need some more and not eof?
+            {
+                nr= read_(p,nleft);
+                nleft-= nr;
+                p+= nr;
+            }
+        }
         else // small block: read it in the buffer first
         {
             inbuf_n = refill_in_buf();
-            if(inbuf_n)
+            while(nleft > 0 && inbuf_n > 0) // need some more and not eof?
             {
                 streamsize k = nleft<inbuf_n ?nleft :inbuf_n;
                 memcpy(p,inbuf_p,k);
                 inbuf_p += k;
                 nleft -= k;
+                p+= k;
+                if(nleft > 0)
+                    inbuf_n = refill_in_buf();
             }
         }
     }
-  
+
     streamsize nread = n-nleft;
+
     if (nread > 0)
-        last_get = (unsigned char) p[nread - 1];
+        last_get = (unsigned char) p[-1];//p has advanced and now points one after the end
     else
         last_get = EOF;
+
     return nread;
 }
 

Modified: trunk/plearn/io/PStreamBuf.h
===================================================================
--- trunk/plearn/io/PStreamBuf.h	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/plearn/io/PStreamBuf.h	2007-02-22 21:05:50 UTC (rev 6676)
@@ -126,6 +126,9 @@
 
 public:
 
+    bool inbufEmpty() const
+    { return !(inbuf_p<inbuf_end); }
+
     bool isReadable() const
     { return is_readable; }
 
@@ -159,6 +162,8 @@
             return -1;
     }
   
+    
+    //! Reads n chars, unless eof is reached or an error occurs; blocks if needed
     streamsize read(char* p, streamsize n);
 
     //! Puts the given characters back in the input buffer

Modified: trunk/plearn/io/Poll.cc
===================================================================
--- trunk/plearn/io/Poll.cc	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/plearn/io/Poll.cc	2007-02-22 21:05:50 UTC (rev 6676)
@@ -3,6 +3,7 @@
 // Poll.cc
 //
 // Copyright (C) 2005 Christian Hudon 
+// Copyright (C) 2007 Xavier Saint-Mleux, ApSTAT Technologies inc.
 // 
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
@@ -43,7 +44,9 @@
 
 #include "Poll.h"
 #include <plearn/base/plerror.h>
+#include <plearn/base/PrUtils.h>
 #include <plearn/io/PrPStreamBuf.h>
+#include <plearn/math/random.h>
 
 namespace PLearn {
 using namespace std;
@@ -54,33 +57,69 @@
 
     int i = 0;
     for (vector<PStream>::const_iterator it = streams.begin();
-         it != streams.end(); ++it, ++i) {
+         it != streams.end(); ++it, ++i) 
+    {
         PStreamBuf* st = *it;
         PrPStreamBuf* pr_st = dynamic_cast<PrPStreamBuf*>(st);
         if (!pr_st)
             PLERROR("Poll::setStreamsToWatch: only PrPStreamBuf streams supported!");
 
         m_streams_to_watch.push_back(*it);
-        m_poll_descriptors[i].fd = pr_st->out;
+        m_poll_descriptors[i].fd = pr_st->in;
         m_poll_descriptors[i].in_flags = PR_POLL_READ;
     }
+
 }
 
-int Poll::waitForEvents(int timeout) {
+int Poll::waitForEvents(int timeout, bool shuffle_events_) 
+{
     if (m_poll_descriptors.size() == 0)
         PLERROR("Poll::waitforEvents: called with no streams to watch.");
-    
+
+    shuffle_events= shuffle_events_;
+    if(shuffle_events)//shuffle index vec if necessary
+    {
+        shuffled_index= TVec<int>(0, m_poll_descriptors.size()-1, 1);
+        shuffleElements(shuffled_index);
+    }
+
     m_next_unexamined_event = 0;
-    return PR_Poll(&m_poll_descriptors[0], PRIntn(m_poll_descriptors.size()),
-                   timeout);
+
+    //first, check for non-empty buffers (ready to read)
+    int nevents= 0;
+    for(unsigned int i= 0; i < m_poll_descriptors.size(); ++i)
+        if(!m_streams_to_watch[i]->inbufEmpty())
+            ++nevents;
+
+    if(nevents > 0)//if we already have some events, poll w/ no wait
+        timeout= PR_INTERVAL_NO_WAIT;
+
+    //poll underlying streams
+    last_n_poll_events= PR_Poll(&m_poll_descriptors[0], PRIntn(m_poll_descriptors.size()), timeout);
+
+    if(last_n_poll_events < 0)
+        PLERROR((string("Poll::waitForEvents: poll error: ") + getPrErrorString()).c_str());
+
+    nevents= 0;// now count _all_ events (non-empty buffers + stream polling)
+    for(unsigned int i= 0; i < m_poll_descriptors.size(); ++i)
+        if ((last_n_poll_events > 0 
+             && m_poll_descriptors[i].out_flags & PR_POLL_READ)
+            || !m_streams_to_watch[i]->inbufEmpty())
+            ++nevents;
+
+    return nevents;
 }
 
 PStream Poll::getNextPendingEvent() {
-    while (m_next_unexamined_event < m_poll_descriptors.size()) {
-        const int i = m_next_unexamined_event++;
-        if (m_poll_descriptors[i].out_flags & PR_POLL_READ) {
+    while (m_next_unexamined_event < m_poll_descriptors.size()) 
+    {
+        int i = m_next_unexamined_event++;
+        if(shuffle_events)
+            i= shuffled_index[i];
+        if ((last_n_poll_events > 0 
+             && m_poll_descriptors[i].out_flags & PR_POLL_READ)
+            || !m_streams_to_watch[i]->inbufEmpty())
             return m_streams_to_watch[i];
-        }
     }
 
     PLERROR("Poll::getNextPendingEvent: called with no more pending events!");

Modified: trunk/plearn/io/Poll.h
===================================================================
--- trunk/plearn/io/Poll.h	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/plearn/io/Poll.h	2007-02-22 21:05:50 UTC (rev 6676)
@@ -3,6 +3,7 @@
 // Poll.h
 //
 // Copyright (C) 2005 Christian Hudon 
+// Copyright (C) 2007 Xavier Saint-Mleux, ApSTAT Technologies inc.
 // 
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
@@ -47,8 +48,8 @@
 #include <vector>
 #include <nspr/prio.h>
 #include <plearn/io/PStream.h>
+#include <plearn/math/TVec.h>
 
-
 namespace PLearn {
 using namespace std;
 
@@ -61,7 +62,7 @@
 public:
     void setStreamsToWatch(const vector<PStream>& streams);
 
-    int waitForEvents(int timeout = 0);
+    int waitForEvents(int timeout = 0, bool shuffle_events_= false);
 
     PStream getNextPendingEvent();
 
@@ -76,6 +77,10 @@
     /** Counter used to iterate through the m_poll_descriptors
         in getNextPendingEvent */
     unsigned int m_next_unexamined_event;
+
+    int last_n_poll_events; //nb. events for the last PR_Poll; indicates wether out_flags s/b used.
+    bool shuffle_events;
+    TVec<int> shuffled_index;
 };
 
 } // end of namespace PLearn

Modified: trunk/plearn/io/PrPStreamBuf.cc
===================================================================
--- trunk/plearn/io/PrPStreamBuf.cc	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/plearn/io/PrPStreamBuf.cc	2007-02-22 21:05:50 UTC (rev 6676)
@@ -42,8 +42,10 @@
 
 
 #include "PrPStreamBuf.h"
+#include <plearn/base/PrUtils.h>
 #include <nspr/prio.h>
 #include <stdio.h>
+#include "PStream.h"
 
 namespace PLearn {
 using namespace std;
@@ -82,7 +84,10 @@
 
 PrPStreamBuf::streamsize PrPStreamBuf::read_(char* p, streamsize n)
 {
-    return PR_Read(in, p, PRInt32(n));
+    PRInt32 nr= PR_Read(in, p, PRInt32(n));
+    if(nr < 0)
+        PLERROR((string("in PrPStreamBuf::read_ : no chars read: ") + getPrErrorString()).c_str());
+    return nr;
 }
 
 //! writes exactly n characters from p (unbuffered, must flush)

Added: trunk/plearn/io/ServerLogStreamBuf.cc
===================================================================
--- trunk/plearn/io/ServerLogStreamBuf.cc	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/plearn/io/ServerLogStreamBuf.cc	2007-02-22 21:05:50 UTC (rev 6676)
@@ -0,0 +1,83 @@
+// -*- C++ -*-
+
+// ServerLogStreamBuf.cc
+//
+// Copyright (C) 2007 Xavier Saint-Mleux, Apstat Technologies, inc.
+// 
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Xavier Saint-Mleux
+
+/*! \file ServerLogStreamBuf.cc */
+
+
+#include "ServerLogStreamBuf.h"
+
+namespace PLearn {
+using namespace std;
+
+ServerLogStreamBuf::ServerLogStreamBuf(PStream log_, const string& module_name_, int verbosity_)
+    : PStreamBuf(false, true, 4096, 4096), 
+      log(log_), module_name(module_name_), verbosity(verbosity_)
+{}
+
+ServerLogStreamBuf::~ServerLogStreamBuf()
+{
+    flush();
+}
+
+ServerLogStreamBuf::streamsize ServerLogStreamBuf::read_(char* p, streamsize n)
+{
+    PLERROR("ServerLogStreamBuf::read_ should never be used!");
+    return 0; // never reached 
+}
+
+//! writes exactly n characters from p (unbuffered, must flush)
+void ServerLogStreamBuf::write_(const char* p, streamsize n)
+{
+    log.write("*L "); 
+    string msg(p, n);
+    log << module_name << verbosity << msg;
+    log.flush();
+}
+  
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/io/ServerLogStreamBuf.h
===================================================================
--- trunk/plearn/io/ServerLogStreamBuf.h	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/plearn/io/ServerLogStreamBuf.h	2007-02-22 21:05:50 UTC (rev 6676)
@@ -0,0 +1,94 @@
+// -*- C++ -*-
+
+// ServerLogStreamBuf.h
+//
+// Copyright (C) 2007 Xavier Saint-Mleux, Apstat Technologies, inc.
+// 
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Xavier Saint-Mleux
+
+/*! \file ServerLogStreamBuf.h */
+
+
+#ifndef ServerLogStreamBuf_INC
+#define ServerLogStreamBuf_INC
+
+#include "PStreamBuf.h"
+#include "PStream.h"
+
+
+namespace PLearn {
+
+class ServerLogStreamBuf: public PStreamBuf
+{
+  
+private:
+  
+    typedef PStreamBuf inherited;
+
+    PStream log;
+    string module_name;
+    int verbosity;
+
+protected:
+    // *********************
+    // * protected options *
+    // *********************
+
+public:
+
+    ServerLogStreamBuf(PStream log_, const string& module_name_= "", int verbosity_= 0);
+    virtual ~ServerLogStreamBuf();
+
+protected:
+
+    virtual streamsize read_(char* p, streamsize n); // SHOULD NOT BE USED!!!
+
+    //! writes exactly n characters from p (unbuffered, must flush)
+    virtual void write_(const char* p, streamsize n);
+
+};
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Modified: trunk/plearn/io/openSocket.cc
===================================================================
--- trunk/plearn/io/openSocket.cc	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/plearn/io/openSocket.cc	2007-02-22 21:05:50 UTC (rev 6676)
@@ -77,6 +77,7 @@
     st.setMode(io_formatting);
   
     PRFileDesc* socket = PR_NewTCPSocket();
+
     if (!socket)
         PLERROR("openSocket: socket creation failed! (Maybe you ran out of file descriptors?)");
 
@@ -96,9 +97,10 @@
         if (PR_Connect(socket, &address, PR_SecondsToInterval(timeout))
                                                                 == PR_SUCCESS)
         {
-            st = new PrPStreamBuf(socket, socket, true, true);
-            return st;
-        } else {
+            return new PrPStreamBuf(socket, socket, true, true);
+        } 
+        else 
+        {
 #ifdef BOUNDCHECK
             string ip_adr = "Unknown IP address";
             if (PR_NetAddrToString(&address, buf, sizeof(buf)) == PR_SUCCESS)

Modified: trunk/plearn/io/pl_log.cc
===================================================================
--- trunk/plearn/io/pl_log.cc	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/plearn/io/pl_log.cc	2007-02-22 21:05:50 UTC (rev 6676)
@@ -69,7 +69,23 @@
     return m_pstream;
 }
 
+//#####  Server Plugin Implementation  #######################################
 
+PStream& PL_LogPluginServer::getStream(
+    PStream::mode_t outmode, const string& module_name, int requested_verbosity)
+{
+    //gets everything as raw ascii, outputs it in desired format.
+    //m_pstream.setOutMode(outmode);
+    m_sstream= new ServerLogStreamBuf(m_pstream, module_name, requested_verbosity);
+    m_sstream.setOutMode(outmode);
+    //m_sstream.setOutMode(PStream::raw_ascii);
+    return m_sstream;
+}
+
+
+
+
+
 //#####  LogInterceptorPStreamBuf  ############################################
 
 /**

Modified: trunk/plearn/io/pl_log.h
===================================================================
--- trunk/plearn/io/pl_log.h	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/plearn/io/pl_log.h	2007-02-22 21:05:50 UTC (rev 6676)
@@ -55,6 +55,7 @@
 
 // From Plearn
 #include "PStream.h"
+#include "ServerLogStreamBuf.h"
 
 namespace PLearn {
 
@@ -118,6 +119,25 @@
 };
 
 
+/**
+ *  Server implementation of PL_LogPlugin :: outputs to client through opened socket
+ */
+class PL_LogPluginServer : public PL_LogPlugin
+{
+public:
+    PL_LogPluginServer(PStream pstream)
+        : m_pstream(pstream)
+    { }
+    
+    virtual PStream& getStream(PStream::mode_t outmode, const string& module_name,
+                               int requested_verbosity);
+
+protected:
+    PStream m_pstream;                      
+    PStream m_sstream;                      
+};
+
+
 class LogInterceptorPStreamBuf;              //!< Forward declare
 
 /**

Modified: trunk/plearn/math/TMat_maths_impl.h
===================================================================
--- trunk/plearn/math/TMat_maths_impl.h	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/plearn/math/TMat_maths_impl.h	2007-02-22 21:05:50 UTC (rev 6676)
@@ -453,13 +453,13 @@
 template<class T>
 T product(const TVec<T>& vec)
 {
-    double res = 1.0;
+    T res(static_cast<T>(1.0));
     if (vec.size() == 0)
         return res;
     T* v = vec.data();
     for(int i=0; i<vec.length(); i++)
         res *= v[i];
-    return T(res);
+    return res;
 }
 
 /*

Modified: trunk/plearn/misc/PLearnServer.cc
===================================================================
--- trunk/plearn/misc/PLearnServer.cc	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/plearn/misc/PLearnServer.cc	2007-02-22 21:05:50 UTC (rev 6676)
@@ -3,6 +3,7 @@
 // PLearnServer.cc
 //
 // Copyright (C) 2005 Pascal Vincent 
+// Copyright (C) 2007 Xavier Saint-Mleux, ApSTAT Technologies inc.
 // 
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
@@ -180,7 +181,7 @@
              "\n"
              "Summary of currently supported functions:\n"
              "  !F cd 1 \"path\" \n\n"
-             "Advanced technical note: objects with objid>=10000 are also inserted in the stream's copies_map\n"
+             "OBSOLETE: Advanced technical note: objects with objid>=10000 are also inserted in the stream's copies_map\n"
              "so that they may be referenced as arguments to method or funtion calls, for ex as: *10001; \n"
              "\n"
         );
@@ -189,8 +190,6 @@
 
 bool PLearnServer::run()
 {
-    const int upper_bound_id = 10000;
-
     int obj_id;
     Object* obj;
     ObjMap::iterator found;
@@ -198,27 +197,36 @@
     int n_args; // number of input arguments to the method call
     string filepath;
 
+    // forward log messages to client
+    PP<PL_LogPlugin> orig_log_plugin= PL_Log::instance().getCurrentPlugin();
+    PL_Log::instance().setPlugin(new PL_LogPluginServer(io));
+    // forward progress messages to client
+    PP<ProgressBarPlugin> orig_pb_plugin= ProgressBar::getCurrentPlugin();
+    ProgressBar::setPlugin(new RemoteProgressBarPlugin(io));
+    // forward pout&perr to client
+    PStream orig_pout= pout;
+    PStream orig_perr= perr;
+    pout= new ServerLogStreamBuf(io, "pout");
+    perr= new ServerLogStreamBuf(io, "perr");
+
     DBG_LOG << "ENTERING PLearnServer::run()" << endl;
+
     for(;;)
     {
         if(clear_maps)
         {
             io.copies_map_in.clear();
             io.copies_map_out.clear();
-
             for (ObjMap::iterator it = objmap.begin(); it != objmap.end(); ++it)
-                if (it->first >= upper_bound_id)
-                    io.copies_map_in[it->first] = it->second;
+                io.copies_map_in[it->first] = it->second;
         }
         int c = -1;
-        do { c = io.get(); }
-        while(c!='!' && c!=EOF);
+        do 
+            c = io.get(); 
+        while(io && c!='!' && c!=EOF);
         
-        if(c==EOF)
-        {
-            // cerr << "Read EOF: quitting" << endl;
+        if(c==EOF || !io)
             return true;
-        }
         int command = io.get();
         
         try 
@@ -235,7 +243,6 @@
                 break;
 
             case 'F': // call function 
-
                 io >> method_name >> n_args;
                 callFunction(method_name, n_args);
                 io << endl;
@@ -251,6 +258,18 @@
                 io << endl;  
                 DBG_LOG << "-> OBJECT CREATED." << endl;
                 break;
+
+            case 'O': // new w/o id; id is returned
+                DBG_LOG << "PLearnServer NEW OBJECT w/o ID" << endl;
+                obj = 0;
+                io >> obj;           // Read new object
+                obj_id= findFreeObjID(obj);
+                DBG_LOG << "  obj_id = " << obj_id << endl;
+                objmap[obj_id] = obj;
+                Object::prepareToSendResults(io,1);
+                io << obj_id << endl;  
+                DBG_LOG << "-> OBJECT CREATED." << endl;
+                break;
             
             case 'L': // load from file
                 DBG_LOG << "PLearnServer LOAD OBJECT" << endl;
@@ -305,6 +324,11 @@
                 break;
 
             case 'Q': // quit
+                PL_Log::instance().setPlugin(orig_log_plugin);
+                ProgressBar::setPlugin(orig_pb_plugin);
+                pout= orig_pout;
+                perr= orig_perr;
+                io.setMode(PStream::plearn_ascii);
                 DBG_LOG << "PLearnServer QUIT" << endl;
                 // cerr << "Quitting" << endl;
                 DBG_LOG << "LEAVING PLearnServer::run()" << endl;
@@ -358,6 +382,25 @@
     return true;
 }
 
+int PLearnServer::findFreeObjID(const Object* obj) const
+{
+    //DUMMY method that tries to find an unused ID (LCG look-alike seeded w/ obj's address)
+    // this algorithm is not guaranteed to work... use at your own risk or modify accordingly
+    int id= reinterpret_cast<unsigned int>(obj) >> 1;
+    if(id < 0) id= -id;
+    const int maxtries= 65536;
+    int ntries= 0;
+    while(objmap.find(id) != objmap.end() && ++ntries < maxtries)
+    {
+        id= id*1664525 + 1013904223;//simple "LCG"
+        if(id < 0) id= -id;
+    }
+    if(ntries >= maxtries)
+        PLERROR("PLearnServer::findFreeObjID : can't find a suitable ID within %d tries.", maxtries);
+    return id;
+}
+
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/misc/PLearnServer.h
===================================================================
--- trunk/plearn/misc/PLearnServer.h	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/plearn/misc/PLearnServer.h	2007-02-22 21:05:50 UTC (rev 6676)
@@ -68,9 +68,12 @@
     PStream io;
     bool clear_maps;
     ObjMap objmap;
+
+    virtual int findFreeObjID(const Object* obj) const;
+
 public:
     PLearnServer(const PStream& input_output);
-    ~PLearnServer();
+    virtual ~PLearnServer();
 
     //! Enters the server loop which listens for commands and executes them.
     //! Returns false only if server kill command '!K' was issued

Modified: trunk/plearn/misc/PLearnService.cc
===================================================================
--- trunk/plearn/misc/PLearnService.cc	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/plearn/misc/PLearnService.cc	2007-02-22 21:05:50 UTC (rev 6676)
@@ -3,6 +3,7 @@
 // PLearnService.cc
 //
 // Copyright (C) 2005 Pascal Vincent 
+// Copyright (C) 2007 Xavier Saint-Mleux, ApSTAT Technologies inc.
 // 
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
@@ -80,6 +81,8 @@
 {
     PStream in = openFile(serversfile, PStream::raw_ascii, "r");
 
+    DBG_LOG << "PLearnService::connectToServers(" << serversfile << ')' << endl;
+
     string hostname;
     string pid;
     int tcpport = -1;
@@ -100,28 +103,56 @@
 
 void PLearnService::connectToServers(TVec< pair<string,int> > hostname_and_port)
 {
+
+    DBG_LOG << "PLearnService::connectToServers(" << hostname_and_port << ')' << endl;
+
     if(available_servers.size()>0)
         disconnectFromServers();
     for(int k=0; k<hostname_and_port.length(); k++)
     {
         pair<string, int> host_port = hostname_and_port[k];
-        PStream servio = openSocket(host_port.first, host_port.second, PStream::plearn_binary);
-        // PStream servio = openSocket(host_port.first, host_port.second, PStream::plearn_ascii);
+        PStream servio = openSocket(host_port.first, host_port.second, PStream::plearn_ascii);
         PP<RemotePLearnServer> serv = new RemotePLearnServer(servio);
         serv->callFunction("binary");
-        serv->expectResults(0);
+        
+        TVec<PP<RemotePLearnServer> > ss;
+        ss.push_back(serv);
+
+        watchServers(ss, log_callback, progress_callback);
+
+        servio << PStream::plearn_binary;
+
+        reserved_servers.insert(serv);
+        serv->getResults();
+        reserved_servers.erase(serv);
         available_servers.push(serv);
-        //serversio.append(servio);
-        //available_servers.push(k);
     }
 }
 
 void PLearnService::disconnectFromServers()
 {
-    available_servers = TVec< PP<RemotePLearnServer> >();
+    //available_servers = TVec< PP<RemotePLearnServer> >();
+    while(available_servers.length() > 0)
+        disconnectFromServer(available_servers[0]);
 }
 
 
+void PLearnService::disconnectFromServer(PP<RemotePLearnServer> server)
+{
+    for(int i= 0; i < available_servers.length(); ++i)
+        if(available_servers[i] == server)
+        {
+            available_servers.remove(i);
+            server->io.write("!Q ");
+            server->io << endl;
+            if(progress_bars.find(server) != progress_bars.end())
+                progress_bars.erase(server);
+            return;
+        }
+    PLERROR("PLearnService::disconnectFromServer : trying to disconnect from a server which is not available"
+            " (not connected to or reserved)");
+}
+
 int PLearnService::availableServers() const
 {    
     return available_servers.size();
@@ -166,8 +197,8 @@
 void PLearnService::freeServer(PP<RemotePLearnServer> server)
 {
     DBG_LOG << "PLearnService::freeServer(...)" << endl;
+    server->deleteAllObjects();
     server->clearMaps();
-    server->deleteAllObjects();
     if(reserved_servers.erase(server)!=1)
         PLERROR("Problem in PLearnService::freeServer are you sure this server had been properly reserved?");
     available_servers.push(server);
@@ -200,6 +231,122 @@
     return -1;  // To make the compiler happy (never reached).
 }
 
+
+int PLearnService::watchServers(TVec< PP<RemotePLearnServer> > servers, 
+                                log_callback_t log_callback, 
+                                progress_callback_t progress_callback)
+{
+    Poll p;
+    int n = servers.size();
+    vector<PStream> streams(n);
+    for(int k=0; k<n; k++)
+        streams[k] = servers[k]->io;
+    p.setStreamsToWatch(streams);
+
+    for(;;)
+    {
+        PRInt32 npending = p.waitForEvents(PR_INTERVAL_NO_TIMEOUT, true);
+        if(npending<=0)
+            return -1;
+
+        int the_k= -1;
+        PStream io = p.getNextPendingEvent();    
+
+        for(int k=0; k<n; k++)
+            if(streams[k] == io)
+                the_k= k;
+
+        if(the_k >= 0)
+        {
+            int c= io.peek();
+            // skip blanks one at a time and poll again
+            if(static_cast<char>(c) == ' ' || static_cast<char>(c) == '\t' 
+               || static_cast<char>(c) == '\n' || static_cast<char>(c) == '\r') 
+            {
+                io.get();
+                continue;
+            }
+            else if(static_cast<char>(c) == '*') //async. message (log or progress)
+            {
+                io.get(); // get '*'
+                c= io.get();// get msg type ('L'og or 'P'rogress)
+
+                int vlevel, c0;
+                unsigned int pos, ptr;
+                string mesg;
+                string module("");
+                switch(static_cast<char>(c))
+                {
+                case 'L' : // log message
+                    io >> module >> vlevel >> mesg;
+                    log_callback(servers[the_k], module, vlevel, mesg);
+                    break;
+                case 'P' : // progress message
+                    c0= io.get(); // action: 'A'dd, 'U'pdate or 'K'ill
+                    io >> ptr;// pbar id.
+                    if(static_cast<char>(c0) != 'K')
+                        io >> pos;// Add: maxpos; Update: curpos
+                    if(static_cast<char>(c0) == 'A')
+                        io >> mesg;// pbar title
+                    progress_callback(servers[the_k], ptr, static_cast<char>(c0), pos, mesg);
+                    break;
+                default:
+                    PLERROR("PLearnService::watchServers : Expected *L or *P, received *%c", c);
+                    break;
+                }
+            }
+            else //synchronous message, return server's id
+            {
+                return the_k;
+            }
+        }
+        else
+            PLERROR("stream returned by NextPendingEvent is none of the servers' io field. This should not happen!");
+    }
+    return -1;  // To make the compiler happy (never reached).
+}
+
+
+
+PP<RemotePLearnServer> PLearnService::waitForResult(TVec< PP<RemotePLearnServer> > servers, 
+                                log_callback_t log_callback, 
+                                progress_callback_t progress_callback)
+{
+    int min_server= 0;
+    if(servers.isEmpty())
+    {
+        servers= available_servers;
+        min_server= available_servers.length();
+        for(std::set<PP<RemotePLearnServer> >::iterator it= reserved_servers.begin();
+            it != reserved_servers.end(); ++it)
+            servers.push_back(*it);
+    }
+
+    if(servers.isEmpty())
+        PLERROR("in PLearnService::waitForResult : cannot wait for a result"
+                " when you are not connected to any server.");
+    int server= servers.length();
+
+    //send results from reserved servers only even if polling all servers
+    while(server >= 0 && server < min_server || server == servers.length())
+        server= watchServers(servers, log_callback, progress_callback);
+
+    if(server < 0)
+        PLERROR("in PLearnService::waitForResult : no server returned anything.");
+    return servers[server];
+}
+
+
+void PLearnService::waitForResultFrom(PP<RemotePLearnServer> from,
+                                      log_callback_t log_callback,
+                                      progress_callback_t progress_callback)
+{
+    PP<RemotePLearnServer> server= waitForResult();
+    while(server != from)
+        server= waitForResult();
+}
+
+
 /*
   void PLearnService::freeServer(RemotePLearnServer* remoteserv)
   {
@@ -218,9 +365,78 @@
 
 PLearnService::~PLearnService()
 {
+    if(reserved_servers.size() != 0)
+        PLERROR("PLearnService::~PLearnService : some servers are still reserved; free them first.");
+
+    TVec<PP<RemotePLearnServer> > servers(available_servers.length());
+    servers << available_servers;
+
+    perr << "servers to disconnect from: " << servers.length() << endl;
+
     disconnectFromServers();
+
+    perr << "start watching " << servers.length() << endl;
+
+    //now, get what's remaining on the servers streams
+    for(int i= 0; i < servers.length(); ++i)
+    {
+        try
+        {
+            for(;;) watchServers(servers, log_callback, progress_callback);
+        }
+        catch(const PLearnError& e)
+        {
+            perr << "one dead " << i << endl;
+        }
+    }
+
+    perr << "finished watching " << endl;
 }
 
+
+void PLearnService::log_callback(PP<RemotePLearnServer> server, const string& module_name, int vlevel, const string& msg)
+{ 
+    unsigned int server_id= reinterpret_cast<unsigned int>(static_cast<RemotePLearnServer*>(server));
+    PL_LOG(vlevel) << "<From server " << server_id << "> [" << module_name << "] " << msg << flush; 
+}
+
+PLearnService::progress_bars_t PLearnService::progress_bars; // init
+
+void PLearnService::progress_callback(PP<RemotePLearnServer> server, unsigned int pbar, char action, 
+                                      unsigned int pos, const string& title)
+{
+    unsigned int server_id= reinterpret_cast<unsigned int>(static_cast<RemotePLearnServer*>(server));
+    static bool need_to_set_pb_plugin= true;
+    if(need_to_set_pb_plugin)
+    {
+        ProgressBar::setPlugin(new LineOutputProgressBarPlugin(cerr));
+        need_to_set_pb_plugin= false;
+    }
+
+    switch(action)
+    {
+    case 'A': // add new progress bar
+        if(progress_bars.find(server) == progress_bars.end())
+            progress_bars[server]= map<unsigned int, PP<ProgressBar> >();
+        {//local environment for 'fulltitle'... silly c++ switch/case...
+            string fulltitle= string("<server#") + tostring(server_id) 
+                + ":pb#" + tostring(pbar) + "> " + title;//adjust title w/server info
+            progress_bars[server][pbar]= new ProgressBar(fulltitle, pos);
+        }
+        break;
+    case 'U': // update progress bar
+        progress_bars[server][pbar]->update(pos);
+        break;
+    case 'K': // kill progress bar
+        progress_bars[server].erase(pbar);
+        break;
+    default:
+        PLERROR("in PLearnService::progress_callback: unknown action %c", action);
+        break;
+    }
+}
+
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/misc/PLearnService.h
===================================================================
--- trunk/plearn/misc/PLearnService.h	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/plearn/misc/PLearnService.h	2007-02-22 21:05:50 UTC (rev 6676)
@@ -3,6 +3,7 @@
 // PLearnService.h
 //
 // Copyright (C) 2005 Pascal Vincent 
+// Copyright (C) 2007 Xavier Saint-Mleux, ApSTAT Technologies inc.
 // 
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
@@ -48,7 +49,7 @@
 #include <plearn/io/PPath.h>
 #include <plearn/io/PStream.h>
 #include <plearn/math/TVec.h>
-// #include <map>
+#include <map>
 #include <set>
 #include <string>
 #include <plearn/misc/RemotePLearnServer.h>
@@ -69,6 +70,9 @@
     TVec< PP<RemotePLearnServer> > available_servers;
     std::set< PP<RemotePLearnServer> > reserved_servers;
 
+    typedef map<RemotePLearnServer*, map<unsigned int, PP<ProgressBar> > > progress_bars_t;
+    static progress_bars_t progress_bars;
+
 public:
     friend class RemotePLearnServer;
 
@@ -86,6 +90,8 @@
 
     void disconnectFromServers();
 
+    void disconnectFromServer(PP<RemotePLearnServer> server);
+
     //! returns the number of available processing ressources
     int availableServers() const;
 
@@ -106,6 +112,26 @@
 
     int watchServers(TVec< PP<RemotePLearnServer> > servers, int timeout=0);
 
+    typedef void (*log_callback_t)(PP<RemotePLearnServer> server, const string& module_name, int vlevel, const string& msg);
+    typedef void (*progress_callback_t)(PP<RemotePLearnServer> server, unsigned int pbar, char action, 
+                                        unsigned int pos, const string& title);
+
+    static void log_callback(PP<RemotePLearnServer> server, const string& module_name, int vlevel, const string& msg);
+    static void progress_callback(PP<RemotePLearnServer> server, unsigned int pbar, char action, 
+                                  unsigned int pos= 0, const string& title= "");
+
+    int watchServers(TVec< PP<RemotePLearnServer> > servers, 
+                     log_callback_t log_callback,
+                     progress_callback_t progress_callback);
+
+    PP<RemotePLearnServer> waitForResult(TVec< PP<RemotePLearnServer> > servers= TVec< PP<RemotePLearnServer> >(), 
+                                         log_callback_t log_callback= log_callback,
+                                         progress_callback_t progress_callback= progress_callback);
+
+    void waitForResultFrom(PP<RemotePLearnServer> from,
+                           log_callback_t log_callback= log_callback,
+                           progress_callback_t progress_callback= progress_callback);
+    
     ~PLearnService();
 };
 

Modified: trunk/plearn/misc/RemotePLearnServer.cc
===================================================================
--- trunk/plearn/misc/RemotePLearnServer.cc	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/plearn/misc/RemotePLearnServer.cc	2007-02-22 21:05:50 UTC (rev 6676)
@@ -3,6 +3,7 @@
 // RemotePLearnServer.cc
 //
 // Copyright (C) 2005 Pascal Vincent 
+// Copyright (C) 2007 Xavier Saint-Mleux, ApSTAT Technologies inc.
 // 
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
@@ -42,6 +43,7 @@
 
 
 #include "RemotePLearnServer.h"
+#include "PLearnService.h"
 #include <plearn/io/pl_log.h>
 
 namespace PLearn {
@@ -80,35 +82,136 @@
     expectResults(0);
 }
 
+int RemotePLearnServer::newObject(const Object& model)
+{ 
+    clearMaps();
+    io.write("!O "); io << model << endl;
+    int objid;
+    getResults(objid);
+    return objid;
+}
+
+int RemotePLearnServer::newObject(PP<Object> model)
+{
+    if(model.isNull())
+        PLERROR("In RemotePLearnServer::newObject model is a Null pointer");
+    return newObject(*model);
+}
+
+int RemotePLearnServer::newObject(const string& description)
+{ 
+    clearMaps();
+    io.write("!O "); 
+    io.write(description);
+    io << endl;
+    int objid;
+    getResults(objid);
+    return objid;
+}
+
+
+void RemotePLearnServer::newObjectAsync(int objid, const Object& model)
+{ 
+    clearMaps();
+    io.write("!N "); io << objid << model << endl;
+}
+
+void RemotePLearnServer::newObjectAsync(int objid, PP<Object> model)
+{
+    if(model.isNull())
+        PLERROR("In RemotePLearnServer::newObject model is a Null pointer");
+    newObjectAsync(objid, *model);
+}
+
+void RemotePLearnServer::newObjectAsync(int objid, const string& description)
+{ 
+    clearMaps();
+    io.write("!N "); io << objid; io.put(' ');
+    io.write(description);
+    io << endl;
+}
+
+void RemotePLearnServer::newObjectAsync(const Object& model)
+{ 
+    clearMaps();
+    io.write("!O "); io << model << endl;
+}
+
+void RemotePLearnServer::newObjectAsync(const PP<Object>& model)
+{
+    if(model.isNull())
+        PLERROR("In RemotePLearnServer::newObject model is a Null pointer");
+    newObjectAsync(*model);
+}
+
+void RemotePLearnServer::newObjectAsync(const string& description)
+{ 
+    clearMaps();
+    io.write("!O "); 
+    io.write(description);
+    io << endl;
+}
+
+
+
 void RemotePLearnServer::deleteObject(int objid)
 {
-    io.write("!D "); io << objid << endl;
+    deleteObjectAsync(objid);
     expectResults(0);
 }
 
+void RemotePLearnServer::deleteObjectAsync(int objid)
+{
+    io.write("!D "); io << objid << endl;
+}
+
 void RemotePLearnServer::deleteAllObjects()
 {
-    io.write("!Z "); 
-    io << endl;
-    expectResults(0);
+    deleteAllObjectsAsync();
+    getResults();
+/*
+    if(io)
+    {
+        io.write("!Z "); 
+        io << endl;
+        expectResults(0);
+    }
+    else
+        DBG_LOG << "in RemotePLearnServer::deleteAllObjects() : stream not good." << endl;
+*/
 }
 
+void RemotePLearnServer::deleteAllObjectsAsync()
+{
+    if(io)
+    {
+        io.write("!Z "); 
+        io << endl;
+    }
+    else
+        DBG_LOG << "in RemotePLearnServer::deleteAllObjectsAsync() : stream not good." << endl;
+}
 
+
 void RemotePLearnServer::expectResults(int nargs_expected)
 {
-    DBG_LOG << "RemotePLearnServer entering expectResults" << endl;
+    PLearnService& service= PLearnService::instance();
+    service.waitForResultFrom(this);
+
+    //DBG_LOG << "RemotePLearnServer entering expectResults" << endl;
     io.skipBlanksAndComments();
     int headchar = io.get();
     if(headchar!='!')
         PLERROR(" Answers from plearn server are expected to start with a !, but I received a %c",headchar);
     int command = io.get();
-    DBG_LOG << "RemotePLearnServer expectResults received command: " << (char)command << endl;
+    //DBG_LOG << "RemotePLearnServer expectResults received command: " << (char)command << endl;
     int nreturned;
     string msg;
     switch(command)
     {
     case 'R':
         io >> nreturned;
+        //DBG_LOG << "RemotePLearnServer expectResults nreturned= " << nreturned << endl;
         if(nreturned!=nargs_expected)
             PLERROR("RemotePLearnServer: expected %d return arguments, but read R %d",nargs_expected,nreturned);
         break;
@@ -123,15 +226,10 @@
 
 RemotePLearnServer::~RemotePLearnServer()
 {
-    DBG_LOG << "ENTERING RemotePLearnServer destructor" << endl;
-    deleteAllObjects();
-    //io.write("!Q");
-    //io = 0;
-    // DBG_LOG << "RemotePLearnServer destructor: BEFORE wait" << endl;
-    // prg->wait();
-    // DBG_LOG << "RemotePLearnServer destructor: AFTER wait" << endl;
-    // PLearnService::instance().freeServer(this);
-    DBG_LOG << "LEAVING RemotePLearnServer destructor" << endl;
+    // The PLearnService is responsible for RemotePLearnServer destruction 
+
+    //DBG_LOG << "ENTERING RemotePLearnServer destructor" << endl;
+    //DBG_LOG << "LEAVING RemotePLearnServer destructor" << endl;
 }
 
 

Modified: trunk/plearn/misc/RemotePLearnServer.h
===================================================================
--- trunk/plearn/misc/RemotePLearnServer.h	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/plearn/misc/RemotePLearnServer.h	2007-02-22 21:05:50 UTC (rev 6676)
@@ -3,6 +3,7 @@
 // RemotePLearnServer.h
 //
 // Copyright (C) 2005 Pascal Vincent 
+// Copyright (C) 2007 Xavier Saint-Mleux, ApSTAT Technologies inc.
 // 
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
@@ -48,6 +49,7 @@
 #include <plearn/io/PStream.h>
 #include <plearn/base/Object.h>
 #include <plearn/sys/Popen.h>
+#include <plearn/io/pl_log.h>
 
 namespace PLearn {
 
@@ -56,11 +58,13 @@
 {    
 private:
     friend class PLearnService;
-
     PStream io; // io communication channel with remote PLearnServer
     RemotePLearnServer(const PStream& serverio);
 
 public:
+
+
+    void killServer() { io << "!K " << endl; }
     
     //! Builds an object based on the given model on the remote server,
     //! assigning it the given id.
@@ -72,12 +76,42 @@
     //! in serialised form.
     void newObject(int objid, const string& description);  
 
+    //! Builds an object based on the given model on the remote server,
+    //! returns an assigned id.
+    int newObject(const Object& model);
+    int newObject(PP<Object> model);
+    //! Builds an object on the remote server, from its description
+    //! in serialised form.
+    int newObject(const string& description);  
+    
+    //! Builds an object based on the given model on the remote server,
+    //! assigning it the given id.
+    void newObjectAsync(int objid, const Object& model);
+    void newObjectAsync(int objid, PP<Object> model);
+    //! Builds an object on the remote server, from its description
+    //! in serialised form.
+    void newObjectAsync(int objid, const string& description);  
+
+    //! Builds an object based on the given model on the remote server,
+    //! id is assigned by the server and returned.
+    void newObjectAsync(const Object& model);
+    void newObjectAsync(const PP<Object>& model);
+    //! Builds an object on the remote server, from its description
+    //! in serialised form.
+    void newObjectAsync(const string& description);  
+
     //! Deletes an object of the remote server.
     void deleteObject(int objid);
 
+    //! Deletes an object of the remote server.
+    void deleteObjectAsync(int objid);
+
     //! Deletes all objects of the remote server.
     void deleteAllObjects();
 
+    //! Deletes all objects of the remote server.
+    void deleteAllObjectsAsync();
+
     void clearMaps();
 
     //! Users generally won't have to call this, but rather one of the callFunction methods.
@@ -210,7 +244,9 @@
     inline void getResults(Arg1& arg1)
     {
         expectResults(1);
+        //DBG_LOG << "RemotePLearnServer getResults(Arg1& arg1)" << endl;
         io >> arg1;
+        //DBG_LOG << "RemotePLearnServer getResults got arg1" << endl;
     }
     //! get results for a method with 2 output results
     //! These are the results for the just previously called method

Modified: trunk/plearn/sys/Popen.cc
===================================================================
--- trunk/plearn/sys/Popen.cc	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/plearn/sys/Popen.cc	2007-02-22 21:05:50 UTC (rev 6676)
@@ -54,7 +54,7 @@
 namespace PLearn {
 using namespace std;
 
-void Popen::launch(const string& program, const vector<string>& arguments)
+void Popen::launch(const string& program, const vector<string>& arguments, bool redirect_stderr)
 {
     // Create pipes to communicate to/from child process.
     PRFileDesc* stdout_child;
@@ -73,10 +73,24 @@
                 getPrErrorString().c_str());
     }
 
+    PRFileDesc* stderr_child;
+    PRFileDesc* stderr_parent;
+
+    if(redirect_stderr)
+        if (PR_CreatePipe(&stderr_child, &stderr_parent) != PR_SUCCESS) 
+        {
+            PR_Close(stderr_child);
+            PR_Close(stderr_parent);
+            PLERROR("Popen: error creating third (err) pipe pair. (%s)",
+                    getPrErrorString().c_str());
+        }
+
     // Set up redirection of stdin/stdout for the (future) child process.
     PRProcessAttr* process_attr = PR_NewProcessAttr();
     PR_ProcessAttrSetStdioRedirect(process_attr, PR_StandardInput, stdin_child);
     PR_ProcessAttrSetStdioRedirect(process_attr, PR_StandardOutput, stdout_child);
+    if(redirect_stderr)
+        PR_ProcessAttrSetStdioRedirect(process_attr, PR_StandardError, stderr_child);
 
     // Set up argument list for the CreateProcess call. args[0] shoud be the
     // name of the program. args[1]...arg[n] hold the actual arguments,
@@ -117,11 +131,15 @@
     // Important: close unused files in the parent.
     PR_Close(stdin_child);
     PR_Close(stdout_child);
+    if(redirect_stderr)
+        PR_Close(stderr_child);
   
     delete[] args;                        
     if (!process) {
         PR_Close(stdin_parent);
         PR_Close(stdout_parent);
+        if(redirect_stderr)
+            PR_Close(stderr_parent);
         PLERROR("Popen: could not create subprocess for command '%s'. (%s)",
                 program.c_str(), getPrErrorString().c_str());
     }
@@ -130,10 +148,16 @@
     in = new PrPStreamBuf(stdout_parent, stdin_parent);
     in.setBufferCapacities(0, 0, 0);
     out = in;
+
+    if(redirect_stderr)
+    {    
+        err= new PrPStreamBuf(stderr_parent);
+        err.setBufferCapacities(0, 0, 0);
+    }
 }
 
 
-void Popen::launch(const string& commandline)
+void Popen::launch(const string& commandline, bool redirect_stderr)
 {
     // Parse command line into individual argments
     PStream s = openString(string("[") + commandline + "]",
@@ -144,7 +168,7 @@
     const string command = command_and_args[0];
     const vector<string> args(command_and_args.begin()+1,
                               command_and_args.end());
-    launch(command, args);
+    launch(command, args, redirect_stderr);
 }
 
 
@@ -172,9 +196,9 @@
 }
 
   
-vector<string> execute(const string& command)
+vector<string> execute(const string& command, bool redirect_stderr)
 {
-    Popen p(command);
+    Popen p(command, redirect_stderr);
     vector<string> result;
     while(p.in)
     {

Modified: trunk/plearn/sys/Popen.h
===================================================================
--- trunk/plearn/sys/Popen.h	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/plearn/sys/Popen.h	2007-02-22 21:05:50 UTC (rev 6676)
@@ -61,10 +61,10 @@
 protected:
     //! Multi-argument variant: the arguments are passed in a vector.
     void launch(const string& command, const vector<string>&
-                commandoptions);
+                commandoptions, bool redirect_stderr= false);
     //! Full text variant. All arguments are passed together in a string.
     //! @deprecated Use the other version of launch instead.
-    void launch(const string& commandline); 
+    void launch(const string& commandline, bool redirect_stderr= false); 
 
     bool verbose;
     bool process_alive;
@@ -74,14 +74,15 @@
 public:
     PStream in; //should these be only one I/O PStream? -xsm
     PStream out;
+    PStream err;
     
     Popen(const string& command, 
-	  bool the_verbose = false) 
-    { verbose = the_verbose; launch(command); }
+	  bool the_verbose = false, bool redirect_stderr= false) 
+    { verbose = the_verbose; launch(command, redirect_stderr); }
 
     Popen(const string& command, const vector<string>& commandoptions, 
-	  bool the_verbose = false) 
-    { verbose = the_verbose; launch(command,commandoptions); }
+	  bool the_verbose = false, bool redirect_stderr= false) 
+    { verbose = the_verbose; launch(command,commandoptions, redirect_stderr); }
 
     /** Wait for process termination and return exit value.
         @note This must be called after all output from the program
@@ -102,7 +103,7 @@
   The command must not be waiting for input on its standard input 
   or this call will never return.
 */
-vector<string> execute(const string& command);
+vector<string> execute(const string& command, bool redirect_stderr= false);
 
 } // end of namespace PLearn
 

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/plearn/vmat/VMatrix.cc	2007-02-22 21:05:50 UTC (rev 6676)
@@ -200,6 +200,25 @@
          ArgDoc ("extra_percent", "if non 0, then the box is enlarged in both ends\n"
                  "of every direction by that given percentage"),
          RetDoc ("bounding box as as a vector of (min,max) pairs")));         
+
+    declareMethod(
+        rmm, "fill", &VMatrix::fill,
+        (BodyDoc("Appends fills the VMatrix with a constant value.\n"),
+         ArgDoc ("value", "The fill value.\n")));
+
+    declareMethod(
+        rmm, "dot", &VMatrix::dot,
+        (BodyDoc("dot product between row i1 and row i2, w/ inputsize first elements."),
+         ArgDoc ("i1", "First row to consider."),
+         ArgDoc ("i2", "Second row to consider."),
+         ArgDoc ("inputsize", "nb. elements to consider."),
+         RetDoc ("dot product")));         
+
+    declareMethod(
+        rmm, "savePMAT", &VMatrix::savePMAT,
+        (BodyDoc("Saves this matrix as a .pmat file."),
+         ArgDoc ("pmatfile", "Path of the file to create.")));
+
 }
 
 

Modified: trunk/plearn_learners/generic/NNet.cc
===================================================================
--- trunk/plearn_learners/generic/NNet.cc	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/plearn_learners/generic/NNet.cc	2007-02-22 21:05:50 UTC (rev 6676)
@@ -73,7 +73,7 @@
 #include <plearn/var/FNetLayerVariable.h>
 
 #include <plearn/vmat/ConcatColumnsVMatrix.h>
-//#include "DisplayUtils.h"
+//#include <plearn/display/DisplayUtils.h>
 //#include "GradientOptimizer.h"
 #include "NNet.h"
 // #include <plearn/math/random.h>

Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/plearn_learners/testers/PTester.cc	2007-02-22 21:05:50 UTC (rev 6676)
@@ -46,6 +46,8 @@
 #include <assert.h>
 #include "PTester.h"
 
+#include <plearn/misc/PLearnService.h>
+
 #include <plearn/base/stringutils.h>
 #if USING_MPI
 #include <plearn/sys/PLMPI.h>
@@ -282,6 +284,12 @@
          RetDoc ("Vector of test statistics corresponding to the requested statnames")));
 
     declareMethod(
+        rmm, "perform1Split", &PTester::perform1Split,
+        (BodyDoc("Performs train/test for one split, returns splitres."),
+         ArgDoc ("splitnum","Split number on which to perform train/test"),
+         RetDoc ("Vector of test statistics corresponding to the requested statnames")));
+
+    declareMethod(
         rmm, "getStatNames", &PTester::getStatNames,
         (BodyDoc("Return the statnames (potentially modified by statmask, if provided);\n"
                  "see the 'statnames' and 'statmask' options."),
@@ -405,7 +413,8 @@
 /////////////
 // perform //
 /////////////
-Vec PTester::perform(bool call_forget)
+// DEPRECATED -- USE PTester::perform
+Vec PTester::oldperform(bool call_forget)
 {
     if (!learner)
         PLERROR("No learner specified for PTester.");
@@ -733,6 +742,397 @@
     return global_result;
 }
 
+
+Vec PTester::perform1Split(int splitnum)
+{
+    if (!learner)
+        PLERROR("PTester::perform1Split : No learner specified for PTester.");
+    if (!splitter)
+        PLERROR("PTester::perform1Split : No splitter specified for PTester");
+
+    const int nstats = statnames_processed.length();
+
+    splitter->setDataSet(dataset);
+
+    TVec<string> testcostnames = learner->getTestCostNames();
+    TVec<string> traincostnames = learner->getTrainCostNames();
+
+    const int nsets = splitter->nSetsPerSplit();
+
+    // Stats collectors for individual sets of a split:
+    TVec< PP<VecStatsCollector> > stcol(nsets);
+    for (int setnum = 0; setnum < nsets; setnum++)
+    {
+        if (template_stats_collector)
+        {
+            CopiesMap copies;
+            stcol[setnum] = template_stats_collector->deepCopy(copies);
+        }
+        else
+            stcol[setnum] = new VecStatsCollector();
+
+        if (setnum == 0)
+            stcol[setnum]->setFieldNames(traincostnames);
+        else
+            stcol[setnum]->setFieldNames(testcostnames);
+
+        stcol[setnum]->build();
+        stcol[setnum]->forget();
+    }
+
+    PP<VecStatsCollector> train_stats = stcol[0];
+    learner->setTrainStatsCollector(train_stats);
+
+
+    // Stat specs
+    TVec<StatSpec> statspecs(nstats);
+    for(int k = 0; k < nstats; k++)
+    {
+        statspecs[k].init(statnames_processed[k]);
+    }
+
+    PPath splitdir;
+    bool is_splitdir = false;
+    if (!expdir.isEmpty())
+    {
+        splitdir = expdir / ("Split" + tostring(splitnum));
+        is_splitdir = true;
+    }
+
+    TVec<VMat> dsets = splitter->getSplit(splitnum);
+
+    if (should_train) {
+        VMat trainset = dsets[0];
+        if (is_splitdir && save_data_sets)
+            PLearn::save(splitdir / "training_set.psave", trainset);
+            
+        if (provide_learner_expdir)
+        {
+            if (is_splitdir)
+                learner->setExperimentDirectory(splitdir / "LearnerExpdir/");
+            else
+                learner->setExperimentDirectory("");
+        }
+
+        learner->setTrainingSet(trainset, should_train);
+        if (dsets.size() > 1)
+            learner->setValidationSet(dsets[1]);
+
+        if (is_splitdir && save_initial_learners)
+            PLearn::save(splitdir / "initial_learner.psave", learner);
+
+        train_stats->forget();
+        learner->train();
+        train_stats->finalize();
+
+        if (is_splitdir)
+        {
+            if (save_stat_collectors)
+                PLearn::save(splitdir / "train_stats.psave", train_stats);
+            if (save_learners)
+                PLearn::save(splitdir / "final_learner.psave", learner);
+        }
+    }
+    else
+        learner->build();
+
+    // This needs to be after the SetTrainingSet() / build() call to the
+    // learner.
+    const int outputsize = learner->outputsize();
+
+    // perf_eval_costs[setnum][perf_evaluator_name][costname] will contain value
+    // of the given cost returned by the given perf_evaluator on the given setnum
+    TVec< map<string, map<string, real> > > perf_eval_costs(dsets.length());
+
+    // Perform the test if required
+    if (should_test)
+    {
+        for (int setnum = 1; setnum < dsets.length(); setnum++)
+        {
+            VMat testset = dsets[setnum];
+            VMat test_outputs;
+            VMat test_costs;
+            VMat test_confidence;
+
+            PP<VecStatsCollector> test_stats = stcol[setnum];
+            const string setname = "test" + tostring(setnum);
+            if (is_splitdir && save_data_sets)
+                PLearn::save(splitdir / (setname + "_set.psave"), testset);
+
+            // QUESTION Why is this done so late? Can't it be moved
+            // somewhere earlier? At least before the save_data_sets?
+            if (is_splitdir)
+                force_mkdir(splitdir);
+
+            if (is_splitdir && save_test_outputs)
+                test_outputs = new FileVMatrix(splitdir / (setname + "_outputs.pmat"),
+                                               0, learner->getOutputNames());
+            else if (!perf_evaluators.empty())
+            {
+                // We don't want to save test outputs to disk, but we
+                // need them for pef_evaluators. So let's store them in
+                // a MemoryVMatrix
+                Mat data(testset.length(), outputsize);
+                data.resize(0, outputsize);
+                test_outputs = new MemoryVMatrix(data);
+                test_outputs->declareFieldNames(learner->getOutputNames());
+            }
+
+            if (is_splitdir)
+            {
+                if (save_test_costs)
+                    test_costs = new FileVMatrix(splitdir / (setname + "_costs.pmat"),
+                                                 0, learner->getTestCostNames());
+                if (save_test_confidence)
+                    test_confidence = new FileVMatrix(splitdir / (setname + "_confidence.pmat"),
+                                                      0, 2 * outputsize);
+            }
+
+            test_stats->forget();
+                    
+            if (testset->length() == 0)
+                PLWARNING("PTester:: test set %s is of length 0, costs will be set to -1",
+                          setname.c_str());
+
+            // Before each test set, reset the internal state of the learner
+            learner->resetInternalState();
+
+            learner->test(testset, test_stats, test_outputs, test_costs);
+            //if (reset_stats)
+            test_stats->finalize();
+            if (is_splitdir && save_stat_collectors)
+                PLearn::save(splitdir / (setname + "_stats.psave"), test_stats);
+
+            perf_evaluators_t::iterator it = perf_evaluators.begin();
+            const perf_evaluators_t::iterator itend = perf_evaluators.end();
+            while (it != itend)
+            {
+                PPath perf_eval_dir;
+                if (is_splitdir)
+                    perf_eval_dir = splitdir / setname / ("perfeval_" + it->first);
+                Vec perf_costvals = it->second->evaluatePerformance(learner, testset, test_outputs, perf_eval_dir);
+                TVec<string> perf_costnames = it->second->getCostNames();
+                if (perf_costvals.length()!=perf_costnames.length())
+                    PLERROR("vector of costs returned by performance evaluator differ in size with its vector of costnames");
+                map<string, real>& costmap = perf_eval_costs[setnum][it->first];
+                for (int costi = 0; costi < perf_costnames.length(); costi++)
+                    costmap[perf_costnames[costi]] = perf_costvals[costi];
+                ++it;
+            }
+            computeConfidence(testset, test_confidence);
+        }
+    }
+
+    Vec splitres(1 + nstats);
+    splitres[0] = splitnum;
+
+    for (int k = 0; k < nstats; k++)
+    {
+        // If we ask for a test-set that's beyond what's currently
+        // available, OR we are asking for test-statistics in
+        // train-only mode, then the statistic is MISSING_VALUE.
+        StatSpec& sp = statspecs[k];
+        if (sp.setnum>=stcol.length() ||
+            (! should_test && sp.setnum > 0))
+        {
+            splitres[k+1] = MISSING_VALUE;
+        }
+        else
+        {
+            string left, right;
+            split_on_first(sp.intstatname, ".",left,right);
+            if (right != "" && perf_evaluators.find(left) != perf_evaluators.end())
+            {
+                // looks like a cost from a performance evaluator
+                map<string, real>& costmap = perf_eval_costs[sp.setnum][left];
+                if (costmap.find(right) == costmap.end())
+                    PLERROR("No cost named %s appears to be returned by evaluator %s",
+                            right.c_str(), left.c_str());
+                splitres[k+1] = costmap[right];
+            }
+            else
+                // must be a cost from a stats collector
+                splitres[k+1] = stcol[sp.setnum]->getStat(sp.intstatname);
+        }
+    }
+
+    return splitres;
+}
+
+
+Vec PTester::perform(bool call_forget)
+{
+    if (!learner)
+        PLERROR("No learner specified for PTester.");
+    if (!splitter)
+        PLERROR("No splitter specified for PTester");
+
+    const int nstats = statnames_processed.length();
+    Vec global_result(nstats);
+
+    if (expdir != "")
+    {
+        if (pathexists(expdir) && enforce_clean_expdir)
+            PLERROR("Directory (or file) %s already exists.\n"
+                    "First move it out of the way.", expdir.c_str());
+        if (!force_mkdir(expdir))
+            PLERROR("In PTester Could not create experiment directory %s",expdir.c_str());
+        expdir = expdir.absolute() / "";
+
+        // Save this tester description in the expdir
+        if (save_initial_tester)
+            PLearn::save(expdir / "tester.psave", *this);
+    }
+
+    const int nsplits = splitter->nsplits();
+    if (nsplits > 1)
+        call_forget = true;
+
+    TVec<string> testcostnames = learner->getTestCostNames();
+    TVec<string> traincostnames = learner->getTrainCostNames();
+
+    // Global stats collector
+    PP<VecStatsCollector> global_statscol;
+    if (global_template_stats_collector)
+    {
+        CopiesMap copies;
+        global_statscol = global_template_stats_collector->deepCopy(copies);
+        global_statscol->build();
+        global_statscol->forget();
+    }
+    else
+        global_statscol = new VecStatsCollector();
+
+    // Stat specs
+    TVec<StatSpec> statspecs(nstats);
+    for(int k = 0; k < nstats; k++)
+    {
+        statspecs[k].init(statnames_processed[k]);
+    }
+
+    //no ACC stats for parallel perform
+    for (int k = 0; k < nstats; k++)
+        if (statspecs[k].extstat == "ACC")
+            PLERROR("ACC stats not supported anymore; please adapt PTester::perform to your needs.");
+
+
+    // The vmat in which to save global result stats specified in statnames
+    VMat global_stats_vm;
+    // The vmat in which to save per split result stats
+    VMat split_stats_vm;
+        
+    if (expdir != "" && report_stats)
+    {
+        saveStringInFile(expdir / "train_cost_names.txt", join(traincostnames, "\n") + "\n");
+        saveStringInFile(expdir / "test_cost_names.txt", join(testcostnames, "\n") + "\n");
+
+        global_stats_vm = new FileVMatrix(expdir / "global_stats.pmat",
+                                          1, nstats);
+        for (int k = 0; k < nstats; k++)
+            global_stats_vm->declareField(k, statspecs[k].statName());
+        global_stats_vm->saveFieldInfos();
+
+        split_stats_vm = new FileVMatrix(expdir / "split_stats.pmat",
+                                         0, 1 + nstats);
+        split_stats_vm->declareField(0, "splitnum");
+        for (int k = 0; k < nstats; k++)
+            split_stats_vm->declareField(k+1, statspecs[k].setname + "." + statspecs[k].intstatname);
+        split_stats_vm->saveFieldInfos();
+    }
+
+
+    PLearnService& service(PLearnService::instance());
+    TVec<PP<RemotePLearnServer> > servers= service.reserveServers(nsplits);
+    int nservers= servers.length();
+
+    if(nservers > 1)
+    {
+        map<PP<RemotePLearnServer>, int> testers_ids;
+        for (int splitnum= 0; splitnum < nservers && splitnum < nsplits; ++splitnum)
+            servers[splitnum]->newObjectAsync(*this);
+
+        int splits_called= 0;
+        //int testers_created= nservers;
+        for (int splits_done= 0; nservers > 0;)//splits_done < nsplits;)
+        {
+            PP<RemotePLearnServer> s= service.waitForResult();
+            if(testers_ids.find(s) == testers_ids.end())
+            {
+                if(splits_called < nsplits)
+                {
+                    int id;
+                    s->getResults(id);
+                    testers_ids[s]= id;
+                    s->callMethod(id, "perform1Split", splits_called);
+                    ++splits_called;
+                }
+                else
+                {
+                    s->getResults(); // tester deleted
+                    service.freeServer(s);
+                    --nservers;
+                }
+            }
+            else // get split result
+            {
+                Vec splitres;
+                s->getResults(splitres);
+                ++splits_done;
+                if (split_stats_vm)
+                {
+                    split_stats_vm->appendRow(splitres);
+                    split_stats_vm->flush();
+                }
+            
+                global_statscol->update(splitres.subVec(1, nstats));
+
+                if(splits_called < nsplits)//call for another split
+                {
+                    s->callMethod(testers_ids[s], "perform1Split", splits_called);
+                    ++splits_called;
+                }
+                else
+                {
+                    s->deleteObjectAsync(testers_ids[s]);
+                    testers_ids.erase(s);
+                }
+            }
+        }
+    }
+    else
+        for (int splitnum= 0; splitnum < nsplits; ++splitnum)
+        {
+            Vec splitres= perform1Split(splitnum);
+            
+            if (split_stats_vm)
+            {
+                split_stats_vm->appendRow(splitres);
+                split_stats_vm->flush();
+            }
+            
+            global_statscol->update(splitres.subVec(1, nstats));
+        }
+
+
+    global_statscol->finalize();
+    for (int k = 0; k < nstats; k++)
+        global_result[k] = global_statscol->getStats(k).getStat(statspecs[k].extstat);
+
+    if (global_stats_vm)
+        global_stats_vm->appendRow(global_result);
+
+#if USING_MPI
+    if (PLMPI::rank == 0)
+#endif
+    // Perform the final commands provided in final_commands.
+    for (int i = 0; i < final_commands.length(); i++)
+    {
+        system(final_commands[i].c_str());
+    }
+
+    return global_result;
+}
+
 void PTester::computeConfidence(VMat test_set, VMat confidence)
 {
     PLASSERT(learner);

Modified: trunk/plearn_learners/testers/PTester.h
===================================================================
--- trunk/plearn_learners/testers/PTester.h	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/plearn_learners/testers/PTester.h	2007-02-22 21:05:50 UTC (rev 6676)
@@ -189,7 +189,10 @@
      *  statnames
      */
     Vec perform(bool call_forget=true);
+    Vec perform1Split(int splitnum);
 
+    Vec oldperform(bool call_forget=true);
+
     //! Transforms a shallow copy into a deep copy
     virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 };

Modified: trunk/python_modules/plearn/io/server.py
===================================================================
--- trunk/python_modules/plearn/io/server.py	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/python_modules/plearn/io/server.py	2007-02-22 21:05:50 UTC (rev 6676)
@@ -36,6 +36,8 @@
 from threading import Timer
 from thread    import interrupt_main
 
+from plearn.utilities.progress import LineOutputProgressBar
+
 from plearn.pyplearn import *
 import plearn.io.serialize
 
@@ -58,6 +60,20 @@
     s.connect((hostname, port))
     io = s.makefile()
     return RemotePLearnServer(io, io, logger=logger)
+
+
+def default_log_callback(module, vlevel, message):
+    print ("SERVER-LOG: [" + module +"] "+ str(vlevel) + ' ' + message)
+    
+def default_pbar_callback(pbars, cmd, ptr, pos, title):
+    if cmd=='A':
+        pbars[ptr]= LineOutputProgressBar(title, pos)
+    else:
+        pbars[ptr].update(pos)
+        if cmd == 'K':
+            del(pbars[ptr])
+
+
         
 class RemotePLearnServer:
 
@@ -96,6 +112,7 @@
         self.clear_maps = True
         self.dbg_dump = False
         self.closed = False
+        self.pbars= {}
 
         ## Ensure that the server is responding. Otherwise raise an error
         if not self.isAlive():
@@ -264,7 +281,39 @@
         self.clearMaps()
         self.logged_write('!M '+str(objid)+' '+methodname+' '+str(nargs)+' ')
 
+
+
+    def waitForResult(self, log_callback= default_log_callback, pbar_callback= default_pbar_callback):
+        while True:
+            self.io.skip_blanks_and_comments()
+            c = self.io.peek()
+            if c=='*': #log or progress message
+                c = self.io.get()#get '*'
+                c = self.io.get()
+                if c == 'L': # Log message
+                    module= self.io.read_string()
+                    vlevel= self.io.read_int()
+                    message= self.io.read_string()
+                    log_callback(module, vlevel, message)
+                elif c == 'P': # Progress message
+                    command= self.io.get()
+                    ptr= self.io.read_int()
+                    pos= 0
+                    title= ''
+                    if command != 'K':
+                        pos= self.io.read_int()
+                    if command == 'A':
+                        title= self.io.read_string()
+                        title= '(pb#'+str(ptr)+') '+title
+                    pbar_callback(self.pbars, command, ptr, pos, title)
+                else:
+                    raise TypeError("Expected *L or *P, but read *"+c)
+            else:
+                return
+        
+
     def getResultsCount(self):
+        self.waitForResult()
         self.io.skip_blanks_and_comments()
         c = self.io.get()
         if c!='!':
@@ -311,7 +360,7 @@
 
     def callMethod(self, objid, methodname, *args):
         self.sendMethodCallHeader(objid, methodname, len(args))
-        # print 'sending ARGS', args
+        #print 'sending ARGS', args
         self.logged_write_args(args)
 
         if self.dbg_dump:

Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-02-22 21:05:50 UTC (rev 6676)
@@ -10,6 +10,8 @@
 from textwrap import dedent
 import pdb
 from time import sleep
+from plearn.pymake import pymake
+
 STATUS_FINISHED = 0
 STATUS_RUNNING = 1
 STATUS_WAITING = 2
@@ -129,6 +131,26 @@
         status = get_config_value(self.log_file,'STATUS')
         return int(status)
 
+    def get_stdout(self):
+        try:
+            if isinstance(self.p.stdout, file):
+                return self.p.stdout
+            else:
+                return open(self.log_file + '.out','r')
+        except:
+            pass
+        return None
+        
+    def get_stderr(self):
+        try:
+            if isinstance(self.p.stderr, file):
+                return self.p.stderr
+            else:
+                return open(self.log_file + '.err','r')
+        except:
+            pass
+        return None
+
     def get_waiting_time(self):
         # get the string representation
         str_sched = get_config_value(self.log_file,'SCHEDULED_TIME')
@@ -399,16 +421,144 @@
 
 
 
-def clean(self):
+    def clean(self):
         pass
 
+
+
+class SshHost:
+    def __init__(self, hostname):
+        self.hostname= hostname
+        self.lastupd= -16
+        self.getAvailability()
+        
+    def getAvailability(self):
+        # simple heuristic: mips / load
+        t= time.time()
+        if t - self.lastupd > 15: # min. 15 sec. before update
+            self.bogomips= self.getBogomips()
+            self.loadavg= self.getLoadavg()
+            self.lastupd= t
+            #print  self.hostname, self.bogomips, self.loadavg, (self.bogomips / (self.loadavg + 0.5))
+        return self.bogomips / (self.loadavg + 0.5)
+        
+    def getBogomips(self):
+        cmd= ["ssh", self.hostname ,"cat /proc/cpuinfo"]
+        p= Popen(cmd, stdout=PIPE)
+        bogomips= 0.0
+        for l in p.stdout:
+            if l.startswith('bogomips'):
+                s= l.split(' ')
+                bogomips+= float(s[-1])
+        return bogomips
+
+    def getLoadavg(self):
+        cmd= ["ssh", self.hostname,"cat /proc/loadavg"]
+        p= Popen(cmd, stdout=PIPE)
+        l= p.stdout.readline().split(' ')
+        return float(l[0])
+        
+    def addToLoadavg(self,n):
+        self.loadavg+= n
+        self.lastupd= time.time()
+
+    def __str__(self):
+        return "SshHost("+self.hostname+" <"+str(self.bogomips) \
+               +','+str(self.loadavg) +','+str(self.getAvailability()) \
+               +','+str(self.lastupd) + '>)'
+
+    def __repr__(self):
+        return str(self)
+        
+def find_all_ssh_hosts():
+    return [SshHost(h) for h in set(pymake.get_distcc_hosts())]
+
+def cmp_ssh_hosts(h1, h2):
+    return cmp(h2.getAvailability(), h1.getAvailability())
+
+class DBISsh(DBIBase):
+
+    def __init__(self, commands, **args ):
+        DBIBase.__init__(self, commands, **args)
+
+        # check if log directory exists, if not create it
+        if not os.path.exists(self.log_dir):
+            os.mkdir(self.log_dir)
+
+        # create the information about the tasks
+        for command in commands:
+            self.tasks.append(Task(command, self.log_dir, self.time_format,
+                                   self.pre_tasks, self.post_tasks))
+        self.hosts= find_all_ssh_hosts()
+        
+
+    def getHost(self):
+        self.hosts.sort(cmp= cmp_ssh_hosts)
+        #print "hosts= "
+        #for h in self.hosts: print h
+        self.hosts[0].addToLoadavg(1.0)
+        return self.hosts[0]
+    
+    def run_one_job(self, task):
+        DBIBase.run(self)
+
+        host= self.getHost()
+
+
+        cwd= os.getcwd()
+        command = "ssh " + host.hostname + " 'cd " + cwd + "; " + string.join(task.commands,';') + "'"
+        print command
+
+        task.launch_time = time.time()
+        set_config_value(task.log_file, 'SCHEDULED_TIME',
+                time.strftime(self.time_format, time.localtime(time.time())))
+        output = PIPE
+        error = PIPE
+        if int(self.file_redirect_stdout):
+            output = file(task.log_file + '.out','w')
+        if int(self.file_redirect_stderr):
+            error = file(task.log_file + '.err','w')
+        task.p = Popen(command, shell=True,stdout=output,stderr=error)
+
+    def run(self):
+        # Execute pre-batch
+        pre_batch_command = ';'.join( self.pre_batch )
+        output = PIPE
+        error = PIPE
+        if int(self.file_redirect_stdout):
+            output = file(self.log_file + '.pre_batch.out', 'w')
+        if int(self.file_redirect_stderr):
+            error = file(self.log_file + '.pre_batch.err', 'w')
+        self.pre = Popen(pre_batch_command, shell=True, stdout=output, stderr=error)
+        print 'pre_batch_command =', pre_batch_command
+
+        # Execute all Tasks (including pre_tasks and post_tasks if any)
+        print "tasks= ", self.tasks
+        for task in self.tasks:
+            self.run_one_job(task)
+
+        # Execute post-batchs
+        post_batch_command = ";".join( self.post_batch );
+        if int(self.file_redirect_stdout):
+            output = file(self.log_file + '.post_batch.out', 'w')
+        if int(self.file_redirect_stderr):
+            error = file(self.log_file + '.post_batch.err', 'w')
+        self.post = Popen(post_batch_command, shell=True, stdout=output, stderr=error)
+        print 'post_batch_command =', post_batch_command
+
+    def clean(self):
+        #TODO: delete all log files for the current batch
+        pass
+
+
+
 # creates an object of type ('DBI' + launch_system) if it exists
 def DBI(commands, launch_system):
     try:
         str = 'DBI'+launch_system+'(commands)'
         jobs = eval('DBI'+launch_system+'(commands)')
     except NameError:
-        print 'The launch system ',launch_system, ' does not exists. Available systems are: Cluster, bqtools and Condor'
+        print 'The launch system ',launch_system, ' does not exists. Available systems are: Cluster, Ssh, bqtools and Condor'
         sys.exit(1)
     return jobs
 

Modified: trunk/python_modules/plearn/parallel/dispatch.py
===================================================================
--- trunk/python_modules/plearn/parallel/dispatch.py	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/python_modules/plearn/parallel/dispatch.py	2007-02-22 21:05:50 UTC (rev 6676)
@@ -15,7 +15,7 @@
     "Task",
     
     # Functions
-    "get_ssh_machines", "launch_task", "set_logdir",
+    "get_ssh_machines", "launch_task", "launch_server", "set_logdir",
 
     # Classes
     "ArgumentsOracle", "Dispatch"
@@ -91,9 +91,14 @@
 
 def launch_task(argv, wait=False):
     assert Task is not None
-    task = Task( argv )
+    task = Task( argv)
     task.launch( wait )
 
+def launch_server(argv):
+    assert Task is not None
+    task = Task( argv)
+    task.launchServer()
+
 def set_logdir(logdir):
     """Instead of writing to stdout, tasks will be logged in a file within I{logdir}."""
     global LOGDIR
@@ -122,6 +127,7 @@
 
 class TaskType:
     _child_processes = {}
+    _launched_servers_info= []
 
     def count( cls ):
         """Return the number of uncompleted tasks."""
@@ -129,6 +135,7 @@
     count = classmethod( count )
 
     def kill_all_tasks( cls ):
+        cls.kill_launched_servers()
         task_list = cls._child_processes.values()
         for task in task_list:
             try:
@@ -143,6 +150,13 @@
         assert not cls._child_processes
     kill_all_tasks = classmethod(kill_all_tasks)
 
+    def kill_launched_servers( cls ):
+        for (host, port, pid) in cls._launched_servers_info:
+            cmd= "ssh %s kill %d"%(host, pid)
+            os.system(cmd)
+            logging.info("Killed process %d on %s [%s]"%(pid, host, cmd))
+    kill_launched_servers= classmethod(kill_launched_servers)
+
     def availableMachinesCount( cls ):
         """Returns the number of machines currently available for cluster job dispatch."""
         avail = 0
@@ -151,8 +165,34 @@
         return avail
     availableMachinesCount = classmethod(availableMachinesCount)
 
+    def getLaunchedServersInfo( cls ):
+        """Returns a list of triples (machine, port, pid) 
+        e.g.: [('midgard', 41492, 2677),('odin', 48972, 1830)]
+        """
+        return cls._launched_servers_info
+    getLaunchedServersInfo= classmethod(getLaunchedServersInfo)
+
+    def getLaunchedServerInfo( cls, child_fd ):
+        """Reads machine name, port and pid for a launched PLearn server.
+        returns a triple (machine, port, pid)
+        e.g.: ('midgard', 41492, 2677)
+        """
+        got_info= False
+        while not got_info:
+            s= child_fd.readline()
+            if s=='':
+                print "Cannot get info from server!"
+                sys.exit()
+            ss= s.split(' ')
+            if ss[0] == "PLEARN_SERVER_TCP":
+                info= (ss[1], int(ss[2]), int(ss[3]))
+                got_info= True
+        cls._launched_servers_info+= [info]
+        return info
+    getLaunchedServerInfo= classmethod(getLaunchedServerInfo)
+
     def select( cls ):
-        """Finds, frees and returns ompleted tasks."""
+        """Finds, frees and returns completed tasks."""
         if cls.count() == 0:
             logging.debug("* Raising EmptyTaskListError")
             raise EmptyTaskListError()
@@ -209,6 +249,11 @@
     def __init__(self, argv):
         self.argv = argv
         
+    def launchServer(self):
+        """Launch a PLearn server"""
+        self.launch(wait= False)
+        Task.getLaunchedServerInfo(self.process.fromchild)
+
     def launch(self, wait=False):
         """Launch process on an available machine"""
         try:
@@ -223,13 +268,14 @@
                 self.logfile.write( task_signature )
 
             logging.info(task_signature)            
+            self._child_processes[ self.process.fromchild ] = self
             if wait:
                 logging.debug(
                     "* TaskType.launch() waits for process (id=%d)"%self.process.pid )
                 self.process.wait( )
                 self.free()
             else:
-                self._child_processes[ self.process.fromchild ] = self
+                #self._child_processes[ self.process.fromchild ] = self
                 logging.debug( "* children %d (%d)"
                                %(self.process.pid,len(self._child_processes)) )
                 
@@ -245,6 +291,7 @@
     def free(self):
         if hasattr(self, 'process'):
             logging.debug("* Freeing task with pid=%d"%self.process.pid)
+
             Self = self.__class__._child_processes.pop(self.process.fromchild)
             assert Self == self
         
@@ -259,6 +306,7 @@
     _machines = get_ssh_machines()
     _loadavg  = {}
     _available_machines = None
+    _max_load= 1.0
     
     def getLoadAvg(cls, machine):
         #print "\nQuery to", machine
@@ -282,7 +330,7 @@
     def listAvailableMachines(cls):
         for m in cls._machines:
             loadavg = cls.getLoadAvg(m)
-            max_loadavg = MAX_LOADAVG.get(m, 1.0)
+            max_loadavg = MAX_LOADAVG.get(m, cls._max_load)
             #print "Load %f / %f"%(loadavg, max_loadavg)
             if loadavg < max_loadavg:
                 # Register the load average *plus* one, taking in account

Modified: trunk/python_modules/plearn/parallel/utils.py
===================================================================
--- trunk/python_modules/plearn/parallel/utils.py	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/python_modules/plearn/parallel/utils.py	2007-02-22 21:05:50 UTC (rev 6676)
@@ -37,7 +37,7 @@
     if len(s) < length:
         return s
     else:
-        return s[:length] + etc
+        return s[:length] #+ etc
     
 def string_replace(s,c,ch=''):
     """Remove any occurrences of characters in c, from string s

Modified: trunk/python_modules/plearn/utilities/progress.py
===================================================================
--- trunk/python_modules/plearn/utilities/progress.py	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/python_modules/plearn/utilities/progress.py	2007-02-22 21:05:50 UTC (rev 6676)
@@ -50,5 +50,29 @@
             self.pos = pos
             
 
+class LineOutputProgressBar(StdoutProgressBar):
+    
+    def __init__(self, title, n):
+        self.n = n
+        self.pos = 0
+        self.title = title
+        self.closed = False
+        titlestr = ' '+title+' ('+str(n)+') '
+        StdoutProgressBar.write('In progress: '+titlestr+'\n')
+            
+    def update(self, pos):
+        if not self.closed:
+            npoints = StdoutProgressBar.npoints
+            oldcharpos = min(npoints, int(self.pos*npoints/(self.n-1)))
+            newcharpos = min(npoints, int(pos*npoints/(self.n-1)))
+            nchars = newcharpos-oldcharpos
+            if nchars>0:
+                StdoutProgressBar.write(self.title + ': ' + str(pos) + '/' + str(self.n)
+                                        + '(' + str(float(pos)*100./float(self.n)) +'%)\n')
+            if pos>=self.n-1:
+                StdoutProgressBar.write('Finished '+self.title+': '+str(pos)+'/'+str(self.n)+' (100%)\n')
+                self.closed = True
+            self.pos = pos
+
 PBar = StdoutProgressBar
 

Added: trunk/scripts/xdispatch
===================================================================
--- trunk/scripts/xdispatch	2007-02-21 15:49:27 UTC (rev 6675)
+++ trunk/scripts/xdispatch	2007-02-22 21:05:50 UTC (rev 6676)
@@ -0,0 +1,80 @@
+#!/usr/bin/python2.3
+#
+# xdispatch: dispatch utility for parallel PLearn
+#
+# Copyright 2007, Apstat Technologies, inc.
+# All rights reserved.
+
+from optparse import OptionParser
+from plearn.parallel.dispatch import *
+import tempfile
+import os
+import time
+
+#
+# Main
+#
+
+def main():
+    parser = OptionParser()
+    parser.add_option('-n', '--num-available-machines',
+                      help='Prints the number of available machines',
+                      action='store_true', default=False)
+
+    parser.add_option('-l', '--max-load', type= "float",
+                      dest= "max_load",
+                      help='Sets the default maximum load',
+                      default=2.0)
+
+    parser.add_option('-s', '--nb-servers', type="int",
+                      dest= "nb_servers",
+                      help='Number of servers to launch (maximum)',
+                      default=1)
+
+    parser.add_option('-m', '--min-nb-servers', type="int",
+                      dest= "min-nb_servers",
+                      help='Number of servers to launch (minimum)',
+                      default=1)
+
+    
+    (options, args) = parser.parse_args()
+
+    #print options
+    #print "==="
+    #print args
+
+    set_logdir('/home/saintmlx/xdispatch-log/')
+
+    Task._max_load= options.max_load
+
+    #xdispatch -l 3.0 -s 25 inslearn exp.pyplearn
+    #25x -> ssh distant_machine inslearn server 0
+    #1x  -> inslearn --servers serv_file exp.pyplearn
+
+    if options.num_available_machines:
+        print Task.availableMachinesCount()
+    elif args:
+        try:
+            plearn_command= args[0]
+            for i in xrange(options.nb_servers):
+                #launch_server( ['time',plearn_command, '--verbosity 999', 'server', '0'] )
+                launch_server( ['time',plearn_command, 'server', '0'] )
+
+            tf, fname= tempfile.mkstemp(suffix= '.plserv', prefix= '.plserv_', dir= '/tmp/')
+
+            for si in Task.getLaunchedServersInfo():
+                print si
+                os.write(tf, "%s %d %d\n"%si)
+            os.close(tf)
+
+            print "Servers listed in: ", fname
+            cmd= plearn_command + ' --servers ' + fname + ' ' + ' '.join(args[1:])
+            print cmd
+
+            os.system(cmd)
+
+        finally:
+            Task.kill_all_tasks()
+
+if __name__ == '__main__':
+    main()


Property changes on: trunk/scripts/xdispatch
___________________________________________________________________
Name: svn:executable
   + *



From saintmlx at mail.berlios.de  Thu Feb 22 22:24:54 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 22 Feb 2007 22:24:54 +0100
Subject: [Plearn-commits] r6677 - trunk/plearn/misc
Message-ID: <200702222124.l1MLOsQG022923@sheep.berlios.de>

Author: saintmlx
Date: 2007-02-22 22:24:54 +0100 (Thu, 22 Feb 2007)
New Revision: 6677

Modified:
   trunk/plearn/misc/PLearnService.cc
Log:
- removed superfluous output to PLearnService



Modified: trunk/plearn/misc/PLearnService.cc
===================================================================
--- trunk/plearn/misc/PLearnService.cc	2007-02-22 21:05:50 UTC (rev 6676)
+++ trunk/plearn/misc/PLearnService.cc	2007-02-22 21:24:54 UTC (rev 6677)
@@ -371,12 +371,8 @@
     TVec<PP<RemotePLearnServer> > servers(available_servers.length());
     servers << available_servers;
 
-    perr << "servers to disconnect from: " << servers.length() << endl;
-
     disconnectFromServers();
 
-    perr << "start watching " << servers.length() << endl;
-
     //now, get what's remaining on the servers streams
     for(int i= 0; i < servers.length(); ++i)
     {
@@ -386,11 +382,9 @@
         }
         catch(const PLearnError& e)
         {
-            perr << "one dead " << i << endl;
+            // do nothing...
         }
     }
-
-    perr << "finished watching " << endl;
 }
 
 



From saintmlx at mail.berlios.de  Thu Feb 22 23:55:09 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 22 Feb 2007 23:55:09 +0100
Subject: [Plearn-commits] r6678 - in trunk: plearn/misc
	python_modules/plearn/pytest
Message-ID: <200702222255.l1MMt99c011406@sheep.berlios.de>

Author: saintmlx
Date: 2007-02-22 23:55:09 +0100 (Thu, 22 Feb 2007)
New Revision: 6678

Modified:
   trunk/plearn/misc/PLearnServer.cc
   trunk/python_modules/plearn/pytest/tests.py
Log:
- allow PLearnServer to run with NullProgressBarPlugin



Modified: trunk/plearn/misc/PLearnServer.cc
===================================================================
--- trunk/plearn/misc/PLearnServer.cc	2007-02-22 21:24:54 UTC (rev 6677)
+++ trunk/plearn/misc/PLearnServer.cc	2007-02-22 22:55:09 UTC (rev 6678)
@@ -200,9 +200,10 @@
     // forward log messages to client
     PP<PL_LogPlugin> orig_log_plugin= PL_Log::instance().getCurrentPlugin();
     PL_Log::instance().setPlugin(new PL_LogPluginServer(io));
-    // forward progress messages to client
+    // forward progress messages to client, unless the pbar plugin is null
     PP<ProgressBarPlugin> orig_pb_plugin= ProgressBar::getCurrentPlugin();
-    ProgressBar::setPlugin(new RemoteProgressBarPlugin(io));
+    if(dynamic_cast<NullProgressBarPlugin*>(static_cast<ProgressBarPlugin*>(orig_pb_plugin)) == 0)
+        ProgressBar::setPlugin(new RemoteProgressBarPlugin(io));
     // forward pout&perr to client
     PStream orig_pout= pout;
     PStream orig_perr= perr;

Modified: trunk/python_modules/plearn/pytest/tests.py
===================================================================
--- trunk/python_modules/plearn/pytest/tests.py	2007-02-22 21:24:54 UTC (rev 6677)
+++ trunk/python_modules/plearn/pytest/tests.py	2007-02-22 22:55:09 UTC (rev 6678)
@@ -815,7 +815,7 @@
                 % self.test.getPath()
                 )
 
-    def start(self):        
+    def start(self):
         self.run_test(RUN_RESULTS)
     
     def status_hook(self):



From yoshua at mail.berlios.de  Fri Feb 23 22:47:49 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Fri, 23 Feb 2007 22:47:49 +0100
Subject: [Plearn-commits] r6679 - trunk/plearn_learners/online
Message-ID: <200702232147.l1NLln90027313@sheep.berlios.de>

Author: yoshua
Date: 2007-02-23 22:47:48 +0100 (Fri, 23 Feb 2007)
New Revision: 6679

Modified:
   trunk/plearn_learners/online/BackConvolution2DModule.cc
   trunk/plearn_learners/online/BackConvolution2DModule.h
   trunk/plearn_learners/online/Convolution2DModule.cc
   trunk/plearn_learners/online/Convolution2DModule.h
   trunk/plearn_learners/online/CostModule.cc
   trunk/plearn_learners/online/CostModule.h
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
   trunk/plearn_learners/online/GradNNetLayerModule.cc
   trunk/plearn_learners/online/GradNNetLayerModule.h
   trunk/plearn_learners/online/ModuleStackModule.cc
   trunk/plearn_learners/online/ModuleStackModule.h
   trunk/plearn_learners/online/OnlineLearningModule.cc
   trunk/plearn_learners/online/OnlineLearningModule.h
   trunk/plearn_learners/online/RBMBinomialLayer.cc
   trunk/plearn_learners/online/RBMBinomialLayer.h
   trunk/plearn_learners/online/RBMClassificationModule.cc
   trunk/plearn_learners/online/RBMClassificationModule.h
   trunk/plearn_learners/online/RBMConv2DConnection.cc
   trunk/plearn_learners/online/RBMConv2DConnection.h
   trunk/plearn_learners/online/RBMGaussianLayer.cc
   trunk/plearn_learners/online/RBMGaussianLayer.h
   trunk/plearn_learners/online/RBMLayer.h
   trunk/plearn_learners/online/RBMMixedConnection.cc
   trunk/plearn_learners/online/RBMMixedConnection.h
   trunk/plearn_learners/online/RBMMixedLayer.cc
   trunk/plearn_learners/online/RBMMixedLayer.h
   trunk/plearn_learners/online/RBMMultinomialLayer.cc
   trunk/plearn_learners/online/RBMMultinomialLayer.h
   trunk/plearn_learners/online/RBMTruncExpLayer.cc
   trunk/plearn_learners/online/RBMTruncExpLayer.h
   trunk/plearn_learners/online/SoftmaxModule.cc
   trunk/plearn_learners/online/SoftmaxModule.h
   trunk/plearn_learners/online/Subsampling2DModule.cc
   trunk/plearn_learners/online/Subsampling2DModule.h
   trunk/plearn_learners/online/Supersampling2DModule.cc
   trunk/plearn_learners/online/Supersampling2DModule.h
   trunk/plearn_learners/online/TanhModule.cc
   trunk/plearn_learners/online/TanhModule.h
Log:


Modified: trunk/plearn_learners/online/BackConvolution2DModule.cc
===================================================================
--- trunk/plearn_learners/online/BackConvolution2DModule.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/BackConvolution2DModule.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -373,8 +373,10 @@
 //! this version allows to obtain the input gradient as well
 void BackConvolution2DModule::bpropUpdate(const Vec& input, const Vec& output,
                                           Vec& input_gradient,
-                                          const Vec& output_gradient)
+                                          const Vec& output_gradient,
+                                          bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     // Check size
     if( input.size() != input_size )
         PLERROR("BackConvolution2DModule::bpropUpdate: input.size() should"
@@ -490,8 +492,10 @@
                                            Vec& input_gradient,
                                            const Vec& output_gradient,
                                            Vec& input_diag_hessian,
-                                           const Vec& output_diag_hessian)
+                                           const Vec& output_diag_hessian,
+                                           bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bbpropUpdate cannot yet handle accumulate=false");
     // This version forwards the second order information, but does not
     // actually use it for the update.
 

Modified: trunk/plearn_learners/online/BackConvolution2DModule.h
===================================================================
--- trunk/plearn_learners/online/BackConvolution2DModule.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/BackConvolution2DModule.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -149,9 +149,12 @@
 
     //! this version allows to obtain the input gradient as well
     //! N.B. THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
+    //! The flag indicates whether the input_gradients gets
+    //! accumulated into or set with the computed derivatives.
     virtual void bpropUpdate(const Vec& input, const Vec& output,
                              Vec& input_gradient,
-                             const Vec& output_gradient);
+                             const Vec& output_gradient,
+                             bool accumulate=false);
 
     //! Similar to bpropUpdate, but adapt based also on the estimation
     //! of the diagonal of the Hessian matrix, and propagates this
@@ -171,7 +174,8 @@
                               Vec& input_gradient,
                               const Vec& output_gradient,
                               Vec& input_diag_hessian,
-                              const Vec& output_diag_hessian);
+                              const Vec& output_diag_hessian,
+                              bool accumulate=false);
 
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from

Modified: trunk/plearn_learners/online/Convolution2DModule.cc
===================================================================
--- trunk/plearn_learners/online/Convolution2DModule.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/Convolution2DModule.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -380,8 +380,10 @@
 //! this version allows to obtain the input gradient as well
 void Convolution2DModule::bpropUpdate(const Vec& input, const Vec& output,
                                       Vec& input_gradient,
-                                      const Vec& output_gradient)
+                                      const Vec& output_gradient,
+                                      bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     // Check size
     if( input.size() != input_size )
         PLERROR("Convolution2DModule::bpropUpdate: input.size() should be\n"
@@ -497,8 +499,11 @@
                                        Vec& input_gradient,
                                        const Vec& output_gradient,
                                        Vec& input_diag_hessian,
-                                       const Vec& output_diag_hessian)
+                                       const Vec& output_diag_hessian,
+                                       bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bbpropUpdate cannot yet handle accumulate=false");
+
     // This version forwards the second order information, but does not
     // actually use it for the update.
 

Modified: trunk/plearn_learners/online/Convolution2DModule.h
===================================================================
--- trunk/plearn_learners/online/Convolution2DModule.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/Convolution2DModule.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -151,7 +151,8 @@
     //! N.B. THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
     virtual void bpropUpdate(const Vec& input, const Vec& output,
                              Vec& input_gradient,
-                             const Vec& output_gradient);
+                             const Vec& output_gradient,
+                             bool accumulate=false);
 
     //! Similar to bpropUpdate, but adapt based also on the estimation
     //! of the diagonal of the Hessian matrix, and propagates this
@@ -171,7 +172,8 @@
                               Vec& input_gradient,
                               const Vec& output_gradient,
                               Vec& input_diag_hessian,
-                              const Vec& output_diag_hessian);
+                              const Vec& output_diag_hessian,
+                              bool accumulate=false);
 
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from

Modified: trunk/plearn_learners/online/CostModule.cc
===================================================================
--- trunk/plearn_learners/online/CostModule.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/CostModule.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -137,8 +137,11 @@
 
 void CostModule::bpropUpdate(const Vec& input_and_target, const Vec& output,
                              Vec& input_and_target_gradient,
-                             const Vec& output_gradient)
+                             const Vec& output_gradient,
+                             bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
+
     inherited::bpropUpdate( input_and_target, output,
                             input_and_target_gradient, output_gradient );
 }
@@ -176,8 +179,10 @@
                               Vec& input_and_target_gradient,
                               const Vec& output_gradient,
                               Vec& input_and_target_diag_hessian,
-                              const Vec& output_diag_hessian)
+                              const Vec& output_diag_hessian,
+                              bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bbpropUpdate cannot yet handle accumulate=false");
     inherited::bbpropUpdate( input_and_target, output,
                              input_and_target_gradient,
                              output_gradient,

Modified: trunk/plearn_learners/online/CostModule.h
===================================================================
--- trunk/plearn_learners/online/CostModule.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/CostModule.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -87,7 +87,8 @@
     //! this version is provided for compatibility with the parent class.
     virtual void bpropUpdate(const Vec& input_and_target, const Vec& output,
                              Vec& input_and_target_gradient,
-                             const Vec& output_gradient);
+                             const Vec& output_gradient,
+                             bool accumulate=false);
 
     //! Similar to bpropUpdate, but adapt based also on the estimation
     //! of the diagonal of the Hessian matrix, and propagates this back.
@@ -102,8 +103,8 @@
                               Vec& input_and_target_gradient,
                               const Vec& output_gradient,
                               Vec& input_and_target_diag_hessian,
-                              const Vec& output_diag_hessian);
-
+                              const Vec& output_diag_hessian,
+                              bool accumulate=false);
     virtual void forget();
 
     //! Indicates the name of the computed costs

Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -58,6 +58,7 @@
     // grad_weight_decay( 0. ),
     use_classification_cost( true ),
     n_layers( 0 ),
+    online ( false ), 
     final_module_has_learning_rate( false ),
     final_cost_has_learning_rate( false ),
     nll_cost_index( -1 ),
@@ -99,8 +100,12 @@
 
     declareOption(ol, "training_schedule", &DeepBeliefNet::training_schedule,
                   OptionBase::buildoption,
-                  "Number of examples to use during each phase of learning:\n"
-                  "first the greedy phases, and then the gradient descent.\n");
+                  "Number of examples before starting each phase except the first\n"
+                  "(first the greedy phases, and then the fine-tuning phase).\n"
+                  "For example for 2 hidden layers, with 1000 examples in each\n"
+                  "greedy phase, this option should be [1000 2000] and the last\n"
+                  "nstages - 2000 examples will be used for fine-tuning.\n"
+                  "When online = true, this vector is ignored and should be empty.\n");
 
     declareOption(ol, "use_classification_cost",
                   &DeepBeliefNet::use_classification_cost,
@@ -173,6 +178,11 @@
                   "(except the first one) of the RBM. These costs are not\n"
                   "back-propagated to previous layers.\n");
 
+    declareOption(ol, "online", &DeepBeliefNet::online,
+                  OptionBase::buildoption,
+                  "If true then all unsupervised training stages (as well as the fine-tuning stage)\n"
+                  "are done simultaneously.\n");
+
     declareOption(ol, "n_layers", &DeepBeliefNet::n_layers,
                   OptionBase::learntoption,
                   "Number of layers");
@@ -211,7 +221,7 @@
     // Initialize some learnt variables
     n_layers = layers.length();
 
-    if( training_schedule.length() != n_layers-1 )
+    if( training_schedule.length() != n_layers-1  && training_schedule.length()!=0)
     {
         MODULE_LOG << "training_schedule.length() != n_layers-1, resizing and"
             " zeroing" << endl;
@@ -515,148 +525,173 @@
 
     PP<ProgressBar> pb;
 
+    real train_recons_error = 0.0;
+
     // clear stats of previous epoch
     train_stats->forget();
 
-    /***** initial greedy training *****/
-    for( int i=0 ; i<n_layers-1 ; i++ )
+    if (online)
+        // train all layers simultaneously AND fine-tuning as well!
     {
-        if( use_classification_cost && i == n_layers-2 )
-            break; // we will do a joint supervised learning instead
+        if( report_progress && stage < nstages )
+            pb = new ProgressBar( "Training "+classname(),
+                                  nstages - stage );
 
-        MODULE_LOG << "Training connection weights between layers " << i
-            << " and " << i+1 << endl;
+        for (int i=0; i<n_layers;i++)
+        {
+            layers[i]->setLearningRate( cd_learning_rate );
+            connections[i]->setLearningRate( cd_learning_rate );
+        }
 
-        int end_stage = min( training_schedule[i], nstages );
-
-        MODULE_LOG << "  stage = " << stage << endl;
-        MODULE_LOG << "  end_stage = " << end_stage << endl;
-        MODULE_LOG << "  cd_learning_rate = " << cd_learning_rate << endl;
-
-        if( report_progress && stage < end_stage )
-            pb = new ProgressBar( "Training layer "+tostring(i)
-                                  +" of "+classname(),
-                                  end_stage - stage );
-
-        layers[i]->setLearningRate( cd_learning_rate );
-        connections[i]->setLearningRate( cd_learning_rate );
-        layers[i+1]->setLearningRate( cd_learning_rate );
-
-        for( ; stage<end_stage ; stage++ )
+        for( ; stage<nstages; stage++)
         {
             int sample = stage % nsamples;
             train_set->getExample(sample, input, target, weight);
-            greedyStep( input, target, i );
-
+            onlineStep( input, target );
             if( pb )
-                if( i == 0 )
-                    pb->update( stage + 1 );
-                else
-                    pb->update( stage - training_schedule[i-1] + 1 );
+                pb->update( stage + 1 );
         }
     }
-
-    // possible supervised part
-    if( use_classification_cost )
+    else // by stages
     {
-        MODULE_LOG << "Training the classification module" << endl;
+        /***** initial greedy training *****/
+        for( int i=0 ; i<n_layers-1 ; i++ )
+        {
+            if( use_classification_cost && i == n_layers-2 )
+                break; // we will do a joint supervised learning instead
 
-        int end_stage = min( training_schedule[n_layers-2], nstages );
+            MODULE_LOG << "Training connection weights between layers " << i
+                       << " and " << i+1 << endl;
 
-        MODULE_LOG << "  stage = " << stage << endl;
-        MODULE_LOG << "  end_stage = " << end_stage << endl;
-        MODULE_LOG << "  cd_learning_rate = " << cd_learning_rate << endl;
+            int end_stage = min( training_schedule[i], nstages );
 
-        if( report_progress && stage < end_stage )
-             pb = new ProgressBar( "Training the classification module",
-                                   end_stage - stage );
+            MODULE_LOG << "  stage = " << stage << endl;
+            MODULE_LOG << "  end_stage = " << end_stage << endl;
+            MODULE_LOG << "  cd_learning_rate = " << cd_learning_rate << endl;
 
-        // set appropriate learning rate
-        setLearningRate( cd_learning_rate );
+            if( report_progress && stage < end_stage )
+                pb = new ProgressBar( "Training layer "+tostring(i)
+                                      +" of "+classname(),
+                                      end_stage - stage );
 
-        int previous_stage = (n_layers < 3) ? 0
-                                            : training_schedule[n_layers-3];
-        for( ; stage<end_stage ; stage++ )
+            layers[i]->setLearningRate( cd_learning_rate );
+            connections[i]->setLearningRate( cd_learning_rate );
+            layers[i+1]->setLearningRate( cd_learning_rate );
+
+            for( ; stage<end_stage ; stage++ )
+            {
+                int sample = stage % nsamples;
+                train_set->getExample(sample, input, target, weight);
+                greedyStep( input, target, i );
+
+                if( pb )
+                    if( i == 0 )
+                        pb->update( stage + 1 );
+                    else
+                        pb->update( stage - training_schedule[i-1] + 1 );
+            }
+        }
+
+        // possible supervised part
+        if(use_classification_cost )
         {
-            int sample = stage % nsamples;
-            train_set->getExample( sample, input, target, weight );
-            jointGreedyStep( input, target );
+            MODULE_LOG << "Training the classification module" << endl;
 
-            if( pb )
-                pb->update( stage - previous_stage + 1 );
+            int end_stage = min( training_schedule[n_layers-2], nstages );
+
+            MODULE_LOG << "  stage = " << stage << endl;
+            MODULE_LOG << "  end_stage = " << end_stage << endl;
+            MODULE_LOG << "  cd_learning_rate = " << cd_learning_rate << endl;
+
+            if( report_progress && stage < end_stage )
+                pb = new ProgressBar( "Training the classification module",
+                                      end_stage - stage );
+
+            // set appropriate learning rate
+            setLearningRate( cd_learning_rate );
+
+            int previous_stage = (n_layers < 3) ? 0
+                : training_schedule[n_layers-3];
+            for( ; stage<end_stage ; stage++ )
+            {
+                int sample = stage % nsamples;
+                train_set->getExample( sample, input, target, weight );
+                jointGreedyStep( input, target );
+
+                if( pb )
+                    pb->update( stage - previous_stage + 1 );
+            }
         }
-    }
 
-    /**** compute reconstruction error*****/
-    real train_recons_error = 0.0;
+        /**** compute reconstruction error*****/
+        RBMLayer * down_layer = get_pointer(layers[0]) ;
+        RBMLayer * up_layer =  get_pointer(layers[1]) ;
+        RBMConnection * parameters = get_pointer(connections[0]);
 
-    RBMLayer * down_layer = get_pointer(layers[0]) ;
-    RBMLayer * up_layer =  get_pointer(layers[1]) ;
-    RBMConnection * parameters = get_pointer(connections[0]);
+        for(int train_index = 0 ; train_index < nsamples ; train_index++)
+        {
 
-    for(int train_index = 0 ; train_index < nsamples ; train_index++)
-    {
+            train_set->getExample( train_index, input, target, weight );
 
-        train_set->getExample( train_index, input, target, weight );
+            down_layer->expectation << input;
 
-          down_layer->expectation << input;
+            // up
+            parameters->setAsDownInput( down_layer->expectation );
+            up_layer->getAllActivations( parameters );
+            up_layer->generateSample();
 
-          // up
-          parameters->setAsDownInput( down_layer->expectation );
-          up_layer->getAllActivations( parameters );
-          up_layer->generateSample();
+            // down
+            parameters->setAsUpInput( up_layer->sample );
 
-          // down
-          parameters->setAsUpInput( up_layer->sample );
+            down_layer->getAllActivations( parameters );
+            down_layer->computeExpectation();
+            down_layer->generateSample();
 
-          down_layer->getAllActivations( parameters );
-          down_layer->computeExpectation();
-          down_layer->generateSample();
+            //    result += powdistance( input, down_layer->expectation );
 
-          //    result += powdistance( input, down_layer->expectation );
+            for( int i=0 ; i<input.size() ; i++ )
+                train_recons_error += (input[i] - down_layer->expectation[i])
+                    * (input[i] - down_layer->expectation[i]);
 
-          for( int i=0 ; i<input.size() ; i++ )
-              train_recons_error += (input[i] - down_layer->expectation[i])
-                                  * (input[i] - down_layer->expectation[i]);
+        }
 
-    }
+        train_recons_error /= nsamples ;
 
-    train_recons_error /= nsamples ;
 
+        /***** fine-tuning by gradient descent *****/
+        if( stage >= nstages )
+            return;
 
-    /***** fine-tuning by gradient descent *****/
-    if( stage >= nstages )
-        return;
+        MODULE_LOG << "Fine-tuning all parameters, by gradient descent" << endl;
+        MODULE_LOG << "  stage = " << stage << endl;
+        MODULE_LOG << "  nstages = " << nstages << endl;
+        MODULE_LOG << "  grad_learning_rate = " << grad_learning_rate << endl;
 
-    MODULE_LOG << "Fine-tuning all parameters, by gradient descent" << endl;
-    MODULE_LOG << "  stage = " << stage << endl;
-    MODULE_LOG << "  nstages = " << nstages << endl;
-    MODULE_LOG << "  grad_learning_rate = " << grad_learning_rate << endl;
+        int init_stage = stage;
+        if( report_progress && stage < nstages )
+            pb = new ProgressBar( "Fine-tuning parameters of all layers of "
+                                  + classname(),
+                                  nstages - init_stage );
 
-    int init_stage = stage;
-    if( report_progress && stage < nstages )
-        pb = new ProgressBar( "Fine-tuning parameters of all layers of "
-                              + classname(),
-                              nstages - init_stage );
+        setLearningRate( grad_learning_rate );
 
-    setLearningRate( grad_learning_rate );
+        int begin_sample = stage % nsamples;
+        for( ; stage<nstages ; stage++ )
+        {
+            int sample = stage % nsamples;
+            if( sample == begin_sample )
+                train_stats->forget();
+            if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
+                setLearningRate( grad_learning_rate
+                                 / (1. + grad_decrease_ct * (stage - init_stage) ) );
 
-    int begin_sample = stage % nsamples;
-    for( ; stage<nstages ; stage++ )
-    {
-        int sample = stage % nsamples;
-        if( sample == begin_sample )
-            train_stats->forget();
-        if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
-            setLearningRate( grad_learning_rate
-                / (1. + grad_decrease_ct * (stage - init_stage) ) );
+            train_set->getExample( sample, input, target, weight );
+            fineTuningStep( input, target, train_costs );
+            train_stats->update( train_costs );
 
-        train_set->getExample( sample, input, target, weight );
-        fineTuningStep( input, target, train_costs );
-        train_stats->update( train_costs );
-
-        if( pb )
-            pb->update( stage - init_stage + 1 );
+            if( pb )
+                pb->update( stage - init_stage + 1 );
+        }
     }
 
     //update the reconstruction error
@@ -667,6 +702,65 @@
     train_stats->finalize();
 }
 
+void DeepBeliefNet::onlineStep( const Vec& input, const Vec& target)
+{
+    Vec cost;
+    if (partial_costs)
+        cost.resize(n_layers);
+
+    layers[0]->expectation << input;
+    for( int i=0 ; i<n_layers-1 ; i++ )
+    {
+        // mean-field fprop from layer i to layer i+1
+        connections[i]->setAsDownInput( layers[i]->expectation );
+        layers[i+1]->getAllActivations( connections[i] );
+        layers[i+1]->computeExpectation();
+        // propagate into local cost
+        if( partial_costs && partial_costs[ i ] )
+            partial_costs[ i ]->fprop( layers[ i+1 ]->expectation,
+                                       target, cost[i] );
+
+        if( partial_costs && partial_costs[ i ] )
+        {
+            // put appropriate learning rate
+            connections[ i ]->setLearningRate( grad_learning_rate );
+            layers[ i+1 ]->setLearningRate( grad_learning_rate );
+
+            // Backward pass
+
+            partial_costs[ i ]->bpropUpdate( layers[ i+1 ]->expectation,
+                                             target, cost,
+                                             expectation_gradients[ i+1 ] );
+
+            // YB - LOUCHE: activation n'est pas vraiment l'output du connection ni l'input de layer i+1
+            // puisque c'est l'output de connection + le biais
+            layers[ i+1 ]->bpropUpdate( layers[ i+1 ]->activation, // - biais
+                                        layers[ i+1 ]->expectation,  
+                                        activation_gradients[ i+1 ],
+                                        expectation_gradients[ i+1 ] );
+
+            connections[ i ]->bpropUpdate( layers[ i ]->expectation,
+                                           layers[ i+1 ]->activation,  // - biais
+                                           expectation_gradients[ i ],
+                                           activation_gradients[ i+1 ] );
+
+            // put back old learning rate
+            connections[ i ]->setLearningRate( cd_learning_rate );
+            layers[ i+1 ]->setLearningRate( cd_learning_rate );
+
+            layers[i]->setLearningRate( cd_learning_rate );
+            connections[i]->setLearningRate( cd_learning_rate );
+        }
+
+        contrastiveDivergenceStep( layers[ i ],
+                                   connections[ i ],
+                                   layers[ i+1 ] );
+
+    }
+    // fprop in joint layer
+    
+}
+
 void DeepBeliefNet::greedyStep( const Vec& input, const Vec& target, int index )
 {
     PLASSERT( index < n_layers );

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -126,6 +126,9 @@
     //! Number of layers
     int n_layers;
 
+    //! whether to do things by stages, including fine-tuning, or on-line
+    bool online;
+
     //#####  Not Options  #####################################################
 
     //! Timer for monitoring the speed
@@ -185,6 +188,8 @@
     virtual TVec<std::string> getTrainCostNames() const;
 
 
+    void onlineStep( const Vec& input, const Vec& target );
+
     void greedyStep( const Vec& input, const Vec& target, int index );
 
     void jointGreedyStep( const Vec& input, const Vec& target );

Modified: trunk/plearn_learners/online/GradNNetLayerModule.cc
===================================================================
--- trunk/plearn_learners/online/GradNNetLayerModule.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/GradNNetLayerModule.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -134,8 +134,10 @@
 // Simply updates and propagates back gradient
 void GradNNetLayerModule::bpropUpdate(const Vec& input, const Vec& output,
                                       Vec& input_gradient,
-                                      const Vec& output_gradient)
+                                      const Vec& output_gradient,
+                                      bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     PLASSERT_MSG( input.size() == input_size,
                   "input.size() should be equal to this->input_size" );
     PLASSERT_MSG( output.size() == output_size,

Modified: trunk/plearn_learners/online/GradNNetLayerModule.h
===================================================================
--- trunk/plearn_learners/online/GradNNetLayerModule.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/GradNNetLayerModule.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -112,7 +112,8 @@
                              const Vec& output_gradient);
 
     virtual void bpropUpdate(const Vec& input, const Vec& output,
-                             Vec& input_gradient, const Vec& output_gradient);
+                             Vec& input_gradient, const Vec& output_gradient,
+                             bool accumulate=false);
 
     virtual void bbpropUpdate(const Vec& input, const Vec& output,
                               const Vec& output_gradient,

Modified: trunk/plearn_learners/online/ModuleStackModule.cc
===================================================================
--- trunk/plearn_learners/online/ModuleStackModule.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/ModuleStackModule.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -126,8 +126,10 @@
 /////////////////
 void ModuleStackModule::bpropUpdate(const Vec& input, const Vec& output,
                                     Vec& input_gradient,
-                                    const Vec& output_gradient)
+                                    const Vec& output_gradient,
+                                    bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     PLASSERT( n_modules > 0 );
     PLASSERT( input.size() == input_size );
     PLASSERT( output.size() == output_size );
@@ -173,8 +175,10 @@
                                      Vec& input_gradient,
                                      const Vec& output_gradient,
                                      Vec& input_diag_hessian,
-                                     const Vec& output_diag_hessian)
+                                     const Vec& output_diag_hessian,
+                                     bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bbpropUpdate cannot yet handle accumulate=false");
     PLASSERT( n_modules > 0 );
     PLASSERT( input.size() == input_size );
     PLASSERT( output.size() == output_size );

Modified: trunk/plearn_learners/online/ModuleStackModule.h
===================================================================
--- trunk/plearn_learners/online/ModuleStackModule.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/ModuleStackModule.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -85,7 +85,8 @@
     //! is 'ready-to-be-used' just after any bpropUpdate.
     virtual void bpropUpdate(const Vec& input, const Vec& output,
                              Vec& input_gradient,
-                             const Vec& output_gradient);
+                             const Vec& output_gradient,
+                             bool accumulate=false);
 
     //! This version does not obtain the input gradient.
     virtual void bpropUpdate(const Vec& input, const Vec& output,
@@ -99,7 +100,8 @@
                               Vec& input_gradient,
                               const Vec& output_gradient,
                               Vec& input_diag_hessian,
-                              const Vec& output_diag_hessian);
+                              const Vec& output_diag_hessian,
+                              bool accumulate=false);
 
     //! This version does not obtain the input gradient and diag_hessian.
     virtual void bbpropUpdate(const Vec& input, const Vec& output,

Modified: trunk/plearn_learners/online/OnlineLearningModule.cc
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/OnlineLearningModule.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -66,7 +66,8 @@
 
 void OnlineLearningModule::bpropUpdate(const Vec& input, const Vec& output,
                                        Vec& input_gradient,
-                                       const Vec& output_gradient)
+                                       const Vec& output_gradient,
+                                       bool accumulate)
 {
     PLERROR("In OnlineLearningModule.cc: method 'bpropUpdate' not"
             " implemented.\n"
@@ -97,7 +98,8 @@
                                         Vec& input_gradient,
                                         const Vec& output_gradient,
                                         Vec& input_diag_hessian,
-                                        const Vec& output_diag_hessian)
+                                        const Vec& output_diag_hessian,
+                                        bool accumulate)
 {
     PLERROR("In OnlineLearningModule.cc: method 'bbpropUpdate' not"
             "implemented.\n"

Modified: trunk/plearn_learners/online/OnlineLearningModule.h
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/OnlineLearningModule.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -120,8 +120,11 @@
 
     //! this version allows to obtain the input gradient as well
     //! N.B. THE DEFAULT IMPLEMENTATION JUST RAISES A PLERROR.
+    //! The flag indicates whether the input_gradients gets
+    //! accumulated into or set with the computed derivatives.
     virtual void bpropUpdate(const Vec& input, const Vec& output,
-                             Vec& input_gradient, const Vec& output_gradient);
+                             Vec& input_gradient, const Vec& output_gradient,
+                             bool accumulate=false);
 
     //! Similar to bpropUpdate, but adapt based also on the estimation
     //! of the diagonal of the Hessian matrix, and propagates this
@@ -137,11 +140,14 @@
                               const Vec& output_diag_hessian);
 
     //! this version allows to obtain the input gradient and diag_hessian
+    //! The flag indicates whether the input_gradient and input_diag_hessian gets
+    //! accumulated into or set with the computed derivatives.
     virtual void bbpropUpdate(const Vec& input, const Vec& output,
                               Vec& input_gradient,
                               const Vec& output_gradient,
                               Vec& input_diag_hessian,
-                              const Vec& output_diag_hessian);
+                              const Vec& output_diag_hessian,
+                              bool accumulate=false);
 
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from

Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -128,8 +128,10 @@
 
 void RBMBinomialLayer::bpropUpdate(const Vec& input, const Vec& output,
                                    Vec& input_gradient,
-                                   const Vec& output_gradient)
+                                   const Vec& output_gradient,
+                                   bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     PLASSERT( input.size() == size );
     PLASSERT( output.size() == size );
     PLASSERT( output_gradient.size() == size );

Modified: trunk/plearn_learners/online/RBMBinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMBinomialLayer.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -95,7 +95,8 @@
 
     //! back-propagates the output gradient to the input
     virtual void bpropUpdate(const Vec& input, const Vec& output,
-                             Vec& input_gradient, const Vec& output_gradient);
+                             Vec& input_gradient, const Vec& output_gradient,
+                             bool accumulate=false);
 
     //! back-propagates the output gradient to the input and the bias
     virtual void bpropUpdate(const Vec& input, const Vec& rbm_bias,

Modified: trunk/plearn_learners/online/RBMClassificationModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMClassificationModule.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMClassificationModule.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -209,8 +209,10 @@
 //! this version allows to obtain the input gradient as well
 void RBMClassificationModule::bpropUpdate(const Vec& input, const Vec& output,
                                           Vec& input_gradient,
-                                          const Vec& output_gradient)
+                                          const Vec& output_gradient,
+                                          bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     // size checks
     PLASSERT( input.size() == input_size );
     PLASSERT( output.size() == output_size );

Modified: trunk/plearn_learners/online/RBMClassificationModule.h
===================================================================
--- trunk/plearn_learners/online/RBMClassificationModule.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMClassificationModule.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -111,7 +111,8 @@
     //! this version allows to obtain the input gradient as well
     virtual void bpropUpdate(const Vec& input, const Vec& output,
                              Vec& input_gradient,
-                             const Vec& output_gradient);
+                             const Vec& output_gradient,
+                             bool accumulate=false);
 
     //! Similar to bpropUpdate, but adapt based also on the estimation
     //! of the diagonal of the Hessian matrix, and propagates this

Modified: trunk/plearn_learners/online/RBMConv2DConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMConv2DConnection.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMConv2DConnection.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -428,8 +428,10 @@
 //! this version allows to obtain the input gradient as well
 void RBMConv2DConnection::bpropUpdate(const Vec& input, const Vec& output,
                                       Vec& input_gradient,
-                                      const Vec& output_gradient)
+                                      const Vec& output_gradient,
+                                      bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     PLASSERT( input.size() == down_size );
     PLASSERT( output.size() == up_size );
     PLASSERT( output_gradient.size() == up_size );

Modified: trunk/plearn_learners/online/RBMConv2DConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMConv2DConnection.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMConv2DConnection.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -151,7 +151,8 @@
     //! N.B. THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
     virtual void bpropUpdate(const Vec& input, const Vec& output,
                              Vec& input_gradient,
-                             const Vec& output_gradient);
+                             const Vec& output_gradient,
+                             bool accumulate=false);
 
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from

Modified: trunk/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -125,8 +125,10 @@
 
 void RBMGaussianLayer::bpropUpdate(const Vec& input, const Vec& output,
                                    Vec& input_gradient,
-                                   const Vec& output_gradient)
+                                   const Vec& output_gradient,
+                                   bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bbpropUpdate cannot yet handle accumulate=false");
     PLASSERT( input.size() == size );
     PLASSERT( output.size() == size );
     PLASSERT( output_gradient.size() == size );

Modified: trunk/plearn_learners/online/RBMGaussianLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMGaussianLayer.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -96,7 +96,8 @@
 
     //! back-propagates the output gradient to the input
     virtual void bpropUpdate(const Vec& input, const Vec& output,
-                             Vec& input_gradient, const Vec& output_gradient);
+                             Vec& input_gradient, const Vec& output_gradient,
+                             bool accumulate=false);
 
     //! Accumulates positive phase statistics
     virtual void accumulatePosStats( const Vec& pos_values );

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMLayer.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -137,13 +137,14 @@
     //! and update the bias (and possibly the quadratic term)
     virtual void bpropUpdate(const Vec& input, const Vec& output,
                              Vec& input_gradient,
-                             const Vec& output_gradient) = 0 ;
+                             const Vec& output_gradient,
+                             bool accumulate=false) = 0 ;
 
     //! back-propagates the output gradient to the input and the bias
     virtual void bpropUpdate(const Vec& input, const Vec& rbm_bias,
                              const Vec& output,
                              Vec& input_gradient, Vec& rbm_bias_gradient,
-                             const Vec& output_gradient) ;
+                             const Vec& output_gradient);
 
     //! Computes the negative log-likelihood of target given the
     //! internal activations of the layer

Modified: trunk/plearn_learners/online/RBMMixedConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMixedConnection.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMMixedConnection.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -503,9 +503,11 @@
 
 //! this version allows to obtain the input gradient as well
 void RBMMixedConnection::bpropUpdate(const Vec& input, const Vec& output,
-                                      Vec& input_gradient,
-                                      const Vec& output_gradient)
+                                     Vec& input_gradient,
+                                     const Vec& output_gradient,
+                                     bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     PLASSERT( input.size() == down_size );
     PLASSERT( output.size() == up_size );
     PLASSERT( output_gradient.size() == up_size );

Modified: trunk/plearn_learners/online/RBMMixedConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMMixedConnection.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMMixedConnection.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -127,7 +127,8 @@
     //! this version allows to obtain the input gradient as well
     virtual void bpropUpdate(const Vec& input, const Vec& output,
                              Vec& input_gradient,
-                             const Vec& output_gradient);
+                             const Vec& output_gradient,
+                             bool accumulate=false);
 
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from

Modified: trunk/plearn_learners/online/RBMMixedLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMMixedLayer.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -149,8 +149,10 @@
 
 void RBMMixedLayer::bpropUpdate( const Vec& input, const Vec& output,
                                  Vec& input_gradient,
-                                 const Vec& output_gradient )
+                                 const Vec& output_gradient,
+                                 bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     PLASSERT( input.size() == size );
     PLASSERT( output.size() == size );
     PLASSERT( output_gradient.size() == size );

Modified: trunk/plearn_learners/online/RBMMixedLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMMixedLayer.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -101,7 +101,8 @@
 
     //! back-propagates the output gradient to the input
     virtual void bpropUpdate(const Vec& input, const Vec& output,
-                             Vec& input_gradient, const Vec& output_gradient);
+                             Vec& input_gradient, const Vec& output_gradient,
+                             bool accumulate=false);
 
     //! back-propagates the output gradient to the input and the bias
     virtual void bpropUpdate(const Vec& input, const Vec& rbm_bias,

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -127,8 +127,10 @@
 
 void RBMMultinomialLayer::bpropUpdate(const Vec& input, const Vec& output,
                                       Vec& input_gradient,
-                                      const Vec& output_gradient)
+                                      const Vec& output_gradient,
+                                      bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     PLASSERT( input.size() == size );
     PLASSERT( output.size() == size );
     PLASSERT( output_gradient.size() == size );

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -95,7 +95,8 @@
 
     //! back-propagates the output gradient to the input
     virtual void bpropUpdate(const Vec& input, const Vec& output,
-                             Vec& input_gradient, const Vec& output_gradient);
+                             Vec& input_gradient, const Vec& output_gradient,
+                             bool accumulate=false);
 
     //! back-propagates the output gradient to the input and the bias
     virtual void bpropUpdate(const Vec& input, const Vec& rbm_bias,

Modified: trunk/plearn_learners/online/RBMTruncExpLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMTruncExpLayer.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMTruncExpLayer.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -158,8 +158,10 @@
 
 void RBMTruncExpLayer::bpropUpdate(const Vec& input, const Vec& output,
                                    Vec& input_gradient,
-                                   const Vec& output_gradient)
+                                   const Vec& output_gradient,
+                                   bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     PLASSERT( input.size() == size );
     PLASSERT( output.size() == size );
     PLASSERT( output_gradient.size() == size );

Modified: trunk/plearn_learners/online/RBMTruncExpLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMTruncExpLayer.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMTruncExpLayer.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -92,9 +92,9 @@
 
     //! back-propagates the output gradient to the input
     virtual void bpropUpdate(const Vec& input, const Vec& output,
-                             Vec& input_gradient, const Vec& output_gradient);
+                             Vec& input_gradient, const Vec& output_gradient,
+                             bool accumulate=false);
 
-
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.

Modified: trunk/plearn_learners/online/SoftmaxModule.cc
===================================================================
--- trunk/plearn_learners/online/SoftmaxModule.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/SoftmaxModule.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -92,8 +92,10 @@
 
 void SoftmaxModule::bpropUpdate(const Vec& input, const Vec& output,
                                 Vec& input_gradient,
-                                const Vec& output_gradient)
+                                const Vec& output_gradient,
+                                bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     PLASSERT( input.size() == input_size );
     PLASSERT( output.size() == output_size );
     PLASSERT( output_gradient.size() == output_size );
@@ -112,11 +114,13 @@
 }
 
 void SoftmaxModule::bbpropUpdate(const Vec& input, const Vec& output,
-                                Vec& input_gradient,
-                                const Vec& output_gradient,
-                                Vec& input_diag_hessian,
-                                const Vec& output_diag_hessian)
+                                 Vec& input_gradient,
+                                 const Vec& output_gradient,
+                                 Vec& input_diag_hessian,
+                                 const Vec& output_diag_hessian,
+                                 bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bbpropUpdate cannot yet handle accumulate=false");
     PLERROR( "Not implemented yet, please come back later or complaint to"
              " lamblinp." );
 }

Modified: trunk/plearn_learners/online/SoftmaxModule.h
===================================================================
--- trunk/plearn_learners/online/SoftmaxModule.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/SoftmaxModule.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -69,14 +69,16 @@
     //! this version allows to obtain the input gradient as well
     virtual void bpropUpdate(const Vec& input, const Vec& output,
                              Vec& input_gradient,
-                             const Vec& output_gradient);
+                             const Vec& output_gradient,
+                             bool accumulate=false);
 
     //! this version allows to obtain the input gradient and diag_hessian
     virtual void bbpropUpdate(const Vec& input, const Vec& output,
                               Vec& input_gradient,
                               const Vec& output_gradient,
                               Vec& input_diag_hessian,
-                              const Vec& output_diag_hessian);
+                              const Vec& output_diag_hessian,
+                              bool accumulate=false);
 
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from

Modified: trunk/plearn_learners/online/Subsampling2DModule.cc
===================================================================
--- trunk/plearn_learners/online/Subsampling2DModule.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/Subsampling2DModule.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -284,8 +284,10 @@
 //! this version allows to obtain the input gradient as well
 void Subsampling2DModule::bpropUpdate(const Vec& input, const Vec& output,
                                       Vec& input_gradient,
-                                      const Vec& output_gradient)
+                                      const Vec& output_gradient,
+                                      bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     // Check size
     if( input.size() != input_size )
         PLERROR("Subsampling2DModule::bpropUpdate: input.size() should be\n"
@@ -384,8 +386,10 @@
                                        Vec& input_gradient,
                                        const Vec& output_gradient,
                                        Vec& input_diag_hessian,
-                                       const Vec& output_diag_hessian)
+                                       const Vec& output_diag_hessian,
+                                       bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bbpropUpdate cannot yet handle accumulate=false");
     // This version forwards the second order information, but does not
     // actually use it for the update.
 

Modified: trunk/plearn_learners/online/Subsampling2DModule.h
===================================================================
--- trunk/plearn_learners/online/Subsampling2DModule.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/Subsampling2DModule.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -135,7 +135,8 @@
     //! N.B. THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
     virtual void bpropUpdate(const Vec& input, const Vec& output,
                              Vec& input_gradient,
-                             const Vec& output_gradient);
+                             const Vec& output_gradient,
+                             bool accumulate=false);
 
     //! Similar to bpropUpdate, but adapt based also on the estimation
     //! of the diagonal of the Hessian matrix, and propagates this
@@ -155,7 +156,8 @@
                               Vec& input_gradient,
                               const Vec& output_gradient,
                               Vec& input_diag_hessian,
-                              const Vec& output_diag_hessian);
+                              const Vec& output_diag_hessian,
+                              bool accumulate=false);
 
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from

Modified: trunk/plearn_learners/online/Supersampling2DModule.cc
===================================================================
--- trunk/plearn_learners/online/Supersampling2DModule.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/Supersampling2DModule.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -274,9 +274,11 @@
 
 //! this version allows to obtain the input gradient as well
 void Supersampling2DModule::bpropUpdate(const Vec& input, const Vec& output,
-                                      Vec& input_gradient,
-                                      const Vec& output_gradient)
+                                        Vec& input_gradient,
+                                        const Vec& output_gradient,
+                                        bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     // Check size
     if( input.size() != input_size )
         PLERROR("Supersampling2DModule::bpropUpdate: input.size() should be\n"
@@ -375,8 +377,10 @@
                                          Vec& input_gradient,
                                          const Vec& output_gradient,
                                          Vec& input_diag_hessian,
-                                         const Vec& output_diag_hessian)
+                                         const Vec& output_diag_hessian,
+                                         bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bbpropUpdate cannot yet handle accumulate=false");
     // This version forwards the second order information, but does not
     // actually use it for the update.
 

Modified: trunk/plearn_learners/online/Supersampling2DModule.h
===================================================================
--- trunk/plearn_learners/online/Supersampling2DModule.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/Supersampling2DModule.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -135,7 +135,8 @@
     //! N.B. THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
     virtual void bpropUpdate(const Vec& input, const Vec& output,
                              Vec& input_gradient,
-                             const Vec& output_gradient);
+                             const Vec& output_gradient,
+                             bool accumulate=false);
 
     //! Similar to bpropUpdate, but adapt based also on the estimation
     //! of the diagonal of the Hessian matrix, and propagates this
@@ -155,7 +156,8 @@
                               Vec& input_gradient,
                               const Vec& output_gradient,
                               Vec& input_diag_hessian,
-                              const Vec& output_diag_hessian);
+                              const Vec& output_diag_hessian,
+                              bool accumulate=false);
 
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from

Modified: trunk/plearn_learners/online/TanhModule.cc
===================================================================
--- trunk/plearn_learners/online/TanhModule.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/TanhModule.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -108,8 +108,11 @@
 
 // Simply propagates output_gradient to input_gradient
 void TanhModule::bpropUpdate(const Vec& input, const Vec& output,
-                             Vec& input_gradient, const Vec& output_gradient)
+                             Vec& input_gradient, const Vec& output_gradient,
+                             bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
+
     int in_size = input.size();
     int out_size = output.size();
     int og_size = output_gradient.size();
@@ -164,8 +167,11 @@
                               Vec& input_gradient,
                               const Vec& output_gradient,
                               Vec& input_diag_hessian,
-                              const Vec& output_diag_hessian)
+                              const Vec& output_diag_hessian,
+                              bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bbpropUpdate cannot yet handle accumulate=false");
+
     int odh_size = output_diag_hessian.size();
 
     // size check

Modified: trunk/plearn_learners/online/TanhModule.h
===================================================================
--- trunk/plearn_learners/online/TanhModule.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/TanhModule.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -79,7 +79,8 @@
                              const Vec& output_gradient);
 
     virtual void bpropUpdate(const Vec& input, const Vec& output,
-                             Vec& input_gradient, const Vec& output_gradient);
+                             Vec& input_gradient, const Vec& output_gradient,
+                             bool accumulate=false);
 
     virtual void bbpropUpdate(const Vec& input, const Vec& output,
                               const Vec& output_gradient,
@@ -89,7 +90,8 @@
                               Vec& input_gradient,
                               const Vec& output_gradient,
                               Vec& input_diag_hessian,
-                              const Vec& output_diag_hessian);
+                              const Vec& output_diag_hessian,
+                              bool accumulate=false);
 
     virtual void forget();
 



From chapados at mail.berlios.de  Sat Feb 24 06:52:12 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Sat, 24 Feb 2007 06:52:12 +0100
Subject: [Plearn-commits] r6680 - trunk/plearn/ker
Message-ID: <200702240552.l1O5qCTS011951@sheep.berlios.de>

Author: chapados
Date: 2007-02-24 06:52:11 +0100 (Sat, 24 Feb 2007)
New Revision: 6680

Modified:
   trunk/plearn/ker/ARDBaseKernel.cc
   trunk/plearn/ker/ARDBaseKernel.h
   trunk/plearn/ker/IIDNoiseKernel.cc
   trunk/plearn/ker/IIDNoiseKernel.h
   trunk/plearn/ker/RationalQuadraticARDKernel.cc
   trunk/plearn/ker/RationalQuadraticARDKernel.h
Log:
All parameters for RationalQuadratic kernel are now specified in the inverse-softplus domain, which is more stable than the log-domain for numerical optimization; not converted SquaredExponential yet -- a test will fail (soon to be corrected)

Modified: trunk/plearn/ker/ARDBaseKernel.cc
===================================================================
--- trunk/plearn/ker/ARDBaseKernel.cc	2007-02-23 21:47:48 UTC (rev 6679)
+++ trunk/plearn/ker/ARDBaseKernel.cc	2007-02-24 05:52:11 UTC (rev 6680)
@@ -53,12 +53,13 @@
     "\n"
     "Note that to make its operations more robust when used with unconstrained\n"
     "optimization of hyperparameters, all hyperparameters of this kernel are\n"
-    "specified in the log-domain.\n"
+    "specified in the inverse softplus domain.  See IIDNoiseKernel for more\n"
+    "explanations.\n"
     );
 
 ARDBaseKernel::ARDBaseKernel()
-    : m_log_signal_sigma(0.0),
-      m_log_global_sigma(0.0)
+    : m_isp_signal_sigma(0.0),
+      m_isp_global_sigma(0.0)
 { }
 
 
@@ -67,22 +68,23 @@
 void ARDBaseKernel::declareOptions(OptionList& ol)
 {
     declareOption(
-        ol, "log_signal_sigma",
-        &ARDBaseKernel::m_log_signal_sigma,
+        ol, "isp_signal_sigma",
+        &ARDBaseKernel::m_isp_signal_sigma,
         OptionBase::buildoption,
-        "Log of the global signal variance.  Default value=0.0");
+        "Inverse softplus of the global signal variance.  Default value=0.0");
 
     declareOption(
-        ol, "log_global_sigma",
-        &ARDBaseKernel::m_log_global_sigma,
+        ol, "isp_global_sigma",
+        &ARDBaseKernel::m_isp_global_sigma,
         OptionBase::buildoption,
-        "Log of the global length-scale.  Note that if ARD is performed on\n"
-        "input-specific sigmas, this hyperparameter should have a fixed value\n"
-        "(and not be varied during the optimization).  Default value=0.0.\n");
+        "Inverse softplus of the global length-scale.  Note that if ARD is\n"
+        "performed on input-specific sigmas, this hyperparameter should have a\n"
+        "fixed value (and not be varied during the optimization).  Default\n"
+        "value=0.0.\n");
 
     declareOption(
-        ol, "log_input_sigma",
-        &ARDBaseKernel::m_log_input_sigma,
+        ol, "isp_input_sigma",
+        &ARDBaseKernel::m_isp_input_sigma,
         OptionBase::buildoption,
         "If specified, contain input-specific length-scales that can be\n"
         "individually optimized for (these are the ARD hyperparameters).\n");
@@ -108,7 +110,7 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
-    deepCopyField(m_log_input_sigma, copies);
+    deepCopyField(m_isp_input_sigma, copies);
     deepCopyField(m_input_sigma,     copies);
 }
 

Modified: trunk/plearn/ker/ARDBaseKernel.h
===================================================================
--- trunk/plearn/ker/ARDBaseKernel.h	2007-02-23 21:47:48 UTC (rev 6679)
+++ trunk/plearn/ker/ARDBaseKernel.h	2007-02-24 05:52:11 UTC (rev 6680)
@@ -55,7 +55,8 @@
  *
  *  Note that to make its operations more robust when used with unconstrained
  *  optimization of hyperparameters, all hyperparameters of this kernel are
- *  specified in the log-domain.
+ *  specified in the inverse softplus domain.  See IIDNoiseKernel for more
+ *  explanations.
  */
 class ARDBaseKernel : public IIDNoiseKernel
 {
@@ -64,21 +65,22 @@
 public:
     //#####  Public Build Options  ############################################
 
-    //! Log of the global signal variance.  Default value=0.0
-    real m_log_signal_sigma;
+    //! Inverse softplus of the global signal variance.  Default value=0.0
+    real m_isp_signal_sigma;
 
     /**
-     *  Log of the global length-scale.  Note that if ARD is performed on
-     *  input-specific sigmas, this hyperparameter should have a fixed value
-     *  (and not be varied during the optimization).  Default value=0.0.
+     *  Inverse softplus of the global length-scale.  Note that if ARD is
+     *  performed on input-specific sigmas, this hyperparameter should have a
+     *  fixed value (and not be varied during the optimization).  Default
+     *  value=0.0.
      */
-    real m_log_global_sigma;
+    real m_isp_global_sigma;
 
     /**
      *  If specified, contain input-specific length-scales that can be
      *  individually optimized for (these are the ARD hyperparameters).
      */
-    Vec m_log_input_sigma;
+    Vec m_isp_input_sigma;
 
 public:
     //#####  Public Member Functions  #########################################

Modified: trunk/plearn/ker/IIDNoiseKernel.cc
===================================================================
--- trunk/plearn/ker/IIDNoiseKernel.cc	2007-02-23 21:47:48 UTC (rev 6679)
+++ trunk/plearn/ker/IIDNoiseKernel.cc	2007-02-24 05:52:11 UTC (rev 6680)
@@ -50,10 +50,10 @@
     "in gaussian processes (see GaussianProcessRegressor).  It represents simple\n"
     "i.i.d. additive noise:\n"
     "\n"
-    "  k(x,y) = delta_x,y * sn2\n"
+    "  k(x,y) = delta_x,y * sn\n"
     "\n"
-    "where delta_x,y is the Kronecker delta function, and sn2 is the exp of\n"
-    "twice the 'log_noise_sigma' option.\n"
+    "where delta_x,y is the Kronecker delta function, and sn is\n"
+    "softplus(isp_noise_sigma), with softplus(x) = log(1+exp(x)).\n"
     "\n"
     "In addition to comparing the complete x and y vectors, this kernel allows\n"
     "adding a Kronecker delta when there is a match in only ONE DIMENSION.  This\n"
@@ -61,20 +61,23 @@
     "the input variables (but is not currently done for performance reasons).\n"
     "With these terms, the kernel function takes the form:\n"
     "\n"
-    "  k(x,y) = delta_x,y * sn2 + \\sum_i delta_x[kr(i)],y[kr(i)] * ks2[i]\n"
+    "  k(x,y) = delta_x,y * sn + \\sum_i delta_x[kr(i)],y[kr(i)] * ks[i]\n"
     "\n"
     "where kr(i) is the i-th element of 'kronecker_indexes' (representing an\n"
-    "index into the input vectors), and ks2[i] is the exp of twice the value of\n"
-    "the i-th element of the 'log_kronecker_sigma' option.\n"
+    "index into the input vectors), and ks[i]=softplus(isp_kronecker_sigma[i]).\n"
     "\n"
     "Note that to make its operations more robust when used with unconstrained\n"
     "optimization of hyperparameters, all hyperparameters of this kernel are\n"
-    "specified in the log-domain.\n"
+    "specified in the inverse softplus domain, hence the 'isp' prefix.  This is\n"
+    "used in preference to the log-domain used by Rasmussen and Williams in\n"
+    "their implementation of gaussian processes, due to numerical stability.\n"
+    "(It may happen that the optimizer jumps 'too far' along one hyperparameter\n"
+    "and this causes the Gram matrix to become extremely ill-conditioned.)\n"
     );
 
 
 IIDNoiseKernel::IIDNoiseKernel()
-    : m_log_noise_sigma(0.0)
+    : m_isp_noise_sigma(0.0)
 { }
 
 
@@ -83,9 +86,9 @@
 void IIDNoiseKernel::declareOptions(OptionList& ol)
 {
     declareOption(
-        ol, "log_noise_sigma", &IIDNoiseKernel::m_log_noise_sigma,
+        ol, "isp_noise_sigma", &IIDNoiseKernel::m_isp_noise_sigma,
         OptionBase::buildoption,
-        "Log of the global noise variance.  Default value=0.0");
+        "Inverse softplus of the global noise variance.  Default value=0.0");
 
     declareOption(
         ol, "kronecker_indexes", &IIDNoiseKernel::m_kronecker_indexes,
@@ -94,10 +97,10 @@
         "Kronecker delta terms");
 
     declareOption(
-        ol, "log_kronecker_sigma", &IIDNoiseKernel::m_log_kronecker_sigma,
+        ol, "isp_kronecker_sigma", &IIDNoiseKernel::m_isp_kronecker_sigma,
         OptionBase::buildoption,
-        "Log of the noise variance terms for the Kronecker deltas associated\n"
-        "with kronecker_indexes");
+        "Inverse softplus of the noise variance terms for the Kronecker deltas\n"
+        "associated with kronecker_indexes");
     
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
@@ -118,10 +121,10 @@
 
 void IIDNoiseKernel::build_()
 {
-    if (m_kronecker_indexes.size() != m_log_kronecker_sigma.size())
+    if (m_kronecker_indexes.size() != m_isp_kronecker_sigma.size())
         PLERROR("IIDNoiseKernel::build_: size of 'kronecker_indexes' (%d) "
-                "does not match that of 'log_kronecker_sigma' (%d)",
-                m_kronecker_indexes.size(), m_log_kronecker_sigma.size());
+                "does not match that of 'iso_kronecker_sigma' (%d)",
+                m_kronecker_indexes.size(), m_isp_kronecker_sigma.size());
 }
 
 
@@ -129,23 +132,18 @@
 
 real IIDNoiseKernel::evaluate(const Vec& x1, const Vec& x2) const
 {
-    // if (fast_is_equal(powdistance(x1,x2,2), 0.0))
-    //     return exp(2*m_log_noise_sigma);
-    // else
-    //     return 0.0;
-
     real value = 0.0;
     if (x1 == x2)
-        value += exp(2*m_log_noise_sigma);
+        value += softplus(m_isp_noise_sigma);
 
     const int n = m_kronecker_indexes.size();
     if (n > 0) {
         int*  cur_index = m_kronecker_indexes.data();
-        real* cur_sigma = m_log_kronecker_sigma.data();
+        real* cur_sigma = m_isp_kronecker_sigma.data();
 
         for (int i=0 ; i<n ; ++i, ++cur_index, ++cur_sigma)
             if (fast_is_equal(x1[*cur_index], x2[*cur_index]))
-                value += exp(2 * *cur_sigma);
+                value += softplus(*cur_sigma);
     }
     return value;
 }
@@ -167,11 +165,10 @@
     PLASSERT( K.size() == 0 || m_data_cache.size() > 0 );  // Ensure data cached OK
 
     // Precompute some terms
-    real noise_sigma  = exp(2 * m_log_noise_sigma);
-    m_kronecker_sigma.resize(m_log_kronecker_sigma.size());
-    m_kronecker_sigma << m_log_kronecker_sigma;
-    m_kronecker_sigma *= 2.0;
-    exp(m_kronecker_sigma, m_kronecker_sigma);
+    real noise_sigma  = softplus(m_isp_noise_sigma);
+    m_kronecker_sigma.resize(m_isp_kronecker_sigma.size());
+    for (int i=0, n=m_isp_kronecker_sigma.size() ; i<n ; ++i)
+        m_kronecker_sigma[i] = softplus(m_isp_kronecker_sigma[i]);
 
     // Prepare kronecker iteration
     int   kronecker_num     = m_kronecker_indexes.size();
@@ -212,10 +209,7 @@
                         Kij += *cur_sigma;
             }
             
-            // Fill upper triangle if not on diagonal
             *Ki++ = Kij;
-            // if (j < i)
-            //     *Kji = Kij;
         }
     }
 }
@@ -228,7 +222,7 @@
     inherited::makeDeepCopyFromShallowCopy(copies);
 
     deepCopyField(m_kronecker_indexes,   copies);
-    deepCopyField(m_log_kronecker_sigma, copies);
+    deepCopyField(m_isp_kronecker_sigma, copies);
     deepCopyField(m_kronecker_sigma,     copies);
 }
 
@@ -238,9 +232,10 @@
 void IIDNoiseKernel::computeGramMatrixDerivative(Mat& KD, const string& kernel_param,
                                                  real epsilon) const
 {
-    static const string LNS("log_noise_sigma");
-    static const string LKS("log_kronecker_sigma[");
-    if (kernel_param == LNS) {
+    static const string INS("isp_noise_sigma");
+    static const string IKS("isp_kronecker_sigma[");
+
+    if (kernel_param == INS) {
         if (!data)
             PLERROR("Kernel::computeGramMatrixDerivative should be called only after "
                     "setDataForKernelMatrix");
@@ -252,50 +247,18 @@
         int W = nExamples();
         KD.resize(W,W);
         KD.fill(0.0);
-        real deriv = 2*exp(2*m_log_noise_sigma);
+        real deriv = sigmoid(m_isp_noise_sigma);
         for (int i=0 ; i<W ; ++i)
             KD(i,i) = deriv;
     }
-    else if (string_begins_with(kernel_param, LKS) &&
+    else if (string_begins_with(kernel_param, IKS) &&
              kernel_param[kernel_param.size()-1] == ']')
     {
         int arg = tolong(kernel_param.substr(
-                             LKS.size(), kernel_param.size() - LKS.size() - 1));
+                             IKS.size(), kernel_param.size() - IKS.size() - 1));
         PLASSERT( arg < m_kronecker_indexes.size() );
 
         computeGramMatrixDerivKronecker(KD, arg);
-        
-        // computeGramMatrixDerivNV<
-        //     IIDNoiseKernel, &IIDNoiseKernel::derivKronecker>(KD, this, arg);
-        
-        // int W = nExamples();
-        // KD.resize(W,W);
-        // real deriv = 2*exp(2*m_log_kronecker_sigma[arg]);
-        // int index  = m_kronecker_indexes[arg];
-        // 
-        // Vec row_i;
-        // Vec row_j;
-        // int m = KD.mod();
-        // real* KDi;                           // Start of row i
-        // real* KDji;                          // Start of column i
-        // for (int i=0 ; i<W ; ++i) {
-        //     KDi = KD[i];
-        //     KDji = &KD[0][i];
-        //     dataRow(i, row_i);
-        //     real row_i_index = row_i[index];
-        //     for (int j=0 ; j<=i ; ++j, KDji += m) {
-        //         dataRow(j, row_j);
-        //         real KDij;
-        //         if (fast_is_equal(row_i_index, row_j[index]))
-        //             KDij = deriv;
-        //         else
-        //             KDij = 0.0;
-        // 
-        //         *KDi++ = KDij;
-        //         if (j < i)
-        //             *KDji = KDij;
-        //     }
-        // }
     }
     else
         inherited::computeGramMatrixDerivative(KD, kernel_param, epsilon);
@@ -310,7 +273,7 @@
     Vec& row_i = *dataRow(i);
     Vec& row_j = *dataRow(j);
     if (fast_is_equal(row_i[index], row_j[index]))
-        return 2*exp(2*m_log_kronecker_sigma[arg]);
+        return sigmoid(m_isp_kronecker_sigma[arg]);
     else
         return 0.0;
 }
@@ -321,10 +284,10 @@
 void IIDNoiseKernel::computeGramMatrixDerivKronecker(Mat& KD, int arg) const
 {
     // Precompute some terms
-    real kronecker_sigma_arg = 2. * exp(2. * m_log_kronecker_sigma[arg]);
+    real kronecker_sigma_arg = sigmoid(m_isp_kronecker_sigma[arg]);
     int index = m_kronecker_indexes[arg];
     
-    // Compute Gram Matrix derivative w.r.t. log_kronecker_sigma[arg]
+    // Compute Gram Matrix derivative w.r.t. isp_kronecker_sigma[arg]
     int  l = data->length();
 
     // Variables that walk over the data matrix

Modified: trunk/plearn/ker/IIDNoiseKernel.h
===================================================================
--- trunk/plearn/ker/IIDNoiseKernel.h	2007-02-23 21:47:48 UTC (rev 6679)
+++ trunk/plearn/ker/IIDNoiseKernel.h	2007-02-24 05:52:11 UTC (rev 6680)
@@ -51,10 +51,10 @@
  *  in gaussian processes (see GaussianProcessRegressor).  It represents simple
  *  i.i.d. additive noise:
  *
- *    k(x,y) = delta_x,y * sn2
+ *    k(x,y) = delta_x,y * sn
  *
- *  where delta_x,y is the Kronecker delta function, and sn2 is the exp of
- *  twice the 'log_noise_sigma' option.
+ *  where delta_x,y is the Kronecker delta function, and sn is
+ *  softplus(isp_noise_sigma), with softplus(x) = log(1+exp(x)).
  *
  *  In addition to comparing the complete x and y vectors, this kernel allows
  *  adding a Kronecker delta when there is a match in only ONE DIMENSION.  This
@@ -62,15 +62,18 @@
  *  the input variables (but is not currently done for performance reasons).
  *  With these terms, the kernel function takes the form:
  *
- *    k(x,y) = delta_x,y * sn2 + \sum_i delta_x[kr(i)],y[kr(i)] * ks2[i]
+ *    k(x,y) = delta_x,y * sn + \sum_i delta_x[kr(i)],y[kr(i)] * ks[i]
  *
  *  where kr(i) is the i-th element of 'kronecker_indexes' (representing an
- *  index into the input vectors), and ks2[i] is the exp of twice the value of
- *  the i-th element of the 'log_kronecker_sigma' option.
+ *  index into the input vectors), and ks[i]=softplus(isp_kronecker_sigma[i]).
  *
  *  Note that to make its operations more robust when used with unconstrained
  *  optimization of hyperparameters, all hyperparameters of this kernel are
- *  specified in the log-domain.
+ *  specified in the inverse softplus domain, hence the 'isp' prefix.  This is
+ *  used in preference to the log-domain used by Rasmussen and Williams in
+ *  their implementation of gaussian processes, due to numerical stability.
+ *  (It may happen that the optimizer jumps 'too far' along one hyperparameter
+ *  and this causes the Gram matrix to become extremely ill-conditioned.)
  */
 class IIDNoiseKernel : public MemoryCachedKernel
 {
@@ -79,16 +82,16 @@
 public:
     //#####  Public Build Options  ############################################
 
-    //! Log of the global noise variance.  Default value=0.0
-    real m_log_noise_sigma;
+    //! Inverse softplus of the global noise variance.  Default value=0.0
+    real m_isp_noise_sigma;
 
     //! Element index in the input vectors that should be subject to additional
     //! Kronecker delta terms
     TVec<int> m_kronecker_indexes;
 
-    //! Log of the noise variance terms for the Kronecker deltas associated
-    //! with kronecker_indexes
-    Vec m_log_kronecker_sigma;
+    //! Inverse softplus of the noise variance terms for the Kronecker deltas
+    //! associated with kronecker_indexes
+    Vec m_isp_kronecker_sigma;
     
 public:
     //#####  Public Member Functions  #########################################
@@ -133,7 +136,7 @@
     void computeGramMatrixDerivKronecker(Mat& KD, int arg) const;
     
 protected:
-    //! Buffer for exponential of m_log_kronecker_sigma
+    //! Buffer for softplus of m_isp_kronecker_sigma
     mutable Vec m_kronecker_sigma;
     
 private:

Modified: trunk/plearn/ker/RationalQuadraticARDKernel.cc
===================================================================
--- trunk/plearn/ker/RationalQuadraticARDKernel.cc	2007-02-23 21:47:48 UTC (rev 6679)
+++ trunk/plearn/ker/RationalQuadraticARDKernel.cc	2007-02-24 05:52:11 UTC (rev 6680)
@@ -51,20 +51,21 @@
     "Similar to C.E. Rasmussen's GPML code (see http://www.gaussianprocess.org),\n"
     "this kernel is specified as:\n"
     "\n"
-    "  k(x,y) = sf2 * [1 + (sum_i (x_i - y_i)^2 / w_i)/(2*alpha)]^(-alpha) + k_iid(x,y)\n"
+    "  k(x,y) = sf * [1 + (sum_i (x_i - y_i)^2 / w_i)/(2*alpha)]^(-alpha) + k_iid(x,y)\n"
     "\n"
-    "where sf2 is the exp of twice the 'log_signal_sigma' option, w_i is\n"
-    "exp(2*log_global_sigma + 2*log_input_sigma[i]), and k_iid(x,y) is the\n"
-    "result of IIDNoiseKernel kernel evaluation.\n"
+    "where sf is softplus(isp_signal_sigma), w_i is softplus(isp_global_sigma +\n"
+    "isp_input_sigma[i]), and k_iid(x,y) is the result of IIDNoiseKernel kernel\n"
+    "evaluation.\n"
     "\n"
     "Note that to make its operations more robust when used with unconstrained\n"
-    "optimizaiton of hyperparameters, all hyperparameters of this kernel are\n"
-    "specified in the log-domain.\n"
+    "optimization of hyperparameters, all hyperparameters of this kernel are\n"
+    "specified in the inverse softplus domain.  See IIDNoiseKernel for more\n"
+    "explanations.\n"
     );
 
 
 RationalQuadraticARDKernel::RationalQuadraticARDKernel()
-    : m_log_alpha(0.0)
+    : m_isp_alpha(0.0)
 { }
 
 
@@ -73,10 +74,10 @@
 void RationalQuadraticARDKernel::declareOptions(OptionList& ol)
 {
     declareOption(
-        ol, "log_alpha",
-        &RationalQuadraticARDKernel::m_log_alpha,
+        ol, "isp_alpha",
+        &RationalQuadraticARDKernel::m_isp_alpha,
         OptionBase::buildoption,
-        "Log of the alpha parameter in the rational-quadratic kernel.\n"
+        "Inverse softplus of the alpha parameter in the rational-quadratic kernel.\n"
         "Default value=0.0");
 
     // Now call the parent class' declareOptions
@@ -116,29 +117,29 @@
 real RationalQuadraticARDKernel::evaluate(const Vec& x1, const Vec& x2) const
 {
     PLASSERT( x1.size() == x2.size() );
-    PLASSERT( !m_log_input_sigma.size() || x1.size() == m_log_input_sigma.size() );
+    PLASSERT( !m_isp_input_sigma.size() || x1.size() == m_isp_input_sigma.size() );
 
     if (x1.size() == 0)
-        return exp(2*m_log_signal_sigma) + inherited::evaluate(x1,x2);
+        return softplus(m_isp_signal_sigma) + inherited::evaluate(x1,x2);
     
     const real* px1 = x1.data();
     const real* px2 = x2.data();
-    real sf2        = exp(2*m_log_signal_sigma);
-    real alpha      = exp(m_log_alpha);
+    real sf         = softplus(m_isp_signal_sigma);
+    real alpha      = softplus(m_isp_alpha);
     real sum_wt     = 0.0;
     real sum_sqdiff = 0.0;
     
-    if (m_log_input_sigma.size() > 0) {
-        const real* pinpsig = m_log_input_sigma.data();
+    if (m_isp_input_sigma.size() > 0) {
+        const real* pinpsig = m_isp_input_sigma.data();
         for (int i=0, n=x1.size() ; i<n ; ++i) {
             real diff   = *px1++ - *px2++;
             real sqdiff = diff * diff;
             sum_sqdiff += sqdiff;
-            sum_wt     += sqdiff / exp(2*(m_log_global_sigma + *pinpsig++));
+            sum_wt     += sqdiff / softplus(m_isp_global_sigma + *pinpsig++);
         }
     }
     else {
-        real global_sigma = exp(2*m_log_global_sigma);
+        real global_sigma = softplus(m_isp_global_sigma);
         for (int i=0, n=x1.size() ; i<n ; ++i) {
             real diff   = *px1++ - *px2++;
             real sqdiff = diff * diff;
@@ -149,7 +150,7 @@
 
     // We add the noise covariance as well
     real noise_cov = inherited::evaluate(x1,x2);
-    return sf2 * pow(1 + sum_wt / (2.*alpha), -alpha) + noise_cov;
+    return sf * pow(1 + sum_wt / (2.*alpha), -alpha) + noise_cov;
 }
 
 
@@ -157,7 +158,7 @@
 
 void RationalQuadraticARDKernel::computeGramMatrix(Mat K) const
 {
-    PLASSERT( !m_log_input_sigma.size() || dataInputsize() == m_log_input_sigma.size() );
+    PLASSERT( !m_isp_input_sigma.size() || dataInputsize() == m_isp_input_sigma.size() );
     PLASSERT( K.size() == 0 || m_data_cache.size() > 0 );  // Ensure data cached OK
 
     // Compute IID noise gram matrix and save it
@@ -166,14 +167,14 @@
     m_noise_gram_cache << K;
 
     // Precompute some terms
-    real sf2   = exp(2*m_log_signal_sigma);
-    real alpha = exp(m_log_alpha);
+    real sf    = softplus(m_isp_signal_sigma);
+    real alpha = softplus(m_isp_alpha);
     m_input_sigma.resize(dataInputsize());
-    m_input_sigma.fill(m_log_global_sigma);
-    if (m_log_input_sigma.size() > 0)
-        m_input_sigma += m_log_input_sigma;
-    m_input_sigma *= 2.0;
-    exp(m_input_sigma, m_input_sigma);
+    m_input_sigma.fill(m_isp_global_sigma);
+    if (m_isp_input_sigma.size() > 0)
+        m_input_sigma += m_isp_input_sigma;
+    for (int i=0, n=m_input_sigma.size() ; i<n ; ++i)
+        m_input_sigma[i] = softplus(m_input_sigma[i]);
 
     // Prepare the cache for the pow terms
     m_pow_minus_alpha_minus_1.resize(K.length(), K.width());
@@ -227,7 +228,7 @@
 
             real inner_pow   = 1 + sum_wt / (2.*alpha);
             real pow_alpha   = pow(inner_pow, -alpha);
-            real Kij_cur     = sf2 * pow_alpha;
+            real Kij_cur     = sf * pow_alpha;
             *pow_cache_cur++ = Kij_cur / inner_pow;
             
             // Update kernel matrix (already pre-filled with IID noise terms)
@@ -247,36 +248,40 @@
 void RationalQuadraticARDKernel::computeGramMatrixDerivative(
     Mat& KD, const string& kernel_param, real epsilon) const
 {
-    static const string LSS("log_signal_sigma");
-    static const string LGS("log_global_sigma");
-    static const string LIS("log_input_sigma[");
-    static const string LAL("log_alpha");
+    static const string ISS("isp_signal_sigma");
+    static const string IGS("isp_global_sigma");
+    static const string IIS("isp_input_sigma[");
+    static const string IAL("isp_alpha");
 
-    if (kernel_param == LSS) {
+    if (kernel_param == ISS) {
         computeGramMatrixDerivNV<
             RationalQuadraticARDKernel,
-            &RationalQuadraticARDKernel::derivLogSignalSigma>(KD, this, -1);
+            &RationalQuadraticARDKernel::derivIspSignalSigma>(KD, this, -1);
     }
-    else if (kernel_param == LGS) {
+    else if (kernel_param == IGS) {
         computeGramMatrixDerivNV<
             RationalQuadraticARDKernel,
-            &RationalQuadraticARDKernel::derivLogGlobalSigma>(KD, this, -1);
+            &RationalQuadraticARDKernel::derivIspGlobalSigma>(KD, this, -1);
     }
-    else if (string_begins_with(kernel_param, LIS) &&
+    else if (string_begins_with(kernel_param, IIS) &&
              kernel_param[kernel_param.size()-1] == ']')
     {
         int arg = tolong(kernel_param.substr(
-                             LIS.size(), kernel_param.size() - LIS.size() - 1));
-        PLASSERT( arg < m_log_input_sigma.size() );
+                             IIS.size(), kernel_param.size() - IIS.size() - 1));
+        PLASSERT( arg < m_isp_input_sigma.size() );
 
-        computeGramMatrixDerivLogInputSigma(KD, arg);
+        computeGramMatrixDerivIspInputSigma(KD, arg);
 
         // computeGramMatrixDerivNV<
         //     RationalQuadraticARDKernel,
-        //     &RationalQuadraticARDKernel::derivLogInputSigma>(KD, this, arg);
+        //     &RationalQuadraticARDKernel::derivIspInputSigma>(KD, this, arg);
     }
-    else if (kernel_param == LAL) {
-        computeGramMatrixDerivLogAlpha(KD);
+    else if (kernel_param == IAL) {
+        computeGramMatrixDerivIspAlpha(KD);
+
+        // computeGramMatrixDerivNV<
+        //     RationalQuadraticARDKernel,
+        //     &RationalQuadraticARDKernel::derivIspAlpha>(KD, this, -1);
     }
     else
         inherited::computeGramMatrixDerivative(KD, kernel_param, epsilon);
@@ -285,81 +290,86 @@
     // Mat KD1;
     // Kernel::computeGramMatrixDerivative(KD1, kernel_param, epsilon);
     // cerr << "Kernel hyperparameter: " << kernel_param << endl;
-    // cerr << "Analytic derivative (1st row):" << endl
-    //      << KD(0) << endl
+    // cerr << "Analytic derivative (200th row):" << endl
+    //      << KD(200) << endl
     //      << "Finite differences:" << endl
-    //      << KD1(0) << endl;
+    //      << KD1(200) << endl;
 }
 
 
-//#####  derivLogSignalSigma  #################################################
+//#####  derivIspSignalSigma  #################################################
 
-real RationalQuadraticARDKernel::derivLogSignalSigma(int i, int j, int arg, real K) const
+real RationalQuadraticARDKernel::derivIspSignalSigma(int i, int j, int arg, real K) const
 {
     real noise = m_noise_gram_cache(i,j);
-    return 2*(K-noise);
+    return (K-noise)*sigmoid(m_isp_signal_sigma)/softplus(m_isp_signal_sigma);
 }
 
 
-//#####  derivLogGlobalSigma  #################################################
+//#####  derivIspGlobalSigma  #################################################
 
-real RationalQuadraticARDKernel::derivLogGlobalSigma(int i, int j, int arg, real K) const
+real RationalQuadraticARDKernel::derivIspGlobalSigma(int i, int j, int arg, real K) const
 {
     // The rational quadratic gives us:
-    //     K = exp(2*s)*k^(-alpha).
-    // Rederive the value of k
-    real alpha = exp(m_log_alpha);
+    //     K = s*k^(-alpha).
+    // Rederive the value of k == (K/s)^(-1/alpha)
+    real alpha = softplus(m_isp_alpha);
     real noise = m_noise_gram_cache(i,j);
     K -= noise;
-    real k     = exp(- (pl_log(K) - 2*m_log_signal_sigma) / alpha);
-    real inner = - (k - 1) * alpha;
-    return -0.5 * (K / k) * inner;
+    real k     = pow(K / softplus(m_isp_signal_sigma), -1. / alpha);
+    real inner = (k - 1) * alpha * sigmoid(m_isp_global_sigma) / softplus(m_isp_global_sigma);
+    return (K / k) * inner;
 }
 
 
-//#####  derivLogInputSigma  ##################################################
+//#####  derivIspInputSigma  ##################################################
 
 // This function computes the derivative element-wise.  The function actually
-// used now is computeGramMatrixDerivLogInputSigma, which computes the whole
+// used now is computeGramMatrixDerivIspInputSigma, which computes the whole
 // matrix much faster.
-real RationalQuadraticARDKernel::derivLogInputSigma(int i, int j, int arg, real K) const
+real RationalQuadraticARDKernel::derivIspInputSigma(int i, int j, int arg, real K) const
 {
     // The rational quadratic gives us:
-    //     K = exp(2*s)*k^(-alpha).
-    // Rederive the value of k
+    //     K = s*k^(-alpha).
+    // Rederive the value of k == (K/s)^(-1/alpha)
+    real alpha   = softplus(m_isp_alpha);
     Vec& row_i   = *dataRow(i);
     Vec& row_j   = *dataRow(j);
-    real K_over_k= m_pow_minus_alpha_minus_1(i,j);
+    real noise   = m_noise_gram_cache(i,j);
+    K -= noise;
+    real k       = pow(K / softplus(m_isp_signal_sigma), -1. / alpha);
     real diff    = row_i[arg] - row_j[arg];
     real sq_diff = diff * diff;
-    return K_over_k * exp(-2 * (m_log_global_sigma + m_log_input_sigma[arg])) * sq_diff;
+    real inner   = m_isp_global_sigma + m_isp_input_sigma[arg];
+    real sig_inn = sigmoid(inner);
+    real spl_inn = softplus(inner);
+    return 0.5 * (K / k) * sig_inn * sq_diff / (spl_inn * spl_inn);
 }
 
 
-//#####  derivLogAlpha  #######################################################
+//#####  derivIspAlpha  #######################################################
 
-real RationalQuadraticARDKernel::derivLogAlpha(int i, int j, int arg, real K) const
+real RationalQuadraticARDKernel::derivIspAlpha(int i, int j, int arg, real K) const
 {
-    real alpha = exp(m_log_alpha);
+    real alpha = softplus(m_isp_alpha);
     real noise = m_noise_gram_cache(i,j);
-    K -= noise;
-    real k     = exp(- (pl_log(K) - 2*m_log_signal_sigma) / alpha);
-    real left  = - alpha * pl_log(k);
-    real num   = (k - 1) * 2 * alpha;
-    real denum = 2 * k;
-    return K * (left + num / denum);
+    K         -= noise;
+    real k     = pow(K / softplus(m_isp_signal_sigma), -1. / alpha);
+    return sigmoid(m_isp_alpha) * K * (1 - pl_log(k) - 1 / k);
 }
 
 
-//#####  computeGramMatrixDerivLogInputSigma  #################################
+//#####  computeGramMatrixDerivIspInputSigma  #################################
 
-void RationalQuadraticARDKernel::computeGramMatrixDerivLogInputSigma(Mat& KD,
+void RationalQuadraticARDKernel::computeGramMatrixDerivIspInputSigma(Mat& KD,
                                                                      int arg) const
 {
     // Precompute some terms
     real input_sigma_arg = m_input_sigma[arg];
+    real input_sigma_sq  = input_sigma_arg * input_sigma_arg;
+    real input_sigmoid   = sigmoid(m_isp_global_sigma + m_isp_input_sigma[arg]);
     
-    // Compute Gram Matrix derivative w.r.t. log_input_sigma[arg]
+    // Compute Gram Matrix derivative w.r.t. isp_input_sigma[arg]
     int  l = data->length();
 
     // Variables that walk over the data matrix
@@ -392,7 +402,8 @@
         {
             real diff    = *xi - *xj;
             real sq_diff = diff * diff;
-            real KD_cur  = *pow_cache_cur * sq_diff / input_sigma_arg;
+            real KD_cur  = 0.5 * *pow_cache_cur *
+                           input_sigmoid * sq_diff / input_sigma_sq;
 
             // Set into derivative matrix
             *KDij++ = KD_cur;
@@ -401,16 +412,16 @@
 }
 
 
-//#####  computeGramMatrixDerivLogAlpha  ######################################
+//#####  computeGramMatrixDerivIspAlpha  ######################################
 
-void RationalQuadraticARDKernel::computeGramMatrixDerivLogAlpha(Mat& KD) const
+void RationalQuadraticARDKernel::computeGramMatrixDerivIspAlpha(Mat& KD) const
 {
     // Precompute some terms
-    real alpha = exp(m_log_alpha);
+    real alpha_sigmoid = sigmoid(m_isp_alpha);
     
-    // Compute Gram Matrix derivative w.r.t. log_alpha
-    int  l = data->length();
-    int  k_mod     = gram_matrix.mod();
+    // Compute Gram Matrix derivative w.r.t. isp_alpha
+    int  l     = data->length();
+    int  k_mod = gram_matrix.mod();
 
     // Variables that walk over the pre-computed kernel matrix (K) 
     real *Ki = &gram_matrix(0,0);            // Current row of kernel matrix
@@ -448,10 +459,7 @@
         {
             real K      = *Kij - *noise_cache_cur;
             real k      = K / *pow_cache_cur;
-            real left   = -alpha * pl_log(k);
-            real num    = (k - 1) * 2. * alpha;
-            real denum  = 2. * k;
-            real KD_cur = K * (left + num / denum);
+            real KD_cur = alpha_sigmoid * K * (1 - pl_log(k) - 1/k);
             
             // Set into derivative matrix
             *KDij++ = KD_cur;

Modified: trunk/plearn/ker/RationalQuadraticARDKernel.h
===================================================================
--- trunk/plearn/ker/RationalQuadraticARDKernel.h	2007-02-23 21:47:48 UTC (rev 6679)
+++ trunk/plearn/ker/RationalQuadraticARDKernel.h	2007-02-24 05:52:11 UTC (rev 6680)
@@ -54,15 +54,16 @@
  *  Similar to C.E. Rasmussen's GPML code (see http://www.gaussianprocess.org),
  *  this kernel is specified as:
  *
- *    k(x,y) = sf2 * [1 + (sum_i (x_i - y_i)^2 / w_i)/(2*alpha)]^(-alpha) + k_iid(x,y)
+ *    k(x,y) = sf * [1 + (sum_i (x_i - y_i)^2 / w_i)/(2*alpha)]^(-alpha) + k_iid(x,y)
  *
- *  where sf2 is the exp of twice the 'log_signal_sigma' option, w_i is
- *  exp(2*log_global_sigma + 2*log_input_sigma[i]), and k_iid(x,y) is the
- *  result of IIDNoiseKernel kernel evaluation.
+ *  where sf is softplus(isp_signal_sigma), w_i is softplus(isp_global_sigma +
+ *  isp_input_sigma[i]), and k_iid(x,y) is the result of IIDNoiseKernel kernel
+ *  evaluation.
  *
  *  Note that to make its operations more robust when used with unconstrained
- *  optimizaiton of hyperparameters, all hyperparameters of this kernel are
- *  specified in the log-domain.
+ *  optimization of hyperparameters, all hyperparameters of this kernel are
+ *  specified in the inverse softplus domain.  See IIDNoiseKernel for more
+ *  explanations.
  */
 class RationalQuadraticARDKernel : public ARDBaseKernel
 {
@@ -71,9 +72,9 @@
 public:
     //#####  Public Build Options  ############################################
 
-    //! Log of the alpha parameter in the rational-quadratic kernel.
+    //! Inverse softplus of the alpha parameter in the rational-quadratic kernel.
     //! Default value=0.0
-    real m_log_alpha;
+    real m_isp_alpha;
 
 public:
     //#####  Public Member Functions  #########################################
@@ -112,23 +113,23 @@
     //! Declares the class options.
     static void declareOptions(OptionList& ol);
 
-    //! Derivative function with respect to log_signal_sigma
-    real derivLogSignalSigma(int i, int j, int arg, real K) const;
+    //! Derivative function with respect to isp_signal_sigma
+    real derivIspSignalSigma(int i, int j, int arg, real K) const;
 
-    //! Derivative function with respect to log_global_sigma
-    real derivLogGlobalSigma(int i, int j, int arg, real K) const;
+    //! Derivative function with respect to isp_global_sigma
+    real derivIspGlobalSigma(int i, int j, int arg, real K) const;
     
-    //! Derivative function with respect to log_input_sigma[arg]
-    real derivLogInputSigma(int i, int j, int arg, real K) const;
+    //! Derivative function with respect to isp_input_sigma[arg]
+    real derivIspInputSigma(int i, int j, int arg, real K) const;
     
-    //! Derivative function with respect to log_alpha
-    real derivLogAlpha(int i, int j, int arg, real K) const;
+    //! Derivative function with respect to isp_alpha
+    real derivIspAlpha(int i, int j, int arg, real K) const;
 
-    // Compute derivative w.r.t. log_input_sigma[arg] for WHOLE MATRIX
-    void computeGramMatrixDerivLogInputSigma(Mat& KD, int arg) const;
+    // Compute derivative w.r.t. isp_input_sigma[arg] for WHOLE MATRIX
+    void computeGramMatrixDerivIspInputSigma(Mat& KD, int arg) const;
     
-    // Compute derivative w.r.t. log_alpha for WHOLE MATRIX
-    void computeGramMatrixDerivLogAlpha(Mat& KD) const;
+    // Compute derivative w.r.t. isp_alpha for WHOLE MATRIX
+    void computeGramMatrixDerivIspAlpha(Mat& KD) const;
     
 protected:
     //! Cached version of IID noise gram matrix



From chapados at mail.berlios.de  Sat Feb 24 17:00:52 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Sat, 24 Feb 2007 17:00:52 +0100
Subject: [Plearn-commits] r6681 - trunk/commands
Message-ID: <200702241600.l1OG0qq9028523@sheep.berlios.de>

Author: chapados
Date: 2007-02-24 17:00:52 +0100 (Sat, 24 Feb 2007)
New Revision: 6681

Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
Temporarily disable SquaredEponential kernel do allow plearn to compile

Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2007-02-24 05:52:11 UTC (rev 6680)
+++ trunk/commands/plearn_noblas_inc.h	2007-02-24 16:00:52 UTC (rev 6681)
@@ -111,7 +111,7 @@
 #include <plearn/ker/NegOutputCostFunction.h>
 //#include <plearn/ker/PolynomialKernel.h>
 #include <plearn/ker/RationalQuadraticARDKernel.h>
-#include <plearn/ker/SquaredExponentialARDKernel.h>
+// #include <plearn/ker/SquaredExponentialARDKernel.h>
 #include <plearn/ker/ThresholdedKernel.h>
 #include <plearn/ker/VMatKernel.h>
 



From chapados at mail.berlios.de  Sat Feb 24 17:21:18 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Sat, 24 Feb 2007 17:21:18 +0100
Subject: [Plearn-commits] r6682 - in
	trunk/plearn_learners/regressors/test/GaussianProcessRegressor: .
	.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results
Message-ID: <200702241621.l1OGLIsU029494@sheep.berlios.de>

Author: chapados
Date: 2007-02-24 17:21:16 +0100 (Sat, 24 Feb 2007)
New Revision: 6682

Modified:
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/learner_hyperopt.plearn
Log:
New test results

Modified: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2007-02-24 16:00:52 UTC (rev 6681)
+++ trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2007-02-24 16:21:16 UTC (rev 6682)
@@ -4,14 +4,14 @@
 !R 0 
 !R 1 GaussianProcessRegressor(
 kernel = *1 ->RationalQuadraticARDKernel(
-log_alpha = 1.04547586264741277 ;
-log_signal_sigma = 1.78885963703595285 ;
-log_global_sigma = 0 ;
-log_input_sigma = 1 [ 1.60502566389644263 ] ;
-log_noise_sigma = -1.08496084417144156 ;
+isp_alpha = 14.8530952696338776 ;
+isp_signal_sigma = 29.6219285856948247 ;
+isp_global_sigma = 0 ;
+isp_input_sigma = 1 [ 22.2544311481394068 ] ;
+isp_noise_sigma = -1.86446658049698821 ;
 kronecker_indexes = []
 ;
-log_kronecker_sigma = []
+isp_kronecker_sigma = []
 ;
 cache_threshold = 1000000 ;
 is_symmetric = 1 ;
@@ -25,8 +25,8 @@
 include_bias = 1 ;
 compute_confidence = 1 ;
 confidence_epsilon = 1.00000000000000002e-08 ;
-hyperparameters = 3 [ ("log_signal_sigma" , "0.0" )("log_noise_sigma" , "0.0" )("log_alpha" , "0.0" )] ;
-ARD_hyperprefix_initval = ("log_input_sigma" , "0.0" );
+hyperparameters = 3 [ ("isp_signal_sigma" , "0.0" )("isp_noise_sigma" , "0.0" )("isp_alpha" , "0.0" )] ;
+ARD_hyperprefix_initval = ("isp_input_sigma" , "0.0" );
 optimizer = *2 ->ConjGradientOptimizer(
 verbosity = 1 ;
 expected_red = 1 ;
@@ -41,20 +41,21 @@
 minibatch_n_line_searches = 3 ;
 nstages = 1  )
 ;
+save_gram_matrix = 0 ;
 alpha = 5  1  [ 
--1.44801815670686862 	
--1.75870009663165083 	
-2.97790711086592319 	
-0.366193237472126842 	
--0.252937647804560684 	
+-1.11770047929343908 	
+-1.44398600686711642 	
+2.34477475121424828 	
+0.359163702824522091 	
+-0.273169390814828383 	
 ]
 ;
 gram_inverse = 5  5  [ 
-2.37504560445266355 	0.318202770254115641 	-2.79501461899015302 	0.183693875492433906 	-0.0157606163667142161 	
-0.318202770254115308 	2.91650706588821063 	-3.00531379641274965 	-0.300764955773723752 	0.0237240838912132698 	
--2.79501461899015258 	-3.00531379641274965 	5.75002897142370362 	0.0451407441539973669 	-0.00106974501214030199 	
-0.183693875492433878 	-0.300764955773723752 	0.0451407441539973669 	0.105748937745051619 	-0.0132961086237992979 	
--0.0157606163667142196 	0.0237240838912132768 	-0.00106974501214030242 	-0.0132961086237992996 	0.0298730633663767363 	
+2.17384303486338926 	-0.0428570226335798429 	-2.24278913027230908 	0.211187944943547978 	-0.0160008498421014035 	
+-0.042857022633580058 	2.63229682421500977 	-2.36514023157865649 	-0.310986674741217162 	0.0223156036853405411 	
+-2.24278913027230908 	-2.36514023157865605 	4.57667695971973743 	0.0265284039079978357 	-8.51433618825349326e-05 	
+0.211187944943548006 	-0.310986674741217162 	0.0265284039079978426 	0.113894354038488216 	-0.0105928862093286214 	
+-0.0160008498421014035 	0.0223156036853405376 	-8.51433618825353527e-05 	-0.0105928862093286214 	0.034633631441663322 	
 ]
 ;
 target_mean = 1 [ 10 ] ;
@@ -92,18 +93,18 @@
 nservers = 0 ;
 save_trainingset_prefix = ""  )
 
-!R 1 1 [ 15 ] 
-!R 1 1 [ 14.4027409350913036 ] 
+!R 1 1 [ 15.0000000000000036 ] 
+!R 1 1 [ 14.4446958356354838 ] 
 !R 2 4  1  [ 
-13.5000588443909812 	
-14.4259996839628197 	
-15 	
-14.4027409350913036 	
+13.4992090127011988 	
+14.4141233331847474 	
+15.0000000000000036 	
+14.4446958356354838 	
 ]
 1 [ 4  4  [ 
-0.495755869965555596 	0.296446857385134876 	5.68434188608080149e-14 	-0.61803333064101551 	
-0.296446857385205931 	0.372151235349497844 	5.68434188608080149e-14 	-0.472278178376861035 	
-1.27897692436818033e-13 	6.39488462184090167e-14 	1.00000497379915034e-08 	6.39488462184090167e-14 	
--0.618033330640987089 	-0.472278178376878799 	4.26325641456060112e-14 	2.53894706592205122 	
+0.510839430933994199 	0.283483493891058203 	-2.84217094304040074e-14 	-0.555619592261717088 	
+0.283483493891065308 	0.391896855149343415 	-2.84217094304040074e-14 	-0.369897017889257995 	
+-2.13162820728030056e-14 	-2.48689957516035065e-14 	9.99997513100424861e-09 	-2.13162820728030056e-14 	
+-0.555619592261709982 	-0.36989701788925089 	-1.77635683940025046e-14 	2.23845483383840982 	
 ]
 ] 

Modified: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/learner_hyperopt.plearn
===================================================================
--- trunk/plearn_learners/regressors/test/GaussianProcessRegressor/learner_hyperopt.plearn	2007-02-24 16:00:52 UTC (rev 6681)
+++ trunk/plearn_learners/regressors/test/GaussianProcessRegressor/learner_hyperopt.plearn	2007-02-24 16:21:16 UTC (rev 6682)
@@ -1,16 +1,16 @@
 GaussianProcessRegressor(
-    kernel                  = RationalQuadraticARDKernel(log_signal_sigma = 0.0,
-                                                         log_noise_sigma  = 0.0,
-                                                         log_alpha        = 0.0,
-                                                         log_global_sigma = 0.0,
-                                                         log_input_sigma  = [ 0.0 ]),
+    kernel                  = RationalQuadraticARDKernel(isp_signal_sigma = 0.0,
+                                                         isp_noise_sigma  = 0.0,
+                                                         isp_alpha        = 0.0,
+                                                         isp_global_sigma = 0.0,
+                                                         isp_input_sigma  = [ 0.0 ]),
     weight_decay            = 0,
     include_bias            = 1,
     compute_confidence      = 1,
-    ARD_hyperprefix_initval = ("log_input_sigma", 0.0),
-    hyperparameters         = [ ("log_signal_sigma", 0.0) ,
-                                ("log_noise_sigma",  0.0) ,
-                                ("log_alpha",        0.0) ],
+    ARD_hyperprefix_initval = ("isp_input_sigma", 0.0),
+    hyperparameters         = [ ("isp_signal_sigma", 0.0) ,
+                                ("isp_noise_sigma",  0.0) ,
+                                ("isp_alpha",        0.0) ],
     optimizer               = ConjGradientOptimizer(nstages     = 1,
                                                     sigma       = 0.1,
                                                     rho         = 0.05,



From chrish at mail.berlios.de  Mon Feb 26 21:53:52 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Mon, 26 Feb 2007 21:53:52 +0100
Subject: [Plearn-commits] r6683 - trunk/plearn/io
Message-ID: <200702262053.l1QKrqK3024319@sheep.berlios.de>

Author: chrish
Date: 2007-02-26 21:53:52 +0100 (Mon, 26 Feb 2007)
New Revision: 6683

Modified:
   trunk/plearn/io/fileutils.cc
   trunk/plearn/io/fileutils.h
Log:
* Fix race condition in implementation of force_mkdir()
* Introduce mkdir_lowlevel(), that has an API that is usable when avoiding
  race conditions matters.


Modified: trunk/plearn/io/fileutils.cc
===================================================================
--- trunk/plearn/io/fileutils.cc	2007-02-24 16:21:16 UTC (rev 6682)
+++ trunk/plearn/io/fileutils.cc	2007-02-26 20:53:52 UTC (rev 6683)
@@ -223,26 +223,34 @@
 }
 
 
+bool mkdir_lowlevel(const PPath& dirname)
+{
+    return PR_MkDir(dirname.c_str(), 0777) == PR_SUCCESS;
+}
+    
+
 /////////////////
 // force_mkdir //
 /////////////////
+// If you can't spot a race condition in the previous version of this function
+// (look in the version control history), please don't change the logic used
+// here.
 bool force_mkdir(const PPath& dirname)
 {
     if (dirname.isEmpty())
         PLERROR("In force_mkdir - Parameter 'dirname' is empty");
-    if(isdir(dirname))
-        return true;
+    
     vector<PPath> paths;
     PPath path = dirname.absolute();
     while (!path.isRoot()) {
         paths.push_back(path);
         path = path.up();
     }
+
     for (int i = int(paths.size()) - 1; i >= 0; i--)
-        if (!isdir(paths[i]) &&
-            PR_MkDir(paths[i].absolute().c_str(), 0777) != PR_SUCCESS)
-            return false;
-    return true;
+        mkdir_lowlevel(paths[i].absolute());
+
+    return isdir(dirname);
 }
 
 //////////////////////////

Modified: trunk/plearn/io/fileutils.h
===================================================================
--- trunk/plearn/io/fileutils.h	2007-02-24 16:21:16 UTC (rev 6682)
+++ trunk/plearn/io/fileutils.h	2007-02-26 20:53:52 UTC (rev 6683)
@@ -86,6 +86,16 @@
 //! Same as lsdir, except the returned entries are full paths.
 vector<PPath> lsdir_fullpath(const PPath& dirpath);
 
+/** Low-level cross-platform mkdir function, with the normal mkdir semantics. 
+ * Returns false if the directory could not be created (including because it
+ * already exists), and does not create intermediate directories along the way. 
+ *
+ * Contrast with the API of force_mkdir, which cannot be used in situations where
+ * race conditions matter, because of its "return true if the directory already
+ * exists" semantics.
+ */
+bool mkdir_lowlevel(const PPath& dirname);
+
 /*! Forces directory creation if it does not already exist. 
   (also creates any missing directory along its path).
   Return value indicates success (true) or failure (false).



From yoshua at mail.berlios.de  Mon Feb 26 23:35:30 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Mon, 26 Feb 2007 23:35:30 +0100
Subject: [Plearn-commits] r6684 - trunk/plearn_learners/online
Message-ID: <200702262235.l1QMZU2i031559@sheep.berlios.de>

Author: yoshua
Date: 2007-02-26 23:35:29 +0100 (Mon, 26 Feb 2007)
New Revision: 6684

Modified:
   trunk/plearn_learners/online/CombiningCostsModule.cc
   trunk/plearn_learners/online/CombiningCostsModule.h
   trunk/plearn_learners/online/CostModule.cc
   trunk/plearn_learners/online/CostModule.h
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/NLLCostModule.cc
   trunk/plearn_learners/online/NLLCostModule.h
   trunk/plearn_learners/online/ProcessInputCostModule.cc
   trunk/plearn_learners/online/ProcessInputCostModule.h
   trunk/plearn_learners/online/SquaredErrorCostModule.cc
   trunk/plearn_learners/online/SquaredErrorCostModule.h
Log:


Modified: trunk/plearn_learners/online/CombiningCostsModule.cc
===================================================================
--- trunk/plearn_learners/online/CombiningCostsModule.cc	2007-02-26 20:53:52 UTC (rev 6683)
+++ trunk/plearn_learners/online/CombiningCostsModule.cc	2007-02-26 22:35:29 UTC (rev 6684)
@@ -133,8 +133,10 @@
 }
 
 void CombiningCostsModule::bpropUpdate(const Vec& input, const Vec& target,
-                                       real cost, Vec& input_gradient)
+                                       real cost, Vec& input_gradient,
+                                       bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     PLASSERT( input.size() == input_size );
     PLASSERT( target.size() == target_size );
     input_gradient.resize( input_size );
@@ -167,8 +169,10 @@
 void CombiningCostsModule::bbpropUpdate(const Vec& input, const Vec& target,
                                         real cost,
                                         Vec& input_gradient,
-                                        Vec& input_diag_hessian)
+                                        Vec& input_diag_hessian,
+                                        bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bbpropUpdate cannot yet handle accumulate=false");
     PLASSERT( input.size() == input_size );
     PLASSERT( target.size() == target_size );
     input_gradient.resize( input_size );

Modified: trunk/plearn_learners/online/CombiningCostsModule.h
===================================================================
--- trunk/plearn_learners/online/CombiningCostsModule.h	2007-02-26 20:53:52 UTC (rev 6683)
+++ trunk/plearn_learners/online/CombiningCostsModule.h	2007-02-26 22:35:29 UTC (rev 6684)
@@ -81,7 +81,7 @@
     //! Adapt based on the output gradient: this method should only
     //! be called just after a corresponding fprop.
     virtual void bpropUpdate(const Vec& input, const Vec& target, real cost,
-                             Vec& input_gradient);
+                             Vec& input_gradient, bool accumulate = false);
 
     //! Calls this method on the sub_costs
     virtual void bpropUpdate(const Vec& input, const Vec& target, real cost);
@@ -89,7 +89,8 @@
     //! Similar to bpropUpdate, but adapt based also on the estimation
     //! of the diagonal of the Hessian matrix, and propagates this back.
     virtual void bbpropUpdate(const Vec& input, const Vec& target, real cost,
-                              Vec& input_gradient, Vec& input_diag_hessian);
+                              Vec& input_gradient, Vec& input_diag_hessian,
+                              bool accumulate=false);
 
     //! Calls this method on the sub_costs
     virtual void bbpropUpdate(const Vec& input, const Vec& target, real cost);

Modified: trunk/plearn_learners/online/CostModule.cc
===================================================================
--- trunk/plearn_learners/online/CostModule.cc	2007-02-26 20:53:52 UTC (rev 6683)
+++ trunk/plearn_learners/online/CostModule.cc	2007-02-26 22:35:29 UTC (rev 6684)
@@ -114,7 +114,7 @@
 
 
 void CostModule::bpropUpdate(const Vec& input, const Vec& target, real cost,
-                             Vec& input_gradient)
+                             Vec& input_gradient, bool accumulate)
 {
     // default version, calling the bpropUpdate with inherited prototype
     Vec input_and_target( input_size + target_size );
@@ -124,7 +124,7 @@
     Vec the_cost( 1, cost );
     Vec one( 1, 1 );
 
-    bpropUpdate( input_and_target, the_cost, input_and_target_gradient, one );
+    bpropUpdate( input_and_target, the_cost, input_and_target_gradient, one, accumulate );
 
     input_gradient = input_and_target_gradient.subVec( 0, input_size );
 }
@@ -148,7 +148,8 @@
 
 
 void CostModule::bbpropUpdate(const Vec& input, const Vec& target, real cost,
-                              Vec& input_gradient, Vec& input_diag_hessian)
+                              Vec& input_gradient, Vec& input_diag_hessian,
+                              bool accumulate)
 {
     // default version, calling the bpropUpdate with inherited prototype
     Vec input_and_target( input_size + target_size );
@@ -162,7 +163,8 @@
 
     bbpropUpdate( input_and_target, the_cost,
                   input_and_target_gradient, one,
-                  input_and_target_diag_hessian, zero );
+                  input_and_target_diag_hessian, zero,
+                  accumulate );
 
     input_gradient = input_and_target_gradient.subVec( 0, input_size );
     input_diag_hessian = input_and_target_diag_hessian.subVec( 0, input_size );

Modified: trunk/plearn_learners/online/CostModule.h
===================================================================
--- trunk/plearn_learners/online/CostModule.h	2007-02-26 20:53:52 UTC (rev 6683)
+++ trunk/plearn_learners/online/CostModule.h	2007-02-26 22:35:29 UTC (rev 6684)
@@ -79,7 +79,7 @@
 
     //! Adapt based on the cost gradient, and obtain the input gradient
     virtual void bpropUpdate(const Vec& input, const Vec& target, real cost,
-                             Vec& input_gradient);
+                             Vec& input_gradient, bool accumulate=false);
 
     //! Without the input gradient
     virtual void bpropUpdate(const Vec& input, const Vec& target, real cost );
@@ -93,7 +93,8 @@
     //! Similar to bpropUpdate, but adapt based also on the estimation
     //! of the diagonal of the Hessian matrix, and propagates this back.
     virtual void bbpropUpdate(const Vec& input, const Vec& target, real cost,
-                              Vec& input_gradient, Vec& input_diag_hessian );
+                              Vec& input_gradient, Vec& input_diag_hessian,
+                              bool accumulate=false);
 
     //! Without the input gradient and diag_hessian
     virtual void bbpropUpdate(const Vec& input, const Vec& target, real cost );

Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-26 20:53:52 UTC (rev 6683)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-26 22:35:29 UTC (rev 6684)
@@ -180,8 +180,8 @@
 
     declareOption(ol, "online", &DeepBeliefNet::online,
                   OptionBase::buildoption,
-                  "If true then all unsupervised training stages (as well as the fine-tuning stage)\n"
-                  "are done simultaneously.\n");
+                  "If true then all unsupervised training stages (as well as\n"
+                  "the fine-tuning stage) are done simultaneously.\n");
 
     declareOption(ol, "n_layers", &DeepBeliefNet::n_layers,
                   OptionBase::learntoption,
@@ -709,49 +709,94 @@
         cost.resize(n_layers);
 
     layers[0]->expectation << input;
-    for( int i=0 ; i<n_layers-1 ; i++ )
+    for( int i=0 ; i<n_layers ; i++ )
     {
         // mean-field fprop from layer i to layer i+1
-        connections[i]->setAsDownInput( layers[i]->expectation );
-        layers[i+1]->getAllActivations( connections[i] );
+
+        Vec input;
+        if (i==n_layers-1 && use_classification_cost) // top layer is a joint layer
+        {
+            // set the input of the joint layer 
+            Vec joint_exp = joint_layer->expectation;
+            joint_exp.subVec( 0, layers[ n_layers-2 ]->size )
+                << layers[ n_layers-2 ]->expectation;
+            fill_one_hot( joint_exp.subVec( layers[ n_layers-2 ]->size, n_classes ),
+                          (int) round(target[0]), 0., 1. );
+            classification_module->joint_connection->setAsDownInput(
+                joint_layer->expectation );
+            layers[ n_layers-1 ]->getAllActivations(
+                get_pointer( classification_module->joint_connection ) );
+        }
+        else
+        {
+            connections[i]->setAsDownInput( layers[i]->expectation );
+            layers[i+1]->getAllActivations( connections[i] );
+        }
         layers[i+1]->computeExpectation();
+
         // propagate into local cost
         if( partial_costs && partial_costs[ i ] )
+        {
             partial_costs[ i ]->fprop( layers[ i+1 ]->expectation,
                                        target, cost[i] );
 
-        if( partial_costs && partial_costs[ i ] )
-        {
-            // put appropriate learning rate
-            connections[ i ]->setLearningRate( grad_learning_rate );
-            layers[ i+1 ]->setLearningRate( grad_learning_rate );
-
             // Backward pass
-
             partial_costs[ i ]->bpropUpdate( layers[ i+1 ]->expectation,
                                              target, cost,
                                              expectation_gradients[ i+1 ] );
+            // HACK: since the update will combine the gradients from all sources,
+            // weigh the gradients according to the desired learning rates
+            expectation_gradients[i+1] *= grad_learning_rate/cd_learning_rate;
+        }
+        else
+            expectation_gradients[i+1].clear();
 
-            // YB - LOUCHE: activation n'est pas vraiment l'output du connection ni l'input de layer i+1
-            // puisque c'est l'output de connection + le biais
-            layers[ i+1 ]->bpropUpdate( layers[ i+1 ]->activation, // - biais
-                                        layers[ i+1 ]->expectation,  
-                                        activation_gradients[ i+1 ],
-                                        expectation_gradients[ i+1 ] );
+        if( i==n_layers-1 && final_cost )
+        {
+            if( final_module )
+            {
+                final_module->fprop( layers[ n_layers-1 ]->expectation,
+                                     final_cost_input );
+                final_cost->fprop( final_cost_input, target, final_cost_value );
+                final_cost->bpropUpdate( final_cost_input, target,
+                                         final_cost_value[0],
+                                         final_cost_gradient );
+                // HACK: since the update will combine the gradients from all sources,
+                // weigh the gradients according to the desired learning rates
+                final_cost_gradient *= grad_learning_rate/cd_learning_rate;
+                final_module->bpropUpdate( layers[ n_layers-1 ]->expectation,
+                                           final_cost_input,
+                                           expectation_gradients[ n_layers-1 ],
+                                           final_cost_gradient );
+            }
+            else
+            {
+                final_cost->fprop( layers[ n_layers-1 ]->expectation, target,
+                                   final_cost_value );
+                final_cost->bpropUpdate( layers[ n_layers-1 ]->expectation,
+                                         target, final_cost_value[0],
+                                         expectation_gradients[ n_layers-1 ] );
+            }
 
-            connections[ i ]->bpropUpdate( layers[ i ]->expectation,
-                                           layers[ i+1 ]->activation,  // - biais
-                                           expectation_gradients[ i ],
-                                           activation_gradients[ i+1 ] );
+            train_costs[final_cost_index] = final_cost_value[0];
 
-            // put back old learning rate
-            connections[ i ]->setLearningRate( cd_learning_rate );
-            layers[ i+1 ]->setLearningRate( cd_learning_rate );
+            layers[ n_layers-1 ]->bpropUpdate( layers[ n_layers-1 ]->activation,
+                                               layers[ n_layers-1 ]->expectation,
+                                               activation_gradients[ n_layers-1 ],
+                                               expectation_gradients[ n_layers-1 ]);
 
-            layers[i]->setLearningRate( cd_learning_rate );
-            connections[i]->setLearningRate( cd_learning_rate );
+            connections[ n_layers-2 ]->bpropUpdate(
+                layers[ n_layers-2 ]->expectation,
+                layers[ n_layers-1 ]->activation,
+                expectation_gradients[ n_layers-2 ],
+                activation_gradients[ n_layers-1 ],
+                true); // accumulate into expectation_gradients[n_layers-2]
         }
 
+
+
+    }
+        
         contrastiveDivergenceStep( layers[ i ],
                                    connections[ i ],
                                    layers[ i+1 ] );

Modified: trunk/plearn_learners/online/NLLCostModule.cc
===================================================================
--- trunk/plearn_learners/online/NLLCostModule.cc	2007-02-26 20:53:52 UTC (rev 6683)
+++ trunk/plearn_learners/online/NLLCostModule.cc	2007-02-26 22:35:29 UTC (rev 6684)
@@ -95,8 +95,9 @@
 }
 
 void NLLCostModule::bpropUpdate(const Vec& input, const Vec& target, real cost,
-                                Vec& input_gradient)
+                                Vec& input_gradient, bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     PLASSERT( input.size() == input_size );
     PLASSERT( target.size() == target_size );
     input_gradient.resize( input_size );
@@ -111,8 +112,10 @@
 
 void NLLCostModule::bbpropUpdate(const Vec& input, const Vec& target,
                                  real cost,
-                                 Vec& input_gradient, Vec& input_diag_hessian)
+                                 Vec& input_gradient, Vec& input_diag_hessian,
+                                 bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     bpropUpdate( input, target, cost, input_gradient );
 
     int the_target = (int) round( target[0] );

Modified: trunk/plearn_learners/online/NLLCostModule.h
===================================================================
--- trunk/plearn_learners/online/NLLCostModule.h	2007-02-26 20:53:52 UTC (rev 6683)
+++ trunk/plearn_learners/online/NLLCostModule.h	2007-02-26 22:35:29 UTC (rev 6684)
@@ -72,7 +72,7 @@
     //! Adapt based on the output gradient: this method should only
     //! be called just after a corresponding fprop.
     virtual void bpropUpdate(const Vec& input, const Vec& target, real cost,
-                             Vec& input_gradient);
+                             Vec& input_gradient, bool accumulate=false);
 
     //! Does nothing
     virtual void bpropUpdate(const Vec& input, const Vec& target, real cost)
@@ -82,7 +82,8 @@
     //! Similar to bpropUpdate, but adapt based also on the estimation
     //! of the diagonal of the Hessian matrix, and propagates this back.
     virtual void bbpropUpdate(const Vec& input, const Vec& target, real cost,
-                              Vec& input_gradient, Vec& input_diag_hessian);
+                              Vec& input_gradient, Vec& input_diag_hessian,
+                              bool accumulate=false);
 
     //! Does nothing
     virtual void bbpropUpdate(const Vec& input, const Vec& target, real cost)

Modified: trunk/plearn_learners/online/ProcessInputCostModule.cc
===================================================================
--- trunk/plearn_learners/online/ProcessInputCostModule.cc	2007-02-26 20:53:52 UTC (rev 6683)
+++ trunk/plearn_learners/online/ProcessInputCostModule.cc	2007-02-26 22:35:29 UTC (rev 6684)
@@ -147,8 +147,9 @@
 // bpropUpdate //
 /////////////////
 void ProcessInputCostModule::bpropUpdate(const Vec& input, const Vec& target,
-                                         real cost, Vec& input_gradient)
+                                         real cost, Vec& input_gradient, bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     PLASSERT( processing_module );
     PLASSERT( cost_module );
     PLASSERT( input.size() == input_size );
@@ -166,8 +167,9 @@
 /////////////////
 void ProcessInputCostModule::bbpropUpdate(const Vec& input, const Vec& target,
                                           real cost, Vec& input_gradient,
-                                          Vec& input_diag_hessian)
+                                          Vec& input_diag_hessian, bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bbpropUpdate cannot yet handle accumulate=false");
     PLASSERT( processing_module );
     PLASSERT( cost_module );
     PLASSERT( input.size() == input_size );

Modified: trunk/plearn_learners/online/ProcessInputCostModule.h
===================================================================
--- trunk/plearn_learners/online/ProcessInputCostModule.h	2007-02-26 20:53:52 UTC (rev 6683)
+++ trunk/plearn_learners/online/ProcessInputCostModule.h	2007-02-26 22:35:29 UTC (rev 6684)
@@ -86,7 +86,7 @@
 
     //! Adapt based on the cost, and compute input gradient to backpropagate.
     virtual void bpropUpdate(const Vec& input, const Vec& target, real cost,
-                             Vec& input_gradient);
+                             Vec& input_gradient, bool accumulate=false);
 
     /* Optional
        N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
@@ -103,7 +103,8 @@
     //! If these methods are defined, you can use them INSTEAD of
     //! bpropUpdate(...)
     virtual void bbpropUpdate(const Vec& input, const Vec& target, real cost,
-                              Vec& input_gradient, Vec& input_diag_hessian);
+                              Vec& input_gradient, Vec& input_diag_hessian,
+                              bool accumulate=false);
 
     /* Optional
        N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS,

Modified: trunk/plearn_learners/online/SquaredErrorCostModule.cc
===================================================================
--- trunk/plearn_learners/online/SquaredErrorCostModule.cc	2007-02-26 20:53:52 UTC (rev 6683)
+++ trunk/plearn_learners/online/SquaredErrorCostModule.cc	2007-02-26 22:35:29 UTC (rev 6684)
@@ -95,8 +95,10 @@
 
 
 void SquaredErrorCostModule::bpropUpdate(const Vec& input, const Vec& target,
-                                         real cost, Vec& input_gradient)
+                                         real cost, Vec& input_gradient, 
+                                         bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     PLASSERT( input.size() == input_size );
     PLASSERT( target.size() == target_size );
     input_gradient.resize( input_size );
@@ -110,9 +112,9 @@
 void SquaredErrorCostModule::bbpropUpdate(const Vec& input, const Vec& target,
                                           real cost,
                                           Vec& input_gradient,
-                                          Vec& input_diag_hessian)
+                                          Vec& input_diag_hessian, bool accumulate)
 {
-    bpropUpdate( input, target, cost, input_gradient );
+    bpropUpdate( input, target, cost, input_gradient, accumulate );
 
     input_diag_hessian.resize( input_size );
     input_diag_hessian.fill( 2. );

Modified: trunk/plearn_learners/online/SquaredErrorCostModule.h
===================================================================
--- trunk/plearn_learners/online/SquaredErrorCostModule.h	2007-02-26 20:53:52 UTC (rev 6683)
+++ trunk/plearn_learners/online/SquaredErrorCostModule.h	2007-02-26 22:35:29 UTC (rev 6684)
@@ -69,7 +69,7 @@
     //! Adapt based on the output gradient: this method should only
     //! be called just after a corresponding fprop.
     virtual void bpropUpdate(const Vec& input, const Vec& target, real cost,
-                             Vec& input_gradient);
+                             Vec& input_gradient, bool accumulate=false);
 
     //! Does nothing
     virtual void bpropUpdate(const Vec& input, const Vec& target, real cost)
@@ -79,7 +79,8 @@
     //! Similar to bpropUpdate, but adapt based also on the estimation
     //! of the diagonal of the Hessian matrix, and propagates this back.
     virtual void bbpropUpdate(const Vec& input, const Vec& target, real cost,
-                              Vec& input_gradient, Vec& input_diag_hessian);
+                              Vec& input_gradient, Vec& input_diag_hessian, 
+                              bool accumulate=false);
 
     //! Does nothing
     virtual void bbpropUpdate(const Vec& input, const Vec& target, real cost)



From yoshua at mail.berlios.de  Mon Feb 26 23:39:58 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Mon, 26 Feb 2007 23:39:58 +0100
Subject: [Plearn-commits] r6685 - trunk/plearn_learners/online
Message-ID: <200702262239.l1QMdw1k031988@sheep.berlios.de>

Author: yoshua
Date: 2007-02-26 23:39:58 +0100 (Mon, 26 Feb 2007)
New Revision: 6685

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
Log:
Oops. Previous commit was wrong!


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-26 22:35:29 UTC (rev 6684)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-26 22:39:58 UTC (rev 6685)
@@ -704,6 +704,7 @@
 
 void DeepBeliefNet::onlineStep( const Vec& input, const Vec& target)
 {
+#if 0
     Vec cost;
     if (partial_costs)
         cost.resize(n_layers);
@@ -803,6 +804,7 @@
 
     }
     // fprop in joint layer
+#endif
     
 }
 



From yoshua at mail.berlios.de  Tue Feb 27 03:35:24 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Tue, 27 Feb 2007 03:35:24 +0100
Subject: [Plearn-commits] r6686 - trunk/plearn_learners/online
Message-ID: <200702270235.l1R2ZOxn029913@sheep.berlios.de>

Author: yoshua
Date: 2007-02-27 03:35:23 +0100 (Tue, 27 Feb 2007)
New Revision: 6686

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
Log:


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-26 22:39:58 UTC (rev 6685)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-27 02:35:23 UTC (rev 6686)
@@ -547,7 +547,7 @@
         {
             int sample = stage % nsamples;
             train_set->getExample(sample, input, target, weight);
-            onlineStep( input, target );
+            onlineStep( input, target, train_costs );
             if( pb )
                 pb->update( stage + 1 );
         }
@@ -702,20 +702,18 @@
     train_stats->finalize();
 }
 
-void DeepBeliefNet::onlineStep( const Vec& input, const Vec& target)
+void DeepBeliefNet::onlineStep( const Vec& input, const Vec& target, Vec& train_costs)
 {
-#if 0
     Vec cost;
     if (partial_costs)
         cost.resize(n_layers);
 
     layers[0]->expectation << input;
-    for( int i=0 ; i<n_layers ; i++ )
+    for( int i=0 ; i<n_layers-1 ; i++ )
     {
         // mean-field fprop from layer i to layer i+1
-
         Vec input;
-        if (i==n_layers-1 && use_classification_cost) // top layer is a joint layer
+        if (i==n_layers-1 && use_classification_cost) // if top layer is a joint layer
         {
             // set the input of the joint layer 
             Vec joint_exp = joint_layer->expectation;
@@ -725,17 +723,19 @@
                           (int) round(target[0]), 0., 1. );
             classification_module->joint_connection->setAsDownInput(
                 joint_layer->expectation );
+            // this does the actual matrix-vector computation
             layers[ n_layers-1 ]->getAllActivations(
                 get_pointer( classification_module->joint_connection ) );
         }
         else
         {
             connections[i]->setAsDownInput( layers[i]->expectation );
+            // this does the actual matrix-vector computation
             layers[i+1]->getAllActivations( connections[i] );
         }
         layers[i+1]->computeExpectation();
 
-        // propagate into local cost
+        // propagate into local cost associated to this layer
         if( partial_costs && partial_costs[ i ] )
         {
             partial_costs[ i ]->fprop( layers[ i+1 ]->expectation,
@@ -744,15 +744,14 @@
             // Backward pass
             partial_costs[ i ]->bpropUpdate( layers[ i+1 ]->expectation,
                                              target, cost,
+                                             // first time we set these gradients: do not accumulate
                                              expectation_gradients[ i+1 ] );
-            // HACK: since the update will combine the gradients from all sources,
-            // weigh the gradients according to the desired learning rates
-            expectation_gradients[i+1] *= grad_learning_rate/cd_learning_rate;
         }
         else
             expectation_gradients[i+1].clear();
 
-        if( i==n_layers-1 && final_cost )
+        // top layer may be connected to a final_module followed by a final_cost
+        if( i==n_layers-2 && final_cost )
         {
             if( final_module )
             {
@@ -761,14 +760,11 @@
                 final_cost->fprop( final_cost_input, target, final_cost_value );
                 final_cost->bpropUpdate( final_cost_input, target,
                                          final_cost_value[0],
-                                         final_cost_gradient );
-                // HACK: since the update will combine the gradients from all sources,
-                // weigh the gradients according to the desired learning rates
-                final_cost_gradient *= grad_learning_rate/cd_learning_rate;
+                                         final_cost_gradient ); //gradient on final_cost_input
                 final_module->bpropUpdate( layers[ n_layers-1 ]->expectation,
                                            final_cost_input,
                                            expectation_gradients[ n_layers-1 ],
-                                           final_cost_gradient );
+                                           final_cost_gradient, true );
             }
             else
             {
@@ -776,9 +772,16 @@
                                    final_cost_value );
                 final_cost->bpropUpdate( layers[ n_layers-1 ]->expectation,
                                          target, final_cost_value[0],
-                                         expectation_gradients[ n_layers-1 ] );
+                                         expectation_gradients[ n_layers-1 ],
+                                         true);
             }
 
+            // HACK: since the update will combine the gradients from all sources,
+            // weigh the gradients according to the desired learning rates
+            expectation_gradients[i+1] *= grad_learning_rate/cd_learning_rate;
+
+// HERE
+
             train_costs[final_cost_index] = final_cost_value[0];
 
             layers[ n_layers-1 ]->bpropUpdate( layers[ n_layers-1 ]->activation,
@@ -798,14 +801,10 @@
 
     }
         
-        contrastiveDivergenceStep( layers[ i ],
-                                   connections[ i ],
-                                   layers[ i+1 ] );
+    //contrastiveDivergenceStep( layers[ i ],
+    //                           connections[ i ],
+    //                           layers[ i+1 ] );
 
-    }
-    // fprop in joint layer
-#endif
-    
 }
 
 void DeepBeliefNet::greedyStep( const Vec& input, const Vec& target, int index )

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2007-02-26 22:39:58 UTC (rev 6685)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2007-02-27 02:35:23 UTC (rev 6686)
@@ -188,7 +188,7 @@
     virtual TVec<std::string> getTrainCostNames() const;
 
 
-    void onlineStep( const Vec& input, const Vec& target );
+    void onlineStep( const Vec& input, const Vec& target, Vec& train_costs );
 
     void greedyStep( const Vec& input, const Vec& target, int index );
 



From yoshua at mail.berlios.de  Tue Feb 27 03:38:50 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Tue, 27 Feb 2007 03:38:50 +0100
Subject: [Plearn-commits] r6687 - trunk
Message-ID: <200702270238.l1R2co63030096@sheep.berlios.de>

Author: yoshua
Date: 2007-02-27 03:38:49 +0100 (Tue, 27 Feb 2007)
New Revision: 6687

Modified:
   trunk/pymake.config.model
Log:


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2007-02-27 02:35:23 UTC (rev 6686)
+++ trunk/pymake.config.model	2007-02-27 02:38:49 UTC (rev 6687)
@@ -110,7 +110,7 @@
         cpp_definitions += [ 'DARWIN', 'LITTLEENDIAN' ]
     else:
         cpp_definitions += [ 'DARWIN', 'BIGENDIAN' ]
-        compileflags += ' -fno-coalesce'
+        # compileflags += ' -fno-coalesce' # cc1plus says unrecognized commands
 elif platform=='win32' :
     cpp_definitions += [ 'WIN32', '_MINGW_', 'LITTLEENDIAN' ]
     # When using the standard Win32 Python (instead of the built-in Cygwin



From lamblin at mail.berlios.de  Tue Feb 27 07:47:29 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 27 Feb 2007 07:47:29 +0100
Subject: [Plearn-commits] r6688 - trunk/plearn_learners/online
Message-ID: <200702270647.l1R6lTlY017891@sheep.berlios.de>

Author: lamblin
Date: 2007-02-27 07:47:27 +0100 (Tue, 27 Feb 2007)
New Revision: 6688

Modified:
   trunk/plearn_learners/online/BackConvolution2DModule.cc
   trunk/plearn_learners/online/ClassErrorCostModule.cc
   trunk/plearn_learners/online/CombiningCostsModule.cc
   trunk/plearn_learners/online/CombiningCostsModule.h
   trunk/plearn_learners/online/Convolution2DModule.cc
   trunk/plearn_learners/online/CostModule.cc
   trunk/plearn_learners/online/GradNNetLayerModule.cc
   trunk/plearn_learners/online/GradNNetLayerModule.h
   trunk/plearn_learners/online/ModuleStackModule.cc
   trunk/plearn_learners/online/NLLCostModule.cc
   trunk/plearn_learners/online/ProcessInputCostModule.cc
   trunk/plearn_learners/online/RBMBinomialLayer.cc
   trunk/plearn_learners/online/RBMClassificationModule.cc
   trunk/plearn_learners/online/RBMClassificationModule.h
   trunk/plearn_learners/online/RBMConv2DConnection.cc
   trunk/plearn_learners/online/RBMGaussianLayer.cc
   trunk/plearn_learners/online/RBMLayer.h
   trunk/plearn_learners/online/RBMMatrixConnection.cc
   trunk/plearn_learners/online/RBMMatrixConnection.h
   trunk/plearn_learners/online/RBMMixedConnection.cc
   trunk/plearn_learners/online/RBMMixedLayer.cc
   trunk/plearn_learners/online/RBMMixedLayer.h
   trunk/plearn_learners/online/RBMMultinomialLayer.cc
   trunk/plearn_learners/online/RBMTruncExpLayer.cc
   trunk/plearn_learners/online/SoftmaxModule.cc
   trunk/plearn_learners/online/SquaredErrorCostModule.cc
   trunk/plearn_learners/online/SquaredErrorCostModule.h
   trunk/plearn_learners/online/Subsampling2DModule.cc
   trunk/plearn_learners/online/Supersampling2DModule.cc
   trunk/plearn_learners/online/TanhModule.cc
Log:
Implements bpropUpdate(..., accumulate) in classes deriving from
OnlineLearningModule.
Note that it should not be possible to accumulate a gradient into a Vec
that does not have the right size.



Modified: trunk/plearn_learners/online/BackConvolution2DModule.cc
===================================================================
--- trunk/plearn_learners/online/BackConvolution2DModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/BackConvolution2DModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -376,7 +376,6 @@
                                           const Vec& output_gradient,
                                           bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     // Check size
     if( input.size() != input_size )
         PLERROR("BackConvolution2DModule::bpropUpdate: input.size() should"
@@ -393,7 +392,13 @@
                 "equal to output_size (%i != %i).\n",
                 output_gradient.size(), output_size);
 
-    input_gradient.resize(input_size);
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+    }
+    else
+        input_gradient.resize(input_size);
 
     // Since fprop() has just been called, we assume that input_images and
     // output_images are up-to-date
@@ -418,7 +423,8 @@
                 backConvolve2Dbackprop( kernels(i,j), input_images[i],
                                         input_gradients[i],
                                         output_gradients[j], kernel_gradient,
-                                        kernel_step1, kernel_step2, false );
+                                        kernel_step1, kernel_step2,
+                                        accumulate );
 
                 // kernel(i,j) -= learning_rate * kernel_gradient
                 multiplyAcc( kernels(i,j), kernel_gradient, -learning_rate );
@@ -495,7 +501,6 @@
                                            const Vec& output_diag_hessian,
                                            bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bbpropUpdate cannot yet handle accumulate=false");
     // This version forwards the second order information, but does not
     // actually use it for the update.
 
@@ -505,8 +510,16 @@
                 " output_diag_hessian.size()\n"
                 "should be equal to output_size (%i != %i).\n",
                 output_diag_hessian.size(), output_size);
-    input_diag_hessian.resize(input_size);
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_diag_hessian.size() == input_size,
+                      "Cannot resize input_diag_hessian AND accumulate into it"
+                    );
+    }
+    else
+        input_diag_hessian.resize(input_size);
+
     // Make input_diag_hessians and output_diag_hessians point to the right
     // places
     for( int i=0 ; i<n_input_images ; i++ )
@@ -529,11 +542,11 @@
 
                 convolve2D( output_diag_hessians[j], squared_kernel,
                             input_diag_hessians[i],
-                            kernel_step1, kernel_step2, false );
+                            kernel_step1, kernel_step2, accumulate );
             }
 
     // Call bpropUpdate()
-    bpropUpdate( input, output, input_gradient, output_gradient );
+    bpropUpdate( input, output, input_gradient, output_gradient, accumulate );
 }
 
 

Modified: trunk/plearn_learners/online/ClassErrorCostModule.cc
===================================================================
--- trunk/plearn_learners/online/ClassErrorCostModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/ClassErrorCostModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -124,7 +124,8 @@
 /* Not supposed to happen
 void ClassErrorCostModule::bpropUpdate(const Vec& input, const Vec& target,
                                        real cost,
-                                       Vec& input_gradient)
+                                       Vec& input_gradient,
+                                       bool accumulate=false)
 {
 }
 */
@@ -141,7 +142,8 @@
 void ClassErrorCostModule::bbpropUpdate(const Vec& input, const Vec& target,
                                        real cost,
                                        Vec& input_gradient,
-                                       Vec& input_diag_hessian)
+                                       Vec& input_diag_hessian,
+                                       bool accumulate=false)
 {
 }
 */

Modified: trunk/plearn_learners/online/CombiningCostsModule.cc
===================================================================
--- trunk/plearn_learners/online/CombiningCostsModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/CombiningCostsModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -136,23 +136,41 @@
                                        real cost, Vec& input_gradient,
                                        bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     PLASSERT( input.size() == input_size );
     PLASSERT( target.size() == target_size );
-    input_gradient.resize( input_size );
-    input_gradient.clear();
 
-    Vec partial_gradient;
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+    }
+    else
+    {
+        input_gradient.resize( input_size );
+        input_gradient.clear();
+    }
+
     for( int i=0 ; i<n_sub_costs ; i++ )
     {
-        if( cost_weights[i] != 0. )
+        if( cost_weights[i] == 0. )
         {
+            // Don't compute input_gradient
+            sub_costs[i]->bpropUpdate( input, target, sub_costs_values[i] );
+        }
+        else if( cost_weights[i] == 1. )
+        {
+            // Accumulate directly into input_gradient
             sub_costs[i]->bpropUpdate( input, target, sub_costs_values[i],
-                                       partial_gradient );
+                                       input_gradient, true );
+        }
+        else
+        {
+            // Put the result into partial_gradient, then accumulate into
+            // input_gradient with the appropriate weight
+            sub_costs[i]->bpropUpdate( input, target, sub_costs_values[i],
+                                       partial_gradient, false );
             multiplyAcc( input_gradient, partial_gradient, cost_weights[i] );
         }
-        else
-            sub_costs[i]->bpropUpdate( input, target, sub_costs_values[i] );
     }
 }
 
@@ -172,23 +190,50 @@
                                         Vec& input_diag_hessian,
                                         bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bbpropUpdate cannot yet handle accumulate=false");
     PLASSERT( input.size() == input_size );
     PLASSERT( target.size() == target_size );
-    input_gradient.resize( input_size );
-    input_gradient.clear();
-    input_diag_hessian.resize( input_size );
-    input_diag_hessian.clear();
 
-    Vec partial_gradient;
-    Vec partial_diag_hessian;
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+        PLASSERT_MSG( input_diag_hessian.size() == input_size,
+                      "Cannot resize input_diag_hessian AND accumulate into it"
+                    );
+    }
+    else
+    {
+        input_gradient.resize( input_size );
+        input_gradient.clear();
+        input_diag_hessian.resize( input_size );
+        input_diag_hessian.clear();
+    }
+
     for( int i=0 ; i<n_sub_costs ; i++ )
     {
-        sub_costs[i]->bbpropUpdate( input, target, sub_costs_values[i],
-                                    partial_gradient, partial_diag_hessian );
-        multiplyAcc( input_gradient, partial_gradient, sub_costs_values[i] );
-        multiplyAcc( input_diag_hessian, partial_diag_hessian,
-                     sub_costs_values[i] );
+        if( cost_weights[i] == 0. )
+        {
+            // Don't compute input_gradient nor input_diag_hessian
+            sub_costs[i]->bbpropUpdate( input, target, sub_costs_values[i] );
+        }
+        else if( cost_weights[i] == 1. )
+        {
+            // Accumulate directly into input_gradient and input_diag_hessian
+            sub_costs[i]->bbpropUpdate( input, target, sub_costs_values[i],
+                                        input_gradient, input_diag_hessian,
+                                        true );
+        }
+        else
+        {
+            // Put temporary results into partial_*, then multiply and add to
+            // input_*
+            sub_costs[i]->bbpropUpdate( input, target, sub_costs_values[i],
+                                        partial_gradient, partial_diag_hessian,
+                                        false );
+            multiplyAcc( input_gradient, partial_gradient, cost_weights[i] );
+            multiplyAcc( input_diag_hessian, partial_diag_hessian,
+                         cost_weights[i] );
+        }
     }
 }
 
@@ -199,7 +244,7 @@
     PLASSERT( target.size() == target_size );
 
     for( int i=0 ; i<n_sub_costs ; i++ )
-        sub_costs[i]->bpropUpdate( input, target, sub_costs_values[i] );
+        sub_costs[i]->bbpropUpdate( input, target, sub_costs_values[i] );
 }
 
 

Modified: trunk/plearn_learners/online/CombiningCostsModule.h
===================================================================
--- trunk/plearn_learners/online/CombiningCostsModule.h	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/CombiningCostsModule.h	2007-02-27 06:47:27 UTC (rev 6688)
@@ -81,7 +81,7 @@
     //! Adapt based on the output gradient: this method should only
     //! be called just after a corresponding fprop.
     virtual void bpropUpdate(const Vec& input, const Vec& target, real cost,
-                             Vec& input_gradient, bool accumulate = false);
+                             Vec& input_gradient, bool accumulate=false);
 
     //! Calls this method on the sub_costs
     virtual void bpropUpdate(const Vec& input, const Vec& target, real cost);
@@ -141,6 +141,12 @@
 
     //! Stores the output values of the sub_costs
     mutable Vec sub_costs_values;
+
+    //! Stores intermediate values of the input gradient
+    mutable Vec partial_gradient;
+
+    //! Stores intermediate values of the input diagonal of Hessian
+    mutable Vec partial_diag_hessian;
 };
 
 // Declares a few other classes and functions related to this class

Modified: trunk/plearn_learners/online/Convolution2DModule.cc
===================================================================
--- trunk/plearn_learners/online/Convolution2DModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/Convolution2DModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -383,7 +383,6 @@
                                       const Vec& output_gradient,
                                       bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     // Check size
     if( input.size() != input_size )
         PLERROR("Convolution2DModule::bpropUpdate: input.size() should be\n"
@@ -398,7 +397,13 @@
                 "equal to output_size (%i != %i).\n",
                 output_gradient.size(), output_size);
 
-    input_gradient.resize(input_size);
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+    }
+    else
+        input_gradient.resize(input_size);
 
     // Since fprop() has just been called, we assume that input_images and
     // output_images are up-to-date
@@ -423,7 +428,7 @@
                 convolve2Dbackprop( input_images[i], kernels(i,j),
                                     output_gradients[j],
                                     input_gradients[i], kernel_gradient,
-                                    kernel_step1, kernel_step2, false );
+                                    kernel_step1, kernel_step2, accumulate );
 
                 // kernel(i,j) -= learning_rate * kernel_gradient
                 multiplyAcc( kernels(i,j), kernel_gradient, -learning_rate );
@@ -502,8 +507,6 @@
                                        const Vec& output_diag_hessian,
                                        bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bbpropUpdate cannot yet handle accumulate=false");
-
     // This version forwards the second order information, but does not
     // actually use it for the update.
 
@@ -513,8 +516,16 @@
                 "\n"
                 "should be equal to output_size (%i != %i).\n",
                 output_diag_hessian.size(), output_size);
-    input_diag_hessian.resize(input_size);
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_diag_hessian.size() == input_size,
+                      "Cannot resize input_diag_hessian AND accumulate into it"
+                    );
+    }
+    else
+        input_diag_hessian.resize(input_size);
+
     // Make input_diag_hessians and output_diag_hessians point to the right
     // places
     for( int i=0 ; i<n_input_images ; i++ )
@@ -537,11 +548,11 @@
 
                 backConvolve2D( input_diag_hessians[i], squared_kernel,
                                 output_diag_hessians[j],
-                                kernel_step1, kernel_step2, false );
+                                kernel_step1, kernel_step2, accumulate );
             }
 
     // Call bpropUpdate()
-    bpropUpdate( input, output, input_gradient, output_gradient );
+    bpropUpdate( input, output, input_gradient, output_gradient, accumulate );
 }
 
 

Modified: trunk/plearn_learners/online/CostModule.cc
===================================================================
--- trunk/plearn_learners/online/CostModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/CostModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -39,7 +39,9 @@
 
 
 #include "CostModule.h"
+#include <plearn/math/TMat_maths.h>
 
+
 namespace PLearn {
 using namespace std;
 
@@ -124,9 +126,16 @@
     Vec the_cost( 1, cost );
     Vec one( 1, 1 );
 
-    bpropUpdate( input_and_target, the_cost, input_and_target_gradient, one, accumulate );
+    bpropUpdate( input_and_target, the_cost, input_and_target_gradient, one );
 
-    input_gradient = input_and_target_gradient.subVec( 0, input_size );
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+        input_gradient += input_and_target_gradient.subVec( 0, input_size );
+    }
+    else
+        input_gradient = input_and_target_gradient.subVec( 0, input_size );
 }
 
 void CostModule::bpropUpdate(const Vec& input, const Vec& target, real cost)
@@ -140,10 +149,9 @@
                              const Vec& output_gradient,
                              bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
-
     inherited::bpropUpdate( input_and_target, output,
-                            input_and_target_gradient, output_gradient );
+                            input_and_target_gradient, output_gradient,
+                            accumulate );
 }
 
 
@@ -166,8 +174,24 @@
                   input_and_target_diag_hessian, zero,
                   accumulate );
 
-    input_gradient = input_and_target_gradient.subVec( 0, input_size );
-    input_diag_hessian = input_and_target_diag_hessian.subVec( 0, input_size );
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+        PLASSERT_MSG( input_diag_hessian.size() == input_size,
+                      "Cannot resize input_diag_hessian AND accumulate into it"
+                    );
+
+        input_gradient += input_and_target_gradient.subVec( 0, input_size );
+        input_diag_hessian +=
+            input_and_target_diag_hessian.subVec( 0, input_size );
+    }
+    else
+    {
+        input_gradient = input_and_target_gradient.subVec( 0, input_size );
+        input_diag_hessian =
+            input_and_target_diag_hessian.subVec( 0, input_size );
+    }
 }
 
 void CostModule::bbpropUpdate(const Vec& input, const Vec& target, real cost)
@@ -184,12 +208,12 @@
                               const Vec& output_diag_hessian,
                               bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bbpropUpdate cannot yet handle accumulate=false");
     inherited::bbpropUpdate( input_and_target, output,
                              input_and_target_gradient,
                              output_gradient,
                              input_and_target_diag_hessian,
-                             output_diag_hessian );
+                             output_diag_hessian,
+                             accumulate );
 }
 
 void CostModule::forget()

Modified: trunk/plearn_learners/online/GradNNetLayerModule.cc
===================================================================
--- trunk/plearn_learners/online/GradNNetLayerModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/GradNNetLayerModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -137,7 +137,6 @@
                                       const Vec& output_gradient,
                                       bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     PLASSERT_MSG( input.size() == input_size,
                   "input.size() should be equal to this->input_size" );
     PLASSERT_MSG( output.size() == output_size,
@@ -146,8 +145,16 @@
                   "output_gradient.size() should be equal to this->output_size"
                 );
 
-    input_gradient.resize( input_size );
-    input_gradient.clear();
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+    }
+    else
+    {
+        input_gradient.resize( input_size );
+        input_gradient.clear();
+    }
 
     learning_rate = start_learning_rate / (1+decrease_constant*step_number);
 
@@ -204,9 +211,10 @@
                                        Vec&  input_gradient,
                                        const Vec& output_gradient,
                                        Vec&  input_diag_hessian,
-                                       const Vec& output_diag_hessian)
+                                       const Vec& output_diag_hessian,
+                                       bool accumulate)
 {
-    bpropUpdate( input, output, input_gradient, output_gradient );
+    bpropUpdate( input, output, input_gradient, output_gradient, accumulate );
 }
 */
 

Modified: trunk/plearn_learners/online/GradNNetLayerModule.h
===================================================================
--- trunk/plearn_learners/online/GradNNetLayerModule.h	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/GradNNetLayerModule.h	2007-02-27 06:47:27 UTC (rev 6688)
@@ -124,7 +124,8 @@
                               Vec& input_gradient,
                               const Vec& output_gradient,
                               Vec& input_diag_hessian,
-                              const Vec& output_diag_hessian);
+                              const Vec& output_diag_hessian,
+                              bool accumulate=false);
     */
 
     virtual void forget();

Modified: trunk/plearn_learners/online/ModuleStackModule.cc
===================================================================
--- trunk/plearn_learners/online/ModuleStackModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/ModuleStackModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -129,22 +129,29 @@
                                     const Vec& output_gradient,
                                     bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     PLASSERT( n_modules > 0 );
     PLASSERT( input.size() == input_size );
     PLASSERT( output.size() == output_size );
     PLASSERT( output_gradient.size() == output_size );
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+    }
+
     // bpropUpdate should be called just after the corresponding fprop,
     // so values should be up-to-date.
     modules[n_modules-1]->bpropUpdate( values[n_modules-2], output,
-                                       gradients[n_modules-2], output_gradient );
+                                       gradients[n_modules-2],
+                                       output_gradient );
 
     for( int i=n_modules-2 ; i>0 ; i-- )
         modules[i]->bpropUpdate( values[i-1], values[i],
                                  gradients[i-1], gradients[i] );
 
-    modules[0]->bpropUpdate( input, values[0], input_gradient, gradients[0] );
+    modules[0]->bpropUpdate( input, values[0], input_gradient, gradients[0],
+                             accumulate );
 }
 
 void ModuleStackModule::bpropUpdate(const Vec& input, const Vec& output,
@@ -178,13 +185,21 @@
                                      const Vec& output_diag_hessian,
                                      bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bbpropUpdate cannot yet handle accumulate=false");
     PLASSERT( n_modules > 0 );
     PLASSERT( input.size() == input_size );
     PLASSERT( output.size() == output_size );
     PLASSERT( output_gradient.size() == output_size );
     PLASSERT( output_diag_hessian.size() == output_size );
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+        PLASSERT_MSG( input_diag_hessian.size() == input_size,
+                      "Cannot resize input_diag_hessian AND accumulate into it"
+                    );
+    }
+
     // bbpropUpdate should be called just after the corresponding fprop,
     // so values should be up-to-date.
     modules[n_modules-1]->bbpropUpdate( values[n_modules-2], output,
@@ -198,7 +213,8 @@
                                   diag_hessians[i-1], diag_hessians[i] );
 
     modules[0]->bbpropUpdate( input, values[0], input_gradient, gradients[0],
-                              input_diag_hessian, diag_hessians[0] );
+                              input_diag_hessian, diag_hessians[0],
+                              accumulate );
 }
 
 void ModuleStackModule::bbpropUpdate(const Vec& input, const Vec& output,

Modified: trunk/plearn_learners/online/NLLCostModule.cc
===================================================================
--- trunk/plearn_learners/online/NLLCostModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/NLLCostModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -97,15 +97,23 @@
 void NLLCostModule::bpropUpdate(const Vec& input, const Vec& target, real cost,
                                 Vec& input_gradient, bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     PLASSERT( input.size() == input_size );
     PLASSERT( target.size() == target_size );
-    input_gradient.resize( input_size );
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+    }
+    else
+    {
+        input_gradient.resize( input_size );
+        input_gradient.clear();
+    }
+
     int the_target = (int) round( target[0] );
     // input_gradient[ i ] = 0 if i != t,
     // input_gradient[ t ] = -1/x[t]
-    input_gradient.clear();
     input_gradient[ the_target ] = - 1. / input[ the_target ];
 }
 
@@ -115,16 +123,25 @@
                                  Vec& input_gradient, Vec& input_diag_hessian,
                                  bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
-    bpropUpdate( input, target, cost, input_gradient );
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_diag_hessian.size() == input_size,
+                      "Cannot resize input_diag_hessian AND accumulate into it"
+                    );
+    }
+    else
+    {
+        input_diag_hessian.resize( input_size );
+        input_diag_hessian.clear();
+    }
 
-    int the_target = (int) round( target[0] );
-    real input_gradient_t = input_gradient[ the_target ];
     // input_diag_hessian[ i ] = 0 if i!=t
     // input_diag_hessian[ t ] = 1/(x[t])^2
-    input_diag_hessian.resize( input_size );
-    input_diag_hessian.clear();
-    input_diag_hessian[ the_target ] = input_gradient_t * input_gradient_t;
+    int the_target = (int) round( target[0] );
+    real input_t = input[ the_target ];
+    input_diag_hessian[ the_target ] += 1. / (input_t * input_t);
+
+    bpropUpdate( input, target, cost, input_gradient, accumulate );
 }
 
 TVec<string> NLLCostModule::name()

Modified: trunk/plearn_learners/online/ProcessInputCostModule.cc
===================================================================
--- trunk/plearn_learners/online/ProcessInputCostModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/ProcessInputCostModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -147,18 +147,25 @@
 // bpropUpdate //
 /////////////////
 void ProcessInputCostModule::bpropUpdate(const Vec& input, const Vec& target,
-                                         real cost, Vec& input_gradient, bool accumulate)
+                                         real cost, Vec& input_gradient,
+                                         bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     PLASSERT( processing_module );
     PLASSERT( cost_module );
     PLASSERT( input.size() == input_size );
     PLASSERT( target.size() == target_size );
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+    }
+
     cost_module->bpropUpdate( processed_value, target, cost,
                               processed_gradient );
     processing_module->bpropUpdate( input, processed_value,
-                                    input_gradient, processed_gradient );
+                                    input_gradient, processed_gradient,
+                                    accumulate );
 }
 
 
@@ -167,20 +174,30 @@
 /////////////////
 void ProcessInputCostModule::bbpropUpdate(const Vec& input, const Vec& target,
                                           real cost, Vec& input_gradient,
-                                          Vec& input_diag_hessian, bool accumulate)
+                                          Vec& input_diag_hessian,
+                                          bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bbpropUpdate cannot yet handle accumulate=false");
     PLASSERT( processing_module );
     PLASSERT( cost_module );
     PLASSERT( input.size() == input_size );
     PLASSERT( target.size() == target_size );
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+        PLASSERT_MSG( input_diag_hessian.size() == input_size,
+                      "Cannot resize input_diag_hessian AND accumulate into it"
+                    );
+    }
+
     cost_module->bbpropUpdate( processed_value, target, cost,
                                processed_gradient, processed_diag_hessian );
     processing_module->bbpropUpdate( input, processed_value,
                                      input_gradient, processed_gradient,
                                      input_diag_hessian,
-                                     processed_diag_hessian );
+                                     processed_diag_hessian,
+                                     accumulate );
 }
 
 

Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin & Dan Popovici
 
-/*! \file RBMPLayer.cc */
+/*! \file RBMBinomialLayer.cc */
 
 
 
@@ -116,7 +116,7 @@
 }
 
 void RBMBinomialLayer::fprop( const Vec& input, const Vec& rbm_bias,
-                      Vec& output ) const
+                              Vec& output ) const
 {
     PLASSERT( input.size() == input_size );
     PLASSERT( rbm_bias.size() == input_size );
@@ -131,38 +131,51 @@
                                    const Vec& output_gradient,
                                    bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     PLASSERT( input.size() == size );
     PLASSERT( output.size() == size );
     PLASSERT( output_gradient.size() == size );
-    input_gradient.resize( size );
 
-    for( int i=0 ; i<size ; i++ )
+    if( accumulate )
     {
-        real output_i = output[i];
-        input_gradient[i] = - output_i * (1-output_i) * output_gradient[i];
+        PLASSERT_MSG( input_gradient.size() == size,
+                      "Cannot resize input_gradient AND accumulate into it" );
     }
-
-    if( momentum == 0. )
+    else
     {
-        // update the bias: bias -= learning_rate * input_gradient
-        multiplyAcc( bias, input_gradient, -learning_rate );
+        input_gradient.resize( size );
+        input_gradient.clear();
     }
-    else
+
+    if( momentum != 0. )
+        bias_inc.resize( size );
+
+    for( int i=0 ; i<size ; i++ )
     {
-        bias_inc.resize( size );
-        // The update rule becomes:
-        // bias_inc = momentum * bias_inc - learning_rate * input_gradient
-        // bias += bias_inc
-        multiplyScaledAdd(input_gradient, momentum, -learning_rate, bias_inc);
-        bias += bias_inc;
+        real output_i = output[i];
+        real in_grad_i = - output_i * (1-output_i) * output_gradient[i];
+        input_gradient[i] += in_grad_i;
+
+        if( momentum == 0. )
+        {
+            // update the bias: bias -= learning_rate * input_gradient
+            bias[i] -= learning_rate * in_grad_i;
+        }
+        else
+        {
+            // The update rule becomes:
+            // bias_inc = momentum * bias_inc - learning_rate * input_gradient
+            // bias += bias_inc
+            bias_inc[i] = momentum * bias_inc[i] - learning_rate * in_grad_i;
+            bias[i] += bias_inc[i];
+        }
     }
 }
 
+//! TODO: add "accumulate" here
 void RBMBinomialLayer::bpropUpdate(const Vec& input, const Vec& rbm_bias,
-                           const Vec& output,
-                           Vec& input_gradient, Vec& rbm_bias_gradient,
-                           const Vec& output_gradient)
+                                   const Vec& output,
+                                   Vec& input_gradient, Vec& rbm_bias_gradient,
+                                   const Vec& output_gradient)
 {
     PLASSERT( input.size() == size );
     PLASSERT( rbm_bias.size() == size );
@@ -177,7 +190,7 @@
         input_gradient[i] = - output_i * (1-output_i) * output_gradient[i];
     }
 
-    rbm_bias << input_gradient;
+    rbm_bias_gradient << input_gradient;
 }
 
 real RBMBinomialLayer::fpropNLL(const Vec& target)

Modified: trunk/plearn_learners/online/RBMClassificationModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMClassificationModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/RBMClassificationModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -212,14 +212,17 @@
                                           const Vec& output_gradient,
                                           bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     // size checks
     PLASSERT( input.size() == input_size );
     PLASSERT( output.size() == output_size );
     PLASSERT( output_gradient.size() == output_size );
-    input_gradient.resize( input_size );
-    input_gradient.clear();
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+    }
+
     // bpropUpdate in target_layer,
     // assuming target_layer->activation is up-to-date, but it should be the
     // case if fprop() has been called just before.
@@ -249,7 +252,7 @@
     // at this point, the gradient can be backpropagated through
     // previous_to_last the usual way (even if output is wrong)
     previous_to_last->bpropUpdate( input, last_act,
-                                   input_gradient, d_last_act );
+                                   input_gradient, d_last_act, accumulate );
 
 }
 
@@ -273,8 +276,8 @@
 //!                  in_hess, out_hess)
 //! AND IGNORES INPUT HESSIAN AND INPUT GRADIENT.
 void RBMClassificationModule::bbpropUpdate(const Vec& input, const Vec& output,
-                                const Vec& output_gradient,
-                                const Vec& output_diag_hessian)
+                                           const Vec& output_gradient,
+                                           const Vec& output_diag_hessian)
 {
 }
 */
@@ -287,10 +290,11 @@
 //! N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
 //! RAISES A PLERROR.
 void RBMClassificationModule::bbpropUpdate(const Vec& input, const Vec& output,
-                                Vec& input_gradient,
-                                const Vec& output_gradient,
-                                Vec& input_diag_hessian,
-                                const Vec& output_diag_hessian)
+                                           Vec& input_gradient,
+                                           const Vec& output_gradient,
+                                           Vec& input_diag_hessian,
+                                           const Vec& output_diag_hessian,
+                                           bool accumulate)
 {
 }
 */

Modified: trunk/plearn_learners/online/RBMClassificationModule.h
===================================================================
--- trunk/plearn_learners/online/RBMClassificationModule.h	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/RBMClassificationModule.h	2007-02-27 06:47:27 UTC (rev 6688)
@@ -92,7 +92,7 @@
 
     // Your other public member functions go here
 
-    //! given the input, compute the output (possibly resize it  appropriately)
+    //! given the input, compute the output (possibly resize it appropriately)
     virtual void fprop(const Vec& input, Vec& output) const;
 
     //! Adapt based on the output gradient: this method should only
@@ -134,7 +134,8 @@
     //                           Vec& input_gradient,
     //                           const Vec& output_gradient,
     //                           Vec& input_diag_hessian,
-    //                           const Vec& output_diag_hessian);
+    //                           const Vec& output_diag_hessian
+    //                           bool accumulate=false);
 
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from

Modified: trunk/plearn_learners/online/RBMConv2DConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMConv2DConnection.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/RBMConv2DConnection.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -431,12 +431,18 @@
                                       const Vec& output_gradient,
                                       bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     PLASSERT( input.size() == down_size );
     PLASSERT( output.size() == up_size );
     PLASSERT( output_gradient.size() == up_size );
-    input_gradient.resize( down_size );
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == down_size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+    }
+    else
+        input_gradient.resize( down_size );
+
     down_image = input.toMat( down_image_length, down_image_width );
     up_image = output.toMat( up_image_length, up_image_width );
     down_image_gradient = input_gradient.toMat( down_image_length,
@@ -448,7 +454,7 @@
     convolve2Dbackprop( down_image, kernel,
                         up_image_gradient, down_image_gradient,
                         kernel_gradient,
-                        kernel_step1, kernel_step2, false );
+                        kernel_step1, kernel_step2, accumulate );
 
     // kernel -= learning_rate * kernel_gradient
     multiplyAcc( kernel, kernel_gradient, -learning_rate );

Modified: trunk/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -128,63 +128,71 @@
                                    const Vec& output_gradient,
                                    bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bbpropUpdate cannot yet handle accumulate=false");
     PLASSERT( input.size() == size );
     PLASSERT( output.size() == size );
     PLASSERT( output_gradient.size() == size );
-    input_gradient.resize( size );
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+    }
+    else
+    {
+        input_gradient.resize( size );
+        input_gradient.clear();
+    }
+
+    if( momentum != 0. )
+    {
+        bias_inc.resize( size );
+        //quad_coeff_inc.resize( size );
+    }
+
+    // real two_lr = 2 * learning_rate;
     for( int i=0 ; i<size ; ++i )
     {
         real a_i = quad_coeff[i];
-        input_gradient[i] = - output_gradient[i] / (2 * a_i * a_i);
-    }
+        real in_grad_i = - output_gradient[i] / (2 * a_i * a_i);
+        input_gradient[i] += in_grad_i;
 
-    if( momentum == 0. )
-    {
-        /*
-        // update the quadratic coefficient:
-        // a_i += learning_rate * out_grad_i * (b_i + input_i) / a_i^3
-        // (or a_i += 2 * learning_rate * in_grad_i * (b_i + input_i) / a_i
-        real two_lr = 2 * learning_rate;
-        for( int i=0 ; i<size ; i++ )
+        if( momentum == 0. )
         {
-            quad_coeff[i] += two_lr * input_gradient[i] * (bias[i] + input[i])
-                                                            / quad_coeff[i];
+            // bias -= learning_rate * input_gradient
+            bias[i] -= learning_rate * in_grad_i;
+
+            /* For the moment, we do not want to change the quadratic
+               coefficient during the gradient descent phase.
+
+            // update the quadratic coefficient:
+            // a_i += learning_rate * out_grad_i * (b_i + input_i) / a_i^3
+            // (or a_i += 2 * learning_rate * in_grad_i * (b_i + input_i) / a_i
+            quad_coeff[i] += two_lr * in_grad_i * (bias[i] + input[i])
+                                                    / quad_coeff[i];
             if( quad_coeff[i] < min_quad_coeff )
                 quad_coeff[i] = min_quad_coeff;
+            */
         }
-        */
+        else
+        {
+            // bias_inc = momentum * bias_inc - learning_rate * input_gradient
+            // bias += bias_inc
+            bias_inc[i] = momentum * bias_inc[i] - learning_rate * in_grad_i;
+            bias[i] += bias_inc[i];
 
-        // bias -= learning_rate * input_gradient
-        multiplyAcc( bias, input_gradient, -learning_rate );
-    }
-    else
-    {
-        bias_inc.resize( size );
-        //quad_coeff_inc.resize( size );
-
-        /*
-        // The update rule becomes:
-        // a_inc_i = momentum * a_i_inc + learning_rate * out_grad_i
-        //                                  * (b_i + input_i) / a_i^3
-        // a_i += a_inc_i
-        real two_lr = 2 * learning_rate;
-        for( int i=0 ; i<size ; i++ )
-        {
+            /*
+            // The update rule becomes:
+            // a_inc_i = momentum * a_i_inc + learning_rate * out_grad_i
+            //                                  * (b_i + input_i) / a_i^3
+            // a_i += a_inc_i
             quad_coeff_inc[i] += momentum * quad_coeff_inc[i]
-                + two_lr * input_gradient[i] * (bias[i] + input[i])
-                                                / quad_coeff[i];
+                + two_lr * in_grad_i * (bias[i] + input[i])
+                                         / quad_coeff[i];
             quad_coeff[i] += quad_coeff_inc[i];
             if( quad_coeff[i] < min_quad_coeff )
                 quad_coeff[i] = min_quad_coeff;
+            */
         }
-        */
-
-        // bias_inc = momentum * bias_inc - learning_rate * input_gradient
-        // bias += bias_inc
-        multiplyScaledAdd(input_gradient, momentum, -learning_rate, bias_inc);
-        bias += bias_inc;
     }
 }
 

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/RBMLayer.h	2007-02-27 06:47:27 UTC (rev 6688)
@@ -207,9 +207,10 @@
     Vec bias_pos_stats;
     //! Accumulates negative contribution to the gradient of bias
     Vec bias_neg_stats;
-    //! Stores the momenconst Vec& pos, const Vec& neg, tum of the gradient
+    //! Stores the momentum of the gradient
     Vec bias_inc;
 
+
     //! Count of positive examples
     int pos_count;
     //! Count of negative examples
@@ -232,6 +233,10 @@
     //#####  Private Data Members  ############################################
 
     // The rest of the private stuff goes here
+    //! Stores the computed input gradient (useful when accumulate)
+    Vec tmp_input_gradient;
+    //! Stores the computed input diag hessian (useful when accumulate)
+    Vec tmp_input_diag_hessian;
 };
 
 // Declares a few other classes and functions related to this class

Modified: trunk/plearn_learners/online/RBMMatrixConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/RBMMatrixConnection.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -276,16 +276,29 @@
 //! this version allows to obtain the input gradient as well
 void RBMMatrixConnection::bpropUpdate(const Vec& input, const Vec& output,
                                       Vec& input_gradient,
-                                      const Vec& output_gradient)
+                                      const Vec& output_gradient,
+                                      bool accumulate)
 {
     PLASSERT( input.size() == down_size );
     PLASSERT( output.size() == up_size );
     PLASSERT( output_gradient.size() == up_size );
-    input_gradient.resize( down_size );
 
-    // input_gradient = weights' * output_gradient
-    transposeProduct( input_gradient, weights, output_gradient );
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == down_size,
+                      "Cannot resize input_gradient AND accumulate into it" );
 
+        // input_gradient += weights' * output_gradient
+        transposeProductAcc( input_gradient, weights, output_gradient );
+    }
+    else
+    {
+        input_gradient.resize( down_size );
+
+        // input_gradient = weights' * output_gradient
+        transposeProduct( input_gradient, weights, output_gradient );
+    }
+
     // weights -= learning_rate * output_gradient * input'
     externalProductScaleAcc( weights, output_gradient, input, -learning_rate );
 }

Modified: trunk/plearn_learners/online/RBMMatrixConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.h	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/RBMMatrixConnection.h	2007-02-27 06:47:27 UTC (rev 6688)
@@ -127,7 +127,8 @@
     //! N.B. THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
     virtual void bpropUpdate(const Vec& input, const Vec& output,
                              Vec& input_gradient,
-                             const Vec& output_gradient);
+                             const Vec& output_gradient,
+                             bool accumulate=false);
 
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from

Modified: trunk/plearn_learners/online/RBMMixedConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMixedConnection.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/RBMMixedConnection.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -507,20 +507,27 @@
                                      const Vec& output_gradient,
                                      bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     PLASSERT( input.size() == down_size );
     PLASSERT( output.size() == up_size );
     PLASSERT( output_gradient.size() == up_size );
-    input_gradient.resize( down_size );
-    input_gradient.clear();
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == down_size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+    }
+    else
+    {
+        input_gradient.resize( down_size );
+        input_gradient.clear();
+    }
+
     for( int j=0 ; j<n_down_blocks ; j++ )
     {
         int init_j = down_init_positions[j];
         int down_size_j = down_block_sizes[j];
         Vec sub_input = input.subVec( init_j, down_size_j );
         Vec sub_input_gradient = input_gradient.subVec( init_j, down_size_j );
-        Vec part_input_gradient( down_size_j );
 
         for( int i=0 ; i<n_up_blocks ; i++ )
         {
@@ -532,10 +539,9 @@
                 Vec sub_output_gradient = output_gradient.subVec( init_i,
                                                                   up_size_i );
                 sub_connections(i,j)->bpropUpdate( sub_input, sub_output,
-                                                   part_input_gradient,
-                                                   sub_output_gradient );
-
-                sub_input_gradient += part_input_gradient;
+                                                   sub_input_gradient,
+                                                   sub_output_gradient,
+                                                   true );
             }
         }
     }

Modified: trunk/plearn_learners/online/RBMMixedLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/RBMMixedLayer.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -122,9 +122,10 @@
     {
         int begin = init_positions[i];
         int size_i = sub_layers[i]->size;
+        Vec sub_input = input.subVec(begin, size_i);
+        Vec sub_output = output.subVec(begin, size_i);
 
-        sub_layers[i]->fprop( input.subVec(begin, size_i), tmp );
-        output.subVec( begin, size_i ) << tmp;
+        sub_layers[i]->fprop( sub_input, sub_output );
     }
 }
 
@@ -139,10 +140,11 @@
     {
         int begin = init_positions[i];
         int size_i = sub_layers[i]->size;
+        Vec sub_input = input.subVec(begin, size_i);
+        Vec sub_rbm_bias = rbm_bias.subVec(begin, size_i);
+        Vec sub_output = output.subVec(begin, size_i);
 
-        sub_layers[i]->fprop( input.subVec(begin, size_i),
-                              rbm_bias.subVec(begin,size_i), tmp );
-        output.subVec( begin, size_i ) << tmp;
+        sub_layers[i]->fprop( sub_input, sub_rbm_bias, sub_output );
     }
 }
 
@@ -152,25 +154,30 @@
                                  const Vec& output_gradient,
                                  bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     PLASSERT( input.size() == size );
     PLASSERT( output.size() == size );
     PLASSERT( output_gradient.size() == size );
 
-    input_gradient.resize( size );
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+    }
+    else
+        input_gradient.resize( size );
 
     for( int i=0 ; i<n_layers ; i++ )
     {
         int begin = init_positions[i];
         int size_i = sub_layers[i]->size;
+        Vec sub_input = input.subVec( begin, size_i );
+        Vec sub_output = output.subVec( begin, size_i );
+        Vec sub_input_gradient = input_gradient.subVec( begin, size_i );
+        Vec sub_output_gradient = output_gradient.subVec( begin, size_i );
 
-        sub_layers[i]->bpropUpdate( input.subVec( begin, size_i ),
-                                    output.subVec( begin, size_i ),
-                                    tmp,
-                                    output_gradient.subVec( begin, size_i ) );
-
-        // because tmp is resizeable
-        input_gradient.subVec( begin, size_i ) << tmp;
+        sub_layers[i]->bpropUpdate( sub_input, sub_output,
+                                    sub_input_gradient, sub_output_gradient,
+                                    accumulate );
     }
 }
 
@@ -191,16 +198,16 @@
     {
         int begin = init_positions[i];
         int size_i = sub_layers[i]->size;
+        Vec sub_input = input.subVec( begin, size_i );
+        Vec sub_rbm_bias = rbm_bias.subVec( begin, size_i );
+        Vec sub_output = output.subVec( begin, size_i );
+        Vec sub_input_gradient = input_gradient.subVec( begin, size_i );
+        Vec sub_rbm_bias_gradient = rbm_bias_gradient.subVec( begin, size_i);
+        Vec sub_output_gradient = output_gradient.subVec( begin, size_i );
 
-        sub_layers[i]->bpropUpdate( input.subVec( begin, size_i ),
-                                    rbm_bias.subVec( begin, size_i),
-                                    output.subVec( begin, size_i ),
-                                    tmp, tmpb,
-                                    output_gradient.subVec( begin, size_i ) );
-
-        // because tmp and tmpb is resizeable
-        input_gradient.subVec( begin, size_i ) << tmp;
-        rbm_bias_gradient.subVec( begin, size_i) << tmpb;
+        sub_layers[i]->bpropUpdate( sub_input, sub_rbm_bias, sub_output,
+                                    sub_input_gradient, sub_rbm_bias_gradient,
+                                    sub_output_gradient );
     }
 }
 
@@ -235,9 +242,10 @@
     {
         int begin = init_positions[i];
         int size_i = sub_layers[i]->size;
-        sub_layers[i]->bpropNLL( target.subVec(begin, size_i), nlls[i],
-                                 tmpb );
-        bias_gradient.subVec(begin, size_i) << tmpb;
+
+        Vec sub_target = target.subVec(begin, size_i);
+        Vec sub_bias_gradient = bias_gradient.subVec(begin, size_i);
+        sub_layers[i]->bpropNLL( sub_target, nlls[i], sub_bias_gradient );
     }
 }
 

Modified: trunk/plearn_learners/online/RBMMixedLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.h	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/RBMMixedLayer.h	2007-02-27 06:47:27 UTC (rev 6688)
@@ -177,7 +177,7 @@
     //#####  Private Data Members  ############################################
 
     // The rest of the private stuff goes here
-    mutable Vec tmp, tmpb, nlls;
+    mutable Vec nlls;
 };
 
 // Declares a few other classes and functions related to this class

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -130,40 +130,59 @@
                                       const Vec& output_gradient,
                                       bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     PLASSERT( input.size() == size );
     PLASSERT( output.size() == size );
     PLASSERT( output_gradient.size() == size );
-    input_gradient.resize( size );
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+    }
+    else
+    {
+        input_gradient.resize( size );
+        input_gradient.clear();
+    }
+
+    if( momentum != 0. )
+        bias_inc.resize( size );
+
     // input_gradient[i] =
     //      (output_gradient . output - output_gradient[i] ) output[i]
     real outg_dot_out = dot( output_gradient, output );
     real* out = output.data();
     real* outg = output_gradient.data();
     real* ing = input_gradient.data();
+    real* b = bias.data();
+    real* binc = bias_inc.data();
+
     for( int i=0 ; i<size ; i++ )
-        ing[i] = (outg_dot_out - outg[i]) * out[i];
+    {
+        real ing_i = (outg_dot_out - outg[i]) * out[i];
+        ing[i] += ing_i;
 
-    if( momentum == 0. )
-    {
-        // update the bias: bias -= learning_rate * input_gradient
-        multiplyAcc( bias, input_gradient, -learning_rate );
+        if( momentum == 0. )
+        {
+            // update the bias: bias -= learning_rate * input_gradient
+            b[i] -= learning_rate * ing_i;
+        }
+        else
+        {
+            // The update rule becomes:
+            // bias_inc = momentum * bias_inc - learning_rate * input_gradient
+            // bias += bias_inc
+            binc[i] = momentum * binc[i] - learning_rate * ing_i;
+            b[i] += binc[i];
+        }
     }
-    else
-    {
-        bias_inc.resize( size );
-        // The update rule becomes:
-        // bias_inc = momentum * bias_inc - learning_rate * input_gradient
-        // bias += bias_inc
-        multiplyScaledAdd(input_gradient, momentum, -learning_rate, bias_inc);
-        bias += bias_inc;
-    }
 }
 
+//! TODO: add "accumulate" here
 void RBMMultinomialLayer::bpropUpdate(const Vec& input, const Vec& rbm_bias,
                                       const Vec& output,
-                                      Vec& input_gradient, Vec& rbm_bias_gradient,
+                                      Vec& input_gradient,
+                                      Vec& rbm_bias_gradient,
                                       const Vec& output_gradient)
 {
     PLASSERT( input.size() == size );
@@ -204,7 +223,8 @@
     return ret;
 }
 
-void RBMMultinomialLayer::bpropNLL(const Vec& target, real nll, Vec bias_gradient)
+void RBMMultinomialLayer::bpropNLL(const Vec& target, real nll,
+                                   Vec bias_gradient)
 {
     computeExpectation();
 

Modified: trunk/plearn_learners/online/RBMTruncExpLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMTruncExpLayer.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/RBMTruncExpLayer.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -161,47 +161,60 @@
                                    const Vec& output_gradient,
                                    bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     PLASSERT( input.size() == size );
     PLASSERT( output.size() == size );
     PLASSERT( output_gradient.size() == size );
-    input_gradient.resize( size );
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+    }
+    else
+    {
+        input_gradient.resize( size );
+        input_gradient.clear();
+    }
+
+    if( momentum != 0. )
+        bias_inc.resize( size );
+
     // df/da = exp(a)/(1-exp(a))^2 - 1/a^2
 
     for( int i=0 ; i<size ; i++ )
     {
         real a_i = input[i] + bias[i];
+        real in_grad_i;
 
         // Polynomial approximation to avoid numerical instability
         // df/da = -1/12 + a^2/240 + O(a^4)
         if( fabs( a_i ) <= 0.01 )
         {
-            input_gradient[i] = output_gradient[i] * (
-                -1./12. + a_i * a_i / 240. );
+            in_grad_i = output_gradient[i] * ( -1./12. + a_i * a_i / 240. );
         }
         else
         {
             real ea_i = exp( a_i );
-            input_gradient[i] = output_gradient[i] * (
+            in_grad_i = output_gradient[i] * (
                 ea_i/( (1 - ea_i) * (1 - ea_i) ) + 1/(a_i * a_i) );
         }
-    }
 
-    if( momentum == 0. )
-    {
-        // update the bias: bias -= learning_rate * input_gradient
-        multiplyAcc( bias, input_gradient, -learning_rate );
+        input_gradient[i] += in_grad_i;
+
+        if( momentum == 0. )
+        {
+            // update the bias: bias -= learning_rate * input_gradient
+            bias[i] -= learning_rate * in_grad_i;
+        }
+        else
+        {
+            // The update rule becomes:
+            // bias_inc = momentum * bias_inc - learning_rate * input_gradient
+            // bias += bias_inc
+            bias_inc[i] = momentum * bias_inc[i] - learning_rate * in_grad_i;
+            bias[i] += bias_inc[i];
+        }
     }
-    else
-    {
-        bias_inc.resize( size );
-        // The update rule becomes:
-        // bias_inc = momentum * bias_inc - learning_rate * input_gradient
-        // bias += bias_inc
-        multiplyScaledAdd(input_gradient, momentum, -learning_rate, bias_inc);
-        bias += bias_inc;
-    }
 }
 
 

Modified: trunk/plearn_learners/online/SoftmaxModule.cc
===================================================================
--- trunk/plearn_learners/online/SoftmaxModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/SoftmaxModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -95,16 +95,29 @@
                                 const Vec& output_gradient,
                                 bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     PLASSERT( input.size() == input_size );
     PLASSERT( output.size() == output_size );
     PLASSERT( output_gradient.size() == output_size );
-    input_gradient.resize( input_size );
 
-    // input_gradient = output_gradient * output
-    //                  - (output_gradient . output ) output
-    multiply( output_gradient, output, input_gradient );
-    multiplyAcc( input_gradient, output, -dot( output_gradient, output ) );
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+    }
+    else
+    {
+        input_gradient.resize( input_size );
+        input_gradient.clear();
+    }
+
+    // input_gradient[i] = output_gradient[i] * output[i]
+    //                  - (output_gradient . output ) output[i]
+    real outg_dot_out = dot( output_gradient, output );
+    for( int i=0 ; i<input_size ; i++ )
+    {
+        real in_grad_i = (output_gradient[i] - outg_dot_out) * output[i];
+        input_gradient[i] += in_grad_i;
+    }
 }
 
 //! reset the parameters to the state they would be BEFORE starting training.
@@ -120,7 +133,6 @@
                                  const Vec& output_diag_hessian,
                                  bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bbpropUpdate cannot yet handle accumulate=false");
     PLERROR( "Not implemented yet, please come back later or complaint to"
              " lamblinp." );
 }

Modified: trunk/plearn_learners/online/SquaredErrorCostModule.cc
===================================================================
--- trunk/plearn_learners/online/SquaredErrorCostModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/SquaredErrorCostModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -95,29 +95,51 @@
 
 
 void SquaredErrorCostModule::bpropUpdate(const Vec& input, const Vec& target,
-                                         real cost, Vec& input_gradient, 
+                                         real cost, Vec& input_gradient,
                                          bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     PLASSERT( input.size() == input_size );
     PLASSERT( target.size() == target_size );
-    input_gradient.resize( input_size );
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+    }
+    else
+    {
+        input_gradient.resize( input_size );
+        input_gradient.clear();
+    }
+
     // input_gradient = 2*(input - target)
-    substract( input, target, input_gradient );
-    input_gradient *= 2.0;
+    for( int i=0 ; i<input_size ; i++ )
+    {
+        input_gradient[i] += 2*(input[i] - target[i]);
+    }
 }
 
 
 void SquaredErrorCostModule::bbpropUpdate(const Vec& input, const Vec& target,
                                           real cost,
                                           Vec& input_gradient,
-                                          Vec& input_diag_hessian, bool accumulate)
+                                          Vec& input_diag_hessian,
+                                          bool accumulate)
 {
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_diag_hessian.size() == input_size,
+                      "Cannot resize input_diag_hessian AND accumulate into it"
+                    );
+        input_diag_hessian += 2.;
+    }
+    else
+    {
+        input_diag_hessian.resize( input_size );
+        input_diag_hessian.fill( 2. );
+    }
+
     bpropUpdate( input, target, cost, input_gradient, accumulate );
-
-    input_diag_hessian.resize( input_size );
-    input_diag_hessian.fill( 2. );
 }
 
 TVec<string> SquaredErrorCostModule::name()

Modified: trunk/plearn_learners/online/SquaredErrorCostModule.h
===================================================================
--- trunk/plearn_learners/online/SquaredErrorCostModule.h	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/SquaredErrorCostModule.h	2007-02-27 06:47:27 UTC (rev 6688)
@@ -79,7 +79,7 @@
     //! Similar to bpropUpdate, but adapt based also on the estimation
     //! of the diagonal of the Hessian matrix, and propagates this back.
     virtual void bbpropUpdate(const Vec& input, const Vec& target, real cost,
-                              Vec& input_gradient, Vec& input_diag_hessian, 
+                              Vec& input_gradient, Vec& input_diag_hessian,
                               bool accumulate=false);
 
     //! Does nothing

Modified: trunk/plearn_learners/online/Subsampling2DModule.cc
===================================================================
--- trunk/plearn_learners/online/Subsampling2DModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/Subsampling2DModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -287,7 +287,6 @@
                                       const Vec& output_gradient,
                                       bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     // Check size
     if( input.size() != input_size )
         PLERROR("Subsampling2DModule::bpropUpdate: input.size() should be\n"
@@ -302,7 +301,13 @@
                 "equal to output_size (%i != %i).\n",
                 output_gradient.size(), output_size);
 
-    input_gradient.resize(input_size);
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+    }
+    else
+        input_gradient.resize(input_size);
 
     // Since fprop() has just been called, we assume that input_images,
     // output_images and gradient are up-to-date
@@ -326,7 +331,7 @@
         convolve2Dbackprop( input_images[i], kernel,
                             output_gradients[i],
                             input_gradients[i], kernel_gradient,
-                            kernel_length, kernel_width, false );
+                            kernel_length, kernel_width, accumulate );
 
         // The scale's gradient is the sum of contributions to kernel_gradient
         scale[i] -= learning_rate * sum( kernel_gradient );
@@ -389,7 +394,6 @@
                                        const Vec& output_diag_hessian,
                                        bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bbpropUpdate cannot yet handle accumulate=false");
     // This version forwards the second order information, but does not
     // actually use it for the update.
 
@@ -399,8 +403,16 @@
                 "\n"
                 "should be equal to output_size (%i != %i).\n",
                 output_diag_hessian.size(), output_size);
-    input_diag_hessian.resize(input_size);
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_diag_hessian.size() == input_size,
+                      "Cannot resize input_diag_hessian AND accumulate into it"
+                    );
+    }
+    else
+        input_diag_hessian.resize(input_size);
+
     // Make input_diag_hessians and output_diag_hessians point to the right
     // places
     for( int i=0 ; i<n_input_images ; i++ )
@@ -421,7 +433,7 @@
         squared_kernel.fill( scale[i]*scale[i] );
         backConvolve2D( input_diag_hessians[i], squared_kernel,
                         output_diag_hessians[i],
-                        kernel_length, kernel_width, false );
+                        kernel_length, kernel_width, accumulate );
     }
 
     // Call bpropUpdate()

Modified: trunk/plearn_learners/online/Supersampling2DModule.cc
===================================================================
--- trunk/plearn_learners/online/Supersampling2DModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/Supersampling2DModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -278,7 +278,6 @@
                                         const Vec& output_gradient,
                                         bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
     // Check size
     if( input.size() != input_size )
         PLERROR("Supersampling2DModule::bpropUpdate: input.size() should be\n"
@@ -293,7 +292,13 @@
                 "equal to output_size (%i != %i).\n",
                 output_gradient.size(), output_size);
 
-    input_gradient.resize(input_size);
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+    }
+    else
+        input_gradient.resize(input_size);
 
     // Since fprop() has just been called, we assume that input_images,
     // output_images and gradient are up-to-date
@@ -317,7 +322,7 @@
         backConvolve2Dbackprop( kernel, input_images[i],
                                 input_gradients[i],
                                 output_gradients[i], kernel_gradient,
-                                kernel_length, kernel_width, false );
+                                kernel_length, kernel_width, accumulate );
 
         // The scale's gradient is the sum of contributions to kernel_gradient
         scale[i] -= learning_rate * sum( kernel_gradient );
@@ -380,7 +385,6 @@
                                          const Vec& output_diag_hessian,
                                          bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bbpropUpdate cannot yet handle accumulate=false");
     // This version forwards the second order information, but does not
     // actually use it for the update.
 
@@ -390,8 +394,16 @@
                 " output_diag_hessian.size()\n"
                 "should be equal to output_size (%i != %i).\n",
                 output_diag_hessian.size(), output_size);
-    input_diag_hessian.resize(input_size);
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_diag_hessian.size() == input_size,
+                      "Cannot resize input_diag_hessian AND accumulate into it"
+                    );
+    }
+    else
+        input_diag_hessian.resize(input_size);
+
     // Make input_diag_hessians and output_diag_hessians point to the right
     // places
     for( int i=0 ; i<n_input_images ; i++ )
@@ -412,7 +424,7 @@
         squared_kernel.fill( scale[i]*scale[i] );
         convolve2D( output_diag_hessians[i], squared_kernel,
                     input_diag_hessians[i],
-                    kernel_length, kernel_width, false );
+                    kernel_length, kernel_width, accumulate );
     }
 
     // Call bpropUpdate()

Modified: trunk/plearn_learners/online/TanhModule.cc
===================================================================
--- trunk/plearn_learners/online/TanhModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/TanhModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -111,8 +111,6 @@
                              Vec& input_gradient, const Vec& output_gradient,
                              bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bpropUpdate cannot yet handle accumulate=false");
-
     int in_size = input.size();
     int out_size = output.size();
     int og_size = output_gradient.size();
@@ -135,11 +133,21 @@
                 og_size, output_size);
     }
 
-    input_gradient.resize( input_size );
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+    }
+    else
+    {
+        input_gradient.resize( input_size );
+        input_gradient.clear();
+    }
+
     for( int i=0 ; i<input_size ; i++ )
     {
         real output_i = output[i];
-        input_gradient[i] = in_scale *
+        input_gradient[i] += in_scale *
             (ex_scale - output_i*output_i/ex_scale)*output_gradient[i];
     }
 
@@ -170,8 +178,6 @@
                               const Vec& output_diag_hessian,
                               bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,"Implementation of bbpropUpdate cannot yet handle accumulate=false");
-
     int odh_size = output_diag_hessian.size();
 
     // size check
@@ -183,24 +189,33 @@
                 odh_size, output_size);
     }
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_diag_hessian.size() == input_size,
+                      "Cannot resize input_diag_hessian AND accumulate into it"
+                    );
+    }
+    else
+    {
+        input_diag_hessian.resize( input_size );
+        input_diag_hessian.clear();
+    }
 
-    bpropUpdate( input, output, input_gradient, output_gradient );
-
-    input_diag_hessian.resize( input_size );
     for( int i=0 ; i<input_size ; i++ )
     {
         real output_i = output[i];
         real fprime_i = in_scale * (ex_scale-output_i*output_i / ex_scale);
 
         if( estimate_simpler_diag_hessian )
-            input_diag_hessian[i] =
+            input_diag_hessian[i] +=
                 fprime_i*fprime_i*output_diag_hessian[i];
         else
-            input_diag_hessian[i] =
+            input_diag_hessian[i] +=
                 fprime_i*fprime_i*output_diag_hessian[i]
                 - 2*in_scale/ex_scale*fprime_i*output_i*output_gradient[i];
     }
 
+    bpropUpdate( input, output, input_gradient, output_gradient, accumulate );
 }
 
 



From yoshua at mail.berlios.de  Tue Feb 27 16:12:37 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Tue, 27 Feb 2007 16:12:37 +0100
Subject: [Plearn-commits] r6689 - trunk/plearn_learners/online
Message-ID: <200702271512.l1RFCbOc014762@sheep.berlios.de>

Author: yoshua
Date: 2007-02-27 16:12:37 +0100 (Tue, 27 Feb 2007)
New Revision: 6689

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
Log:


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-27 06:47:27 UTC (rev 6688)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-27 15:12:37 UTC (rev 6689)
@@ -709,11 +709,12 @@
         cost.resize(n_layers);
 
     layers[0]->expectation << input;
+    // FORWARD PHASE
     for( int i=0 ; i<n_layers-1 ; i++ )
     {
         // mean-field fprop from layer i to layer i+1
         Vec input;
-        if (i==n_layers-1 && use_classification_cost) // if top layer is a joint layer
+        if (i==n_layers-2 && use_classification_cost) // if top layer is a joint layer
         {
             // set the input of the joint layer 
             Vec joint_exp = joint_layer->expectation;
@@ -724,8 +725,10 @@
             classification_module->joint_connection->setAsDownInput(
                 joint_layer->expectation );
             // this does the actual matrix-vector computation
+            // from (layer[n-2],target) to layer[n-1]
             layers[ n_layers-1 ]->getAllActivations(
                 get_pointer( classification_module->joint_connection ) );
+            // and prepares for a supervised (joint likelihood) contrastive divergence step
         }
         else
         {
@@ -735,7 +738,7 @@
         }
         layers[i+1]->computeExpectation();
 
-        // propagate into local cost associated to this layer
+        // propagate into local cost associated to output of layer i+1
         if( partial_costs && partial_costs[ i ] )
         {
             partial_costs[ i ]->fprop( layers[ i+1 ]->expectation,
@@ -751,59 +754,123 @@
             expectation_gradients[i+1].clear();
 
         // top layer may be connected to a final_module followed by a final_cost
-        if( i==n_layers-2 && final_cost )
+        // and / or may be used to predict class probabilities through a joint classification_module
+        if( i==n_layers-2)
         {
-            if( final_module )
+            if ( final_cost )
             {
-                final_module->fprop( layers[ n_layers-1 ]->expectation,
-                                     final_cost_input );
-                final_cost->fprop( final_cost_input, target, final_cost_value );
-                final_cost->bpropUpdate( final_cost_input, target,
-                                         final_cost_value[0],
-                                         final_cost_gradient ); //gradient on final_cost_input
-                final_module->bpropUpdate( layers[ n_layers-1 ]->expectation,
-                                           final_cost_input,
-                                           expectation_gradients[ n_layers-1 ],
-                                           final_cost_gradient, true );
+                if( final_module )
+                {
+                    final_module->fprop( layers[ n_layers-1 ]->expectation,
+                                         final_cost_input );
+                    final_cost->fprop( final_cost_input, target, final_cost_value );
+                    final_cost->bpropUpdate( final_cost_input, target,
+                                             final_cost_value[0],
+                                             final_cost_gradient ); //gradient on final_cost_input
+                    final_module->bpropUpdate( layers[ n_layers-1 ]->expectation,
+                                               final_cost_input,
+                                               expectation_gradients[ n_layers-1 ],
+                                               final_cost_gradient, true );
+                }
+                else
+                {
+                    final_cost->fprop( layers[ n_layers-1 ]->expectation, target,
+                                       final_cost_value );
+                    final_cost->bpropUpdate( layers[ n_layers-1 ]->expectation,
+                                             target, final_cost_value[0],
+                                             expectation_gradients[ n_layers-1 ],
+                                             true);
+                }
+
+                train_costs[final_cost_index] = final_cost_value[0];
+
+                layers[n_layers-1]->setLearningRate( grad_learning_rate );
+                connections[n_layers-2]->setLearningRate( grad_learning_rate );
+
+                layers[ n_layers-1 ]->bpropUpdate( layers[ n_layers-1 ]->activation,
+                                                   layers[ n_layers-1 ]->expectation,
+                                                   activation_gradients[ n_layers-1 ],
+                                                   expectation_gradients[ n_layers-1 ],
+                                                   false);
+
+                connections[ n_layers-2 ]->bpropUpdate(
+                    layers[ n_layers-2 ]->expectation,
+                    layers[ n_layers-1 ]->activation,
+                    expectation_gradients[ n_layers-2 ],
+                    activation_gradients[ n_layers-1 ],
+                    true); // accumulate into expectation_gradients[n_layers-2]
+                // because a partial cost may have already put a gradient there
             }
-            else
+        
+
+            if( use_classification_cost )
             {
-                final_cost->fprop( layers[ n_layers-1 ]->expectation, target,
-                                   final_cost_value );
-                final_cost->bpropUpdate( layers[ n_layers-1 ]->expectation,
-                                         target, final_cost_value[0],
-                                         expectation_gradients[ n_layers-1 ],
-                                         true);
-            }
+                classification_module->fprop( layers[ n_layers-2 ]->expectation,
+                                              class_output );
+                real nll_cost;
 
-            // HACK: since the update will combine the gradients from all sources,
-            // weigh the gradients according to the desired learning rates
-            expectation_gradients[i+1] *= grad_learning_rate/cd_learning_rate;
+                // This doesn't work. gcc bug?
+                // classification_cost->fprop( class_output, target, cost );
+                classification_cost->CostModule::fprop( class_output, target,
+                                                        nll_cost );
 
-// HERE
+                real class_error =
+                    ( argmax(class_output) == (int) round(target[0]) ) ? 0: 1;
 
-            train_costs[final_cost_index] = final_cost_value[0];
+                train_costs[nll_cost_index] = nll_cost;
+                train_costs[class_cost_index] = class_error;
 
-            layers[ n_layers-1 ]->bpropUpdate( layers[ n_layers-1 ]->activation,
-                                               layers[ n_layers-1 ]->expectation,
-                                               activation_gradients[ n_layers-1 ],
-                                               expectation_gradients[ n_layers-1 ]);
+                classification_cost->bpropUpdate( class_output, target, nll_cost,
+                                                  class_gradient );
 
-            connections[ n_layers-2 ]->bpropUpdate(
-                layers[ n_layers-2 ]->expectation,
-                layers[ n_layers-1 ]->activation,
-                expectation_gradients[ n_layers-2 ],
-                activation_gradients[ n_layers-1 ],
-                true); // accumulate into expectation_gradients[n_layers-2]
+                classification_module->bpropUpdate( layers[ n_layers-2 ]->expectation,
+                                                    class_output,
+                                                    expectation_gradients[n_layers-2],
+                                                    class_gradient,
+                                                    true );
+            }
+
+            layers[i]->setLearningRate( cd_learning_rate );
+            layers[i+1]->setLearningRate( cd_learning_rate );
+            connections[i]->setLearningRate( cd_learning_rate );
+            contrastiveDivergenceStep( layers[ i ],
+                                       connections[ i ],
+                                       layers[ i+1 ] );
         }
 
+    }
 
+    // DOWNWARD PHASE (the downward phase for top layer is already done above)
+    for( int i=n_layers-3 ; i>=0 ; i-- )
+    {
+        
+        connections[ i ]->setLearningRate( grad_learning_rate );
+        layers[ i+1 ]->setLearningRate( grad_learning_rate );
 
+        layers[i+1]->bpropUpdate( layers[i+1]->activation,
+                                  layers[i+1]->expectation,
+                                  activation_gradients[i+1],
+                                  expectation_gradients[i+1] );
+
+        connections[i]->bpropUpdate( layers[i]->expectation,
+                                     layers[i+1]->activation,
+                                     expectation_gradients[i],
+                                     activation_gradients[i+1],
+                                     true);
+
+        // N.B. the contrastiveDivergenceStep changes the activation
+        // and expectation fields of top layer of the RBM, so it must be done last
+        layers[i]->setLearningRate( cd_learning_rate );
+        layers[i+1]->setLearningRate( cd_learning_rate );
+        connections[i]->setLearningRate( cd_learning_rate );
+        save_layer_activation.resize(layers[i]->size);
+        save_layer_activation << layers[i]->activation;
+        contrastiveDivergenceStep( layers[ i ],
+                                   connections[ i ],
+                                   layers[ i+1 ] ,
+                                   true);
+        layers[i]->activation << save_layer_activation;
     }
-        
-    //contrastiveDivergenceStep( layers[ i ],
-    //                           connections[ i ],
-    //                           layers[ i+1 ] );
 
 }
 
@@ -1036,12 +1103,16 @@
 void DeepBeliefNet::contrastiveDivergenceStep(
     const PP<RBMLayer>& down_layer,
     const PP<RBMConnection>& connection,
-    const PP<RBMLayer>& up_layer )
+    const PP<RBMLayer>& up_layer,
+    bool nofprop)
 {
     // positive phase
-    connection->setAsDownInput( down_layer->expectation );
-    up_layer->getAllActivations( connection );
-    up_layer->computeExpectation();
+    if (!nofprop)
+    {
+        connection->setAsDownInput( down_layer->expectation );
+        up_layer->getAllActivations( connection );
+        up_layer->computeExpectation();
+    }
     up_layer->generateSample();
 
     // accumulate positive stats using the expectation

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2007-02-27 06:47:27 UTC (rev 6688)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2007-02-27 15:12:37 UTC (rev 6689)
@@ -199,7 +199,8 @@
 
     void contrastiveDivergenceStep( const PP<RBMLayer>& down_layer,
                                     const PP<RBMConnection>& connection,
-                                    const PP<RBMLayer>& up_layer );
+                                    const PP<RBMLayer>& up_layer,
+                                    bool nofprop=false);
 
 
     // *** SUBCLASS WRITING: ***
@@ -259,6 +260,9 @@
     //! Stores the gradient of the cost at the input of final_cost
     mutable Vec final_cost_gradient;
 
+    //! buffers bottom layer activation during onlineStep 
+    mutable Vec save_layer_activation;
+
     //! Does final_module exist and have a "learning_rate" option
     bool final_module_has_learning_rate;
 



From saintmlx at mail.berlios.de  Tue Feb 27 19:15:12 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 27 Feb 2007 19:15:12 +0100
Subject: [Plearn-commits] r6690 - in trunk: plearn/base plearn/misc
	plearn_learners/testers scripts
Message-ID: <200702271815.l1RIFCnr006595@sheep.berlios.de>

Author: saintmlx
Date: 2007-02-27 19:15:08 +0100 (Tue, 27 Feb 2007)
New Revision: 6690

Modified:
   trunk/plearn/base/ProgressBar.cc
   trunk/plearn/base/ProgressBar.h
   trunk/plearn/misc/PLearnServer.cc
   trunk/plearn/misc/PLearnServer.h
   trunk/plearn/misc/PLearnService.cc
   trunk/plearn/misc/PLearnService.h
   trunk/plearn_learners/testers/PTester.cc
   trunk/plearn_learners/testers/PTester.h
   trunk/scripts/xdispatch
Log:
- removed unhealthy casts (for x86_64)



Modified: trunk/plearn/base/ProgressBar.cc
===================================================================
--- trunk/plearn/base/ProgressBar.cc	2007-02-27 15:12:37 UTC (rev 6689)
+++ trunk/plearn/base/ProgressBar.cc	2007-02-27 18:15:08 UTC (rev 6690)
@@ -187,6 +187,17 @@
 void RemoteProgressBarPlugin::addProgressBar(ProgressBar* pb)
 { printTitle(pb); }
 
+map<ProgressBar*, unsigned int> RemoteProgressBarPlugin::pb_ids;//init
+unsigned int RemoteProgressBarPlugin::next_pb_id= 0;//init
+
+unsigned int RemoteProgressBarPlugin::getPBarID(ProgressBar* pb)
+{
+    if(pb_ids.find(pb) == pb_ids.end())
+        pb_ids[pb]= ++next_pb_id;
+    return pb_ids[pb];
+}
+
+
 void RemoteProgressBarPlugin::update(ProgressBar* pb, unsigned long newpos)
 {
     // this handles the case where we reuse the same progress bar
@@ -199,7 +210,7 @@
              int(round( pb->currentpos / (double(pb->maxpos) / nticks) ))))
     {
         out.write("*PU ");
-        out << reinterpret_cast<unsigned int>(pb) << newpos << endl;
+        out << getPBarID(pb) << newpos << endl;
         pb->currentpos = newpos;
     }
 }
@@ -208,13 +219,13 @@
 {
     string fulltitle = string(" ") + pb->title + " (" + tostring(pb->maxpos) + ") ";
     out.write("*PA ");
-    out << reinterpret_cast<unsigned int>(pb) << pb->maxpos << fulltitle << endl;
+    out << getPBarID(pb) << pb->maxpos << fulltitle << endl;
 }
 
 void RemoteProgressBarPlugin::killProgressBar(ProgressBar* pb)
 {
     out.write("*PK ");
-    out << reinterpret_cast<unsigned int>(pb) << endl;
+    out << getPBarID(pb) << endl;
 }
 
 //*************************/

Modified: trunk/plearn/base/ProgressBar.h
===================================================================
--- trunk/plearn/base/ProgressBar.h	2007-02-27 15:12:37 UTC (rev 6689)
+++ trunk/plearn/base/ProgressBar.h	2007-02-27 18:15:08 UTC (rev 6690)
@@ -105,6 +105,9 @@
     virtual void killProgressBar(ProgressBar* pb);
 
 protected:
+    static map<ProgressBar*, unsigned int> pb_ids;
+    static unsigned int next_pb_id;
+    static unsigned int getPBarID(ProgressBar* pb);
     void printTitle(ProgressBar* pb);
     unsigned int nticks;
 };

Modified: trunk/plearn/misc/PLearnServer.cc
===================================================================
--- trunk/plearn/misc/PLearnServer.cc	2007-02-27 15:12:37 UTC (rev 6689)
+++ trunk/plearn/misc/PLearnServer.cc	2007-02-27 18:15:08 UTC (rev 6690)
@@ -50,6 +50,7 @@
 #include <plearn/io/fileutils.h> //!< For chdir()
 #include <plearn/io/load_and_save.h>
 #include <plearn/io/pl_log.h>
+#include <plearn/math/random.h>
 
 #include <plearn/io/PyPLearnScript.h> // For smartLoadObject
 
@@ -264,7 +265,7 @@
                 DBG_LOG << "PLearnServer NEW OBJECT w/o ID" << endl;
                 obj = 0;
                 io >> obj;           // Read new object
-                obj_id= findFreeObjID(obj);
+                obj_id= findFreeObjID();
                 DBG_LOG << "  obj_id = " << obj_id << endl;
                 objmap[obj_id] = obj;
                 Object::prepareToSendResults(io,1);
@@ -383,19 +384,16 @@
     return true;
 }
 
-int PLearnServer::findFreeObjID(const Object* obj) const
+int PLearnServer::findFreeObjID() const
 {
-    //DUMMY method that tries to find an unused ID (LCG look-alike seeded w/ obj's address)
+    //DUMMY method that tries to find an unused ID
     // this algorithm is not guaranteed to work... use at your own risk or modify accordingly
-    int id= reinterpret_cast<unsigned int>(obj) >> 1;
-    if(id < 0) id= -id;
+    int id;
     const int maxtries= 65536;
     int ntries= 0;
-    while(objmap.find(id) != objmap.end() && ++ntries < maxtries)
-    {
-        id= id*1664525 + 1013904223;//simple "LCG"
-        if(id < 0) id= -id;
-    }
+    do
+        id= static_cast<int>(bounded_uniform(0,2000000000));
+    while(objmap.find(id) != objmap.end() && ++ntries < maxtries);
     if(ntries >= maxtries)
         PLERROR("PLearnServer::findFreeObjID : can't find a suitable ID within %d tries.", maxtries);
     return id;

Modified: trunk/plearn/misc/PLearnServer.h
===================================================================
--- trunk/plearn/misc/PLearnServer.h	2007-02-27 15:12:37 UTC (rev 6689)
+++ trunk/plearn/misc/PLearnServer.h	2007-02-27 18:15:08 UTC (rev 6690)
@@ -69,7 +69,7 @@
     bool clear_maps;
     ObjMap objmap;
 
-    virtual int findFreeObjID(const Object* obj) const;
+    virtual int findFreeObjID() const;
 
 public:
     PLearnServer(const PStream& input_output);

Modified: trunk/plearn/misc/PLearnService.cc
===================================================================
--- trunk/plearn/misc/PLearnService.cc	2007-02-27 15:12:37 UTC (rev 6689)
+++ trunk/plearn/misc/PLearnService.cc	2007-02-27 18:15:08 UTC (rev 6690)
@@ -76,7 +76,7 @@
 PLearnService::PLearnService()
 {}
 
-
+map<RemotePLearnServer*, pair<string,int> > PLearnService::servers_ids;//init.
 void PLearnService::connectToServers(PPath serversfile)
 {
     PStream in = openFile(serversfile, PStream::raw_ascii, "r");
@@ -118,6 +118,8 @@
         TVec<PP<RemotePLearnServer> > ss;
         ss.push_back(serv);
 
+        servers_ids[serv]= host_port;
+
         watchServers(ss, log_callback, progress_callback);
 
         servio << PStream::plearn_binary;
@@ -390,8 +392,9 @@
 
 void PLearnService::log_callback(PP<RemotePLearnServer> server, const string& module_name, int vlevel, const string& msg)
 { 
-    unsigned int server_id= reinterpret_cast<unsigned int>(static_cast<RemotePLearnServer*>(server));
-    PL_LOG(vlevel) << "<From server " << server_id << "> [" << module_name << "] " << msg << flush; 
+    //unsigned int server_id= reinterpret_cast<unsigned int>(static_cast<RemotePLearnServer*>(server));
+    //unsigned int server_id= getServerID(server);
+    PL_LOG(vlevel) << "<From server " << servers_ids[server] << "> [" << module_name << "] " << msg << flush; 
 }
 
 PLearnService::progress_bars_t PLearnService::progress_bars; // init
@@ -399,7 +402,8 @@
 void PLearnService::progress_callback(PP<RemotePLearnServer> server, unsigned int pbar, char action, 
                                       unsigned int pos, const string& title)
 {
-    unsigned int server_id= reinterpret_cast<unsigned int>(static_cast<RemotePLearnServer*>(server));
+    //unsigned int server_id= reinterpret_cast<unsigned int>(static_cast<RemotePLearnServer*>(server));
+    //unsigned int server_id= getServerID(server);
     static bool need_to_set_pb_plugin= true;
     if(need_to_set_pb_plugin)
     {
@@ -413,7 +417,7 @@
         if(progress_bars.find(server) == progress_bars.end())
             progress_bars[server]= map<unsigned int, PP<ProgressBar> >();
         {//local environment for 'fulltitle'... silly c++ switch/case...
-            string fulltitle= string("<server#") + tostring(server_id) 
+            string fulltitle= string("<server#") + tostring(servers_ids[server]) 
                 + ":pb#" + tostring(pbar) + "> " + title;//adjust title w/server info
             progress_bars[server][pbar]= new ProgressBar(fulltitle, pos);
         }

Modified: trunk/plearn/misc/PLearnService.h
===================================================================
--- trunk/plearn/misc/PLearnService.h	2007-02-27 15:12:37 UTC (rev 6689)
+++ trunk/plearn/misc/PLearnService.h	2007-02-27 18:15:08 UTC (rev 6690)
@@ -72,6 +72,7 @@
 
     typedef map<RemotePLearnServer*, map<unsigned int, PP<ProgressBar> > > progress_bars_t;
     static progress_bars_t progress_bars;
+    static map<RemotePLearnServer*, pair<string,int> > servers_ids;
 
 public:
     friend class RemotePLearnServer;

Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2007-02-27 15:12:37 UTC (rev 6689)
+++ trunk/plearn_learners/testers/PTester.cc	2007-02-27 18:15:08 UTC (rev 6690)
@@ -93,7 +93,8 @@
        save_test_confidence(false),
        should_train(true),
        should_test(true),
-       enforce_clean_expdir(true)
+       enforce_clean_expdir(true),
+       parallelize_here(true)
 {}
 
 PLEARN_IMPLEMENT_OBJECT(
@@ -261,6 +262,10 @@
         "loaded, so it is not empty.  In those contexts, it makes sense to allow\n"
         "this option to be false.\n");
 
+    declareOption(
+        ol, "parallelize_here", &PTester::parallelize_here, OptionBase::buildoption | OptionBase::nosave,
+        "Reserve remote servers at this level if true.");
+
     inherited::declareOptions(ol);
 }
 
@@ -1033,7 +1038,7 @@
         global_stats_vm->saveFieldInfos();
 
         split_stats_vm = new FileVMatrix(expdir / "split_stats.pmat",
-                                         0, 1 + nstats);
+                                         nsplits, 1 + nstats);
         split_stats_vm->declareField(0, "splitnum");
         for (int k = 0; k < nstats; k++)
             split_stats_vm->declareField(k+1, statspecs[k].setname + "." + statspecs[k].intstatname);
@@ -1045,9 +1050,10 @@
     TVec<PP<RemotePLearnServer> > servers= service.reserveServers(nsplits);
     int nservers= servers.length();
 
-    if(nservers > 1)
+    if(nservers > 1 && parallelize_here)
     {
         map<PP<RemotePLearnServer>, int> testers_ids;
+        map<PP<RemotePLearnServer>, int> splitnums;
         for (int splitnum= 0; splitnum < nservers && splitnum < nsplits; ++splitnum)
             servers[splitnum]->newObjectAsync(*this);
 
@@ -1064,6 +1070,7 @@
                     s->getResults(id);
                     testers_ids[s]= id;
                     s->callMethod(id, "perform1Split", splits_called);
+                    splitnums[s]= splits_called;
                     ++splits_called;
                 }
                 else
@@ -1080,7 +1087,8 @@
                 ++splits_done;
                 if (split_stats_vm)
                 {
-                    split_stats_vm->appendRow(splitres);
+                    split_stats_vm->putRow(splitnums[s],splitres);
+                    //split_stats_vm->appendRow(splitres);
                     split_stats_vm->flush();
                 }
             
@@ -1106,7 +1114,8 @@
             
             if (split_stats_vm)
             {
-                split_stats_vm->appendRow(splitres);
+                split_stats_vm->putRow(splitnum, splitres);
+                //split_stats_vm->appendRow(splitres);
                 split_stats_vm->flush();
             }
             

Modified: trunk/plearn_learners/testers/PTester.h
===================================================================
--- trunk/plearn_learners/testers/PTester.h	2007-02-27 15:12:37 UTC (rev 6689)
+++ trunk/plearn_learners/testers/PTester.h	2007-02-27 18:15:08 UTC (rev 6690)
@@ -123,6 +123,7 @@
      */
     bool enforce_clean_expdir;
      
+    bool parallelize_here;
     // ****************
     // * Constructors *
     // ****************

Modified: trunk/scripts/xdispatch
===================================================================
--- trunk/scripts/xdispatch	2007-02-27 15:12:37 UTC (rev 6689)
+++ trunk/scripts/xdispatch	2007-02-27 18:15:08 UTC (rev 6690)
@@ -39,10 +39,6 @@
     
     (options, args) = parser.parse_args()
 
-    #print options
-    #print "==="
-    #print args
-
     set_logdir('/home/saintmlx/xdispatch-log/')
 
     Task._max_load= options.max_load



From lamblin at mail.berlios.de  Tue Feb 27 20:44:14 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 27 Feb 2007 20:44:14 +0100
Subject: [Plearn-commits] r6691 - trunk/plearn_learners/online
Message-ID: <200702271944.l1RJiEZa032097@sheep.berlios.de>

Author: lamblin
Date: 2007-02-27 20:44:14 +0100 (Tue, 27 Feb 2007)
New Revision: 6691

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
Log:
Fixes and changes in online training


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-27 18:15:08 UTC (rev 6690)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-27 19:44:14 UTC (rev 6691)
@@ -58,7 +58,7 @@
     // grad_weight_decay( 0. ),
     use_classification_cost( true ),
     n_layers( 0 ),
-    online ( false ), 
+    online ( false ),
     final_module_has_learning_rate( false ),
     final_cost_has_learning_rate( false ),
     nll_cost_index( -1 ),
@@ -183,6 +183,12 @@
                   "If true then all unsupervised training stages (as well as\n"
                   "the fine-tuning stage) are done simultaneously.\n");
 
+    declareOption(ol, "top_layer_joint_cd", &DeepBeliefNet::top_layer_joint_cd,
+                  OptionBase::buildoption,
+                  "Wether we do a step of joint contrastive divergence on"
+                  " top-layer.\n"
+                  "Only used if online for the moment.\n");
+
     declareOption(ol, "n_layers", &DeepBeliefNet::n_layers,
                   OptionBase::learntoption,
                   "Number of layers");
@@ -221,7 +227,7 @@
     // Initialize some learnt variables
     n_layers = layers.length();
 
-    if( training_schedule.length() != n_layers-1  && training_schedule.length()!=0)
+    if( training_schedule.length() != n_layers-1  && !online )
     {
         MODULE_LOG << "training_schedule.length() != n_layers-1, resizing and"
             " zeroing" << endl;
@@ -537,12 +543,6 @@
             pb = new ProgressBar( "Training "+classname(),
                                   nstages - stage );
 
-        for (int i=0; i<n_layers;i++)
-        {
-            layers[i]->setLearningRate( cd_learning_rate );
-            connections[i]->setLearningRate( cd_learning_rate );
-        }
-
         for( ; stage<nstages; stage++)
         {
             int sample = stage % nsamples;
@@ -702,11 +702,12 @@
     train_stats->finalize();
 }
 
-void DeepBeliefNet::onlineStep( const Vec& input, const Vec& target, Vec& train_costs)
+void DeepBeliefNet::onlineStep( const Vec& input, const Vec& target,
+                                Vec& train_costs)
 {
     Vec cost;
     if (partial_costs)
-        cost.resize(n_layers);
+        cost.resize(n_layers-1);
 
     layers[0]->expectation << input;
     // FORWARD PHASE
@@ -714,28 +715,10 @@
     {
         // mean-field fprop from layer i to layer i+1
         Vec input;
-        if (i==n_layers-2 && use_classification_cost) // if top layer is a joint layer
-        {
-            // set the input of the joint layer 
-            Vec joint_exp = joint_layer->expectation;
-            joint_exp.subVec( 0, layers[ n_layers-2 ]->size )
-                << layers[ n_layers-2 ]->expectation;
-            fill_one_hot( joint_exp.subVec( layers[ n_layers-2 ]->size, n_classes ),
-                          (int) round(target[0]), 0., 1. );
-            classification_module->joint_connection->setAsDownInput(
-                joint_layer->expectation );
-            // this does the actual matrix-vector computation
-            // from (layer[n-2],target) to layer[n-1]
-            layers[ n_layers-1 ]->getAllActivations(
-                get_pointer( classification_module->joint_connection ) );
-            // and prepares for a supervised (joint likelihood) contrastive divergence step
-        }
-        else
-        {
-            connections[i]->setAsDownInput( layers[i]->expectation );
-            // this does the actual matrix-vector computation
-            layers[i+1]->getAllActivations( connections[i] );
-        }
+
+        connections[i]->setAsDownInput( layers[i]->expectation );
+        // this does the actual matrix-vector computation
+        layers[i+1]->getAllActivations( connections[i] );
         layers[i+1]->computeExpectation();
 
         // propagate into local cost associated to output of layer i+1
@@ -745,105 +728,124 @@
                                        target, cost[i] );
 
             // Backward pass
+            // first time we set these gradients: do not accumulate
             partial_costs[ i ]->bpropUpdate( layers[ i+1 ]->expectation,
                                              target, cost,
-                                             // first time we set these gradients: do not accumulate
                                              expectation_gradients[ i+1 ] );
         }
         else
             expectation_gradients[i+1].clear();
+    }
 
-        // top layer may be connected to a final_module followed by a final_cost
-        // and / or may be used to predict class probabilities through a joint classification_module
-        if( i==n_layers-2)
+    // top layer may be connected to a final_module followed by a
+    // final_cost and / or may be used to predict class probabilities
+    // through a joint classification_module
+
+    if ( final_cost )
+    {
+        if( final_module )
         {
-            if ( final_cost )
-            {
-                if( final_module )
-                {
-                    final_module->fprop( layers[ n_layers-1 ]->expectation,
-                                         final_cost_input );
-                    final_cost->fprop( final_cost_input, target, final_cost_value );
-                    final_cost->bpropUpdate( final_cost_input, target,
-                                             final_cost_value[0],
-                                             final_cost_gradient ); //gradient on final_cost_input
-                    final_module->bpropUpdate( layers[ n_layers-1 ]->expectation,
-                                               final_cost_input,
-                                               expectation_gradients[ n_layers-1 ],
-                                               final_cost_gradient, true );
-                }
-                else
-                {
-                    final_cost->fprop( layers[ n_layers-1 ]->expectation, target,
-                                       final_cost_value );
-                    final_cost->bpropUpdate( layers[ n_layers-1 ]->expectation,
-                                             target, final_cost_value[0],
-                                             expectation_gradients[ n_layers-1 ],
-                                             true);
-                }
+            final_module->fprop( layers[ n_layers-1 ]->expectation,
+                                 final_cost_input );
+            final_cost->fprop( final_cost_input, target,
+                               final_cost_value );
+            final_cost->bpropUpdate( final_cost_input, target,
+                                     final_cost_value[0],
+                                     final_cost_gradient );
 
-                train_costs[final_cost_index] = final_cost_value[0];
+            final_module->bpropUpdate(
+                                      layers[ n_layers-1 ]->expectation,
+                                      final_cost_input,
+                                      expectation_gradients[ n_layers-1 ],
+                                      final_cost_gradient, true );
+        }
+        else
+        {
+            final_cost->fprop( layers[ n_layers-1 ]->expectation,
+                               target,
+                               final_cost_value );
+            final_cost->bpropUpdate( layers[ n_layers-1 ]->expectation,
+                                     target, final_cost_value[0],
+                                     expectation_gradients[n_layers-1],
+                                     true);
+        }
 
-                layers[n_layers-1]->setLearningRate( grad_learning_rate );
-                connections[n_layers-2]->setLearningRate( grad_learning_rate );
+        train_costs[final_cost_index] = final_cost_value[0];
 
-                layers[ n_layers-1 ]->bpropUpdate( layers[ n_layers-1 ]->activation,
-                                                   layers[ n_layers-1 ]->expectation,
-                                                   activation_gradients[ n_layers-1 ],
-                                                   expectation_gradients[ n_layers-1 ],
-                                                   false);
+        layers[n_layers-1]->setLearningRate( grad_learning_rate );
+        connections[n_layers-2]->setLearningRate( grad_learning_rate );
 
-                connections[ n_layers-2 ]->bpropUpdate(
-                    layers[ n_layers-2 ]->expectation,
-                    layers[ n_layers-1 ]->activation,
-                    expectation_gradients[ n_layers-2 ],
-                    activation_gradients[ n_layers-1 ],
-                    true); // accumulate into expectation_gradients[n_layers-2]
-                // because a partial cost may have already put a gradient there
-            }
-        
+        layers[ n_layers-1 ]->bpropUpdate( layers[ n_layers-1 ]->activation,
+                                           layers[ n_layers-1 ]->expectation,
+                                           activation_gradients[ n_layers-1 ],
+                                           expectation_gradients[ n_layers-1 ],
+                                           false);
 
-            if( use_classification_cost )
-            {
-                classification_module->fprop( layers[ n_layers-2 ]->expectation,
-                                              class_output );
-                real nll_cost;
+        connections[ n_layers-2 ]->bpropUpdate(
+            layers[ n_layers-2 ]->expectation,
+            layers[ n_layers-1 ]->activation,
+            expectation_gradients[ n_layers-2 ],
+            activation_gradients[ n_layers-1 ],
+            true);
+        // accumulate into expectation_gradients[n_layers-2]
+        // because a partial cost may have already put a gradient there
+    }
 
-                // This doesn't work. gcc bug?
-                // classification_cost->fprop( class_output, target, cost );
-                classification_cost->CostModule::fprop( class_output, target,
-                                                        nll_cost );
+    if( use_classification_cost )
+    {
+        classification_module->fprop( layers[ n_layers-2 ]->expectation,
+                                      class_output );
+        real nll_cost;
 
-                real class_error =
-                    ( argmax(class_output) == (int) round(target[0]) ) ? 0: 1;
+        // This doesn't work. gcc bug?
+        // classification_cost->fprop( class_output, target, cost );
+        classification_cost->CostModule::fprop( class_output, target,
+                                                nll_cost );
 
-                train_costs[nll_cost_index] = nll_cost;
-                train_costs[class_cost_index] = class_error;
+        real class_error =
+            ( argmax(class_output) == (int) round(target[0]) ) ? 0: 1;
 
-                classification_cost->bpropUpdate( class_output, target, nll_cost,
-                                                  class_gradient );
+        train_costs[nll_cost_index] = nll_cost;
+        train_costs[class_cost_index] = class_error;
 
-                classification_module->bpropUpdate( layers[ n_layers-2 ]->expectation,
-                                                    class_output,
-                                                    expectation_gradients[n_layers-2],
-                                                    class_gradient,
-                                                    true );
-            }
+        classification_cost->bpropUpdate( class_output, target, nll_cost,
+                                          class_gradient );
 
-            layers[i]->setLearningRate( cd_learning_rate );
-            layers[i+1]->setLearningRate( cd_learning_rate );
-            connections[i]->setLearningRate( cd_learning_rate );
-            contrastiveDivergenceStep( layers[ i ],
-                                       connections[ i ],
-                                       layers[ i+1 ] );
+        classification_module->bpropUpdate( layers[ n_layers-2 ]->expectation,
+                                            class_output,
+                                            expectation_gradients[n_layers-2],
+                                            class_gradient,
+                                            true );
+        if( top_layer_joint_cd )
+        {
+            // set the input of the joint layer
+            Vec target_exp = classification_module->target_layer->expectation;
+            fill_one_hot( target_exp, (int) round(target[0]), 0., 1. );
+
+            joint_layer->setLearningRate( cd_learning_rate );
+            layers[ n_layers-1 ]->setLearningRate( cd_learning_rate );
+            classification_module->joint_connection->setLearningRate(
+                cd_learning_rate );
+
+            save_layer_activation.resize(layers[ n_layers-2 ]->size);
+            save_layer_activation << layers[ n_layers-2 ]->activation;
+            save_layer_expectation.resize(layers[ n_layers-2 ]->size);
+            save_layer_expectation << layers[ n_layers-2 ]->expectation;
+
+            contrastiveDivergenceStep(
+                get_pointer(joint_layer),
+                get_pointer(classification_module->joint_connection),
+                layers[ n_layers-1 ] );
+
+            layers[ n_layers-2 ]->activation << save_layer_activation;
+            layers[ n_layers-2 ]->expectation << save_layer_expectation;
         }
-
     }
 
+
     // DOWNWARD PHASE (the downward phase for top layer is already done above)
     for( int i=n_layers-3 ; i>=0 ; i-- )
     {
-        
         connections[ i ]->setLearningRate( grad_learning_rate );
         layers[ i+1 ]->setLearningRate( grad_learning_rate );
 
@@ -858,18 +860,29 @@
                                      activation_gradients[i+1],
                                      true);
 
-        // N.B. the contrastiveDivergenceStep changes the activation
-        // and expectation fields of top layer of the RBM, so it must be done last
+        // N.B. the contrastiveDivergenceStep changes the activation and
+        // expectation fields of top layer of the RBM, so it must be
+        // done last
         layers[i]->setLearningRate( cd_learning_rate );
         layers[i+1]->setLearningRate( cd_learning_rate );
         connections[i]->setLearningRate( cd_learning_rate );
-        save_layer_activation.resize(layers[i]->size);
-        save_layer_activation << layers[i]->activation;
+
+        if( i > 0 )
+        {
+            save_layer_activation.resize(layers[i]->size);
+            save_layer_activation << layers[i]->activation;
+            save_layer_expectation.resize(layers[i]->size);
+            save_layer_expectation << layers[i]->expectation;
+        }
         contrastiveDivergenceStep( layers[ i ],
                                    connections[ i ],
                                    layers[ i+1 ] ,
                                    true);
-        layers[i]->activation << save_layer_activation;
+        if( i > 0 )
+        {
+            layers[i]->activation << save_layer_activation;
+            layers[i]->expectation << save_layer_expectation;
+        }
     }
 
 }
@@ -940,13 +953,9 @@
         layers[i+1]->computeExpectation();
     }
 
-    Vec joint_exp = joint_layer->expectation;
-    joint_exp.subVec( 0, layers[ n_layers-2 ]->size )
-        << layers[ n_layers-2 ]->expectation;
+    Vec target_exp = classification_module->target_layer->expectation;
+    fill_one_hot( target_exp, (int) round(target[0]), 0., 1. );
 
-    fill_one_hot( joint_exp.subVec( layers[ n_layers-2 ]->size, n_classes ),
-                  (int) round(target[0]), 0., 1. );
-
     if( partial_costs && partial_costs[ n_layers-2 ] )
     {
         // Deterministic forward pass

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2007-02-27 18:15:08 UTC (rev 6690)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2007-02-27 19:44:14 UTC (rev 6691)
@@ -129,6 +129,9 @@
     //! whether to do things by stages, including fine-tuning, or on-line
     bool online;
 
+    //! Wether we do a step of joint contrastive divergence on top-layer
+    bool top_layer_joint_cd;
+
     //#####  Not Options  #####################################################
 
     //! Timer for monitoring the speed
@@ -263,6 +266,9 @@
     //! buffers bottom layer activation during onlineStep 
     mutable Vec save_layer_activation;
 
+    //! buffers bottom layer expectation during onlineStep 
+    mutable Vec save_layer_expectation;
+
     //! Does final_module exist and have a "learning_rate" option
     bool final_module_has_learning_rate;
 



From saintmlx at mail.berlios.de  Tue Feb 27 21:27:14 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 27 Feb 2007 21:27:14 +0100
Subject: [Plearn-commits] r6692 - in trunk: plearn/misc scripts
Message-ID: <200702272027.l1RKREqu003327@sheep.berlios.de>

Author: saintmlx
Date: 2007-02-27 21:27:13 +0100 (Tue, 27 Feb 2007)
New Revision: 6692

Modified:
   trunk/plearn/misc/PLearnServer.cc
   trunk/plearn/misc/PLearnServer.h
   trunk/plearn/misc/PLearnService.cc
   trunk/scripts/xdispatch
Log:
- remote servers now get their master's verbosity level for logs



Modified: trunk/plearn/misc/PLearnServer.cc
===================================================================
--- trunk/plearn/misc/PLearnServer.cc	2007-02-27 19:44:14 UTC (rev 6691)
+++ trunk/plearn/misc/PLearnServer.cc	2007-02-27 20:27:13 UTC (rev 6692)
@@ -100,7 +100,12 @@
     getInstance()->io.implicit_storage = impl_stor;
 }
 
+void PLearnServer::setVerbosity(int verbosity)
+{
+    PL_Log::instance().verbosity(verbosity);
+}
 
+
 BEGIN_DECLARE_REMOTE_FUNCTIONS
 
 declareFunction("cd", &PLearnServer::cd,
@@ -117,6 +122,10 @@
                 (BodyDoc("change the implicit_storage mode of the io of the PLearnServer instance.\n"),
                  ArgDoc ("impl_stor", "Whether or not to use implicit_storage")));
 
+declareFunction("setVerbosity", &PLearnServer::setVerbosity,
+                (BodyDoc("change the verbosity for logs of the PLearnServer instance.\n"),
+                 ArgDoc ("verbosity", "verbosity level")));
+
 END_DECLARE_REMOTE_FUNCTIONS
 
 

Modified: trunk/plearn/misc/PLearnServer.h
===================================================================
--- trunk/plearn/misc/PLearnServer.h	2007-02-27 19:44:14 UTC (rev 6691)
+++ trunk/plearn/misc/PLearnServer.h	2007-02-27 20:27:13 UTC (rev 6692)
@@ -98,6 +98,8 @@
     //! change the implicit_storage mode of the io of the PLearnServer instance.
     static void implicit_storage(bool impl_stor);
 
+    static void setVerbosity(int verbosity);
+
 };
 
 

Modified: trunk/plearn/misc/PLearnService.cc
===================================================================
--- trunk/plearn/misc/PLearnService.cc	2007-02-27 19:44:14 UTC (rev 6691)
+++ trunk/plearn/misc/PLearnService.cc	2007-02-27 20:27:13 UTC (rev 6692)
@@ -126,6 +126,8 @@
 
         reserved_servers.insert(serv);
         serv->getResults();
+        serv->callFunction("setVerbosity", PL_Log::instance().verbosity());
+        serv->getResults();
         reserved_servers.erase(serv);
         available_servers.push(serv);
     }

Modified: trunk/scripts/xdispatch
===================================================================
--- trunk/scripts/xdispatch	2007-02-27 19:44:14 UTC (rev 6691)
+++ trunk/scripts/xdispatch	2007-02-27 20:27:13 UTC (rev 6692)
@@ -1,16 +1,33 @@
-#!/usr/bin/python2.3
+#!/usr/bin/env python
 #
 # xdispatch: dispatch utility for parallel PLearn
 #
 # Copyright 2007, Apstat Technologies, inc.
 # All rights reserved.
 
+# Author: Xavier Saint-Mleux
+
 from optparse import OptionParser
 from plearn.parallel.dispatch import *
 import tempfile
 import os
 import time
 
+
+usage= """
+Usage: xdispatch -s<nb_servers> [-l<max_load>] -- <plearn_command> <plearn_args>
+       Will launch nb_servers remote servers on machines with a load no
+       more than max_load for the last minute (default=1.0).
+       plearn_command is your favorite version of plearn.
+       Each server (slave) is launched like:
+         ssh <remote_host> <plearn_command> server 0
+       The master is launched like (on the local host):
+         <plearn_command> --servers <serv_file> <plearn_args>
+       where serv_file is a file that contains one line for each
+       of the running slave servers, with hostname, port and pid.
+       Once the master returns, all slaves are killed.
+"""
+
 #
 # Main
 #
@@ -30,30 +47,19 @@
                       dest= "nb_servers",
                       help='Number of servers to launch (maximum)',
                       default=1)
-
-    parser.add_option('-m', '--min-nb-servers', type="int",
-                      dest= "min-nb_servers",
-                      help='Number of servers to launch (minimum)',
-                      default=1)
-
-    
+   
     (options, args) = parser.parse_args()
 
-    set_logdir('/home/saintmlx/xdispatch-log/')
+    set_logdir('/tmp/xdispatch-log/')
 
     Task._max_load= options.max_load
 
-    #xdispatch -l 3.0 -s 25 inslearn exp.pyplearn
-    #25x -> ssh distant_machine inslearn server 0
-    #1x  -> inslearn --servers serv_file exp.pyplearn
-
     if options.num_available_machines:
         print Task.availableMachinesCount()
     elif args:
         try:
             plearn_command= args[0]
             for i in xrange(options.nb_servers):
-                #launch_server( ['time',plearn_command, '--verbosity 999', 'server', '0'] )
                 launch_server( ['time',plearn_command, 'server', '0'] )
 
             tf, fname= tempfile.mkstemp(suffix= '.plserv', prefix= '.plserv_', dir= '/tmp/')
@@ -71,6 +77,8 @@
 
         finally:
             Task.kill_all_tasks()
+    else:
+        print usage
 
 if __name__ == '__main__':
     main()



From dorionc at mail.berlios.de  Tue Feb 27 23:29:43 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Tue, 27 Feb 2007 23:29:43 +0100
Subject: [Plearn-commits] r6693 - in trunk/python_modules/plearn: report xp
Message-ID: <200702272229.l1RMThZm016580@sheep.berlios.de>

Author: dorionc
Date: 2007-02-27 23:29:43 +0100 (Tue, 27 Feb 2007)
New Revision: 6693

Modified:
   trunk/python_modules/plearn/report/formatter.py
   trunk/python_modules/plearn/report/graphical_tools.py
   trunk/python_modules/plearn/xp/Experiment.py
Log:
Robustification

Modified: trunk/python_modules/plearn/report/formatter.py
===================================================================
--- trunk/python_modules/plearn/report/formatter.py	2007-02-27 20:27:13 UTC (rev 6692)
+++ trunk/python_modules/plearn/report/formatter.py	2007-02-27 22:29:43 UTC (rev 6693)
@@ -267,3 +267,4 @@
     assert os.path.exists(pdf_name), "PDF could not be created!"
     os.system("pdflatex %s >& /dev/null"%file_name)
     os.system("pdflatex %s >& /dev/null"%file_name)
+    os.system("pdflatex %s >& /dev/null"%file_name)

Modified: trunk/python_modules/plearn/report/graphical_tools.py
===================================================================
--- trunk/python_modules/plearn/report/graphical_tools.py	2007-02-27 20:27:13 UTC (rev 6692)
+++ trunk/python_modules/plearn/report/graphical_tools.py	2007-02-27 22:29:43 UTC (rev 6693)
@@ -38,6 +38,12 @@
 LINE_COLORS = [ '#660033', 'b', 'r', 'k', "#CDBE70",
                 "#FF8C69", "#65754D", "#4d6575", "#754d65" ]
 
+STYLELIST = [
+    'b-',  'g-',  'r-',  'c-',  'm-',  'k-',  'y-',
+    'b--', 'g--', 'r--', 'c--', 'm--', 'k--', 'y--',
+    'b:',  'g:',  'r:',  'c:',  'm:',  'k:',  'y:',
+    'b-.', 'g-.', 'r-.', 'c-.', 'm-.', 'k-.', 'y-.' ] * 5
+
 _figure_counter = 0
 def getNewFigure(figsize=(12,10)):
     global _figure_counter

Modified: trunk/python_modules/plearn/xp/Experiment.py
===================================================================
--- trunk/python_modules/plearn/xp/Experiment.py	2007-02-27 20:27:13 UTC (rev 6692)
+++ trunk/python_modules/plearn/xp/Experiment.py	2007-02-27 22:29:43 UTC (rev 6693)
@@ -129,11 +129,11 @@
     # PyPLearnObject's classmethod
     _by_value = classmethod( lambda cls: True )
 
-    def cache_experiments( cls, exproot=None, forget=True ):
+    def cache_experiments( cls, exproot=None, forget=True, name_key=None ):
         if exproot is None:
             roots = pyplearn.config.get_option( 'EXPERIMENTS', 'expdir_root' ).split(',')            
             for exproot in roots:
-                cls.cache_experiments( exproot=exproot, forget=False )
+                cls.cache_experiments( exproot=exproot, forget=False, name_key=name_key )
             return
 
         if cls._cached is None:
@@ -151,8 +151,13 @@
             dirlist = os.listdir( os.getcwd() )            
 
         for fname in dirlist:
-            if fname.startswith( cls._expdir_prefix ):                
-                x = cls( path = os.path.join(exproot, fname) )
+            candidate_path = os.path.join(exproot, fname)
+            candidate_infopath = os.path.join(candidate_path, cls._metainfos_fname)
+            if fname.startswith( cls._expdir_prefix ) \
+                   and os.path.exists(candidate_infopath):
+                x = cls( path = candidate_path )
+                if name_key:
+                    x.setName(x.expkey[name_key])
                 cls._cached.append( x )
         return cls._cached
     cache_experiments = classmethod( cache_experiments )
@@ -175,10 +180,11 @@
     #
     def __init__( self, **overrides ):
         PyPLearnObject.__init__( self, **overrides )
+        self._name = None
         if self.expkey is None:
             self.expkey = ExpKey( os.path.join( self.path, self._metainfos_fname ) )
 
-        # Update abspath
+        # Update abspath        
         self.abspath = os.path.abspath( self.path )
 
     def __del__(self):
@@ -205,6 +211,7 @@
                 pmat = PMat(os.path.join(self.abspath, pmat))
                 setattr(self, attr_name, pmat)
                 self._opened_pmats.append(pmat)
+        return self._opened_pmats[-1]
 
     def getKey( self, expkey = None ):
         if expkey is None:
@@ -261,3 +268,16 @@
 
     def running( self ):
         return len(self.expkey) == 0
+
+
+    ###  set/getName
+
+    def getName(self):
+        assert self._name
+        return self._name
+
+    def setName(self, name):
+        self._name = name
+        
+
+    



From yoshua at mail.berlios.de  Tue Feb 27 23:55:36 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Tue, 27 Feb 2007 23:55:36 +0100
Subject: [Plearn-commits] r6694 - in trunk: commands plearn_learners/generic
	plearn_learners/online
Message-ID: <200702272255.l1RMtaVS019116@sheep.berlios.de>

Author: yoshua
Date: 2007-02-27 23:55:35 +0100 (Tue, 27 Feb 2007)
New Revision: 6694

Modified:
   trunk/commands/plearn_noblas_inc.h
   trunk/plearn_learners/generic/NNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
   trunk/plearn_learners/online/ModuleStackModule.cc
   trunk/plearn_learners/online/RBMMultinomialLayer.cc
Log:


Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2007-02-27 22:29:43 UTC (rev 6693)
+++ trunk/commands/plearn_noblas_inc.h	2007-02-27 22:55:35 UTC (rev 6694)
@@ -288,6 +288,7 @@
 #include <plearn/vmat/ProcessingVMatrix.h>
 #include <plearn/vmat/ProcessSymbolicSequenceVMatrix.h>
 #include <plearn/vmat/RandomSamplesVMatrix.h>
+#include <plearn/vmat/RandomSamplesFromVMatrix.h>
 #include <plearn/vmat/RankedVMatrix.h>
 #include <plearn/vmat/RegularGridVMatrix.h>
 #include <plearn/vmat/RemoveDuplicateVMatrix.h>

Modified: trunk/plearn_learners/generic/NNet.cc
===================================================================
--- trunk/plearn_learners/generic/NNet.cc	2007-02-27 22:29:43 UTC (rev 6693)
+++ trunk/plearn_learners/generic/NNet.cc	2007-02-27 22:55:35 UTC (rev 6694)
@@ -469,7 +469,7 @@
         {
             if (the_output->size() == 1) {
                 // Assume sigmoid output here!
-                costs[k] = cross_entropy(the_output, the_target);
+                costs[k] = stable_cross_entropy(before_transfer_func, the_target);
             } else {
                 if (output_transfer_func == "log_softmax")
                     costs[k] = -the_output[the_target];
@@ -478,7 +478,12 @@
             }
         } 
         else if(cost_funcs[k]=="class_error")
-            costs[k] = classification_loss(the_output, the_target);
+        {
+            if (the_output->size()==1)
+                costs[k] = binary_classification_loss(the_output, the_target);
+            else
+                costs[k] = classification_loss(the_output, the_target);
+        }
         else if(cost_funcs[k]=="binary_class_error")
             costs[k] = binary_classification_loss(the_output, the_target);
         else if(cost_funcs[k]=="multiclass_error")

Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-27 22:29:43 UTC (rev 6693)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-27 22:55:35 UTC (rev 6694)
@@ -515,6 +515,15 @@
         nll_cost_index = train_cost_names.find(classification_cost->name()[0]);
     if ( final_cost )
         final_cost_index = train_cost_names.find(final_cost->name()[0]);
+    if ( partial_costs )
+    {
+        partial_cost_indices.resize(partial_costs.size());
+        for (int i=0;i<partial_costs.size();i++)
+            if (partial_costs[i])
+                partial_cost_indices[i] = train_cost_names.find(partial_costs[i]->name()[0] + "_" + tostring(i+1) );
+            else
+                partial_cost_indices[i] = -1;
+    }
 
     recons_cost_index = train_cost_names.find("recons_error");
 
@@ -730,8 +739,10 @@
             // Backward pass
             // first time we set these gradients: do not accumulate
             partial_costs[ i ]->bpropUpdate( layers[ i+1 ]->expectation,
-                                             target, cost,
+                                             target, cost[i],
                                              expectation_gradients[ i+1 ] );
+
+            train_costs[partial_cost_indices[i]] = cost[i];
         }
         else
             expectation_gradients[i+1].clear();
@@ -1259,6 +1270,12 @@
 
     if( final_cost )
         cost_names.append( final_cost->name() );
+
+    if (partial_costs)
+        for (int i=0;i<n_layers-1;i++)
+            if (partial_costs[i])
+                cost_names.append( partial_costs[i]->name()[0] + "_" + tostring(i+1) );
+            
     return cost_names;
 }
 

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2007-02-27 22:29:43 UTC (rev 6693)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2007-02-27 22:55:35 UTC (rev 6694)
@@ -291,6 +291,9 @@
     //! Keeps the index of the reconstruction cost in train_costs
     int recons_cost_index;
 
+    //! indices of the partial costs in train_costs
+    TVec<int> partial_cost_indices;
+
 protected:
     //#####  Protected Member Functions  ######################################
 

Modified: trunk/plearn_learners/online/ModuleStackModule.cc
===================================================================
--- trunk/plearn_learners/online/ModuleStackModule.cc	2007-02-27 22:29:43 UTC (rev 6693)
+++ trunk/plearn_learners/online/ModuleStackModule.cc	2007-02-27 22:55:35 UTC (rev 6694)
@@ -113,7 +113,6 @@
 {
     PLASSERT( n_modules > 0 );
     PLASSERT( input.size() == input_size );
-    PLASSERT( output.size() == output_size );
 
     modules[0]->fprop( input, values[0] );
     for( int i=1 ; i<n_modules-1 ; i++ )

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-02-27 22:29:43 UTC (rev 6693)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-02-27 22:55:35 UTC (rev 6694)
@@ -155,7 +155,7 @@
     real* outg = output_gradient.data();
     real* ing = input_gradient.data();
     real* b = bias.data();
-    real* binc = bias_inc.data();
+    real* binc = momentum==0?0:bias_inc.data();
 
     for( int i=0 ; i<size ; i++ )
     {



From lamblin at mail.berlios.de  Wed Feb 28 00:37:45 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 28 Feb 2007 00:37:45 +0100
Subject: [Plearn-commits] r6695 - trunk/plearn_learners/online
Message-ID: <200702272337.l1RNbjh6008007@sheep.berlios.de>

Author: lamblin
Date: 2007-02-28 00:37:43 +0100 (Wed, 28 Feb 2007)
New Revision: 6695

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
   trunk/plearn_learners/online/OnlineLearningModule.cc
Log:
Various minor improvements


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-27 22:55:35 UTC (rev 6694)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-27 23:37:43 UTC (rev 6695)
@@ -333,6 +333,7 @@
     MODULE_LOG << "build_final_cost() called" << endl;
 
     final_cost_gradient.resize( final_cost->input_size );
+    final_cost->setLearningRate( grad_learning_rate );
 
     if( final_module )
     {
@@ -348,17 +349,7 @@
                     "\n", n_layers-1, layers[n_layers-1]->size,
                     final_module->input_size);
 
-        Object* obj = 0;
-        OptionList::iterator it;
-        string opt_id;
-
-        // try to see if final_cost has an option named "learning_rate"
-        final_module_has_learning_rate =
-            final_cost->parseOptionName( "learning_rate", obj, it, opt_id );
-
-        final_cost_has_learning_rate = final_module &&
-            final_module->parseOptionName( "learning_rate", obj, it, opt_id );
-
+        final_module->setLearningRate( grad_learning_rate );
     }
     else
     {
@@ -368,10 +359,33 @@
                     "\n", n_layers-1, layers[n_layers-1]->size,
                     final_cost->input_size);
     }
-    // TODO: check target size
+
+    // check target size and final_cost->input_size
+    if( n_classes == 0 ) // regression
+    {
+        if( final_cost->input_size != targetsize() )
+            PLERROR("DeepBeliefNet::build_final_cost() - \n"
+                    "final_cost->input_size (%d) != targetsize() (%d),\n"
+                    "although we are doing regression (n_classes == 0).\n",
+                    final_cost->input_size, targetsize());
+    }
+    else
+    {
+        if( final_cost->input_size != n_classes )
+            PLERROR("DeepBeliefNet::build_final_cost() - \n"
+                    "final_cost->input_size (%d) != n_classes (%d),\n"
+                    "although we are doing classification (n_classes != 0).\n",
+                    final_cost->input_size, n_classes);
+
+        if( targetsize() != 1 )
+            PLERROR("DeepBeliefNet::build_final_cost() - \n"
+                    "targetsize() (%d) != 1,\n"
+                    "although we are doing regression (n_classes == 0).\n",
+                    targetsize());
+    }
+
 }
 
-// ### Nothing to add here, simply calls build_
 void DeepBeliefNet::build()
 {
     inherited::build();
@@ -421,10 +435,6 @@
         out_size += layers[n_layers-1]->size;
 
     return out_size;
-/*
-    return (n_classes > 0) ? n_classes
-                           : targetsize();
-*/
 }
 
 void DeepBeliefNet::forget()
@@ -602,7 +612,7 @@
         }
 
         // possible supervised part
-        if(use_classification_cost )
+        if( use_classification_cost )
         {
             MODULE_LOG << "Training the classification module" << endl;
 
@@ -617,7 +627,10 @@
                                       end_stage - stage );
 
             // set appropriate learning rate
-            setLearningRate( cd_learning_rate );
+            joint_layer->setLearningRate( cd_learning_rate );
+            classification_module->joint_connection->setLearningRate(
+                cd_learning_rate );
+            layers[ n_layers-1 ]->setLearningRate( cd_learning_rate );
 
             int previous_stage = (n_layers < 3) ? 0
                 : training_schedule[n_layers-3];
@@ -903,7 +916,7 @@
     PLASSERT( index < n_layers );
 
     layers[0]->expectation << input;
-    for( int i=0 ; i<index ; i++ )
+    for( int i=0 ; i<=index ; i++ )
     {
         connections[i]->setAsDownInput( layers[i]->expectation );
         layers[i+1]->getAllActivations( connections[i] );
@@ -913,11 +926,6 @@
     // TODO: add another learning rate?
     if( partial_costs && partial_costs[ index ] )
     {
-        // Deterministic forward pass
-        connections[ index ]->setAsDownInput( layers[ index ]->expectation );
-        layers[ index+1 ]->getAllActivations( connections[ index ] );
-        layers[ index+1 ]->computeExpectation();
-
         // put appropriate learning rate
         connections[ index ]->setLearningRate( grad_learning_rate );
         layers[ index+1 ]->setLearningRate( grad_learning_rate );
@@ -949,7 +957,8 @@
 
     contrastiveDivergenceStep( layers[ index ],
                                connections[ index ],
-                               layers[ index+1 ] );
+                               layers[ index+1 ],
+                               true );
 }
 
 void DeepBeliefNet::jointGreedyStep( const Vec& input, const Vec& target )
@@ -964,21 +973,16 @@
         layers[i+1]->computeExpectation();
     }
 
-    Vec target_exp = classification_module->target_layer->expectation;
-    fill_one_hot( target_exp, (int) round(target[0]), 0., 1. );
-
     if( partial_costs && partial_costs[ n_layers-2 ] )
     {
         // Deterministic forward pass
-        classification_module->joint_connection->setAsDownInput(
-            joint_layer->expectation );
-        layers[ n_layers-1 ]->getAllActivations(
-            get_pointer( classification_module->joint_connection ) );
+        connections[ n_layers-2 ]->setAsDownInput(
+            layers[ n_layers-2 ]->expectation );
+        layers[ n_layers-1 ]->getAllActivations( connections[ n_layers-2 ] );
         layers[ n_layers-1 ]->computeExpectation();
 
         // put appropriate learning rate
-        classification_module->previous_to_last->setLearningRate(
-            grad_learning_rate );
+        connections[ n_layers-2 ]->setLearningRate( grad_learning_rate );
         layers[ n_layers-1 ]->setLearningRate( grad_learning_rate );
 
         // Backward pass
@@ -996,18 +1000,20 @@
                                            expectation_gradients[ n_layers-1 ]
                                          );
 
-        classification_module->previous_to_last->bpropUpdate(
+        connections[ n_layers-2 ]->bpropUpdate(
             layers[ n_layers-2 ]->expectation,
             layers[ n_layers-1 ]->activation,
             expectation_gradients[ n_layers-2 ],
             activation_gradients[ n_layers-1 ] );
 
         // put back old learning rate
-        classification_module->previous_to_last->setLearningRate(
-            cd_learning_rate );
+        connections[ n_layers-2 ]->setLearningRate( cd_learning_rate );
         layers[ n_layers-1 ]->setLearningRate( cd_learning_rate );
     }
 
+    Vec target_exp = classification_module->target_layer->expectation;
+    fill_one_hot( target_exp, (int) round(target[0]), 0., 1. );
+
     contrastiveDivergenceStep(
         get_pointer( joint_layer ),
         get_pointer( classification_module->joint_connection ),
@@ -1271,17 +1277,19 @@
     if( final_cost )
         cost_names.append( final_cost->name() );
 
-    if (partial_costs)
-        for (int i=0;i<n_layers-1;i++)
-            if (partial_costs[i])
-                cost_names.append( partial_costs[i]->name()[0] + "_" + tostring(i+1) );
-            
     return cost_names;
 }
 
 TVec<string> DeepBeliefNet::getTrainCostNames() const
 {
     TVec<string> cost_names = getTestCostNames() ;
+
+    if (partial_costs)
+        for (int i=0;i<n_layers-1;i++)
+            if (partial_costs[i])
+                cost_names.append( partial_costs[i]->name()[0]
+                                   + "_" + tostring(i+1) );
+
     cost_names.append("recons_error");
     return cost_names;
 }
@@ -1304,14 +1312,6 @@
             the_learning_rate );
         joint_layer->setLearningRate( the_learning_rate );
     }
-
-    if( final_module_has_learning_rate )
-        final_cost->setOption( "learning_rate",
-                               tostring(the_learning_rate ) );
-
-    if( final_cost_has_learning_rate )
-        final_cost->setOption( "learning_rate",
-                               tostring(the_learning_rate ) );
 }
 
 

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2007-02-27 22:55:35 UTC (rev 6694)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2007-02-27 23:37:43 UTC (rev 6695)
@@ -130,6 +130,7 @@
     bool online;
 
     //! Wether we do a step of joint contrastive divergence on top-layer
+    //! Only used if online for the moment
     bool top_layer_joint_cd;
 
     //#####  Not Options  #####################################################

Modified: trunk/plearn_learners/online/OnlineLearningModule.cc
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.cc	2007-02-27 22:55:35 UTC (rev 6694)
+++ trunk/plearn_learners/online/OnlineLearningModule.cc	2007-02-27 23:37:43 UTC (rev 6695)
@@ -109,10 +109,10 @@
 
 void OnlineLearningModule::setLearningRate( real dynamic_learning_rate )
 {
-    PLERROR("OnlineLearningModule does not have a learning rate that can be\n"
-            "changed from outside.\n"
-            "If your derived class has one, please implement setLearningrate()"
-            " in it.\n");
+    PLWARNING("OnlineLearningModule does not have a learning rate that can be\n"
+              "changed from outside.\n"
+              "If your derived class has one, please implement setLearningrate()"
+              " in it.\n");
 }
 
 



From yoshua at mail.berlios.de  Wed Feb 28 02:31:11 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Wed, 28 Feb 2007 02:31:11 +0100
Subject: [Plearn-commits] r6696 - trunk/plearn_learners/online
Message-ID: <200702280131.l1S1VB6h009360@sheep.berlios.de>

Author: yoshua
Date: 2007-02-28 02:31:10 +0100 (Wed, 28 Feb 2007)
New Revision: 6696

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
Log:


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-27 23:37:43 UTC (rev 6695)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-28 01:31:10 UTC (rev 6696)
@@ -478,6 +478,7 @@
 {
     MODULE_LOG << "train() called " << endl;
     MODULE_LOG << "  training_schedule = " << training_schedule << endl;
+    MODULE_LOG << "stage = " << stage << ", target nstages = " << nstages << endl;
 
     // The role of the train method is to bring the learner up to
     // stage==nstages, updating train_stats with training costs measured



From larocheh at mail.berlios.de  Wed Feb 28 15:12:44 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 28 Feb 2007 15:12:44 +0100
Subject: [Plearn-commits] r6697 - trunk/plearn_learners/online
Message-ID: <200702281412.l1SECi1C003757@sheep.berlios.de>

Author: larocheh
Date: 2007-02-28 15:12:43 +0100 (Wed, 28 Feb 2007)
New Revision: 6697

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
Log:
Corrected a bug in a PLERROR output...


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-28 01:31:10 UTC (rev 6696)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-28 14:12:43 UTC (rev 6697)
@@ -275,7 +275,7 @@
         if( connections[i]->up_size != layers[i+1]->size )
             PLERROR("DeepBeliefNet::build_layers_and_connections() - \n"
                     "connections[%i]->up_size (%d) != layers[%i]->size (%d)."
-                    "\n", i, connections[i]->up_size, i+1, layers[i]->size);
+                    "\n", i, connections[i]->up_size, i+1, layers[i+1]->size);
 
         layers[i]->random_gen = random_gen;
         layers[i]->build();



From manzagop at mail.berlios.de  Wed Feb 28 16:29:29 2007
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Wed, 28 Feb 2007 16:29:29 +0100
Subject: [Plearn-commits] r6698 - trunk/plearn/opt/EXPERIMENTAL
Message-ID: <200702281529.l1SFTTvt022263@sheep.berlios.de>

Author: manzagop
Date: 2007-02-28 16:29:28 +0100 (Wed, 28 Feb 2007)
New Revision: 6698

Added:
   trunk/plearn/opt/EXPERIMENTAL/OnlineGramNaturalGradientOptimizer.cc
   trunk/plearn/opt/EXPERIMENTAL/OnlineGramNaturalGradientOptimizer.h
Log:
Initial version of the code with bounding of the principal eigen values with the reg option.


Added: trunk/plearn/opt/EXPERIMENTAL/OnlineGramNaturalGradientOptimizer.cc
===================================================================
--- trunk/plearn/opt/EXPERIMENTAL/OnlineGramNaturalGradientOptimizer.cc	2007-02-28 14:12:43 UTC (rev 6697)
+++ trunk/plearn/opt/EXPERIMENTAL/OnlineGramNaturalGradientOptimizer.cc	2007-02-28 15:29:28 UTC (rev 6698)
@@ -0,0 +1,399 @@
+// -*- C++ -*-
+
+// OnlineGramNaturalGradientOptimizer.cc
+//
+// Copyright (C) 2007 Pierre-Antoine Manzagol
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pierre-Antoine Manzagol
+
+/*! \file OnlineGramNaturalGradientOptimizer.cc */
+
+
+#define PL_LOG_MODULE_NAME "OnlineGramNaturalGradientOptimizer"
+
+#include "OnlineGramNaturalGradientOptimizer.h"
+#include <plearn/io/pl_log.h>
+#include <plearn/math/TMat_maths.h>
+#include <plearn/display/DisplayUtils.h>
+#include <plearn/var/SumOfVariable.h>
+
+#include <plearn/math/plapack.h>
+
+
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    OnlineGramNaturalGradientOptimizer,
+    "Optimization by Schraudolph's stochastic meta descent (SMD).", 
+    "OnlineGramNaturalGradientOptimizer is \n"
+    "blabla \n"
+    "\n"
+);
+
+OnlineGramNaturalGradientOptimizer::OnlineGramNaturalGradientOptimizer():
+    learning_rate(0.01),
+    gamma(1.0),
+    reg(1e-6),
+    opt_batch_size(1),
+    n_eigen(6)
+{}
+
+
+void OnlineGramNaturalGradientOptimizer::declareOptions(OptionList& ol)
+{
+    declareOption(
+        ol, "learning_rate", &OnlineGramNaturalGradientOptimizer::learning_rate,
+        OptionBase::buildoption, 
+        "Learning rate used in the natural gradient descent.\n");
+    declareOption(
+        ol, "gamma", &OnlineGramNaturalGradientOptimizer::gamma,
+        OptionBase::buildoption, 
+        "Discount factor used in the update of the estimate of the gradient covariance.\n");
+    declareOption(
+        ol, "reg", &OnlineGramNaturalGradientOptimizer::reg,
+        OptionBase::buildoption, 
+        "Regularizer used in computing the natural gradient, C^{-1} mu. Added to C^{-1} diagonal.\n");
+    declareOption(
+        ol, "opt_batch_size", &OnlineGramNaturalGradientOptimizer::opt_batch_size,
+        OptionBase::buildoption, 
+        "Size of the optimizer's batches (examples before parameter and gradient covariance updates).\n");
+    declareOption(
+        ol, "n_eigen", &OnlineGramNaturalGradientOptimizer::n_eigen,
+        OptionBase::buildoption, 
+        "The number of eigen vectors to model the gradient covariance matrix\n");
+
+    inherited::declareOptions(ol);
+}
+
+void OnlineGramNaturalGradientOptimizer::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{ 
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(gradients, copies);
+    deepCopyField(mu, copies);
+    deepCopyField(gram, copies);
+    deepCopyField(U, copies);
+    deepCopyField(D, copies);
+    deepCopyField(cov_eigen_vec, copies);
+    deepCopyField(cov_eigen_val, copies);
+    deepCopyField(cov_norm_eigen_vec, copies);
+    deepCopyField(dot_prod, copies);
+    deepCopyField(scaled_dot_prod, copies);
+    deepCopyField(naturalg, copies);
+
+}
+
+void OnlineGramNaturalGradientOptimizer::build_()
+{
+    n_optimizeN_calls=0;
+    n_eigen_cur = 0;
+    n_eigen_old = 0;
+
+    total_variance = 0.0;
+    variance_percentage = 0.;
+
+    int n = params.nelems();
+
+    cout << "Number of parameters: " << n << endl;
+
+    if (n > 0) {
+        gradients.resize( opt_batch_size, n );
+        gradients.clear();
+        mu.resize(n);
+        mu.clear();
+        naturalg.resize(n);
+        naturalg.clear();
+        // other variables will have different lengths
+        // depending on the current number of eigen vectors
+    }
+}
+
+// 'stage' is to be interpreted as "the number of examples to use
+// in batches of size 'batch_size' "
+// Note that a batch could be spread over two epochs
+bool OnlineGramNaturalGradientOptimizer::optimizeN(VecStatsCollector& stats_coll) 
+{
+    n_optimizeN_calls++;
+
+    if( nstages%opt_batch_size != 0 )   {
+        PLWARNING("OnlineGramNaturalGradientOptimizer::optimizeN(...) - nstages%opt_batch_size != 0");
+    }
+
+    int stage_max = stage + nstages; // the stage to reach
+
+    PP<ProgressBar> pb;
+    pb = new ProgressBar("Training " + classname() + " from stage " 
+                + tostring(stage) + " to " + tostring(stage_max), (int)(stage_max-stage)/opt_batch_size );
+
+    int initial_stage = stage;
+    while( stage < stage_max )    {
+
+        /*if( bi == 0 )
+            t0 = clock();*/
+
+        // Get the new gradient and append it
+        params.clearGradient();
+        proppath.clearGradient();
+        cost->gradient[0] = -1.0;
+        proppath.fbprop();
+        params.copyGradientTo( gradients(bi) );
+
+        // End of batch. Compute natural gradient and update parameters.
+        bi++;
+        if( bi == opt_batch_size )  {
+            //t1 = clock();
+
+            bi = 0;
+            gramEigenNaturalGradient();
+
+            //t2 = clock();
+
+            // set params += -learning_rate * params.gradient
+            naturalg *= learning_rate;
+            params.copyGradientFrom( naturalg );
+            params.updateAndClear();
+
+            //t3 = clock();
+
+            //cout << double(t1-t0) << " " << double(t2-t1) << " " << double(t3-t2) << endl;
+
+            if(pb)
+                pb->update((stage-initial_stage)/opt_batch_size);
+
+        }
+
+        stats_coll.update(cost->value);
+        stage++;
+    }
+
+    return false;
+}
+
+
+void OnlineGramNaturalGradientOptimizer::gramEigenNaturalGradient()
+{
+    // We don't have any eigen vectors yet
+    if( n_eigen_cur == 0 )  {
+
+        // The number of eigen vectors we will have after incorporating the new data
+        // (the gram matrix of gradients might have a rank smaller than n_eigen)
+        n_eigen_cur = min( gradients.length(), n_eigen);
+
+        // Compute the total variance - to do this, compute the trace of the covariance matrix
+        // could also use the trace of the gram matrix since we compute it, ie sum(diag(gram))
+/*        for( int i=0; i<gradients.length(); i++)   {
+            Vec v = gradients(i);
+            total_variance += sumsquare(v);
+        }
+        total_variance /= gradients.length();*/
+
+        // Compute the gram matrix - TODO does this recognize gram is symetric? (and save the computations?)
+        gram.resize( gradients.length(), gradients.length() );
+        productTranspose(gram, gradients, gradients);
+        gram /= gradients.length();
+
+        // Extract eigenvectors/eigenvalues - destroys the content of gram, D and U are resized
+        // gram = U D U' (if we took all values)
+        eigenVecOfSymmMat(gram, n_eigen_cur, D, U);
+
+        // Percentage of the variance we keep is the sum of the kept eigenvalues divided
+        // by the total variance.
+        //variance_percentage = sum(D)/total_variance;
+
+	// The eigenvectors V of C are deduced from the eigenvectors U of G by the
+	// formula V = AUD^{-1/2} (D the eigenvalues of G).  The nonzero eigenvalues of
+	// C and D are the same.
+
+	// The true eigenvalues are norm_eigen_vec. However, we shall keep in memory
+	// the eigenvectors of C rescaled by the square root of their associated
+	// eigenvalues, so that C can be written VV' instead of VDV'. Thus, the "new" V
+	// is equal to VD^{1/2} = AU.
+        // We have row vectors so AU = (U'A')'
+
+        cov_eigen_vec.resize(n_eigen_cur, gradients.width() );
+        product( cov_eigen_vec, U, gradients );
+        cov_eigen_vec /= sqrt( gradients.length() );
+        cov_eigen_val.resize( D.length() );
+        cov_eigen_val << D;
+
+        ofstream fd_eigval("eigen_vals.txt", ios_base::app);
+        fd_eigval << cov_eigen_val << endl;
+        fd_eigval.close();
+
+        cov_norm_eigen_vec.resize( n_eigen_cur, gradients.width() );
+        for( int i=0; i<n_eigen_cur; i++)   {
+            Vec v = cov_norm_eigen_vec(i);
+            divide( cov_eigen_vec(i), sqrt(D[i]), v );
+        }
+
+    }
+
+    // We already have some eigen vectors, so it's an update
+    else    {
+
+        // The number of eigen vectors we will have after incorporating the new data
+        n_eigen_old = cov_eigen_vec.length();
+        n_eigen_cur = min( cov_eigen_vec.length() + gradients.length(), n_eigen);
+
+        // Update the total variance, by computing that of the covariance matrix
+        // total_variance = gamma*total_variance + (1-gamma)*sum(sum(A.^2))/n_new_vec
+        /*total_variance *= gamma;
+        for( int i=0; i<gradients.length(); i++)   {
+            Vec v = gradients(i);
+            total_variance += (1.-gamma) * sumsquare(v) / gradients.length();
+        }*/
+
+        // Compute the gram matrix
+	// To find the equivalence between the covariance matrix and the Gram matrix,
+	// we need to have the covariance matrix under the form C = UU' + AA'. However,
+	// what we have is C = gamma UU' + (1-gamma)AA'/n_new_vec. Thus, we will
+	// rescale U and A using U = sqrt(gamma) U and A = sqrt((1 - gamma)/n_new_vec)
+	// A. Now, the Gram matrix is of the form [U'U U'A;A'U A'A] using the new U and
+	// A.
+
+        gram.resize( n_eigen_old + gradients.length(), n_eigen_old + gradients.length() );
+
+        Mat m = gram.subMat(0, 0, n_eigen_old, n_eigen_old);
+        m.clear();
+        addToDiagonal(m, gamma*D);
+
+        // Nicolas says "use C_{n+1} = gamma C_n + gg'" so no (1.-gamma)
+        m = gram.subMat(n_eigen_old, n_eigen_old, gradients.length(), gradients.length());
+        productTranspose(m, gradients, gradients);
+        //m *= (1.-gamma) / gradients.length();
+        m /= gradients.length();
+
+        m = gram.subMat(n_eigen_old, 0, gradients.length(), n_eigen_old );
+        productTranspose(m, gradients, cov_eigen_vec);
+        //m *= sqrt(gamma*(1.-gamma)/gradients.length());
+        m *= sqrt(gamma/gradients.length());
+
+        Mat m2 = gram.subMat( 0, n_eigen_old, n_eigen_old, gradients.length() );
+        transpose( m, m2 );
+
+        //G = (G + G')/2; % Solving numerical mistakes
+
+//cout << "--" << endl << gram << endl;
+
+        // Extract eigenvectors/eigenvalues - destroys the content of gram, D and U are resized
+        // gram = U D U' (if we took all values)
+        eigenVecOfSymmMat(gram, n_eigen_cur, D, U);
+
+        // Percentage of the variance we keep is the sum of the kept eigenvalues divided
+        // by the total variance.
+        //variance_percentage = sum(D)/total_variance;
+
+	// The new (rescaled) eigenvectors are of the form [U A]*V where V is the
+	// eigenvector of G. Rewriting V = [V1;V2], we have [U A]*V = UV1 + AV2.
+        // for us cov_eigen_vec = U1 eigen_vec + U2 gradients
+
+        swap = old_cov_eigen_vec;
+        old_cov_eigen_vec = cov_eigen_vec;
+        cov_eigen_vec = swap;
+
+        cov_eigen_vec.resize(n_eigen_cur, gradients.width());
+        product( cov_eigen_vec, U.subMatColumns(0, n_eigen_old), old_cov_eigen_vec );
+
+//  C = alpha A.B + beta C
+productScaleAcc(cov_eigen_vec, U.subMatColumns(n_eigen_old, gradients.length()), false, gradients, false,
+                   sqrt((1.-gamma)/gradients.length()), sqrt(gamma));
+
+        cov_eigen_val.resize( D.length() );
+        cov_eigen_val << D;
+
+        cov_norm_eigen_vec.resize( n_eigen_cur, gradients.width() );
+        for( int i=0; i<n_eigen_cur; i++)   {
+            Vec v = cov_norm_eigen_vec(i);
+            divide( cov_eigen_vec(i), sqrt(D[i]), v );
+        }
+
+    }
+
+    // ### Determine reg - Should be set automaticaly.
+    //reg = cov_eigen_val[n_eigen_cur-1];
+    for( int i=0; i<n_eigen_cur; i++)   {
+        if( cov_eigen_val[i] < reg )  {
+            PLWARNING("cov_eigen_val[i] < reg. Setting to reg.");
+            cov_eigen_val[i] = reg;
+        }
+    }
+
+
+    // *** Compute C^{-1} mu, where mu is the mean of gradients ***
+
+    // Compute mu
+    columnMean( gradients, mu );
+
+
+/*    cout << "mu  " << mu << endl;
+    cout << "norm(mu) " << norm(mu) << endl;
+    cout << "cov_eigen_val " << cov_eigen_val << endl;
+    cout << "cov_eigen_vec " << cov_eigen_vec << endl;
+    cout << "cov_norm_eigen_vec " << cov_norm_eigen_vec << endl;*/
+
+    // Compute the dot product with the eigenvectors
+    dot_prod.resize(n_eigen_cur);
+    product( dot_prod, cov_norm_eigen_vec, mu);
+
+//    cout << "dot_prod " << dot_prod << endl;
+
+    // Rescale according to the eigenvectors. Since the regularization constant will
+    // be added to all the eigenvalues (and not only the ones we didn't keep), we
+    // have to remove it from the ones we kept.
+    scaled_dot_prod.resize(n_eigen_cur);
+
+    divide( dot_prod, cov_eigen_val, scaled_dot_prod);
+    scaled_dot_prod -= dot_prod/reg;
+
+    transposeProduct(naturalg, cov_norm_eigen_vec, scaled_dot_prod);
+
+    naturalg += mu / reg;
+
+
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/opt/EXPERIMENTAL/OnlineGramNaturalGradientOptimizer.h
===================================================================
--- trunk/plearn/opt/EXPERIMENTAL/OnlineGramNaturalGradientOptimizer.h	2007-02-28 14:12:43 UTC (rev 6697)
+++ trunk/plearn/opt/EXPERIMENTAL/OnlineGramNaturalGradientOptimizer.h	2007-02-28 15:29:28 UTC (rev 6698)
@@ -0,0 +1,174 @@
+// -*- C++ -*-
+
+// OnlineGramNaturalGradientOptimizer.h
+//
+// Copyright (C) 2007 Pierre-Antoine Manzagol
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pierre-Antoine Manzagol
+
+/*! \file OnlineGramNaturalGradientOptimizer.h */
+
+
+#ifndef ONLINEGRAMNATURALGRADIENTOPTIMIZER_INC
+#define ONLINEGRAMNATURALGRADIENTOPTIMIZER_INC
+
+#include <plearn/opt/Optimizer.h>
+
+#include <ctime>
+
+namespace PLearn {
+using namespace std;
+
+/**
+ * Implements an online natural gradient, based on keeping an estimate
+ * of the gradients' covariance C through its main eigen vectors and values
+ * which are updated through those of the gram matrix. This is n_eigen^2
+ * instead of n_parameter^2.
+ *
+ * @todo 
+ * @deprecated 
+ */
+class OnlineGramNaturalGradientOptimizer : public Optimizer
+{
+    typedef Optimizer inherited;
+      
+public:
+    //#####  Public Build Options  ############################################
+
+    real learning_rate;
+    real gamma;
+    real reg;
+    int opt_batch_size;
+    int n_eigen;
+
+
+public:
+    //#####  Public Member Functions  #########################################    
+
+    OnlineGramNaturalGradientOptimizer();
+
+    void gramEigenNaturalGradient();
+
+    //#####  Optimizer Member Functions  #######################################
+
+    virtual bool optimizeN(VecStatsCollector& stats_coll);
+
+    //#####  PLearn::Object Protocol  #########################################
+    PLEARN_DECLARE_OBJECT(OnlineGramNaturalGradientOptimizer);
+
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+    virtual void build()
+    {
+        inherited::build();
+        build_();
+    }
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    int n_optimizeN_calls;
+
+    // The batch index
+    int bi;
+    int n_eigen_cur;
+    int n_eigen_old;
+
+    // Holds the gradients and their mean
+    Mat gradients;
+    Vec mu;
+
+    real total_variance, variance_percentage;
+
+    // the gram matrix - G = UDU' or in our case U'DU
+    Mat gram;
+    Mat U;
+    Vec D;
+
+    // The covariance matrix is C = VDV' (with the eigen vectors in V's columns)
+    // or in our case cov_eigen_vectors' diag(cov_eigen_values) cov_eigen_vectors
+    Mat cov_eigen_vec;
+    Mat old_cov_eigen_vec;
+    Mat swap;
+    Vec cov_eigen_val;
+
+    Mat cov_norm_eigen_vec;
+
+    // 
+    Vec dot_prod;
+    Vec scaled_dot_prod;
+
+    // The natural gradient, ie C^{-1} mu
+    Vec naturalg;
+
+    clock_t t0, t1, t2, t3;
+
+};
+
+DECLARE_OBJECT_PTR(OnlineGramNaturalGradientOptimizer);
+
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From manzagop at mail.berlios.de  Wed Feb 28 16:36:04 2007
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Wed, 28 Feb 2007 16:36:04 +0100
Subject: [Plearn-commits] r6699 - trunk/plearn/opt/EXPERIMENTAL
Message-ID: <200702281536.l1SFa4wO024229@sheep.berlios.de>

Author: manzagop
Date: 2007-02-28 16:36:03 +0100 (Wed, 28 Feb 2007)
New Revision: 6699

Modified:
   trunk/plearn/opt/EXPERIMENTAL/OnlineGramNaturalGradientOptimizer.cc
Log:
Corrected the computation of the total variance, which still used the formula /gamma C + (1-/gamma) g'g instead of /gamma C + g'g.


Modified: trunk/plearn/opt/EXPERIMENTAL/OnlineGramNaturalGradientOptimizer.cc
===================================================================
--- trunk/plearn/opt/EXPERIMENTAL/OnlineGramNaturalGradientOptimizer.cc	2007-02-28 15:29:28 UTC (rev 6698)
+++ trunk/plearn/opt/EXPERIMENTAL/OnlineGramNaturalGradientOptimizer.cc	2007-02-28 15:36:03 UTC (rev 6699)
@@ -270,7 +270,9 @@
         /*total_variance *= gamma;
         for( int i=0; i<gradients.length(); i++)   {
             Vec v = gradients(i);
-            total_variance += (1.-gamma) * sumsquare(v) / gradients.length();
+            // To reflect the new update
+            //total_variance += (1.-gamma) * sumsquare(v) / gradients.length();
+            total_variance += sumsquare(v) / gradients.length();
         }*/
 
         // Compute the gram matrix



From saintmlx at mail.berlios.de  Wed Feb 28 18:44:36 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Wed, 28 Feb 2007 18:44:36 +0100
Subject: [Plearn-commits] r6700 - in trunk: plearn/misc
	plearn_learners/testers scripts
Message-ID: <200702281744.l1SHiaIp010007@sheep.berlios.de>

Author: saintmlx
Date: 2007-02-28 18:44:33 +0100 (Wed, 28 Feb 2007)
New Revision: 6700

Modified:
   trunk/plearn/misc/PLearnService.cc
   trunk/plearn_learners/testers/PTester.cc
   trunk/plearn_learners/testers/PTester.h
   trunk/scripts/xdispatch
Log:
- fixed PTester to take call_forget_in_run into account



Modified: trunk/plearn/misc/PLearnService.cc
===================================================================
--- trunk/plearn/misc/PLearnService.cc	2007-02-28 15:36:03 UTC (rev 6699)
+++ trunk/plearn/misc/PLearnService.cc	2007-02-28 17:44:33 UTC (rev 6700)
@@ -388,6 +388,7 @@
         {
             // do nothing...
         }
+
     }
 }
 
@@ -404,8 +405,6 @@
 void PLearnService::progress_callback(PP<RemotePLearnServer> server, unsigned int pbar, char action, 
                                       unsigned int pos, const string& title)
 {
-    //unsigned int server_id= reinterpret_cast<unsigned int>(static_cast<RemotePLearnServer*>(server));
-    //unsigned int server_id= getServerID(server);
     static bool need_to_set_pb_plugin= true;
     if(need_to_set_pb_plugin)
     {

Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2007-02-28 15:36:03 UTC (rev 6699)
+++ trunk/plearn_learners/testers/PTester.cc	2007-02-28 17:44:33 UTC (rev 6700)
@@ -292,6 +292,7 @@
         rmm, "perform1Split", &PTester::perform1Split,
         (BodyDoc("Performs train/test for one split, returns splitres."),
          ArgDoc ("splitnum","Split number on which to perform train/test"),
+         ArgDoc ("call_forget","Whether forget() should be called in setTrainingSet()."),
          RetDoc ("Vector of test statistics corresponding to the requested statnames")));
 
     declareMethod(
@@ -748,7 +749,7 @@
 }
 
 
-Vec PTester::perform1Split(int splitnum)
+Vec PTester::perform1Split(int splitnum, bool call_forget)
 {
     if (!learner)
         PLERROR("PTester::perform1Split : No learner specified for PTester.");
@@ -819,7 +820,7 @@
                 learner->setExperimentDirectory("");
         }
 
-        learner->setTrainingSet(trainset, should_train);
+        learner->setTrainingSet(trainset, call_forget && should_train);
         if (dsets.size() > 1)
             learner->setValidationSet(dsets[1]);
 
@@ -1069,7 +1070,7 @@
                     int id;
                     s->getResults(id);
                     testers_ids[s]= id;
-                    s->callMethod(id, "perform1Split", splits_called);
+                    s->callMethod(id, "perform1Split", splits_called, call_forget);
                     splitnums[s]= splits_called;
                     ++splits_called;
                 }
@@ -1088,7 +1089,6 @@
                 if (split_stats_vm)
                 {
                     split_stats_vm->putRow(splitnums[s],splitres);
-                    //split_stats_vm->appendRow(splitres);
                     split_stats_vm->flush();
                 }
             
@@ -1096,7 +1096,7 @@
 
                 if(splits_called < nsplits)//call for another split
                 {
-                    s->callMethod(testers_ids[s], "perform1Split", splits_called);
+                    s->callMethod(testers_ids[s], "perform1Split", splits_called, call_forget);
                     ++splits_called;
                 }
                 else
@@ -1110,12 +1110,11 @@
     else
         for (int splitnum= 0; splitnum < nsplits; ++splitnum)
         {
-            Vec splitres= perform1Split(splitnum);
+            Vec splitres= perform1Split(splitnum, call_forget);
             
             if (split_stats_vm)
             {
                 split_stats_vm->putRow(splitnum, splitres);
-                //split_stats_vm->appendRow(splitres);
                 split_stats_vm->flush();
             }
             

Modified: trunk/plearn_learners/testers/PTester.h
===================================================================
--- trunk/plearn_learners/testers/PTester.h	2007-02-28 15:36:03 UTC (rev 6699)
+++ trunk/plearn_learners/testers/PTester.h	2007-02-28 17:44:33 UTC (rev 6700)
@@ -190,7 +190,7 @@
      *  statnames
      */
     Vec perform(bool call_forget=true);
-    Vec perform1Split(int splitnum);
+    Vec perform1Split(int splitnum, bool call_forget=true);
 
     Vec oldperform(bool call_forget=true);
 

Modified: trunk/scripts/xdispatch
===================================================================
--- trunk/scripts/xdispatch	2007-02-28 15:36:03 UTC (rev 6699)
+++ trunk/scripts/xdispatch	2007-02-28 17:44:33 UTC (rev 6700)
@@ -41,12 +41,12 @@
     parser.add_option('-l', '--max-load', type= "float",
                       dest= "max_load",
                       help='Sets the default maximum load',
-                      default=2.0)
+                      default=1.0)
 
     parser.add_option('-s', '--nb-servers', type="int",
                       dest= "nb_servers",
                       help='Number of servers to launch (maximum)',
-                      default=1)
+                      default=3)
    
     (options, args) = parser.parse_args()
 



From saintmlx at mail.berlios.de  Wed Feb 28 19:29:45 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Wed, 28 Feb 2007 19:29:45 +0100
Subject: [Plearn-commits] r6701 - in trunk: plearn/misc
	plearn_learners/testers
Message-ID: <200702281829.l1SITjAQ002740@sheep.berlios.de>

Author: saintmlx
Date: 2007-02-28 19:29:44 +0100 (Wed, 28 Feb 2007)
New Revision: 6701

Modified:
   trunk/plearn/misc/PLearnService.cc
   trunk/plearn_learners/testers/PTester.cc
Log:
- PTester: don't perform splits in parallel if should_train && !call_forget



Modified: trunk/plearn/misc/PLearnService.cc
===================================================================
--- trunk/plearn/misc/PLearnService.cc	2007-02-28 17:44:33 UTC (rev 6700)
+++ trunk/plearn/misc/PLearnService.cc	2007-02-28 18:29:44 UTC (rev 6701)
@@ -299,10 +299,10 @@
                     break;
                 }
             }
+            else if(c == EOF)
+                PLERROR("Got EOF while reading a RemoteServer's io stream (connection reset by peer?).");
             else //synchronous message, return server's id
-            {
                 return the_k;
-            }
         }
         else
             PLERROR("stream returned by NextPendingEvent is none of the servers' io field. This should not happen!");

Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2007-02-28 17:44:33 UTC (rev 6700)
+++ trunk/plearn_learners/testers/PTester.cc	2007-02-28 18:29:44 UTC (rev 6701)
@@ -820,7 +820,7 @@
                 learner->setExperimentDirectory("");
         }
 
-        learner->setTrainingSet(trainset, call_forget && should_train);
+        learner->setTrainingSet(trainset, call_forget);
         if (dsets.size() > 1)
             learner->setValidationSet(dsets[1]);
 
@@ -1051,7 +1051,7 @@
     TVec<PP<RemotePLearnServer> > servers= service.reserveServers(nsplits);
     int nservers= servers.length();
 
-    if(nservers > 1 && parallelize_here)
+    if(nservers > 1 && parallelize_here && (!should_train || call_forget))
     {
         map<PP<RemotePLearnServer>, int> testers_ids;
         map<PP<RemotePLearnServer>, int> splitnums;



From saintmlx at mail.berlios.de  Wed Feb 28 19:51:22 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Wed, 28 Feb 2007 19:51:22 +0100
Subject: [Plearn-commits] r6702 - trunk/plearn_learners/testers
Message-ID: <200702281851.l1SIpM0j006204@sheep.berlios.de>

Author: saintmlx
Date: 2007-02-28 19:51:22 +0100 (Wed, 28 Feb 2007)
New Revision: 6702

Modified:
   trunk/plearn_learners/testers/PTester.cc
Log:
- fixed split stats for parallel PTester::perform


Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2007-02-28 18:29:44 UTC (rev 6701)
+++ trunk/plearn_learners/testers/PTester.cc	2007-02-28 18:51:22 UTC (rev 6702)
@@ -1097,6 +1097,7 @@
                 if(splits_called < nsplits)//call for another split
                 {
                     s->callMethod(testers_ids[s], "perform1Split", splits_called, call_forget);
+                    splitnums[s]= splits_called;
                     ++splits_called;
                 }
                 else



From saintmlx at mail.berlios.de  Wed Feb 28 20:28:24 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Wed, 28 Feb 2007 20:28:24 +0100
Subject: [Plearn-commits] r6703 - trunk/scripts
Message-ID: <200702281928.l1SJSOsp011554@sheep.berlios.de>

Author: saintmlx
Date: 2007-02-28 20:28:24 +0100 (Wed, 28 Feb 2007)
New Revision: 6703

Modified:
   trunk/scripts/xdispatch
Log:
- more comments in xdispatch



Modified: trunk/scripts/xdispatch
===================================================================
--- trunk/scripts/xdispatch	2007-02-28 18:51:22 UTC (rev 6702)
+++ trunk/scripts/xdispatch	2007-02-28 19:28:24 UTC (rev 6703)
@@ -16,22 +16,27 @@
 
 usage= """
 Usage: xdispatch -s<nb_servers> [-l<max_load>] -- <plearn_command> <plearn_args>
-       Will launch nb_servers remote servers on machines with a load no
-       more than max_load for the last minute (default=1.0).
+
+       Will launch nb_servers remote servers, plus a master that connects
+       to those servers.  With ssh, servers are launched on machines with
+       a load no more than max_load for the last minute (default=1.0).
+       
        plearn_command is your favorite version of plearn.
+       
        Each server (slave) is launched like:
          ssh <remote_host> <plearn_command> server 0
+         -- or --
+         cluster --execute <plearn_command> server 0
+         
        The master is launched like (on the local host):
          <plearn_command> --servers <serv_file> <plearn_args>
        where serv_file is a file that contains one line for each
        of the running slave servers, with hostname, port and pid.
-       Once the master returns, all slaves are killed.
+       
+       Once the master dies, all slaves are killed and buried
+       with him.
 """
 
-#
-# Main
-#
-
 def main():
     parser = OptionParser()
     parser.add_option('-n', '--num-available-machines',
@@ -50,7 +55,7 @@
    
     (options, args) = parser.parse_args()
 
-    set_logdir('/tmp/xdispatch-log/')
+    set_logdir('/tmp/xdispatch-log/') 
 
     Task._max_load= options.max_load
 
@@ -59,11 +64,12 @@
     elif args:
         try:
             plearn_command= args[0]
+            # launch slaves with the desired plearn executable
             for i in xrange(options.nb_servers):
                 launch_server( ['time',plearn_command, 'server', '0'] )
 
+            # temp file for the list of slaves (host, port, pid)
             tf, fname= tempfile.mkstemp(suffix= '.plserv', prefix= '.plserv_', dir= '/tmp/')
-
             for si in Task.getLaunchedServersInfo():
                 print si
                 os.write(tf, "%s %d %d\n"%si)
@@ -73,10 +79,11 @@
             cmd= plearn_command + ' --servers ' + fname + ' ' + ' '.join(args[1:])
             print cmd
 
+            # call the master w/ all options plus a list of slave servers
             os.system(cmd)
 
         finally:
-            Task.kill_all_tasks()
+            Task.kill_all_tasks()# kill all slaves no matter what happened
     else:
         print usage
 



From larocheh at mail.berlios.de  Wed Feb 28 20:45:08 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 28 Feb 2007 20:45:08 +0100
Subject: [Plearn-commits] r6704 - trunk/plearn_learners/online
Message-ID: <200702281945.l1SJj8J3014101@sheep.berlios.de>

Author: larocheh
Date: 2007-02-28 20:45:08 +0100 (Wed, 28 Feb 2007)
New Revision: 6704

Modified:
   trunk/plearn_learners/online/CombiningCostsModule.cc
Log:
input_size and target_size are now set at build() time, and these options are redeclared...


Modified: trunk/plearn_learners/online/CombiningCostsModule.cc
===================================================================
--- trunk/plearn_learners/online/CombiningCostsModule.cc	2007-02-28 19:28:24 UTC (rev 6703)
+++ trunk/plearn_learners/online/CombiningCostsModule.cc	2007-02-28 19:45:08 UTC (rev 6704)
@@ -78,6 +78,13 @@
 
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
+    
+    redeclareOption(ol, "input_size", &CombiningCostsModule::input_size,
+                    OptionBase::learntoption,
+                    "Is set to sub_costs[0]->input_size.");
+    redeclareOption(ol, "target_size", &CombiningCostsModule::target_size,
+                    OptionBase::learntoption,
+                    "Is set to sub_costs[0]->target_size.");
 }
 
 void CombiningCostsModule::build_()
@@ -97,6 +104,27 @@
                  "should be equal to n_sub_costs (%d != %d).\n",
                  cost_weights.length(), n_sub_costs );
 
+    if(sub_costs.length() == 0)
+        PLERROR( "CombiningCostsModule::build_(): sub_costs.length()\n"
+                 "should be > 0.\n");                 
+
+    input_size = sub_costs[0]->input_size;
+    target_size = sub_costs[0]->target_size;
+    for(int i=1; i<sub_costs.length(); i++)
+    {
+        if(sub_costs[i]->input_size != input_size)
+            PLERROR( "CombiningCostsModule::build_(): sub_costs[%d]->input_size"
+                     " (%d)\n"
+                     "should be equal to %d.\n",
+                     i,sub_costs[i]->input_size, input_size);  
+
+        if(sub_costs[i]->target_size != target_size)
+            PLERROR( "CombiningCostsModule::build_(): sub_costs[%d]->target_size"
+                     " (%d)\n"
+                     "should be equal to %d.\n",
+                     i,sub_costs[i]->target_size, target_size);  
+    }
+
     sub_costs_values.resize( n_sub_costs );
     output_size = n_sub_costs+1;
 }



From lamblin at mail.berlios.de  Wed Feb 28 21:16:50 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 28 Feb 2007 21:16:50 +0100
Subject: [Plearn-commits] r6705 - trunk/scripts/Skeletons
Message-ID: <200702282016.l1SKGouY018529@sheep.berlios.de>

Author: lamblin
Date: 2007-02-28 21:16:50 +0100 (Wed, 28 Feb 2007)
New Revision: 6705

Modified:
   trunk/scripts/Skeletons/CostModule.cc
   trunk/scripts/Skeletons/CostModule.h
   trunk/scripts/Skeletons/OnlineLearningModule.cc
   trunk/scripts/Skeletons/OnlineLearningModule.h
Log:
Update the skeletons to reflect changes made to OnlineLearningModule and
CostModule.


Modified: trunk/scripts/Skeletons/CostModule.cc
===================================================================
--- trunk/scripts/Skeletons/CostModule.cc	2007-02-28 19:45:08 UTC (rev 6704)
+++ trunk/scripts/Skeletons/CostModule.cc	2007-02-28 20:16:50 UTC (rev 6705)
@@ -92,7 +92,7 @@
 // bpropUpdate //
 /////////////////
 void DERIVEDCLASS::bpropUpdate(const Vec& input, const Vec& target, real cost,
-                               Vec& input_gradient)
+                               Vec& input_gradient, bool accumulate)
 {
 }
 
@@ -107,7 +107,8 @@
 //////////////////
 /* THIS METHOD IS OPTIONAL
 void DERIVEDCLASS::bbpropUpdate(const Vec& input, const Vec& target, real cost,
-                                Vec& input_gradient, Vec& input_diag_hessian)
+                                Vec& input_gradient, Vec& input_diag_hessian,
+                                bool accumulate)
 {
 }
 */

Modified: trunk/scripts/Skeletons/CostModule.h
===================================================================
--- trunk/scripts/Skeletons/CostModule.h	2007-02-28 19:45:08 UTC (rev 6704)
+++ trunk/scripts/Skeletons/CostModule.h	2007-02-28 20:16:50 UTC (rev 6705)
@@ -47,8 +47,9 @@
     */
 
     //! Adapt based on the cost, and compute input gradient to backpropagate.
+    //! The flag indicates wether input_gradient is accumulated or set.
     virtual void bpropUpdate(const Vec& input, const Vec& target, real cost,
-                             Vec& input_gradient);
+                             Vec& input_gradient, bool accumulate=false);
 
     /* Optional
        N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
@@ -68,7 +69,8 @@
     //! If these methods are defined, you can use them INSTEAD of
     //! bpropUpdate(...)
     virtual void bbpropUpdate(const Vec& input, const Vec& target, real cost,
-                              Vec& input_gradient, Vec& input_diag_hessian);
+                              Vec& input_gradient, Vec& input_diag_hessian,
+                              bool accumulate=false);
     */
 
     /* Optional
@@ -103,8 +105,9 @@
     */
 
     /* Optional
-       Default implementation does nothing
-    //! If this class has a learning rate (or something close to it), set it
+       Default implementation prints a warning and does nothing
+    //! If this class has a learning rate (or something close to it), set it.
+    //! If not, you can redefine this method to get rid of the warning.
     virtual void setLearningRate(real dynamic_learning_rate);
     */
 

Modified: trunk/scripts/Skeletons/OnlineLearningModule.cc
===================================================================
--- trunk/scripts/Skeletons/OnlineLearningModule.cc	2007-02-28 19:45:08 UTC (rev 6704)
+++ trunk/scripts/Skeletons/OnlineLearningModule.cc	2007-02-28 20:16:50 UTC (rev 6705)
@@ -88,7 +88,8 @@
 /* THIS METHOD IS OPTIONAL
 void DERIVEDCLASS::bpropUpdate(const Vec& input, const Vec& output,
                                Vec& input_gradient,
-                               const Vec& output_gradient)
+                               const Vec& output_gradient,
+                               bool accumulate)
 {
 }
 */
@@ -108,7 +109,8 @@
                                 Vec& input_gradient,
                                 const Vec& output_gradient,
                                 Vec& input_diag_hessian,
-                                const Vec& output_diag_hessian)
+                                const Vec& output_diag_hessian,
+                                bool accumulate)
 {
 }
 */

Modified: trunk/scripts/Skeletons/OnlineLearningModule.h
===================================================================
--- trunk/scripts/Skeletons/OnlineLearningModule.h	2007-02-28 19:45:08 UTC (rev 6704)
+++ trunk/scripts/Skeletons/OnlineLearningModule.h	2007-02-28 20:16:50 UTC (rev 6705)
@@ -41,6 +41,7 @@
     /* Optional
        THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
     //! Adapt based on the output gradient, and obtain the input gradient.
+    //! The flag indicates wether the input_gradient is accumulated or set.
     //! This method should only be called just after a corresponding
     //! fprop; it should be called with the same arguments as fprop
     //! for the first two arguments (and output should not have been
@@ -49,7 +50,8 @@
     //! is 'ready-to-be-used' just after any bpropUpdate.
     virtual void bpropUpdate(const Vec& input, const Vec& output,
                              Vec& input_gradient,
-                             const Vec& output_gradient);
+                             const Vec& output_gradient,
+                             bool accumulate=false);
     */
 
     /* Optional
@@ -73,7 +75,8 @@
                               Vec& input_gradient,
                               const Vec& output_gradient,
                               Vec& input_diag_hessian,
-                              const Vec& output_diag_hessian);
+                              const Vec& output_diag_hessian,
+                              bool accumulate=false);
     */
 
     /* Optional
@@ -111,8 +114,9 @@
     */
 
     /* Optional
-       Default implementation does nothing
-    //! If this class has a learning rate (or something close to it), set it
+       Default implementation prints a warning and does nothing
+    //! If this class has a learning rate (or something close to it), set it.
+    //! If not, you can redefine this method to get rid of the warning.
     virtual void setLearningRate(real dynamic_learning_rate);
     */
 



From lamblin at mail.berlios.de  Wed Feb 28 21:25:55 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 28 Feb 2007 21:25:55 +0100
Subject: [Plearn-commits] r6706 - trunk/plearn_learners/online
Message-ID: <200702282025.l1SKPtmk019693@sheep.berlios.de>

Author: lamblin
Date: 2007-02-28 21:25:54 +0100 (Wed, 28 Feb 2007)
New Revision: 6706

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/GradNNetLayerModule.cc
   trunk/plearn_learners/online/GradNNetLayerModule.h
   trunk/plearn_learners/online/OnlineLearningModule.h
Log:
Put back the setting of final_module's and final_cost's learning rate in
DeepBeliefNet::setLearningRate()
Implement GradNNetLayerModule::setLearningRate()



Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-28 20:16:50 UTC (rev 6705)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-28 20:25:54 UTC (rev 6706)
@@ -1313,6 +1313,12 @@
             the_learning_rate );
         joint_layer->setLearningRate( the_learning_rate );
     }
+
+    if( final_module )
+        final_module->setLearningRate( the_learning_rate );
+
+    if( final_cost )
+        final_cost->setLearningRate( the_learning_rate );
 }
 
 

Modified: trunk/plearn_learners/online/GradNNetLayerModule.cc
===================================================================
--- trunk/plearn_learners/online/GradNNetLayerModule.cc	2007-02-28 20:16:50 UTC (rev 6705)
+++ trunk/plearn_learners/online/GradNNetLayerModule.cc	2007-02-28 20:25:54 UTC (rev 6706)
@@ -254,6 +254,12 @@
     step_number = 0;
 }
 
+void GradNNetLayerModule::setLearningRate( real dynamic_learning_rate )
+{
+    start_learning_rate = dynamic_learning_rate;
+    step_number = 0;
+    // learning_rate will automaticly be set in bpropUpdate()
+}
 
 void GradNNetLayerModule::build()
 {

Modified: trunk/plearn_learners/online/GradNNetLayerModule.h
===================================================================
--- trunk/plearn_learners/online/GradNNetLayerModule.h	2007-02-28 20:16:50 UTC (rev 6705)
+++ trunk/plearn_learners/online/GradNNetLayerModule.h	2007-02-28 20:25:54 UTC (rev 6706)
@@ -130,6 +130,8 @@
 
     virtual void forget();
 
+    virtual void setLearningRate(real dynamic_learning_rate);
+
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.

Modified: trunk/plearn_learners/online/OnlineLearningModule.h
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.h	2007-02-28 20:16:50 UTC (rev 6705)
+++ trunk/plearn_learners/online/OnlineLearningModule.h	2007-02-28 20:25:54 UTC (rev 6706)
@@ -140,8 +140,8 @@
                               const Vec& output_diag_hessian);
 
     //! this version allows to obtain the input gradient and diag_hessian
-    //! The flag indicates whether the input_gradient and input_diag_hessian gets
-    //! accumulated into or set with the computed derivatives.
+    //! The flag indicates whether the input_gradient and input_diag_hessian
+    //! gets accumulated into or set with the computed derivatives.
     virtual void bbpropUpdate(const Vec& input, const Vec& output,
                               Vec& input_gradient,
                               const Vec& output_gradient,
@@ -170,8 +170,6 @@
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.
-    // ### If your class is not instantiatable (it has pure virtual methods)
-    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
     PLEARN_DECLARE_ABSTRACT_OBJECT(OnlineLearningModule);
 
     // Simply calls inherited::build() then build_()



