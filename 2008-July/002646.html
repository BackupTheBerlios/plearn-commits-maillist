<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r9198 - trunk/plearn_learners_experimental
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2008-July/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9198%20-%20trunk/plearn_learners_experimental&In-Reply-To=%3C200807022306.m62N62gx011874%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="002645.html">
   <LINK REL="Next"  HREF="002647.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r9198 - trunk/plearn_learners_experimental</H1>
    <B>laulysta at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9198%20-%20trunk/plearn_learners_experimental&In-Reply-To=%3C200807022306.m62N62gx011874%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r9198 - trunk/plearn_learners_experimental">laulysta at mail.berlios.de
       </A><BR>
    <I>Thu Jul  3 01:06:02 CEST 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="002645.html">[Plearn-commits] r9197 - trunk/plearn_learners/online
</A></li>
        <LI>Next message: <A HREF="002647.html">[Plearn-commits] r9199 - in trunk/plearn: base io vmat
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2646">[ date ]</a>
              <a href="thread.html#2646">[ thread ]</a>
              <a href="subject.html#2646">[ subject ]</a>
              <a href="author.html#2646">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: laulysta
Date: 2008-07-03 01:06:01 +0200 (Thu, 03 Jul 2008)
New Revision: 9198

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
Log:
Development progressing...


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-07-02 18:53:14 UTC (rev 9197)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-07-02 23:06:01 UTC (rev 9198)
@@ -58,7 +58,7 @@
 //     * du code pour entra&#238;ner s&#233;par&#233;ment les hidden_connections (si pr&#233;sentes)
 // - pourrait avoir le gradient du denoising recurrent net en m&#234;me temps que
 //   celui du &quot;fine-tuning&quot;
-// - add dynamic_activations_list and use it in recurrent_update
+// - add dynamic_activations_list and use it in recurrentUpdate
 
 
 namespace PLearn {
@@ -70,13 +70,20 @@
     &quot;&quot;
     );
 
-
 DenoisingRecurrentNet::DenoisingRecurrentNet() :
-    recurrent_net_learning_rate( 0.01),
     use_target_layers_masks( false ),
     end_of_sequence_symbol( -1000 ),
     encoding(&quot;raw_masked_supervised&quot;),
-    input_window_size(1)
+    input_window_size(1),
+    tied_input_reconstruction_weights( true ),
+    input_noise_prob( 0.15 ),
+    input_reconstruction_lr( 0 ),
+    hidden_noise_prob( 0.15 ),
+    hidden_reconstruction_lr( 0 ),
+    tied_hidden_reconstruction_weights( false ),
+    noisy_recurrent_lr( 0),
+    dynamic_gradient_scale_factor( 0.5 ),
+    recurrent_lr( 0.01 )
 {
     random_gen = new PRandom();
 }
@@ -88,11 +95,6 @@
 //                  &quot;The learning rate used during RBM contrastive &quot;
 //                  &quot;divergence learning phase.\n&quot;);
 
-    declareOption(ol, &quot;recurrent_net_learning_rate&quot;, 
-                  &amp;DenoisingRecurrentNet::recurrent_net_learning_rate,
-                  OptionBase::buildoption,
-                  &quot;The learning rate used during the recurrent phase.\n&quot;);
-
 //    declareOption(ol, &quot;rbm_nstages&quot;, &amp;DenoisingRecurrentNet::rbm_nstages,
 //                  OptionBase::buildoption,
 //                  &quot;Number of epochs for rbm phase.\n&quot;);
@@ -103,12 +105,6 @@
                   OptionBase::buildoption,
                   &quot;The training weights of each target layers.\n&quot;);
 
-    declareOption(ol, &quot;use_target_layers_masks&quot;, 
-                  &amp;DenoisingRecurrentNet::use_target_layers_masks,
-                  OptionBase::buildoption,
-                  &quot;Indication that a mask indicating which target to predict\n&quot;
-                  &quot;is present in the input part of the VMatrix dataset.\n&quot;);
-
     declareOption(ol, &quot;end_of_sequence_symbol&quot;, 
                   &amp;DenoisingRecurrentNet::end_of_sequence_symbol,
                   OptionBase::buildoption,
@@ -153,19 +149,6 @@
                   OptionBase::buildoption,
                   &quot;The RBMConnection from input_layer to hidden_layer.\n&quot;);
 
-    declareOption(ol, &quot;encoding&quot;, 
-                  &amp;DenoisingRecurrentNet::encoding,
-                  OptionBase::buildoption,
-                  &quot;Chooses what type of encoding to apply to an input sequence\n&quot;
-                  &quot;Possibilities: timeframe, note_duration, note_octav_duration, raw_masked_supervised&quot;);
-
-    declareOption(ol, &quot;input_window_size&quot;, 
-                  &amp;DenoisingRecurrentNet::input_window_size,
-                  OptionBase::buildoption,
-                  &quot;How many time steps to present as input\n&quot;
-                  &quot;This option is ignored when mode is raw_masked_supervised,&quot;
-                  &quot;since in this mode the full expanded and preprocessed input and target are given explicitly.&quot;);
-
     declareOption(ol, &quot;target_layers_n_of_target_elements&quot;, 
                   &amp;DenoisingRecurrentNet::target_layers_n_of_target_elements,
                   OptionBase::learntoption,
@@ -182,7 +165,73 @@
                   OptionBase::learntoption,
                   &quot;Number of symbols for each symbolic field of train_set.\n&quot;);
 
-    /*
+
+
+
+
+    
+    declareOption(ol, &quot;encoding&quot;, 
+                  &amp;DenoisingRecurrentNet::encoding,
+                  OptionBase::buildoption,
+                  &quot;Chooses what type of encoding to apply to an input sequence\n&quot;
+                  &quot;Possibilities: timeframe, note_duration, note_octav_duration, raw_masked_supervised&quot;);
+
+    declareOption(ol, &quot;input_window_size&quot;, 
+                  &amp;DenoisingRecurrentNet::input_window_size,
+                  OptionBase::buildoption,
+                  &quot;How many time steps to present as input\n&quot;
+                  &quot;This option is ignored when mode is raw_masked_supervised,&quot;
+                  &quot;since in this mode the full expanded and preprocessed input and target are given explicitly.&quot;);
+
+    declareOption(ol, &quot;tied_input_reconstruction_weights&quot;, 
+                  &amp;DenoisingRecurrentNet::tied_input_reconstruction_weights,
+                  OptionBase::buildoption,
+                  &quot;Do we want the input reconstruction weights tied or not\n&quot;
+                  &quot;Boolean, yes or no&quot;);
+
+    declareOption(ol, &quot;input_noise_prob&quot;, 
+                  &amp;DenoisingRecurrentNet::input_noise_prob,
+                  OptionBase::buildoption,
+                  &quot;Probability, for each neurone of each input, to be set to zero\n&quot;);
+
+    declareOption(ol, &quot;input_reconstruction_lr&quot;, 
+                  &amp;DenoisingRecurrentNet::input_reconstruction_lr,
+                  OptionBase::buildoption,
+                  &quot;The learning rate used for the reconstruction\n&quot;);
+
+    declareOption(ol, &quot;hidden_noise_prob&quot;, 
+                  &amp;DenoisingRecurrentNet::hidden_noise_prob,
+                  OptionBase::buildoption,
+                  &quot;Probability, for each neurone of each hidden layer, to be set to zero\n&quot;);
+
+    declareOption(ol, &quot;hidden_reconstruction_lr&quot;, 
+                  &amp;DenoisingRecurrentNet::hidden_reconstruction_lr,
+                  OptionBase::buildoption,
+                  &quot;The learning rate used for the dynamic reconstruction through time\n&quot;);
+
+    declareOption(ol, &quot;tied_hidden_reconstruction_weights&quot;, 
+                  &amp;DenoisingRecurrentNet::tied_hidden_reconstruction_weights,
+                  OptionBase::buildoption,
+                  &quot;Do we want the dynamic reconstruction weights tied or not\n&quot;
+                  &quot;Boolean, yes or no&quot;);
+
+    declareOption(ol, &quot;noisy_recurrent_lr&quot;, 
+                  &amp;DenoisingRecurrentNet::noisy_recurrent_lr,
+                  OptionBase::buildoption,
+                  &quot;The learning rate used in the noisy recurrent phase for the input reconstruction\n&quot;);
+
+    declareOption(ol, &quot;dynamic_gradient_scale_factor&quot;, 
+                  &amp;DenoisingRecurrentNet::dynamic_gradient_scale_factor,
+                  OptionBase::buildoption,
+                  &quot;The scale factor of the learning rate used in the noisy recurrent phase for the dynamic hidden reconstruction\n&quot;);
+
+    declareOption(ol, &quot;recurrent_lr&quot;, 
+                  &amp;DenoisingRecurrentNet::recurrent_lr,
+                  OptionBase::buildoption,
+                  &quot;The learning rate used in the fine tuning phase\n&quot;);
+
+
+ /*
     declareOption(ol, &quot;&quot;, &amp;DenoisingRecurrentNet::,
                   OptionBase::learntoption,
                   &quot;&quot;);
@@ -209,6 +258,8 @@
 
     if(train_set)
     {
+        use_target_layers_masks = (encoding==&quot;raw_masked_supervised&quot;);
+
         PLASSERT( target_layers_weights.length() == target_layers.length() );
         PLASSERT( target_connections.length() == target_layers.length() );
         PLASSERT( target_layers.length() &gt; 0 );
@@ -497,15 +548,19 @@
 
         MODULE_LOG &lt;&lt; &quot;  stage = &quot; &lt;&lt; stage &lt;&lt; endl;
         MODULE_LOG &lt;&lt; &quot;  end_stage = &quot; &lt;&lt; end_stage &lt;&lt; endl;
-        MODULE_LOG &lt;&lt; &quot;  recurrent_net_learning_rate = &quot; &lt;&lt; recurrent_net_learning_rate &lt;&lt; endl;
+        MODULE_LOG &lt;&lt; &quot;  input_noise_prob = &quot; &lt;&lt;                input_noise_prob  &lt;&lt; endl;              
+        MODULE_LOG &lt;&lt; &quot;  input_reconstruction_lr = &quot; &lt;&lt;         input_reconstruction_lr  &lt;&lt; endl;       
+        MODULE_LOG &lt;&lt; &quot;  hidden_noise_prob = &quot; &lt;&lt;               hidden_noise_prob  &lt;&lt; endl;             
+        MODULE_LOG &lt;&lt; &quot;  hidden_reconstruction_lr = &quot; &lt;&lt;        hidden_reconstruction_lr  &lt;&lt; endl;      
+        MODULE_LOG &lt;&lt; &quot;  noisy_recurrent_lr = &quot; &lt;&lt;              noisy_recurrent_lr  &lt;&lt; endl;            
+        MODULE_LOG &lt;&lt; &quot;  dynamic_gradient_scale_factor = &quot; &lt;&lt;   dynamic_gradient_scale_factor  &lt;&lt; endl; 
+        MODULE_LOG &lt;&lt; &quot;  recurrent_lr = &quot; &lt;&lt;                    recurrent_lr  &lt;&lt; endl;                  
 
+
         if( report_progress &amp;&amp; stage &lt; end_stage )
             pb = new ProgressBar( &quot;Recurrent training phase of &quot;+classname(),
                                   end_stage - init_stage );
 
-        // TO DO: check this line
-        setLearningRate( recurrent_net_learning_rate );
-
         while(stage &lt; end_stage)
         {
             train_costs.clear();
@@ -516,8 +571,36 @@
             {
                 getSequence(i, seq);
                 encodeSequenceAndPopulateLists(seq);
-                fprop(train_costs, train_n_items);
-                recurrent_update();
+              
+                bool corrupt_input = input_noise_prob!=0 &amp;&amp; (noisy_recurrent_lr!=0 || input_reconstruction_lr!=0);
+
+                clean_encoded_seq.resize(encoded_seq.length(), encoded_seq.width());
+                clean_encoded_seq &lt;&lt; encoded_seq;
+
+                if(corrupt_input)  // WARNING: encoded_sequence will be dirty!!!!
+                    inject_zero_forcing_noise(encoded_seq, input_noise_prob);
+                
+                // greedy phase
+                if(input_reconstruction_lr!=0 || hidden_reconstruction_lr!=0)
+                    performGreedyDenoisingPhase();
+
+                // recurrent noisy phase
+                if(noisy_recurrent_lr!=0)
+                {
+                    setLearningRate( noisy_recurrent_lr );
+                    recurrentFprop(train_costs, train_n_items);
+                    recurrentUpdate();
+                }
+
+                // recurrent no noise phase
+                if(recurrent_lr!=0)
+                {
+                    if(corrupt_input) // need to recover the clean sequence
+                        encoded_seq &lt;&lt; clean_encoded_seq;                    
+                    setLearningRate( recurrent_lr );
+                    recurrentFprop(train_costs, train_n_items);
+                    recurrentUpdate();
+                }
             }
 
             if( pb )
@@ -549,6 +632,13 @@
 }
 
 
+void DenoisingRecurrentNet::performGreedyDenoisingPhase()
+{
+    // TO DO!
+    PLERROR(&quot;performGreedyDenoisingPhase not yet implemented&quot;);
+}
+
+
 //! does encoding if needed and populates the list.
 void DenoisingRecurrentNet::encodeSequenceAndPopulateLists(Mat seq) const
 {
@@ -558,24 +648,30 @@
         encodeAndCreateSupervisedSequence(seq);
 }
 
-// TO DO: penser a gestion des prepended dans ce cas
-// encodes sequ, then populates: inputslist, targets_list, masks_list
+// encodes sequ, then populates: input_list, targets_list, masks_list
 void DenoisingRecurrentNet::encodeAndCreateSupervisedSequence(Mat seq) const
 {
+    if(use_target_layers_masks)
+        PLERROR(&quot;Bug: use_target_layers_masks is expected to be false (no masks) when in encodeAndCreateSupervisedSequence&quot;);
+
     encodeSequence(seq, encoded_seq);
     // now work with encoded_seq
     int l = encoded_seq.length();
     resize_lists(l);
 
-    // TO DO: populate lists
-    // ....
-
-    PLERROR(&quot;Not implemented yet&quot;);
+    Mat targets = targets_list[0];
+    targets.resize(l, encoded_seq.width());
+                   
+    for(int t=input_window_size; t&lt;l; t++)
+    {
+        input_list[t] = encoded_seq.subMatRows(t-input_window_size,input_window_size).toVec();
+        // target is copied so that when adding noise to input, it doesn't modify target 
+        targets(t) &lt;&lt; encoded_seq(t);
+    }
 }
 
 
 
-// TO DO: penser a prepend dans ce cas
 
 // For the (backward testing) raw_masked_supervised case. Populates: input_list, targets_list, masks_list
 void DenoisingRecurrentNet::splitRawMaskedSupervisedSequence(Mat seq) const
@@ -633,7 +729,7 @@
 // TODO: think properly about prepended stuff
 
 // fprop accumulates costs in costs and counts in n_items
-void DenoisingRecurrentNet::fprop(Vec train_costs, Vec train_n_items) const
+void DenoisingRecurrentNet::recurrentFprop(Vec train_costs, Vec train_n_items) const
 {
     int l = input_list.length();
     int ntargets = target_layers.length();
@@ -709,7 +805,7 @@
 nll_list
 */
 
-void DenoisingRecurrentNet::recurrent_update()
+void DenoisingRecurrentNet::recurrentUpdate()
 {
     hidden_temporal_gradient.resize(hidden_layer-&gt;size);
     hidden_temporal_gradient.clear();
@@ -813,413 +909,10 @@
     
 }
 
-/*
-void DenoisingRecurrentNet::old_recurrent_update()
-{
-    hidden_temporal_gradient.resize(hidden_layer-&gt;size);
-    hidden_temporal_gradient.clear();
-    for(int i=hidden_list.length()-1; i&gt;=0; i--){   
 
-        if( hidden_layer2 )
-            hidden_gradient.resize(hidden_layer2-&gt;size);
-        else
-            hidden_gradient.resize(hidden_layer-&gt;size);
-        hidden_gradient.clear();
-        if(use_target_layers_masks)
-        {
-            for( int tar=0; tar&lt;target_layers.length(); tar++)
-            {
-                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                {
-                    target_layers[tar]-&gt;activation &lt;&lt; target_prediction_act_no_bias_list[tar][i];
-                    target_layers[tar]-&gt;activation += target_layers[tar]-&gt;bias;
-                    target_layers[tar]-&gt;setExpectation(target_prediction_list[tar][i]);
-                    target_layers[tar]-&gt;bpropNLL(targets_list[tar][i],nll_list(i,tar),bias_gradient);
-                    bias_gradient *= target_layers_weights[tar];
-                    bias_gradient *= masks_list[tar][i];
-                    target_layers[tar]-&gt;update(bias_gradient);
-                    if( hidden_layer2 )
-                        target_connections[tar]-&gt;bpropUpdate(hidden2_list[i],target_prediction_act_no_bias_list[tar][i],
-                                                             hidden_gradient, bias_gradient,true);
-                    else
-                        target_connections[tar]-&gt;bpropUpdate(hidden_list[i],target_prediction_act_no_bias_list[tar][i],
-                                                             hidden_gradient, bias_gradient,true);
-                }
-            }
-        }
-        else
-        {
-            for( int tar=0; tar&lt;target_layers.length(); tar++)
-            {
-                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                {
-                    target_layers[tar]-&gt;activation &lt;&lt; target_prediction_act_no_bias_list[tar][i];
-                    target_layers[tar]-&gt;activation += target_layers[tar]-&gt;bias;
-                    target_layers[tar]-&gt;setExpectation(target_prediction_list[tar][i]);
-                    target_layers[tar]-&gt;bpropNLL(targets_list[tar][i],nll_list(i,tar),bias_gradient);
-                    bias_gradient *= target_layers_weights[tar];
-                    target_layers[tar]-&gt;update(bias_gradient);
-                    if( hidden_layer2 )
-                        target_connections[tar]-&gt;bpropUpdate(hidden2_list[i],target_prediction_act_no_bias_list[tar][i],
-                                                             hidden_gradient, bias_gradient,true); 
-                    else
-                        target_connections[tar]-&gt;bpropUpdate(hidden_list[i],target_prediction_act_no_bias_list[tar][i],
-                                                             hidden_gradient, bias_gradient,true); 
-                        
-                }
-            }
-        }
-
-        if (hidden_layer2)
-        {
-            hidden_layer2-&gt;bpropUpdate(
-                hidden2_act_no_bias_list[i], hidden2_list[i],
-                bias_gradient, hidden_gradient);
-                
-            hidden_connections-&gt;bpropUpdate(
-                hidden_list[i],
-                hidden2_act_no_bias_list[i], 
-                hidden_gradient, bias_gradient);
-        }
-            
-        if(i!=0 &amp;&amp; dynamic_connections )
-        {   
-            hidden_gradient += hidden_temporal_gradient;
-                
-            hidden_layer-&gt;bpropUpdate(
-                hidden_act_no_bias_list[i], hidden_list[i],
-                hidden_temporal_gradient, hidden_gradient);
-                
-            dynamic_connections-&gt;bpropUpdate(
-                hidden_list[i-1],
-                hidden_act_no_bias_list[i], // Here, it should be cond_bias, but doesn't matter
-                hidden_gradient, hidden_temporal_gradient);
-                
-            hidden_temporal_gradient &lt;&lt; hidden_gradient;
-                
-            input_connections-&gt;bpropUpdate(
-                input_list[i],
-                hidden_act_no_bias_list[i], 
-                visi_bias_gradient, hidden_temporal_gradient);// Here, it should be activations - cond_bias, but doesn't matter
-                
-        }
-        else
-        {
-            hidden_layer-&gt;bpropUpdate(
-                hidden_act_no_bias_list[i], hidden_list[i],
-                hidden_temporal_gradient, hidden_gradient); // Not really temporal gradient, but this is the final iteration...
-            input_connections-&gt;bpropUpdate(
-                input_list[i],
-                hidden_act_no_bias_list[i], 
-                visi_bias_gradient, hidden_temporal_gradient);// Here, it should be activations - cond_bias, but doesn't matter
-
-        }
-    }
-    
-}
-*/
-
-
-/*
-void DenoisingRecurrentNet::oldtrain()
-{
-    MODULE_LOG &lt;&lt; &quot;train() called &quot; &lt;&lt; endl;
-
-    Vec input( inputsize() );
-    Vec target( targetsize() );
-    real weight = 0; // Unused
-    Vec train_costs( getTrainCostNames().length() );
-    train_costs.clear();
-    Vec train_n_items( getTrainCostNames().length() );
-
-    if( !initTrain() )
-    {
-        MODULE_LOG &lt;&lt; &quot;train() aborted&quot; &lt;&lt; endl;
-        return;
-    }
-
-    ProgressBar* pb = 0;
-
-    // clear stats of previous epoch
-    train_stats-&gt;forget();
-
-
-//    if(rbm_stage &lt; rbm_nstages)
-//    {
-//    }
-
-
-    if( stage &gt;= nstages )
-        return;
-
-    if( stage &lt; nstages )
-    {        
-
-        MODULE_LOG &lt;&lt; &quot;Training the whole model&quot; &lt;&lt; endl;
-
-        int init_stage = stage;
-        //int end_stage = max(0,nstages-(rbm_nstages + dynamic_nstages));
-        int end_stage = nstages;
-
-        MODULE_LOG &lt;&lt; &quot;  stage = &quot; &lt;&lt; stage &lt;&lt; endl;
-        MODULE_LOG &lt;&lt; &quot;  end_stage = &quot; &lt;&lt; end_stage &lt;&lt; endl;
-        MODULE_LOG &lt;&lt; &quot;  recurrent_net_learning_rate = &quot; &lt;&lt; recurrent_net_learning_rate &lt;&lt; endl;
-
-        if( report_progress &amp;&amp; stage &lt; end_stage )
-            pb = new ProgressBar( &quot;Recurrent training phase of &quot;+classname(),
-                                  end_stage - init_stage );
-
-        setLearningRate( recurrent_net_learning_rate );
-
-        int ith_sample_in_sequence = 0;
-        int inputsize_without_masks = inputsize() 
-            - ( use_target_layers_masks ? targetsize() : 0 );
-        int sum_target_elements = 0;
-        while(stage &lt; end_stage)
-        {
-            train_costs.clear();
-            train_n_items.clear();
-            for(int sample=0 ; sample&lt;train_set-&gt;length() ; sample++ )
-            {
-                train_set-&gt;getExample(sample, input, target, weight);
-
-                if( fast_exact_is_equal(input[0],end_of_sequence_symbol) )
-                {
-                    //update
-                    recurrent_update();
-                    
-                    ith_sample_in_sequence = 0;
-                    hidden_list.resize(0);
-                    hidden_act_no_bias_list.resize(0);
-                    hidden2_list.resize(0);
-                    hidden2_act_no_bias_list.resize(0);
-                    target_prediction_list.resize(0);
-                    target_prediction_act_no_bias_list.resize(0);
-                    input_list.resize(0);
-                    targets_list.resize(0);
-                    nll_list.resize(0,0);
-                    masks_list.resize(0);
-                    continue;
-                }
-
-                // Resize internal variables
-                hidden_list.resize(ith_sample_in_sequence+1);
-                hidden_act_no_bias_list.resize(ith_sample_in_sequence+1);
-                if( hidden_layer2 )
-                {
-                    hidden2_list.resize(ith_sample_in_sequence+1);
-                    hidden2_act_no_bias_list.resize(ith_sample_in_sequence+1);
-                }
-                 
-                input_list.resize(ith_sample_in_sequence+1);
-                input_list[ith_sample_in_sequence].resize(input_layer-&gt;size);
-
-                targets_list.resize( target_layers.length() );
-                target_prediction_list.resize( target_layers.length() );
-                target_prediction_act_no_bias_list.resize( target_layers.length() );
-                for( int tar=0; tar &lt; target_layers.length(); tar++ )
-                {
-                    if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                    {                        
-                        targets_list[tar].resize( ith_sample_in_sequence+1);
-                        targets_list[tar][ith_sample_in_sequence].resize( 
-                            target_layers[tar]-&gt;size);
-                        target_prediction_list[tar].resize(
-                            ith_sample_in_sequence+1);
-                        target_prediction_act_no_bias_list[tar].resize(
-                            ith_sample_in_sequence+1);
-                    }
-                }
-                nll_list.resize(ith_sample_in_sequence+1,target_layers.length());
-                if( use_target_layers_masks )
-                {
-                    masks_list.resize( target_layers.length() );
-                    for( int tar=0; tar &lt; target_layers.length(); tar++ )
-                        if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                            masks_list[tar].resize( ith_sample_in_sequence+1 );
-                }
-
-                // Forward propagation
-
-                // Fetch right representation for input
-                clamp_units(input.subVec(0,inputsize_without_masks),
-                            input_layer,
-                            input_symbol_sizes);                
-                input_list[ith_sample_in_sequence] &lt;&lt; input_layer-&gt;expectation;
-
-                // Fetch right representation for target
-                sum_target_elements = 0;
-                for( int tar=0; tar &lt; target_layers.length(); tar++ )
-                {
-                    if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                    {
-                        if( use_target_layers_masks )
-                        {
-                            clamp_units(target.subVec(
-                                            sum_target_elements,
-                                            target_layers_n_of_target_elements[tar]),
-                                        target_layers[tar],
-                                        target_symbol_sizes[tar],
-                                        input.subVec(
-                                            inputsize_without_masks 
-                                            + sum_target_elements, 
-                                            target_layers_n_of_target_elements[tar]),
-                                        masks_list[tar][ith_sample_in_sequence]
-                                );
-                            
-                        }
-                        else
-                        {
-                            clamp_units(target.subVec(
-                                            sum_target_elements,
-                                            target_layers_n_of_target_elements[tar]),
-                                        target_layers[tar],
-                                        target_symbol_sizes[tar]);
-                        }
-                        targets_list[tar][ith_sample_in_sequence] &lt;&lt; 
-                            target_layers[tar]-&gt;expectation;
-                    }
-                    sum_target_elements += target_layers_n_of_target_elements[tar];
-                }
-                
-                input_connections-&gt;fprop( input_list[ith_sample_in_sequence], 
-                                          hidden_act_no_bias_list[ith_sample_in_sequence]);
-                
-                if( ith_sample_in_sequence &gt; 0 &amp;&amp; dynamic_connections )
-                {
-                    dynamic_connections-&gt;fprop( 
-                        hidden_list[ith_sample_in_sequence-1],
-                        dynamic_act_no_bias_contribution );
-
-                    hidden_act_no_bias_list[ith_sample_in_sequence] += 
-                        dynamic_act_no_bias_contribution;
-                }
-                 
-                hidden_layer-&gt;fprop( hidden_act_no_bias_list[ith_sample_in_sequence], 
-                                     hidden_list[ith_sample_in_sequence] );
-                 
-                if( hidden_layer2 )
-                {
-                    hidden_connections-&gt;fprop( 
-                        hidden_list[ith_sample_in_sequence],
-                        hidden2_act_no_bias_list[ith_sample_in_sequence]);
-
-                    hidden_layer2-&gt;fprop( 
-                        hidden2_act_no_bias_list[ith_sample_in_sequence],
-                        hidden2_list[ith_sample_in_sequence] 
-                        );
-
-                    for( int tar=0; tar &lt; target_layers.length(); tar++ )
-                    {
-                        if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                        {
-                            target_connections[tar]-&gt;fprop(
-                                hidden2_list[ith_sample_in_sequence],
-                                target_prediction_act_no_bias_list[tar][
-                                    ith_sample_in_sequence]
-                                );
-                            target_layers[tar]-&gt;fprop(
-                                target_prediction_act_no_bias_list[tar][
-                                    ith_sample_in_sequence],
-                                target_prediction_list[tar][
-                                    ith_sample_in_sequence] );
-                            if( use_target_layers_masks )
-                                target_prediction_list[tar][ ith_sample_in_sequence] *= 
-                                    masks_list[tar][ith_sample_in_sequence];
-                        }
-                    }
-                }
-                else
-                {
-                    for( int tar=0; tar &lt; target_layers.length(); tar++ )
-                    {
-                        if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                        {
-                            target_connections[tar]-&gt;fprop(
-                                hidden_list[ith_sample_in_sequence],
-                                target_prediction_act_no_bias_list[tar][
-                                    ith_sample_in_sequence]
-                                );
-                            target_layers[tar]-&gt;fprop(
-                                target_prediction_act_no_bias_list[tar][
-                                    ith_sample_in_sequence],
-                                target_prediction_list[tar][
-                                    ith_sample_in_sequence] );
-                            if( use_target_layers_masks )
-                                target_prediction_list[tar][ ith_sample_in_sequence] *= 
-                                    masks_list[tar][ith_sample_in_sequence];
-                        }
-                    }
-                }
-
-                sum_target_elements = 0;
-                for( int tar=0; tar &lt; target_layers.length(); tar++ )
-                {
-                    if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                    {
-                        target_layers[tar]-&gt;activation &lt;&lt; 
-                            target_prediction_act_no_bias_list[tar][
-                                ith_sample_in_sequence];
-                        target_layers[tar]-&gt;activation += target_layers[tar]-&gt;bias;
-                        target_layers[tar]-&gt;setExpectation(
-                            target_prediction_list[tar][
-                                ith_sample_in_sequence]);
-                        nll_list(ith_sample_in_sequence,tar) = 
-                            target_layers[tar]-&gt;fpropNLL( 
-                                targets_list[tar][ith_sample_in_sequence] ); 
-                        train_costs[tar] += nll_list(ith_sample_in_sequence,tar);
-                        
-                        // Normalize by the number of things to predict
-                        if( use_target_layers_masks )
-                        {
-                            train_n_items[tar] += sum(
-                                input.subVec( inputsize_without_masks 
-                                              + sum_target_elements, 
-                                              target_layers_n_of_target_elements[tar]) );
-                        }
-                        else
-                            train_n_items[tar]++;
-                    }
-                    if( use_target_layers_masks )
-                        sum_target_elements += 
-                            target_layers_n_of_target_elements[tar];
-                    
-                }
-                ith_sample_in_sequence++;
-            }
-            if( pb )
-                pb-&gt;update( stage + 1 - init_stage);
-            
-            for(int i=0; i&lt;train_costs.length(); i++)
-            {
-                if( !fast_exact_is_equal(target_layers_weights[i],0) )
-                    train_costs[i] /= train_n_items[i];
-                else
-                    train_costs[i] = MISSING_VALUE;
-            }
-
-            if(verbosity&gt;0)
-                cout &lt;&lt; &quot;mean costs at stage &quot; &lt;&lt; stage &lt;&lt; 
-                    &quot; = &quot; &lt;&lt; train_costs &lt;&lt; endl;
-            stage++;
-            train_stats-&gt;update(train_costs);
-        }    
-        if( pb )
-        {
-            delete pb;
-            pb = 0;
-        }
-
-    }
-
-
-    train_stats-&gt;finalize();
-}
-*/
-
 /* TO DO:
 verifier nombre de temps
-implementer correctmeent duration_to_number_of_timeframes(duration)
+implementer correctement duration_to_number_of_timeframes(duration)
 declare nouvelles options et valeurs par defaut correctes
 */
 
@@ -1235,8 +928,8 @@
       ou -1 (missing)
       ou -999 (fin de sequence)
 
-duree: 1 double-croche
-       2 
+duree: 0 double-croche
+       1 
 ..16   exprimee en 1/16 de mesure (resultat du quantize de Stan)
 
 
@@ -1253,11 +946,11 @@
     if(encoding==&quot;timeframe&quot;)
         encode_onehot_timeframe(sequence, encoded_seq, prepend_zero_rows);
     else if(encoding==&quot;note_duration&quot;)
-        encode_onehot_note_octav_duration(sequence, encoded_seq, prepend_zero_rows);
+        encode_onehot_note_octav_duration(sequence, encoded_seq, prepend_zero_rows, true, 0);
     else if(encoding==&quot;note_octav_duration&quot;)
-        encode_onehot_note_octav_duration(sequence, encoded_seq, prepend_zero_rows, true, 4);    
+        encode_onehot_note_octav_duration(sequence, encoded_seq, prepend_zero_rows, true, 5);    
     else if(encoding==&quot;raw_masked_supervised&quot;)
-        PLERROR(&quot;raw_masked_supervised encoding not yet implemented&quot;);
+        PLERROR(&quot;raw_masked_supervised means already encoded! You shouldnt have landed here!!!&quot;);
     else if(encoding==&quot;generic&quot;)
         PLERROR(&quot;generic encoding not yet implemented&quot;);
     else
@@ -1370,6 +1063,7 @@
 
 int DenoisingRecurrentNet::duration_to_number_of_timeframes(int duration)
 {
+    PLERROR(&quot;duration_to_number_of_timeframes (used only when encoding==timeframe) is not yet implemented&quot;);
     return duration+1;
 }
 
@@ -1495,7 +1189,7 @@
     hidden_layer-&gt;setLearningRate( the_learning_rate );
     input_connections-&gt;setLearningRate( the_learning_rate );
     if( dynamic_connections )
-        dynamic_connections-&gt;setLearningRate( the_learning_rate ); //HUGO: multiply by dynamic_connections_learning_weight;
+        dynamic_connections-&gt;setLearningRate( dynamic_gradient_scale_factor*the_learning_rate ); 
     if( hidden_layer2 )
     {
         hidden_layer2-&gt;setLearningRate( the_learning_rate );
@@ -1569,7 +1263,7 @@
         seq.resize(seqlen, w);
         testset-&gt;getMat(start,0,seq);
         encodeSequenceAndPopulateLists(seq);
-        fprop(costs, n_items);
+        recurrentFprop(costs, n_items);
 
         if (testoutputs)
         {
@@ -1611,295 +1305,7 @@
         test_stats-&gt;update(costs, 1.);
 }
 
-/*
-void DenoisingRecurrentNet::oldtest(VMat testset, PP&lt;VecStatsCollector&gt; test_stats,
-                  VMat testoutputs, VMat testcosts)const
-{ 
 
-    int len = testset.length();
-    Vec input;
-    Vec target;
-    real weight;
-
-    Vec output(outputsize());
-    output.clear();
-    Vec costs(nTestCosts());
-    costs.clear();
-    Vec n_items(nTestCosts());
-    n_items.clear();
-
-    PP&lt;ProgressBar&gt; pb;
-    if (report_progress) 
-        pb = new ProgressBar(&quot;Testing learner&quot;, len);
-
-    if (len == 0) {
-        // Empty test set: we give -1 cost arbitrarily.
-        costs.fill(-1);
-        test_stats-&gt;update(costs);
-    }
-    
-    int ith_sample_in_sequence = 0;
-    int inputsize_without_masks = inputsize() 
-        - ( use_target_layers_masks ? targetsize() : 0 );
-    int sum_target_elements = 0;
-    for (int i = 0; i &lt; len; i++)
-    {
-        testset.getExample(i, input, target, weight);
-
-        if( fast_exact_is_equal(input[0],end_of_sequence_symbol) )
-        {
-            ith_sample_in_sequence = 0;
-            hidden_list.resize(0);
-            hidden_act_no_bias_list.resize(0);
-            hidden2_list.resize(0);
-            hidden2_act_no_bias_list.resize(0);
-            target_prediction_list.resize(0);
-            target_prediction_act_no_bias_list.resize(0);
-            input_list.resize(0);
-            targets_list.resize(0);
-            nll_list.resize(0,0);
-            masks_list.resize(0);
-
-            if (testoutputs)
-            {
-                output.fill(end_of_sequence_symbol);
-                testoutputs-&gt;putOrAppendRow(i, output);
-            }
-
-            continue;
-        }
-
-        // Resize internal variables
-        hidden_list.resize(ith_sample_in_sequence+1);
-        hidden_act_no_bias_list.resize(ith_sample_in_sequence+1);
-        if( hidden_layer2 )
-        {
-            hidden2_list.resize(ith_sample_in_sequence+1);
-            hidden2_act_no_bias_list.resize(ith_sample_in_sequence+1);
-        }
-                 
-        input_list.resize(ith_sample_in_sequence+1);
-        input_list[ith_sample_in_sequence].resize(input_layer-&gt;size);
-
-        targets_list.resize( target_layers.length() );
-        target_prediction_list.resize( target_layers.length() );
-        target_prediction_act_no_bias_list.resize( target_layers.length() );
-        for( int tar=0; tar &lt; target_layers.length(); tar++ )
-        {
-            if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-            {
-                targets_list[tar].resize( ith_sample_in_sequence+1);
-                targets_list[tar][ith_sample_in_sequence].resize( 
-                    target_layers[tar]-&gt;size);
-                target_prediction_list[tar].resize(
-                    ith_sample_in_sequence+1);
-                target_prediction_act_no_bias_list[tar].resize(
-                    ith_sample_in_sequence+1);
-            }
-        }
-        nll_list.resize(ith_sample_in_sequence+1,target_layers.length());
-        if( use_target_layers_masks )
-        {
-            masks_list.resize( target_layers.length() );
-            for( int tar=0; tar &lt; target_layers.length(); tar++ )
-                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                    masks_list[tar].resize( ith_sample_in_sequence+1 );
-        }
-
-        // Forward propagation
-
-        // Fetch right representation for input
-        clamp_units(input.subVec(0,inputsize_without_masks),
-                    input_layer,
-                    input_symbol_sizes);                
-        input_list[ith_sample_in_sequence] &lt;&lt; input_layer-&gt;expectation;
-
-        // Fetch right representation for target
-        sum_target_elements = 0;
-        for( int tar=0; tar &lt; target_layers.length(); tar++ )
-        {
-            if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-            {
-                if( use_target_layers_masks )
-                {
-                    clamp_units(target.subVec(
-                                    sum_target_elements,
-                                    target_layers_n_of_target_elements[tar]),
-                                target_layers[tar],
-                                target_symbol_sizes[tar],
-                                input.subVec(
-                                    inputsize_without_masks 
-                                    + sum_target_elements, 
-                                    target_layers_n_of_target_elements[tar]),
-                                masks_list[tar][ith_sample_in_sequence]
-                        );
-                    
-                }
-                else
-                {
-                    clamp_units(target.subVec(
-                                    sum_target_elements,
-                                    target_layers_n_of_target_elements[tar]),
-                                target_layers[tar],
-                                target_symbol_sizes[tar]);
-                }
-                targets_list[tar][ith_sample_in_sequence] &lt;&lt; 
-                    target_layers[tar]-&gt;expectation;
-            }
-            sum_target_elements += target_layers_n_of_target_elements[tar];
-        }
-                
-        input_connections-&gt;fprop( input_list[ith_sample_in_sequence], 
-                                  hidden_act_no_bias_list[ith_sample_in_sequence]);
-                
-        if( ith_sample_in_sequence &gt; 0 &amp;&amp; dynamic_connections )
-        {
-            dynamic_connections-&gt;fprop( 
-                hidden_list[ith_sample_in_sequence-1],
-                dynamic_act_no_bias_contribution );
-
-            hidden_act_no_bias_list[ith_sample_in_sequence] += 
-                dynamic_act_no_bias_contribution;
-        }
-                 
-        hidden_layer-&gt;fprop( hidden_act_no_bias_list[ith_sample_in_sequence], 
-                             hidden_list[ith_sample_in_sequence] );
-                 
-        if( hidden_layer2 )
-        {
-            hidden_connections-&gt;fprop( 
-                hidden_list[ith_sample_in_sequence],
-                hidden2_act_no_bias_list[ith_sample_in_sequence]);
-
-            hidden_layer2-&gt;fprop( 
-                hidden2_act_no_bias_list[ith_sample_in_sequence],
-                hidden2_list[ith_sample_in_sequence] 
-                );
-
-            for( int tar=0; tar &lt; target_layers.length(); tar++ )
-            {
-                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                {
-                    target_connections[tar]-&gt;fprop(
-                        hidden2_list[ith_sample_in_sequence],
-                        target_prediction_act_no_bias_list[tar][
-                            ith_sample_in_sequence]
-                        );
-                    target_layers[tar]-&gt;fprop(
-                        target_prediction_act_no_bias_list[tar][
-                            ith_sample_in_sequence],
-                        target_prediction_list[tar][
-                            ith_sample_in_sequence] );
-                    if( use_target_layers_masks )
-                        target_prediction_list[tar][ ith_sample_in_sequence] *= 
-                            masks_list[tar][ith_sample_in_sequence];
-                }
-            }
-        }
-        else
-        {
-            for( int tar=0; tar &lt; target_layers.length(); tar++ )
-            {
-                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                {
-                    target_connections[tar]-&gt;fprop(
-                        hidden_list[ith_sample_in_sequence],
-                        target_prediction_act_no_bias_list[tar][
-                            ith_sample_in_sequence]
-                        );
-                    target_layers[tar]-&gt;fprop(
-                        target_prediction_act_no_bias_list[tar][
-                            ith_sample_in_sequence],
-                        target_prediction_list[tar][
-                            ith_sample_in_sequence] );
-                    if( use_target_layers_masks )
-                        target_prediction_list[tar][ ith_sample_in_sequence] *= 
-                            masks_list[tar][ith_sample_in_sequence];
-                }
-            }
-        }
-
-        if (testoutputs)
-        {
-            int sum_target_layers_size = 0;
-            for( int tar=0; tar &lt; target_layers.length(); tar++ )
-            {
-                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                {
-                    output.subVec(sum_target_layers_size,target_layers[tar]-&gt;size)
-                        &lt;&lt; target_prediction_list[tar][ ith_sample_in_sequence ];
-                }
-                sum_target_layers_size += target_layers[tar]-&gt;size;
-            }
-            testoutputs-&gt;putOrAppendRow(i, output);
-        }
-
-        sum_target_elements = 0;
-        for( int tar=0; tar &lt; target_layers.length(); tar++ )
-        {
-            if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-            {
-                target_layers[tar]-&gt;activation &lt;&lt; 
-                    target_prediction_act_no_bias_list[tar][
-                        ith_sample_in_sequence];
-                target_layers[tar]-&gt;activation += target_layers[tar]-&gt;bias;
-                target_layers[tar]-&gt;setExpectation(
-                    target_prediction_list[tar][
-                        ith_sample_in_sequence]);
-                nll_list(ith_sample_in_sequence,tar) = 
-                    target_layers[tar]-&gt;fpropNLL( 
-                        targets_list[tar][ith_sample_in_sequence] ); 
-                costs[tar] += nll_list(ith_sample_in_sequence,tar);
-                
-                // Normalize by the number of things to predict
-                if( use_target_layers_masks )
-                {
-                    n_items[tar] += sum(
-                        input.subVec( inputsize_without_masks 
-                                      + sum_target_elements, 
-                                      target_layers_n_of_target_elements[tar]) );
-                }
-                else
-                    n_items[tar]++;
-            }
-            if( use_target_layers_masks )
-                sum_target_elements += 
-                    target_layers_n_of_target_elements[tar];
-        }
-        ith_sample_in_sequence++;
-
-        if (report_progress)
-            pb-&gt;update(i);
-
-    }
-
-    for(int i=0; i&lt;costs.length(); i++)
-    {
-        if( !fast_exact_is_equal(target_layers_weights[i],0) )
-            costs[i] /= n_items[i];
-        else
-            costs[i] = MISSING_VALUE;
-    }
-    if (testcosts)
-        testcosts-&gt;putOrAppendRow(0, costs);
-    
-    if (test_stats)
-        test_stats-&gt;update(costs, weight);
-    
-    ith_sample_in_sequence = 0;
-    hidden_list.resize(0);
-    hidden_act_no_bias_list.resize(0);
-    hidden2_list.resize(0);
-    hidden2_act_no_bias_list.resize(0);
-    target_prediction_list.resize(0);
-    target_prediction_act_no_bias_list.resize(0);
-    input_list.resize(0);
-    targets_list.resize(0);
-    nll_list.resize(0,0);
-    masks_list.resize(0);   
-}
-*/
-
 TVec&lt;string&gt; DenoisingRecurrentNet::getTestCostNames() const
 {
     TVec&lt;string&gt; cost_names(0);

Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-07-02 18:53:14 UTC (rev 9197)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-07-02 23:06:01 UTC (rev 9198)
@@ -68,9 +68,6 @@
     ////! The learning rate used during RBM contrastive divergence learning phase
     //real rbm_learning_rate;
 
-    //! The learning rate used during the recurrent phase
-    real recurrent_net_learning_rate;
-
     ////! Number of epochs for rbm phase
     //int rbm_nstages;
 
@@ -79,6 +76,7 @@
     
     //! Indication that a mask indicating which target to predict
     //! is present in the input part of the VMatrix dataset.
+    //! no loinger an option: this is set to true only for (encoding==&quot;raw_masked_supervised&quot;)
     bool use_target_layers_masks;
 
     //! Value of the first input component for end-of-sequence delimiter
@@ -128,12 +126,15 @@
     int input_window_size;
 
     // Phase greedy (unsupervised)
+    bool tied_input_reconstruction_weights;
     double input_noise_prob;
     double input_reconstruction_lr;
     double hidden_noise_prob;
-    double hidden_reconstruciton_lr;
+    double hidden_reconstruction_lr;
+    bool tied_hidden_reconstruction_weights;
 
     // Phase noisy recurrent (supervised): uses input_noise_prob
+    // this phase *also* uses dynamic_gradient_scale_factor;
     double noisy_recurrent_lr;
     double dynamic_gradient_scale_factor;
     
@@ -157,7 +158,7 @@
     void encodeSequence(Mat sequence, Mat&amp; encoded_seq) const;
 
     static void encode_onehot_note_octav_duration(Mat sequence, Mat&amp; encoded_sequence, int prepend_zero_rows,
-                                                  bool use_silence=true, int octav_nbits=0, int duration_nbits=8);
+                                                  bool use_silence, int octav_nbits, int duration_nbits=20);
     
     static void encode_onehot_timeframe(Mat sequence, Mat&amp; encoded_sequence, 
                                         int prepend_zero_rows, bool use_silence=true);    
@@ -253,7 +254,7 @@
     //! Updates both the RBM parameters and the 
     //! dynamic connections in the recurrent tuning phase,
     //! after the visible units have been clamped
-    void recurrent_update();
+    void recurrentUpdate();
 
     virtual void test(VMat testset, PP&lt;VecStatsCollector&gt; test_stats,
                       VMat testoutputs=0, VMat testcosts=0) const;
@@ -344,7 +345,8 @@
     mutable TVec&lt;int&gt; testset_boundaries;
 
     mutable Mat seq; // contains the current train or test sequence
-    mutable Mat encoded_seq; // contains encoded version of current train or test sequence
+    mutable Mat encoded_seq; // contains encoded version of current train or test sequence (possibly corrupted by noise)
+    mutable Mat clean_encoded_seq; // copy of clean sequence contains encoded version of current train or test sequence
 
 protected:
     //#####  Protected Member Functions  ######################################
@@ -358,10 +360,13 @@
     //! This does the actual building.
     void build_();
 
+
+    void performGreedyDenoisingPhase();
+
     // note: the following functions are declared const because they have
     // to be called by test (which is const). Similarly, the members they 
     // manipulate are all declared mutable.
-    void fprop(Vec train_costs, Vec train_n_items) const;
+    void recurrentFprop(Vec train_costs, Vec train_n_items) const;
 
     //! does encoding if needed and populates the list.
     void encodeSequenceAndPopulateLists(Mat seq) const;


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="002645.html">[Plearn-commits] r9197 - trunk/plearn_learners/online
</A></li>
	<LI>Next message: <A HREF="002647.html">[Plearn-commits] r9199 - in trunk/plearn: base io vmat
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2646">[ date ]</a>
              <a href="thread.html#2646">[ thread ]</a>
              <a href="subject.html#2646">[ subject ]</a>
              <a href="author.html#2646">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
