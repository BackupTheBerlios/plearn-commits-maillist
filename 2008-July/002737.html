<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r9288 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2008-July/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9288%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200807252020.m6PKKwLt027645%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="002736.html">
   <LINK REL="Next"  HREF="002738.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r9288 - trunk/plearn_learners/online</H1>
    <B>larocheh at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9288%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200807252020.m6PKKwLt027645%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r9288 - trunk/plearn_learners/online">larocheh at mail.berlios.de
       </A><BR>
    <I>Fri Jul 25 22:20:58 CEST 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="002736.html">[Plearn-commits] r9287 - trunk/plearn_learners/meta
</A></li>
        <LI>Next message: <A HREF="002738.html">[Plearn-commits] r9289 - trunk/plearn_learners/meta
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2737">[ date ]</a>
              <a href="thread.html#2737">[ thread ]</a>
              <a href="subject.html#2737">[ subject ]</a>
              <a href="author.html#2737">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: larocheh
Date: 2008-07-25 22:20:57 +0200 (Fri, 25 Jul 2008)
New Revision: 9288

Added:
   trunk/plearn_learners/online/RBMRateLayer.cc
   trunk/plearn_learners/online/RBMRateLayer.h
Log:
Generalization of binary neurons to binomial neurons (i.e. neurons taking value in 0 and N, N being an hyper-parameter).



Added: trunk/plearn_learners/online/RBMRateLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMRateLayer.cc	2008-07-25 20:11:47 UTC (rev 9287)
+++ trunk/plearn_learners/online/RBMRateLayer.cc	2008-07-25 20:20:57 UTC (rev 9288)
@@ -0,0 +1,347 @@
+// -*- C++ -*-
+
+// RBMRateLayer.cc
+//
+// Copyright (C) 2008 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file RBMRateLayer.cc */
+
+
+
+#include &quot;RBMRateLayer.h&quot;
+#include &lt;plearn/math/TMat_maths.h&gt;
+#include &quot;RBMConnection.h&quot;
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    RBMRateLayer,
+    &quot;Layer in an RBM consisting in rate-coded units&quot;,
+    &quot;&quot;);
+
+RBMRateLayer::RBMRateLayer( real the_learning_rate ) :
+    inherited( the_learning_rate ),
+    n_spikes( 10 )
+{
+}
+
+void RBMRateLayer::generateSample()
+{
+    PLASSERT_MSG(random_gen,
+                 &quot;random_gen should be initialized before generating samples&quot;);
+
+    PLCHECK_MSG(expectation_is_up_to_date, &quot;Expectation should be computed &quot;
+            &quot;before calling generateSample()&quot;);
+
+    real exp_i = 0;
+    for( int i=0; i&lt;size; i++)
+    {
+        exp_i = expectation[i];
+        sample[i] = round(random_gen-&gt;gaussian_mu_sigma(
+                              exp_i,exp_i*(1-exp_i/n_spikes)) );
+    }
+}
+
+void RBMRateLayer::generateSamples()
+{
+    PLASSERT_MSG(random_gen,
+                 &quot;random_gen should be initialized before generating samples&quot;);
+
+    PLCHECK_MSG(expectations_are_up_to_date, &quot;Expectations should be computed &quot;
+                        &quot;before calling generateSamples()&quot;);
+
+    PLASSERT( samples.width() == size &amp;&amp; samples.length() == batch_size );
+
+    real exp_i = 0;
+    for (int k = 0; k &lt; batch_size; k++)
+    {
+        for( int i=0; i&lt;size; i++)
+        {
+            exp_i = expectations(k,i);
+            samples(k,i) = round(random_gen-&gt;gaussian_mu_sigma(
+                                     exp_i,exp_i*(1-exp_i/n_spikes)) );
+        }
+    }
+}
+
+void RBMRateLayer::computeExpectation()
+{
+    if( expectation_is_up_to_date )
+        return;
+
+    if (use_fast_approximations)
+        for(int i=0; i&lt;size; i++)
+            expectation[i] = n_spikes*fastsigmoid(activation[i]);
+    else
+        for(int i=0; i&lt;size; i++)
+            expectation[i] = n_spikes*sigmoid(activation[i]);
+    expectation_is_up_to_date = true;
+}
+
+void RBMRateLayer::computeExpectations()
+{
+    if( expectations_are_up_to_date )
+        return;
+
+    PLASSERT( expectations.width() == size
+              &amp;&amp; expectations.length() == batch_size );
+
+    if (use_fast_approximations)
+        for (int k = 0; k &lt; batch_size; k++)
+            for(int i=0; i&lt;size; i++)
+                expectations(k,i) =  n_spikes*fastsigmoid(activations(k,i));
+    else
+        for (int k = 0; k &lt; batch_size; k++)
+            for(int i=0; i&lt;size; i++)
+                expectations(k,i) = n_spikes*sigmoid(activations(k,i));
+    expectations_are_up_to_date = true;
+}
+
+
+void RBMRateLayer::fprop( const Vec&amp; input, Vec&amp; output ) const
+{
+    PLASSERT( input.size() == input_size );
+    output.resize( output_size );
+    if (use_fast_approximations)
+        for(int i=0; i&lt;size; i++)
+            output[i] = n_spikes*fastsigmoid(input[i]+bias[i]);
+    else
+        for(int i=0; i&lt;size; i++)
+            output[i] = n_spikes*sigmoid(input[i]+bias[i]);
+}
+
+/////////////////
+// bpropUpdate //
+/////////////////
+void RBMRateLayer::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                                      Vec&amp; input_gradient,
+                                      const Vec&amp; output_gradient,
+                                      bool accumulate)
+{
+    PLASSERT( input.size() == size );
+    PLASSERT( output.size() == size );
+    PLASSERT( output_gradient.size() == size );
+
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+    }
+    else
+    {
+        input_gradient.resize( size );
+        input_gradient.clear();
+    }
+
+    if( momentum != 0. )
+        bias_inc.resize( size );
+    
+    for( int i=0 ; i&lt;size ; i++ )
+    {
+        real output_i = output[i];
+        real in_grad_i;
+        in_grad_i = output_i * (1-output_i) * output_gradient[i] * n_spikes;
+        input_gradient[i] += in_grad_i;
+        
+        if( momentum == 0. )
+        {
+            // update the bias: bias -= learning_rate * input_gradient
+            bias[i] -= learning_rate * in_grad_i;
+        }
+        else
+        {
+            // The update rule becomes:
+            // bias_inc = momentum * bias_inc - learning_rate * input_gradient
+            // bias += bias_inc
+            bias_inc[i] = momentum * bias_inc[i] - learning_rate * in_grad_i;
+            bias[i] += bias_inc[i];
+        }
+    }
+    applyBiasDecay();
+}
+
+void RBMRateLayer::bpropUpdate(const Mat&amp; inputs, const Mat&amp; outputs,
+                             Mat&amp; input_gradients,
+                             const Mat&amp; output_gradients,
+                             bool accumulate)
+{
+    PLERROR(&quot;In RBMRateLayer::bpropUpdate(): mini-batch version of bpropUpdate is not &quot;
+            &quot;implemented yet&quot;);
+}
+
+//////////////
+// fpropNLL //
+//////////////
+real RBMRateLayer::fpropNLL(const Vec&amp; target)
+{
+    PLERROR(&quot;In RBMRateLayer::fpropNLL(): not implemented&quot;);
+    PLASSERT( target.size() == input_size );
+    real ret = 0;
+    real target_i, activation_i;
+    if(use_fast_approximations){
+        for( int i=0 ; i&lt;size ; i++ )
+        {
+            target_i = target[i];
+            activation_i = activation[i];
+            ret += tabulated_softplus(activation_i) - target_i * activation_i;
+            // nll = - target*log(sigmoid(act)) -(1-target)*log(1-sigmoid(act))
+            // but it is numerically unstable, so use instead the following identity:
+            //     = target*softplus(-act) +(1-target)*(act+softplus(-act))
+            //     = act + softplus(-act) - target*act
+            //     = softplus(act) - target*act
+        }
+    } else {
+        for( int i=0 ; i&lt;size ; i++ )
+        {
+            target_i = target[i];
+            activation_i = activation[i];
+            ret += softplus(activation_i) - target_i * activation_i;
+        }
+    }
+
+    return ret;
+}
+
+void RBMRateLayer::bpropNLL(const Vec&amp; target, real nll,
+                                   Vec&amp; bias_gradient)
+{
+    PLERROR(&quot;In RBMRateLayer::bpropNLL(): not implemented&quot;);
+    computeExpectation();
+
+    PLASSERT( target.size() == input_size );
+    bias_gradient.resize( size );
+
+    // bias_gradient = expectation - target
+    substract(expectation, target, bias_gradient);
+}
+
+void RBMRateLayer::declareOptions(OptionList&amp; ol)
+{
+
+    declareOption(ol, &quot;n_spikes&quot;, &amp;RBMRateLayer::n_spikes,
+                  OptionBase::buildoption,
+                  &quot;Maximum number of spikes for each neuron.\n&quot;);
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void RBMRateLayer::build_()
+{
+    if( n_spikes &lt; 1 )
+        PLERROR(&quot;In RBMRateLayer::build_(): n_spikes should be positive&quot;);
+}
+
+void RBMRateLayer::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void RBMRateLayer::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+    //deepCopyField(tmp_softmax, copies);
+}
+
+real RBMRateLayer::energy(const Vec&amp; unit_values) const
+{
+    return -dot(unit_values, bias);
+}
+
+real RBMRateLayer::freeEnergyContribution(const Vec&amp; unit_activations)
+    const
+{
+    PLASSERT( unit_activations.size() == size );
+
+    // result = -\sum_{i=0}^{size-1} softplus(a_i)
+    real result = 0;
+    real* a = unit_activations.data();
+    for (int i=0; i&lt;size; i++)
+    {
+        if (use_fast_approximations)
+            result -= n_spikes*tabulated_softplus(a[i]);
+        else
+            result -= n_spikes*softplus(a[i]);
+    }
+    return result;
+}
+
+void RBMRateLayer::freeEnergyContributionGradient(
+    const Vec&amp; unit_activations,
+    Vec&amp; unit_activations_gradient,
+    real output_gradient, bool accumulate) const
+{
+    PLASSERT( unit_activations.size() == size );
+    unit_activations_gradient.resize( size );
+    if( !accumulate ) unit_activations_gradient.clear();
+    real* a = unit_activations.data();
+    real* ga = unit_activations_gradient.data();
+    for (int i=0; i&lt;size; i++)
+    {
+        if (use_fast_approximations)
+            ga[i] -= output_gradient * n_spikes *
+                fastsigmoid( a[i] );
+        else
+            ga[i] -= output_gradient * n_spikes *
+                sigmoid( a[i] );
+    }
+}
+
+int RBMRateLayer::getConfigurationCount()
+{
+    return INFINITE_CONFIGURATIONS;
+}
+
+void RBMRateLayer::getConfiguration(int conf_index, Vec&amp; output)
+{
+    PLERROR(&quot;In RBMRateLayer::getConfiguration(): not implemented&quot;);
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/RBMRateLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMRateLayer.h	2008-07-25 20:11:47 UTC (rev 9287)
+++ trunk/plearn_learners/online/RBMRateLayer.h	2008-07-25 20:20:57 UTC (rev 9288)
@@ -0,0 +1,175 @@
+// -*- C++ -*-
+
+// RBMRateLayer.h
+//
+// Copyright (C) 2008 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file RBMRateLayer.h */
+
+
+#ifndef RBMRateLayer_INC
+#define RBMRateLayer_INC
+
+#include &quot;RBMLayer.h&quot;
+
+namespace PLearn {
+using namespace std;
+
+/**
+ * Layer in an RBM consisting in rate-coded units
+ */
+class RBMRateLayer: public RBMLayer
+{
+    typedef RBMLayer inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! Maximum number of spikes for each neuron
+    int n_spikes;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    RBMRateLayer( real the_learning_rate=0. );
+
+    //! generate a sample, and update the sample field
+    virtual void generateSample();
+
+    //! batch version
+    virtual void generateSamples();
+
+    //! compute the expectation
+    virtual void computeExpectation();
+
+    //! batch version
+    virtual void computeExpectations();
+
+    //! forward propagation
+    virtual void fprop( const Vec&amp; input, Vec&amp; output ) const;
+
+    //! back-propagates the output gradient to the input
+    virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                             Vec&amp; input_gradient, const Vec&amp; output_gradient,
+                             bool accumulate=false);
+
+    //! back-propagates the output gradient to the input, in mini-batch mode
+    virtual void bpropUpdate(const Mat&amp; inputs, const Mat&amp; outputs,
+                             Mat&amp; input_gradients,
+                             const Mat&amp; output_gradients,
+                             bool accumulate=false);
+
+    //! Computes the negative log-likelihood of target given the
+    //! internal activations of the layer
+    virtual real fpropNLL(const Vec&amp; target);
+
+    //! Computes the gradient of the negative log-likelihood of target
+    //! with respect to the layer's bias, given the internal activations
+    virtual void bpropNLL(const Vec&amp; target, real nll, Vec&amp; bias_gradient);
+
+    // Compute -bias' unit_values
+    virtual real energy(const Vec&amp; unit_values) const;
+
+    //! Computes -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! This quantity is used for computing the free energy of a sample x in
+    //! the OTHER layer of an RBM, from which unit_activations was computed.
+    virtual real freeEnergyContribution(const Vec&amp; unit_activations) const;
+
+    //! Computes gradient of the result of freeEnergyContribution
+    //! -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! with respect to unit_activations. Optionally, a gradient
+    //! with respect to freeEnergyContribution can be given
+    virtual void freeEnergyContributionGradient(const Vec&amp; unit_activations,
+                                                Vec&amp; unit_activations_gradient,
+                                                real output_gradient = 1,
+                                                bool accumulate = false) 
+        const;
+
+
+    virtual int getConfigurationCount();
+
+    virtual void getConfiguration(int conf_index, Vec&amp; output);
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(RBMRateLayer);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
+
+protected:
+    //#####  Not Options  #####################################################
+    mutable Vec tmp_softmax;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList&amp; ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(RBMRateLayer);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="002736.html">[Plearn-commits] r9287 - trunk/plearn_learners/meta
</A></li>
	<LI>Next message: <A HREF="002738.html">[Plearn-commits] r9289 - trunk/plearn_learners/meta
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2737">[ date ]</a>
              <a href="thread.html#2737">[ thread ]</a>
              <a href="subject.html#2737">[ subject ]</a>
              <a href="author.html#2737">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
