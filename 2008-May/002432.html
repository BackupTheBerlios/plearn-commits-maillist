<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r8984 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2008-May/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8984%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200805121447.m4CElnKE016578%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="002431.html">
   <LINK REL="Next"  HREF="002433.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r8984 - trunk/plearn_learners/online</H1>
    <B>larocheh at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8984%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200805121447.m4CElnKE016578%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r8984 - trunk/plearn_learners/online">larocheh at mail.berlios.de
       </A><BR>
    <I>Mon May 12 16:47:49 CEST 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="002431.html">[Plearn-commits] r8983 - trunk/plearn/vmat
</A></li>
        <LI>Next message: <A HREF="002433.html">[Plearn-commits] r8985 - trunk/plearn/vmat
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2432">[ date ]</a>
              <a href="thread.html#2432">[ thread ]</a>
              <a href="subject.html#2432">[ subject ]</a>
              <a href="author.html#2432">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: larocheh
Date: 2008-05-12 16:47:48 +0200 (Mon, 12 May 2008)
New Revision: 8984

Added:
   trunk/plearn_learners/online/RBMDiagonalMatrixConnection.cc
   trunk/plearn_learners/online/RBMDiagonalMatrixConnection.h
Log:
Diagonal matrix connection...


Added: trunk/plearn_learners/online/RBMDiagonalMatrixConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMDiagonalMatrixConnection.cc	2008-05-12 14:08:10 UTC (rev 8983)
+++ trunk/plearn_learners/online/RBMDiagonalMatrixConnection.cc	2008-05-12 14:47:48 UTC (rev 8984)
@@ -0,0 +1,658 @@
+// -*- C++ -*-
+
+// RBMDiagonalMatrixConnection.cc
+//
+// Copyright (C) 2006 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file RBMDiagonalMatrixConnection.cc */
+
+
+
+#include &quot;RBMDiagonalMatrixConnection.h&quot;
+#include &lt;plearn/math/TMat_maths.h&gt;
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    RBMDiagonalMatrixConnection,
+    &quot;Stores and learns the parameters between two linear layers of an RBM&quot;,
+    &quot;&quot;);
+
+RBMDiagonalMatrixConnection::RBMDiagonalMatrixConnection( real the_learning_rate ) :
+    inherited(the_learning_rate)
+{
+}
+
+void RBMDiagonalMatrixConnection::declareOptions(OptionList&amp; ol)
+{
+    declareOption(ol, &quot;weights_diag&quot;, &amp;RBMDiagonalMatrixConnection::weights_diag,
+                  OptionBase::learntoption,
+                  &quot;Vector containing the diagonal of the weight matrix.\n&quot;);
+
+    declareOption(ol, &quot;L1_penalty_factor&quot;, 
+                  &amp;RBMDiagonalMatrixConnection::L1_penalty_factor,
+                  OptionBase::buildoption,
+                  &quot;Optional (default=0) factor of L1 regularization term, i.e.\n&quot;
+                  &quot;minimize L1_penalty_factor * sum_{ij} |weights(i,j)| &quot;
+                  &quot;during training.\n&quot;);
+
+    declareOption(ol, &quot;L2_penalty_factor&quot;, 
+                  &amp;RBMDiagonalMatrixConnection::L2_penalty_factor,
+                  OptionBase::buildoption,
+                  &quot;Optional (default=0) factor of L2 regularization term, i.e.\n&quot;
+                  &quot;minimize 0.5 * L2_penalty_factor * sum_{ij} weights(i,j)^2 &quot;
+                  &quot;during training.\n&quot;);
+
+    declareOption(ol, &quot;L2_decrease_constant&quot;, 
+                  &amp;RBMDiagonalMatrixConnection::L2_decrease_constant,
+                  OptionBase::buildoption,
+        &quot;Parameter of the L2 penalty decrease (see L2_decrease_type).&quot;,
+        OptionBase::advanced_level);
+
+    declareOption(ol, &quot;L2_shift&quot;, 
+                  &amp;RBMDiagonalMatrixConnection::L2_shift,
+                  OptionBase::buildoption,
+        &quot;Parameter of the L2 penalty decrease (see L2_decrease_type).&quot;,
+        OptionBase::advanced_level);
+
+    declareOption(ol, &quot;L2_decrease_type&quot;, 
+                  &amp;RBMDiagonalMatrixConnection::L2_decrease_type,
+                  OptionBase::buildoption,
+        &quot;The kind of L2 decrease that is being applied. The decrease\n&quot;
+        &quot;consists in scaling the L2 penalty by a factor that depends on the\n&quot;
+        &quot;number 't' of times this penalty has been used to modify the\n&quot;
+        &quot;weights of the connection. It can be one of:\n&quot;
+        &quot; - 'one_over_t': 1 / (1 + t * L2_decrease_constant)\n&quot;
+        &quot; - 'sigmoid_like': sigmoid((L2_shift - t) * L2_decrease_constant)&quot;,
+        OptionBase::advanced_level);
+
+    declareOption(ol, &quot;L2_n_updates&quot;, 
+                  &amp;RBMDiagonalMatrixConnection::L2_n_updates,
+                  OptionBase::learntoption,
+        &quot;Number of times that weights have been changed by the L2 penalty\n&quot;
+        &quot;update rule.&quot;);
+
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void RBMDiagonalMatrixConnection::build_()
+{
+    if( up_size &lt;= 0 || down_size &lt;= 0 )
+        return;
+
+    if( up_size != down_size )
+        PLERROR(&quot;In RBMDiagonalMatrixConnection::build_(): up_size should be &quot;
+            &quot;equal to down_size&quot;);
+
+    bool needs_forget = false; // do we need to reinitialize the parameters?
+
+    if( weights_diag.length() != up_size )
+    {
+        weights_diag.resize( up_size );
+        needs_forget = true;
+    }
+
+    weights_pos_stats.resize( up_size );
+    weights_neg_stats.resize( up_size );
+
+    if( momentum != 0. )
+        weights_inc.resize( up_size );
+
+    if( needs_forget )
+        forget();
+
+    clearStats();
+}
+
+void RBMDiagonalMatrixConnection::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void RBMDiagonalMatrixConnection::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(weights_diag, copies);
+    deepCopyField(weights_pos_stats, copies);
+    deepCopyField(weights_neg_stats, copies);
+    deepCopyField(weights_inc, copies);
+}
+
+void RBMDiagonalMatrixConnection::accumulatePosStats( const Vec&amp; down_values,
+                                              const Vec&amp; up_values )
+{
+    real* wps = weights_pos_stats.data();
+    real* uv = up_values.data();
+    real* dv = down_values.data();
+    for( int i=0; i&lt;up_size; i++ )
+        wps[i] += uv[i]*dv[i];
+
+    pos_count++;
+}
+
+void RBMDiagonalMatrixConnection::accumulatePosStats( const Mat&amp; down_values,
+                                              const Mat&amp; up_values )
+{
+    int mbs=down_values.length();
+    PLASSERT(up_values.length()==mbs);
+
+    real* wps;
+    real* uv;
+    real* dv;
+    for( int t=0; t&lt;mbs; t++ )
+    {
+        wps = weights_pos_stats.data();
+        uv = up_values[t];
+        dv = down_values[t];
+        for( int i=0; i&lt;up_size; i++ )
+            wps[i] += uv[i]*dv[i];
+    }
+    pos_count+=mbs;
+}
+
+////////////////////////
+// accumulateNegStats //
+////////////////////////
+void RBMDiagonalMatrixConnection::accumulateNegStats( const Vec&amp; down_values,
+                                              const Vec&amp; up_values )
+{
+    real* wns = weights_neg_stats.data();
+    real* uv = up_values.data();
+    real* dv = down_values.data();
+    for( int i=0; i&lt;up_size; i++ )
+        wns[i] += uv[i]*dv[i];
+    neg_count++;
+}
+
+void RBMDiagonalMatrixConnection::accumulateNegStats( const Mat&amp; down_values,
+                                              const Mat&amp; up_values )
+{
+    int mbs=down_values.length();
+    PLASSERT(up_values.length()==mbs);
+
+    real* wns;
+    real* uv;
+    real* dv;
+    for( int t=0; t&lt;mbs; t++ )
+    {
+        wns = weights_neg_stats.data();
+        uv = up_values[t];
+        dv = down_values[t];
+        for( int i=0; i&lt;up_size; i++ )
+            wns[i] += uv[i]*dv[i];
+    }
+    neg_count+=mbs;
+}
+
+////////////
+// update //
+////////////
+void RBMDiagonalMatrixConnection::update()
+{
+    // updates parameters
+    //weights += learning_rate * (weights_pos_stats/pos_count
+    //                              - weights_neg_stats/neg_count)
+    real pos_factor = learning_rate / pos_count;
+    real neg_factor = -learning_rate / neg_count;
+
+    int l = weights_diag.length();
+
+    real* w_i = weights_diag.data();
+    real* wps_i = weights_pos_stats.data();
+    real* wns_i = weights_neg_stats.data();
+
+    if( momentum == 0. )
+    {
+        // no need to use weights_inc
+        for( int i=0 ; i&lt;l ; i++ )
+            w_i[i] += pos_factor * wps_i[i] + neg_factor * wns_i[i];
+    }
+    else
+    {
+        // ensure that weights_inc has the right size
+        weights_inc.resize( l );
+
+        // The update rule becomes:
+        // weights_inc = momentum * weights_inc
+        //               - learning_rate * (weights_pos_stats/pos_count
+        //                                  - weights_neg_stats/neg_count);
+        // weights += weights_inc;
+        real* winc_i = weights_inc.data();
+        for( int i=0 ; i&lt;l ; i++ )
+        {
+            winc_i[i] = momentum * winc_i[i]
+                + pos_factor * wps_i[i] + neg_factor * wns_i[i];
+            w_i[i] += winc_i[i];
+        }
+    }
+
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+        applyWeightPenalty();
+
+    clearStats();
+}
+
+// Instead of using the statistics, we assume we have only one markov chain
+// runned and we update the parameters from the first 4 values of the chain
+void RBMDiagonalMatrixConnection::update( const Vec&amp; pos_down_values, // v_0
+                                  const Vec&amp; pos_up_values,   // h_0
+                                  const Vec&amp; neg_down_values, // v_1
+                                  const Vec&amp; neg_up_values )  // h_1
+{
+    int l = weights_diag.length();
+    PLASSERT( pos_up_values.length() == l );
+    PLASSERT( neg_up_values.length() == l );
+    PLASSERT( pos_down_values.length() == l );
+    PLASSERT( neg_down_values.length() == l );
+
+    real* w_i = weights_diag.data();
+    real* pdv = pos_down_values.data();
+    real* puv = pos_up_values.data();
+    real* ndv = neg_down_values.data();
+    real* nuv = neg_up_values.data();
+
+    if( momentum == 0. )
+    {
+        for( int i=0 ; i&lt;l ; i++)
+            w_i[i] += learning_rate * (puv[i] * pdv[i] - nuv[i] * ndv[i]);
+    }
+    else
+    {
+        // ensure that weights_inc has the right size
+        weights_inc.resize( l );
+
+        real* winc_i = weights_inc.data();
+        for( int i=0 ; i&lt;l ; i++ )
+        {
+            winc_i[i] = momentum * winc_i[i]
+                + learning_rate * (puv[i] * pdv[i] - nuv[i] * ndv[i]);
+            w_i[i] += winc_i[i];
+        }
+    }
+
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+        applyWeightPenalty();
+}
+
+void RBMDiagonalMatrixConnection::update( const Mat&amp; pos_down_values, // v_0
+                                  const Mat&amp; pos_up_values,   // h_0
+                                  const Mat&amp; neg_down_values, // v_1
+                                  const Mat&amp; neg_up_values )  // h_1
+{
+    // weights += learning_rate * ( h_0 v_0' - h_1 v_1' );
+    // or:
+    // weights[i][j] += learning_rate * (h_0[i] v_0[j] - h_1[i] v_1[j]);
+
+    int l = weights_diag.length();
+
+    PLASSERT( pos_up_values.width() == l );
+    PLASSERT( neg_up_values.width() == l );
+    PLASSERT( pos_down_values.width() == l );
+    PLASSERT( neg_down_values.width() == l );
+
+    real* w_i = weights_diag.data();
+    real* pdv;
+    real* puv;
+    real* ndv;
+    real* nuv;
+
+    if( momentum == 0. )
+    {
+        // We use the average gradient over a mini-batch.
+        real avg_lr = learning_rate / pos_down_values.length();
+
+        for( int t=0; t&lt;pos_up_values.length(); t++ )
+        {
+            pdv = pos_down_values[t];
+            puv = pos_up_values[t];
+            ndv = neg_down_values[t];
+            nuv = neg_up_values[t];
+            for( int i=0 ; i&lt;l ; i++)
+                w_i[i] += avg_lr * (puv[i] * pdv[i] - nuv[i] * ndv[i]);
+        }
+    }
+    else
+    {
+        PLERROR(&quot;RBMDiagonalMatrixConnection::update minibatch with momentum - Not implemented&quot;);
+    }
+
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+        applyWeightPenalty();
+}
+
+////////////////
+// clearStats //
+////////////////
+void RBMDiagonalMatrixConnection::clearStats()
+{
+    weights_pos_stats.clear();
+    weights_neg_stats.clear();
+
+    pos_count = 0;
+    neg_count = 0;
+}
+
+////////////////////
+// computeProduct //
+////////////////////
+void RBMDiagonalMatrixConnection::computeProduct( int start, int length,
+                                          const Vec&amp; activations,
+                                          bool accumulate ) const
+{
+    PLASSERT( activations.length() == length );
+    PLASSERT( start+length &lt;= up_size );
+    real* act = activations.data();
+    real* w = weights_diag.data();
+    real* iv = input_vec.data();
+    if( accumulate )
+        for( int i=0; i&lt;length; i++ )
+            act[i] += w[i+start] * iv[i+start];
+    else
+        for( int i=0; i&lt;length; i++ )
+            act[i] = w[i+start] * iv[i+start];
+}
+
+/////////////////////
+// computeProducts //
+/////////////////////
+void RBMDiagonalMatrixConnection::computeProducts(int start, int length,
+                                          Mat&amp; activations,
+                                          bool accumulate ) const
+{
+    PLASSERT( activations.width() == length );
+    activations.resize(inputs_mat.length(), length);
+    real* act;
+    real* w = weights_diag.data();
+    real* iv;
+    if( accumulate )
+        for( int t=0; t&lt;inputs_mat.length(); t++ )
+        {
+            act = activations[t];
+            iv = inputs_mat[t];
+            for( int i=0; i&lt;length; i++ )
+                act[i] += w[i+start] * iv[i+start];
+        }
+    else
+        for( int t=0; t&lt;inputs_mat.length(); t++ )
+        {
+            act = activations[t];
+            iv = inputs_mat[t];
+            for( int i=0; i&lt;length; i++ )
+                act[i] = w[i+start] * iv[i+start];
+        }
+}
+
+/////////////////
+// bpropUpdate //
+/////////////////
+void RBMDiagonalMatrixConnection::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                                      Vec&amp; input_gradient,
+                                      const Vec&amp; output_gradient,
+                                      bool accumulate)
+{
+    PLASSERT( input.size() == down_size );
+    PLASSERT( output.size() == up_size );
+    PLASSERT( output_gradient.size() == up_size );
+
+    real* w = weights_diag.data();
+    real* in = input.data();
+    real* ing = input_gradient.data();
+    real* outg = output_gradient.data();
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == down_size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+
+        for( int i=0; i&lt;down_size; i++ )
+        {
+            ing[i] += outg[i]*w[i];
+            w[i] -= learning_rate * in[i] * outg[i];
+        }
+    }
+    else
+    {
+        input_gradient.resize( down_size );
+        for( int i=0; i&lt;down_size; i++ )
+        {
+            ing[i] = outg[i]*w[i];
+            w[i] -= learning_rate * in[i] * outg[i];
+        }
+    }
+
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+        applyWeightPenalty();
+}
+
+void RBMDiagonalMatrixConnection::bpropUpdate(const Mat&amp; inputs, const Mat&amp; outputs,
+                                      Mat&amp; input_gradients,
+                                      const Mat&amp; output_gradients,
+                                      bool accumulate)
+{
+    PLASSERT( inputs.width() == down_size );
+    PLASSERT( outputs.width() == up_size );
+    PLASSERT( output_gradients.width() == up_size );
+
+    int mbatch = inputs.length();
+
+    real* w = weights_diag.data();
+    real* in;
+    real* ing;
+    real* outg;
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradients.width() == down_size &amp;&amp;
+                      input_gradients.length() == inputs.length(),
+                      &quot;Cannot resize input_gradients and accumulate into it&quot; );
+
+        for( int t=0; t&lt;mbatch; t++ )
+        {
+            ing = input_gradients[t];
+            outg = output_gradients[t];
+            for( int i=0; i&lt;down_size; i++ )
+                ing[i] += outg[i]*w[i];
+        }
+    }
+    else
+    {
+        input_gradients.resize(inputs.length(), down_size);
+        for( int t=0; t&lt;mbatch; t++ )
+        {
+            ing = input_gradients[t];
+            outg = output_gradients[t];
+            for( int i=0; i&lt;down_size; i++ )
+                ing[i] = outg[i]*w[i];
+        }
+    }
+
+    real avg_lr = learning_rate / mbatch;
+    for( int t=0; t&lt;mbatch; t++ )
+    {
+        in = inputs[t];
+        outg = output_gradients[t];
+        for( int i=0; i&lt;down_size; i++ )
+            w[i] -= avg_lr * in[i] * outg[i];
+    }
+
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+        applyWeightPenalty();
+}
+
+////////////////////////
+// applyWeightPenalty //
+////////////////////////
+void RBMDiagonalMatrixConnection::applyWeightPenalty()
+{
+    // Apply penalty (decay) on weights.
+    real delta_L1 = learning_rate * L1_penalty_factor;
+    real delta_L2 = learning_rate * L2_penalty_factor;
+    if (L2_decrease_type == &quot;one_over_t&quot;)
+        delta_L2 /= (1 + L2_decrease_constant * L2_n_updates);
+    else if (L2_decrease_type == &quot;sigmoid_like&quot;)
+        delta_L2 *= sigmoid((L2_shift - L2_n_updates) * L2_decrease_constant);
+    else
+        PLERROR(&quot;In RBMDiagonalMatrixConnection::applyWeightPenalty - Invalid value &quot;
+                &quot;for L2_decrease_type: %s&quot;, L2_decrease_type.c_str());
+    real* w_ = weights_diag.data();
+    for( int i=0; i&lt;down_size; i++ )
+    {
+        if( delta_L2 != 0. )
+            w_[i] *= (1 - delta_L2);
+        
+        if( delta_L1 != 0. )
+        {
+            if( w_[i] &gt; delta_L1 )
+                w_[i] -= delta_L1;
+            else if( w_[i] &lt; -delta_L1 )
+                w_[i] += delta_L1;
+            else
+                w_[i] = 0.;
+        }
+    }
+    
+    if (delta_L2 &gt; 0)
+        L2_n_updates++;
+}
+
+//////////////////////
+// addWeightPenalty //
+//////////////////////
+void RBMDiagonalMatrixConnection::addWeightPenalty(Vec weights_diag, Vec weight_diag_gradients)
+{
+    // Add penalty (decay) gradient.
+    real delta_L1 = L1_penalty_factor;
+    real delta_L2 = L2_penalty_factor;
+    PLASSERT_MSG( is_equal(L2_decrease_constant, 0) &amp;&amp; is_equal(L2_shift, 100),
+                  &quot;L2 decrease not implemented in this method&quot; );
+    real* w_ = weights_diag.data();
+    real* gw_ = weight_diag_gradients.data();
+    for( int i=0; i&lt;down_size; i++ )
+    {
+        if( delta_L2 != 0. )
+            gw_[i] += delta_L2*w_[i];
+        
+        if( delta_L1 != 0. )
+        {
+            if( w_[i] &gt; 0 )
+                gw_[i] += delta_L1;
+            else if( w_[i] &lt; 0 )
+                gw_[i] -= delta_L1;
+        }
+    }
+}
+
+////////////
+// forget //
+////////////
+// Reset the parameters to the state they would be BEFORE starting training.
+// Note that this method is necessarily called from build().
+void RBMDiagonalMatrixConnection::forget()
+{
+    clearStats();
+    if( initialization_method == &quot;zero&quot; )
+        weights_diag.clear();
+    else
+    {
+        if( !random_gen )
+        {
+            PLWARNING( &quot;RBMDiagonalMatrixConnection: cannot forget() without&quot;
+                       &quot; random_gen&quot; );
+            return;
+        }
+
+        //random_gen-&gt;manual_seed(1827);
+
+        real d = 1. / max( down_size, up_size );
+        if( initialization_method == &quot;uniform_sqrt&quot; )
+            d = sqrt( d );
+
+        random_gen-&gt;fill_random_uniform( weights_diag, -d, d );
+    }
+    L2_n_updates = 0;
+}
+
+
+/* THIS METHOD IS OPTIONAL
+//! reset the parameters to the state they would be BEFORE starting training.
+//! Note that this method is necessarily called from build().
+//! THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS DOES NOT DO
+//! ANYTHING.
+void RBMDiagonalMatrixConnection::finalize()
+{
+}
+*/
+
+//! return the number of parameters
+int RBMDiagonalMatrixConnection::nParameters() const
+{
+    return weights_diag.size();
+}
+
+//! Make the parameters data be sub-vectors of the given global_parameters.
+//! The argument should have size &gt;= nParameters. The result is a Vec
+//! that starts just after this object's parameters end, i.e.
+//!    result = global_parameters.subVec(nParameters(),global_parameters.size()-nParameters());
+//! This allows to easily chain calls of this method on multiple RBMParameters.
+Vec RBMDiagonalMatrixConnection::makeParametersPointHere(const Vec&amp; global_parameters)
+{
+    int n=weights_diag.size();
+    int m = global_parameters.size();
+    if (m&lt;n)
+        PLERROR(&quot;RBMDiagonalMatrixConnection::makeParametersPointHere: argument has length %d, should be longer than nParameters()=%d&quot;,m,n);
+    real* p = global_parameters.data();
+    weights_diag.makeSharedValue(p,n);
+
+    return global_parameters.subVec(n,m-n);
+}
+
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/RBMDiagonalMatrixConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMDiagonalMatrixConnection.h	2008-05-12 14:08:10 UTC (rev 8983)
+++ trunk/plearn_learners/online/RBMDiagonalMatrixConnection.h	2008-05-12 14:47:48 UTC (rev 8984)
@@ -0,0 +1,237 @@
+// -*- C++ -*-
+
+// RBMDiagonalMatrixConnection.h
+//
+// Copyright (C) 2006 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file RBMDiagonalMatrixConnection.h */
+
+
+#ifndef RBMDiagonalMatrixConnection_INC
+#define RBMDiagonalMatrixConnection_INC
+
+#include &quot;RBMConnection.h&quot;
+
+namespace PLearn {
+using namespace std;
+
+
+/**
+ * Stores and learns the parameters between two linear layers of an RBM.
+ *
+ */
+class RBMDiagonalMatrixConnection: public RBMConnection
+{
+    typedef RBMConnection inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! Vector containing the diagonal of the weight matrix
+    Vec weights_diag;
+
+    //! Optional (default=0) factor of L1 regularization term
+    real L1_penalty_factor;
+
+    //! Optional (default=0) factor of L2 regularization term
+    real L2_penalty_factor;
+
+    real L2_decrease_constant;
+    real L2_shift;
+    string L2_decrease_type;
+    int L2_n_updates;
+
+  //#####  Not Options  #####################################################
+
+
+    //! Accumulates positive contribution to the weights' gradient
+    Vec weights_pos_stats;
+
+    //! Accumulates negative contribution to the weights' gradient
+    Vec weights_neg_stats;
+
+    //! Used if momentum != 0.
+    Vec weights_inc;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    RBMDiagonalMatrixConnection( real the_learning_rate=0 );
+
+    // Your other public member functions go here
+
+    //! Accumulates positive phase statistics to *_pos_stats
+    virtual void accumulatePosStats( const Vec&amp; down_values,
+                                     const Vec&amp; up_values );
+
+    virtual void accumulatePosStats( const Mat&amp; down_values,
+                                     const Mat&amp; up_values );
+
+    //! Accumulates negative phase statistics to *_neg_stats
+    virtual void accumulateNegStats( const Vec&amp; down_values,
+                                     const Vec&amp; up_values );
+
+    virtual void accumulateNegStats( const Mat&amp; down_values,
+                                     const Mat&amp; up_values );
+
+    //! Updates parameters according to contrastive divergence gradient
+    virtual void update();
+
+    //! Updates parameters according to contrastive divergence gradient,
+    //! not using the statistics but the explicit values passed
+    virtual void update( const Vec&amp; pos_down_values,
+                         const Vec&amp; pos_up_values,
+                         const Vec&amp; neg_down_values,
+                         const Vec&amp; neg_up_values );
+
+    //! Not implemented.
+    virtual void update( const Mat&amp; pos_down_values,
+                         const Mat&amp; pos_up_values,
+                         const Mat&amp; neg_down_values,
+                         const Mat&amp; neg_up_values);
+
+    //! Clear all information accumulated during stats
+    virtual void clearStats();
+
+    //! Computes the vectors of activation of &quot;length&quot; units,
+    //! starting from &quot;start&quot;, and stores (or add) them into &quot;activations&quot;.
+    //! &quot;start&quot; indexes an up unit if &quot;going_up&quot;, else a down unit.
+    virtual void computeProduct( int start, int length,
+                                 const Vec&amp; activations,
+                                 bool accumulate=false ) const;
+
+    //! Same as 'computeProduct' but for mini-batches.
+    virtual void computeProducts(int start, int length,
+                                 Mat&amp; activations,
+                                 bool accumulate=false ) const;
+
+    //! Adapt based on the output gradient: this method should only
+    //! be called just after a corresponding fprop; it should be
+    //! called with the same arguments as fprop for the first two arguments
+    //! (and output should not have been modified since then).
+    //! Since sub-classes are supposed to learn ONLINE, the object
+    //! is 'ready-to-be-used' just after any bpropUpdate.
+    //! N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+    //! JUST CALLS
+    //!     bpropUpdate(input, output, input_gradient, output_gradient)
+    //! AND IGNORES INPUT GRADIENT.
+    // virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
+    //                          const Vec&amp; output_gradient);
+
+    //! this version allows to obtain the input gradient as well
+    //! N.B. THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
+    virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                             Vec&amp; input_gradient,
+                             const Vec&amp; output_gradient,
+                             bool accumulate=false);
+
+    virtual void bpropUpdate(const Mat&amp; inputs, const Mat&amp; outputs,
+                             Mat&amp; input_gradients,
+                             const Mat&amp; output_gradients,
+                             bool accumulate = false);
+
+    //! Applies penalty (decay) on weights
+    virtual void applyWeightPenalty();
+
+    //! Adds penalty (decay) gradient
+    virtual void addWeightPenalty(Vec weights_diag, Vec weights_diag_gradients);
+
+    //! reset the parameters to the state they would be BEFORE starting
+    //! training.  Note that this method is necessarily called from
+    //! build().
+    virtual void forget();
+
+    //! optionally perform some processing after training, or after a
+    //! series of fprop/bpropUpdate calls to prepare the model for truly
+    //! out-of-sample operation.  THE DEFAULT IMPLEMENTATION PROVIDED IN
+    //! THE SUPER-CLASS DOES NOT DO ANYTHING.
+    // virtual void finalize();
+
+    //! return the number of parameters
+    virtual int nParameters() const;
+
+    //! Make the parameters data be sub-vectors of the given global_parameters.
+    //! The argument should have size &gt;= nParameters. The result is a Vec
+    //! that starts just after this object's parameters end, i.e.
+    //!    result = global_parameters.subVec(nParameters(),global_parameters.size()-nParameters());
+    //! This allows to easily chain calls of this method on multiple RBMParameters.
+    virtual Vec makeParametersPointHere(const Vec&amp; global_parameters);
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(RBMDiagonalMatrixConnection);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
+
+protected:
+    //#####  Protected Member Functions  ######################################
+    //! Declares the class options.
+    static void declareOptions(OptionList&amp; ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(RBMDiagonalMatrixConnection);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="002431.html">[Plearn-commits] r8983 - trunk/plearn/vmat
</A></li>
	<LI>Next message: <A HREF="002433.html">[Plearn-commits] r8985 - trunk/plearn/vmat
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2432">[ date ]</a>
              <a href="thread.html#2432">[ thread ]</a>
              <a href="subject.html#2432">[ subject ]</a>
              <a href="author.html#2432">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
