From larocheh at mail.berlios.de  Mon Feb  2 16:43:17 2009
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Mon, 2 Feb 2009 16:43:17 +0100
Subject: [Plearn-commits] r9884 - trunk/plearn/vmat
Message-ID: <200902021543.n12FhHb4029975@sheep.berlios.de>

Author: larocheh
Date: 2009-02-02 16:43:16 +0100 (Mon, 02 Feb 2009)
New Revision: 9884

Modified:
   trunk/plearn/vmat/InfiniteMNISTVMatrix.cc
   trunk/plearn/vmat/InfiniteMNISTVMatrix.h
Log:
Corrected a bug when switching to the training set, which was such that test samples were sometimes given instead of training samples!



Modified: trunk/plearn/vmat/InfiniteMNISTVMatrix.cc
===================================================================
--- trunk/plearn/vmat/InfiniteMNISTVMatrix.cc	2009-01-30 03:30:15 UTC (rev 9883)
+++ trunk/plearn/vmat/InfiniteMNISTVMatrix.cc	2009-02-02 15:43:16 UTC (rev 9884)
@@ -60,6 +60,9 @@
 InfiniteMNISTVMatrix::InfiniteMNISTVMatrix():
     include_test_examples(false),
     include_validation_examples(false),
+    random_switch_to_original_training_set(false),
+    proportion_of_switches(0.0),
+    seed(1834),
     input_divisor(1.),
     test_images(TEST_IMAGES_PATH),
     test_labels(TEST_LABELS_PATH),
@@ -70,6 +73,8 @@
 /* ### Initialize all fields to their default value */
 {
     InfiniteMNISTVMatrix::n_pointers_to_dataset++;
+    random_gen = new PRandom();
+    image = (unsigned char*)malloc(EXSIZE);
 }
 
 InfiniteMNISTVMatrix::~InfiniteMNISTVMatrix()
@@ -80,6 +85,7 @@
             destroy_mnistproblem(dataset);
             InfiniteMNISTVMatrix::dataset = 0;
     }
+    free( image );
 }
 
 void InfiniteMNISTVMatrix::getNewRow(int i, const Vec& v) const
@@ -99,8 +105,12 @@
         else
             i_dataset = i + (i/50000)*10000 + 10000;
 
-    unsigned char *image = cache_transformed_vector(InfiniteMNISTVMatrix::dataset, i_dataset);
+    if( random_switch_to_original_training_set && 
+        random_gen->uniform_sample() < proportion_of_switches )
+        i_dataset = (i_dataset % 50000)+10000;
 
+    image = compute_transformed_vector_in_place(InfiniteMNISTVMatrix::dataset, i_dataset, image);
+
     unsigned char* xj=image;
     real* vj=v.data();
     for( int j=0; j<inputsize_; j++, xj++, vj++ )
@@ -123,6 +133,21 @@
                    "Indication that the validation set examples (the last 10000 examples from the\n"
                    "training set) should be included in this VMatrix.\n");     
 
+     declareOption(ol, "random_switch_to_original_training_set", 
+                   &InfiniteMNISTVMatrix::random_switch_to_original_training_set,
+                   OptionBase::buildoption,
+                   "Indication that the VMatrix should randomly (from time to time) provide\n"
+                   "an example from the original training set instead of an example\n"
+                   "from the global dataset.\n");     
+
+     declareOption(ol, "proportion_of_switches", &InfiniteMNISTVMatrix::proportion_of_switches,
+                   OptionBase::buildoption,
+                   "Proportion of switches to the original training set.\n");     
+
+     declareOption(ol, "seed", &InfiniteMNISTVMatrix::seed,
+                   OptionBase::buildoption,
+                   "Seed of random number generator.\n");
+
      declareOption(ol, "input_divisor", &InfiniteMNISTVMatrix::input_divisor,
                    OptionBase::buildoption,
                    "Value that the inputs should be divided by.\n");     
@@ -157,6 +182,7 @@
 
 void InfiniteMNISTVMatrix::build_()
 {
+    random_gen->manual_seed(seed);
 
     if( !InfiniteMNISTVMatrix::dataset )
     {
@@ -233,6 +259,7 @@
     PLWARNING("InfiniteMNISTVMatrix::makeDeepCopyFromShallowCopy is not totally implemented. Need "
               "to figure out how to deep copy the \"dataset\" variable (mnistproblem_t*).\n");
     InfiniteMNISTVMatrix::n_pointers_to_dataset++;
+    image = (unsigned char*)malloc(EXSIZE);
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn/vmat/InfiniteMNISTVMatrix.h
===================================================================
--- trunk/plearn/vmat/InfiniteMNISTVMatrix.h	2009-01-30 03:30:15 UTC (rev 9883)
+++ trunk/plearn/vmat/InfiniteMNISTVMatrix.h	2009-02-02 15:43:16 UTC (rev 9884)
@@ -48,7 +48,9 @@
 #define TANGVEC_PATH "/home/fringant2/lisa/data/mnist/tangVec_float_60000x28x28.bin"
 
 #include <plearn/vmat/RowBufferedVMatrix.h>
+#include <plearn/math/PRandom.h>
 #include <kernel-invariant.h>
+//#include "kernel-invariant2.h"
 
 namespace PLearn {
 
@@ -74,10 +76,24 @@
     //! Indication that the validation set examples (the last 10000 examples from the
     //! training set) should be included in this VMatrix.
     bool include_validation_examples;
+
+    //! Indication that the VMatrix should randomly (from time to time) provide
+    //! an example from the original training set instead of an example
+    //! from the global dataset
+    bool random_switch_to_original_training_set;
     
+    //! Proportion of switches to the original training set
+    real proportion_of_switches;
+    
+    //! Seed of random number generator
+    int seed;
+
     //! Value that the inputs should be divided by.
     real input_divisor;
 
+    //! Random number generator
+    PP< PRandom > random_gen;
+
     // Files required for loading infinite MNIST dataset
     //! File path of MNIST test images.
     string test_images;
@@ -121,6 +137,7 @@
 
     static mnistproblem_t *dataset;
     static int n_pointers_to_dataset;
+    mutable unsigned char* image;
 
 protected:
     //#####  Protected Member Functions  ######################################



From nouiz at mail.berlios.de  Mon Feb  2 17:11:10 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 2 Feb 2009 17:11:10 +0100
Subject: [Plearn-commits] r9885 - trunk/python_modules/plearn/parallel
Message-ID: <200902021611.n12GBAov000196@sheep.berlios.de>

Author: nouiz
Date: 2009-02-02 17:11:09 +0100 (Mon, 02 Feb 2009)
New Revision: 9885

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
detect if the logfile (stdout, stderr) gis too long. If yes, we raise an error. We suppose the filesystem can support 255 caractere, that is not true for old file system.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-02-02 15:43:16 UTC (rev 9884)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-02-02 16:11:09 UTC (rev 9885)
@@ -635,6 +635,10 @@
         logfiles_file = open( 'logfiles', 'w' )
         if self.base_tasks_log_file:
             for task,base in zip(self.tasks,self.base_tasks_log_file):
+                if len(base) > 255-4:
+                    raise DBIError("WARNING: the filename for the stdout and"+
+                                   " stderr file will be too long, so the jobs will fail."+
+                                   " Use the --tasks_filename option to change those name.")
                 tasks_file.write( ';'.join(task.commands) + '\n' )
                 logfiles_file.write( os.path.join(self.log_dir,base) + '\n' )
         else:
@@ -1128,24 +1132,27 @@
         condor_dag_file = self.condor_submit_file+".dag"
         condor_dag_fd = open( condor_dag_file, 'w' )
 
+        def print_task(id, task, stdout_file, stderr_file):
+            argstring =condor_dag_escape_argument(' ; '.join(task.commands))
+            condor_dag_fd.write("JOB %d %s\n"%(id,self.condor_submit_file))
+            if len(stdout_file)<255 or len(stderr_file)>255:
+                raise DBIError("WARNING: the filename for the stdout and stderr"+
+                               " file will be too long, so the jobs will fail."+
+                               " Use the --tasks_filename option to change those name.")
+                
+            condor_dag_fd.write('VARS %d args="%s"\n'%(id,argstring))
+            condor_dag_fd.write('VARS %d stdout="%s"\n'%(id,stdout_file))
+            condor_dag_fd.write('VARS %d stderr="%s"\n\n'%(id,stderr_file))
+
         if self.base_tasks_log_file:
             for i in range(len(self.tasks)):
                 task=self.tasks[i]
-                argstring =condor_dag_escape_argument(' ; '.join(task.commands))
-                condor_dag_fd.write("JOB %d %s\n"%(i,self.condor_submit_file))
-                condor_dag_fd.write('VARS %d args="%s"\n'%(i,argstring))
-                s=os.path.join(self.log_dir,
-                               "condor."+self.base_tasks_log_file[i])
-                condor_dag_fd.write('VARS %d stdout="%s"\n'%(i,s+".out"))
-                condor_dag_fd.write('VARS %d stderr="%s"\n\n'%(i,s+".err"))
+                s=os.path.join(self.log_dir,"condor."+self.base_tasks_log_file[i])
+                print_task(i,task,s+".out",s+".err")
         elif self.stdouts and self.stderrs:
             assert len(self.stdouts)==len(self.stderrs)==len(self.tasks)
-            for (task,stdout_file,stderr_file) in zip(self.tasks,self.stdouts,self.stderrs):
-                argstring =condor_dag_escape_argument(' ; '.join(task.commands))
-                condor_dag_fd.write("JOB %d %s\n"%(i,self.condor_submit_file))
-                condor_dag_fd.write('VARS %d args="%s"\n'%(i,argstring))
-                condor_dag_fd.write('VARS %d stdout="%s"\n'%(i,stdout_file))
-                condor_dag_fd.write('VARS %d stderr="%s"\n\n'%(i,stderr_file))
+            for (i,task,stdout_file,stderr_file) in zip(range(len(self.tasks)),self.tasks,self.stdouts,self.stderrs):
+                print_task(i,task,stdout_file,stderr_file)
         else:
             #should not happen
             raise NotImplementedError()
@@ -1224,6 +1231,10 @@
             def print_task(task, stdout_file, stderr_file,req=""):
                 argstring = condor_escape_argument(' ; '.join(task.commands))
                 condor_submit_fd.write("arguments    = %s \n" %argstring)
+                if len(stdout_file) > 255 or len(stderr_file) > 255:
+                    raise DBIError("WARNING: the filename for the stdout and stderr file"+
+                                   " will be too long, so the jobs will fail."+
+                                   " Use the --tasks_filename option to change those name.")
                 if stdout_file:
                     condor_submit_fd.write("output       = %s \n" %stdout_file)
                 if stderr_file:
@@ -1233,9 +1244,8 @@
             if self.base_tasks_log_file:
                 for (task,task_log,req) in zip(self.tasks,self.base_tasks_log_file,
                                                self.tasks_req):
-                    stdout_file=self.log_dir+"/condor"+task_log+".out"
-                    stderr_file=self.log_dir+"/condor"+task_log+".err"
-                    print_task(task,stdout_file,stderr_file,req)
+                    s=os.path.join(self.log_dir,"condor."+task_log)
+                    print_task(task,s+".out",s+".err",req)
             elif self.stdouts and self.stderrs:
                 assert len(self.stdouts)==len(self.stderrs)==len(self.tasks)
                 for (task,stdout_file,stderr_file,req) in zip(self.tasks,self.stdouts,



From nouiz at mail.berlios.de  Mon Feb  2 17:14:06 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 2 Feb 2009 17:14:06 +0100
Subject: [Plearn-commits] r9886 - trunk/python_modules/plearn/parallel
Message-ID: <200902021614.n12GE6ow000412@sheep.berlios.de>

Author: nouiz
Date: 2009-02-02 17:14:06 +0100 (Mon, 02 Feb 2009)
New Revision: 9886

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
Make a variable of the size maximum of a filename.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-02-02 16:11:09 UTC (rev 9885)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-02-02 16:14:06 UTC (rev 9886)
@@ -31,6 +31,7 @@
 STATUS_RUNNING = 1
 STATUS_WAITING = 2
 STATUS_INIT = 3
+MAX_FILENAME_SIZE=255
 
 class DBIError(Exception):
     """Base class for exceptions in this module."""
@@ -635,7 +636,8 @@
         logfiles_file = open( 'logfiles', 'w' )
         if self.base_tasks_log_file:
             for task,base in zip(self.tasks,self.base_tasks_log_file):
-                if len(base) > 255-4:
+                #-4 as we will happend .err or .out
+                if len(base) > MAX_FILENAME_SIZE-4:
                     raise DBIError("WARNING: the filename for the stdout and"+
                                    " stderr file will be too long, so the jobs will fail."+
                                    " Use the --tasks_filename option to change those name.")
@@ -1135,7 +1137,7 @@
         def print_task(id, task, stdout_file, stderr_file):
             argstring =condor_dag_escape_argument(' ; '.join(task.commands))
             condor_dag_fd.write("JOB %d %s\n"%(id,self.condor_submit_file))
-            if len(stdout_file)<255 or len(stderr_file)>255:
+            if len(stdout_file)>MAX_FILENAME_SIZE or len(stderr_file)>MAX_FILENAME_SIZE:
                 raise DBIError("WARNING: the filename for the stdout and stderr"+
                                " file will be too long, so the jobs will fail."+
                                " Use the --tasks_filename option to change those name.")
@@ -1231,7 +1233,7 @@
             def print_task(task, stdout_file, stderr_file,req=""):
                 argstring = condor_escape_argument(' ; '.join(task.commands))
                 condor_submit_fd.write("arguments    = %s \n" %argstring)
-                if len(stdout_file) > 255 or len(stderr_file) > 255:
+                if len(stdout_file) > MAX_FILENAME_SIZE or len(stderr_file) > MAX_FILENAME_SIZE:
                     raise DBIError("WARNING: the filename for the stdout and stderr file"+
                                    " will be too long, so the jobs will fail."+
                                    " Use the --tasks_filename option to change those name.")



From nouiz at mail.berlios.de  Mon Feb  2 17:51:20 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 2 Feb 2009 17:51:20 +0100
Subject: [Plearn-commits] r9887 - trunk/python_modules/plearn/parallel
Message-ID: <200902021651.n12GpKT4004430@sheep.berlios.de>

Author: nouiz
Date: 2009-02-02 17:51:20 +0100 (Mon, 02 Feb 2009)
New Revision: 9887

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
added optin --jobs_name for bqtools


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-02-02 16:14:06 UTC (rev 9886)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-02-02 16:51:20 UTC (rev 9887)
@@ -558,6 +558,8 @@
         self.long = False
         self.duree = "120:00:00"
         self.submit_options = ""
+        self.jobs_name = ""
+        
         DBIBase.__init__(self, commands, **args)
 
         self.nb_proc = int(self.nb_proc)
@@ -664,17 +666,21 @@
             l+="walltime="+self.duree
         if l:
             tmp_options+=" -l "+l
+        batchName = self.jobs_name
+        if not batchName:
+            batchName = "dbi_"+self.unique_id[1:12]
+
         # Create the bqsubmit.dat, with
         bqsubmit_dat = open( 'bqsubmit.dat', 'w' )
         bqsubmit_dat.write( dedent('''\
-                batchName = dbi_%s
+                batchName = %s
                 command = sh launcher
                 templateFiles = launcher
                 submitOptions = %s
                 param1 = (task, logfile) = load tasks, logfiles
                 linkFiles = launcher
                 preBatch = rm -f _*.BQ
-                '''%(self.unique_id[1:12],tmp_options)) )
+                '''%(batchName,tmp_options)))
         if self.micro>0:
             bqsubmit_dat.write('''microJobs = %d\n'''%(self.micro))
         if self.nano>0:



From nouiz at mail.berlios.de  Mon Feb  2 18:15:51 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 2 Feb 2009 18:15:51 +0100
Subject: [Plearn-commits] r9888 - trunk/python_modules/plearn/parallel
Message-ID: <200902021715.n12HFpr9007668@sheep.berlios.de>

Author: nouiz
Date: 2009-02-02 18:15:50 +0100 (Mon, 02 Feb 2009)
New Revision: 9888

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
added check of the return status as pkdilly now return 1 when if fail.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-02-02 16:51:20 UTC (rev 9887)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-02-02 17:15:50 UTC (rev 9888)
@@ -907,6 +907,7 @@
         self.p = Popen( cmd, shell=True, stdout=PIPE, stderr=PIPE)
         self.p.wait()
         assert self.p.stdout.readline()==""
+        assert self.p.returncode==0
 
 #example de sortie de pkdilly
 #La tache a soumettre est dans: /tmp/soumet_12368_Qbr7Av



From nouiz at mail.berlios.de  Mon Feb  2 18:22:09 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 2 Feb 2009 18:22:09 +0100
Subject: [Plearn-commits] r9889 - trunk/python_modules/plearn/parallel
Message-ID: <200902021722.n12HM9CU008212@sheep.berlios.de>

Author: nouiz
Date: 2009-02-02 18:22:09 +0100 (Mon, 02 Feb 2009)
New Revision: 9889

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
print an error instead of using an assert, this give more info the the reason of the failure.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-02-02 17:15:50 UTC (rev 9888)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-02-02 17:22:09 UTC (rev 9889)
@@ -906,8 +906,11 @@
         cmd="pkdilly -S "+self.condor_submit_file
         self.p = Popen( cmd, shell=True, stdout=PIPE, stderr=PIPE)
         self.p.wait()
-        assert self.p.stdout.readline()==""
-        assert self.p.returncode==0
+        l = self.p.stdout.readline()
+        if l!="":
+            DBIError("pkdilly returned something on the stdout, this should not happen:\n"+l+"\n"+self.p.stdout.readlines())
+        if self.p.returncode!=0:
+            DBIError("pkdilly returned an error code of "+str(self.p.returncode))
 
 #example de sortie de pkdilly
 #La tache a soumettre est dans: /tmp/soumet_12368_Qbr7Av



From nouiz at mail.berlios.de  Mon Feb  2 19:00:54 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 2 Feb 2009 19:00:54 +0100
Subject: [Plearn-commits] r9890 - trunk/scripts
Message-ID: <200902021800.n12I0seM009701@sheep.berlios.de>

Author: nouiz
Date: 2009-02-02 19:00:52 +0100 (Mon, 02 Feb 2009)
New Revision: 9890

Modified:
   trunk/scripts/dbidispatch
Log:
Added modif needed for the option --jobs_name=X


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-02-02 17:22:09 UTC (rev 9889)
+++ trunk/scripts/dbidispatch	2009-02-02 18:00:52 UTC (rev 9890)
@@ -18,7 +18,7 @@
     cluster, condor options  :[--32|--64|--3264] [--os=X]
     bqtools options          :[--micro[=nb_batch]] [--[*no_]long]
                               [--queue=X] [--nano=X] [--submit_options=X]
-                              [*--[no_]clean_up]
+                              [*--[no_]clean_up] [--jobs_name=X]
     cluster option           :[*--[no_]cwait]  [--[*no_]force]
                               [--[*no_]interruptible]
     condor option            :[--req="CONDOR_REQUIREMENT"] [--[*no_]nice]
@@ -36,7 +36,8 @@
 '''
 LongHelp="""Dispatches jobs with dbi.py. dbi can dispatch jobs on condor, bqtools, cluster,
 local and ssh. If no system is selected on the command line, we try them in the
-previous order. ssh is never automaticaly selected.
+previous order. ssh is never automaticaly selected. Currently we have a common interface for parallel jobs that run on SMP node. We
+we don't have a common interface for parallel job that run on multiple nodes.
 
 %(ShortHelp)s
 
@@ -124,6 +125,7 @@
   The '--queue=X' tell on witch queue the jobs will be launched.
   The '--nano=X' add nanoJobs=X in the submit file.
   The '--submit_options=X' X is appended to the submitOptions in the submit file.
+  The '--jobs_name=X' option give the X as the jobs name to bqtools. Default, random.
   
 cluster only options:
   The '--[no_]cwait' is transfered to cluster. 
@@ -312,7 +314,7 @@
     elif argv.split('=')[0] in ["--duree","--cpu","--mem","--os","--nb_proc",
                                 "--req", "--files", "--raw", "--rank", "--env",
                                 "--universe", "--exp_dir", "--machine", "--machines",
-                                "--queue", "--nano", "--submit_options"]:
+                                "--queue", "--nano", "--submit_options", "--jobs_name"]:
         sp = argv.split('=',1)
         param=sp[0][2:]
         val = sp[1]
@@ -367,7 +369,7 @@
                        "universe", "machine", "machines", "to_all"]
 elif launch_cmd=="Bqtools":
     valid_dbi_param +=["cpu", "duree", "long", "mem", "micro",
-                       "nano", "queue", "raw", "submit_options"]
+                       "nano", "queue", "raw", "submit_options", "jobs_name" ]
 
 if  launch_cmd == 'Condor' and gethostname().endswith(".iro.umontreal.ca"):
     #default value for pkdilly is true.



From nouiz at mail.berlios.de  Mon Feb  2 19:14:52 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 2 Feb 2009 19:14:52 +0100
Subject: [Plearn-commits] r9891 - trunk/python_modules/plearn/parallel
Message-ID: <200902021814.n12IEqgv027869@sheep.berlios.de>

Author: nouiz
Date: 2009-02-02 19:14:51 +0100 (Mon, 02 Feb 2009)
New Revision: 9891

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
better error msg and print more stuff to debug kerberos problem.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-02-02 18:00:52 UTC (rev 9890)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-02-02 18:14:51 UTC (rev 9891)
@@ -910,7 +910,7 @@
         if l!="":
             DBIError("pkdilly returned something on the stdout, this should not happen:\n"+l+"\n"+self.p.stdout.readlines())
         if self.p.returncode!=0:
-            DBIError("pkdilly returned an error code of "+str(self.p.returncode))
+            DBIError("pkdilly returned an error code of "+str(self.p.returncode)+":\n"+self.p.stderr.readlines()+"\n"+self.p.stdout.readlines())
 
 #example de sortie de pkdilly
 #La tache a soumettre est dans: /tmp/soumet_12368_Qbr7Av
@@ -1049,6 +1049,7 @@
                 if self.condor_home:
                     fd.write('export HOME=%s\n' % self.condor_home)
                 fd.write(dedent('''
+                    klist
                     cd %s
                     '''%(os.path.abspath("."))))
                 if self.source_file:



From nouiz at mail.berlios.de  Mon Feb  2 20:25:28 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 2 Feb 2009 20:25:28 +0100
Subject: [Plearn-commits] r9892 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200902021925.n12JPS6x004805@sheep.berlios.de>

Author: nouiz
Date: 2009-02-02 20:25:27 +0100 (Mon, 02 Feb 2009)
New Revision: 9892

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
added the option --m32G that is used on ms2 so that we specify a node with 32G.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-02-02 18:14:51 UTC (rev 9891)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-02-02 19:25:27 UTC (rev 9892)
@@ -559,6 +559,7 @@
         self.duree = "120:00:00"
         self.submit_options = ""
         self.jobs_name = ""
+        self.m32G = False
         
         DBIBase.__init__(self, commands, **args)
 
@@ -664,6 +665,9 @@
         if self.duree:
             if l: l+=","
             l+="walltime="+self.duree
+        if self.m32G:
+            if l: l+=","
+            l+="nodes=1:m32G"
         if l:
             tmp_options+=" -l "+l
         batchName = self.jobs_name

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-02-02 18:14:51 UTC (rev 9891)
+++ trunk/scripts/dbidispatch	2009-02-02 19:25:27 UTC (rev 9892)
@@ -18,7 +18,7 @@
     cluster, condor options  :[--32|--64|--3264] [--os=X]
     bqtools options          :[--micro[=nb_batch]] [--[*no_]long]
                               [--queue=X] [--nano=X] [--submit_options=X]
-                              [*--[no_]clean_up] [--jobs_name=X]
+                              [*--[no_]clean_up] [--jobs_name=X] [*--[no_]m32G]
     cluster option           :[*--[no_]cwait]  [--[*no_]force]
                               [--[*no_]interruptible]
     condor option            :[--req="CONDOR_REQUIREMENT"] [--[*no_]nice]
@@ -126,6 +126,7 @@
   The '--nano=X' add nanoJobs=X in the submit file.
   The '--submit_options=X' X is appended to the submitOptions in the submit file.
   The '--jobs_name=X' option give the X as the jobs name to bqtools. Default, random.
+  The '--[no_]m32G' option tell to use node with 32G of RAM. Usefull on mammouth-serie2 only.
   
 cluster only options:
   The '--[no_]cwait' is transfered to cluster. 
@@ -295,11 +296,13 @@
         tasks_filename = val
     elif argv in  ["--force", "--interruptible", "--long", 
                    "--getenv", "--cwait", "--clean_up" ,"--nice",
-                   "--set_special_env", "--abs_path", "--pkdilly", "--to_all"]:
+                   "--set_special_env", "--abs_path", "--pkdilly", "--to_all",
+                   "--m32G"]:
         dbi_param[argv[2:]]=True
     elif argv in ["--no_force", "--no_interruptible", "--no_long",
                   "--no_getenv", "--no_cwait", "--no_clean_up" , "--no_nice",
-                  "--no_set_special_env", "--no_abs_path", "--no_pkdilly"]:
+                  "--no_set_special_env", "--no_abs_path", "--no_pkdilly",
+                  "--no_m32G"]:
         dbi_param[argv[5:]]=False
     elif argv=="--testdbi":
         dbi_param["test"]=True



From nouiz at mail.berlios.de  Mon Feb  2 21:05:49 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 2 Feb 2009 21:05:49 +0100
Subject: [Plearn-commits] r9893 - trunk/scripts
Message-ID: <200902022005.n12K5nx0008238@sheep.berlios.de>

Author: nouiz
Date: 2009-02-02 21:05:49 +0100 (Mon, 02 Feb 2009)
New Revision: 9893

Modified:
   trunk/scripts/dbidispatch
Log:
changed the default filename for the stdout and stderr file of the jobs. now it is nb0,compact.


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-02-02 19:25:27 UTC (rev 9892)
+++ trunk/scripts/dbidispatch	2009-02-02 20:05:49 UTC (rev 9893)
@@ -10,7 +10,7 @@
     bqtools, cluster, condor option  : [--mem=N]
                               [--cpu=nb_cpu_per_node]
     bqtools, cluster option  :[--duree=X]
-    bqtools, condor options  :[--tasks_filename={compact,explicit,*nb0,nb1,
+    bqtools, condor options  :[--tasks_filename={compact,explicit,nb0,nb1,
                                                  sh(condor only),
                                                  clusterid(condor only),
                                                  processid(condor only))}+]
@@ -88,7 +88,7 @@
     filename where the stdout, stderr are redirected. We can put many option 
     separated by comma. They will apper in the filename in order separated by a 
     dot. For all except sh, they have this pattern condor.X.{out,error} where X=:
-      - default : same as nb0
+      - default : as --tasks_filename=nb0,compact
       - compact : a unic string with parameter that change of value between jobs
       - explicit: a unic string that represent the full command to execute
       - nb0     : a number from 0 to nb job -1.
@@ -396,10 +396,8 @@
         if source_with_kerb:
             dbi_param['copy_local_source_file']=True
 
-if launch_cmd=="Bqtools" and not tasks_filename:
-    tasks_filename = ["explicit"]
-elif launch_cmd=="Condor" and not tasks_filename:
-    tasks_filename = ["nb0"]
+if not tasks_filename and launch_cmd in ["Bqtools","Condor"]:
+    tasks_filename = ["nb0","compact"]
 elif tasks_filename:
     print "WARNING: The parameter --tasks_filename={} is only supported by condor and bqtools.",
     print "It will be ignored"
@@ -557,7 +555,7 @@
         dbi_param["stdouts"]=stdouts
         dbi_param["stderrs"]=stderrs
     else:
-        print "internal error!"
+        print "internal error! bad pattern:",pattern
         sys.exit(2)
     assert(not (dbi_param.has_key("stdouts") and (dbi_param[n])==0))
 



From nouiz at mail.berlios.de  Mon Feb  2 21:17:17 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 2 Feb 2009 21:17:17 +0100
Subject: [Plearn-commits] r9894 - trunk/scripts
Message-ID: <200902022017.n12KHH4Z009152@sheep.berlios.de>

Author: nouiz
Date: 2009-02-02 21:17:17 +0100 (Mon, 02 Feb 2009)
New Revision: 9894

Modified:
   trunk/scripts/dbidispatch
Log:
added the option --tasks_filename=none


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-02-02 20:05:49 UTC (rev 9893)
+++ trunk/scripts/dbidispatch	2009-02-02 20:17:17 UTC (rev 9894)
@@ -98,6 +98,7 @@
                   to /dev/null
       - clusterid: (condor only)put the cluster id of the jobs.
       - processid: (condor only)put the process id of the jobs. Idem as nb0
+      - none    : remove all preceding pattern
   The '--raw=STRING1[\nSTRING2...]' option append all STRINGX in the submit file.
       if this option appread many time, they will be concatanated with a new line.
 
@@ -287,7 +288,7 @@
             dbi_param["micro"]=argv[8:]
     elif argv.startswith("--tasks_filename="):
         part = argv.split('=',1)
-        accepted_value=["compact","explicit","nb0","nb1","sh","clusterid","processid"]
+        accepted_value=["compact","explicit","nb0","nb1","sh","clusterid","processid","none"]
         val=part[1].split(",") 
         for v in val:
             if v not in accepted_value:
@@ -522,6 +523,8 @@
         dbi_param[n]=merge_pattern(map(str,range(1,len(commands)+1)))
     elif pattern == "":
         pass
+    elif pattern == "none":
+        dbi_param[n]=[""]*len(commands)
     elif pattern == "clusterid":#$(Cluster)
         dbi_param[n]=merge_pattern(["$(Cluster)"]*len(dbi_param[n]))
     elif pattern == "processid":#$(Process)



From nouiz at mail.berlios.de  Mon Feb  2 21:21:10 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 2 Feb 2009 21:21:10 +0100
Subject: [Plearn-commits] r9895 - trunk/python_modules/plearn/parallel
Message-ID: <200902022021.n12KLAeb009739@sheep.berlios.de>

Author: nouiz
Date: 2009-02-02 21:21:09 +0100 (Mon, 02 Feb 2009)
New Revision: 9895

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
stdout and stderr file with condor don't start with condor to be consistent with bqtools.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-02-02 20:17:17 UTC (rev 9894)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-02-02 20:21:09 UTC (rev 9895)
@@ -1164,7 +1164,7 @@
         if self.base_tasks_log_file:
             for i in range(len(self.tasks)):
                 task=self.tasks[i]
-                s=os.path.join(self.log_dir,"condor."+self.base_tasks_log_file[i])
+                s=os.path.join(self.log_dir,self.base_tasks_log_file[i])
                 print_task(i,task,s+".out",s+".err")
         elif self.stdouts and self.stderrs:
             assert len(self.stdouts)==len(self.stderrs)==len(self.tasks)
@@ -1207,8 +1207,8 @@
                 executable     = %s
                 universe       = %s
                 requirements   = %s
-                output         = %s/condor.$(Process).out
-                error          = %s/condor.$(Process).error
+                output         = %s/$(Process).out
+                error          = %s/$(Process).error
                 log            = %s
                 getenv         = %s
                 nice_user      = %s
@@ -1261,7 +1261,7 @@
             if self.base_tasks_log_file:
                 for (task,task_log,req) in zip(self.tasks,self.base_tasks_log_file,
                                                self.tasks_req):
-                    s=os.path.join(self.log_dir,"condor."+task_log)
+                    s=os.path.join(self.log_dir,task_log)
                     print_task(task,s+".out",s+".err",req)
             elif self.stdouts and self.stderrs:
                 assert len(self.stdouts)==len(self.stderrs)==len(self.tasks)



From nouiz at mail.berlios.de  Tue Feb  3 18:06:39 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 3 Feb 2009 18:06:39 +0100
Subject: [Plearn-commits] r9896 - trunk
Message-ID: <200902031706.n13H6dW8030929@sheep.berlios.de>

Author: nouiz
Date: 2009-02-03 18:06:38 +0100 (Tue, 03 Feb 2009)
New Revision: 9896

Modified:
   trunk/pymake.config.model
Log:
make more pytest work at mammouth, their is 2 test that fail


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2009-02-02 20:21:09 UTC (rev 9895)
+++ trunk/pymake.config.model	2009-02-03 17:06:38 UTC (rev 9896)
@@ -867,7 +867,7 @@
 
 pymakeLinkOption( name = 'mammouthblas',
               description = 'linking BLAS for P4 Mammouth-Serie cluster',
-              linkeroptions = '-L/opt/intel/mkl/10.0.011/lib/em64t -lmkl -openmp' ) #-lmkl_lapack -lmkl_p4 -lmkl_vml_p4 -lpthread ' )
+              linkeroptions = '-L/opt/intel/mkl/9.0/lib/em64t -lmkl -lmkl_lapack -openmp' ) #-lmkl_lapack -lmkl_p4 -lmkl_vml_p4 -lpthread ' )
 
 pymakeLinkOption( name = 'apintelblas',
               description = 'Intel BLAS+LAPACK for generic install in /usr/local/lib (incl. ApSTAT)',



From laulysta at mail.berlios.de  Tue Feb  3 21:52:04 2009
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Tue, 3 Feb 2009 21:52:04 +0100
Subject: [Plearn-commits] r9897 - trunk/plearn_learners_experimental
Message-ID: <200902032052.n13Kq4EF007552@sheep.berlios.de>

Author: laulysta
Date: 2009-02-03 21:52:04 +0100 (Tue, 03 Feb 2009)
New Revision: 9897

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
Log:
deep copy


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-02-03 17:06:38 UTC (rev 9896)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-02-03 20:52:04 UTC (rev 9897)
@@ -480,6 +480,8 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
+    // Public fields
+    deepCopyField( target_layers_weights, copies );
     deepCopyField( input_layer, copies);
     deepCopyField( target_layers , copies);
     deepCopyField( hidden_layer, copies);
@@ -492,8 +494,18 @@
     deepCopyField( target_layers_n_of_target_elements, copies);
     deepCopyField( input_symbol_sizes, copies);
     deepCopyField( target_symbol_sizes, copies);
-    
+    deepCopyField( mean_encoded_vec, copies);
+    deepCopyField( input_reconstruction_bias, copies);
+    deepCopyField( hidden_reconstruction_bias, copies);
 
+    // Protected fields
+    deepCopyField( data, copies);
+    deepCopyField( acc_target_connections_gr, copies);
+    deepCopyField( acc_input_connections_gr, copies);
+    deepCopyField( acc_dynamic_connections_gr, copies);
+    deepCopyField( acc_target_bias_gr, copies);
+    deepCopyField( acc_hidden_bias_gr, copies);
+    deepCopyField( acc_recons_bias_gr, copies);
     deepCopyField( bias_gradient , copies);
     deepCopyField( visi_bias_gradient , copies);
     deepCopyField( hidden_gradient , copies);
@@ -509,8 +521,15 @@
     deepCopyField( nll_list , copies);
     deepCopyField( masks_list , copies);
     deepCopyField( dynamic_act_no_bias_contribution, copies);
+    deepCopyField( trainset_boundaries, copies);
+    deepCopyField( testset_boundaries, copies);
+    deepCopyField( seq, copies);
+    deepCopyField( encoded_seq, copies);
+    deepCopyField( clean_encoded_seq, copies);
+    deepCopyField( input_reconstruction_prob, copies);
+    deepCopyField( hidden_reconstruction_prob, copies);
+    
 
-
     // deepCopyField(, copies);
 
     //PLERROR("DenoisingRecurrentNet::makeDeepCopyFromShallowCopy(): "
@@ -1240,13 +1259,14 @@
 
     // update weight
     externalProductScaleAcc(acc_weights_gr, hidden, hidden_reconstruction_activation_grad, -lr);
+    
     /********************************************************************************/
 
     double result_cost = 0;
     double neg_log_cost = 0; // neg log softmax
     for(int k=0; k<reconstruction_prob.length(); k++)
         if(hidden_target[k]!=0)
-            neg_log_cost -= hidden_target[k]*safelog(reconstruction_prob[k]);
+            neg_log_cost -= hidden_target[k]*safelog(reconstruction_prob[k]) + (1-hidden_target[k])*safelog(1-reconstruction_prob[k]);
     result_cost = neg_log_cost;
     
     return result_cost;

Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2009-02-03 17:06:38 UTC (rev 9896)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2009-02-03 20:52:04 UTC (rev 9897)
@@ -312,7 +312,7 @@
     mutable double current_learning_rate;
 
     //! Store external data;
-    AutoVMatrix*  data;
+    PP<AutoVMatrix> data;
    
     mutable TVec< Mat > acc_target_connections_gr;
 



From nouiz at mail.berlios.de  Tue Feb  3 22:48:22 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 3 Feb 2009 22:48:22 +0100
Subject: [Plearn-commits] r9898 - trunk/scripts
Message-ID: <200902032148.n13LmMLo014299@sheep.berlios.de>

Author: nouiz
Date: 2009-02-03 22:48:22 +0100 (Tue, 03 Feb 2009)
New Revision: 9898

Modified:
   trunk/scripts/dbidispatch
Log:
fixed a warning.


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-02-03 20:52:04 UTC (rev 9897)
+++ trunk/scripts/dbidispatch	2009-02-03 21:48:22 UTC (rev 9898)
@@ -399,9 +399,8 @@
 
 if not tasks_filename and launch_cmd in ["Bqtools","Condor"]:
     tasks_filename = ["nb0","compact"]
-elif tasks_filename:
+elif tasks_filename and launch_cmd not in ["Bqtools", "Condor"]:
     print "WARNING: The parameter --tasks_filename={} is only supported by condor and bqtools.",
-    print "It will be ignored"
     
 print "\n\nThe jobs will be launched on the system:", launch_cmd
 print "With options: ",dbi_param



From nouiz at mail.berlios.de  Tue Feb  3 23:00:03 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 3 Feb 2009 23:00:03 +0100
Subject: [Plearn-commits] r9899 - trunk/python_modules/plearn/parallel
Message-ID: <200902032200.n13M03uR015736@sheep.berlios.de>

Author: nouiz
Date: 2009-02-03 23:00:03 +0100 (Tue, 03 Feb 2009)
New Revision: 9899

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
we need to check the length of each dir and the file in the path. This is a kwik fix that is incomplet.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-02-03 21:48:22 UTC (rev 9898)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-02-03 22:00:03 UTC (rev 9899)
@@ -640,8 +640,8 @@
         if self.base_tasks_log_file:
             for task,base in zip(self.tasks,self.base_tasks_log_file):
                 #-4 as we will happend .err or .out
-                if len(base) > MAX_FILENAME_SIZE-4:
-                    raise DBIError("WARNING: the filename for the stdout and"+
+                if len(os.path.basename(base)) > MAX_FILENAME_SIZE-4:
+                    raise DBIError("ERROR: the filename for the stdout and"+
                                    " stderr file will be too long, so the jobs will fail."+
                                    " Use the --tasks_filename option to change those name.")
                 tasks_file.write( ';'.join(task.commands) + '\n' )
@@ -1152,8 +1152,8 @@
         def print_task(id, task, stdout_file, stderr_file):
             argstring =condor_dag_escape_argument(' ; '.join(task.commands))
             condor_dag_fd.write("JOB %d %s\n"%(id,self.condor_submit_file))
-            if len(stdout_file)>MAX_FILENAME_SIZE or len(stderr_file)>MAX_FILENAME_SIZE:
-                raise DBIError("WARNING: the filename for the stdout and stderr"+
+            if len(os.path.basename(stdout_file))>MAX_FILENAME_SIZE or len(os.path.basename(stderr_file))>MAX_FILENAME_SIZE:
+                raise DBIError("ERROR: the filename for the stdout and stderr"+
                                " file will be too long, so the jobs will fail."+
                                " Use the --tasks_filename option to change those name.")
                 
@@ -1248,8 +1248,8 @@
             def print_task(task, stdout_file, stderr_file,req=""):
                 argstring = condor_escape_argument(' ; '.join(task.commands))
                 condor_submit_fd.write("arguments    = %s \n" %argstring)
-                if len(stdout_file) > MAX_FILENAME_SIZE or len(stderr_file) > MAX_FILENAME_SIZE:
-                    raise DBIError("WARNING: the filename for the stdout and stderr file"+
+                if len(os.path.basename(stdout_file)) > MAX_FILENAME_SIZE or len(os.path.basename(stderr_file)) > MAX_FILENAME_SIZE:
+                    raise DBIError("ERROR: the filename for the stdout and stderr file"+
                                    " will be too long, so the jobs will fail."+
                                    " Use the --tasks_filename option to change those name.")
                 if stdout_file:



From nouiz at mail.berlios.de  Tue Feb  3 23:22:29 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 3 Feb 2009 23:22:29 +0100
Subject: [Plearn-commits] r9900 - trunk/python_modules/plearn/parallel
Message-ID: <200902032222.n13MMTjk018062@sheep.berlios.de>

Author: nouiz
Date: 2009-02-03 23:22:29 +0100 (Tue, 03 Feb 2009)
New Revision: 9900

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
now fully check the path to file for their length.y


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-02-03 22:00:03 UTC (rev 9899)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-02-03 22:22:29 UTC (rev 9900)
@@ -276,6 +276,27 @@
         print "[DBI] %d jobs. finished: %d, running: %d, waiting: %d, init: %d"%(len(self.tasks),finished, running, waiting, init)
         print "[DBI] jobs unfinished (starting at 1): ",unfinished
 
+    def check_path(self, p):
+        """
+        A function that check we use a path to file that is valid.
+        We currently check that each directory and the file in the path
+        are not too long.
+        """
+        l = [p]
+        while True:
+            sp=os.path.split(l[0])
+            if sp[0]=="": break
+            l.append(sp[1])
+            l[0]=sp[0]
+        for pp in l:
+            if len(pp)> MAX_FILENAME_SIZE:
+                raise DBIError("ERROR: a path containt a diretory or a filename that "+
+                               " is too long, so the jobs will fail. Maybe"+
+                               " use the --tasks_filename option to change those name.\n"+
+                               "The full bad path: "+p+"\n"+"The bad part: "+pp)
+                
+
+
 class Task:
 
     def __init__(self, command, tmp_dir, log_dir, time_format, pre_tasks=[], post_tasks=[], dolog = True, id=-1, gen_unique_id = True, args = {}):
@@ -640,12 +661,10 @@
         if self.base_tasks_log_file:
             for task,base in zip(self.tasks,self.base_tasks_log_file):
                 #-4 as we will happend .err or .out
-                if len(os.path.basename(base)) > MAX_FILENAME_SIZE-4:
-                    raise DBIError("ERROR: the filename for the stdout and"+
-                                   " stderr file will be too long, so the jobs will fail."+
-                                   " Use the --tasks_filename option to change those name.")
+                p=os.path.join(self.log_dir,base)
+                self.check_path(p)
                 tasks_file.write( ';'.join(task.commands) + '\n' )
-                logfiles_file.write( os.path.join(self.log_dir,base) + '\n' )
+                logfiles_file.write( p + '\n' )
         else:
             for task in self.tasks:
                 tasks_file.write( ';'.join(task.commands) + '\n' )
@@ -1152,11 +1171,8 @@
         def print_task(id, task, stdout_file, stderr_file):
             argstring =condor_dag_escape_argument(' ; '.join(task.commands))
             condor_dag_fd.write("JOB %d %s\n"%(id,self.condor_submit_file))
-            if len(os.path.basename(stdout_file))>MAX_FILENAME_SIZE or len(os.path.basename(stderr_file))>MAX_FILENAME_SIZE:
-                raise DBIError("ERROR: the filename for the stdout and stderr"+
-                               " file will be too long, so the jobs will fail."+
-                               " Use the --tasks_filename option to change those name.")
-                
+            self.check_path(stdout_file)
+            self.check_path(stderr_file)
             condor_dag_fd.write('VARS %d args="%s"\n'%(id,argstring))
             condor_dag_fd.write('VARS %d stdout="%s"\n'%(id,stdout_file))
             condor_dag_fd.write('VARS %d stderr="%s"\n\n'%(id,stderr_file))
@@ -1248,10 +1264,8 @@
             def print_task(task, stdout_file, stderr_file,req=""):
                 argstring = condor_escape_argument(' ; '.join(task.commands))
                 condor_submit_fd.write("arguments    = %s \n" %argstring)
-                if len(os.path.basename(stdout_file)) > MAX_FILENAME_SIZE or len(os.path.basename(stderr_file)) > MAX_FILENAME_SIZE:
-                    raise DBIError("ERROR: the filename for the stdout and stderr file"+
-                                   " will be too long, so the jobs will fail."+
-                                   " Use the --tasks_filename option to change those name.")
+                self.check_path(stdout_file)
+                self.check_path(stderr_file)
                 if stdout_file:
                     condor_submit_fd.write("output       = %s \n" %stdout_file)
                 if stderr_file:



From nouiz at mail.berlios.de  Tue Feb  3 23:25:15 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 3 Feb 2009 23:25:15 +0100
Subject: [Plearn-commits] r9901 - trunk/python_modules/plearn/parallel
Message-ID: <200902032225.n13MPFAT018340@sheep.berlios.de>

Author: nouiz
Date: 2009-02-03 23:25:15 +0100 (Tue, 03 Feb 2009)
New Revision: 9901

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
check more file path.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-02-03 22:22:29 UTC (rev 9900)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-02-03 22:25:15 UTC (rev 9901)
@@ -217,10 +217,12 @@
         output = PIPE
         error = PIPE
         if int(self.file_redirect_stdout):
+            self.check_path(stdout_file)
             output = open(stdout_file, 'w')
         if self.redirect_stderr_to_stdout:
             error = STDOUT
         elif int(self.file_redirect_stderr):
+            self.check_path(stderr_file)
             error = open(stderr_file, 'w')
         return (output,error)
 



From nouiz at mail.berlios.de  Tue Feb  3 23:35:59 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 3 Feb 2009 23:35:59 +0100
Subject: [Plearn-commits] r9902 - trunk/python_modules/plearn/parallel
Message-ID: <200902032235.n13MZxkg019146@sheep.berlios.de>

Author: nouiz
Date: 2009-02-03 23:35:58 +0100 (Tue, 03 Feb 2009)
New Revision: 9902

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
forgeted a case.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-02-03 22:25:15 UTC (rev 9901)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-02-03 22:35:58 UTC (rev 9902)
@@ -288,6 +288,7 @@
         while True:
             sp=os.path.split(l[0])
             if sp[0]=="": break
+            if sp[1]=="": l[0]=sp[0];break
             l.append(sp[1])
             l[0]=sp[0]
         for pp in l:



From laulysta at mail.berlios.de  Wed Feb  4 07:06:13 2009
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Wed, 4 Feb 2009 07:06:13 +0100
Subject: [Plearn-commits] r9903 - trunk/plearn_learners_experimental
Message-ID: <200902040606.n1466DtJ024754@sheep.berlios.de>

Author: laulysta
Date: 2009-02-04 07:06:11 +0100 (Wed, 04 Feb 2009)
New Revision: 9903

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
Log:
more gradient


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-02-03 22:35:58 UTC (rev 9902)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-02-04 06:06:11 UTC (rev 9903)
@@ -1622,7 +1622,8 @@
                                   input_connections->learning_rate,
                                   false);
                 
-            hidden_temporal_gradient << hidden_gradient;                
+            hidden_temporal_gradient << hidden_gradient;  
+            hidden_temporal_gradient +=  hidden_reconstruction_activation_grad;
         }
         else
         {



From nouiz at mail.berlios.de  Thu Feb  5 17:31:15 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 5 Feb 2009 17:31:15 +0100
Subject: [Plearn-commits] r9904 - trunk/plearn/io
Message-ID: <200902051631.n15GVFut001979@sheep.berlios.de>

Author: nouiz
Date: 2009-02-05 17:31:14 +0100 (Thu, 05 Feb 2009)
New Revision: 9904

Modified:
   trunk/plearn/io/fileutils.cc
   trunk/plearn/io/fileutils.h
Log:
make mvforce cross-compatible.


Modified: trunk/plearn/io/fileutils.cc
===================================================================
--- trunk/plearn/io/fileutils.cc	2009-02-04 06:06:11 UTC (rev 9903)
+++ trunk/plearn/io/fileutils.cc	2009-02-05 16:31:14 UTC (rev 9904)
@@ -399,11 +399,15 @@
 /////////////
 // mvforce //
 /////////////
-void mvforce(const PPath& source, const PPath& destination)
+PRStatus mvforce(const PPath& source, const PPath& destination, bool fail_on_error)
 {
-    // TODO Cross-platform, PR_Access, PR_Delete and PR_Rename
-    string command = "\\mv -f '" + source.absolute() + "' '" + destination.absolute()+"'";
-    system(command.c_str());
+     if(PR_Access(destination.c_str(), PR_ACCESS_EXISTS)==PR_SUCCESS)
+         if(PR_Delete(destination.c_str())!=PR_SUCCESS)
+             if(fail_on_error)
+                 PLERROR("In mvforce(%s,%s) - we failed to delete the destination!",source.c_str(),destination.c_str());
+             else
+                 return PR_FAILURE;
+     return mv(source,destination);
 }
 
 

Modified: trunk/plearn/io/fileutils.h
===================================================================
--- trunk/plearn/io/fileutils.h	2009-02-04 06:06:11 UTC (rev 9903)
+++ trunk/plearn/io/fileutils.h	2009-02-05 16:31:14 UTC (rev 9904)
@@ -136,7 +136,7 @@
 PRStatus mv(const PPath& source, const PPath& dest, bool fail_on_error = true);
 
 //! Same as mv, but will not prompt before overwriting.
-void mvforce(const PPath& source, const PPath& dest);
+PRStatus mvforce(const PPath& source, const PPath& dest, bool fail_on_error = true);
 
 //! Trivial unix touch.
 void touch(const PPath& file);



From nouiz at mail.berlios.de  Thu Feb  5 17:35:36 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 5 Feb 2009 17:35:36 +0100
Subject: [Plearn-commits] r9905 - trunk/commands/PLearnCommands
Message-ID: <200902051635.n15GZaQQ002345@sheep.berlios.de>

Author: nouiz
Date: 2009-02-05 17:35:35 +0100 (Thu, 05 Feb 2009)
New Revision: 9905

Modified:
   trunk/commands/PLearnCommands/ReadAndWriteCommand.cc
Log:
don't print useless stuff


Modified: trunk/commands/PLearnCommands/ReadAndWriteCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/ReadAndWriteCommand.cc	2009-02-05 16:31:14 UTC (rev 9904)
+++ trunk/commands/PLearnCommands/ReadAndWriteCommand.cc	2009-02-05 16:35:35 UTC (rev 9905)
@@ -93,7 +93,6 @@
     string right;
     for(uint i=2; i<args.size();i++){
         split_on_first(args[i], "=", left, right);
-        cout <<left<<endl<<right<<endl;
         o->setOption(left, right);
     }
 



From nouiz at mail.berlios.de  Thu Feb  5 17:41:14 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 5 Feb 2009 17:41:14 +0100
Subject: [Plearn-commits] r9906 - trunk/plearn/io
Message-ID: <200902051641.n15GfEVj002751@sheep.berlios.de>

Author: nouiz
Date: 2009-02-05 17:41:14 +0100 (Thu, 05 Feb 2009)
New Revision: 9906

Modified:
   trunk/plearn/io/load_and_save.h
Log:
added the fct PLearn::tmpsave that will save in a temp file then move it. This is to don't have partial file.


Modified: trunk/plearn/io/load_and_save.h
===================================================================
--- trunk/plearn/io/load_and_save.h	2009-02-05 16:35:35 UTC (rev 9905)
+++ trunk/plearn/io/load_and_save.h	2009-02-05 16:41:14 UTC (rev 9906)
@@ -77,6 +77,16 @@
 }
 
 
+//! We save in a tmp file, then we move it to the real file. 
+//! This help to don't have file partially saved.
+template<class T> 
+inline void tmpsave(const PPath& filepath, const T& x, PStream::mode_t io_formatting=PStream::plearn_ascii, bool implicit_storage = true)
+{ 
+    PPath tmp_file=filepath+".plearn_tmpsave";
+    perr<<tmp_file<<endl;
+    PLearn::save(tmp_file, x, io_formatting, implicit_storage);
+    mvforce(tmp_file, filepath);
+}
 
 } // end of namespace PLearn
 



From nouiz at mail.berlios.de  Thu Feb  5 17:45:22 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 5 Feb 2009 17:45:22 +0100
Subject: [Plearn-commits] r9907 - trunk/plearn/vmat
Message-ID: <200902051645.n15GjMub003233@sheep.berlios.de>

Author: nouiz
Date: 2009-02-05 17:45:22 +0100 (Thu, 05 Feb 2009)
New Revision: 9907

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
Log:
save in a temp file to don't have partial file.


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2009-02-05 16:41:14 UTC (rev 9906)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2009-02-05 16:45:22 UTC (rev 9907)
@@ -107,8 +107,9 @@
 
     if(idxfile)
         fclose(idxfile);
-
-    idxfile = fopen(( getMetaDataDir()/"txtmat.idx").c_str(),"wb");
+    PPath ft=getMetaDataDir()/"txtmat.idx.tmp";
+    PPath f=getMetaDataDir()/"txtmat.idx";
+    idxfile = fopen(ft.c_str(),"wb");
     FILE* logfile = fopen((getMetaDataDir()/"txtmat.idx.log").c_str(),"a");
 
     if (! idxfile)
@@ -204,7 +205,7 @@
     // close files
     fclose(logfile);
     fclose(idxfile);
-
+    mvforce(ft,f);
     perr << "Index file built." << endl;
 }
 



From nouiz at mail.berlios.de  Thu Feb  5 22:26:36 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 5 Feb 2009 22:26:36 +0100
Subject: [Plearn-commits] r9908 - in trunk/plearn_learners/meta: .
	test/MultiClassAdaBoost
	test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir
	test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0
Message-ID: <200902052126.n15LQaU3008351@sheep.berlios.de>

Author: nouiz
Date: 2009-02-05 22:26:35 +0100 (Thu, 05 Feb 2009)
New Revision: 9908

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
   trunk/plearn_learners/meta/MultiClassAdaBoost.h
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_costs.pmat
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_costs.pmat
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/experiment.plearn
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/PL_MutiClassAdaBoost.pyplearn
Log:
-use PTimer instead of Profiler as Profiler is not compatible on windows.
-Added the option time_costs that if false, we don't generate the timing and use this option for the test.
-updated the test.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-02-05 16:45:22 UTC (rev 9907)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-02-05 21:26:35 UTC (rev 9908)
@@ -62,6 +62,8 @@
     total_train_time(0),
     test_time(0),
     total_test_time(0),
+    time_costs(true),
+    timer(new PTimer()),
     time_sum(0),
     time_sum_rtr(0),
     time_last_stage(0),
@@ -137,6 +139,10 @@
                   &MultiClassAdaBoost::total_test_time, OptionBase::learntoption,
                   "The total time spent in the test() function in second.");
 
+    declareOption(ol, "time_costs",
+                  &MultiClassAdaBoost::time_costs, OptionBase::buildoption,
+                  "If true, generate the time costs. Else they are nan.");
+
     declareOption(ol, "time_sum",
                   &MultiClassAdaBoost::time_sum, 
                   OptionBase::learntoption|OptionBase::nosave,
@@ -189,8 +195,7 @@
                 setTrainingSet(train_set);
     }
 
-    Profiler::activate();
-    Profiler::reset("MultiClassAdaBoost::test()");
+    timer->newTimer("MultiClassAdaBoost::test()", true);
 }
 
 // ### Nothing to add here, simply calls build_
@@ -248,7 +253,8 @@
 void MultiClassAdaBoost::train()
 {
     EXTREME_MODULE_LOG<<"train() start"<<endl;
-    Profiler::start("MultiClassAdaBoost::train");
+    timer->startTimer("MultiClassAdaBoost::train");
+    Profiler::pl_profile_start("MultiClassAdaBoost::train");
 
     learner1->nstages = nstages;
     learner2->nstages = nstages;
@@ -288,17 +294,16 @@
         train_stats->append(*(v),"sublearner1.");
     if(v=learner2->getTrainStatsCollector())
         train_stats->append(*(v),"sublearner2.");
-
-    Profiler::end("MultiClassAdaBoost::train");
-    const Profiler::Stats& stats = Profiler::getStats("MultiClassAdaBoost::train");
-    real tmp=stats.wall_duration/Profiler::ticksPerSecond();
+    timer->stopTimer("MultiClassAdaBoost::train");
+    Profiler::pl_profile_end("MultiClassAdaBoost::train");
+    
+    real tmp = timer->getTimer("MultiClassAdaBoost::train");
     train_time=tmp - total_train_time;
     total_train_time=tmp;
 
     //we get the test_time here as we want the test time for all dataset.
     //if we put it in the test function, we would have it for one dataset.
-    const Profiler::Stats& stats_test = Profiler::getStats("MultiClassAdaBoost::test()");
-    tmp=stats_test.wall_duration/Profiler::ticksPerSecond();
+    tmp = timer->getTimer("MultiClassAdaBoost::test()");
     test_time=tmp-total_test_time;
     total_test_time=tmp;
     EXTREME_MODULE_LOG<<"train() end"<<endl;
@@ -397,10 +402,14 @@
 
     costs[4]=costs[5]=costs[6]=0;
     costs[out+4]=1;
-    costs[7]=train_time;
-    costs[8]=total_train_time;
-    costs[9]=test_time;
-    costs[10]=total_test_time;
+    if(time_costs){
+        costs[7]=train_time;
+        costs[8]=total_train_time;
+        costs[9]=test_time;
+        costs[10]=total_test_time;
+    }else{
+        costs[7]=costs[8]=costs[9]=costs[10]=MISSING_VALUE;
+    }
     if(forward_sub_learner_test_costs){
         costs.resize(7+4);
         subcosts1+=subcosts2;
@@ -592,10 +601,12 @@
 void MultiClassAdaBoost::test(VMat testset, PP<VecStatsCollector> test_stats,
                               VMat testoutputs, VMat testcosts) const
 {
-    Profiler::start("MultiClassAdaBoost::test()");
+    Profiler::pl_profile_start("MultiClassAdaBoost::test()");
+    timer->startTimer("MultiClassAdaBoost::test()");
     if(!forward_test){
          inherited::test(testset,test_stats,testoutputs,testcosts);
-         Profiler::end("MultiClassAdaBoost::test()");
+         Profiler::pl_profile_end("MultiClassAdaBoost::test()");
+         timer->stopTimer("MultiClassAdaBoost::test()");
          return;
     }
 
@@ -610,20 +621,20 @@
 
     if(forward_test==2 && time_last_stage<time_last_stage_rtr){
         EXTREME_MODULE_LOG<<"inherited start time_sum="<<time_sum<<" time_sum_rtr="<<time_sum_rtr<<" last_stage="<<last_stage <<" stage=" <<stage <<" time_last_stage=" <<time_last_stage<<" time_last_stage_rtr=" <<time_last_stage_rtr<<endl;
-        Profiler::reset("MultiClassAdaBoost::test() current");
-        Profiler::start("MultiClassAdaBoost::test() current");
+        timer->resetTimer("MultiClassAdaBoost::test() current");
+        timer->startTimer("MultiClassAdaBoost::test() current");
         PLCHECK(last_stage<=stage);
         inherited::test(testset,test_stats,testoutputs,testcosts);
-        Profiler::end("MultiClassAdaBoost::test() current");
-        Profiler::end("MultiClassAdaBoost::test()");
-        time_sum += Profiler::getStats("MultiClassAdaBoost::test() current").wall_duration;
+        timer->stopTimer("MultiClassAdaBoost::test() current");
+        Profiler::pl_profile_end("MultiClassAdaBoost::test()");
+        time_sum += timer->getTimer("MultiClassAdaBoost::test() current");
         last_stage=stage;
         EXTREME_MODULE_LOG<<"inherited end time_sum="<<time_sum<<" time_sum_rtr="<<time_sum_rtr<<" last_stage="<<last_stage <<" stage=" <<stage <<" time_last_stage=" <<time_last_stage<<" time_last_stage_rtr=" <<time_last_stage_rtr<<endl;
         return;
     }
     EXTREME_MODULE_LOG<<"start time_sum="<<time_sum<<" time_sum_rtr="<<time_sum_rtr<<" last_stage="<<last_stage <<" stage=" <<stage <<" time_last_stage=" <<time_last_stage<<" time_last_stage_rtr=" <<time_last_stage_rtr<<endl;
-    Profiler::reset("MultiClassAdaBoost::test() current");
-    Profiler::start("MultiClassAdaBoost::test() current");
+    timer->resetTimer("MultiClassAdaBoost::test() current");
+    timer->startTimer("MultiClassAdaBoost::test() current");
     //Profiler::pl_profile_start("MultiClassAdaBoost::test() part1");//cheap
     int index=-1;
     for(int i=0;i<saved_testset.length();i++){
@@ -669,10 +680,10 @@
         
     }
     //Profiler::pl_profile_end("MultiClassAdaBoost::test() part1");//cheap
-    Profiler::start("MultiClassAdaBoost::test() subtest");
+    Profiler::pl_profile_start("MultiClassAdaBoost::test() subtest");
     learner1->test(testset1,test_stats1,testoutputs1,testcosts1);
     learner2->test(testset2,test_stats2,testoutputs2,testcosts2);
-    Profiler::end("MultiClassAdaBoost::test() subtest");
+    Profiler::pl_profile_end("MultiClassAdaBoost::test() subtest");
 
     VMat my_outputs = 0;
     VMat my_costs = 0;
@@ -764,11 +775,12 @@
 	}
     }
     Profiler::pl_profile_end("MultiClassAdaBoost::test() test_stats");
-    Profiler::end("MultiClassAdaBoost::test() current");
-    Profiler::end("MultiClassAdaBoost::test()");
+    timer->stopTimer("MultiClassAdaBoost::test() current");
+    Profiler::pl_profile_end("MultiClassAdaBoost::test()");
     
-    time_sum_rtr += Profiler::getStats("MultiClassAdaBoost::test() current").wall_duration;
+    time_sum_rtr +=timer->getTimer("MultiClassAdaBoost::test() current");
 
+
     last_stage=stage;
     EXTREME_MODULE_LOG<<"end time_sum="<<time_sum<<" time_sum_rtr="<<time_sum_rtr<<" last_stage="<<last_stage <<" stage=" <<stage <<" time_last_stage=" <<time_last_stage<<" time_last_stage_rtr=" <<time_last_stage_rtr<<endl;
 

Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.h	2009-02-05 16:45:22 UTC (rev 9907)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.h	2009-02-05 21:26:35 UTC (rev 9908)
@@ -42,6 +42,7 @@
 
 #include <plearn_learners/generic/PLearner.h>
 #include <plearn_learners/meta/AdaBoost.h>
+#include <plearn/misc/PTimer.h>
 
 namespace PLearn {
 
@@ -83,10 +84,14 @@
     //! The total time passed in test()
     real total_test_time;
 
-    mutable long time_sum;
-    mutable long time_sum_rtr;
-    mutable long time_last_stage;
-    mutable long time_last_stage_rtr;
+    bool time_costs;
+
+    PP<PTimer> timer;
+
+    mutable real time_sum;
+    mutable real time_sum_rtr;
+    mutable real time_last_stage;
+    mutable real time_last_stage_rtr;
     mutable int last_stage;
 
 public:

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2009-02-05 16:45:22 UTC (rev 9907)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2009-02-05 21:26:35 UTC (rev 9908)
@@ -927,7 +927,8 @@
 train_time = 0 ;
 total_train_time = 0 ;
 test_time = 0 ;
-total_test_time = 0  )
+total_test_time = 0 ;
+time_costs = 0  )
 ;
 perf_evaluators = {};
 report_stats = 1 ;

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/experiment.plearn
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/experiment.plearn	2009-02-05 16:45:22 UTC (rev 9907)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/experiment.plearn	2009-02-05 21:26:35 UTC (rev 9908)
@@ -33,7 +33,8 @@
                     ),
                 weight_by_resampling = 0
                 ),
-            test_minibatch_size = 1
+            test_minibatch_size = 1,
+            time_costs = False
             ),
         nstages = 1,
         option_fields = [ "nstages" ],

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt	2009-02-05 16:45:22 UTC (rev 9907)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt	2009-02-05 21:26:35 UTC (rev 9908)
@@ -1,4 +1,4 @@
-__REVISION__ = "PL9866"
+__REVISION__ = "PL9903"
 conf                                          = False
 pseudo                                        = False
 tms                                           = 1

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave	2009-02-05 16:45:22 UTC (rev 9907)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave	2009-02-05 21:26:35 UTC (rev 9908)
@@ -316,7 +316,8 @@
 train_time = 0 ;
 total_train_time = 0 ;
 test_time = 0 ;
-total_test_time = 0  )
+total_test_time = 0 ;
+time_costs = 0  )
 ;
 perf_evaluators = {};
 report_stats = 1 ;

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/PL_MutiClassAdaBoost.pyplearn
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/PL_MutiClassAdaBoost.pyplearn	2009-02-05 16:45:22 UTC (rev 9907)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/PL_MutiClassAdaBoost.pyplearn	2009-02-05 21:26:35 UTC (rev 9908)
@@ -7,6 +7,7 @@
 learner=pl.MultiClassAdaBoost( 
     forward_sub_learner_test_costs=1,
     test_minibatch_size=plargs.tms,
+    time_costs=False,
     learner_template = pl.AdaBoost(
         weak_learner_template=pl.RegressionTree(
             nstages = 4,



From nouiz at mail.berlios.de  Thu Feb  5 22:41:55 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 5 Feb 2009 22:41:55 +0100
Subject: [Plearn-commits] r9909 - trunk/plearn/misc
Message-ID: <200902052141.n15LftBV009883@sheep.berlios.de>

Author: nouiz
Date: 2009-02-05 22:41:55 +0100 (Thu, 05 Feb 2009)
New Revision: 9909

Modified:
   trunk/plearn/misc/PTimer.cc
   trunk/plearn/misc/PTimer.h
Log:
allow to create multiple time a PTimer with a special parameter.


Modified: trunk/plearn/misc/PTimer.cc
===================================================================
--- trunk/plearn/misc/PTimer.cc	2009-02-05 21:26:35 UTC (rev 9908)
+++ trunk/plearn/misc/PTimer.cc	2009-02-05 21:41:55 UTC (rev 9909)
@@ -134,9 +134,10 @@
 //////////////
 // newTimer //
 //////////////
-void PTimer::newTimer(const string& timer_name)
+void PTimer::newTimer(const string& timer_name, bool can_exist)
 {
-    PLASSERT( name_to_idx.find(timer_name) == name_to_idx.end() );
+    if(!can_exist)
+        PLASSERT( name_to_idx.find(timer_name) == name_to_idx.end() );
     int n_timers = total_times.length();
     name_to_idx[timer_name] = n_timers;
     total_times.append(0);

Modified: trunk/plearn/misc/PTimer.h
===================================================================
--- trunk/plearn/misc/PTimer.h	2009-02-05 21:26:35 UTC (rev 9908)
+++ trunk/plearn/misc/PTimer.h	2009-02-05 21:41:55 UTC (rev 9909)
@@ -76,7 +76,7 @@
     real getTimer(const string& timer_name);
 
     //! Add a new timer.
-    void newTimer(const string& timer_name);
+    void newTimer(const string& timer_name, bool can_exit = false);
 
     //! Reset all timers.
     void resetAllTimers();



From nouiz at mail.berlios.de  Thu Feb  5 23:03:31 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 5 Feb 2009 23:03:31 +0100
Subject: [Plearn-commits] r9910 - trunk/plearn_learners/meta
Message-ID: <200902052203.n15M3V6R011648@sheep.berlios.de>

Author: nouiz
Date: 2009-02-05 23:03:30 +0100 (Thu, 05 Feb 2009)
New Revision: 9910

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
Log:
fixed test in now saving the time.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-02-05 21:41:55 UTC (rev 9909)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-02-05 22:03:30 UTC (rev 9910)
@@ -124,19 +124,23 @@
                   " based on past  test time.\n");
 
     declareOption(ol, "train_time",
-                  &MultiClassAdaBoost::train_time, OptionBase::learntoption,
+                  &MultiClassAdaBoost::train_time, 
+                  OptionBase::learntoption|OptionBase::nosave,
                   "The time spent in the last call to train() in second.");
 
     declareOption(ol, "total_train_time",
-                  &MultiClassAdaBoost::total_train_time, OptionBase::learntoption,
+                  &MultiClassAdaBoost::total_train_time, 
+                  OptionBase::learntoption|OptionBase::nosave,
                   "The total time spent in the train() function in second.");
 
     declareOption(ol, "test_time",
-                  &MultiClassAdaBoost::test_time, OptionBase::learntoption,
+                  &MultiClassAdaBoost::test_time, 
+                  OptionBase::learntoption|OptionBase::nosave,
                   "The time spent in the last call to test() in second.");
 
     declareOption(ol, "total_test_time",
-                  &MultiClassAdaBoost::total_test_time, OptionBase::learntoption,
+                  &MultiClassAdaBoost::total_test_time, 
+                  OptionBase::learntoption|OptionBase::nosave,
                   "The total time spent in the test() function in second.");
 
     declareOption(ol, "time_costs",



From saintmlx at mail.berlios.de  Thu Feb  5 23:36:42 2009
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 5 Feb 2009 23:36:42 +0100
Subject: [Plearn-commits] r9911 - trunk/python_modules/plearn/utilities
Message-ID: <200902052236.n15MagwC016400@sheep.berlios.de>

Author: saintmlx
Date: 2009-02-05 23:36:42 +0100 (Thu, 05 Feb 2009)
New Revision: 9911

Modified:
   trunk/python_modules/plearn/utilities/ppath.py
Log:
- allow non-qualified host name other than localhost



Modified: trunk/python_modules/plearn/utilities/ppath.py
===================================================================
--- trunk/python_modules/plearn/utilities/ppath.py	2009-02-05 22:03:30 UTC (rev 9910)
+++ trunk/python_modules/plearn/utilities/ppath.py	2009-02-05 22:36:42 UTC (rev 9911)
@@ -82,10 +82,9 @@
 def get_domain_name():
     from socket import getfqdn
     host_name = getfqdn()
-    if host_name == 'localhost':
-        return ''
     i = host_name.find('.')
-    assert i != -1, "getfqdn didn't return a fully-qualified name (no '.')."
+    if i == -1 or host_name == 'localhost':
+        return ''
     return host_name[i+1:]
 
 def exempt_of_subdirectories( directories ):



From laulysta at mail.berlios.de  Fri Feb  6 00:36:36 2009
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Fri, 6 Feb 2009 00:36:36 +0100
Subject: [Plearn-commits] r9912 - trunk/plearn_learners_experimental
Message-ID: <200902052336.n15NaadM028786@sheep.berlios.de>

Author: laulysta
Date: 2009-02-06 00:36:35 +0100 (Fri, 06 Feb 2009)
New Revision: 9912

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
Log:
gradient


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-02-05 22:36:42 UTC (rev 9911)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-02-05 23:36:35 UTC (rev 9912)
@@ -1252,15 +1252,19 @@
     //transposeProductAcc(hidden_gradient, reconstruction_weights, hidden_reconstruction_activation_grad);
     
     //update bias
-    multiplyAcc(reconstruction_bias, hidden_reconstruction_activation_grad, -lr);
+    //multiplyAcc(reconstruction_bias, hidden_reconstruction_activation_grad, -lr);
     // update weight
     //externalProductScaleAcc(reconstruction_weights, hidden_reconstruction_activation_grad, hidden, -lr);
                 
 
     // update weight
-    externalProductScaleAcc(acc_weights_gr, hidden, hidden_reconstruction_activation_grad, -lr);
+    //externalProductScaleAcc(acc_weights_gr, hidden, hidden_reconstruction_activation_grad, -lr);
     
     /********************************************************************************/
+    
+    hidden_reconstruction_activation_grad.clear();
+    for(int k=0; k<reconstruction_prob.length(); k++)
+            hidden_reconstruction_activation_grad[k] = safelog(reconstruction_prob[k] - safelog(1-reconstruction_prob[k]);
 
     double result_cost = 0;
     double neg_log_cost = 0; // neg log softmax



From laulysta at mail.berlios.de  Fri Feb  6 00:47:52 2009
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Fri, 6 Feb 2009 00:47:52 +0100
Subject: [Plearn-commits] r9913 - trunk/plearn_learners_experimental
Message-ID: <200902052347.n15NlqMI006604@sheep.berlios.de>

Author: laulysta
Date: 2009-02-06 00:47:51 +0100 (Fri, 06 Feb 2009)
New Revision: 9913

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
Log:
erreur


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-02-05 23:36:35 UTC (rev 9912)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-02-05 23:47:51 UTC (rev 9913)
@@ -1264,7 +1264,7 @@
     
     hidden_reconstruction_activation_grad.clear();
     for(int k=0; k<reconstruction_prob.length(); k++)
-            hidden_reconstruction_activation_grad[k] = safelog(reconstruction_prob[k] - safelog(1-reconstruction_prob[k]);
+        hidden_reconstruction_activation_grad[k] = safelog(reconstruction_prob[k]) - safelog(1-reconstruction_prob[k]);
 
     double result_cost = 0;
     double neg_log_cost = 0; // neg log softmax



From nouiz at mail.berlios.de  Fri Feb  6 15:34:33 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 6 Feb 2009 15:34:33 +0100
Subject: [Plearn-commits] r9914 - trunk/plearn/io
Message-ID: <200902061434.n16EYXpO030530@sheep.berlios.de>

Author: nouiz
Date: 2009-02-06 15:34:33 +0100 (Fri, 06 Feb 2009)
New Revision: 9914

Modified:
   trunk/plearn/io/load_and_save.h
Log:
removed printing.


Modified: trunk/plearn/io/load_and_save.h
===================================================================
--- trunk/plearn/io/load_and_save.h	2009-02-05 23:47:51 UTC (rev 9913)
+++ trunk/plearn/io/load_and_save.h	2009-02-06 14:34:33 UTC (rev 9914)
@@ -83,7 +83,6 @@
 inline void tmpsave(const PPath& filepath, const T& x, PStream::mode_t io_formatting=PStream::plearn_ascii, bool implicit_storage = true)
 { 
     PPath tmp_file=filepath+".plearn_tmpsave";
-    perr<<tmp_file<<endl;
     PLearn::save(tmp_file, x, io_formatting, implicit_storage);
     mvforce(tmp_file, filepath);
 }



From saintmlx at mail.berlios.de  Fri Feb  6 17:53:38 2009
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Fri, 6 Feb 2009 17:53:38 +0100
Subject: [Plearn-commits] r9915 - trunk/python_modules/plearn/pyext
Message-ID: <200902061653.n16Grcgb013305@sheep.berlios.de>

Author: saintmlx
Date: 2009-02-06 17:53:38 +0100 (Fri, 06 Feb 2009)
New Revision: 9915

Modified:
   trunk/python_modules/plearn/pyext/__init__.py
Log:
- set PLearn python module



Modified: trunk/python_modules/plearn/pyext/__init__.py
===================================================================
--- trunk/python_modules/plearn/pyext/__init__.py	2009-02-06 14:34:33 UTC (rev 9914)
+++ trunk/python_modules/plearn/pyext/__init__.py	2009-02-06 16:53:38 UTC (rev 9915)
@@ -36,6 +36,10 @@
 exec 'from %s.%s import *' % (pl_lib_dir, pl_lib_name)
 exec 'from %s import %s as pl' % (pl_lib_dir, pl_lib_name)
 
+
+from plearn.pybridge import wrapped_plearn_object
+wrapped_plearn_object.plearn_module= pl
+
 import gc, atexit
 from plearn.pyplearn.plargs import *
 from plearn.utilities.options_dialog import *



From nouiz at mail.berlios.de  Fri Feb  6 17:58:15 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 6 Feb 2009 17:58:15 +0100
Subject: [Plearn-commits] r9916 - trunk/python_modules/plearn/parallel
Message-ID: <200902061658.n16GwFOs013962@sheep.berlios.de>

Author: nouiz
Date: 2009-02-06 17:58:15 +0100 (Fri, 06 Feb 2009)
New Revision: 9916

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
-print more debug stuff about the renew of the kerb script.
-put klist after the source.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-02-06 16:53:38 UTC (rev 9915)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-02-06 16:58:15 UTC (rev 9916)
@@ -977,6 +977,8 @@
         if pid==0:#in the childreen
             #renew each hour
             out=open(renew_out_file,"w")
+            out.write(line_header()+"will renew the lauch file "+self.launch_file+" each "+str(seconds)+"s\n")
+            out.flush()
             while True:
                 p = Popen( cmd, shell=True, stdout=out, stderr=STDOUT)
                 ret = p.wait()
@@ -994,14 +996,17 @@
                     break
                 else:
                     out.write(line_header()+
-                              "renew the launch file "+self.launch_file+"\n")
+                              "renew the launch file "+self.launch_file+". The old version had a size of \n")
                     out.flush()
                     launch_tmp_file=self.launch_file+".tmp"
                     fd=open(launch_tmp_file,'w')
-                    self.make_kerb_script(fd,self.second_lauch_file)
+                    kerb_vars=self.make_kerb_script(fd,self.second_lauch_file)
                     fd.close()
                     os.chmod(launch_tmp_file, 0755)
                     os.rename(launch_tmp_file, self.launch_file)
+                    s=os.stat(self.launch_file)[os.path.stat.ST_SIZE]
+                    out.write(line_header()+
+                              "generated "+len(kerb_vars)+" kerberos variables. The file size is "+str(s)+"\n")
                 out.flush()
                 #we do this as in some case(with dagman) the log file can 
                 #take a few second to be created. So we don't loop too fast
@@ -1027,6 +1032,7 @@
                 export KRVEXECUTE=%s
                 /usr/sbin/circus "$@"
                 '''%(os.path.abspath(second_lauch_file))))
+        return get
 
     def make_launch_script(self, bash_exec):
             
@@ -1062,7 +1068,8 @@
             
             if self.pkdilly:
                 self.second_lauch_file = self.launch_file+"2.sh"
-                self.make_kerb_script(fd, self.second_lauch_file)
+                kerb_vars=self.make_kerb_script(fd, self.second_lauch_file)
+                assert(len(kerb_vars)>0)
                 fd.close()
 
                 fd = open(self.second_lauch_file,'w')
@@ -1075,13 +1082,13 @@
                 if self.condor_home:
                     fd.write('export HOME=%s\n' % self.condor_home)
                 fd.write(dedent('''
-                    klist
                     cd %s
                     '''%(os.path.abspath("."))))
                 if self.source_file:
                     fd.write('source ' + self.source_file + '\n')
 
                 fd.write(dedent('''\
+                    klist
                     echo "Executing on " `/bin/hostname` 1>&2
                     echo "HOSTNAME: ${HOSTNAME}" 1>&2
                     echo "PATH: $PATH" 1>&2



From nouiz at mail.berlios.de  Fri Feb  6 18:11:39 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 6 Feb 2009 18:11:39 +0100
Subject: [Plearn-commits] r9917 - trunk/python_modules/plearn/parallel
Message-ID: <200902061711.n16HBdjg020009@sheep.berlios.de>

Author: nouiz
Date: 2009-02-06 18:11:38 +0100 (Fri, 06 Feb 2009)
New Revision: 9917

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
..


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-02-06 16:58:15 UTC (rev 9916)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-02-06 17:11:38 UTC (rev 9917)
@@ -995,8 +995,9 @@
                               "expected a return code of 0 or 1. Exiting\n")
                     break
                 else:
+                    s=os.stat(self.launch_file)[os.path.stat.ST_SIZE]
                     out.write(line_header()+
-                              "renew the launch file "+self.launch_file+". The old version had a size of \n")
+                              "renew the launch file "+self.launch_file+". The old version had a size of "+str(s)+"\n")
                     out.flush()
                     launch_tmp_file=self.launch_file+".tmp"
                     fd=open(launch_tmp_file,'w')
@@ -1006,7 +1007,7 @@
                     os.rename(launch_tmp_file, self.launch_file)
                     s=os.stat(self.launch_file)[os.path.stat.ST_SIZE]
                     out.write(line_header()+
-                              "generated "+len(kerb_vars)+" kerberos variables. The file size is "+str(s)+"\n")
+                              "generated "+str(len(kerb_vars))+" kerberos variables. The file size is "+str(s)+"\n")
                 out.flush()
                 #we do this as in some case(with dagman) the log file can 
                 #take a few second to be created. So we don't loop too fast



From nouiz at mail.berlios.de  Fri Feb  6 18:14:32 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 6 Feb 2009 18:14:32 +0100
Subject: [Plearn-commits] r9918 - trunk/python_modules/plearn/parallel
Message-ID: <200902061714.n16HEWAd020829@sheep.berlios.de>

Author: nouiz
Date: 2009-02-06 18:14:32 +0100 (Fri, 06 Feb 2009)
New Revision: 9918

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
don't print duplicated info.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-02-06 17:11:38 UTC (rev 9917)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-02-06 17:14:32 UTC (rev 9918)
@@ -997,7 +997,7 @@
                 else:
                     s=os.stat(self.launch_file)[os.path.stat.ST_SIZE]
                     out.write(line_header()+
-                              "renew the launch file "+self.launch_file+". The old version had a size of "+str(s)+"\n")
+                              "renew the launch file. The old version had a size of "+str(s)+"\n")
                     out.flush()
                     launch_tmp_file=self.launch_file+".tmp"
                     fd=open(launch_tmp_file,'w')



From nouiz at mail.berlios.de  Fri Feb  6 20:25:01 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 6 Feb 2009 20:25:01 +0100
Subject: [Plearn-commits] r9919 - trunk/plearn_learners/meta
Message-ID: <200902061925.n16JP1oN025367@sheep.berlios.de>

Author: nouiz
Date: 2009-02-06 20:25:00 +0100 (Fri, 06 Feb 2009)
New Revision: 9919

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
Log:
fix the timing of the test.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-02-06 17:14:32 UTC (rev 9918)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-02-06 19:25:00 UTC (rev 9919)
@@ -630,6 +630,7 @@
         PLCHECK(last_stage<=stage);
         inherited::test(testset,test_stats,testoutputs,testcosts);
         timer->stopTimer("MultiClassAdaBoost::test() current");
+        timer->stopTimer("MultiClassAdaBoost::test()");
         Profiler::pl_profile_end("MultiClassAdaBoost::test()");
         time_sum += timer->getTimer("MultiClassAdaBoost::test() current");
         last_stage=stage;
@@ -780,6 +781,7 @@
     }
     Profiler::pl_profile_end("MultiClassAdaBoost::test() test_stats");
     timer->stopTimer("MultiClassAdaBoost::test() current");
+    timer->stopTimer("MultiClassAdaBoost::test()");
     Profiler::pl_profile_end("MultiClassAdaBoost::test()");
     
     time_sum_rtr +=timer->getTimer("MultiClassAdaBoost::test() current");



From nouiz at mail.berlios.de  Fri Feb  6 21:19:01 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 6 Feb 2009 21:19:01 +0100
Subject: [Plearn-commits] r9920 - trunk/plearn/io
Message-ID: <200902062019.n16KJ1AA032595@sheep.berlios.de>

Author: nouiz
Date: 2009-02-06 21:19:01 +0100 (Fri, 06 Feb 2009)
New Revision: 9920

Modified:
   trunk/plearn/io/fileutils.cc
Log:
removed a useless gcc 4.3 warning.


Modified: trunk/plearn/io/fileutils.cc
===================================================================
--- trunk/plearn/io/fileutils.cc	2009-02-06 19:25:00 UTC (rev 9919)
+++ trunk/plearn/io/fileutils.cc	2009-02-06 20:19:01 UTC (rev 9920)
@@ -401,12 +401,13 @@
 /////////////
 PRStatus mvforce(const PPath& source, const PPath& destination, bool fail_on_error)
 {
-     if(PR_Access(destination.c_str(), PR_ACCESS_EXISTS)==PR_SUCCESS)
+    if(PR_Access(destination.c_str(), PR_ACCESS_EXISTS)==PR_SUCCESS){
          if(PR_Delete(destination.c_str())!=PR_SUCCESS)
              if(fail_on_error)
                  PLERROR("In mvforce(%s,%s) - we failed to delete the destination!",source.c_str(),destination.c_str());
              else
                  return PR_FAILURE;
+    }
      return mv(source,destination);
 }
 



From nouiz at mail.berlios.de  Mon Feb  9 23:06:09 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 9 Feb 2009 23:06:09 +0100
Subject: [Plearn-commits] r9921 - trunk/plearn/io
Message-ID: <200902092206.n19M69rC029682@sheep.berlios.de>

Author: nouiz
Date: 2009-02-09 23:06:08 +0100 (Mon, 09 Feb 2009)
New Revision: 9921

Modified:
   trunk/plearn/io/fileutils.cc
Log:
added {} to remove useless gcc 4.3 warning.


Modified: trunk/plearn/io/fileutils.cc
===================================================================
--- trunk/plearn/io/fileutils.cc	2009-02-06 20:19:01 UTC (rev 9920)
+++ trunk/plearn/io/fileutils.cc	2009-02-09 22:06:08 UTC (rev 9921)
@@ -402,11 +402,12 @@
 PRStatus mvforce(const PPath& source, const PPath& destination, bool fail_on_error)
 {
     if(PR_Access(destination.c_str(), PR_ACCESS_EXISTS)==PR_SUCCESS){
-         if(PR_Delete(destination.c_str())!=PR_SUCCESS)
+        if(PR_Delete(destination.c_str())!=PR_SUCCESS){
              if(fail_on_error)
                  PLERROR("In mvforce(%s,%s) - we failed to delete the destination!",source.c_str(),destination.c_str());
              else
                  return PR_FAILURE;
+        }
     }
      return mv(source,destination);
 }



From nouiz at mail.berlios.de  Mon Feb  9 23:06:31 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 9 Feb 2009 23:06:31 +0100
Subject: [Plearn-commits] r9922 - trunk/plearn_learners/hyper
Message-ID: <200902092206.n19M6VnA029750@sheep.berlios.de>

Author: nouiz
Date: 2009-02-09 23:06:31 +0100 (Mon, 09 Feb 2009)
New Revision: 9922

Modified:
   trunk/plearn_learners/hyper/HyperLearner.cc
   trunk/plearn_learners/hyper/HyperLearner.h
Log:
implemented HyperLearner::finalize()


Modified: trunk/plearn_learners/hyper/HyperLearner.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.cc	2009-02-09 22:06:08 UTC (rev 9921)
+++ trunk/plearn_learners/hyper/HyperLearner.cc	2009-02-09 22:06:31 UTC (rev 9922)
@@ -255,6 +255,9 @@
     }
 }
 
+////////////
+// forget //
+////////////
 void HyperLearner::forget()
 {
     learner_->forget();
@@ -265,6 +268,15 @@
         strategy[i]->forget();
 }
 
+//////////////
+// finalize //
+//////////////
+void HyperLearner::finalize()
+{
+    inherited::finalize();
+    learner_->finalize();
+}
+
 ////////////
 // build_ //
 ////////////

Modified: trunk/plearn_learners/hyper/HyperLearner.h
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.h	2009-02-09 22:06:08 UTC (rev 9921)
+++ trunk/plearn_learners/hyper/HyperLearner.h	2009-02-09 22:06:31 UTC (rev 9922)
@@ -99,6 +99,7 @@
     virtual void train();
 
     virtual void forget();
+    virtual void finalize();
 
     //! Returns the getResultNames() of its last strategy command
     TVec<string> getTrainCostNames() const;



From nouiz at mail.berlios.de  Tue Feb 10 15:12:36 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 10 Feb 2009 15:12:36 +0100
Subject: [Plearn-commits] r9923 - trunk/plearn_learners/meta
Message-ID: <200902101412.n1AECa8S002391@sheep.berlios.de>

Author: nouiz
Date: 2009-02-10 15:12:35 +0100 (Tue, 10 Feb 2009)
New Revision: 9923

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
when we reload a learner, it is possible that we finalize a learner without having set the train_set.


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2009-02-09 22:06:31 UTC (rev 9922)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2009-02-10 14:12:35 UTC (rev 9923)
@@ -319,7 +319,7 @@
     for(int i=0;i<weak_learners.size();i++){
         weak_learners[i]->finalize();
     }
-    if(train_set->classname()=="RegressionTreeRegisters")
+    if(train_set && train_set->classname()=="RegressionTreeRegisters")
         ((PP<RegressionTreeRegisters>)train_set)->finalize();
 }
 



From laulysta at mail.berlios.de  Tue Feb 10 19:13:14 2009
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Tue, 10 Feb 2009 19:13:14 +0100
Subject: [Plearn-commits] r9924 - trunk/plearn_learners_experimental
Message-ID: <200902101813.n1AIDEBQ006487@sheep.berlios.de>

Author: laulysta
Date: 2009-02-10 19:13:13 +0100 (Tue, 10 Feb 2009)
New Revision: 9924

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
Log:
resolved



Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-02-10 14:12:35 UTC (rev 9923)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-02-10 18:13:13 UTC (rev 9924)
@@ -410,6 +410,7 @@
 
         if( dynamic_reconstruction_connections )
         {
+
             dynamic_reconstruction_connections->down_size = hidden_layer->size;
             dynamic_reconstruction_connections->up_size = hidden_layer->size;
             if( !dynamic_reconstruction_connections->random_gen )
@@ -418,6 +419,9 @@
                 dynamic_reconstruction_connections->forget();
             }
             dynamic_reconstruction_connections->build();
+            RBMMatrixConnection* conn = dynamic_cast<RBMMatrixConnection*>((RBMConnection*)dynamic_connections);
+            RBMMatrixConnection* conn_rec = dynamic_cast<RBMMatrixConnection*>((RBMConnection*)dynamic_reconstruction_connections);
+            conn_rec->weights << conn->weights;
         }
 
         if( hidden_layer2 )
@@ -503,6 +507,7 @@
     deepCopyField( acc_target_connections_gr, copies);
     deepCopyField( acc_input_connections_gr, copies);
     deepCopyField( acc_dynamic_connections_gr, copies);
+    deepCopyField( acc_reconstruction_dynamic_connections_gr, copies);
     deepCopyField( acc_target_bias_gr, copies);
     deepCopyField( acc_hidden_bias_gr, copies);
     deepCopyField( acc_recons_bias_gr, copies);
@@ -1231,8 +1236,8 @@
     reconstruction_prob.resize(fullhiddenlength);
 
     // predict (denoised) input_reconstruction 
-    transposeProduct(reconstruction_activation, reconstruction_weights, hidden);
-    //product(reconstruction_activation, reconstruction_weights, hidden);
+    //transposeProduct(reconstruction_activation, reconstruction_weights, hidden); //dynamic matrice tied
+    product(reconstruction_activation, reconstruction_weights, hidden); //dynamic matrice not tied
     reconstruction_activation += reconstruction_bias;
 
     for( int j=0 ; j<fullhiddenlength ; j++ )
@@ -1241,30 +1246,28 @@
     //hidden_layer->fprop(reconstruction_activation, reconstruction_prob);
 
     /********************************************************************************/
-    // Vec hidden_reconstruction_activation_grad;
     hidden_reconstruction_activation_grad.resize(reconstruction_prob.size());
     hidden_reconstruction_activation_grad << reconstruction_prob;
     hidden_reconstruction_activation_grad -= hidden_target;
     hidden_reconstruction_activation_grad *= hidden_reconstruction_cost_weight;
+    
 
-
-    productAcc(hidden_gradient, reconstruction_weights, hidden_reconstruction_activation_grad);
-    //transposeProductAcc(hidden_gradient, reconstruction_weights, hidden_reconstruction_activation_grad);
+    //productAcc(hidden_gradient, reconstruction_weights, hidden_reconstruction_activation_grad); //dynamic matrice tied
+    transposeProductAcc(hidden_gradient, reconstruction_weights, hidden_reconstruction_activation_grad); //dynamic matrice not tied
     
     //update bias
-    //multiplyAcc(reconstruction_bias, hidden_reconstruction_activation_grad, -lr);
+    multiplyAcc(reconstruction_bias, hidden_reconstruction_activation_grad, -lr);
     // update weight
-    //externalProductScaleAcc(reconstruction_weights, hidden_reconstruction_activation_grad, hidden, -lr);
+    //externalProductScaleAcc(acc_weights_gr, hidden, hidden_reconstruction_activation_grad, -lr); //dynamic matrice tied
+    externalProductScaleAcc(acc_weights_gr, hidden_reconstruction_activation_grad, hidden, -lr); //dynamic matrice not tied
                 
-
-    // update weight
-    //externalProductScaleAcc(acc_weights_gr, hidden, hidden_reconstruction_activation_grad, -lr);
     
     /********************************************************************************/
+    // Vec hidden_reconstruction_activation_grad;
+    //hidden_reconstruction_activation_grad.clear();
+    //for(int k=0; k<reconstruction_prob.length(); k++)
+    //    hidden_reconstruction_activation_grad[k] = safelog(reconstruction_prob[k]) - safelog(1-reconstruction_prob[k]);
     
-    hidden_reconstruction_activation_grad.clear();
-    for(int k=0; k<reconstruction_prob.length(); k++)
-        hidden_reconstruction_activation_grad[k] = safelog(reconstruction_prob[k]) - safelog(1-reconstruction_prob[k]);
 
     double result_cost = 0;
     double neg_log_cost = 0; // neg log softmax
@@ -1477,6 +1480,8 @@
     { 
         acc_dynamic_connections_gr.resize(dynamic_connections->up_size, dynamic_connections->down_size);
         acc_dynamic_connections_gr.clear();
+        acc_reconstruction_dynamic_connections_gr.resize(dynamic_connections->down_size, dynamic_connections->up_size);
+        acc_reconstruction_dynamic_connections_gr.clear();
     }
 
 
@@ -1574,7 +1579,8 @@
 
                 //truc stan
                 //fpropHiddenSymmetricDynamicMatrix(hidden_list(i-1), reconstruction_weights, hidden_reconstruction_prob, hidden_list(i), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
-                fpropHiddenReconstructionFromLastHidden(hidden_list(i), dynamicWeights, acc_dynamic_connections_gr, hidden_reconstruction_bias, hidden_reconstruction_activation_grad, hidden_reconstruction_prob, hidden_list(i-1), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
+                //fpropHiddenReconstructionFromLastHidden(hidden_list(i), dynamicWeights, acc_dynamic_connections_gr, hidden_reconstruction_bias, hidden_reconstruction_activation_grad, hidden_reconstruction_prob, hidden_list(i-1), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
+                fpropHiddenReconstructionFromLastHidden(hidden_list(i), reconsWeights, acc_reconstruction_dynamic_connections_gr, hidden_reconstruction_bias, hidden_reconstruction_activation_grad, hidden_reconstruction_prob, hidden_list(i-1), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
                 
             }
             
@@ -1627,7 +1633,7 @@
                                   false);
                 
             hidden_temporal_gradient << hidden_gradient;  
-            hidden_temporal_gradient +=  hidden_reconstruction_activation_grad;
+            //hidden_temporal_gradient +=  hidden_reconstruction_activation_grad;
         }
         else
         {
@@ -1658,7 +1664,10 @@
     }
     multiplyAcc(inputWeights, acc_input_connections_gr, 1);
     if(dynamic_connections )
+    {
         multiplyAcc(dynamicWeights, acc_dynamic_connections_gr, 1);
+        multiplyAcc(reconsWeights, acc_reconstruction_dynamic_connections_gr, 1);
+    }
 }
 
 



From nouiz at mail.berlios.de  Tue Feb 10 22:34:53 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 10 Feb 2009 22:34:53 +0100
Subject: [Plearn-commits] r9925 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200902102134.n1ALYrCS010513@sheep.berlios.de>

Author: nouiz
Date: 2009-02-10 22:34:53 +0100 (Tue, 10 Feb 2009)
New Revision: 9925

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
-added the dbidispatch option --keep_failed_jobs_in_queue for condor backend
-restructured to remove duplicate code.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-02-10 18:13:13 UTC (rev 9924)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-02-10 21:34:53 UTC (rev 9925)
@@ -797,6 +797,7 @@
         self.machine = []
         self.machines = []
         self.to_all = False
+        self.keep_failed_jobs_in_queue = False
 
         DBIBase.__init__(self, commands, **args)
 
@@ -1132,47 +1133,54 @@
 
             os.chmod(launch_tmp_file, 0755)
             os.rename(launch_tmp_file, self.launch_file)
-
-    def run_dag(self):
-        if self.to_all:
-            raise DBIError("[DBI] ERROR: condor backend don't support the option --to_all and a maximum number of process")
-        condor_submit_fd = open( self.condor_submit_file, 'w' )
-
-        self.log_file = os.path.join("/tmp/bastienf/dbidispatch",self.log_dir)
-        os.system('mkdir -p ' + self.log_file)
-        self.log_file = os.path.join(self.log_file,"condor.log")
-
-        condor_submit_fd.write( dedent('''\
+    def print_common_condor_submit(self, fd, output, error, arguments=None):
+        fd.write( dedent('''\
                 executable     = %s
                 universe       = %s
                 requirements   = %s
-                output         = $(stdout)
-                error          = $(stderr)
+                output         = %s
+                error          = %s
                 log            = %s
                 getenv         = %s
                 nice_user      = %s
-                arguments      = $(args)
                 ''' % (self.launch_file, self.universe, self.req,
+                       output,
+                       error,
                        self.log_file,str(self.getenv),str(self.nice))))
+        if arguments:
+            fd.write('arguments      = '+arguments+'\n')
+        if self.keep_failed_jobs_in_queue:
+            fd.write('leave_in_queue = (ExitCode!=0)\n')
         if self.mem>0:
             #condor need value in Kb
-            condor_submit_fd.write('ImageSize      = %d\n'%(self.mem))
+            fd.write('ImageSize      = %d\n'%(self.mem))
 
         if self.files: #ON_EXIT_OR_EVICT
-            condor_submit_fd.write( dedent('''\
+            fd.write( dedent('''\
                 when_to_transfer_output = ON_EXIT
                 should_transfer_files   = Yes
                 transfer_input_files    = %s
                 '''%(self.files+','+self.launch_file+','+self.tasks[0].commands[0].split()[0]))) # no directory
         if self.env:
-            condor_submit_fd.write('environment    = '+self.env+'\n')
+            fd.write('environment    = '+self.env+'\n')
         if self.raw:
-            condor_submit_fd.write( self.raw+'\n')
+            fd.write( self.raw+'\n')
         if self.rank:
-            condor_submit_fd.write( dedent('''\
+            fd.write( dedent('''\
                 rank = %s
                 ''' %(self.rank)))
 
+        
+    def run_dag(self):
+        if self.to_all:
+            raise DBIError("[DBI] ERROR: condor backend don't support the option --to_all and a maximum number of process")
+        condor_submit_fd = open( self.condor_submit_file, 'w' )
+
+        self.log_file = os.path.join("/tmp/bastienf/dbidispatch",self.log_dir)
+        os.system('mkdir -p ' + self.log_file)
+        self.log_file = os.path.join(self.log_file,"condor.log")
+        self.print_common_condor_submit(condor_submit_fd, "$(stdout)", "$(stderr)","$(args)")
+
         condor_submit_fd.write("\nqueue\n")
         condor_submit_fd.close()
 
@@ -1227,47 +1235,16 @@
 
 
         condor_submit_fd = open( self.condor_submit_file, 'w' )
-
         self.log_file= os.path.join(self.log_dir,"condor.log")
+        self.print_common_condor_submit(condor_submit_fd, self.log_dir+"/$(Process).out", self.log_dir+"/$(Process).error")
 
-        condor_submit_fd.write( dedent('''\
-                executable     = %s
-                universe       = %s
-                requirements   = %s
-                output         = %s/$(Process).out
-                error          = %s/$(Process).error
-                log            = %s
-                getenv         = %s
-                nice_user      = %s
-                ''' % (self.launch_file, self.universe, self.req,
-                       self.log_dir,
-                       self.log_dir,
-                       self.log_file,str(self.getenv),str(self.nice))))
         if self.pkdilly:
             condor_submit_fd.write(dedent("""
-            stream_error = True
-            stream_output = True
-            transfer_executable = True
+            stream_error            = True
+            stream_output           = True
+            transfer_executable     = True
             when_to_transfer_output = ON_EXIT
             """))
-        if self.mem>0:
-            #condor need value in Kb
-            condor_submit_fd.write('ImageSize      = %d\n'%(self.mem))
-
-        if self.files: #ON_EXIT_OR_EVICT
-            condor_submit_fd.write( dedent('''\
-                when_to_transfer_output = ON_EXIT
-                should_transfer_files   = Yes
-                transfer_input_files    = %s
-                '''%(self.files+','+self.launch_file+','+self.tasks[0].commands[0].split()[0]))) # no directory
-        if self.env:
-            condor_submit_fd.write('environment    = '+self.env+'\n')
-        if self.raw:
-            condor_submit_fd.write( self.raw+'\n')
-        if self.rank:
-            condor_submit_fd.write( dedent('''\
-                rank = %s
-                ''' %(self.rank)))
         if len(condor_datas)!=0:
             for i in condor_datas:
                 condor_submit_fd.write("arguments      = sh "+i+" $$(Arch) \nqueue\n")

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-02-10 18:13:13 UTC (rev 9924)
+++ trunk/scripts/dbidispatch	2009-02-10 21:34:53 UTC (rev 9925)
@@ -31,6 +31,8 @@
                               [--universe={vanilla*, standard, grid, java,
                                            scheduler, local, parallel, vm}]
                               [--machine=HOSTNAME+] [--machines=regex+]
+                              [--[*no_]keep_failed_jobs_in_queue]
+
 An * after '[', '{' or ',' signals the default value.
 An + tell that we can put one or more separeted by a comma
 '''
@@ -187,6 +189,9 @@
        universe to test your script as this make the all the jobs start 
        immediately without being preempted on the local host. Take care to 
        don't send too much jobs.
+  The '--[no_]keep_failed_jobs_in_queue' option will cause the jobs to stay 
+      in the queue in completed status if it failed. You should do condor_rm 
+      after to remove it from the queue.
 
 where <command-template> is interpreted as follows: the first argument
 is the <command> above, and the rest are interpreted as <arguments>.
@@ -298,12 +303,12 @@
     elif argv in  ["--force", "--interruptible", "--long", 
                    "--getenv", "--cwait", "--clean_up" ,"--nice",
                    "--set_special_env", "--abs_path", "--pkdilly", "--to_all",
-                   "--m32G"]:
+                   "--m32G", "--keep_failed_jobs_in_queue"]:
         dbi_param[argv[2:]]=True
     elif argv in ["--no_force", "--no_interruptible", "--no_long",
                   "--no_getenv", "--no_cwait", "--no_clean_up" , "--no_nice",
                   "--no_set_special_env", "--no_abs_path", "--no_pkdilly",
-                  "--no_m32G"]:
+                  "--no_m32G", "--no_keep_failed_jobs_in_queue"]:
         dbi_param[argv[5:]]=False
     elif argv=="--testdbi":
         dbi_param["test"]=True
@@ -370,7 +375,7 @@
 elif launch_cmd=="Condor":
     valid_dbi_param +=["req", "arch", "getenv", "nice", "files", "rank", "env",
                        "raw", "os", "set_special_env", "mem", "cpu", "pkdilly",
-                       "universe", "machine", "machines", "to_all"]
+                       "universe", "machine", "machines", "to_all", "keep_failed_jobs_in_queue"]
 elif launch_cmd=="Bqtools":
     valid_dbi_param +=["cpu", "duree", "long", "mem", "micro",
                        "nano", "queue", "raw", "submit_options", "jobs_name" ]



From laulysta at mail.berlios.de  Wed Feb 11 00:37:53 2009
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Wed, 11 Feb 2009 00:37:53 +0100
Subject: [Plearn-commits] r9926 - trunk/plearn_learners_experimental
Message-ID: <200902102337.n1ANbrgC000410@sheep.berlios.de>

Author: laulysta
Date: 2009-02-11 00:37:52 +0100 (Wed, 11 Feb 2009)
New Revision: 9926

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
Log:
test reconstruction


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-02-10 21:34:53 UTC (rev 9925)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-02-10 23:37:52 UTC (rev 9926)
@@ -419,9 +419,7 @@
                 dynamic_reconstruction_connections->forget();
             }
             dynamic_reconstruction_connections->build();
-            RBMMatrixConnection* conn = dynamic_cast<RBMMatrixConnection*>((RBMConnection*)dynamic_connections);
-            RBMMatrixConnection* conn_rec = dynamic_cast<RBMMatrixConnection*>((RBMConnection*)dynamic_reconstruction_connections);
-            conn_rec->weights << conn->weights;
+            
         }
 
         if( hidden_layer2 )
@@ -695,7 +693,7 @@
             pb = new ProgressBar( "Recurrent training phase of "+classname(),
                                   end_stage - init_stage );
 
-        int nCost = 0;
+        int nCost = 2;
         train_costs.resize(train_costs.length() + nCost);
         train_n_items.resize(train_n_items.length() + nCost);
         while(stage < end_stage)
@@ -720,33 +718,35 @@
                 // recurrent no noise phase
                 if(recurrent_lr!=0)
                 {
+                    
                     if(corrupt_input) // need to recover the clean sequence                        
                         encoded_seq << clean_encoded_seq;                  
                     setLearningRate( recurrent_lr );                    
                     recurrentFprop(train_costs, train_n_items);
-                    recurrentUpdate(0,0,1, prediction_cost_weight );
+                    recurrentUpdate(0,0,1, prediction_cost_weight, train_costs, train_n_items );
+                    
                 }
-                
-                // greedy phase input
-                if(input_reconstruction_lr!=0){
-                    setLearningRate( input_reconstruction_lr );
-                    recurrentFprop(train_costs, train_n_items);
-                    recurrentUpdate(input_reconstruction_cost_weight, 0, 0, prediction_cost_weight );
+                if(stage<2){
+                    // greedy phase input
+                    if(input_reconstruction_lr!=0){
+                        setLearningRate( input_reconstruction_lr );
+                        recurrentFprop(train_costs, train_n_items);
+                        recurrentUpdate(input_reconstruction_cost_weight, 0, 1, 0, train_costs, train_n_items );
+                    }
+                    
+                    // greedy phase hidden
+                    if(hidden_reconstruction_lr!=0){
+                        setLearningRate( dynamic_gradient_scale_factor*hidden_reconstruction_lr);
+                        recurrentFprop(train_costs, train_n_items);
+                        recurrentUpdate(0, hidden_reconstruction_cost_weight, 1, 0, train_costs, train_n_items );
+                    }
                 }
-                
-                // greedy phase hidden
-                if(hidden_reconstruction_lr!=0){
-                    setLearningRate( dynamic_gradient_scale_factor*hidden_reconstruction_lr);
-                    recurrentFprop(train_costs, train_n_items);
-                    recurrentUpdate(0, hidden_reconstruction_cost_weight, 1, 0 );
-                }
-
                 // recurrent noisy phase
                 if(noisy_recurrent_lr!=0)
                 {
                     setLearningRate( noisy_recurrent_lr );
                     recurrentFprop(train_costs, train_n_items);
-                    recurrentUpdate(input_reconstruction_cost_weight, hidden_reconstruction_cost_weight, 1, prediction_cost_weight );
+                    recurrentUpdate(input_reconstruction_cost_weight, hidden_reconstruction_cost_weight, 1, prediction_cost_weight, train_costs, train_n_items );
                 }
 
                 
@@ -766,14 +766,14 @@
                     else
                         train_costs[i] = MISSING_VALUE;
                 }
-                /*
+                
                 if (i == train_costs.length()-nCost ){
                     train_costs[i] /= train_n_items[i];
-                    totalCosts += train_costs[i]*input_reconstruction_cost_weight;
+                    //totalCosts += train_costs[i]*input_reconstruction_cost_weight;
                 }
                 else if (i == train_costs.length()-1)
-                    train_costs[i] = totalCosts;
-                */
+                    train_costs[i] /= train_n_items[i];
+                
             }
 
             if(verbosity>0)
@@ -1144,11 +1144,11 @@
     //applyBiasDecay();
 }
 
-double DenoisingRecurrentNet::fpropUpdateInputReconstructionFromHidden(Vec hidden, Mat& reconstruction_weights, Vec& input_reconstruction_bias, Vec& input_reconstruction_prob, 
+double DenoisingRecurrentNet::fpropUpdateInputReconstructionFromHidden(Vec hidden, Mat& reconstruction_weights, Mat& acc_weights_gr, Vec& input_reconstruction_bias, Vec& input_reconstruction_prob, 
                                                                        Vec clean_input, Vec hidden_gradient, double input_reconstruction_cost_weight, double lr)
 {
     double cost = fpropInputReconstructionFromHidden(hidden, reconstruction_weights, input_reconstruction_bias, input_reconstruction_prob, clean_input);
-    updateInputReconstructionFromHidden(hidden, reconstruction_weights, input_reconstruction_bias, input_reconstruction_prob, 
+    updateInputReconstructionFromHidden(hidden, reconstruction_weights, acc_weights_gr, input_reconstruction_bias, input_reconstruction_prob, 
                                         clean_input, hidden_gradient, input_reconstruction_cost_weight, lr);
     return cost;
 }
@@ -1198,7 +1198,7 @@
 
 //! Backpropagates reconstruction cost (after comparison with clean_input) with learning rate input_reconstruction_lr
 //! accumulates gradient in hidden_gradient, and updates reconstruction_weights and input_reconstruction_bias
-void DenoisingRecurrentNet::updateInputReconstructionFromHidden(Vec hidden, Mat& reconstruction_weights, Vec& input_reconstruction_bias, Vec input_reconstruction_prob, 
+void DenoisingRecurrentNet::updateInputReconstructionFromHidden(Vec hidden, Mat& reconstruction_weights, Mat& acc_weights_gr, Vec& input_reconstruction_bias, Vec input_reconstruction_prob, 
                                                                 Vec clean_input, Vec hidden_gradient, double input_reconstruction_cost_weight, double lr)
 {
     // gradient of -log softmax is just  output_of_softmax - onehot_target
@@ -1214,7 +1214,7 @@
     // THIS IS COMMENTED OUT BECAUSE THE reconstruction_weights ARE tied (same) TO THE input_connection weights, 
     // WHICH GET UPDATED LATER IN recurrentUpdate SO IF WE UPDATE THEM HERE THEY WOULD GET UPDATED TWICE.
     // WARNING: THIS WOULD NO LONGER BE THE CASE IF THEY WERE NOT TIED!
-    // externalProductScaleAcc(reconstruction_weights, hidden, input_reconstruction_activation_grad, -lr);
+    externalProductScaleAcc(acc_weights_gr, hidden, input_reconstruction_activation_grad, -lr);
 
     // accumulate in hidden_gradient
     productAcc(hidden_gradient, reconstruction_weights, input_reconstruction_activation_grad);
@@ -1236,8 +1236,8 @@
     reconstruction_prob.resize(fullhiddenlength);
 
     // predict (denoised) input_reconstruction 
-    //transposeProduct(reconstruction_activation, reconstruction_weights, hidden); //dynamic matrice tied
-    product(reconstruction_activation, reconstruction_weights, hidden); //dynamic matrice not tied
+    transposeProduct(reconstruction_activation, reconstruction_weights, hidden); //dynamic matrice tied
+    //product(reconstruction_activation, reconstruction_weights, hidden); //dynamic matrice not tied
     reconstruction_activation += reconstruction_bias;
 
     for( int j=0 ; j<fullhiddenlength ; j++ )
@@ -1252,14 +1252,14 @@
     hidden_reconstruction_activation_grad *= hidden_reconstruction_cost_weight;
     
 
-    //productAcc(hidden_gradient, reconstruction_weights, hidden_reconstruction_activation_grad); //dynamic matrice tied
-    transposeProductAcc(hidden_gradient, reconstruction_weights, hidden_reconstruction_activation_grad); //dynamic matrice not tied
+    productAcc(hidden_gradient, reconstruction_weights, hidden_reconstruction_activation_grad); //dynamic matrice tied
+    //transposeProductAcc(hidden_gradient, reconstruction_weights, hidden_reconstruction_activation_grad); //dynamic matrice not tied
     
     //update bias
     multiplyAcc(reconstruction_bias, hidden_reconstruction_activation_grad, -lr);
     // update weight
-    //externalProductScaleAcc(acc_weights_gr, hidden, hidden_reconstruction_activation_grad, -lr); //dynamic matrice tied
-    externalProductScaleAcc(acc_weights_gr, hidden_reconstruction_activation_grad, hidden, -lr); //dynamic matrice not tied
+    externalProductScaleAcc(acc_weights_gr, hidden, hidden_reconstruction_activation_grad, -lr); //dynamic matrice tied
+    //externalProductScaleAcc(acc_weights_gr, hidden_reconstruction_activation_grad, hidden, -lr); //dynamic matrice not tied
                 
     
     /********************************************************************************/
@@ -1451,7 +1451,9 @@
 void DenoisingRecurrentNet::recurrentUpdate(real input_reconstruction_weight,
                                             real hidden_reconstruction_weight,
                                             real temporal_gradient_contribution,
-                                            real predic_cost_weight)
+                                            real predic_cost_weight,
+                                            Vec train_costs, 
+                                            Vec train_n_items )
 {
     TVec < Mat> targetWeights ;
     Mat inputWeights;
@@ -1553,109 +1555,111 @@
                     hidden_gradient, bias_gradient);
             }
         }
-            
-        // Add contribution of input reconstruction cost in hidden_gradient
-        if(input_reconstruction_weight!=0)
-        {
-            Mat reconstruction_weights = getInputConnectionsWeightMatrix();
-            Vec clean_input = clean_encoded_seq.subMatRows(i, input_window_size).toVec();
-
-            fpropUpdateInputReconstructionFromHidden(hidden_list(i), reconstruction_weights, input_reconstruction_bias, input_reconstruction_prob, 
-                                                     clean_input, hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
-        }
-
-
-        if(i!=0 && dynamic_connections )
-        {   
-            
-            // Add contribution of hidden reconstruction cost in hidden_gradient
-            Vec hidden_reconstruction_activation_grad;
-            hidden_reconstruction_activation_grad.resize(hidden_layer->size);
-            //Mat reconstruction_weights = getDynamicConnectionsWeightMatrix();
-            if(hidden_reconstruction_weight!=0)
+        else{   
+            // Add contribution of input reconstruction cost in hidden_gradient
+            if(input_reconstruction_weight!=0)
             {
-                //Vec hidden_reconstruction_activation_grad;
-                //Mat reconstruction_weights = getDynamicConnectionsWeightMatrix();
-
-                //truc stan
-                //fpropHiddenSymmetricDynamicMatrix(hidden_list(i-1), reconstruction_weights, hidden_reconstruction_prob, hidden_list(i), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
-                //fpropHiddenReconstructionFromLastHidden(hidden_list(i), dynamicWeights, acc_dynamic_connections_gr, hidden_reconstruction_bias, hidden_reconstruction_activation_grad, hidden_reconstruction_prob, hidden_list(i-1), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
-                fpropHiddenReconstructionFromLastHidden(hidden_list(i), reconsWeights, acc_reconstruction_dynamic_connections_gr, hidden_reconstruction_bias, hidden_reconstruction_activation_grad, hidden_reconstruction_prob, hidden_list(i-1), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
+                //Mat reconstruction_weights = getInputConnectionsWeightMatrix();
+                Vec clean_input = clean_encoded_seq.subMatRows(i, input_window_size).toVec();
                 
+                train_costs[4] += fpropUpdateInputReconstructionFromHidden(hidden_list(i), inputWeights, acc_input_connections_gr, input_reconstruction_bias, input_reconstruction_prob, 
+                                                                           clean_input, hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
+                train_n_items[4]++;
             }
             
-
-            // add contribution to gradient of next time step hidden layer
-            if(temporal_gradient_contribution>0)
-            { // add weighted contribution of hidden_temporal gradient to hidden_gradient
-                // It does this: hidden_gradient += temporal_gradient_contribution*hidden_temporal_gradient;
-                multiplyAcc(hidden_gradient, hidden_temporal_gradient, temporal_gradient_contribution);
+            
+            if(i!=0 && dynamic_connections )
+            {   
                 
+                // Add contribution of hidden reconstruction cost in hidden_gradient
+                Vec hidden_reconstruction_activation_grad;
+                hidden_reconstruction_activation_grad.resize(hidden_layer->size);
+                //Mat reconstruction_weights = getDynamicConnectionsWeightMatrix();
+                if(hidden_reconstruction_weight!=0)
+                {
+                    //Vec hidden_reconstruction_activation_grad;
+                    //Mat reconstruction_weights = getDynamicConnectionsWeightMatrix();
+                    
+                    //truc stan
+                    //fpropHiddenSymmetricDynamicMatrix(hidden_list(i-1), reconstruction_weights, hidden_reconstruction_prob, hidden_list(i), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
+                    train_costs[5] += fpropHiddenReconstructionFromLastHidden(hidden_list(i), dynamicWeights, acc_dynamic_connections_gr, hidden_reconstruction_bias, hidden_reconstruction_activation_grad, hidden_reconstruction_prob, hidden_list(i-1), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
+                    //fpropHiddenReconstructionFromLastHidden(hidden_list(i), reconsWeights, acc_reconstruction_dynamic_connections_gr, hidden_reconstruction_bias, hidden_reconstruction_activation_grad, hidden_reconstruction_prob, hidden_list(i-1), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
+                    train_n_items[5]++;
+                }
+                
+                
+                // add contribution to gradient of next time step hidden layer
+                if(temporal_gradient_contribution>0)
+                { // add weighted contribution of hidden_temporal gradient to hidden_gradient
+                    // It does this: hidden_gradient += temporal_gradient_contribution*hidden_temporal_gradient;
+                    multiplyAcc(hidden_gradient, hidden_temporal_gradient, temporal_gradient_contribution);
+                    
+                }
+                
+                bpropUpdateHiddenLayer(hidden_act_no_bias_list(i), 
+                                       hidden_list(i),
+                                       hidden_temporal_gradient, 
+                                       hidden_gradient,
+                                       hidden_layer->bias, 
+                                       hidden_layer->learning_rate );
+                //Dynamic
+                bpropUpdateConnection(hidden_list(i-1),
+                                      hidden_act_no_bias_list(i), // Here, it should be dynamic_act_no_bias_contribution, but doesn't matter because a RBMMatrixConnection::bpropUpdate doesn't use its second argument
+                                      hidden_gradient, 
+                                      hidden_temporal_gradient, 
+                                      dynamicWeights,
+                                      acc_dynamic_connections_gr,
+                                      dynamic_connections->down_size,
+                                      dynamic_connections->up_size,
+                                      dynamic_connections->learning_rate,
+                                      false);
+                
+                /*if(hidden_reconstruction_weight!=0)
+                  {
+                  // update bias
+                  multiplyAcc(hidden_reconstruction_bias, hidden_reconstruction_activation_grad, -current_learning_rate);
+                  // update weight
+                  externalProductScaleAcc(reconstruction_weights, hidden_list(i), hidden_reconstruction_activation_grad, -current_learning_rate);
+                  
+                  }*/
+                
+                //input
+                bpropUpdateConnection(input_list[i],
+                                      hidden_act_no_bias_list(i), 
+                                      visi_bias_gradient, 
+                                      hidden_temporal_gradient,// Here, it should be activations - cond_bias, but doesn't matter
+                                      inputWeights,
+                                      acc_input_connections_gr,
+                                      input_connections->down_size,
+                                      input_connections->up_size,
+                                      input_connections->learning_rate,
+                                      false);
+                
+                hidden_temporal_gradient << hidden_gradient;  
+                //hidden_temporal_gradient +=  hidden_reconstruction_activation_grad;
             }
-
-            bpropUpdateHiddenLayer(hidden_act_no_bias_list(i), 
-                                   hidden_list(i),
-                                   hidden_temporal_gradient, 
-                                   hidden_gradient,
-                                   hidden_layer->bias, 
-                                   hidden_layer->learning_rate );
-            //Dynamic
-            bpropUpdateConnection(hidden_list(i-1),
-                                  hidden_act_no_bias_list(i), // Here, it should be dynamic_act_no_bias_contribution, but doesn't matter because a RBMMatrixConnection::bpropUpdate doesn't use its second argument
-                                  hidden_gradient, 
-                                  hidden_temporal_gradient, 
-                                  dynamicWeights,
-                                  acc_dynamic_connections_gr,
-                                  dynamic_connections->down_size,
-                                  dynamic_connections->up_size,
-                                  dynamic_connections->learning_rate,
-                                  false);
-
-            /*if(hidden_reconstruction_weight!=0)
+            else
             {
-                // update bias
-                multiplyAcc(hidden_reconstruction_bias, hidden_reconstruction_activation_grad, -current_learning_rate);
-                // update weight
-                externalProductScaleAcc(reconstruction_weights, hidden_list(i), hidden_reconstruction_activation_grad, -current_learning_rate);
+                bpropUpdateHiddenLayer(hidden_act_no_bias_list(i), 
+                                       hidden_list(i),
+                                       hidden_temporal_gradient, // Not really temporal gradient, but this is the final iteration...
+                                       hidden_gradient,
+                                       hidden_layer->bias, 
+                                       hidden_layer->learning_rate );
                 
-                }*/
-
-            //input
-            bpropUpdateConnection(input_list[i],
-                                  hidden_act_no_bias_list(i), 
-                                  visi_bias_gradient, 
-                                  hidden_temporal_gradient,// Here, it should be activations - cond_bias, but doesn't matter
-                                  inputWeights,
-                                  acc_input_connections_gr,
-                                  input_connections->down_size,
-                                  input_connections->up_size,
-                                  input_connections->learning_rate,
-                                  false);
-                
-            hidden_temporal_gradient << hidden_gradient;  
-            //hidden_temporal_gradient +=  hidden_reconstruction_activation_grad;
+                //input
+                bpropUpdateConnection(input_list[i],
+                                      hidden_act_no_bias_list(i), 
+                                      visi_bias_gradient, 
+                                      hidden_temporal_gradient,// Here, it should be activations - cond_bias, but doesn't matter
+                                      inputWeights,
+                                      acc_input_connections_gr,
+                                      input_connections->down_size,
+                                      input_connections->up_size,
+                                      input_connections->learning_rate,
+                                      false);
+            }
         }
-        else
-        {
-            bpropUpdateHiddenLayer(hidden_act_no_bias_list(i), 
-                                   hidden_list(i),
-                                   hidden_temporal_gradient, // Not really temporal gradient, but this is the final iteration...
-                                   hidden_gradient,
-                                   hidden_layer->bias, 
-                                   hidden_layer->learning_rate );
-            
-            //input
-            bpropUpdateConnection(input_list[i],
-                                  hidden_act_no_bias_list(i), 
-                                  visi_bias_gradient, 
-                                  hidden_temporal_gradient,// Here, it should be activations - cond_bias, but doesn't matter
-                                  inputWeights,
-                                  acc_input_connections_gr,
-                                  input_connections->down_size,
-                                  input_connections->up_size,
-                                  input_connections->learning_rate,
-                                  false);
-        }
     }
     //update matrice's connections
     for( int tar=0; tar<target_layers.length(); tar++)

Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2009-02-10 21:34:53 UTC (rev 9925)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2009-02-10 23:37:52 UTC (rev 9926)
@@ -273,8 +273,10 @@
     //! after the visible units have been clamped
     void recurrentUpdate(real input_reconstruction_weight,
                          real hidden_reconstruction_cost_weight,
-                         real temporal_gradient_contribution = 1,
-                         real prediction_cost_weight = 1);
+                         real temporal_gradient_contribution,
+                         real prediction_cost_weight,
+                         Vec train_costs,
+                         Vec train_n_items);
 
     virtual void test(VMat testset, PP<VecStatsCollector> test_stats,
                       VMat testoutputs=0, VMat testcosts=0) const;
@@ -320,6 +322,8 @@
 
     mutable Mat acc_dynamic_connections_gr;
 
+    mutable Mat acc_reconstruction_dynamic_connections_gr;
+
     //! Stores accumulate target bias gradient
     mutable Vec acc_target_bias_gr;
 
@@ -456,7 +460,7 @@
     //! accumulates gradient in hidden_gradient, and updates reconstruction_weights and input_reconstruction_bias
     //! Also computes neg log cost and returns it
     
-    double fpropUpdateInputReconstructionFromHidden(Vec hidden, Mat& reconstruction_weights, Vec& input_reconstruction_bias, Vec& input_reconstruction_prob, 
+    double fpropUpdateInputReconstructionFromHidden(Vec hidden, Mat& reconstruction_weights, Mat& acc_weights_gr, Vec& input_reconstruction_bias, Vec& input_reconstruction_prob, 
                                                 Vec clean_input, Vec hidden_gradient, double input_reconstruction_cost_weight, double lr);
 
     
@@ -467,7 +471,7 @@
 
     //! Backpropagates reconstruction cost (after comparison with clean_input) with learning rate input_reconstruction_lr
     //! accumulates gradient in hidden_gradient, and updates reconstruction_weights and input_reconstruction_bias
-    void updateInputReconstructionFromHidden(Vec hidden, Mat& reconstruction_weights, Vec& input_reconstruction_bias, Vec input_reconstruction_prob, 
+    void updateInputReconstructionFromHidden(Vec hidden, Mat& reconstruction_weights, Mat& acc_weights_gr, Vec& input_reconstruction_bias, Vec input_reconstruction_prob, 
                                              Vec clean_input, Vec hidden_gradient, double input_reconstruction_cost_weight, double lr);
 
     double fpropHiddenReconstructionFromLastHidden(Vec hidden, Mat reconstruction_weights, Mat& acc_weights_gr, Vec& reconstruction_bias, Vec hidden_reconstruction_activation_grad, Vec& reconstruction_prob, 



From laulysta at mail.berlios.de  Wed Feb 11 06:34:55 2009
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Wed, 11 Feb 2009 06:34:55 +0100
Subject: [Plearn-commits] r9927 - trunk/plearn_learners_experimental
Message-ID: <200902110534.n1B5Yt1w022217@sheep.berlios.de>

Author: laulysta
Date: 2009-02-11 06:34:54 +0100 (Wed, 11 Feb 2009)
New Revision: 9927

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
Log:
reconstruction 


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-02-10 23:37:52 UTC (rev 9926)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-02-11 05:34:54 UTC (rev 9927)
@@ -87,7 +87,8 @@
     prediction_cost_weight(1),
     input_reconstruction_cost_weight(0),
     hidden_reconstruction_cost_weight(0),
-    current_learning_rate(0)
+    current_learning_rate(0),
+    nb_stage_reconstruction(0)
 {
     random_gen = new PRandom();
 }
@@ -259,6 +260,10 @@
                   OptionBase::learntoption,
                   "The training weight for the hidden reconstruction");
 
+    declareOption(ol, "nb_stage_reconstruction", &DenoisingRecurrentNet::nb_stage_reconstruction,
+                  OptionBase::learntoption,
+                  "The nomber of stage for de reconstructions");
+
  /*
     declareOption(ol, "", &DenoisingRecurrentNet::,
                   OptionBase::learntoption,
@@ -716,17 +721,19 @@
                     inject_zero_forcing_noise(encoded_seq, input_noise_prob);
 
                 // recurrent no noise phase
-                if(recurrent_lr!=0)
-                {
-                    
-                    if(corrupt_input) // need to recover the clean sequence                        
-                        encoded_seq << clean_encoded_seq;                  
-                    setLearningRate( recurrent_lr );                    
-                    recurrentFprop(train_costs, train_n_items);
-                    recurrentUpdate(0,0,1, prediction_cost_weight, train_costs, train_n_items );
-                    
+                if(stage>=nb_stage_reconstruction){
+                    if(recurrent_lr!=0)
+                    {
+                        
+                        if(corrupt_input) // need to recover the clean sequence                        
+                            encoded_seq << clean_encoded_seq;                  
+                        setLearningRate( recurrent_lr );                    
+                        recurrentFprop(train_costs, train_n_items);
+                        recurrentUpdate(0,0,1, prediction_cost_weight, train_costs, train_n_items );
+                        
+                    }
                 }
-                if(stage<2){
+                if(stage<nb_stage_reconstruction){
                     // greedy phase input
                     if(input_reconstruction_lr!=0){
                         setLearningRate( input_reconstruction_lr );

Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2009-02-10 23:37:52 UTC (rev 9926)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2009-02-11 05:34:54 UTC (rev 9927)
@@ -157,7 +157,7 @@
     double input_reconstruction_cost_weight;
     double hidden_reconstruction_cost_weight;
 
-
+    double nb_stage_reconstruction;
     //#####  Not Options  #####################################################
 
 



From nouiz at mail.berlios.de  Wed Feb 11 16:42:25 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 11 Feb 2009 16:42:25 +0100
Subject: [Plearn-commits] r9928 - trunk/plearn/io
Message-ID: <200902111542.n1BFgPTf009381@sheep.berlios.de>

Author: nouiz
Date: 2009-02-11 16:42:24 +0100 (Wed, 11 Feb 2009)
New Revision: 9928

Modified:
   trunk/plearn/io/load_and_save.h
Log:
now PLearn::save save in a tmp file that it move in all case.


Modified: trunk/plearn/io/load_and_save.h
===================================================================
--- trunk/plearn/io/load_and_save.h	2009-02-11 05:34:54 UTC (rev 9927)
+++ trunk/plearn/io/load_and_save.h	2009-02-11 15:42:24 UTC (rev 9928)
@@ -71,19 +71,12 @@
 inline void save(const PPath& filepath, const T& x, PStream::mode_t io_formatting=PStream::plearn_ascii, bool implicit_storage = true)
 { 
     force_mkdir_for_file(filepath);
-    PStream out = openFile( filepath, io_formatting, "w" );
-    out.implicit_storage = implicit_storage;
-    out << x;
-}
-
-
-//! We save in a tmp file, then we move it to the real file. 
-//! This help to don't have file partially saved.
-template<class T> 
-inline void tmpsave(const PPath& filepath, const T& x, PStream::mode_t io_formatting=PStream::plearn_ascii, bool implicit_storage = true)
-{ 
     PPath tmp_file=filepath+".plearn_tmpsave";
-    PLearn::save(tmp_file, x, io_formatting, implicit_storage);
+    {
+        PStream out = openFile( tmp_file, io_formatting, "w" );
+        out.implicit_storage = implicit_storage;
+        out << x;
+    }//to be sure out is closed.
     mvforce(tmp_file, filepath);
 }
 



From nouiz at mail.berlios.de  Wed Feb 11 17:16:38 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 11 Feb 2009 17:16:38 +0100
Subject: [Plearn-commits] r9929 - trunk/plearn/math
Message-ID: <200902111616.n1BGGcrv013600@sheep.berlios.de>

Author: nouiz
Date: 2009-02-11 17:16:38 +0100 (Wed, 11 Feb 2009)
New Revision: 9929

Modified:
   trunk/plearn/math/VecStatsCollector.cc
Log:
put a static var non-static to be more thread safe. We can do this as the fct that use the variable take much more time then the allocation.


Modified: trunk/plearn/math/VecStatsCollector.cc
===================================================================
--- trunk/plearn/math/VecStatsCollector.cc	2009-02-11 15:42:24 UTC (rev 9928)
+++ trunk/plearn/math/VecStatsCollector.cc	2009-02-11 16:16:38 UTC (rev 9929)
@@ -674,7 +674,8 @@
     // The first  case occurs when sum^i,j_k w(k) == 0.
     // The second case occurs when sum^i,j_k w(k) == sqrt(sum^i,j_k w(k)^2)
     //                                            == sum^{i or j}_k w(k)
-    static Vec meanvec;
+    Vec meanvec;
+
     PLASSERT( compute_covariance && cov.length() == cov.width() );
     int d = cov.length();
     getMean(meanvec);



From nouiz at mail.berlios.de  Wed Feb 11 20:25:42 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 11 Feb 2009 20:25:42 +0100
Subject: [Plearn-commits] r9930 - trunk/scripts
Message-ID: <200902111925.n1BJPgvH016606@sheep.berlios.de>

Author: nouiz
Date: 2009-02-11 20:25:42 +0100 (Wed, 11 Feb 2009)
New Revision: 9930

Modified:
   trunk/scripts/dbidispatch
Log:
added the dbidispatch option --restart. It will restard condor_jobs that is in the history.


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-02-11 16:16:38 UTC (rev 9929)
+++ trunk/scripts/dbidispatch	2009-02-11 19:25:42 UTC (rev 9930)
@@ -2,9 +2,10 @@
 import sys,os,re,time,datetime
 from plearn.utilities.toolkit import search_file
 from socket import gethostname
+from subprocess import Popen,PIPE
 
 ScriptName="launchdbi.py"
-ShortHelp='''Usage: dbidispatch [--help|-h] [--[*no_]dbilog] [--condor[=N]|--bqtools[=N]|--cluster[=N]|--local[=N]|--ssh[=N]] [--[*no_]test] [--[*no_]testdbi] [--[*no_]nb_proc=N] [--exp_dir=dir] <back-end parameter> {--file=FILEPATH | <command-template>}
+ShortHelp='''Usage: dbidispatch [--help|-h] [--[*no_]dbilog] [--condor[=N]|--bqtools[=N]|--cluster[=N]|--local[=N]|--ssh[=N]] [--[*no_]test] [--[*no_]testdbi] [--[*no_]nb_proc=N] [--exp_dir=dir] <back-end parameter> {--file=FILEPATH | <command-template>|--[*no_]restart_jobs condor_jobs_number... }
 
 <back-end parameter>:
     bqtools, cluster, condor option  : [--mem=N]
@@ -73,6 +74,7 @@
     option for dbidispatch. They can be overrided on the command line.
   The 'DBIDISPATCH_LOGDIR' environnement variable set the name of the directory
     where all the individual logs directory will be put. Default to LOGS.
+  The '--[*no_]restart_jobs' option work only for condor. The parameter following it should be condor jobs number. We will parse the history on the local host and relauchn those jobs. We only take the command line that was executed, all other option are the new one passed to dbidispatch.
 
 bqtools, cluster and condor option:
   The '--mem=X' speficify the number of ram in meg the program need to execute.
@@ -303,12 +305,12 @@
     elif argv in  ["--force", "--interruptible", "--long", 
                    "--getenv", "--cwait", "--clean_up" ,"--nice",
                    "--set_special_env", "--abs_path", "--pkdilly", "--to_all",
-                   "--m32G", "--keep_failed_jobs_in_queue"]:
+                   "--m32G", "--keep_failed_jobs_in_queue", "--restart"]:
         dbi_param[argv[2:]]=True
     elif argv in ["--no_force", "--no_interruptible", "--no_long",
                   "--no_getenv", "--no_cwait", "--no_clean_up" , "--no_nice",
                   "--no_set_special_env", "--no_abs_path", "--no_pkdilly",
-                  "--no_m32G", "--no_keep_failed_jobs_in_queue"]:
+                  "--no_m32G", "--no_keep_failed_jobs_in_queue", "--no_restart"]:
         dbi_param[argv[5:]]=False
     elif argv=="--testdbi":
         dbi_param["test"]=True
@@ -474,6 +476,19 @@
         commands+=t1
         choise_args+=t2
     FD.close
+elif dbi_param.get("restart",False):
+    assert launch_cmd=="Condor"
+    cmds=[]
+    for arg in command_argv:
+        p=Popen("condor_history -l "+arg, shell=True, stdout=PIPE)
+        p.wait()
+        lines=p.stdout.readlines()
+        for l in lines:
+            if l.startswith("Arguments = "):
+                cmd=l[13:-2]
+                cmds.append(cmd.replace("'",""))
+    commands=cmds
+    choise_args=[]
 else:
     (commands,choise_args)=generate_commands(command_argv)
 



From nouiz at mail.berlios.de  Wed Feb 11 21:21:01 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 11 Feb 2009 21:21:01 +0100
Subject: [Plearn-commits] r9931 - trunk/scripts
Message-ID: <200902112021.n1BKL1W6022102@sheep.berlios.de>

Author: nouiz
Date: 2009-02-11 21:21:01 +0100 (Wed, 11 Feb 2009)
New Revision: 9931

Modified:
   trunk/scripts/dbidispatch
Log:
-better help
-moved some global variable to the dictionary to be more consistent.
-more consistent parsing of the tasks_filename option


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-02-11 19:25:42 UTC (rev 9930)
+++ trunk/scripts/dbidispatch	2009-02-11 20:21:01 UTC (rev 9931)
@@ -74,7 +74,11 @@
     option for dbidispatch. They can be overrided on the command line.
   The 'DBIDISPATCH_LOGDIR' environnement variable set the name of the directory
     where all the individual logs directory will be put. Default to LOGS.
-  The '--[*no_]restart_jobs' option work only for condor. The parameter following it should be condor jobs number. We will parse the history on the local host and relauchn those jobs. We only take the command line that was executed, all other option are the new one passed to dbidispatch.
+  The '--[*no_]restart_jobs' option work only for condor. The parameter 
+    following this option should be condor jobs number. We will parse the
+    history on the local host and relaunch those jobs. We only take the command
+    line that was executed, all other options are are those passed to 
+    dbidispatch this time. Work only with jobs launched with dbidispatch.
 
 bqtools, cluster and condor option:
   The '--mem=X' speficify the number of ram in meg the program need to execute.
@@ -236,13 +240,8 @@
 
 """%{'ShortHelp':ShortHelp,'ScriptName':ScriptName}
 
-if len(sys.argv) == 1:
-    print ShortHelp
-    sys.exit(1)
-FILE = ""
 dbi_param={}
 testmode=False
-tasks_filename = []
 
 PATH=os.getenv('PATH')
 if search_file('condor_submit',PATH):
@@ -284,8 +283,6 @@
         if argv.startswith("--ssh"):
             dbi_param["file_redirect_stdout"]=False
             dbi_param["file_redirect_stderr"]=False
-    elif argv.startswith("--file="):
-        FILE = argv[7:]
     elif argv in ["--32","--64","--3264"]:
         dbi_param["arch"]=argv[2:]
     elif argv.startswith("--micro"):
@@ -293,15 +290,6 @@
         if len(argv)>7:
             assert(argv[7]=="=")
             dbi_param["micro"]=argv[8:]
-    elif argv.startswith("--tasks_filename="):
-        part = argv.split('=',1)
-        accepted_value=["compact","explicit","nb0","nb1","sh","clusterid","processid","none"]
-        val=part[1].split(",") 
-        for v in val:
-            if v not in accepted_value:
-                print "The option '"+argv+"' have an invalid value. possible value are:", accepted_value
-                sys.exit(2)
-        tasks_filename = val
     elif argv in  ["--force", "--interruptible", "--long", 
                    "--getenv", "--cwait", "--clean_up" ,"--nice",
                    "--set_special_env", "--abs_path", "--pkdilly", "--to_all",
@@ -325,7 +313,8 @@
     elif argv.split('=')[0] in ["--duree","--cpu","--mem","--os","--nb_proc",
                                 "--req", "--files", "--raw", "--rank", "--env",
                                 "--universe", "--exp_dir", "--machine", "--machines",
-                                "--queue", "--nano", "--submit_options", "--jobs_name"]:
+                                "--queue", "--nano", "--submit_options",
+                                "--jobs_name", "--file", "--tasks_filename"]:
         sp = argv.split('=',1)
         param=sp[0][2:]
         val = sp[1]
@@ -339,7 +328,7 @@
         elif param=="env":
             dbi_param.setdefault(param,"")
             dbi_param[param]+='"'+val+'"'
-        elif param=="machines" or param=="machine":
+        elif param in ["machines", "machine", "tasks_filename"]:
             dbi_param.setdefault(param,[])
             dbi_param[param]+=val.split(",")
         else:
@@ -364,23 +353,34 @@
         break
     command_argv.remove(argv)
 
-if len(command_argv) == 0 and FILE == "":
+if launch_cmd in ["Bqtools","Condor"]:
+    dbi_param.setdefault("tasks_filename", ["nb0","compact"])
+
+if len(command_argv) == 0 and not dbi_param.has_key("file"):
     print "No command or file with command to execute!"
     print
     print ShortHelp
     sys.exit(1)
+if dbi_param.has_key("file") and dbi_param.has_key("restart"):
+    print "the --file= and --restart option are incompatible!"
+    print
+    print ShortHelp
+    sys.exit(1)
 
-valid_dbi_param=["clean_up", "test", "dolog", "nb_proc", "exp_dir"]
+
+valid_dbi_param=["clean_up", "test", "dolog", "nb_proc", "exp_dir", "file"]
 if launch_cmd=="Cluster":
     valid_dbi_param +=["cwait","force","arch","interruptible",
                        "duree","cpu","mem","os"]
 elif launch_cmd=="Condor":
     valid_dbi_param +=["req", "arch", "getenv", "nice", "files", "rank", "env",
                        "raw", "os", "set_special_env", "mem", "cpu", "pkdilly",
-                       "universe", "machine", "machines", "to_all", "keep_failed_jobs_in_queue"]
+                       "universe", "machine", "machines", "to_all", 
+                       "keep_failed_jobs_in_queue", "tasks_filename"]
 elif launch_cmd=="Bqtools":
     valid_dbi_param +=["cpu", "duree", "long", "mem", "micro",
-                       "nano", "queue", "raw", "submit_options", "jobs_name" ]
+                       "nano", "queue", "raw", "submit_options", "jobs_name",
+                       "tasks_filename" ]
 
 if  launch_cmd == 'Condor' and gethostname().endswith(".iro.umontreal.ca"):
     #default value for pkdilly is true.
@@ -404,10 +404,6 @@
         if source_with_kerb:
             dbi_param['copy_local_source_file']=True
 
-if not tasks_filename and launch_cmd in ["Bqtools","Condor"]:
-    tasks_filename = ["nb0","compact"]
-elif tasks_filename and launch_cmd not in ["Bqtools", "Condor"]:
-    print "WARNING: The parameter --tasks_filename={} is only supported by condor and bqtools.",
     
 print "\n\nThe jobs will be launched on the system:", launch_cmd
 print "With options: ",dbi_param
@@ -463,11 +459,11 @@
     return (argscombination,args_modif)
 
 #generate the command
-if FILE != "":
-    FD = open(FILE,'r')#|| die "couldn't open the file $FILE!";
+if dbi_param.has_key("file"):
+    fd = open(dbi_param.get("file"),'r')
     commands=[]
     choise_args = []
-    for line in FD.readlines():
+    for line in fd.readlines():
         line = line.rstrip()
         if not line:
             continue
@@ -475,7 +471,7 @@
         (t1,t2)=generate_commands(sp)
         commands+=t1
         choise_args+=t2
-    FD.close
+    fd.close
 elif dbi_param.get("restart",False):
     assert launch_cmd=="Condor"
     cmds=[]
@@ -500,7 +496,7 @@
 
 if dbi_param.has_key("exp_dir"):
     dbi_param["log_dir"]=os.path.join(LOGDIR,dbi_param["exp_dir"])
-elif FILE == "":
+elif not dbi_param.has_key("file"):
     t = [x for x in sys.argv[1:] if not x[:2]=="--"]
     t[0]=os.path.split(t[0])[1]
     tmp="_".join(t)
@@ -508,6 +504,9 @@
     ### We need to remove the symbols "," as this cause trouble with bqtools
     tmp=re.sub( ',', '-', tmp )
     date_str=str(datetime.datetime.now())
+    if dbi_param.has_key("restart"):
+        tmp="jobs_restarted_"+tmp
+    
     if launch_cmd == "Bqtools":
         #bqtools have a limit. It must have a abspath size < max_file_size -16
         #(255 on ext3)
@@ -521,7 +520,7 @@
     tmp+='_'+date_str.replace(' ','_')
     dbi_param["log_dir"]=os.path.join(LOGDIR,tmp)
 else:
-    dbi_param["log_dir"]=os.path.join(LOGDIR,FILE)
+    dbi_param["log_dir"]=os.path.join(LOGDIR,dbi_param["file"])
 dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')
 
 n="base_tasks_log_file"
@@ -529,7 +528,7 @@
 
 def merge_pattern(new_list):
     return [x+'.'+y if x else y for (x,y) in  zip(dbi_param[n], new_list)]
-for pattern in tasks_filename:
+for pattern in dbi_param.get("tasks_filename"):
     if pattern == "explicit":
         dbi_param[n]=merge_pattern([re.sub( '[^a-zA-Z=0-9-]', '_', x ) for x in commands])
     elif pattern == "compact":
@@ -577,8 +576,7 @@
         dbi_param["stdouts"]=stdouts
         dbi_param["stderrs"]=stderrs
     else:
-        print "internal error! bad pattern:",pattern
-        sys.exit(2)
+        raise Exception("bad value for tasks_filename ("+pattern+"). Accepted value: compact, explicit, nb0, nb1, sh, clusterid, processid, none.")
     assert(not (dbi_param.has_key("stdouts") and (dbi_param[n])==0))
 
 #undef merge_pattern



From laulysta at mail.berlios.de  Fri Feb 13 02:12:28 2009
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Fri, 13 Feb 2009 02:12:28 +0100
Subject: [Plearn-commits] r9932 - trunk/plearn_learners_experimental
Message-ID: <200902130112.n1D1CS5s013296@sheep.berlios.de>

Author: laulysta
Date: 2009-02-13 02:12:27 +0100 (Fri, 13 Feb 2009)
New Revision: 9932

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
Log:
test


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-02-11 20:21:01 UTC (rev 9931)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-02-13 01:12:27 UTC (rev 9932)
@@ -88,7 +88,8 @@
     input_reconstruction_cost_weight(0),
     hidden_reconstruction_cost_weight(0),
     current_learning_rate(0),
-    nb_stage_reconstruction(0)
+    nb_stage_reconstruction(0),
+    nb_stage_target(0)
 {
     random_gen = new PRandom();
 }
@@ -264,6 +265,10 @@
                   OptionBase::learntoption,
                   "The nomber of stage for de reconstructions");
 
+    declareOption(ol, "nb_stage_target", &DenoisingRecurrentNet::nb_stage_target,
+                  OptionBase::learntoption,
+                  "The nomber of stage for de target");
+
  /*
     declareOption(ol, "", &DenoisingRecurrentNet::,
                   OptionBase::learntoption,
@@ -721,7 +726,7 @@
                     inject_zero_forcing_noise(encoded_seq, input_noise_prob);
 
                 // recurrent no noise phase
-                if(stage>=nb_stage_reconstruction){
+                if(stage>=nb_stage_reconstruction && stage<nb_stage_target){
                     if(recurrent_lr!=0)
                     {
                         
@@ -729,23 +734,38 @@
                             encoded_seq << clean_encoded_seq;                  
                         setLearningRate( recurrent_lr );                    
                         recurrentFprop(train_costs, train_n_items);
-                        recurrentUpdate(0,0,1, prediction_cost_weight, train_costs, train_n_items );
+                        recurrentUpdate(0,0,1, prediction_cost_weight,0, train_costs, train_n_items );
                         
                     }
                 }
+
+                if(stage>=nb_stage_target){
+                    if(recurrent_lr!=0)
+                    {
+                        
+                        if(corrupt_input) // need to recover the clean sequence                        
+                            encoded_seq << clean_encoded_seq;                  
+                        setLearningRate( recurrent_lr );                    
+                        recurrentFprop(train_costs, train_n_items);
+                        recurrentUpdate(0,0,1, prediction_cost_weight,1, train_costs, train_n_items );
+                        
+                    }
+                }
+
                 if(stage<nb_stage_reconstruction){
+
                     // greedy phase input
                     if(input_reconstruction_lr!=0){
                         setLearningRate( input_reconstruction_lr );
                         recurrentFprop(train_costs, train_n_items);
-                        recurrentUpdate(input_reconstruction_cost_weight, 0, 1, 0, train_costs, train_n_items );
+                        recurrentUpdate(input_reconstruction_cost_weight, 0, 1, 0,1, train_costs, train_n_items );
                     }
                     
                     // greedy phase hidden
                     if(hidden_reconstruction_lr!=0){
                         setLearningRate( dynamic_gradient_scale_factor*hidden_reconstruction_lr);
                         recurrentFprop(train_costs, train_n_items);
-                        recurrentUpdate(0, hidden_reconstruction_cost_weight, 1, 0, train_costs, train_n_items );
+                        recurrentUpdate(0, hidden_reconstruction_cost_weight, 1, 0,1, train_costs, train_n_items );
                     }
                 }
                 // recurrent noisy phase
@@ -753,7 +773,7 @@
                 {
                     setLearningRate( noisy_recurrent_lr );
                     recurrentFprop(train_costs, train_n_items);
-                    recurrentUpdate(input_reconstruction_cost_weight, hidden_reconstruction_cost_weight, 1, prediction_cost_weight, train_costs, train_n_items );
+                    recurrentUpdate(input_reconstruction_cost_weight, hidden_reconstruction_cost_weight, 1,1, prediction_cost_weight, train_costs, train_n_items );
                 }
 
                 
@@ -1271,10 +1291,11 @@
     
     /********************************************************************************/
     // Vec hidden_reconstruction_activation_grad;
-    //hidden_reconstruction_activation_grad.clear();
-    //for(int k=0; k<reconstruction_prob.length(); k++)
-    //    hidden_reconstruction_activation_grad[k] = safelog(reconstruction_prob[k]) - safelog(1-reconstruction_prob[k]);
-    
+    /*hidden_reconstruction_activation_grad.clear();
+    for(int k=0; k<reconstruction_prob.length(); k++){
+        //    hidden_reconstruction_activation_grad[k] = safelog(1-reconstruction_prob[k]) - safelog(reconstruction_prob[k]);
+        hidden_reconstruction_activation_grad[k] = - reconstruction_activation[k];
+        }*/
 
     double result_cost = 0;
     double neg_log_cost = 0; // neg log softmax
@@ -1459,6 +1480,7 @@
                                             real hidden_reconstruction_weight,
                                             real temporal_gradient_contribution,
                                             real predic_cost_weight,
+                                            real inputAndDynamicPart,
                                             Vec train_costs, 
                                             Vec train_n_items )
 {
@@ -1562,7 +1584,8 @@
                     hidden_gradient, bias_gradient);
             }
         }
-        else{   
+
+        if(inputAndDynamicPart){   
             // Add contribution of input reconstruction cost in hidden_gradient
             if(input_reconstruction_weight!=0)
             {
@@ -1570,7 +1593,7 @@
                 Vec clean_input = clean_encoded_seq.subMatRows(i, input_window_size).toVec();
                 
                 train_costs[4] += fpropUpdateInputReconstructionFromHidden(hidden_list(i), inputWeights, acc_input_connections_gr, input_reconstruction_bias, input_reconstruction_prob, 
-                                                                           clean_input, hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
+                                                                           clean_input, hidden_gradient, input_reconstruction_weight, current_learning_rate);
                 train_n_items[4]++;
             }
             
@@ -1642,8 +1665,9 @@
                                       input_connections->learning_rate,
                                       false);
                 
-                hidden_temporal_gradient << hidden_gradient;  
-                //hidden_temporal_gradient +=  hidden_reconstruction_activation_grad;
+                hidden_temporal_gradient << hidden_gradient; 
+                //if(hidden_reconstruction_weight!=0)
+                //    hidden_temporal_gradient +=  hidden_reconstruction_activation_grad;
             }
             else
             {

Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2009-02-11 20:21:01 UTC (rev 9931)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2009-02-13 01:12:27 UTC (rev 9932)
@@ -158,6 +158,8 @@
     double hidden_reconstruction_cost_weight;
 
     double nb_stage_reconstruction;
+    double nb_stage_target;
+
     //#####  Not Options  #####################################################
 
 
@@ -275,6 +277,7 @@
                          real hidden_reconstruction_cost_weight,
                          real temporal_gradient_contribution,
                          real prediction_cost_weight,
+                         real inputAndDynamicPart,
                          Vec train_costs,
                          Vec train_n_items);
 



From laulysta at mail.berlios.de  Sat Feb 14 00:31:56 2009
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Sat, 14 Feb 2009 00:31:56 +0100
Subject: [Plearn-commits] r9933 - trunk/plearn_learners_experimental
Message-ID: <200902132331.n1DNVulx021229@sheep.berlios.de>

Author: laulysta
Date: 2009-02-14 00:31:55 +0100 (Sat, 14 Feb 2009)
New Revision: 9933

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
Log:
rconstruction stage and fine tuning stage


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-02-13 01:12:27 UTC (rev 9932)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-02-13 23:31:55 UTC (rev 9933)
@@ -726,7 +726,7 @@
                     inject_zero_forcing_noise(encoded_seq, input_noise_prob);
 
                 // recurrent no noise phase
-                if(stage>=nb_stage_reconstruction && stage<nb_stage_target){
+                if(stage>=nb_stage_reconstruction && stage<nb_stage_target+nb_stage_reconstruction){
                     if(recurrent_lr!=0)
                     {
                         
@@ -739,7 +739,7 @@
                     }
                 }
 
-                if(stage>=nb_stage_target){
+                if(stage>=nb_stage_target+nb_stage_reconstruction){
                     if(recurrent_lr!=0)
                     {
                         



From ducharme at mail.berlios.de  Mon Feb 16 16:57:59 2009
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Mon, 16 Feb 2009 16:57:59 +0100
Subject: [Plearn-commits] r9934 - trunk/plearn/vmat
Message-ID: <200902161557.n1GFvxqB021916@sheep.berlios.de>

Author: ducharme
Date: 2009-02-16 16:57:59 +0100 (Mon, 16 Feb 2009)
New Revision: 9934

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
Some correct-size checking.


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2009-02-13 23:31:55 UTC (rev 9933)
+++ trunk/plearn/vmat/VMatrix.cc	2009-02-16 15:57:59 UTC (rev 9934)
@@ -1830,6 +1830,9 @@
 ////////////////////
 void VMatrix::putOrAppendRow(int i, Vec v)
 {
+    if (v.length() != width())
+        PLERROR("In putOrAppendRow, Vec to append must have same length (%d) as VMatrix width (%d)", v.length(), width());
+
     if(i==length())
         appendRow(v);
     else if(i<length())
@@ -1843,6 +1846,9 @@
 /////////////////
 void VMatrix::forcePutRow(int i, Vec v)
 {
+    if (v.length() != width())
+        PLERROR("In forcePutRow, Vec to append must have same length (%d) as VMatrix width (%d)", v.length(), width());
+
     if(i<length())
         putRow(i,v);
     else



From nouiz at mail.berlios.de  Mon Feb 16 18:58:58 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 16 Feb 2009 18:58:58 +0100
Subject: [Plearn-commits] r9935 - trunk/plearn_learners/regressors
Message-ID: <200902161758.n1GHwwRe004365@sheep.berlios.de>

Author: nouiz
Date: 2009-02-16 18:58:56 +0100 (Mon, 16 Feb 2009)
New Revision: 9935

Modified:
   trunk/plearn_learners/regressors/RegressionTreeLeave.h
Log:
added some getter fct.


Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.h	2009-02-16 15:57:59 UTC (rev 9934)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.h	2009-02-16 17:58:56 UTC (rev 9935)
@@ -101,7 +101,8 @@
     inline int           length()const{return length_;}
     virtual void         getOutputAndError(Vec& output, Vec& error)const;
     virtual void         printStats();
-  
+    real                 getWeightsSum(){return weights_sum;}
+    real                 getTargetsSum(){return targets_sum;}
 private:
     void         build_();
     void         verbose(string msg, int level);



From nouiz at mail.berlios.de  Mon Feb 16 21:14:53 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 16 Feb 2009 21:14:53 +0100
Subject: [Plearn-commits] r9936 - trunk/plearn_learners/regressors
Message-ID: <200902162014.n1GKErE5007228@sheep.berlios.de>

Author: nouiz
Date: 2009-02-16 21:14:53 +0100 (Mon, 16 Feb 2009)
New Revision: 9936

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
fixed the progress bar number to be sure it is automatically closed. This allow the correct printing of following PLWARNING.


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-02-16 17:58:56 UTC (rev 9935)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-02-16 20:14:53 UTC (rev 9936)
@@ -286,7 +286,7 @@
     for (int sample_dim = 0; sample_dim < inputsize(); sample_dim++)
     {
         sortEachDim(sample_dim);
-        if (report_progress) pb->update(sample_dim);
+        if (report_progress) pb->update(sample_dim+1);
     }
 }
   



From nouiz at mail.berlios.de  Mon Feb 16 22:33:30 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 16 Feb 2009 22:33:30 +0100
Subject: [Plearn-commits] r9937 - trunk/plearn_learners/hyper
Message-ID: <200902162133.n1GLXUFs017990@sheep.berlios.de>

Author: nouiz
Date: 2009-02-16 22:33:30 +0100 (Mon, 16 Feb 2009)
New Revision: 9937

Modified:
   trunk/plearn_learners/hyper/HyperLearner.cc
   trunk/plearn_learners/hyper/HyperLearner.h
Log:
added the option HyperLearner::finalize_learner that default to false.


Modified: trunk/plearn_learners/hyper/HyperLearner.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.cc	2009-02-16 20:14:53 UTC (rev 9936)
+++ trunk/plearn_learners/hyper/HyperLearner.cc	2009-02-16 21:33:30 UTC (rev 9937)
@@ -79,7 +79,8 @@
 HyperLearner::HyperLearner()
     : provide_strategy_expdir(true),
       save_final_learner(true),
-      reloaded(false)
+      reloaded(false),
+      finalize_learner(false)
 {
     // Forward the 'test' method to the underlying learner.
     forward_test = true;
@@ -131,6 +132,11 @@
     declareOption(ol, "save_final_learner", &HyperLearner::save_final_learner, OptionBase::buildoption,
                   "should final learner be saved in expdir/final_learner.psave");
 
+    declareOption(
+        ol, "finalize_learner", &HyperLearner::finalize_learner,
+        OptionBase::buildoption,
+        "Default false. If true, will finalize the learner after the training.");
+
     declareOption(ol, "reloaded", &HyperLearner::reloaded,
                   OptionBase::learntoption|OptionBase::nosave,
         "Used internally to avoid reloading a file, since the build function\n"
@@ -242,6 +248,9 @@
 
         train_stats->update(results);
 
+        if(finalize_learner)
+            learner_->finalize();
+
         if(save_final_learner)
         {
             if(expdir=="")

Modified: trunk/plearn_learners/hyper/HyperLearner.h
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.h	2009-02-16 20:14:53 UTC (rev 9936)
+++ trunk/plearn_learners/hyper/HyperLearner.h	2009-02-16 21:33:30 UTC (rev 9937)
@@ -79,6 +79,9 @@
     static bool reloading;
     // HyperLearner methods
 
+    //! if true, we finalize the learner after training.
+    bool finalize_learner;
+
     HyperLearner();
 
     inline void setLearner(PP<PLearner> learner)



From nouiz at mail.berlios.de  Mon Feb 16 22:53:19 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 16 Feb 2009 22:53:19 +0100
Subject: [Plearn-commits] r9938 - trunk/plearn/vmat
Message-ID: <200902162153.n1GLrJds020129@sheep.berlios.de>

Author: nouiz
Date: 2009-02-16 22:53:18 +0100 (Mon, 16 Feb 2009)
New Revision: 9938

Modified:
   trunk/plearn/vmat/SubVMatrix.cc
   trunk/plearn/vmat/SubVMatrix.h
Log:
set a metadatadir in the SubVMatrix class that depend on the source metadatadir and the SubVMatrix option.


Modified: trunk/plearn/vmat/SubVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SubVMatrix.cc	2009-02-16 21:33:30 UTC (rev 9937)
+++ trunk/plearn/vmat/SubVMatrix.cc	2009-02-16 21:53:18 UTC (rev 9938)
@@ -39,6 +39,7 @@
  ******************************************************* */
 
 #include "SubVMatrix.h"
+#include <plearn/base/tostring.h>
 
 namespace PLearn {
 using namespace std;
@@ -97,6 +98,9 @@
 ////////////////////
 void SubVMatrix::declareOptions(OptionList &ol)
 {
+    //WARNING: If you add option make sure the we correctly build the metadatadir!
+    //search for setMetaDataDir in this file.
+
     declareOption(ol, "parent", &SubVMatrix::source, OptionBase::buildoption,
                   "DEPRECATED - Use 'source' instead.");
 
@@ -237,8 +241,14 @@
         }
     }
     // else, nothing to change
-// */
 
+    if(!hasMetaDataDir() && source->hasMetaDataDir())
+        setMetaDataDir(source->getMetaDataDir() / classname()+
+                       "_istart=" + tostring(istart) + 
+                       "_jstart=" +tostring(jstart) + 
+                       "_length="+tostring(length()) + 
+                       "_width="+tostring(width()));
+
     //  cerr << "inputsize: "<<inputsize_ << "  targetsize:"<<targetsize_<<"weightsize:"<<weightsize_<<endl;
 }
 

Modified: trunk/plearn/vmat/SubVMatrix.h
===================================================================
--- trunk/plearn/vmat/SubVMatrix.h	2009-02-16 21:33:30 UTC (rev 9937)
+++ trunk/plearn/vmat/SubVMatrix.h	2009-02-16 21:53:18 UTC (rev 9938)
@@ -61,14 +61,12 @@
 
     //! Build options.
 
-    // DEPRECATED - Use inherited::source instead.
-    // VMat parent;
     int istart;
     int jstart;
     real fistart;
     real flength;
 
-    //! The appropriate VMFields of the parent VMat are copied upon
+    //! The appropriate VMFields of the source VMat are copied upon
     //! construction
     SubVMatrix(bool call_build_ = false);
 



From nouiz at mail.berlios.de  Mon Feb 16 22:54:39 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 16 Feb 2009 22:54:39 +0100
Subject: [Plearn-commits] r9939 - trunk/plearn/base
Message-ID: <200902162154.n1GLsdES020303@sheep.berlios.de>

Author: nouiz
Date: 2009-02-16 22:54:38 +0100 (Mon, 16 Feb 2009)
New Revision: 9939

Modified:
   trunk/plearn/base/Object.cc
Log:
to have better readability of psave file, we happend in comment the name of the class that we close.


Modified: trunk/plearn/base/Object.cc
===================================================================
--- trunk/plearn/base/Object.cc	2009-02-16 21:53:18 UTC (rev 9938)
+++ trunk/plearn/base/Object.cc	2009-02-16 21:54:38 UTC (rev 9939)
@@ -601,7 +601,7 @@
         if (i < optnames.size() - 1)
             out.write(";\n");
     }
-    out.write(" )\n");
+    out.write(" ) # "+classname()+"\n");
 }
 
 



From nouiz at mail.berlios.de  Mon Feb 16 23:29:22 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 16 Feb 2009 23:29:22 +0100
Subject: [Plearn-commits] r9940 - trunk/plearn/base
Message-ID: <200902162229.n1GMTMLH023855@sheep.berlios.de>

Author: nouiz
Date: 2009-02-16 23:29:22 +0100 (Mon, 16 Feb 2009)
New Revision: 9940

Modified:
   trunk/plearn/base/Object.cc
Log:
removed a commit as this break test.


Modified: trunk/plearn/base/Object.cc
===================================================================
--- trunk/plearn/base/Object.cc	2009-02-16 21:54:38 UTC (rev 9939)
+++ trunk/plearn/base/Object.cc	2009-02-16 22:29:22 UTC (rev 9940)
@@ -601,7 +601,8 @@
         if (i < optnames.size() - 1)
             out.write(";\n");
     }
-    out.write(" ) # "+classname()+"\n");
+//    out.write(" ) # "+classname()+"\n");
+    out.write(" )\n");
 }
 
 



From laulysta at mail.berlios.de  Tue Feb 17 20:56:23 2009
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Tue, 17 Feb 2009 20:56:23 +0100
Subject: [Plearn-commits] r9941 - trunk/plearn_learners_experimental
Message-ID: <200902171956.n1HJuNJh026852@sheep.berlios.de>

Author: laulysta
Date: 2009-02-17 20:56:23 +0100 (Tue, 17 Feb 2009)
New Revision: 9941

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
Log:
input noise


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-02-16 22:29:22 UTC (rev 9940)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-02-17 19:56:23 UTC (rev 9941)
@@ -89,7 +89,8 @@
     hidden_reconstruction_cost_weight(0),
     current_learning_rate(0),
     nb_stage_reconstruction(0),
-    nb_stage_target(0)
+    nb_stage_target(0),
+    noise(false)
 {
     random_gen = new PRandom();
 }
@@ -714,10 +715,13 @@
             int nseq = nSequences();
             for(int i=0; i<nseq; i++)
             {
+
+                if(stage<nb_stage_reconstruction && input_noise_prob!=0 )
+                    noise = true;
                 getSequence(i, seq);
                 encodeSequenceAndPopulateLists(seq);
               
-                bool corrupt_input = input_noise_prob!=0 && (noisy_recurrent_lr!=0 || input_reconstruction_lr!=0);
+                bool corrupt_input = false;//input_noise_prob!=0 && (noisy_recurrent_lr!=0 || input_reconstruction_lr!=0);
 
                 clean_encoded_seq.resize(encoded_seq.length(), encoded_seq.width());
                 clean_encoded_seq << encoded_seq;
@@ -754,6 +758,7 @@
 
                 if(stage<nb_stage_reconstruction){
 
+
                     // greedy phase input
                     if(input_reconstruction_lr!=0){
                         setLearningRate( input_reconstruction_lr );
@@ -871,10 +876,15 @@
     int l = seq.length();
     resize_lists(l);
     int inputsize_without_masks = inputsize()-targetsize();
-    Mat input_part = seq.subMatColumns(0,inputsize_without_masks);
+    Mat input_part;
+    input_part.resize(seq.length(),inputsize_without_masks);
+    input_part << seq.subMatColumns(0,inputsize_without_masks);
     Mat mask_part = seq.subMatColumns(inputsize_without_masks, targetsize());
     Mat target_part = seq.subMatColumns(inputsize_without_masks+targetsize(), targetsize());
 
+    if(noise)
+        inject_zero_forcing_noise(input_part, input_noise_prob);
+
     for(int i=0; i<l; i++)
         input_list[i] = input_part(i);
 
@@ -1295,7 +1305,7 @@
     for(int k=0; k<reconstruction_prob.length(); k++){
         //    hidden_reconstruction_activation_grad[k] = safelog(1-reconstruction_prob[k]) - safelog(reconstruction_prob[k]);
         hidden_reconstruction_activation_grad[k] = - reconstruction_activation[k];
-        }*/
+    }*/
 
     double result_cost = 0;
     double neg_log_cost = 0; // neg log softmax
@@ -1961,7 +1971,7 @@
     
 
 // input noise injection
-void DenoisingRecurrentNet::inject_zero_forcing_noise(Mat sequence, double noise_prob)
+void DenoisingRecurrentNet::inject_zero_forcing_noise(Mat sequence, double noise_prob) const
 {
     if(!sequence.isCompact())
         PLERROR("Expected a compact sequence");

Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2009-02-16 22:29:22 UTC (rev 9940)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2009-02-17 19:56:23 UTC (rev 9941)
@@ -124,6 +124,8 @@
     //! Chooses what type of encoding to apply to an input sequence
     //! Possibilities: "timeframe", "note_duration", "note_octav_duration", "raw_masked_supervised"
     string encoding;
+
+    bool noise;
     
     //! Input window size
     int input_window_size;
@@ -186,7 +188,7 @@
 
 
     // input noise injection
-    void inject_zero_forcing_noise(Mat sequence, double noise_prob);
+    void inject_zero_forcing_noise(Mat sequence, double noise_prob) const;
 
     inline static Vec getInputWindow(Mat sequence, int startpos, int winsize)
     { return sequence.subMatRows(startpos, winsize).toVec(); }



From nouiz at mail.berlios.de  Tue Feb 17 22:02:36 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 17 Feb 2009 22:02:36 +0100
Subject: [Plearn-commits] r9942 - in trunk/plearn_learners: meta regressors
Message-ID: <200902172102.n1HL2abi000429@sheep.berlios.de>

Author: nouiz
Date: 2009-02-17 22:02:36 +0100 (Tue, 17 Feb 2009)
New Revision: 9942

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
modif that allow the saving of the sorted train set.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-02-17 19:56:23 UTC (rev 9941)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-02-17 21:02:36 UTC (rev 9942)
@@ -577,6 +577,8 @@
     if(training_set_has_changed || !learner1->getTrainingSet()){
         VMat vmat1 = new ProcessingVMatrix(training_set, input_prg,
                                            target_prg1,  weight_prg);
+        if(training_set->hasMetaDataDir())
+            vmat1->setMetaDataDir(training_set->getMetaDataDir()/"0vsOther");
         learner1->setTrainingSet(vmat1, call_forget);
     }
     if(training_set_has_changed || !learner2->getTrainingSet()){

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-02-17 19:56:23 UTC (rev 9941)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-02-17 21:02:36 UTC (rev 9942)
@@ -40,9 +40,13 @@
  ********************************************************************************** */
 
 #include "RegressionTreeRegisters.h"
+#define PL_LOG_MODULE_NAME RegressionTreeRegisters
+#include <plearn/io/pl_log.h>
 #include <plearn/vmat/TransposeVMatrix.h>
 #include <plearn/vmat/MemoryVMatrixNoSave.h>
 #include <plearn/vmat/SubVMatrix.h>
+#include <plearn/io/fileutils.h>
+#include <plearn/io/load_and_save.h>
 #include <limits>
 
 namespace PLearn {
@@ -267,6 +271,14 @@
         verbose("RegressionTreeRegisters: Sorted train set indices are present, no sort required", 3);
         return;
     }
+    string f=source->getMetaDataDir()+"RTR_tsorted_row.psave";
+
+    if(isUpToDate(f)){
+        NORMAL_LOG<<"RegressionTreeRegisters:: Reloading the sorted source VMatrix"<<endl;
+        PLearn::load(f,tsorted_row);
+        return;
+    }
+
     verbose("RegressionTreeRegisters: The train set is being sorted", 3);
     tsorted_row.resize(inputsize(), length());
     PP<ProgressBar> pb;
@@ -288,6 +300,13 @@
         sortEachDim(sample_dim);
         if (report_progress) pb->update(sample_dim+1);
     }
+    if(source->hasMetaDataDir()){
+        NORMAL_LOG<<"RegressionTreeRegisters:: Saving the sorted source VMatrix"<<endl;
+        PLearn::save(f,tsorted_row);
+    }else{
+        NORMAL_LOG<<"RegressionTreeRegisters:: can't save the sorted source VMatrix as we don't have a metadatadir"<<endl;
+    }
+
 }
   
 void RegressionTreeRegisters::sortEachDim(int dim)



From nouiz at mail.berlios.de  Tue Feb 17 22:42:00 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 17 Feb 2009 22:42:00 +0100
Subject: [Plearn-commits] r9943 - in trunk: commands/PLearnCommands plearn/io
Message-ID: <200902172142.n1HLfx88004418@sheep.berlios.de>

Author: nouiz
Date: 2009-02-17 22:41:58 +0100 (Tue, 17 Feb 2009)
New Revision: 9943

Modified:
   trunk/commands/PLearnCommands/HelpCommand.cc
   trunk/commands/PLearnCommands/plearn_main.cc
   trunk/plearn/io/PStream.cc
   trunk/plearn/io/PStream.h
Log:
added the plearn option --windows_endl that change endl to be "\r\n" instea of "\n". Will not work for hard coded endl in string.



Modified: trunk/commands/PLearnCommands/HelpCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/HelpCommand.cc	2009-02-17 21:02:36 UTC (rev 9942)
+++ trunk/commands/PLearnCommands/HelpCommand.cc	2009-02-17 21:41:58 UTC (rev 9943)
@@ -67,6 +67,7 @@
         "Global parameter:\n"
         "                 --no-progress: don't print progress bar\n"
         "                 --no-version: do not print the version \n"
+        "                 --windows_endl: use windows end of line\n"
         "                 --profile: print some profiling information\n"
         "                     Must have been compiled with '-logging=dbg-profile'\n"
         "                 --verbosity LEVEL: The level of log to print.\n"

Modified: trunk/commands/PLearnCommands/plearn_main.cc
===================================================================
--- trunk/commands/PLearnCommands/plearn_main.cc	2009-02-17 21:02:36 UTC (rev 9942)
+++ trunk/commands/PLearnCommands/plearn_main.cc	2009-02-17 21:41:58 UTC (rev 9943)
@@ -184,6 +184,8 @@
     // If we don't want no progress bars
     int no_progress_bars     = findpos( command_line, "--no-progress" );
 
+    int windows_endl         = findpos( command_line, "--windows_endl" );
+    
     // Note that the verbosity_value_pos IS NOT EQUAL TO verbosity_pos+1 if
     // (verbosity_pos == -1)!!!
     int verbosity_pos                = findpos( command_line, "--verbosity"  );
@@ -268,6 +270,7 @@
         if ( c != no_version_pos             &&
              c != profile_pos                &&
              c != no_progress_bars           &&
+             c != windows_endl               &&
              c != verbosity_pos              &&
              c != verbosity_value_pos        &&
              c != enable_logging_pos         &&
@@ -300,6 +303,9 @@
     if (no_progress_bars != -1)
         ProgressBar::setPlugin(new NullProgressBarPlugin);
 
+    if (windows_endl != -1)
+        PStream::windows_endl = true;
+
     if (enabled_modules_pos != -1)
         perr << "Logging enabled for modules: "
              << join(PL_Log::instance().namedLogging(), ", ")

Modified: trunk/plearn/io/PStream.cc
===================================================================
--- trunk/plearn/io/PStream.cc	2009-02-17 21:02:36 UTC (rev 9942)
+++ trunk/plearn/io/PStream.cc	2009-02-17 21:41:58 UTC (rev 9943)
@@ -56,7 +56,7 @@
 // Default format string for floats and doubles
 const char* PStream::format_float_default  = "%.8g";
 const char* PStream::format_double_default = "%.18g";
-
+bool PStream::windows_endl = false;
 // Initialization for pin, pout, ...
 
 PStream& get_pnull()

Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2009-02-17 21:02:36 UTC (rev 9942)
+++ trunk/plearn/io/PStream.h	2009-02-17 21:41:58 UTC (rev 9943)
@@ -185,7 +185,9 @@
     //! Should be true if this stream is used to communicate with a remote
     //! PLearn host.  Will serialize options accordingly.
     bool remote_plearn_comm;
-
+    
+    //! If true, we should emit windows end of line(\r\n)
+    static bool windows_endl;
 public:
 
     PStream();
@@ -600,6 +602,8 @@
 
     inline PStream& endl()
     {
+        if(windows_endl)
+            put('\r');
         put('\n');
         flush();
         return *this;



From nouiz at mail.berlios.de  Wed Feb 18 15:32:45 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 18 Feb 2009 15:32:45 +0100
Subject: [Plearn-commits] r9944 - trunk/plearn_learners/regressors
Message-ID: <200902181432.n1IEWjWI025301@sheep.berlios.de>

Author: nouiz
Date: 2009-02-18 15:32:45 +0100 (Wed, 18 Feb 2009)
New Revision: 9944

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
moved some log to debug level. This repair test and is not always needed.


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-02-17 21:41:58 UTC (rev 9943)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-02-18 14:32:45 UTC (rev 9944)
@@ -274,7 +274,7 @@
     string f=source->getMetaDataDir()+"RTR_tsorted_row.psave";
 
     if(isUpToDate(f)){
-        NORMAL_LOG<<"RegressionTreeRegisters:: Reloading the sorted source VMatrix"<<endl;
+        DBG_LOG<<"RegressionTreeRegisters:: Reloading the sorted source VMatrix"<<endl;
         PLearn::load(f,tsorted_row);
         return;
     }
@@ -301,10 +301,10 @@
         if (report_progress) pb->update(sample_dim+1);
     }
     if(source->hasMetaDataDir()){
-        NORMAL_LOG<<"RegressionTreeRegisters:: Saving the sorted source VMatrix"<<endl;
+        DBG_LOG<<"RegressionTreeRegisters:: Saving the sorted source VMatrix"<<endl;
         PLearn::save(f,tsorted_row);
     }else{
-        NORMAL_LOG<<"RegressionTreeRegisters:: can't save the sorted source VMatrix as we don't have a metadatadir"<<endl;
+        DBG_LOG<<"RegressionTreeRegisters:: can't save the sorted source VMatrix as we don't have a metadatadir"<<endl;
     }
 
 }



From nouiz at mail.berlios.de  Wed Feb 18 22:24:05 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 18 Feb 2009 22:24:05 +0100
Subject: [Plearn-commits] r9945 - trunk/scripts
Message-ID: <200902182124.n1ILO5Zu021973@sheep.berlios.de>

Author: nouiz
Date: 2009-02-18 22:24:04 +0100 (Wed, 18 Feb 2009)
New Revision: 9945

Modified:
   trunk/scripts/dbidispatch
Log:
moved the mkdir later, after we have parsed the parameter. That way if we only want to print the help, we don't do the directory for nothing.


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-02-18 14:32:45 UTC (rev 9944)
+++ trunk/scripts/dbidispatch	2009-02-18 21:24:04 UTC (rev 9945)
@@ -255,10 +255,7 @@
 LOGDIR=os.getenv("DBIDISPATCH_LOGDIR")
 if not LOGDIR:
     LOGDIR="LOGS"
-if not os.path.exists(LOGDIR):
-    os.mkdir(LOGDIR)
 
-
 to_parse=[]
 env=os.getenv("DBIDISPATCH_DEFAULT_OPTION")
 if env:
@@ -367,6 +364,8 @@
     print ShortHelp
     sys.exit(1)
 
+if not os.path.exists(LOGDIR):
+    os.mkdir(LOGDIR)
 
 valid_dbi_param=["clean_up", "test", "dolog", "nb_proc", "exp_dir", "file"]
 if launch_cmd=="Cluster":



From nouiz at mail.berlios.de  Thu Feb 19 19:46:01 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 19 Feb 2009 19:46:01 +0100
Subject: [Plearn-commits] r9946 - trunk/plearn_learners/regressors
Message-ID: <200902191846.n1JIk1Sb008338@sheep.berlios.de>

Author: nouiz
Date: 2009-02-19 19:46:01 +0100 (Thu, 19 Feb 2009)
New Revision: 9946

Modified:
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
   trunk/plearn_learners/regressors/RegressionTreeNode.h
Log:
put a tmp_vec static as we never use its value.


Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2009-02-18 21:24:04 UTC (rev 9945)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2009-02-19 18:46:01 UTC (rev 9946)
@@ -56,6 +56,7 @@
                         "a left leave for samples with values below the value of the splitting attribute, and a right leave for the others,\n"
     );
 int RegressionTreeNode::dummy_int = 0;
+Vec RegressionTreeNode::tmp_vec;
 RegressionTreeNode::RegressionTreeNode():
     missing_is_valid(0),
     split_col(-1),
@@ -125,22 +126,22 @@
     declareOption(ol, "right_leave", &RegressionTreeNode::right_leave, OptionBase::learntoption,
                   "The leave with the rows greater thean the split feature value after split\n");
 
-    declareOption(ol, "left_error", &RegressionTreeNode::tmp_vec,
+    declareStaticOption(ol, "left_error", &RegressionTreeNode::tmp_vec,
                   OptionBase::learntoption | OptionBase::nosave,
                   "DEPRECATED The left leave error vector\n");
-    declareOption(ol, "right_error", &RegressionTreeNode::tmp_vec,
+    declareStaticOption(ol, "right_error", &RegressionTreeNode::tmp_vec,
                   OptionBase::learntoption | OptionBase::nosave,
                   "DEPRECATED The right leave error vector\n");
-    declareOption(ol, "missing_error", &RegressionTreeNode::tmp_vec,
+    declareStaticOption(ol, "missing_error", &RegressionTreeNode::tmp_vec,
                   OptionBase::learntoption | OptionBase::nosave,
                   "DEPRECATED The missing leave error vector\n");
-    declareOption(ol, "left_output", &RegressionTreeNode::tmp_vec,
+    declareStaticOption(ol, "left_output", &RegressionTreeNode::tmp_vec,
                   OptionBase::learntoption | OptionBase::nosave,
                   "DEPRECATED The left leave output vector\n");
-    declareOption(ol, "right_output", &RegressionTreeNode::tmp_vec,
+    declareStaticOption(ol, "right_output", &RegressionTreeNode::tmp_vec,
                   OptionBase::learntoption | OptionBase::nosave,
                   "DEPRECATED The right leave output vector\n");
-    declareOption(ol, "missing_output", &RegressionTreeNode::tmp_vec,
+    declareStaticOption(ol, "missing_output", &RegressionTreeNode::tmp_vec,
                   OptionBase::learntoption | OptionBase::nosave,
                   "DEPRECATED The mising leave output vector\n");
 
@@ -249,7 +250,7 @@
     Vec registered_target(0, leave->length()); 
     Vec registered_weight(0, leave->length());
     Vec registered_value(0, leave->length());
-   tmp_vec.resize(2);
+    tmp_vec.resize(2);
     Vec left_error(3);
     Vec right_error(3);
     Vec missing_error(3);

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.h	2009-02-18 21:24:04 UTC (rev 9945)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.h	2009-02-19 18:46:01 UTC (rev 9946)
@@ -89,7 +89,7 @@
     PP<RegressionTreeLeave> right_leave;
     
     static int dummy_int;
-    Vec tmp_vec;
+    static Vec tmp_vec;
 public:  
     RegressionTreeNode();
     RegressionTreeNode(int missing_is_valid);



From nouiz at mail.berlios.de  Fri Feb 20 21:57:22 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 20 Feb 2009 21:57:22 +0100
Subject: [Plearn-commits] r9947 - trunk/python_modules/plearn/parallel
Message-ID: <200902202057.n1KKvMh4020088@sheep.berlios.de>

Author: nouiz
Date: 2009-02-20 21:57:21 +0100 (Fri, 20 Feb 2009)
New Revision: 9947

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
removed useless parenthesis.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-02-19 18:46:01 UTC (rev 9946)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-02-20 20:57:21 UTC (rev 9947)
@@ -1344,7 +1344,7 @@
                 self.tasks_req.append(self.req+'&&(Machine=="'+m+'")')
 
         for m in self.machines:
-            machine_choice.append('(regexp("'+m+'", target.Machine))')
+            machine_choice.append('regexp("'+m+'", target.Machine)')
         if machine_choice:
             self.req+="&&(False "
             for m in machine_choice:



From nouiz at mail.berlios.de  Fri Feb 20 22:03:39 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 20 Feb 2009 22:03:39 +0100
Subject: [Plearn-commits] r9948 - trunk/python_modules/plearn/parallel
Message-ID: <200902202103.n1KL3dGt021226@sheep.berlios.de>

Author: nouiz
Date: 2009-02-20 22:03:35 +0100 (Fri, 20 Feb 2009)
New Revision: 9948

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
remove temp file now so to don't polluate the /tmp


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-02-20 20:57:21 UTC (rev 9947)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-02-20 21:03:35 UTC (rev 9948)
@@ -798,6 +798,7 @@
         self.machines = []
         self.to_all = False
         self.keep_failed_jobs_in_queue = False
+        self.clean_up = True
 
         DBIBase.__init__(self, commands, **args)
 
@@ -951,7 +952,10 @@
         pkdilly_fd = open( pkdilly_file, 'r' )
         lines = pkdilly_fd.readlines()
         pkdilly_fd.close()
-        self.temp_files.append(pkdilly_file)
+        if self.clean_up:
+            os.remove(pkdilly_file)
+        else:
+            self.temp_files.append(pkdilly_file)
 
         get=[]
         for line in lines:



From nouiz at mail.berlios.de  Fri Feb 20 22:49:38 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 20 Feb 2009 22:49:38 +0100
Subject: [Plearn-commits] r9949 - trunk/python_modules/plearn/parallel
Message-ID: <200902202149.n1KLncmt028325@sheep.berlios.de>

Author: nouiz
Date: 2009-02-20 22:49:38 +0100 (Fri, 20 Feb 2009)
New Revision: 9949

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
remove the kerberos ticket file.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-02-20 21:03:35 UTC (rev 9948)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-02-20 21:49:38 UTC (rev 9949)
@@ -1106,6 +1106,7 @@
                     echo "nb args: $#" 1>&2
                     echo "Running: command: \\"$@\\"" 1>&2
                     %s
+                    rm -f echo ${KRB5CCNAME:5}
                     '''%(bash_exec)))
             else:
                 fd.write(dedent('''\
@@ -1130,6 +1131,7 @@
                 pwd
                 echo "Running command: $argv"
                 $argv
+                rm -f `echo  $KRB5CCNAME| cut -d':' -f2`
                 '''))
             fd.close()
             if self.pkdilly:



From nouiz at mail.berlios.de  Mon Feb 23 17:20:09 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 23 Feb 2009 17:20:09 +0100
Subject: [Plearn-commits] r9950 - trunk/scripts
Message-ID: <200902231620.n1NGK9Z3017582@sheep.berlios.de>

Author: nouiz
Date: 2009-02-23 17:20:09 +0100 (Mon, 23 Feb 2009)
New Revision: 9950

Modified:
   trunk/scripts/perlgrep
Log:
perlgrep now look in .bat file.


Modified: trunk/scripts/perlgrep
===================================================================
--- trunk/scripts/perlgrep	2009-02-20 21:49:38 UTC (rev 9949)
+++ trunk/scripts/perlgrep	2009-02-23 16:20:09 UTC (rev 9950)
@@ -102,7 +102,7 @@
             { 
                 my @flist = lsdir($fname);
                 
-                @flist = grep { -d $_ or /Makefile|makefile|pytest\.config|
+                @flist = grep { -d $_ or /Makefile|makefile|pytest\.config|\.bat$|
                                     \.c$|\.C$|\.cc$|\.cpp$|\.CC$|\.h$|\.hpp$|\.tex$
                                    |\.plearn$|\.pyplearn$|\.psave$|\.vmat$|\.py$|\.pymat$
                                    |\.txt$|^readme|^Readme|^README/x } @flist;
@@ -129,7 +129,7 @@
 
 Will perform the specified grep operation on every file in the
 list and recursively in directories.  (only certain kinds of files are
-considered when recursing in directories, .c .C .cc .cpp .CC .h .hpp .txt .plearn .pyplearn .psave .py .vmat .pymat Makefile makefile readme Readme README .tex)
+considered when recursing in directories, .bat .c .C .cc .cpp .CC .h .hpp .txt .plearn .pyplearn .psave .py .vmat .pymat Makefile makefile readme Readme README .tex)
 
 Ex: perlgrep '\\bVMatrix\\b' .
 ";



From nouiz at mail.berlios.de  Mon Feb 23 22:35:40 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 23 Feb 2009 22:35:40 +0100
Subject: [Plearn-commits] r9951 - trunk/plearn/math
Message-ID: <200902232135.n1NLZeYK003410@sheep.berlios.de>

Author: nouiz
Date: 2009-02-23 22:35:40 +0100 (Mon, 23 Feb 2009)
New Revision: 9951

Modified:
   trunk/plearn/math/TMat_impl.h
   trunk/plearn/math/TVec_decl.h
Log:
added fct parameter to be: TVec<T>::sortingPermutation(bool stable)
this allow to return a stable sort.


Modified: trunk/plearn/math/TMat_impl.h
===================================================================
--- trunk/plearn/math/TMat_impl.h	2009-02-23 16:20:09 UTC (rev 9950)
+++ trunk/plearn/math/TMat_impl.h	2009-02-23 21:35:40 UTC (rev 9951)
@@ -242,11 +242,14 @@
 }
 // Actual body of the method
 template <class T>
-TVec<int> TVec<T>::sortingPermutation() const
+TVec<int> TVec<T>::sortingPermutation(bool stable) const
 {    
     TVec<int> indices(length_);
     for (int i=0; i < length_; i++) indices[i] = i;
-    sort(indices.begin(), indices.end(), index_cmp<T>(*this));
+    if(stable)
+        stable_sort(indices.begin(), indices.end(), index_cmp<T>(*this));
+    else        
+        sort(indices.begin(), indices.end(), index_cmp<T>(*this));
     return indices;
 }
 

Modified: trunk/plearn/math/TVec_decl.h
===================================================================
--- trunk/plearn/math/TVec_decl.h	2009-02-23 16:20:09 UTC (rev 9950)
+++ trunk/plearn/math/TVec_decl.h	2009-02-23 21:35:40 UTC (rev 9951)
@@ -535,8 +535,8 @@
     }
 
     //! Returns an index vector I so that (*this)(I) returns a sorted version
-    //! of this vec in ascending order.
-    TVec<int> sortingPermutation() const;
+    //! of this vec in ascending order. If stable is true, will return a stable sort
+    TVec<int> sortingPermutation(bool stable = false) const;
     
     int findSorted(T value) const
     {



From nouiz at mail.berlios.de  Tue Feb 24 00:15:29 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 24 Feb 2009 00:15:29 +0100
Subject: [Plearn-commits] r9952 - trunk/python_modules/plearn/parallel
Message-ID: <200902232315.n1NNFTeB011633@sheep.berlios.de>

Author: nouiz
Date: 2009-02-24 00:15:29 +0100 (Tue, 24 Feb 2009)
New Revision: 9952

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
full path to klist to be able to find it.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-02-23 21:35:40 UTC (rev 9951)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-02-23 23:15:29 UTC (rev 9952)
@@ -1094,7 +1094,7 @@
                     fd.write('source ' + self.source_file + '\n')
 
                 fd.write(dedent('''\
-                    klist
+                    /usr/kerberos/bin/klist
                     echo "Executing on " `/bin/hostname` 1>&2
                     echo "HOSTNAME: ${HOSTNAME}" 1>&2
                     echo "PATH: $PATH" 1>&2
@@ -1121,6 +1121,7 @@
                     fd.write('source ' + self.source_file + '\n')
 
                 fd.write(dedent('''\
+                /usr/kerberos/bin/klist
                 echo "Executing on " `/bin/hostname`
                 echo "HOSTNAME: ${HOSTNAME}"
                 echo "PATH: $PATH"



From nouiz at mail.berlios.de  Tue Feb 24 17:59:24 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 24 Feb 2009 17:59:24 +0100
Subject: [Plearn-commits] r9953 - trunk/plearn/math
Message-ID: <200902241659.n1OGxOBQ004109@sheep.berlios.de>

Author: nouiz
Date: 2009-02-24 17:59:23 +0100 (Tue, 24 Feb 2009)
New Revision: 9953

Modified:
   trunk/plearn/math/TMat_impl.h
   trunk/plearn/math/TVec_decl.h
Log:
added the missing parameter to the fct: TVec<T>::sortingPermutation(bool stable = false, bool missing = false)
when true, will use a cmp methode that handle correctly missing value.


Modified: trunk/plearn/math/TMat_impl.h
===================================================================
--- trunk/plearn/math/TMat_impl.h	2009-02-23 23:15:29 UTC (rev 9952)
+++ trunk/plearn/math/TMat_impl.h	2009-02-24 16:59:23 UTC (rev 9953)
@@ -236,20 +236,38 @@
   struct index_cmp : public binary_function<int, int, bool>
   {
       const TVec<T>& m_values;
-      index_cmp(const Vec& values): m_values(values) { }        
+      index_cmp(const Vec& values): m_values(values) { }
       bool operator()(int x, int y) { return m_values[x] < m_values[y]; }
   };
+  template <class T>
+  struct index_missing_cmp : public binary_function<int, int, bool>
+  {
+      const TVec<T>& m_values;
+      index_missing_cmp(const Vec& values): m_values(values) { }
+      bool operator()(int x, int y) {
+          T v1=m_values[x];
+          T v2=m_values[y];
+          if(is_missing(v1) && is_missing(v2)) return false;
+          else if(is_missing(v1)) return false;
+          else if(is_missing(v2)) return true;
+          return v1 < v2;
+      }
+  };
 }
 // Actual body of the method
 template <class T>
-TVec<int> TVec<T>::sortingPermutation(bool stable) const
+TVec<int> TVec<T>::sortingPermutation(bool stable, bool missing) const
 {    
     TVec<int> indices(length_);
     for (int i=0; i < length_; i++) indices[i] = i;
-    if(stable)
+    if(stable && ! missing)
         stable_sort(indices.begin(), indices.end(), index_cmp<T>(*this));
-    else        
+    else if(! stable && !missing)
         sort(indices.begin(), indices.end(), index_cmp<T>(*this));
+    else if(stable && missing)
+        stable_sort(indices.begin(), indices.end(),index_missing_cmp<T>(*this));
+    else if(!stable && missing)
+        sort(indices.begin(), indices.end(), index_missing_cmp<T>(*this));
     return indices;
 }
 

Modified: trunk/plearn/math/TVec_decl.h
===================================================================
--- trunk/plearn/math/TVec_decl.h	2009-02-23 23:15:29 UTC (rev 9952)
+++ trunk/plearn/math/TVec_decl.h	2009-02-24 16:59:23 UTC (rev 9953)
@@ -536,7 +536,7 @@
 
     //! Returns an index vector I so that (*this)(I) returns a sorted version
     //! of this vec in ascending order. If stable is true, will return a stable sort
-    TVec<int> sortingPermutation(bool stable = false) const;
+    TVec<int> sortingPermutation(bool stable = false, bool missing = false) const;
     
     int findSorted(T value) const
     {



From nouiz at mail.berlios.de  Tue Feb 24 20:11:03 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 24 Feb 2009 20:11:03 +0100
Subject: [Plearn-commits] r9954 - trunk/plearn_learners/regressors
Message-ID: <200902241911.n1OJB35m000679@sheep.berlios.de>

Author: nouiz
Date: 2009-02-24 20:11:03 +0100 (Tue, 24 Feb 2009)
New Revision: 9954

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.h
Log:
use a stable sort for the index. This give a 5% speed up in my test.


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-02-24 16:59:23 UTC (rev 9953)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-02-24 19:11:03 UTC (rev 9954)
@@ -219,10 +219,10 @@
     getAllRegisteredRow(leave_id,col,reg);
     target.resize(reg.length());
     weight.resize(reg.length());
-    value.resize(reg.length());    
+    value.resize(reg.length());
     if(weightsize() <= 0){
         weight.fill(1.0 / length());
-        for(int i=0;i<reg.length();i++){            
+        for(int i=0;i<reg.length();i++){
             target[i] = target_weight[int(reg[i])].first;
             value[i]  = tsource->get(col, reg[i]);
         }
@@ -274,7 +274,7 @@
     string f=source->getMetaDataDir()+"RTR_tsorted_row.psave";
 
     if(isUpToDate(f)){
-        DBG_LOG<<"RegressionTreeRegisters:: Reloading the sorted source VMatrix"<<endl;
+        DBG_LOG<<"RegressionTreeRegisters:: Reloading the sorted source VMatrix: "<<f<<endl;
         PLearn::load(f,tsorted_row);
         return;
     }
@@ -300,8 +300,9 @@
         sortEachDim(sample_dim);
         if (report_progress) pb->update(sample_dim+1);
     }
+    if (report_progress) pb->close();//in case of parallel sort.
     if(source->hasMetaDataDir()){
-        DBG_LOG<<"RegressionTreeRegisters:: Saving the sorted source VMatrix"<<endl;
+        DBG_LOG<<"RegressionTreeRegisters:: Saving the sorted source VMatrix: "<<f<<endl;
         PLearn::save(f,tsorted_row);
     }else{
         DBG_LOG<<"RegressionTreeRegisters:: can't save the sorted source VMatrix as we don't have a metadatadir"<<endl;
@@ -311,110 +312,32 @@
   
 void RegressionTreeRegisters::sortEachDim(int dim)
 {
-    int start_index = 0;
-    int end_index = length() - 1;
-    int forward_index;
-    int backward_index;
-    int stack_index = -1;
-    TVec<int> stack(50);
-    for (;;)
-    {
-        if ((end_index - start_index) < 7)
-        {
-            if (end_index > start_index)
-            {
-                sortSmallSubArray(start_index, end_index, dim);
-            }
-            if (stack_index < 0)
-            {
-                break;
-            }
-            end_index = stack[stack_index--];
-            start_index = stack[stack_index--];
-        }
-        else
-        {
-            swapIndex(start_index + 1, (start_index + end_index) / 2, dim);
-            if (compare(tsource->get(dim, tsorted_row(dim, start_index)),
-                        tsource->get(dim, tsorted_row(dim, end_index))) > 0.0)
-                swapIndex(start_index, end_index, dim);
-            if (compare(tsource->get(dim, tsorted_row(dim, start_index + 1)),
-                        tsource->get(dim, tsorted_row(dim, end_index))) > 0.0)
-                swapIndex(start_index + 1, end_index, dim);
-            if (compare(tsource->get(dim, tsorted_row(dim, start_index)),
-                        tsource->get(dim, tsorted_row(dim, start_index + 1))) > 0.0)
-                swapIndex(start_index, start_index + 1, dim);
-            forward_index = start_index + 1;
-            backward_index = end_index;
-            real sample_feature = tsource->get(dim, tsorted_row(dim, start_index + 1));
-            for (;;)
-            {
-                do forward_index++; while (compare(tsource->get(dim, tsorted_row(dim, forward_index)), sample_feature) < 0.0);
-                do backward_index--; while (compare(tsource->get(dim, tsorted_row(dim, backward_index)), sample_feature) > 0.0);
-                if (backward_index < forward_index)
-                {
-                    break;
-                }
-                swapIndex(forward_index, backward_index, dim);
-            }
-            swapIndex(start_index + 1, backward_index, dim);
-            stack_index += 2;
-            if (stack_index > 50)
-                PLERROR("RegressionTreeRegistersVMatrix: the stack for sorting the rows is too small");
-            if ((end_index - forward_index + 1) >= (backward_index - start_index))
-            {
-                stack[stack_index] = end_index;
-                stack[stack_index - 1] = forward_index;
-                end_index = backward_index - 1;
-            }
-            else
-            {
-                stack[stack_index] = backward_index - 1;
-                stack[stack_index - 1] = start_index;
-                start_index = forward_index;
-            }
-        }
+    PLCHECK(tsource->classname()=="MemoryVMatrixNoSave");
+    Mat m = tsource.toMat();
+    Vec v = m(dim);
+    TVec<int> order = v.sortingPermutation(true, true);
+    tsorted_row(dim)<<order;
+
+#ifndef NDEBUG
+    for(int i=0;i<length()-1;i++){
+        int reg1 = tsorted_row(dim,i);
+        int reg2 = tsorted_row(dim,i+1);
+        real v1 = tsource(dim,reg1);
+        real v2 = tsource(dim,reg2);
+//check that the sort is valid.
+        PLASSERT(v1<=v2);
+//check that the sort is stable
+        if(v1==v2 && reg1>reg2)
+            PLWARNING("In RegressionTreeRegisters::sortEachDim(%d) - "
+                      "sort is not stable. make it stable to be more optimized:"
+                      " reg1=%d, reg2=%d, v1=%f, v2=%f", 
+                      reg1, reg2, v1, v2);
     }
-}
-  
-void RegressionTreeRegisters::sortSmallSubArray(int the_start_index, int the_end_index, int dim)
-{
-    for (int next_train_sample_index = the_start_index + 1;
-         next_train_sample_index <= the_end_index;
-         next_train_sample_index++)
-    {
-        int saved_index = tsorted_row(dim, next_train_sample_index);
-        real sample_feature = tsource->get(dim,saved_index);
-        int each_train_sample_index;
-        for (each_train_sample_index = next_train_sample_index - 1;
-             each_train_sample_index >= the_start_index;
-             each_train_sample_index--)
-        {
-            if (compare(tsource->get(dim,tsorted_row(dim, each_train_sample_index)), sample_feature) <= 0.0)
-            {
-                break;
-            }
-            tsorted_row(dim, each_train_sample_index + 1) = tsorted_row(dim, each_train_sample_index);
-        }
-        tsorted_row(dim, each_train_sample_index + 1) = saved_index;
-    }  
-}
+#endif
+    return;
 
-void RegressionTreeRegisters::swapIndex(int index_i, int index_j, int dim)
-{
-    int saved_index = tsorted_row(dim, index_i);
-    tsorted_row(dim, index_i) = tsorted_row(dim, index_j);
-    tsorted_row(dim, index_j) = saved_index;
 }
 
-real RegressionTreeRegisters::compare(real field1, real field2)
-{
-    if (is_missing(field1) && is_missing(field2)) return 0.0;
-    if (is_missing(field1)) return -1.0;
-    if (is_missing(field2)) return +1.0;
-    return field1 - field2;
-}
-
 void RegressionTreeRegisters::printRegisters()
 {
     cout << " register:  ";

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2009-02-24 16:59:23 UTC (rev 9953)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2009-02-24 19:11:03 UTC (rev 9954)
@@ -148,9 +148,6 @@
     void         build_();
     void         sortRows();
     void         sortEachDim(int dim);
-    void         sortSmallSubArray(int start_index, int end_index, int dim);
-    void         swapIndex(int index_i, int index_j, int dim);
-    real         compare(real field1, real field2);
     void         verbose(string msg, int level);
 
 };



From nouiz at mail.berlios.de  Tue Feb 24 20:23:31 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 24 Feb 2009 20:23:31 +0100
Subject: [Plearn-commits] r9955 - trunk/plearn_learners/regressors
Message-ID: <200902241923.n1OJNVvt001686@sheep.berlios.de>

Author: nouiz
Date: 2009-02-24 20:23:30 +0100 (Tue, 24 Feb 2009)
New Revision: 9955

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
small fix that lower the memory usage.


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-02-24 19:11:03 UTC (rev 9954)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-02-24 19:23:30 UTC (rev 9955)
@@ -180,7 +180,7 @@
         tsource = VMat(new TransposeVMatrix(new SubVMatrix(
                                                 source, 0,0,source->length(),
                                                 source->inputsize())));
-        if(do_sort_rows){
+        if(mem_tsource){
             PP<MemoryVMatrixNoSave> tmp = new MemoryVMatrixNoSave(tsource);
             tsource = VMat(tmp);
         }



From nouiz at mail.berlios.de  Wed Feb 25 19:10:32 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 25 Feb 2009 19:10:32 +0100
Subject: [Plearn-commits] r9956 - trunk/plearn/vmat
Message-ID: <200902251810.n1PIAWJQ030120@sheep.berlios.de>

Author: nouiz
Date: 2009-02-25 19:10:24 +0100 (Wed, 25 Feb 2009)
New Revision: 9956

Modified:
   trunk/plearn/vmat/VariableDeletionVMatrix.cc
Log:
we display in a more compact way the variable that are constant.


Modified: trunk/plearn/vmat/VariableDeletionVMatrix.cc
===================================================================
--- trunk/plearn/vmat/VariableDeletionVMatrix.cc	2009-02-24 19:23:30 UTC (rev 9955)
+++ trunk/plearn/vmat/VariableDeletionVMatrix.cc	2009-02-25 18:10:24 UTC (rev 9956)
@@ -317,16 +317,23 @@
     // Then remove columns that are too constant.
     TVec<int> final_indices;
     if (is_equal(max_constant_threshold,1)){
+        TVec<int> const_indices;
         for (int k = 0; k < indices.length(); k++) {
             int i = indices[k];
             StatsCollector stat = stats[i];
             if(!(stat.min()==stat.max() && stat.nnonmissing()>0))
                 final_indices.append(i);
             else if (warn_removed_var)
-                PLWARNING("In VariableDeletionVMatrix::build_() var '%s'"
-                          " is constant with value: %f. We remove it.",
-                          source->fieldName(i).c_str(), stat.min());
+                const_indices.append(i);
         }
+        if(warn_removed_var && const_indices.length()>0){
+            NORMAL_LOG<<" WARNING: In VariableDeletionVMatrix::build_() - The following tuple (variable, constant value) indicate variable that are removed because they are constant: " <<endl;
+            for(int i=0;i<const_indices.length();i++){
+                StatsCollector stat = stats[i];
+                NORMAL_LOG<<"("<<source->fieldName(i)<<","<<stat.min()<<"),";
+            }
+            NORMAL_LOG<<endl;
+        }
         indices.resize(final_indices.length());
         indices << final_indices; 
     }else if (max_constant_threshold > 0){



From nouiz at mail.berlios.de  Wed Feb 25 22:57:45 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 25 Feb 2009 22:57:45 +0100
Subject: [Plearn-commits] r9957 - trunk/plearn/vmat
Message-ID: <200902252157.n1PLvj6x030473@sheep.berlios.de>

Author: nouiz
Date: 2009-02-25 22:57:42 +0100 (Wed, 25 Feb 2009)
New Revision: 9957

Modified:
   trunk/plearn/vmat/DichotomizeVMatrix.cc
   trunk/plearn/vmat/MissingIndicatorVMatrix.cc
   trunk/plearn/vmat/MissingInstructionVMatrix.cc
   trunk/plearn/vmat/SubVMatrix.cc
Log:
made SubVMatrix, MissingIndicatorVMatrix, MissingInstructionVMatrix, and DichotomizeVMatrix compabible with extra fields.


Modified: trunk/plearn/vmat/DichotomizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/DichotomizeVMatrix.cc	2009-02-25 18:10:24 UTC (rev 9956)
+++ trunk/plearn/vmat/DichotomizeVMatrix.cc	2009-02-25 21:57:42 UTC (rev 9957)
@@ -153,7 +153,6 @@
 
     updateMtime(source);
 
-    instruction_index.fill(-1);
     TVec<string> source_names = source->fieldNames();
 
     instruction_index.resize(source->width());
@@ -185,9 +184,11 @@
     length_=source->length();
     int sisize=source->inputsize();
     int stsize=source->targetsize();
+    int swsize=source->weightsize();
     int isize=0;
     int tsize=0;
     int wsize=0;
+    int esize=0;
     for (int source_col = 0; source_col < sisize; source_col++)
     {
         if (instruction_index[source_col] < 0) isize++;
@@ -210,7 +211,7 @@
         }
     }
     for (int source_col = sisize + stsize;
-         source_col < sisize + stsize + source->weightsize() ; source_col++)
+         source_col < sisize + stsize + swsize ; source_col++)
     {
         if (instruction_index[source_col] < 0) wsize++;
         else
@@ -220,8 +221,19 @@
             wsize += instruction_ptr.size();
         }
     }
-    defineSizes(isize, tsize, wsize);
-    width_ = isize + tsize + wsize;
+    for (int source_col = sisize + stsize + swsize;
+         source_col < sisize + stsize + swsize + source->extrasize() ; source_col++)
+    {
+        if (instruction_index[source_col] < 0) esize++;
+        else
+        {
+            TVec<pair<real, real> > instruction_ptr = 
+                discrete_variable_instructions[instruction_index[source_col]].second;
+            esize += instruction_ptr.size();
+        }
+    }
+    defineSizes(isize, tsize, wsize, esize);
+    width_ = isize + tsize + wsize + esize;
 
     //get the fieldnames
     TVec<string> fnames(width());

Modified: trunk/plearn/vmat/MissingIndicatorVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MissingIndicatorVMatrix.cc	2009-02-25 18:10:24 UTC (rev 9956)
+++ trunk/plearn/vmat/MissingIndicatorVMatrix.cc	2009-02-25 21:57:42 UTC (rev 9957)
@@ -266,9 +266,8 @@
       new_col += 1;
     }
     length_ = source->length();
-    inputsize_ = source_inputsize + added_colomns;
-    targetsize_ = source->targetsize();
-    weightsize_ = source->weightsize();
+    defineSizes(source_inputsize + added_colomns, source->targetsize(), source->weightsize(), source->extrasize());
+
     source_input.resize(source_inputsize);
     declareFieldNames(new_field_names);
 

Modified: trunk/plearn/vmat/MissingInstructionVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MissingInstructionVMatrix.cc	2009-02-25 18:10:24 UTC (rev 9956)
+++ trunk/plearn/vmat/MissingInstructionVMatrix.cc	2009-02-25 21:57:42 UTC (rev 9957)
@@ -156,6 +156,7 @@
     int skip_instruction_input = 0;
     int skip_instruction_target = 0;
     int skip_instruction_weight = 0;
+    int skip_instruction_extra = 0;
     if(default_instruction!="skip")
         width_=source->width();
     else{
@@ -221,16 +222,20 @@
         if (ins[source_col] == "skip"){
             if(source_col<source->inputsize())
                 skip_instruction_input++;
-            else if(source_col<(source->inputsize()+source->targetsize()))
+            else if(source_col<(source->inputsize() + source->targetsize()))
                 skip_instruction_target++;
-            else skip_instruction_weight++;
+            else if (source_col<(source->inputsize() + source->targetsize()
+                                 + source->weightsize()))
+                skip_instruction_weight++;
+            else skip_instruction_extra++;
         }
     }
     setMetaInfoFromSource();
-    inputsize_ = source->inputsize() - skip_instruction_input;
-    targetsize_ = source->targetsize() - skip_instruction_target;
-    weightsize_ = source->weightsize() - skip_instruction_weight;
-    width_ = inputsize_ + targetsize_ + weightsize_;
+    defineSizes(source->inputsize() - skip_instruction_input,
+                source->targetsize() - skip_instruction_target,
+                source->weightsize() - skip_instruction_weight,
+                source->extrasize() - skip_instruction_extra);
+    width_ = inputsize_ + targetsize_ + weightsize_ + extrasize_;
     int missing_instruction = 0;
     for (int col = 0; col < source->width(); col++)
     {

Modified: trunk/plearn/vmat/SubVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SubVMatrix.cc	2009-02-25 18:10:24 UTC (rev 9956)
+++ trunk/plearn/vmat/SubVMatrix.cc	2009-02-25 21:57:42 UTC (rev 9957)
@@ -199,7 +199,7 @@
 //*
     bool sizes_are_inconsistent =
         inputsize_ < 0 || targetsize_ < 0 || weightsize_ < 0 ||
-        inputsize_ + targetsize_ + weightsize_ != width();
+        inputsize_ + targetsize_ + weightsize_ + extrasize_ != width();
     if( sizes_are_inconsistent )
     {
         int source_is = source->inputsize();



From nouiz at mail.berlios.de  Wed Feb 25 22:58:15 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 25 Feb 2009 22:58:15 +0100
Subject: [Plearn-commits] r9958 - trunk/plearn/vmat
Message-ID: <200902252158.n1PLwFHp030690@sheep.berlios.de>

Author: nouiz
Date: 2009-02-25 22:58:12 +0100 (Wed, 25 Feb 2009)
New Revision: 9958

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
corrected warning.


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2009-02-25 21:57:42 UTC (rev 9957)
+++ trunk/plearn/vmat/VMatrix.cc	2009-02-25 21:58:12 UTC (rev 9958)
@@ -2242,14 +2242,15 @@
 
     if (width_ < 0 && v <= -1) {
         if (warn_if_cannot_compute)
-            PLWARNING("In VMatrix::computeMissingSizeValue - Cannot compute "
-                      "the missing size value when the width is undefined");
+            PLWARNING("In VMatrix::computeMissingSizeValue for %s - Cannot "
+                      "compute the missing size value when the width is undefined",
+                      classname().c_str());
         return;
     }
 
     if(v < -1){
         if(warn_if_cannot_compute)
-            PLWARNING("In VMatrix::compute_missing_size_value() - in class %s"
+            PLWARNING("In VMatrix::computeMissingSizeValue() - in class %s"
                       " more then one of"
                       " inputsize(%d), targetsize(%d), weightsize(%d) and"
                       " extrasize(%d) is unknow so we cannot compute them with"
@@ -2259,7 +2260,7 @@
         return;
     }else if(v==0 && warn_if_size_mismatch && width_ >= 0 &&
             width_ != inputsize_ + targetsize_ + weightsize_ + extrasize_)
-        PLWARNING("In VMatrix::compute_missing_size_value() for class %s - "
+        PLWARNING("In VMatrix::computeMissingSizeValue() for class %s - "
                   "inputsize_(%d) + targetsize_(%d) + weightsize_(%d) + "
                   "extrasize_(%d) != width_(%d) !",
                   classname().c_str(), inputsize_, targetsize_, weightsize_,



From nouiz at mail.berlios.de  Wed Feb 25 22:59:08 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 25 Feb 2009 22:59:08 +0100
Subject: [Plearn-commits] r9959 - trunk/plearn/vmat
Message-ID: <200902252159.n1PLx87l030820@sheep.berlios.de>

Author: nouiz
Date: 2009-02-25 22:59:07 +0100 (Wed, 25 Feb 2009)
New Revision: 9959

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
fixed comment.


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2009-02-25 21:58:12 UTC (rev 9958)
+++ trunk/plearn/vmat/VMatrix.cc	2009-02-25 21:59:07 UTC (rev 9959)
@@ -2219,9 +2219,9 @@
     return;
 }
 
-/////////////////////////
-// max_fieldnames_size //
-/////////////////////////
+///////////////////////
+// maxFieldNamesSize //
+///////////////////////
 int VMatrix::maxFieldNamesSize() const
 {
     uint size_fieldnames=0;
@@ -2231,9 +2231,9 @@
     return size_fieldnames;
 }
 
-////////////////////////////////
-// compute_missing_size_value //
-////////////////////////////////
+/////////////////////////////
+// computeMissingSizeValue //
+/////////////////////////////
 void VMatrix::computeMissingSizeValue(bool warn_if_cannot_compute,
                                       bool warn_if_size_mismatch)
 {



From nouiz at mail.berlios.de  Thu Feb 26 17:18:51 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 26 Feb 2009 17:18:51 +0100
Subject: [Plearn-commits] r9960 - trunk/plearn_learners/testers
Message-ID: <200902261618.n1QGIpDU026086@sheep.berlios.de>

Author: nouiz
Date: 2009-02-26 17:18:50 +0100 (Thu, 26 Feb 2009)
New Revision: 9960

Modified:
   trunk/plearn_learners/testers/PTester.cc
Log:
save file with .vmat extension instead of .psave. The comment was telling .vmat.


Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2009-02-25 21:59:07 UTC (rev 9959)
+++ trunk/plearn_learners/testers/PTester.cc	2009-02-26 16:18:50 UTC (rev 9960)
@@ -513,7 +513,7 @@
     if (should_train) {
         VMat trainset = dsets[0];
         if (is_splitdir && save_data_sets)
-            PLearn::save(splitdir / "training_set.psave", trainset);
+            PLearn::save(splitdir / "training_set.vmat", trainset);
             
         if (provide_learner_expdir)
         {
@@ -594,7 +594,7 @@
             PP<VecStatsCollector> test_stats = stcol[setnum];
             const string setname = "test" + tostring(setnum);
             if (is_splitdir && save_data_sets)
-                PLearn::save(splitdir / (setname + "_set.psave"), testset);
+                PLearn::save(splitdir / (setname + "_set.vmat"), testset);
 
             // QUESTION Why is this done so late? Can't it be moved
             // somewhere earlier? At least before the save_data_sets?



From nouiz at mail.berlios.de  Thu Feb 26 18:38:04 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 26 Feb 2009 18:38:04 +0100
Subject: [Plearn-commits] r9961 - in trunk: plearn/vmat
	plearn_learners/distributions/test/.pytest/PL_RandomGaussMix/expected_results/random_gaussmix_samples_test_metadata
Message-ID: <200902261738.n1QHc40C006489@sheep.berlios.de>

Author: nouiz
Date: 2009-02-26 18:38:02 +0100 (Thu, 26 Feb 2009)
New Revision: 9961

Added:
   trunk/plearn_learners/distributions/test/.pytest/PL_RandomGaussMix/expected_results/random_gaussmix_samples_test_metadata/fieldnames
   trunk/plearn_learners/distributions/test/.pytest/PL_RandomGaussMix/expected_results/random_gaussmix_samples_test_metadata/sizes
Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
In VMatrix::setMetaInfoFrom save the metainfo if we have a metadatadir. This is for the case when the program crash, we won't have it save in FileVMatrix. Fix broken test.


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2009-02-26 16:18:50 UTC (rev 9960)
+++ trunk/plearn/vmat/VMatrix.cc	2009-02-26 17:38:02 UTC (rev 9961)
@@ -1359,6 +1359,10 @@
             // number): we can get its string mapping.
             setStringMapping(i, vm->getStringToRealMapping(vm_index));
     }
+
+    //we save it now in case the program crash
+    if(hasMetaDataDir())
+        saveFieldInfos();
 }
 
 ////////////////////

Added: trunk/plearn_learners/distributions/test/.pytest/PL_RandomGaussMix/expected_results/random_gaussmix_samples_test_metadata/fieldnames
===================================================================
--- trunk/plearn_learners/distributions/test/.pytest/PL_RandomGaussMix/expected_results/random_gaussmix_samples_test_metadata/fieldnames	2009-02-26 16:18:50 UTC (rev 9960)
+++ trunk/plearn_learners/distributions/test/.pytest/PL_RandomGaussMix/expected_results/random_gaussmix_samples_test_metadata/fieldnames	2009-02-26 17:38:02 UTC (rev 9961)
@@ -0,0 +1,3 @@
+0	0
+1	0
+2	0

Added: trunk/plearn_learners/distributions/test/.pytest/PL_RandomGaussMix/expected_results/random_gaussmix_samples_test_metadata/sizes
===================================================================
--- trunk/plearn_learners/distributions/test/.pytest/PL_RandomGaussMix/expected_results/random_gaussmix_samples_test_metadata/sizes	2009-02-26 16:18:50 UTC (rev 9960)
+++ trunk/plearn_learners/distributions/test/.pytest/PL_RandomGaussMix/expected_results/random_gaussmix_samples_test_metadata/sizes	2009-02-26 17:38:02 UTC (rev 9961)
@@ -0,0 +1 @@
+3 0 0 0 



From nouiz at mail.berlios.de  Thu Feb 26 18:39:21 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 26 Feb 2009 18:39:21 +0100
Subject: [Plearn-commits] r9962 - trunk/python_modules/plearn/pymake
Message-ID: <200902261739.n1QHdLXF006977@sheep.berlios.de>

Author: nouiz
Date: 2009-02-26 18:39:19 +0100 (Thu, 26 Feb 2009)
New Revision: 9962

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
a few change in the printing. print less stuff in v2 mode.


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2009-02-26 17:38:02 UTC (rev 9961)
+++ trunk/python_modules/plearn/pymake/pymake.py	2009-02-26 17:39:19 UTC (rev 9962)
@@ -375,7 +375,8 @@
     return files_to_copy
 
 def copy_ofiles_locally(executables_to_link):
-    print '++++ Copying remaining ofiles locally for ', string.join(map(lambda x: x.filebase, executables_to_link))
+    if verbose > 1:
+        print '++++ Copying remaining ofiles locally for ', string.join(map(lambda x: x.filebase, executables_to_link))
     files_to_copy= get_ofiles_to_copy(executables_to_link)
     for f in files_to_copy:
         copy_ofile_locally(f)
@@ -1106,7 +1107,8 @@
                 link_exit_code = 1
         else:
             if not local_compilation:
-                print 'Waiting for NFS to catch up...',
+                if verbose>=2:
+                    print 'Waiting for NFS to catch up...',
                 # Flush stdhout to make sure the previous message shows up,
                 # so if wait_for_all_...() doesn't work properly, then at
                 # least we learn we are waiting for NFS now and not when we
@@ -1955,7 +1957,6 @@
                 print self.filepath, "not handled here."
                 return False
         except OSError: # OSError: [Errno 116] Stale NFS file handle
-            print "OSError... (probably NFS latency); Will be retrying"
             return False
         
     def corresponding_ofile_is_up_to_date(self):
@@ -2251,6 +2252,8 @@
         else:
             self.retry_compilation = False
 
+        if msg and verbose == 1 and self.hostname!="localhost":
+            print "On host:",self.hostname
         if msg:
             print self.filebase+self.fileext,':', string.join(msg,'')
 
@@ -3027,6 +3030,10 @@
                 print str(len(ccfiles_to_compile))+'/'+str(len(ccfiles_to_link)),
                 print 'files. '+str(ccf)+' code and '+str(hf),
                 print 'headers file were modified.'
+                if verbose >=4:
+                    for i in ccfiles_to_compile:
+                        if i.file_is_modified():
+                            print i.filename(),"were modified"
 
             if platform=='win32':
                 win32_parallel_compile(ccfiles_to_compile.keys())



From nouiz at mail.berlios.de  Thu Feb 26 19:14:26 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 26 Feb 2009 19:14:26 +0100
Subject: [Plearn-commits] r9963 - trunk/plearn_learners/regressors
Message-ID: <200902261814.n1QIEQ3P014467@sheep.berlios.de>

Author: nouiz
Date: 2009-02-26 19:14:25 +0100 (Thu, 26 Feb 2009)
New Revision: 9963

Added:
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.cc
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.h
Log:
first implementation of RegressionTreeMulticlassLeaveFast. We had some restriction as the number of class should be named from 0 to nb_class -1 to have better speed.


Copied: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.cc (from rev 9955, trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc)
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2009-02-24 19:23:30 UTC (rev 9955)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.cc	2009-02-26 18:14:25 UTC (rev 9963)
@@ -0,0 +1,271 @@
+// -*- C++ -*-
+
+// RegressionTreeMulticlassLeaveFast.cc
+// Copyright (c) 1998-2002 Pascal Vincent
+// Copyright (C) 1999-2002 Yoshua Bengio and University of Montreal
+// Copyright (c) 2002 Jean-Sebastien Senecal, Xavier Saint-Mleux, Rejean Ducharme
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* ********************************************************************************    
+ * $Id: RegressionTreeMulticlassLeaveFast.cc, v 1.0 2004/07/19 10:00:00 Bengio/Kegl/Godbout    *
+ * This file is part of the PLearn library.                                     *
+ ******************************************************************************** */
+
+#include "RegressionTreeMulticlassLeaveFast.h"
+#include "RegressionTreeRegisters.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(RegressionTreeMulticlassLeaveFast,
+                        "Object to represent the leaves of a regression tree.",
+                        "It maintains the necessary statistics to compute the output and the train error\n"
+                        "of the samples in the leave.\n"
+    );
+
+RegressionTreeMulticlassLeaveFast::RegressionTreeMulticlassLeaveFast()
+    : nb_class(-1),
+      objective_function("l1")
+{
+    build();
+}
+
+RegressionTreeMulticlassLeaveFast::~RegressionTreeMulticlassLeaveFast()
+{
+}
+
+void RegressionTreeMulticlassLeaveFast::declareOptions(OptionList& ol)
+{ 
+    declareOption(ol, "nb_class", 
+                  &RegressionTreeMulticlassLeaveFast::nb_class,
+                  OptionBase::buildoption,
+                  "The number of class. Should be numbered from 0 to nb_class -1.\n"
+                  );
+    declareOption(ol, "objective_function",
+                  &RegressionTreeMulticlassLeaveFast::objective_function,
+                  OptionBase::buildoption,
+                  "The function to be used to compute the leave error.\n"
+                  "Current supported values are l1 and l2 (default is l1).");
+      
+    declareOption(ol, "multiclass_weights_sum",
+                  &RegressionTreeMulticlassLeaveFast::multiclass_weights_sum,
+                  OptionBase::learntoption,
+                  "A vector to count the weight sum of each possible output "
+                  "for the sample in this leave.\n");
+    declareOption(ol, "l1_loss_function_factor",
+                  &RegressionTreeMulticlassLeaveFast::l1_loss_function_factor,
+                  OptionBase::learntoption,
+                  "2 / loss_function_weight.\n");
+    declareOption(ol, "l2_loss_function_factor",
+                  &RegressionTreeMulticlassLeaveFast::l2_loss_function_factor,
+                  OptionBase::learntoption,
+                  "2 / pow(loss_function_weight, 2.0).\n");
+    inherited::declareOptions(ol);
+}
+
+void RegressionTreeMulticlassLeaveFast::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+    deepCopyField(objective_function, copies);
+    deepCopyField(l1_loss_function_factor, copies);
+    deepCopyField(l2_loss_function_factor, copies);
+    deepCopyField(multiclass_weights_sum, copies);
+}
+
+void RegressionTreeMulticlassLeaveFast::build()
+{
+    inherited::build();
+    build_();
+}
+
+void RegressionTreeMulticlassLeaveFast::build_()
+{
+}
+
+void RegressionTreeMulticlassLeaveFast::initStats()
+{
+    length_ = 0;
+    weights_sum = 0.0;
+    if (loss_function_weight != 0.0)
+    {
+        l1_loss_function_factor = 2.0 / loss_function_weight;
+        l2_loss_function_factor = 2.0 / pow(loss_function_weight, 2);
+    }
+    else
+    {
+        l1_loss_function_factor = 1.0;
+        l2_loss_function_factor = 1.0;
+    }
+    multiclass_weights_sum.resize(nb_class);
+    multiclass_weights_sum.fill(0);
+}
+
+void RegressionTreeMulticlassLeaveFast::addRow(int row)
+{
+    real weight = train_set->getWeight(row);
+    real target = train_set->getTarget(row);
+    addRow(row, target, weight);
+}
+
+void RegressionTreeMulticlassLeaveFast::addRow(int row, real target, real weight,
+                                 Vec outputv, Vec errorv)
+{
+    addRow(row, target, weight);
+    getOutputAndError(outputv,errorv);
+}
+
+void RegressionTreeMulticlassLeaveFast::addRow(int row, real target, real weight)
+{
+    length_ += 1;
+    weights_sum += weight;
+    multiclass_weights_sum[int(target)] += weight;
+}
+
+void RegressionTreeMulticlassLeaveFast::addRow(int row, Vec outputv, Vec errorv)
+{
+    addRow(row);
+    getOutputAndError(outputv,errorv);    
+}
+
+void RegressionTreeMulticlassLeaveFast::removeRow(int row, Vec outputv, Vec errorv)
+{
+    real weight = train_set->getWeight(row);
+    real target = train_set->getTarget(row);
+    removeRow(row,target,weight,outputv,errorv);
+}
+
+void RegressionTreeMulticlassLeaveFast::removeRow(int row, real target, real weight,
+                                 Vec outputv, Vec errorv){
+    removeRow(row,target,weight);
+    getOutputAndError(outputv,errorv);
+}
+
+void RegressionTreeMulticlassLeaveFast::removeRow(int row, real target, real weight)
+{
+    length_ -= 1;
+    weights_sum -= weight;
+    PLASSERT(length_>=0);
+    PLASSERT(weights_sum>=0);
+    PLASSERT(length_>0 || weights_sum==0);
+    multiclass_weights_sum[int(target)] -= weight;
+}
+
+void RegressionTreeMulticlassLeaveFast::getOutputAndError(Vec& output, Vec& error)const
+{
+#ifdef BOUNDCHECK
+    if(nb_class<=0)
+        PLERROR("In RegressionTreeMulticlassLeaveFast::getOutputAndError() -"
+                " nb_class must be set.");
+#endif
+    if(length_==0){        
+        output.clear();
+        output[0]=MISSING_VALUE;
+        error.clear();
+        return;
+    }
+    int mc_winer = 0;
+    //index of the max. Is their an optimized version?
+    for (int mc_ind = 1; mc_ind < nb_class; mc_ind++)
+    {
+        if (multiclass_weights_sum[mc_ind] > multiclass_weights_sum[mc_winer])
+            mc_winer = mc_ind;
+    }
+    output[0] = mc_winer;
+    if (missing_leave)
+    {
+        output[1] = 0.0;
+        error[0] = 0.0;
+        error[1] = weights_sum;
+        error[2] = 0.0;
+    }
+    else
+    {
+        output[1] = multiclass_weights_sum[mc_winer] / weights_sum;;
+        error[0] = 0.0;
+        if (objective_function == "l1")
+        {
+            for (int mc_ind = 0; mc_ind < nb_class;mc_ind++)
+            {
+                error[0] += abs(output[0] - mc_ind) 
+                    * multiclass_weights_sum[mc_ind];
+            }
+            error[0] *= l1_loss_function_factor * length_ / weights_sum;
+            if (error[0] < 1E-10) error[0] = 0.0;
+            if (error[0] > weights_sum * l1_loss_function_factor)
+                error[2] = weights_sum * l1_loss_function_factor;
+            else error[2] = error[0];
+        }
+        else
+        {
+            for (int mc_ind = 0; mc_ind < nb_class;mc_ind++)
+            {
+                error[0] += pow(output[0] - mc_ind, 2) 
+                    * multiclass_weights_sum[mc_ind];
+            }
+            error[0] *= l2_loss_function_factor * length_ / weights_sum;
+            if (error[0] < 1E-10) error[0] = 0.0;
+            if (error[0] > weights_sum * l2_loss_function_factor) 
+                error[2] = weights_sum * l2_loss_function_factor; 
+            else error[2] = error[0];
+        }
+        error[1] = (1.0 - output[1]) * length_;
+    }
+}
+
+void RegressionTreeMulticlassLeaveFast::printStats()
+{
+    cout << " l " << length_;
+    Vec output(2);
+    Vec error(3);
+    getOutputAndError(output,error);
+    cout << " o0 " << output[0];
+    cout << " o1 " << output[1];
+    cout << " e0 " << error[0];
+    cout << " e1 " << error[1];
+    cout << " ws " << weights_sum;
+    cout << endl;
+    cout << " mws " << multiclass_weights_sum << endl;
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Copied: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.h (from rev 9955, trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h)
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h	2009-02-24 19:23:30 UTC (rev 9955)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.h	2009-02-26 18:14:25 UTC (rev 9963)
@@ -0,0 +1,110 @@
+// -*- C++ -*-
+
+// RegressionTreeMulticlassLeaveFast.h
+// Copyright (c) 1998-2002 Pascal Vincent
+// Copyright (C) 1999-2002 Yoshua Bengio and University of Montreal
+// Copyright (c) 2002 Jean-Sebastien Senecal, Xavier Saint-Mleux, Rejean Ducharme
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* *********************************************************************************   
+ * $Id: RegressionTreeMulticlassLeaveFast.h, v 1.0 2004/07/19 10:00:00 Bengio/Kegl/Godbout     *
+ * This file is part of the PLearn library.                                      *
+ ********************************************************************************* */
+
+#ifndef RegressionTreeMulticlassLeaveFast_INC
+#define RegressionTreeMulticlassLeaveFast_INC
+
+#include "RegressionTreeLeave.h"
+
+namespace PLearn {
+using namespace std;
+
+class RegressionTreeMulticlassLeaveFast: public RegressionTreeLeave
+{
+    typedef RegressionTreeLeave inherited;
+  
+private:
+
+/*
+  Build options: they have to be set before building
+*/
+
+    int nb_class;
+    string objective_function;       
+ 
+/*
+  Learnt options: they are sized and initialized if need be, in initLeave(...)
+*/
+
+    real loss_function_factor;
+    Vec multiclass_weights_sum;
+ 
+public:
+    RegressionTreeMulticlassLeaveFast();
+    virtual              ~RegressionTreeMulticlassLeaveFast();
+    PLEARN_DECLARE_OBJECT(RegressionTreeMulticlassLeaveFast);
+
+    static  void         declareOptions(OptionList& ol);
+    virtual void         makeDeepCopyFromShallowCopy(CopiesMap &copies);
+    virtual void         build();
+    void         initStats();
+    void         addRow(int row);
+    void         addRow(int row, real target, real weight);
+    void         addRow(int row, Vec outputv, Vec errorv);
+    void         addRow(int row, real target, real weight, Vec outputv, Vec errorv);
+    void         removeRow(int row, real target, real weight);
+    void         removeRow(int row, Vec outputv, Vec errorv);
+    void         removeRow(int row, real target, real weight, Vec outputv, Vec errorv);
+    void         getOutputAndError(Vec& output, Vec& error)const;
+    void         printStats();
+  
+private:
+    void         build_();
+};
+
+DECLARE_OBJECT_PTR(RegressionTreeMulticlassLeaveFast);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From nouiz at mail.berlios.de  Thu Feb 26 19:31:52 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 26 Feb 2009 19:31:52 +0100
Subject: [Plearn-commits] r9964 - in
 trunk/plearn_learners/regressors/test/RegressionTree: . .pytest
 .pytest/PL_RegressionTree_MultiClassFast
 .pytest/PL_RegressionTree_MultiClassFast/expected_results
 .pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir
 .pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0
 .pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir
 .pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0
 .pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0
 .pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0
 .pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata
 .pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata
 .pytest/PL_RegressionTree_!
 MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1
 .pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0
 .pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata
 .pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata
 .pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2
 .pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0
 .pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata
 .pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata
 .pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/!
 Strat0/Trials3 .pytest/PL_RegressionTree_MultiClassFast/expect! ed_resul
Message-ID: <200902261831.n1QIVqQ8009524@sheep.berlios.de>

Author: nouiz
Date: 2009-02-26 19:31:49 +0100 (Thu, 26 Feb 2009)
New Revision: 9964

Added:
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/RUN.log
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/LearnerExpdir/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/LearnerExpdir/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/LearnerExpdir/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/LearnerExpdir/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_costs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_costs.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_costs.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_outputs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_outputs.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_outputs.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_costs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_costs.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_costs.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_outputs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_outputs.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_outputs.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/experiment.plearn
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/global_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/global_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/global_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/split_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/split_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/split_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/test_cost_names.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/tester.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/train_cost_names.txt
   trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass_fast.pyplearn
Modified:
   trunk/plearn_learners/regressors/test/RegressionTree/pytest.config
Log:
added test for the new class RegressionTreeMultiClassLeaveFast



Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast
___________________________________________________________________
Name: svn:ignore
   + .plearn
run_results
PSAVEDIFF


Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/RUN.log	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/RUN.log	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,11 @@
+HyperLearner: starting the optimization
+split_cols: []
+
+split_values: []
+
+split_cols: 1 
+split_values: 0.547790627008831965 
+split_cols: 1 0 
+split_values: 0.547790627008831965 1.88430442629138994 
+split_cols: 1 0 0 
+split_values: 0.547790627008831965 1.88430442629138994 3.07754902034128275 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,171 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 7 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 7 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 100 ;
+sumsquare_ = 100 ;
+sumcube_ = 100 ;
+sumfourth_ = 100 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 99 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0.5 ;
+max_ = 0.5 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 0.5 ;
+last_ = 0.5 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -200 ;
+sumsquare_ = 400 ;
+sumcube_ = -800 ;
+sumfourth_ = 1600 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 99 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = -1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -200 ;
+sumsquare_ = 400 ;
+sumcube_ = -800 ;
+sumfourth_ = 1600 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 99 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = -1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 100 ;
+sumsquare_ = 100 ;
+sumcube_ = 100 ;
+sumfourth_ = 100 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 99 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,171 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 7 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 7 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 3177 ;
+sumsquare_ = 3177 ;
+sumcube_ = 3177 ;
+sumfourth_ = 3177 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 4899 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0.5 ;
+max_ = 0.5 ;
+agmemin_ = 6830 ;
+agemax_ = 6830 ;
+first_ = 0.5 ;
+last_ = 0.5 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -6354 ;
+sumsquare_ = 12708 ;
+sumcube_ = -25416 ;
+sumfourth_ = 50832 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 4899 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = -1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -6354 ;
+sumsquare_ = 12708 ;
+sumcube_ = -25416 ;
+sumfourth_ = 50832 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 4899 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = -1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 3177 ;
+sumsquare_ = 3177 ;
+sumcube_ = 3177 ;
+sumfourth_ = 3177 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 4899 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 6830 ;
+agemax_ = 6830 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,171 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 7 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 100 ;
+sumsquare_ = 100 ;
+sumcube_ = 100 ;
+sumfourth_ = 100 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 99 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0.5 ;
+max_ = 0.5 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 0.5 ;
+last_ = 0.5 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -200 ;
+sumsquare_ = 400 ;
+sumcube_ = -800 ;
+sumfourth_ = 1600 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 99 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = -1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -200 ;
+sumsquare_ = 400 ;
+sumcube_ = -800 ;
+sumfourth_ = 1600 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 99 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = -1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 100 ;
+sumsquare_ = 100 ;
+sumcube_ = 100 ;
+sumfourth_ = 100 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 99 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/fieldnames	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/fieldnames	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,8 @@
+E[test1.E[class_error]]	0
+E[test1.E[base_confidence]]	0
+E[test1.E[base_reward_l2]]	0
+E[test1.E[base_reward_l1]]	0
+E[test2.E[class_error]]	0
+E[test2.E[base_confidence]]	0
+E[test2.E[base_reward_l2]]	0
+E[test2.E[base_reward_l1]]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/sizes	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/sizes	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/fieldnames	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/fieldnames	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,9 @@
+splitnum	0
+test1.E[class_error]	0
+test1.E[base_confidence]	0
+test1.E[base_reward_l2]	0
+test1.E[base_reward_l1]	0
+test2.E[class_error]	0
+test2.E[base_confidence]	0
+test2.E[base_reward_l2]	0
+test2.E[base_reward_l1]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/sizes	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/sizes	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,171 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 7 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 7 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 51 ;
+sumsquare_ = 51 ;
+sumcube_ = 51 ;
+sumfourth_ = 51 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -13.0253164556962204 ;
+sumsquare_ = 1.40213941133083142 ;
+sumcube_ = -0.150936442542046217 ;
+sumfourth_ = 0.0162478919736128886 ;
+min_ = 0.702479338842975198 ;
+max_ = 0.810126582278481 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 0.810126582278481 ;
+last_ = 0.702479338842975198 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -102 ;
+sumsquare_ = 204 ;
+sumcube_ = -408 ;
+sumfourth_ = 816 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -102 ;
+sumsquare_ = 204 ;
+sumcube_ = -408 ;
+sumfourth_ = 816 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 51 ;
+sumsquare_ = 51 ;
+sumcube_ = 51 ;
+sumfourth_ = 51 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 121 ;
+sumsquare_ = 121 ;
+sumcube_ = 121 ;
+sumfourth_ = 121 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -121 ;
+sumsquare_ = 121 ;
+sumcube_ = -121 ;
+sumfourth_ = 121 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 2 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,171 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 7 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 7 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 1145 ;
+sumsquare_ = 1145 ;
+sumcube_ = 1145 ;
+sumfourth_ = 1145 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 4899 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -356.527670258402452 ;
+sumsquare_ = 38.3792209118009708 ;
+sumcube_ = -4.13141733635762964 ;
+sumfourth_ = 0.444735687740564989 ;
+min_ = 0.702479338842975198 ;
+max_ = 0.810126582278481 ;
+agmemin_ = 3311 ;
+agemax_ = 6830 ;
+first_ = 0.810126582278481 ;
+last_ = 0.702479338842975198 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -2290 ;
+sumsquare_ = 4580 ;
+sumcube_ = -9160 ;
+sumfourth_ = 18320 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 4899 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -2290 ;
+sumsquare_ = 4580 ;
+sumcube_ = -9160 ;
+sumfourth_ = 18320 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 4899 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 1145 ;
+sumsquare_ = 1145 ;
+sumcube_ = 1145 ;
+sumfourth_ = 1145 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 4899 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 3312 ;
+sumsquare_ = 3312 ;
+sumcube_ = 3312 ;
+sumfourth_ = 3312 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 3311 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -3312 ;
+sumsquare_ = 3312 ;
+sumcube_ = -3312 ;
+sumfourth_ = 3312 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 3311 ;
+agemax_ = 6830 ;
+first_ = 2 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,171 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 7 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 51 ;
+sumsquare_ = 51 ;
+sumcube_ = 51 ;
+sumfourth_ = 51 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -13.0253164556962204 ;
+sumsquare_ = 1.40213941133083142 ;
+sumcube_ = -0.150936442542046217 ;
+sumfourth_ = 0.0162478919736128886 ;
+min_ = 0.702479338842975198 ;
+max_ = 0.810126582278481 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 0.810126582278481 ;
+last_ = 0.702479338842975198 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -102 ;
+sumsquare_ = 204 ;
+sumcube_ = -408 ;
+sumfourth_ = 816 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -102 ;
+sumsquare_ = 204 ;
+sumcube_ = -408 ;
+sumfourth_ = 816 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 51 ;
+sumsquare_ = 51 ;
+sumcube_ = 51 ;
+sumfourth_ = 51 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 121 ;
+sumsquare_ = 121 ;
+sumcube_ = 121 ;
+sumfourth_ = 121 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -121 ;
+sumsquare_ = 121 ;
+sumcube_ = -121 ;
+sumfourth_ = 121 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 2 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/fieldnames	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/fieldnames	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,8 @@
+E[test1.E[class_error]]	0
+E[test1.E[base_confidence]]	0
+E[test1.E[base_reward_l2]]	0
+E[test1.E[base_reward_l1]]	0
+E[test2.E[class_error]]	0
+E[test2.E[base_confidence]]	0
+E[test2.E[base_reward_l2]]	0
+E[test2.E[base_reward_l1]]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/sizes	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/sizes	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/fieldnames	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/fieldnames	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,9 @@
+splitnum	0
+test1.E[class_error]	0
+test1.E[base_confidence]	0
+test1.E[base_reward_l2]	0
+test1.E[base_reward_l1]	0
+test2.E[class_error]	0
+test2.E[base_confidence]	0
+test2.E[base_reward_l2]	0
+test2.E[base_reward_l1]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/sizes	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/sizes	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,171 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 7 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 7 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 43 ;
+sumsquare_ = 43 ;
+sumcube_ = 43 ;
+sumfourth_ = 43 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -5.02531645569621244 ;
+sumsquare_ = 0.321449501143033556 ;
+sumcube_ = -0.0289169130068278983 ;
+sumfourth_ = 0.0030033253501558126 ;
+min_ = 0.699999999999999845 ;
+max_ = 0.810126582278481 ;
+agmemin_ = 190 ;
+agemax_ = 199 ;
+first_ = 0.810126582278481 ;
+last_ = 0.782178217821782096 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -86 ;
+sumsquare_ = 172 ;
+sumcube_ = -344 ;
+sumfourth_ = 688 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -86 ;
+sumsquare_ = 172 ;
+sumcube_ = -344 ;
+sumfourth_ = 688 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 43 ;
+sumsquare_ = 43 ;
+sumcube_ = 43 ;
+sumfourth_ = 43 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 141 ;
+sumsquare_ = 181 ;
+sumcube_ = 261 ;
+sumfourth_ = 421 ;
+min_ = 0 ;
+max_ = 2 ;
+agmemin_ = 199 ;
+agemax_ = 190 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -20 ;
+sumsquare_ = 20 ;
+sumcube_ = -20 ;
+sumfourth_ = 20 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 190 ;
+agemax_ = 199 ;
+first_ = 2 ;
+last_ = 2 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,171 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 7 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 7 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 1229 ;
+sumsquare_ = 1229 ;
+sumcube_ = 1229 ;
+sumfourth_ = 1229 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 4899 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -187.234290011284173 ;
+sumsquare_ = 15.6584993949765128 ;
+sumcube_ = -1.58576593913658415 ;
+sumfourth_ = 0.170759912175163953 ;
+min_ = 0.699999999999999845 ;
+max_ = 0.810126582278481 ;
+agmemin_ = 3266 ;
+agemax_ = 6830 ;
+first_ = 0.810126582278481 ;
+last_ = 0.699999999999999845 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -2458 ;
+sumsquare_ = 4916 ;
+sumcube_ = -9832 ;
+sumfourth_ = 19664 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 4899 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = -1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -2458 ;
+sumsquare_ = 4916 ;
+sumcube_ = -9832 ;
+sumfourth_ = 19664 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 4899 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = -1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 1229 ;
+sumsquare_ = 1229 ;
+sumcube_ = 1229 ;
+sumfourth_ = 1229 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 4899 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 4464 ;
+sumsquare_ = 6768 ;
+sumcube_ = 11376 ;
+sumfourth_ = 20592 ;
+min_ = 0 ;
+max_ = 2 ;
+agmemin_ = 6830 ;
+agemax_ = 3266 ;
+first_ = 0 ;
+last_ = 2 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -1152 ;
+sumsquare_ = 1152 ;
+sumcube_ = -1152 ;
+sumfourth_ = 1152 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 3266 ;
+agemax_ = 6830 ;
+first_ = 2 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,171 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 7 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 43 ;
+sumsquare_ = 43 ;
+sumcube_ = 43 ;
+sumfourth_ = 43 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -5.02531645569621244 ;
+sumsquare_ = 0.321449501143033556 ;
+sumcube_ = -0.0289169130068278983 ;
+sumfourth_ = 0.0030033253501558126 ;
+min_ = 0.699999999999999845 ;
+max_ = 0.810126582278481 ;
+agmemin_ = 190 ;
+agemax_ = 199 ;
+first_ = 0.810126582278481 ;
+last_ = 0.782178217821782096 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -86 ;
+sumsquare_ = 172 ;
+sumcube_ = -344 ;
+sumfourth_ = 688 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -86 ;
+sumsquare_ = 172 ;
+sumcube_ = -344 ;
+sumfourth_ = 688 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 43 ;
+sumsquare_ = 43 ;
+sumcube_ = 43 ;
+sumfourth_ = 43 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 141 ;
+sumsquare_ = 181 ;
+sumcube_ = 261 ;
+sumfourth_ = 421 ;
+min_ = 0 ;
+max_ = 2 ;
+agmemin_ = 199 ;
+agemax_ = 190 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -20 ;
+sumsquare_ = 20 ;
+sumcube_ = -20 ;
+sumfourth_ = 20 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 190 ;
+agemax_ = 199 ;
+first_ = 2 ;
+last_ = 2 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/fieldnames	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/fieldnames	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,8 @@
+E[test1.E[class_error]]	0
+E[test1.E[base_confidence]]	0
+E[test1.E[base_reward_l2]]	0
+E[test1.E[base_reward_l1]]	0
+E[test2.E[class_error]]	0
+E[test2.E[base_confidence]]	0
+E[test2.E[base_reward_l2]]	0
+E[test2.E[base_reward_l1]]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/sizes	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/sizes	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/fieldnames	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/fieldnames	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,9 @@
+splitnum	0
+test1.E[class_error]	0
+test1.E[base_confidence]	0
+test1.E[base_reward_l2]	0
+test1.E[base_reward_l1]	0
+test2.E[class_error]	0
+test2.E[base_confidence]	0
+test2.E[base_reward_l2]	0
+test2.E[base_reward_l1]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/sizes	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/sizes	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test1_stats.psave	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test1_stats.psave	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,171 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 7 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 7 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 39 ;
+sumsquare_ = 39 ;
+sumcube_ = 39 ;
+sumfourth_ = 39 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -1.02531645569621088 ;
+sumsquare_ = 0.278532081010422594 ;
+sumcube_ = 0.021133659590955596 ;
+sumfourth_ = 0.00281881493806235587 ;
+min_ = 0.782178217821782096 ;
+max_ = 0.928571428571428492 ;
+agmemin_ = 197 ;
+agemax_ = 190 ;
+first_ = 0.810126582278481 ;
+last_ = 0.782178217821782096 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -78 ;
+sumsquare_ = 156 ;
+sumcube_ = -312 ;
+sumfourth_ = 624 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -78 ;
+sumsquare_ = 156 ;
+sumcube_ = -312 ;
+sumfourth_ = 624 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 39 ;
+sumsquare_ = 39 ;
+sumcube_ = 39 ;
+sumfourth_ = 39 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 147 ;
+sumsquare_ = 211 ;
+sumcube_ = 375 ;
+sumfourth_ = 811 ;
+min_ = 0 ;
+max_ = 3 ;
+agmemin_ = 199 ;
+agemax_ = 140 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -6 ;
+sumsquare_ = 6 ;
+sumcube_ = -6 ;
+sumfourth_ = 6 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 140 ;
+agemax_ = 199 ;
+first_ = 2 ;
+last_ = 2 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test2_stats.psave	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test2_stats.psave	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,171 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 7 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 7 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 673 ;
+sumsquare_ = 673 ;
+sumcube_ = 673 ;
+sumfourth_ = 673 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 4899 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 21.2228528458643559 ;
+sumsquare_ = 10.0782152306775235 ;
+sumcube_ = 0.91717454487854666 ;
+sumfourth_ = 0.114852084949136693 ;
+min_ = 0.782178217821782096 ;
+max_ = 0.928571428571428492 ;
+agmemin_ = 3311 ;
+agemax_ = 3266 ;
+first_ = 0.810126582278481 ;
+last_ = 0.833333333333333259 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -1346 ;
+sumsquare_ = 2692 ;
+sumcube_ = -5384 ;
+sumfourth_ = 10768 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 4899 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -1346 ;
+sumsquare_ = 2692 ;
+sumcube_ = -5384 ;
+sumfourth_ = 10768 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 4899 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 673 ;
+sumsquare_ = 673 ;
+sumcube_ = 673 ;
+sumfourth_ = 673 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 4899 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 5040 ;
+sumsquare_ = 9648 ;
+sumcube_ = 22320 ;
+sumfourth_ = 58032 ;
+min_ = 0 ;
+max_ = 3 ;
+agmemin_ = 6830 ;
+agemax_ = 3254 ;
+first_ = 0 ;
+last_ = 3 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -576 ;
+sumsquare_ = 576 ;
+sumcube_ = -576 ;
+sumfourth_ = 576 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 3254 ;
+agemax_ = 6830 ;
+first_ = 2 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/train_stats.psave	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/train_stats.psave	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,171 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 7 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 39 ;
+sumsquare_ = 39 ;
+sumcube_ = 39 ;
+sumfourth_ = 39 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -1.02531645569621088 ;
+sumsquare_ = 0.278532081010422594 ;
+sumcube_ = 0.021133659590955596 ;
+sumfourth_ = 0.00281881493806235587 ;
+min_ = 0.782178217821782096 ;
+max_ = 0.928571428571428492 ;
+agmemin_ = 197 ;
+agemax_ = 190 ;
+first_ = 0.810126582278481 ;
+last_ = 0.782178217821782096 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -78 ;
+sumsquare_ = 156 ;
+sumcube_ = -312 ;
+sumfourth_ = 624 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -78 ;
+sumsquare_ = 156 ;
+sumcube_ = -312 ;
+sumfourth_ = 624 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 39 ;
+sumsquare_ = 39 ;
+sumcube_ = 39 ;
+sumfourth_ = 39 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 147 ;
+sumsquare_ = 211 ;
+sumcube_ = 375 ;
+sumfourth_ = 811 ;
+min_ = 0 ;
+max_ = 3 ;
+agmemin_ = 199 ;
+agemax_ = 140 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -6 ;
+sumsquare_ = 6 ;
+sumcube_ = -6 ;
+sumfourth_ = 6 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 140 ;
+agemax_ = 199 ;
+first_ = 2 ;
+last_ = 2 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat.metadata/fieldnames	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat.metadata/fieldnames	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,8 @@
+E[test1.E[class_error]]	0
+E[test1.E[base_confidence]]	0
+E[test1.E[base_reward_l2]]	0
+E[test1.E[base_reward_l1]]	0
+E[test2.E[class_error]]	0
+E[test2.E[base_confidence]]	0
+E[test2.E[base_reward_l2]]	0
+E[test2.E[base_reward_l1]]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat.metadata/sizes	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat.metadata/sizes	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat.metadata/fieldnames	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat.metadata/fieldnames	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,9 @@
+splitnum	0
+test1.E[class_error]	0
+test1.E[base_confidence]	0
+test1.E[base_reward_l2]	0
+test1.E[base_reward_l1]	0
+test2.E[class_error]	0
+test2.E[base_confidence]	0
+test2.E[base_reward_l2]	0
+test2.E[base_reward_l1]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat.metadata/sizes	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat.metadata/sizes	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/fieldnames	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/fieldnames	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,11 @@
+_trial_	0
+_objective_	0
+nstages	0
+E[test1.E[class_error]]	0
+E[test1.E[base_confidence]]	0
+E[test1.E[base_reward_l2]]	0
+E[test1.E[base_reward_l1]]	0
+E[test2.E[class_error]]	0
+E[test2.E[base_confidence]]	0
+E[test2.E[base_reward_l2]]	0
+E[test2.E[base_reward_l1]]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/sizes	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/sizes	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_costs.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_costs.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,7 @@
+mse	0
+base_confidence	0
+base_reward_l2	0
+base_reward_l1	0
+class_error	0
+SPLIT_VAR_x1	0
+SPLIT_VAR_y2	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_costs.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_costs.pmat.metadata/sizes	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_costs.pmat.metadata/sizes	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_outputs.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_outputs.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,2 @@
+out0	0
+out1	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_outputs.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_outputs.pmat.metadata/sizes	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_outputs.pmat.metadata/sizes	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_stats.psave	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_stats.psave	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,171 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 7 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 7 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 39 ;
+sumsquare_ = 39 ;
+sumcube_ = 39 ;
+sumfourth_ = 39 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -1.02531645569621088 ;
+sumsquare_ = 0.278532081010422594 ;
+sumcube_ = 0.021133659590955596 ;
+sumfourth_ = 0.00281881493806235587 ;
+min_ = 0.782178217821782096 ;
+max_ = 0.928571428571428492 ;
+agmemin_ = 197 ;
+agemax_ = 190 ;
+first_ = 0.810126582278481 ;
+last_ = 0.782178217821782096 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -78 ;
+sumsquare_ = 156 ;
+sumcube_ = -312 ;
+sumfourth_ = 624 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -78 ;
+sumsquare_ = 156 ;
+sumcube_ = -312 ;
+sumfourth_ = 624 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 39 ;
+sumsquare_ = 39 ;
+sumcube_ = 39 ;
+sumfourth_ = 39 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 147 ;
+sumsquare_ = 211 ;
+sumcube_ = 375 ;
+sumfourth_ = 811 ;
+min_ = 0 ;
+max_ = 3 ;
+agmemin_ = 199 ;
+agemax_ = 140 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -6 ;
+sumsquare_ = 6 ;
+sumcube_ = -6 ;
+sumfourth_ = 6 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 140 ;
+agemax_ = 199 ;
+first_ = 2 ;
+last_ = 2 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_costs.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_costs.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,7 @@
+mse	0
+base_confidence	0
+base_reward_l2	0
+base_reward_l1	0
+class_error	0
+SPLIT_VAR_x1	0
+SPLIT_VAR_y2	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_costs.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_costs.pmat.metadata/sizes	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_costs.pmat.metadata/sizes	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_outputs.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_outputs.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,2 @@
+out0	0
+out1	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_outputs.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_outputs.pmat.metadata/sizes	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_outputs.pmat.metadata/sizes	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_stats.psave	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_stats.psave	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,171 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 7 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 7 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 673 ;
+sumsquare_ = 673 ;
+sumcube_ = 673 ;
+sumfourth_ = 673 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 4899 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 21.2228528458643559 ;
+sumsquare_ = 10.0782152306775235 ;
+sumcube_ = 0.91717454487854666 ;
+sumfourth_ = 0.114852084949136693 ;
+min_ = 0.782178217821782096 ;
+max_ = 0.928571428571428492 ;
+agmemin_ = 3311 ;
+agemax_ = 3266 ;
+first_ = 0.810126582278481 ;
+last_ = 0.833333333333333259 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -1346 ;
+sumsquare_ = 2692 ;
+sumcube_ = -5384 ;
+sumfourth_ = 10768 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 4899 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -1346 ;
+sumsquare_ = 2692 ;
+sumcube_ = -5384 ;
+sumfourth_ = 10768 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 4899 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 673 ;
+sumsquare_ = 673 ;
+sumcube_ = 673 ;
+sumfourth_ = 673 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 4899 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 5040 ;
+sumsquare_ = 9648 ;
+sumcube_ = 22320 ;
+sumfourth_ = 58032 ;
+min_ = 0 ;
+max_ = 3 ;
+agmemin_ = 6830 ;
+agemax_ = 3254 ;
+first_ = 0 ;
+last_ = 3 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -576 ;
+sumsquare_ = 576 ;
+sumcube_ = -576 ;
+sumfourth_ = 576 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 3254 ;
+agemax_ = 6830 ;
+first_ = 2 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/train_stats.psave	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/train_stats.psave	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,192 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 8 [ "E[test1.E[class_error]]" "E[test1.E[base_confidence]]" "E[test1.E[base_reward_l2]]" "E[test1.E[base_reward_l1]]" "E[test2.E[class_error]]" "E[test2.E[base_confidence]]" "E[test2.E[base_reward_l2]]" "E[test2.E[base_reward_l1]]" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 8 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0.195000000000000007 ;
+max_ = 0.195000000000000007 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 0.195000000000000007 ;
+last_ = 0.195000000000000007 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0.805000000000000049 ;
+max_ = 0.805000000000000049 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 0.805000000000000049 ;
+last_ = 0.805000000000000049 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0.609999999999999987 ;
+max_ = 0.609999999999999987 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 0.609999999999999987 ;
+last_ = 0.609999999999999987 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0.609999999999999987 ;
+max_ = 0.609999999999999987 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 0.609999999999999987 ;
+last_ = 0.609999999999999987 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0.0985214463475333063 ;
+max_ = 0.0985214463475333063 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 0.0985214463475333063 ;
+last_ = 0.0985214463475333063 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0.813233426495413303 ;
+max_ = 0.813233426495413303 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 0.813233426495413303 ;
+last_ = 0.813233426495413303 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0.802957107304933415 ;
+max_ = 0.802957107304933415 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 0.802957107304933415 ;
+last_ = 0.802957107304933415 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0.802957107304933415 ;
+max_ = 0.802957107304933415 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 0.802957107304933415 ;
+last_ = 0.802957107304933415 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/experiment.plearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/experiment.plearn	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/experiment.plearn	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,116 @@
+*12 -> PTester(
+    dataset = *3 -> ConcatRowsVMatrix(
+        inputsize = 2,
+        sources = [
+            *1 -> AutoVMatrix(
+                inputsize = 2,
+                specification = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat",
+                targetsize = 1
+                ),
+            *2 -> AutoVMatrix(
+                inputsize = 2,
+                specification = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat",
+                targetsize = 1
+                )
+            ],
+        targetsize = 1,
+        weightsize = 0
+        ),
+    expdir = "expdir",
+    learner = *10 -> HyperLearner(
+        dont_restart_upon_change = [ "nstages" ],
+        forget_when_training_set_changes = 0,
+        learner = *5 -> RegressionTree(
+            complexity_penalty_factor = 0.0,
+            compute_train_stats = 1,
+            forget_when_training_set_changes = 1,
+            leave_template = *4 -> RegressionTreeMulticlassLeaveFast( nb_class = 2 ),
+            loss_function_weight = 1,
+            maximum_number_of_nodes = 50,
+            nstages = 10,
+            output_confidence_target = 1,
+            report_progress = 1,
+            verbosity = 2
+            ),
+        nstages = 1,
+        option_fields = [ "nstages" ],
+        provide_learner_expdir = 1,
+        provide_strategy_expdir = 1,
+        report_progress = 1,
+        save_final_learner = 0,
+        strategy = [
+            *7 -> HyperOptimize(
+                oracle = *6 -> EarlyStoppingOracle(
+                    max_degradation = 3.40282e+38,
+                    max_degraded_steps = 120,
+                    max_value = 3.40282e+38,
+                    min_improvement = -3.40282e+38,
+                    min_n_steps = 2,
+                    min_value = -3.40282e+38,
+                    option = "nstages",
+                    range = [
+                        1,
+                        5,
+                        1
+                        ],
+                    relative_max_degradation = -1,
+                    relative_min_improvement = -1
+                    ),
+                provide_tester_expdir = 1,
+                which_cost = "E[test2.E[class_error]]"
+                )
+            ],
+        tester = *9 -> PTester(
+            provide_learner_expdir = 1,
+            report_stats = 1,
+            save_data_sets = 0,
+            save_initial_learners = 0,
+            save_initial_tester = 0,
+            save_learners = 0,
+            save_test_confidence = 0,
+            save_test_costs = 0,
+            save_test_names = 0,
+            save_test_outputs = 0,
+            splitter = *8 -> FractionSplitter(
+                splits = 1 3 [
+                        (0, 200),
+                        (0, 200),
+                        (200, 1)
+                        ]
+                ),
+            statnames = [
+                "E[test1.E[class_error]]",
+                "E[test1.E[base_confidence]]",
+                "E[test1.E[base_reward_l2]]",
+                "E[test1.E[base_reward_l1]]",
+                "E[test2.E[class_error]]",
+                "E[test2.E[base_confidence]]",
+                "E[test2.E[base_reward_l2]]",
+                "E[test2.E[base_reward_l1]]"
+                ]
+            ),
+        verbosity = 1
+        ),
+    provide_learner_expdir = 1,
+    save_learners = 0,
+    save_test_confidence = 0,
+    save_test_costs = 1,
+    save_test_outputs = 1,
+    splitter = *11 -> FractionSplitter(
+        splits = 1 3 [
+                (0, 1),
+                (0, 200),
+                (200, 1)
+                ]
+        ),
+    statnames = [
+        "E[test1.E[class_error]]",
+        "E[test1.E[base_confidence]]",
+        "E[test1.E[base_reward_l2]]",
+        "E[test1.E[base_reward_l1]]",
+        "E[test2.E[class_error]]",
+        "E[test2.E[base_confidence]]",
+        "E[test2.E[base_reward_l2]]",
+        "E[test2.E[base_reward_l1]]"
+        ]
+    )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/global_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/global_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/global_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/global_stats.pmat.metadata/fieldnames	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/global_stats.pmat.metadata/fieldnames	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,8 @@
+E[test1.E[class_error]]	0
+E[test1.E[base_confidence]]	0
+E[test1.E[base_reward_l2]]	0
+E[test1.E[base_reward_l1]]	0
+E[test2.E[class_error]]	0
+E[test2.E[base_confidence]]	0
+E[test2.E[base_reward_l2]]	0
+E[test2.E[base_reward_l1]]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/global_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/global_stats.pmat.metadata/sizes	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/global_stats.pmat.metadata/sizes	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/metainfos.txt	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/metainfos.txt	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,3 @@
+__REVISION__ = "PL9955"
+datatest                                      = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat
+datatrain                                     = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/split_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/split_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/split_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/split_stats.pmat.metadata/fieldnames	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/split_stats.pmat.metadata/fieldnames	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,9 @@
+splitnum	0
+test1.E[class_error]	0
+test1.E[base_confidence]	0
+test1.E[base_reward_l2]	0
+test1.E[base_reward_l1]	0
+test2.E[class_error]	0
+test2.E[base_confidence]	0
+test2.E[base_reward_l2]	0
+test2.E[base_reward_l1]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/split_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/split_stats.pmat.metadata/sizes	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/split_stats.pmat.metadata/sizes	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/test_cost_names.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/test_cost_names.txt	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/test_cost_names.txt	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,7 @@
+mse
+base_confidence
+base_reward_l2
+base_reward_l1
+class_error
+SPLIT_VAR_x1
+SPLIT_VAR_y2

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/tester.psave	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/tester.psave	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,232 @@
+PTester(
+expdir = "PYTEST__PL_RegressionTree_MultiClassFast__RESULTS:expdir/" ;
+dataset = *1 ->ConcatRowsVMatrix(
+sources = 2 [ *2 ->AutoVMatrix(
+filename = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat" ;
+load_in_memory = 0 ;
+writable = 0 ;
+length = 200 ;
+width = 3 ;
+inputsize = 2 ;
+targetsize = 1 ;
+weightsize = 0 ;
+extrasize = 0 ;
+metadatadir = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat.metadata/" ;
+fieldinfos = 3 [ "x1" 0 "y2" 0 "class" 0 ]  )
+*3 ->AutoVMatrix(
+filename = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat" ;
+load_in_memory = 0 ;
+writable = 0 ;
+length = 6831 ;
+width = 3 ;
+inputsize = 2 ;
+targetsize = 1 ;
+weightsize = 0 ;
+extrasize = 0 ;
+metadatadir = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat.metadata/" ;
+fieldinfos = 3 [ "x1" 0 "x2" 0 "class" 0 ]  )
+] ;
+fill_missing = 0 ;
+fully_check_mappings = 0 ;
+only_common_fields = 0 ;
+writable = 0 ;
+length = 7031 ;
+width = 3 ;
+inputsize = 2 ;
+targetsize = 1 ;
+weightsize = 0 ;
+extrasize = 0 ;
+metadatadir = "" ;
+fieldinfos = 3 [ "x1" 0 "y2" 0 "class" 0 ]  )
+;
+splitter = *4 ->FractionSplitter(
+round_to_closest = 0 ;
+splits = 1  3  [ 
+(0 , 1 )	(0 , 200 )	(200 , 1 )	
+]
+;
+one_is_absolute = 0  )
+;
+statnames = 8 [ "E[test1.E[class_error]]" "E[test1.E[base_confidence]]" "E[test1.E[base_reward_l2]]" "E[test1.E[base_reward_l1]]" "E[test2.E[class_error]]" "E[test2.E[base_confidence]]" "E[test2.E[base_reward_l2]]" "E[test2.E[base_reward_l1]]" ] ;
+statmask = []
+;
+learner = *5 ->HyperLearner(
+tester = *6 ->PTester(
+expdir = "" ;
+dataset = *0 ;
+splitter = *7 ->FractionSplitter(
+round_to_closest = 0 ;
+splits = 1  3  [ 
+(0 , 200 )	(0 , 200 )	(200 , 1 )	
+]
+;
+one_is_absolute = 0  )
+;
+statnames = 8 [ "E[test1.E[class_error]]" "E[test1.E[base_confidence]]" "E[test1.E[base_reward_l2]]" "E[test1.E[base_reward_l1]]" "E[test2.E[class_error]]" "E[test2.E[base_confidence]]" "E[test2.E[base_reward_l2]]" "E[test2.E[base_reward_l1]]" ] ;
+statmask = []
+;
+learner = *8 ->RegressionTree(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+maximum_number_of_nodes = 50 ;
+compute_train_stats = 1 ;
+complexity_penalty_factor = 0 ;
+output_confidence_target = 1 ;
+multiclass_outputs = []
+;
+leave_template = *9 ->RegressionTreeMulticlassLeaveFast(
+nb_class = 2 ;
+objective_function = "l1" ;
+multiclass_weights_sum = []
+;
+l1_loss_function_factor = 9.71580092546811329e-320 ;
+l2_loss_function_factor = 0 ;
+id = 0 ;
+missing_leave = 0 ;
+loss_function_weight = 0 ;
+verbosity = 0 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 1  )
+;
+root = *0 ;
+priority_queue = *0 ;
+first_leave = *0 ;
+split_cols = []
+;
+split_values = []
+;
+random_gen = *0 ;
+seed = 1827 ;
+stage = 0 ;
+n_examples = -1 ;
+inputsize = -1 ;
+targetsize = -1 ;
+weightsize = -1 ;
+forget_when_training_set_changes = 1 ;
+nstages = 10 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
+;
+perf_evaluators = {};
+report_stats = 1 ;
+save_initial_tester = 0 ;
+save_stat_collectors = 1 ;
+save_split_stats = 1 ;
+save_learners = 0 ;
+save_initial_learners = 0 ;
+save_data_sets = 0 ;
+save_test_outputs = 0 ;
+call_forget_in_run = 1 ;
+save_test_costs = 0 ;
+save_test_names = 0 ;
+provide_learner_expdir = 1 ;
+should_train = 1 ;
+should_test = 1 ;
+finalize_learner = 0 ;
+template_stats_collector = *0 ;
+global_template_stats_collector = *0 ;
+final_commands = []
+;
+save_test_confidence = 0 ;
+enforce_clean_expdir = 1 ;
+redirect_stdout = 0 ;
+redirect_stderr = 0  )
+;
+option_fields = 1 [ "nstages" ] ;
+dont_restart_upon_change = 1 [ "nstages" ] ;
+strategy = 1 [ *10 ->HyperOptimize(
+which_cost = "E[test2.E[class_error]]" ;
+min_n_trials = 0 ;
+oracle = *11 ->EarlyStoppingOracle(
+option = "nstages" ;
+values = []
+;
+range = 3 [ 1 5 1 ] ;
+min_value = -3.40282000000000014e+38 ;
+max_value = 3.40282000000000014e+38 ;
+max_degradation = 3.40282000000000014e+38 ;
+relative_max_degradation = -1 ;
+min_improvement = -3.40282000000000014e+38 ;
+relative_min_improvement = -1 ;
+max_degraded_steps = 120 ;
+min_n_steps = 2 ;
+nreturned = 0 ;
+best_objective = 1.79769313486231571e+308 ;
+best_step = -1 ;
+met_early_stopping = 0  )
+;
+provide_tester_expdir = 1 ;
+sub_strategy = []
+;
+rerun_after_sub = 0 ;
+provide_sub_expdir = 1 ;
+save_best_learner = 0 ;
+splitter = *0 ;
+auto_save = 0 ;
+auto_save_diff_time = 10800 ;
+auto_save_test = 0 ;
+best_objective = 1.79769313486231571e+308 ;
+best_results = []
+;
+best_learner = *0 ;
+trialnum = 0 ;
+option_vals = []
+;
+verbosity = 0  )
+] ;
+provide_strategy_expdir = 1 ;
+save_final_learner = 0 ;
+finalize_learner = 0 ;
+learner = *8  ;
+provide_learner_expdir = 1 ;
+expdir_append = "" ;
+forward_nstages = 0 ;
+random_gen = *0 ;
+stage = 0 ;
+n_examples = -1 ;
+inputsize = -1 ;
+targetsize = -1 ;
+weightsize = -1 ;
+forget_when_training_set_changes = 0 ;
+nstages = 1 ;
+report_progress = 1 ;
+verbosity = 1 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
+;
+perf_evaluators = {};
+report_stats = 1 ;
+save_initial_tester = 1 ;
+save_stat_collectors = 1 ;
+save_split_stats = 1 ;
+save_learners = 0 ;
+save_initial_learners = 0 ;
+save_data_sets = 0 ;
+save_test_outputs = 1 ;
+call_forget_in_run = 1 ;
+save_test_costs = 1 ;
+save_test_names = 1 ;
+provide_learner_expdir = 1 ;
+should_train = 1 ;
+should_test = 1 ;
+finalize_learner = 0 ;
+template_stats_collector = *0 ;
+global_template_stats_collector = *0 ;
+final_commands = []
+;
+save_test_confidence = 0 ;
+enforce_clean_expdir = 1 ;
+redirect_stdout = 0 ;
+redirect_stderr = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/train_cost_names.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/train_cost_names.txt	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/train_cost_names.txt	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,8 @@
+E[test1.E[class_error]]
+E[test1.E[base_confidence]]
+E[test1.E[base_reward_l2]]
+E[test1.E[base_reward_l1]]
+E[test2.E[class_error]]
+E[test2.E[base_confidence]]
+E[test2.E[base_reward_l2]]
+E[test2.E[base_reward_l1]]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/pytest.config
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/pytest.config	2009-02-26 18:14:25 UTC (rev 9963)
+++ trunk/plearn_learners/regressors/test/RegressionTree/pytest.config	2009-02-26 18:31:49 UTC (rev 9964)
@@ -74,6 +74,22 @@
     
 """
 Test(
+    name = "PL_RegressionTree_MultiClassFast",
+    description = "Exercise basic functionality of RegressionTree with RegressionTreeMultiClassLeaveFast",
+    category = "General",
+    program = Program(
+        name = "plearn_tests",
+        compiler = "pymake"
+        ),
+    arguments = "regression_tree_multiclass_fast.pyplearn",
+    resources = [ "regression_tree_multiclass_fast.pyplearn" ],
+    precision = 1e-06,
+    pfileprg = "__program__",
+    disabled = False,
+    runtime = None
+    )
+
+Test(
     name = "PL_RegressionTree_MultiClass",
     description = "Exercise basic functionality of RegressionTree with RegressionTreeMultiClassLeave",
     category = "General",

Copied: trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass_fast.pyplearn (from rev 9955, trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass.pyplearn)
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass.pyplearn	2009-02-24 19:23:30 UTC (rev 9955)
+++ trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass_fast.pyplearn	2009-02-26 18:31:49 UTC (rev 9964)
@@ -0,0 +1,96 @@
+import os.path
+from plearn.pyplearn import *
+
+plarg_defaults.datatrain  = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat"
+plarg_defaults.datatest   = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat"
+
+dataset = pl.ConcatRowsVMatrix(
+sources = [ pl.AutoVMatrix(specification=plargs.datatrain,inputsize=2,targetsize=1),
+	pl.AutoVMatrix(specification=plargs.datatest,inputsize=2,targetsize=1) ],
+inputsize=2,
+targetsize=1,
+weightsize=0
+)
+
+learner = pl.HyperLearner(
+    option_fields = [ "nstages" ],
+    dont_restart_upon_change = [ "nstages" ] ,
+    provide_strategy_expdir = 1 ,
+    save_final_learner = 0 ,
+    provide_learner_expdir = 1 ,
+    forget_when_training_set_changes = 0 ,
+    nstages = 1 ,
+    report_progress = 1 ,
+    verbosity = 1 ,
+    learner = pl.RegressionTree(
+        nstages = 10
+        ,loss_function_weight = 1
+#        ,missing_is_valid = 0
+#        ,multiclass_outputs = []
+        ,maximum_number_of_nodes = 50
+        ,compute_train_stats = 1
+        ,complexity_penalty_factor = 0.0
+        ,output_confidence_target = 1
+        ,verbosity = 2
+        ,report_progress = 1
+        ,forget_when_training_set_changes = 1
+#        ,conf_rated_adaboost = 0
+        ,leave_template = pl.RegressionTreeMulticlassLeaveFast(nb_class=2 )
+        ),
+    tester = pl.PTester(
+    splitter = pl.FractionSplitter(splits = TMat(1,3,[ (0,200), (0,200), (200,1) ])),
+    statnames = [ #'E[train.E[class_error]]', 'E[train.E[base_confidence]]', 'E[train.E[base_reward_l2]]', 'E[train.E[base_reward_l1]]',
+                 'E[test1.E[class_error]]',  'E[test1.E[base_confidence]]',  'E[test1.E[base_reward_l2]]',  'E[test1.E[base_reward_l1]]',
+                 'E[test2.E[class_error]]',  'E[test2.E[base_confidence]]',  'E[test2.E[base_reward_l2]]',  'E[test2.E[base_reward_l1]]'],
+    save_test_outputs = 0 ,
+    report_stats = 1  ,
+    save_initial_tester = 0 ,
+    save_learners = 0 ,
+    save_initial_learners = 0  ,
+    save_data_sets = 0  ,
+    save_test_costs = 0  ,
+    provide_learner_expdir = 1  ,
+    save_test_confidence = 0  ,
+    save_test_names = 0,
+    ),
+    strategy = [
+
+    pl.HyperOptimize(
+    which_cost = "E[test2.E[class_error]]" ,
+    provide_tester_expdir = 1 ,
+    oracle = pl.EarlyStoppingOracle(
+    option = "nstages" ,
+    range = [ 1, 5, 1 ],
+    min_value = -3.40282e+38 ,
+    max_value = 3.40282e+38 ,
+    max_degradation = 3.40282e+38 ,
+    relative_max_degradation = -1 ,
+    min_improvement = -3.40282e+38 ,
+    relative_min_improvement = -1 ,
+    max_degraded_steps = 120 ,
+    min_n_steps = 2 
+    )  # end of EarlyStoppingOracle
+    )  # end of sub_strategy.HyperOptimize
+    ]  # end of HyperLearner strategy
+    )
+splitter = pl.FractionSplitter(
+    splits = TMat(1,3, [ (0,1), (0,200), (200,1) ])
+    )
+tester = pl.PTester(
+    expdir = plargs.expdir,
+    dataset = dataset,
+    splitter = splitter,
+    learner = learner,
+    statnames = [#'E[train.E[class_error]]', 'E[train.E[base_confidence]]', 'E[train.E[base_reward_l2]]', 'E[train.E[base_reward_l1]]',
+                 'E[test1.E[class_error]]',  'E[test1.E[base_confidence]]',  'E[test1.E[base_reward_l2]]',  'E[test1.E[base_reward_l1]]',
+                 'E[test2.E[class_error]]',  'E[test2.E[base_confidence]]',  'E[test2.E[base_reward_l2]]',  'E[test2.E[base_reward_l1]]'],
+    provide_learner_expdir = 1,
+    save_test_costs = 1,
+    save_test_outputs = 1,
+    save_test_confidence = 0,
+    save_learners = 0
+
+    )
+
+def main():
+    return tester



From nouiz at mail.berlios.de  Thu Feb 26 19:41:47 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 26 Feb 2009 19:41:47 +0100
Subject: [Plearn-commits] r9965 - in trunk/plearn_learners/regressors: .
	test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir
Message-ID: <200902261841.n1QIfl5L022129@sheep.berlios.de>

Author: nouiz
Date: 2009-02-26 19:41:47 +0100 (Thu, 26 Feb 2009)
New Revision: 9965

Modified:
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.cc
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.h
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/tester.psave
Log:
more optimization to the new class RegressionTreeMulticlassLeaveFast.


Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.cc	2009-02-26 18:31:49 UTC (rev 9964)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.cc	2009-02-26 18:41:47 UTC (rev 9965)
@@ -64,6 +64,8 @@
 
 void RegressionTreeMulticlassLeaveFast::declareOptions(OptionList& ol)
 { 
+    inherited::declareOptions(ol);
+
     declareOption(ol, "nb_class", 
                   &RegressionTreeMulticlassLeaveFast::nb_class,
                   OptionBase::buildoption,
@@ -80,23 +82,16 @@
                   OptionBase::learntoption,
                   "A vector to count the weight sum of each possible output "
                   "for the sample in this leave.\n");
-    declareOption(ol, "l1_loss_function_factor",
-                  &RegressionTreeMulticlassLeaveFast::l1_loss_function_factor,
+    redeclareOption(ol, "loss_function_factor",
+                  &RegressionTreeMulticlassLeaveFast::loss_function_factor,
                   OptionBase::learntoption,
-                  "2 / loss_function_weight.\n");
-    declareOption(ol, "l2_loss_function_factor",
-                  &RegressionTreeMulticlassLeaveFast::l2_loss_function_factor,
-                  OptionBase::learntoption,
-                  "2 / pow(loss_function_weight, 2.0).\n");
-    inherited::declareOptions(ol);
+                  "The loss fct factor. Depend of the objective_function.\n");
 }
 
 void RegressionTreeMulticlassLeaveFast::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(objective_function, copies);
-    deepCopyField(l1_loss_function_factor, copies);
-    deepCopyField(l2_loss_function_factor, copies);
     deepCopyField(multiclass_weights_sum, copies);
 }
 
@@ -116,13 +111,14 @@
     weights_sum = 0.0;
     if (loss_function_weight != 0.0)
     {
-        l1_loss_function_factor = 2.0 / loss_function_weight;
-        l2_loss_function_factor = 2.0 / pow(loss_function_weight, 2);
+        if(objective_function == "l1")
+            loss_function_factor = 2.0 / loss_function_weight;
+        else
+            loss_function_factor = 2.0 / pow(loss_function_weight, 2);
     }
     else
     {
-        l1_loss_function_factor = 1.0;
-        l2_loss_function_factor = 1.0;
+        loss_function_factor = 1.0;
     }
     multiclass_weights_sum.resize(nb_class);
     multiclass_weights_sum.fill(0);
@@ -132,14 +128,14 @@
 {
     real weight = train_set->getWeight(row);
     real target = train_set->getTarget(row);
-    addRow(row, target, weight);
+    RegressionTreeMulticlassLeaveFast::addRow(row, target, weight);
 }
 
 void RegressionTreeMulticlassLeaveFast::addRow(int row, real target, real weight,
                                  Vec outputv, Vec errorv)
 {
-    addRow(row, target, weight);
-    getOutputAndError(outputv,errorv);
+    RegressionTreeMulticlassLeaveFast::addRow(row, target, weight);
+    RegressionTreeMulticlassLeaveFast::getOutputAndError(outputv,errorv);
 }
 
 void RegressionTreeMulticlassLeaveFast::addRow(int row, real target, real weight)
@@ -151,21 +147,21 @@
 
 void RegressionTreeMulticlassLeaveFast::addRow(int row, Vec outputv, Vec errorv)
 {
-    addRow(row);
-    getOutputAndError(outputv,errorv);    
+    RegressionTreeMulticlassLeaveFast::addRow(row);
+    RegressionTreeMulticlassLeaveFast::getOutputAndError(outputv,errorv);    
 }
 
 void RegressionTreeMulticlassLeaveFast::removeRow(int row, Vec outputv, Vec errorv)
 {
     real weight = train_set->getWeight(row);
     real target = train_set->getTarget(row);
-    removeRow(row,target,weight,outputv,errorv);
+    RegressionTreeMulticlassLeaveFast::removeRow(row,target,weight,outputv,errorv);
 }
 
 void RegressionTreeMulticlassLeaveFast::removeRow(int row, real target, real weight,
                                  Vec outputv, Vec errorv){
-    removeRow(row,target,weight);
-    getOutputAndError(outputv,errorv);
+    RegressionTreeMulticlassLeaveFast::removeRow(row,target,weight);
+    RegressionTreeMulticlassLeaveFast::getOutputAndError(outputv,errorv);
 }
 
 void RegressionTreeMulticlassLeaveFast::removeRow(int row, real target, real weight)
@@ -217,11 +213,6 @@
                 error[0] += abs(output[0] - mc_ind) 
                     * multiclass_weights_sum[mc_ind];
             }
-            error[0] *= l1_loss_function_factor * length_ / weights_sum;
-            if (error[0] < 1E-10) error[0] = 0.0;
-            if (error[0] > weights_sum * l1_loss_function_factor)
-                error[2] = weights_sum * l1_loss_function_factor;
-            else error[2] = error[0];
         }
         else
         {
@@ -230,12 +221,12 @@
                 error[0] += pow(output[0] - mc_ind, 2) 
                     * multiclass_weights_sum[mc_ind];
             }
-            error[0] *= l2_loss_function_factor * length_ / weights_sum;
-            if (error[0] < 1E-10) error[0] = 0.0;
-            if (error[0] > weights_sum * l2_loss_function_factor) 
-                error[2] = weights_sum * l2_loss_function_factor; 
-            else error[2] = error[0];
         }
+        error[0] *= loss_function_factor * length_ / weights_sum;
+        if (error[0] < 1E-10) error[0] = 0.0;
+        if (error[0] > weights_sum * loss_function_factor) 
+            error[2] = weights_sum * loss_function_factor; 
+        else error[2] = error[0];
         error[1] = (1.0 - output[1]) * length_;
     }
 }

Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.h	2009-02-26 18:31:49 UTC (rev 9964)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.h	2009-02-26 18:41:47 UTC (rev 9965)
@@ -64,7 +64,6 @@
   Learnt options: they are sized and initialized if need be, in initLeave(...)
 */
 
-    real loss_function_factor;
     Vec multiclass_weights_sum;
  
 public:

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/tester.psave	2009-02-26 18:31:49 UTC (rev 9964)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/tester.psave	2009-02-26 18:41:47 UTC (rev 9965)
@@ -75,12 +75,6 @@
 multiclass_outputs = []
 ;
 leave_template = *9 ->RegressionTreeMulticlassLeaveFast(
-nb_class = 2 ;
-objective_function = "l1" ;
-multiclass_weights_sum = []
-;
-l1_loss_function_factor = 9.71580092546811329e-320 ;
-l2_loss_function_factor = 0 ;
 id = 0 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
@@ -90,7 +84,11 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1  )
+loss_function_factor = 1 ;
+nb_class = 2 ;
+objective_function = "l1" ;
+multiclass_weights_sum = []
+ )
 ;
 root = *0 ;
 priority_queue = *0 ;



From nouiz at mail.berlios.de  Thu Feb 26 20:02:23 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 26 Feb 2009 20:02:23 +0100
Subject: [Plearn-commits] r9966 - trunk/commands
Message-ID: <200902261902.n1QJ2NEe025632@sheep.berlios.de>

Author: nouiz
Date: 2009-02-26 20:02:23 +0100 (Thu, 26 Feb 2009)
New Revision: 9966

Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
add the missing import for the new test.


Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2009-02-26 18:41:47 UTC (rev 9965)
+++ trunk/commands/plearn_noblas_inc.h	2009-02-26 19:02:23 UTC (rev 9966)
@@ -189,6 +189,7 @@
 #include <plearn_learners/regressors/RankLearner.h>
 #include <plearn_learners/regressors/RegressorFromDistribution.h>
 #include <plearn_learners/regressors/RegressionTree.h>
+#include <plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.h>
 // Unsupervised
 #include <plearn_learners/unsupervised/UniformizeLearner.h>
 



From nouiz at mail.berlios.de  Fri Feb 27 21:21:30 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 27 Feb 2009 21:21:30 +0100
Subject: [Plearn-commits] r9967 - in trunk: commands
	plearn_learners/regressors
Message-ID: <200902272021.n1RKLUc4025856@sheep.berlios.de>

Author: nouiz
Date: 2009-02-27 21:21:29 +0100 (Fri, 27 Feb 2009)
New Revision: 9967

Modified:
   trunk/commands/plearn_noblas_inc.h
   trunk/plearn_learners/regressors/RegressionTree.cc
Log:
moved include so that it is not always included.


Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2009-02-26 19:02:23 UTC (rev 9966)
+++ trunk/commands/plearn_noblas_inc.h	2009-02-27 20:21:29 UTC (rev 9967)
@@ -189,6 +189,7 @@
 #include <plearn_learners/regressors/RankLearner.h>
 #include <plearn_learners/regressors/RegressorFromDistribution.h>
 #include <plearn_learners/regressors/RegressionTree.h>
+#include <plearn_learners/regressors/RegressionTreeMulticlassLeave.h>
 #include <plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.h>
 // Unsupervised
 #include <plearn_learners/unsupervised/UniformizeLearner.h>

Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2009-02-26 19:02:23 UTC (rev 9966)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2009-02-27 20:21:29 UTC (rev 9967)
@@ -42,7 +42,6 @@
 #include "RegressionTree.h"
 #include "RegressionTreeQueue.h"
 #include "RegressionTreeLeave.h"
-#include "RegressionTreeMulticlassLeave.h"
 #include "RegressionTreeRegisters.h"
 #include "RegressionTreeNode.h"
 



From nouiz at mail.berlios.de  Fri Feb 27 21:29:00 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 27 Feb 2009 21:29:00 +0100
Subject: [Plearn-commits] r9968 - trunk/plearn_learners/regressors
Message-ID: <200902272029.n1RKT0D1027046@sheep.berlios.de>

Author: nouiz
Date: 2009-02-27 21:28:59 +0100 (Fri, 27 Feb 2009)
New Revision: 9968

Modified:
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.cc
Log:
cosmetic change.


Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.cc	2009-02-27 20:21:29 UTC (rev 9967)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.cc	2009-02-27 20:28:59 UTC (rev 9968)
@@ -204,13 +204,13 @@
     }
     else
     {
-        output[1] = multiclass_weights_sum[mc_winer] / weights_sum;;
+        output[1] = multiclass_weights_sum[mc_winer] / weights_sum;
         error[0] = 0.0;
         if (objective_function == "l1")
         {
             for (int mc_ind = 0; mc_ind < nb_class;mc_ind++)
             {
-                error[0] += abs(output[0] - mc_ind) 
+                error[0] += abs(mc_winer - mc_ind) 
                     * multiclass_weights_sum[mc_ind];
             }
         }
@@ -218,14 +218,14 @@
         {
             for (int mc_ind = 0; mc_ind < nb_class;mc_ind++)
             {
-                error[0] += pow(output[0] - mc_ind, 2) 
+                error[0] += pow(mc_winer - mc_ind, 2) 
                     * multiclass_weights_sum[mc_ind];
             }
         }
         error[0] *= loss_function_factor * length_ / weights_sum;
         if (error[0] < 1E-10) error[0] = 0.0;
-        if (error[0] > weights_sum * loss_function_factor) 
-            error[2] = weights_sum * loss_function_factor; 
+        if (error[0] > weights_sum * loss_function_factor)
+            error[2] = weights_sum * loss_function_factor;
         else error[2] = error[0];
         error[1] = (1.0 - output[1]) * length_;
     }



From nouiz at mail.berlios.de  Fri Feb 27 21:40:12 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 27 Feb 2009 21:40:12 +0100
Subject: [Plearn-commits] r9969 - trunk/plearn_learners/regressors
Message-ID: <200902272040.n1RKeCZQ028775@sheep.berlios.de>

Author: nouiz
Date: 2009-02-27 21:40:12 +0100 (Fri, 27 Feb 2009)
New Revision: 9969

Modified:
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.cc
Log:
fixed compilation at apstat.


Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.cc	2009-02-27 20:28:59 UTC (rev 9968)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.cc	2009-02-27 20:40:12 UTC (rev 9969)
@@ -218,7 +218,7 @@
         {
             for (int mc_ind = 0; mc_ind < nb_class;mc_ind++)
             {
-                error[0] += pow(mc_winer - mc_ind, 2) 
+                error[0] += pow(mc_winer - mc_ind, 2.) 
                     * multiclass_weights_sum[mc_ind];
             }
         }



From nouiz at mail.berlios.de  Fri Feb 27 22:24:14 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 27 Feb 2009 22:24:14 +0100
Subject: [Plearn-commits] r9970 - trunk/plearn_learners/hyper
Message-ID: <200902272124.n1RLOEis004110@sheep.berlios.de>

Author: nouiz
Date: 2009-02-27 22:24:14 +0100 (Fri, 27 Feb 2009)
New Revision: 9970

Modified:
   trunk/plearn_learners/hyper/HyperOptimize.cc
Log:
better message error


Modified: trunk/plearn_learners/hyper/HyperOptimize.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperOptimize.cc	2009-02-27 20:40:12 UTC (rev 9969)
+++ trunk/plearn_learners/hyper/HyperOptimize.cc	2009-02-27 21:24:14 UTC (rev 9970)
@@ -260,8 +260,9 @@
             resultsmat = new FileVMatrix(fname, true);
             if(resultsmat.width()!=w)
                 PLERROR("In HyperOptimize::getResultsMat() - The existing "
-                        "results mat that we should reload don't have the "
-                        "width that we need. Did you added some statnames?");
+                        "results mat(%s) that we should reload don't have the "
+                        "width that we need. Did you added some statnames?",
+                        fname.c_str());
             return;
         }else
             resultsmat = new FileVMatrix(fname,0,w);



From laulysta at mail.berlios.de  Fri Feb 27 23:59:28 2009
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Fri, 27 Feb 2009 23:59:28 +0100
Subject: [Plearn-commits] r9971 - trunk/plearn_learners_experimental
Message-ID: <200902272259.n1RMxSlU018034@sheep.berlios.de>

Author: laulysta
Date: 2009-02-27 23:59:28 +0100 (Fri, 27 Feb 2009)
New Revision: 9971

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
Log:
with noise in fine tuning


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-02-27 21:24:14 UTC (rev 9970)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-02-27 22:59:28 UTC (rev 9971)
@@ -716,8 +716,14 @@
             for(int i=0; i<nseq; i++)
             {
 
-                if(stage<nb_stage_reconstruction && input_noise_prob!=0 )
+                if(input_noise_prob!=0 )
                     noise = true;
+                else
+                    noise = false;
+
+                
+                
+                noise = false;
                 getSequence(i, seq);
                 encodeSequenceAndPopulateLists(seq);
               
@@ -730,33 +736,20 @@
                     inject_zero_forcing_noise(encoded_seq, input_noise_prob);
 
                 // recurrent no noise phase
-                if(stage>=nb_stage_reconstruction && stage<nb_stage_target+nb_stage_reconstruction){
+                if(stage>=nb_stage_reconstruction){
                     if(recurrent_lr!=0)
                     {
                         
-                        if(corrupt_input) // need to recover the clean sequence                        
+                        if(noise) // need to recover the clean sequence                        
                             encoded_seq << clean_encoded_seq;                  
                         setLearningRate( recurrent_lr );                    
                         recurrentFprop(train_costs, train_n_items);
-                        recurrentUpdate(0,0,1, prediction_cost_weight,0, train_costs, train_n_items );
-                        
-                    }
-                }
-
-                if(stage>=nb_stage_target+nb_stage_reconstruction){
-                    if(recurrent_lr!=0)
-                    {
-                        
-                        if(corrupt_input) // need to recover the clean sequence                        
-                            encoded_seq << clean_encoded_seq;                  
-                        setLearningRate( recurrent_lr );                    
-                        recurrentFprop(train_costs, train_n_items);
                         recurrentUpdate(0,0,1, prediction_cost_weight,1, train_costs, train_n_items );
                         
                     }
                 }
 
-                if(stage<nb_stage_reconstruction){
+                if(stage<nb_stage_reconstruction || nb_stage_reconstruction == 0 ){
 
 
                     // greedy phase input
@@ -773,6 +766,24 @@
                         recurrentUpdate(0, hidden_reconstruction_cost_weight, 1, 0,1, train_costs, train_n_items );
                     }
                 }
+
+                // recurrent no noise phase
+                /*if(stage>=nb_stage_reconstruction && stage<nb_stage_target+nb_stage_reconstruction){
+                    if(recurrent_lr!=0)
+                    {
+                        
+                        if(noise) // need to recover the clean sequence                        
+                            encoded_seq << clean_encoded_seq;                  
+                        setLearningRate( recurrent_lr );                    
+                        recurrentFprop(train_costs, train_n_items);
+                        recurrentUpdate(0,0,1, prediction_cost_weight,0, train_costs, train_n_items );
+                        
+                        }
+                    }*/
+
+                
+
+
                 // recurrent noisy phase
                 if(noisy_recurrent_lr!=0)
                 {
@@ -783,7 +794,7 @@
 
                 
             }
-
+            noise= false;
             if( pb )
                 pb->update( stage + 1 - init_stage);
             
@@ -1037,6 +1048,8 @@
             }
         }
     }
+    if(noise)
+        inject_zero_forcing_noise(hidden_list, input_noise_prob);
 }
 
 
@@ -1211,14 +1224,37 @@
     transposeProduct(reconstruction_activation, reconstruction_weights, hidden); 
     reconstruction_activation += reconstruction_bias;
 
+    for( int j=0 ; j<fullinputlength ; j++ ){
+        if(clean_input[j]==1 || clean_input[j]==0)
+            reconstruction_prob[j] = fastsigmoid( reconstruction_activation[j] );
+        else
+            reconstruction_prob[j] = reconstruction_activation[j] ;
+    }
+
     double result_cost = 0;
     if(encoding=="raw_masked_supervised") // complicated input format... consider it's squared error
     {
-        real r;
-        reconstruction_prob << reconstruction_activation;
-        for(int i=0; i<reconstruction_activation.length(); i++)
-            r += reconstruction_activation[i] - clean_input[i];
-        result_cost = r*r;
+        double r = 0;
+        double neg_log_cost = 0; // neg log softmax
+        for(int k=0; k<reconstruction_prob.length(); k++){
+            if(clean_input[k]==1 || clean_input[k]==0){
+                neg_log_cost -= clean_input[k]*safelog(reconstruction_prob[k]) + (1-clean_input[k])*safelog(1-reconstruction_prob[k]);
+            }                
+            else{
+                r = reconstruction_prob[k] - clean_input[k];
+                neg_log_cost += r*r;
+            }
+            
+            
+        }
+        result_cost = neg_log_cost;
+        
+        /*real r;
+        //reconstruction_prob << reconstruction_activation;
+        for(int i=0; i<reconstruction_activation.length(); i++){
+            r = reconstruction_activation[i] - clean_input[i];
+            result_cost += r*r;
+            }*/
     }
     else // suppose it's a multiple softmax
     {
@@ -1295,7 +1331,7 @@
     //update bias
     multiplyAcc(reconstruction_bias, hidden_reconstruction_activation_grad, -lr);
     // update weight
-    externalProductScaleAcc(acc_weights_gr, hidden, hidden_reconstruction_activation_grad, -lr); //dynamic matrice tied
+    //externalProductScaleAcc(acc_weights_gr, hidden, hidden_reconstruction_activation_grad, -lr); //dynamic matrice tied
     //externalProductScaleAcc(acc_weights_gr, hidden_reconstruction_activation_grad, hidden, -lr); //dynamic matrice not tied
                 
     
@@ -1305,13 +1341,14 @@
     for(int k=0; k<reconstruction_prob.length(); k++){
         //    hidden_reconstruction_activation_grad[k] = safelog(1-reconstruction_prob[k]) - safelog(reconstruction_prob[k]);
         hidden_reconstruction_activation_grad[k] = - reconstruction_activation[k];
-    }*/
+        }*/
 
     double result_cost = 0;
     double neg_log_cost = 0; // neg log softmax
-    for(int k=0; k<reconstruction_prob.length(); k++)
-        if(hidden_target[k]!=0)
-            neg_log_cost -= hidden_target[k]*safelog(reconstruction_prob[k]) + (1-hidden_target[k])*safelog(1-reconstruction_prob[k]);
+    for(int k=0; k<reconstruction_prob.length(); k++){
+        //if(hidden_target[k]!=0)
+        neg_log_cost -= hidden_target[k]*safelog(reconstruction_prob[k]) + (1-hidden_target[k])*safelog(1-reconstruction_prob[k]);
+    }
     result_cost = neg_log_cost;
     
     return result_cost;



