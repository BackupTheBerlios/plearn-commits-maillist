From nouiz at mail.berlios.de  Mon Mar  3 15:43:40 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 3 Mar 2008 15:43:40 +0100
Subject: [Plearn-commits] r8616 - trunk/scripts/Skeletons
Message-ID: <200803031443.m23EheXV001139@sheep.berlios.de>

Author: nouiz
Date: 2008-03-03 15:43:39 +0100 (Mon, 03 Mar 2008)
New Revision: 8616

Modified:
   trunk/scripts/Skeletons/RowBufferedVMatrix.cc
   trunk/scripts/Skeletons/SourceVMatrix.cc
   trunk/scripts/Skeletons/VMatrix.cc
Log:
updated the skeleton for the mtime


Modified: trunk/scripts/Skeletons/RowBufferedVMatrix.cc
===================================================================
--- trunk/scripts/Skeletons/RowBufferedVMatrix.cc	2008-02-29 22:23:14 UTC (rev 8615)
+++ trunk/scripts/Skeletons/RowBufferedVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
@@ -57,6 +57,13 @@
     // ###    options have been modified.
     // ### You should assume that the parent class' build_() has already been
     // ### called.
+
+    // ### You should keep the line 'updateMtime(0);' if you don't implement the 
+    // ### update of the mtime. Otherwise you can have an mtime != 0 that is not valid.
+    updateMtime(0);
+    //updateMtime(filename);
+    //updateMtime(VMat);
+    //updateMtime(VMatrix);
 }
 
 // ### Nothing to add here, simply calls build_

Modified: trunk/scripts/Skeletons/SourceVMatrix.cc
===================================================================
--- trunk/scripts/Skeletons/SourceVMatrix.cc	2008-02-29 22:23:14 UTC (rev 8615)
+++ trunk/scripts/Skeletons/SourceVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
@@ -72,6 +72,14 @@
 
     // ### In a SourceVMatrix, you will typically end build_() with:
     // setMetaInfoFromSource();
+
+    // ### You should keep the line 'updateMtime(0);' if you don't implement the 
+    // ### update of the mtime. Otherwise you can have an mtime != 0 that is not valid.
+    // ### setMetaInfoFromSource() update the mtime to the same as the source.
+    updateMtime(0);
+    //updateMtime(filename);
+    //updateMtime(VMat);
+    //updateMtime(VMatrix);
 }
 
 ///////////////

Modified: trunk/scripts/Skeletons/VMatrix.cc
===================================================================
--- trunk/scripts/Skeletons/VMatrix.cc	2008-02-29 22:23:14 UTC (rev 8615)
+++ trunk/scripts/Skeletons/VMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
@@ -69,6 +69,13 @@
     // ###    options have been modified.
     // ### You should assume that the parent class' build_() has already been
     // ### called.
+
+    // ### You should keep the line 'updateMtime(0);' if you don't implement the 
+    // ### update of the mtime. Otherwise you can have an mtime != 0 that is not valid.
+    updateMtime(0);
+    //updateMtime(filename);
+    //updateMtime(VMat);
+    //updateMtime(VMatrix);
 }
 
 /////////



From nouiz at mail.berlios.de  Mon Mar  3 18:46:01 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 3 Mar 2008 18:46:01 +0100
Subject: [Plearn-commits] r8617 - trunk/plearn/vmat
Message-ID: <200803031746.m23Hk14p013130@sheep.berlios.de>

Author: nouiz
Date: 2008-03-03 18:45:54 +0100 (Mon, 03 Mar 2008)
New Revision: 8617

Modified:
   trunk/plearn/vmat/AddBagInformationVMatrix.cc
   trunk/plearn/vmat/AddMissingVMatrix.cc
   trunk/plearn/vmat/AppendNeighborsVMatrix.cc
   trunk/plearn/vmat/AsciiVMatrix.cc
   trunk/plearn/vmat/BinaryNumbersVMatrix.cc
   trunk/plearn/vmat/BinaryOpVMatrix.cc
   trunk/plearn/vmat/BootstrapVMatrix.cc
   trunk/plearn/vmat/ClassSubsetVMatrix.cc
   trunk/plearn/vmat/ConcatRowsSubVMatrix.cc
   trunk/plearn/vmat/ConcatRowsVMatrix.cc
   trunk/plearn/vmat/CrossReferenceVMatrix.cc
   trunk/plearn/vmat/DatedJoinVMatrix.cc
   trunk/plearn/vmat/DictionaryVMatrix.cc
   trunk/plearn/vmat/DisregardRowsVMatrix.cc
   trunk/plearn/vmat/EncodedVMatrix.cc
   trunk/plearn/vmat/FilteredVMatrix.cc
   trunk/plearn/vmat/FinancePreprocVMatrix.cc
   trunk/plearn/vmat/GetInputVMatrix.cc
   trunk/plearn/vmat/GramVMatrix.cc
   trunk/plearn/vmat/InterleaveVMatrix.cc
   trunk/plearn/vmat/JoinVMatrix.cc
   trunk/plearn/vmat/KNNImputationVMatrix.cc
   trunk/plearn/vmat/KNNVMatrix.cc
   trunk/plearn/vmat/KernelVMatrix.cc
   trunk/plearn/vmat/LemmatizeVMatrix.cc
   trunk/plearn/vmat/LocalNeighborsDifferencesVMatrix.cc
   trunk/plearn/vmat/LocallyPrecomputedVMatrix.cc
   trunk/plearn/vmat/MeanImputationVMatrix.cc
   trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
   trunk/plearn/vmat/MixUnlabeledNeighbourVMatrix.cc
   trunk/plearn/vmat/MixtureVMatrix.cc
   trunk/plearn/vmat/MultiInstanceVMatrix.cc
   trunk/plearn/vmat/MultiTargetOneHotVMatrix.cc
   trunk/plearn/vmat/MultiToUniInstanceSelectRandomVMatrix.cc
   trunk/plearn/vmat/OneVsAllVMatrix.cc
   trunk/plearn/vmat/PLearnerOutputVMatrix.cc
   trunk/plearn/vmat/ProcessDatasetVMatrix.cc
   trunk/plearn/vmat/ProcessSymbolicSequenceVMatrix.cc
   trunk/plearn/vmat/RandomNeighborsDifferencesVMatrix.cc
   trunk/plearn/vmat/RandomSamplesFromVMatrix.cc
   trunk/plearn/vmat/RealFunctionsProcessedVMatrix.cc
   trunk/plearn/vmat/RemapLastColumnVMatrix.cc
   trunk/plearn/vmat/RemoveDuplicateVMatrix.cc
   trunk/plearn/vmat/ReorderByMissingVMatrix.cc
   trunk/plearn/vmat/ReplicateSamplesVMatrix.cc
   trunk/plearn/vmat/SelectRowsFileIndexVMatrix.cc
   trunk/plearn/vmat/SelectRowsMultiInstanceVMatrix.cc
   trunk/plearn/vmat/SeparateInputVMatrix.cc
   trunk/plearn/vmat/ShuffleColumnsVMatrix.cc
   trunk/plearn/vmat/SortRowsVMatrix.cc
   trunk/plearn/vmat/SparseVMatrix.cc
   trunk/plearn/vmat/SplitWiseValidationVMatrix.cc
   trunk/plearn/vmat/TemporalHorizonVMatrix.cc
   trunk/plearn/vmat/TemporaryDiskVMatrix.cc
   trunk/plearn/vmat/TemporaryFileVMatrix.cc
   trunk/plearn/vmat/TextStreamVMatrix.cc
   trunk/plearn/vmat/ThresholdVMatrix.cc
   trunk/plearn/vmat/UCIDataVMatrix.cc
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VariableDeletionVMatrix.cc
   trunk/plearn/vmat/VecExtendedVMatrix.cc
   trunk/plearn/vmat/ViewSplitterVMatrix.cc
Log:
-implemented the computation of mtime for many VMatrix.



Modified: trunk/plearn/vmat/AddBagInformationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/AddBagInformationVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/AddBagInformationVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -95,6 +95,7 @@
 void AddBagInformationVMatrix::build_()
 {
     if (source) {
+        updateMtime(source);
         bag_info_idx = source->getFieldIndex(bag_info_column);
         sourcerow.resize(source->width());
         width_ = source->width() + 1;

Modified: trunk/plearn/vmat/AddMissingVMatrix.cc
===================================================================
--- trunk/plearn/vmat/AddMissingVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/AddMissingVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -109,6 +109,7 @@
 ////////////
 void AddMissingVMatrix::build_()
 {
+    updateMtime(source);
     // Ensure we are not using both a missing values proportion and
     // user-specified missing columns.
     if (!fast_exact_is_equal(missing_prop, 0) &&

Modified: trunk/plearn/vmat/AppendNeighborsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/AppendNeighborsVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/AppendNeighborsVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -125,6 +125,7 @@
 
 void AppendNeighborsVMatrix::build_()
 {
+    updateMtime(source);
     // find the nearest neighbors, if not done already
     if (source && (input_parts.length() != source->length() || input_parts.width() != n_neighbors+1 ))
         // WARNING: will not work if source is changed but has the same dimensions

Modified: trunk/plearn/vmat/AsciiVMatrix.cc
===================================================================
--- trunk/plearn/vmat/AsciiVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/AsciiVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -119,7 +119,7 @@
 
 void AsciiVMatrix::build_()
 {
-    //updateMtime(filename);
+    updateMtime(filename);
 
     if(!newfile)  // open old file
     {

Modified: trunk/plearn/vmat/BinaryNumbersVMatrix.cc
===================================================================
--- trunk/plearn/vmat/BinaryNumbersVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/BinaryNumbersVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -143,6 +143,7 @@
 {
     if (f)
         PR_Close(f);
+    updateMtime(filename);
     f = PR_Open(filename.c_str(), PR_RDONLY, 0666);
     if (width_>0)
     {

Modified: trunk/plearn/vmat/BinaryOpVMatrix.cc
===================================================================
--- trunk/plearn/vmat/BinaryOpVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/BinaryOpVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -94,6 +94,8 @@
         PLERROR("BinaryOpVMatrix::build_: source1 not defined");
     if (! source2)
         PLERROR("BinaryOpVMatrix::build_: source2 not defined");
+    updateMtime(source1);
+    updateMtime(source2);
     if (source1.length() != source2.length())
         PLERROR("BinaryOpVMatrix::build_: source1 has %d rows but\n"
                 "source2 has %d rows; both must have the same number of"

Modified: trunk/plearn/vmat/BootstrapVMatrix.cc
===================================================================
--- trunk/plearn/vmat/BootstrapVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/BootstrapVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -157,7 +157,8 @@
 void BootstrapVMatrix::build_()
 {
     if (source) 
-    {
+    { 
+        updateMtime(source);
         // Ensure we are not using the deprecated 'own_seed' option.
         if (own_seed != -3) {
             PLDEPRECATED("In BootstrapVMatrix::build_ - The 'own_seed' option "

Modified: trunk/plearn/vmat/ClassSubsetVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ClassSubsetVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/ClassSubsetVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -104,6 +104,7 @@
 void ClassSubsetVMatrix::build_()
 {
   if (source) {
+    setMetaInfoFrom(source);
     input.resize(inputsize());
     target.resize(targetsize());
     indices.clear();

Modified: trunk/plearn/vmat/ConcatRowsSubVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ConcatRowsSubVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/ConcatRowsSubVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -194,6 +194,7 @@
 {
     if (source) {
         fieldinfos = source->getFieldInfos();
+        setMetaInfoFrom(source);
         length_=0;
         for (int i = 0; i < start.length(); i++) {
             if (start[i]<0 || start[i]+len[i]>source->length())

Modified: trunk/plearn/vmat/ConcatRowsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ConcatRowsVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/ConcatRowsVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -144,7 +144,8 @@
     if (n < 1)
         // No sources provided yet: nothing to do.
         return;
-
+    for(int i=0;i<sources.size();i++)
+        updateMtime(sources[i]);
     if (only_common_fields && fill_missing)
         PLERROR("In ConcatRowsVMatrix::build_ - You can't set both 'only_common_fields' and 'fill_missing'");
 

Modified: trunk/plearn/vmat/CrossReferenceVMatrix.cc
===================================================================
--- trunk/plearn/vmat/CrossReferenceVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/CrossReferenceVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -95,6 +95,8 @@
     if (master && slave) {
         fieldinfos = master->getFieldInfos();
         fieldinfos &= slave->getFieldInfos();
+        updateMtime(master);
+        updateMtime(slave);
     }
 }
 

Modified: trunk/plearn/vmat/DatedJoinVMatrix.cc
===================================================================
--- trunk/plearn/vmat/DatedJoinVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/DatedJoinVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -274,6 +274,8 @@
 {
     if (master && slave) // we can't really build if we don't have them
     {
+        updateMtime(master);
+        updateMtime(slave);
         // convert field names into indices
         // * get master key indices
         if (master_key_names.length()>0)

Modified: trunk/plearn/vmat/DictionaryVMatrix.cc
===================================================================
--- trunk/plearn/vmat/DictionaryVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/DictionaryVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -195,6 +195,7 @@
     {
         PPath input_file = file_names[k];
         PStream input_stream = openFile(input_file, PStream::raw_ascii);
+        updateMtime(input_file);
         input_stream.skipBlanks();
         while (!input_stream.eof()){
             input_stream.getline(line);

Modified: trunk/plearn/vmat/DisregardRowsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/DisregardRowsVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/DisregardRowsVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -145,6 +145,7 @@
     if ( !source )
         return;
 
+    updateMtime(source);
     /* Option: inspected_fieldnames */
 
     // Default: All fields are inspected.

Modified: trunk/plearn/vmat/EncodedVMatrix.cc
===================================================================
--- trunk/plearn/vmat/EncodedVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/EncodedVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -88,6 +88,7 @@
 {
     if(source)
     {
+        updateMtime(source);
         copySizesFrom(source);
         declareFieldNames(source->fieldNames());
         for(int i= 0; i < source.width(); ++i)

Modified: trunk/plearn/vmat/FilteredVMatrix.cc
===================================================================
--- trunk/plearn/vmat/FilteredVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/FilteredVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -194,6 +194,7 @@
 void FilteredVMatrix::build_()
 {
     if (source) {
+        updateMtime(source);
         vector<string> fieldnames;
         program.setSource(source);
         program.compileString(prg,fieldnames);

Modified: trunk/plearn/vmat/FinancePreprocVMatrix.cc
===================================================================
--- trunk/plearn/vmat/FinancePreprocVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/FinancePreprocVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -256,6 +256,7 @@
                     (add_last_day_of_month?1:0) +
                     (add_moving_average?asset_name.size()*prices_tag.size()*moving_average_window.size():0) +
                     (add_rollover_info?asset_name.size():0) );
+        updateMtime(source);
     }
 
     // stuff about the tradable information

Modified: trunk/plearn/vmat/GetInputVMatrix.cc
===================================================================
--- trunk/plearn/vmat/GetInputVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/GetInputVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -93,6 +93,7 @@
 {
     weightsize_ = 0;
     if (source) {
+        updateMtime(source);
         if(targetsize_ < 0) targetsize_ = 0;
         if(inputsize_ < 0) inputsize_ = source->inputsize();
 

Modified: trunk/plearn/vmat/GramVMatrix.cc
===================================================================
--- trunk/plearn/vmat/GramVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/GramVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -109,6 +109,8 @@
 void GramVMatrix::build_()
 {
     if (kernel) {
+        updateMtime(kernel->getData());
+        updateMtime(kernel->specify_dataset);
         bool old_report_progress = kernel->report_progress != 0;
         if (verbosity < 1) {
             kernel->report_progress = false;

Modified: trunk/plearn/vmat/InterleaveVMatrix.cc
===================================================================
--- trunk/plearn/vmat/InterleaveVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/InterleaveVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -102,6 +102,7 @@
             int l = sources[i]->length();
             if (l > maxl)
                 maxl=l;
+            updateMtime(sources[i]);
         }
         length_ = n * maxl;
 

Modified: trunk/plearn/vmat/JoinVMatrix.cc
===================================================================
--- trunk/plearn/vmat/JoinVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/JoinVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -80,6 +80,8 @@
                 tempkey[j]=temp[slave_idx[j]];
             mp.insert(make_pair(tempkey,i));
         }
+        updateMtime(master);
+        updateMtime(slave);
     }
 }
 

Modified: trunk/plearn/vmat/KNNImputationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/KNNImputationVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/KNNImputationVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -134,6 +134,10 @@
     PLASSERT( neighbors || full_source->length() == source->length() );
     PLASSERT( full_source->width() == source->width()  );
 
+    updateMtime(source);
+    updateMtime(full_source);
+    updateMtime(neighbors);
+
     VMat candidates;
     if (neighbors)
         candidates = full_source ? full_source : source;

Modified: trunk/plearn/vmat/KNNVMatrix.cc
===================================================================
--- trunk/plearn/vmat/KNNVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/KNNVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -110,6 +110,8 @@
 // build_ //
 ////////////
 void KNNVMatrix::build_() {
+    updateMtime(source);
+    updateMtime(k_nn_mat);
     if (source) {
         int n = source->length();
         bool recompute_nn = true;

Modified: trunk/plearn/vmat/KernelVMatrix.cc
===================================================================
--- trunk/plearn/vmat/KernelVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/KernelVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -84,6 +84,9 @@
         input1.resize(source1->width());
     if (source2)
         input2.resize(source2->width());
+    updateMtime(source1);
+    updateMtime(source2);
+    updateMtime(ker.getData());
 }
 
 void

Modified: trunk/plearn/vmat/LemmatizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/LemmatizeVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/LemmatizeVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -209,7 +209,7 @@
         weightsize_ = source->weightsize();
         width_ = inputsize_+targetsize_+weightsize_;
         length_ = source->length();
-
+        updateMtime(source);
         if(word_pos_to_lemma_table.length() != 0 && word_pos_to_lemma_table.width() != 3)
            PLERROR("In LemmatizeVMatrix::build_(): word_pos_to_lemma_table doesn't have three columns");
         if(word_to_lemma_table.length() != 0 && word_to_lemma_table.width() != 2)

Modified: trunk/plearn/vmat/LocalNeighborsDifferencesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/LocalNeighborsDifferencesVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/LocalNeighborsDifferencesVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -171,6 +171,8 @@
             computeNearestNeighbors(source,a_row,neighbors_of_i,i);
         }
     }
+
+    updateMtime(source);
 }
 
 // ### Nothing to add here, simply calls build_

Modified: trunk/plearn/vmat/LocallyPrecomputedVMatrix.cc
===================================================================
--- trunk/plearn/vmat/LocallyPrecomputedVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/LocallyPrecomputedVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -150,6 +150,8 @@
         if (sequential_access)
             source->unlockMetaDataDir();
     }
+    updateMtime(precomp_source);
+    updateMtime(source);
 }
 
 /////////////////////////////////

Modified: trunk/plearn/vmat/MeanImputationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MeanImputationVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/MeanImputationVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -192,7 +192,10 @@
             obtained_weightsize_from_source = true;
         } else if (obtained_weightsize_from_source && weightsize_ != source->weightsize())
             PLERROR(error_msg.c_str());
+
         setMetaInfoFromSource();
+        updateMtime(mean_source);
+        
         computeMeanVector();
 
         // Train the user-provided distribution if needed.

Modified: trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -159,6 +159,10 @@
     if (!source) PLERROR("In MeanMedianModeImputationVMatrix:: source vmat must be supplied");
     if (!train_set)
       train_set = source;
+
+    updateMtime(train_set);
+    updateMtime(source);
+
     int train_length = train_set->length();
     if (number_of_train_samples_to_use > 0.0)
         if (number_of_train_samples_to_use < 1.0) train_length = (int) (number_of_train_samples_to_use * (real) train_length);

Modified: trunk/plearn/vmat/MixUnlabeledNeighbourVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MixUnlabeledNeighbourVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/MixUnlabeledNeighbourVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -190,6 +190,7 @@
                 }
             }
         }
+        updateMtime(source_select);
     }
     // ?? Modify the width, length, (targetsize, inputsize and weight) size attribute.
     inputsize_ = source->inputsize();

Modified: trunk/plearn/vmat/MixtureVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MixtureVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/MixtureVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -143,6 +143,8 @@
 
     if (incorrect_period)
         buildPeriod();
+    for(int i=0;i<sources.length();i++)
+        updateMtime(sources[i]);
 }
 
 void MixtureVMatrix::buildPeriod()

Modified: trunk/plearn/vmat/MultiInstanceVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MultiInstanceVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/MultiInstanceVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -116,11 +116,13 @@
 
 void MultiInstanceVMatrix::build_()
 {
+
+    updateMtime(filename_);
     //this->setMetaDataDir(filename_ + ".metadata");
 
     // To be used in the end.. it is about 5 secs slower in debug
     //int nRows = countNonBlankLinesOfFile(filename_);
-
+    
     PStream inFile = openFile(filename_, PStream::raw_ascii, "r");
 //    PStream inFile = openFile(filename_, PStream::raw_ascii, "r");
 

Modified: trunk/plearn/vmat/MultiTargetOneHotVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MultiTargetOneHotVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/MultiTargetOneHotVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -144,6 +144,11 @@
   // ### You should assume that the parent class' build_() has already been called.
 
   if (source && source_target) {
+    updateMtime(source);
+    updateMtime(source_target);
+    updateMtime(target_descriptor);
+    updateMtime(source_and_target);
+
     if (source->targetsize() > 0)
       PLERROR("In MultiTargetOneHotVMatrix::build_ - The source VMatrix must have a targetsize <= 0");
     if (source->length() != source_target->length())

Modified: trunk/plearn/vmat/MultiToUniInstanceSelectRandomVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MultiToUniInstanceSelectRandomVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/MultiToUniInstanceSelectRandomVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -93,6 +93,9 @@
     else if (seed != 0)
         PLERROR("In MultiToUniInstanceSelectRandomVMatrix::build_ - The seed must be either -1 or >= 0");
 
+    updateMtime(indices_vmat);
+    updateMtime(source_);
+
     // Building the source VMatrix (uni instances conforming version of source_)
     source = new SubVMatrix(source_, 0, 0, source_->length(), source_->inputsize()+source_->targetsize() - 1);
     source->defineSizes(source_->inputsize(), source_->targetsize()-1, 0);

Modified: trunk/plearn/vmat/OneVsAllVMatrix.cc
===================================================================
--- trunk/plearn/vmat/OneVsAllVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/OneVsAllVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -94,6 +94,7 @@
 {
     if(source)
     {
+        updateMtime(source);
         inputsize_ = source->inputsize();
         targetsize_ = source->targetsize();
         weightsize_ = source->weightsize();

Modified: trunk/plearn/vmat/PLearnerOutputVMatrix.cc
===================================================================
--- trunk/plearn/vmat/PLearnerOutputVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/PLearnerOutputVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -235,6 +235,8 @@
 ////////////
 void PLearnerOutputVMatrix::build_()
 {
+    updateMtime(source);
+        
     if (source && learners.length()>0 && learners[0])
     {
         learners_need_train = train_learners;

Modified: trunk/plearn/vmat/ProcessDatasetVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ProcessDatasetVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/ProcessDatasetVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -139,6 +139,7 @@
     }
     if (source->inputsize() < 0 || source->targetsize() < 0 || source->weightsize() < 0)
         PLERROR("In ProcessDatasetVMatrix::build_ - The source VMat's sizes must be defined");
+    updateMtime(source);
     vm = source;
     PPath filename_base = string("processed_dataset")
         + "-input_normalization="  + input_normalization

Modified: trunk/plearn/vmat/ProcessSymbolicSequenceVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ProcessSymbolicSequenceVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/ProcessSymbolicSequenceVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -379,9 +379,10 @@
 
     if(inputsize_+targetsize_ != width_) PLERROR("In ProcessSymbolicSequenceVMatrix:build_() : inputsize_ + targetsize_ != width_");
 
+
     // Should we call:
     // setMetaInfoFromSource(); // ?
-
+    updateMtime(source);
 }
 
 void ProcessSymbolicSequenceVMatrix::build()

Modified: trunk/plearn/vmat/RandomNeighborsDifferencesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/RandomNeighborsDifferencesVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/RandomNeighborsDifferencesVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -111,6 +111,7 @@
     if (source)
         // will not work if source is changed but has the same dimensions
     {
+        updateMtime(source);
         width_ = source->width()*n_neighbors;
         if(append_current_point_indexe) width_ += 1;
         if(append_random_neighbors_indexes) width_ += n_neighbors;

Modified: trunk/plearn/vmat/RandomSamplesFromVMatrix.cc
===================================================================
--- trunk/plearn/vmat/RandomSamplesFromVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/RandomSamplesFromVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -96,6 +96,7 @@
 {
     if(source)
     {
+        updateMtime(source);
         if(flength > 0)
             length_ = int(flength * source->length());
         if(length_ < 0)

Modified: trunk/plearn/vmat/RealFunctionsProcessedVMatrix.cc
===================================================================
--- trunk/plearn/vmat/RealFunctionsProcessedVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/RealFunctionsProcessedVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -98,6 +98,7 @@
 {
     if(source)
     {
+        updateMtime(source);
         length_= source->length();
         if(functions)
         {

Modified: trunk/plearn/vmat/RemapLastColumnVMatrix.cc
===================================================================
--- trunk/plearn/vmat/RemapLastColumnVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/RemapLastColumnVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -99,6 +99,8 @@
     else
         width_=source->width() + n_extra;
 
+    updateMtime(source);
+
     if( !mapping.isEmpty() && n_extra > 0 )
     {
         // width() is different from source->width(),

Modified: trunk/plearn/vmat/RemoveDuplicateVMatrix.cc
===================================================================
--- trunk/plearn/vmat/RemoveDuplicateVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/RemoveDuplicateVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -118,6 +118,9 @@
 ////////////
 void RemoveDuplicateVMatrix::build_()
 {
+    updateMtime(indices_vmat);
+    updateMtime(source);
+
     if (source) {
         DistanceKernel dk;
         dk.pow_distance = true;

Modified: trunk/plearn/vmat/ReorderByMissingVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ReorderByMissingVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/ReorderByMissingVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -131,6 +131,8 @@
 ////////////
 void ReorderByMissingVMatrix::build_()
 {
+    updateMtime(indices_vmat);
+    updateMtime(source);
     if (source) {
         // Construct a vector containing each sample index associated with its
         // missing flags.

Modified: trunk/plearn/vmat/ReplicateSamplesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ReplicateSamplesVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/ReplicateSamplesVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -116,7 +116,10 @@
             "In ReplicateSamplesVMatrix::build_ - The source VMat must have a "
             "targetsize equal to 2 when operating on bags, but its targetsize "
             "is " + tostring(source->targetsize()));
-    
+
+    updateMtime(indices_vmat);
+    updateMtime(source);
+
     // Build the vector of indices.
     indices.resize(0);
     Vec input, target;

Modified: trunk/plearn/vmat/SelectRowsFileIndexVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SelectRowsFileIndexVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/SelectRowsFileIndexVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -68,6 +68,7 @@
 SelectRowsFileIndexVMatrix::build_()
 {
     if (distr) {
+        updateMtime(distr);
         fieldinfos = distr->fieldinfos;
         length_ = indices.length();
         if (length_ == -1) {

Modified: trunk/plearn/vmat/SelectRowsMultiInstanceVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SelectRowsMultiInstanceVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/SelectRowsMultiInstanceVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -208,6 +208,7 @@
 
     // ### In a SourceVMatrix, you will typically end build_() with:
     setMetaInfoFromSource();
+    updateMtime(source_select);
 }
 
 ///////////////

Modified: trunk/plearn/vmat/SeparateInputVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SeparateInputVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/SeparateInputVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -96,6 +96,7 @@
 void SeparateInputVMatrix::build_()
 {
     if (source) {
+        updateMtime(source);
         if(source->inputsize() % nsep != 0)
             PLERROR("In SeparateInputVMatrix::build_(): inputsize=%d of source vmat should be a multiple of nsep=%d",source->inputsize(),nsep);
         inputsize_ = source->inputsize()/nsep;

Modified: trunk/plearn/vmat/ShuffleColumnsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ShuffleColumnsVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/ShuffleColumnsVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -97,6 +97,7 @@
 void ShuffleColumnsVMatrix::build_()
 {
     if (source) {
+        updateMtime(source);
         int n_shuffle = source->width();
         if (only_shuffle_inputs) {
             if (source->inputsize() < 0)

Modified: trunk/plearn/vmat/SortRowsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SortRowsVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/SortRowsVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -133,6 +133,7 @@
     }
     // Construct the indices vector.
     if (source) {
+        updateMtime(source);
         indices = TVec<int>(0, source.length()-1, 1);
         if (sort_columns.length() > 1) {
             // We need to sort many columns: we use the unefficient method.
@@ -158,6 +159,7 @@
         }
         inherited::build(); // Since we have just changed the indices.
     }
+    updateMtime(indices_vmat);
 }
 
 //////////////

Modified: trunk/plearn/vmat/SparseVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SparseVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/SparseVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -52,6 +52,7 @@
     : nelements(0), positions(0), values(0), rows(0)
 {
     load(filename);
+    updateMtime(filename);
 }
 
 SparseVMatrix::SparseVMatrix(VMat m)
@@ -59,6 +60,8 @@
 {
     fieldinfos = m->getFieldInfos();                // Copy the field infos
 
+    updateMtime(m);
+
     if(m.width()>USHRT_MAX)
         PLERROR("In SparseVMatrix constructor: m.width()=%d can't be greater than USHRT_MAX=%d",m.width(),USHRT_MAX);
     Vec v(m.width());

Modified: trunk/plearn/vmat/SplitWiseValidationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SplitWiseValidationVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/SplitWiseValidationVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -92,7 +92,10 @@
     Mat stats_i;
     for(int i=0; i<stats.length(); i++)
     {
-        stats_i = getDataSet(split_stats_ppaths[i])->toMat();
+        VMat v = getDataSet(split_stats_ppaths[i]);
+        updateMtime(v);
+        stats_i = v->toMat();
+        
         if(i==0)
         {
             if(nsplits<0) nsplits = stats_i.length();

Modified: trunk/plearn/vmat/TemporalHorizonVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TemporalHorizonVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/TemporalHorizonVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -157,6 +157,7 @@
 void TemporalHorizonVMatrix::build_()
 {
     if (source) {
+        updateMtime(source);
         length_ = source->length()-horizon;
         width_ = source->width();
         fieldinfos = source->fieldinfos;

Modified: trunk/plearn/vmat/TemporaryDiskVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TemporaryDiskVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/TemporaryDiskVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -95,6 +95,7 @@
     addReferenceToFile(dirname);
     if (indexf)
         last_files_opened.append(dirname / "indexfile");
+    updateMtime(dirname/"indexfile");
     for (int i = 0; i < dataf.length(); i++)
         if (dataf[i])
             last_files_opened.append(dirname / (tostring(i) + ".data"));

Modified: trunk/plearn/vmat/TemporaryFileVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TemporaryFileVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/TemporaryFileVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -118,6 +118,7 @@
         last_filename = filename_;
     } else
         last_filename = "";
+    updateMtime(filename_);
 }
 
 //////////////////////

Modified: trunk/plearn/vmat/TextStreamVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextStreamVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/TextStreamVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -99,6 +99,8 @@
 	for (int i = 0; i < symbol_width; ++i)
 		ids[(unsigned int)symbols[i]] = i;
 
+        updateMtime(data_file);
+
 	int text_length = 0;
 	string text;
 

Modified: trunk/plearn/vmat/ThresholdVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ThresholdVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/ThresholdVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -61,6 +61,7 @@
       gt_threshold(gt_threshold_)
 {
     // build_() isn't defined anyways...
+    updateMtime(the_source);
 }
 
 void ThresholdVMatrix::getNewRow(int i, const Vec& v) const

Modified: trunk/plearn/vmat/UCIDataVMatrix.cc
===================================================================
--- trunk/plearn/vmat/UCIDataVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/UCIDataVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -88,6 +88,13 @@
 {
     inherited::build();
     build_();
+    updateMtime(0);
+//     updateMtime(uci_spec->data_all);
+//     updateMtime(uci_spec->data_train);
+//     updateMtime(uci_spec->data_test);
+//     updateMtime(uci_spec->file_all);
+//     updateMtime(uci_spec->file_train);
+//     updateMtime(uci_spec->file_test);
 }
 
 void UCIDataVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/VMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -1433,9 +1433,9 @@
     else if(t==0)
         mtime_=numeric_limits<time_t>::max();
 }
-void VMatrix::updateMtime(const PPath& p){updateMtime(mtime(p));}
+void VMatrix::updateMtime(const PPath& p){if(!p.empty())updateMtime(mtime(p));}
 
-void VMatrix::updateMtime(VMat v){updateMtime(v->getMtime());}
+void VMatrix::updateMtime(VMat v){if(v)updateMtime(v->getMtime());}
 
 ////////////////////
 // isFileUpToDate //

Modified: trunk/plearn/vmat/VariableDeletionVMatrix.cc
===================================================================
--- trunk/plearn/vmat/VariableDeletionVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/VariableDeletionVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -231,6 +231,9 @@
     if(!source)
         PLERROR("In VariableDeletionVMatrix::build_ - The source VMat do not exist!");
 
+    updateMtime(source);
+    updateMtime(train_set);
+
     int is = source->inputsize();
     if (is < 0)
         PLERROR("In VariableDeletionVMatrix::build_ - The source VMat must "

Modified: trunk/plearn/vmat/VecExtendedVMatrix.cc
===================================================================
--- trunk/plearn/vmat/VecExtendedVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/VecExtendedVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -90,6 +90,7 @@
 {
     if (source)
         fieldinfos = source->getFieldInfos();
+    updateMtime(source);
 }
 
 void VecExtendedVMatrix::declareOptions(OptionList &ol)

Modified: trunk/plearn/vmat/ViewSplitterVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ViewSplitterVMatrix.cc	2008-03-03 14:43:39 UTC (rev 8616)
+++ trunk/plearn/vmat/ViewSplitterVMatrix.cc	2008-03-03 17:45:54 UTC (rev 8617)
@@ -97,6 +97,9 @@
     splitter->setDataSet(source);
     sets = splitter->getSplit(split);
     setMetaInfoFrom(sets[set]);
+    updateMtime(source);
+    for(int i=0;i<sets.length();i++)
+        updateMtime(sets[i]);
 }
 
 ///////////////



From nouiz at mail.berlios.de  Mon Mar  3 19:12:04 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 3 Mar 2008 19:12:04 +0100
Subject: [Plearn-commits] r8618 - in trunk: commands/PLearnCommands
	plearn/misc
Message-ID: <200803031812.m23IC432009764@sheep.berlios.de>

Author: nouiz
Date: 2008-03-03 19:12:03 +0100 (Mon, 03 Mar 2008)
New Revision: 8618

Modified:
   trunk/commands/PLearnCommands/VMatCommand.cc
   trunk/plearn/misc/vmatmain.cc
Log:
Added plearn vmat mtime <dataset>


Modified: trunk/commands/PLearnCommands/VMatCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/VMatCommand.cc	2008-03-03 17:45:54 UTC (rev 8617)
+++ trunk/commands/PLearnCommands/VMatCommand.cc	2008-03-03 18:12:03 UTC (rev 8618)
@@ -116,8 +116,13 @@
         "       Will output the content of <dataset>, using its string mappings\n"
         "   or: vmat compare_stats <dataset1> <dataset2> [stdev threshold] [missing threshold]\n"
         "       Will compare stats from dataset1 to dataset2\n\n"
+        "   or: vmat compare_stats_ks <dataset1> <dataset2> [--mat_to_mem]\n"
+        "       Will compare stats from dataset2 to dataset2 with "
+        "       Kolmogorov-Smirnov 2 samples statistic\n\n"
+        "   or: vmat mtime <dataset>\n"
+        "       Print the mtime of a dataset\n"
+        "<dataset> is a parameter understandable by getDataSet: \n"
 
-        "<dataset> is a parameter understandable by getDataSet: \n"
         + getDataSetHelp()
         ) 
 {}

Modified: trunk/plearn/misc/vmatmain.cc
===================================================================
--- trunk/plearn/misc/vmatmain.cc	2008-03-03 17:45:54 UTC (rev 8617)
+++ trunk/plearn/misc/vmatmain.cc	2008-03-03 18:12:03 UTC (rev 8618)
@@ -54,7 +54,7 @@
 #include <plearn/display/Gnuplot.h>
 #include <plearn/io/openFile.h>
 #include <plearn/base/HelpSystem.h>
-
+#include <plearn/base/PDate.h>
 #include <algorithm>                         // for max
 #include <iostream>
 #include <iomanip>                           // for setw and such
@@ -448,64 +448,7 @@
     if(argc<3)
     {
         // Use the VMatCommand help instead of repeating the same help message twice...
-#if 0
-        cerr << 
-            "Usage: vmat [options] command [params...]\n"
-            "Options:\n"
-            "       -i <indexfile> : use indexfile as index to access the 1st table\n"
-            "Commands:\n"
-            "       vmat info <dataset> \n"
-            "       Will info about dataset (size, etc..)\n"
-            "   or: vmat fields <dataset> [name_only] [transpose]\n"
-            "       To list the fields with their names (if 'name_only' is specified, the indexes won't be displayed,\n"
-            "       and if 'transpose' is also added, the fields will be listed on a single line)\n"
-            "   or: vmat fieldinfo <dataset> <fieldname_or_num>\n"
-            "       To display statistics for that field \n"
-            "   or: vmat cat <dataset> [<optional_vpl_filtering_code>]\n"
-            "       To display the dataset \n"
-            "   or: vmat sascat <dataset.vmat> <dataset.txt>\n"
-            "       To output in <dataset.txt> the dataset in SAS-like tab-separated format with field names on the first line\n"
-            "   or: vmat stats <dataset> \n"
-            "       Will display basic statistics for each field \n"
-            "   or: vmat convert <source> <destination> \n"
-            "       To convert any dataset into a .amat .pmat or .dmat format \n"
-            "       The extension of the destination is used to determine the format you want \n"
-            "   or: vmat gendef <source> [binnum1 binnum2 ...] \n"
-            "       Generate stats for dataset (will put them in its associated metadatadir). \n"
-            "   or: vmat genvmat <source_dataset> <dest_vmat> [binned{num} | onehot{num} | normalized]\n"
-            "       Will generate a template .vmat file with all the fields of the source preprocessed\n"
-            "       with the processing you specify\n"
-            "   or: vmat genkfold <source_dataset> <fileprefix> <kvalue>\n"
-            "       Will generate <kvalue> pairs of .vmat that are splitted so they can be used for kfold trainings\n"
-            "       The first .vmat-pair will be named <fileprefix>_train_1.vmat (all source_dataset except the first 1/k)\n"
-            "       and <fileprefix>_test_1.vmat (the first 1/k of <source_dataset>\n"
-            "   or: vmat diff <dataset1> <dataset2> [<tolerance> [<verbose>]]\n"
-            "       Will report all elements that differ by more than tolerance (defauts to 1e-6) \n"
-            "       If verbose==0 then print only total number of differences \n"
-            "   or: vmat cdf <dataset> [<dataset> ...] \n"
-            "       To interactively display cumulative density function for each field \n"
-            "       along with its basic statistics \n"
-            //      "   or: vmat cond <dataset> <condfield#> \n"
-            //      "       Interactive display of coditional statistics conditioned on the \n"
-            //      "       conditioning field <condfield#> \n"
-            "   or: vmat diststat <dataset> <inputsize>\n"
-            "       Will compute and output basic statistics on the euclidean distance \n"
-            "       between two consecutive input points \n"
-            "   or: vmat catstr <dataset> [separator]\n"
-            "       Will output the content of <dataset>, using its string mappings.\n"
-            "       A column separator can be provided. By default, \"\t\" is used.\n"
-            "   or: vmat compare_stats <dataset1> <dataset2> [stdev threshold] [missing threshold]\n"
-            "       Will compare stats from dataset1 to dataset2\n\n"
-            "   or: vmat compare_stats_ks <dataset1> <dataset2> [--mat_to_mem]"
-            "       Will compare stats from dataset2 to dataset2 with "
-            "Kolmogorov-Smirnov 2 samples statistic\n\n"
-
-            "<dataset> is a parameter understandable by getDataSet. This includes \n"
-            "all matrix file formats. Type 'vmat help dataset' to see what other\n"
-            "<dataset> strings are available." << endl;
-#endif
-        
-        //PLearnCommandRegistry::help("vmat", cout);
+        // help message in file commands/PLearnCommands/VMatCommand.cc
         pout << HelpSystem::helpOnCommand("vmat") << flush;
         exit(0);
     }
@@ -1292,6 +1235,11 @@
         }
 
     }
+    else if(command=="mtime")
+    {    
+        VMat m1 = getVMat(argv[2], indexf);
+        pout<<m1->getMtime()<<endl;
+    }
     else
         PLERROR("Unknown command : %s",command.c_str());
     return 0;



From saintmlx at mail.berlios.de  Mon Mar  3 20:07:10 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Mon, 3 Mar 2008 20:07:10 +0100
Subject: [Plearn-commits] r8619 - trunk/python_modules/plearn/utilities
Message-ID: <200803031907.m23J7AUv014950@sheep.berlios.de>

Author: saintmlx
Date: 2008-03-03 20:07:10 +0100 (Mon, 03 Mar 2008)
New Revision: 8619

Modified:
   trunk/python_modules/plearn/utilities/pldiff.py
Log:
- no progress bars for pldiff



Modified: trunk/python_modules/plearn/utilities/pldiff.py
===================================================================
--- trunk/python_modules/plearn/utilities/pldiff.py	2008-03-03 18:12:03 UTC (rev 8618)
+++ trunk/python_modules/plearn/utilities/pldiff.py	2008-03-03 19:07:10 UTC (rev 8619)
@@ -165,7 +165,7 @@
 
     ## Actual comparison
     report = []
-    diff = toolkit.command_output(plearn_cmd("diff %s %s %s") \
+    diff = toolkit.command_output(plearn_cmd("--no-progress diff %s %s %s") \
                           % (former_rw, later_rw, precision))
     diff = "".join(diff)
     # diff = toldiff(former_rw, later_rw, precision)



From tihocan at mail.berlios.de  Mon Mar  3 20:42:07 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 3 Mar 2008 20:42:07 +0100
Subject: [Plearn-commits] r8620 - trunk/plearn/vmat
Message-ID: <200803031942.m23Jg7SD019994@sheep.berlios.de>

Author: tihocan
Date: 2008-03-03 20:42:06 +0100 (Mon, 03 Mar 2008)
New Revision: 8620

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
On a PPath, use isEmpty() instead of empty()

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-03-03 19:07:10 UTC (rev 8619)
+++ trunk/plearn/vmat/VMatrix.cc	2008-03-03 19:42:06 UTC (rev 8620)
@@ -1433,7 +1433,7 @@
     else if(t==0)
         mtime_=numeric_limits<time_t>::max();
 }
-void VMatrix::updateMtime(const PPath& p){if(!p.empty())updateMtime(mtime(p));}
+void VMatrix::updateMtime(const PPath& p){if(!p.isEmpty())updateMtime(mtime(p));}
 
 void VMatrix::updateMtime(VMat v){if(v)updateMtime(v->getMtime());}
 



From tihocan at mail.berlios.de  Mon Mar  3 20:43:42 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 3 Mar 2008 20:43:42 +0100
Subject: [Plearn-commits] r8621 - in trunk: plearn/math plearn_learners/misc
Message-ID: <200803031943.m23JhgPU020160@sheep.berlios.de>

Author: tihocan
Date: 2008-03-03 20:43:36 +0100 (Mon, 03 Mar 2008)
New Revision: 8621

Modified:
   trunk/plearn/math/pl_math.cc
   trunk/plearn/math/pl_math.h
   trunk/plearn_learners/misc/Grapher.cc
Log:
Removed extra logadd function that was not needed, instead it is just the code of Grapher.cc that had to be fixed to compile with -float

Modified: trunk/plearn/math/pl_math.cc
===================================================================
--- trunk/plearn/math/pl_math.cc	2008-03-03 19:42:06 UTC (rev 8620)
+++ trunk/plearn/math/pl_math.cc	2008-03-03 19:43:36 UTC (rev 8621)
@@ -179,10 +179,6 @@
 {
     return logadd(double(log_a), double(log_b));
 }
-real logadd(double log_a, real log_b)
-{
-    return logadd(log_a, double(log_b));
-}
 #endif
 
 real square_f(real x)

Modified: trunk/plearn/math/pl_math.h
===================================================================
--- trunk/plearn/math/pl_math.h	2008-03-03 19:42:06 UTC (rev 8620)
+++ trunk/plearn/math/pl_math.h	2008-03-03 19:43:36 UTC (rev 8621)
@@ -572,7 +572,6 @@
 #ifdef USEFLOAT
 //! Required in some template functions when compiling in float mode.
 real logadd(real log_a, real log_b);
-real logadd(double log_a, real log_b);
 #endif
 
 //!  compute log(exp(log_a)-exp(log_b)) without losing too much precision

Modified: trunk/plearn_learners/misc/Grapher.cc
===================================================================
--- trunk/plearn_learners/misc/Grapher.cc	2008-03-03 19:42:06 UTC (rev 8620)
+++ trunk/plearn_learners/misc/Grapher.cc	2008-03-03 19:43:36 UTC (rev 8621)
@@ -442,7 +442,7 @@
             if(is_equal(logsum, -FLT_MAX))
                 logsum = output[0];
             else 
-                logsum = logadd(logsum, output[0]);
+                logsum = logadd(logsum, double(output[0]));
             pb.update(n++);
         }
     }



From nouiz at mail.berlios.de  Mon Mar  3 21:12:57 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 3 Mar 2008 21:12:57 +0100
Subject: [Plearn-commits] r8622 - trunk/plearn/vmat
Message-ID: <200803032012.m23KCvPQ023118@sheep.berlios.de>

Author: nouiz
Date: 2008-03-03 21:12:56 +0100 (Mon, 03 Mar 2008)
New Revision: 8622

Modified:
   trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
Log:
-correctly check if precomputed file are uptodate
-added a none imputation mode
-added PLERROR
-code cleanup


Modified: trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-03-03 19:43:36 UTC (rev 8621)
+++ trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-03-03 20:12:56 UTC (rev 8622)
@@ -116,10 +116,16 @@
 { 
   real variable_value = source->get(i, j);
   if (!is_missing(variable_value)) return variable_value;
-  if (variable_imputation_instruction[j] == 1) return variable_mean[j];
-  if (variable_imputation_instruction[j] == 2) return variable_median[j];
-  if (variable_imputation_instruction[j] == 3) return variable_mode[j];
-  return variable_value;
+  else if (variable_imputation_instruction[j] == 1) return variable_mean[j];
+  else if (variable_imputation_instruction[j] == 2) return variable_median[j];
+  else if (variable_imputation_instruction[j] == 3) return variable_mode[j];
+  else if (variable_imputation_instruction[j] == 4) return variable_value;
+  else
+    PLERROR("In MeanMedianModeImputationVMatrix::get(%d,%d) - "
+	    "unknow variable_imputation_instruction value of %d",i,j,
+	    variable_imputation_instruction[j] );
+  //Should not be executed, to remove a warning
+  return MISSING_VALUE;
 }
 
 void MeanMedianModeImputationVMatrix::getSubRow(int i, int j, Vec v) const
@@ -127,9 +133,19 @@
   source->getSubRow(i, j, v);
   for (int source_col = 0; source_col < v->length(); source_col++) 
     if (is_missing(v[source_col]))
-      if (variable_imputation_instruction[source_col + j] == 1) v[source_col] = variable_mean[source_col + j];
-      else if (variable_imputation_instruction[source_col + j] == 2) v[source_col] = variable_median[source_col + j];
-      else if (variable_imputation_instruction[source_col + j] == 3) v[source_col] = variable_mode[source_col + j];
+      if (variable_imputation_instruction[source_col + j] == 1)
+	v[source_col] = variable_mean[source_col + j];
+      else if (variable_imputation_instruction[source_col + j] == 2)
+	v[source_col] = variable_median[source_col + j];
+      else if (variable_imputation_instruction[source_col + j] == 3)
+	v[source_col] = variable_mode[source_col + j];
+      else if (variable_imputation_instruction[source_col + j] == 4)
+	;//do nothing
+      else
+	PLERROR("In MeanMedianModeImputationVMatrix::getSubRow(%d,%d, Vec) - "
+		"unknow variable_imputation_instruction value of %d",i,j,
+		variable_imputation_instruction[source_col + j] );
+
 }
 
 void MeanMedianModeImputationVMatrix::getRow(int i, Vec v) const
@@ -137,9 +153,19 @@
   source-> getRow(i, v);
   for (int source_col = 0; source_col < v->length(); source_col++)
     if (is_missing(v[source_col]))
-      if (variable_imputation_instruction[source_col] == 1) v[source_col] = variable_mean[source_col];
-      else if (variable_imputation_instruction[source_col] == 2) v[source_col] = variable_median[source_col];
-      else if (variable_imputation_instruction[source_col] == 3) v[source_col] = variable_mode[source_col]; 
+      if (variable_imputation_instruction[source_col] == 1)
+	v[source_col] = variable_mean[source_col];
+      else if (variable_imputation_instruction[source_col] == 2)
+	v[source_col] = variable_median[source_col];
+      else if (variable_imputation_instruction[source_col] == 3)
+	v[source_col] = variable_mode[source_col]; 
+      else if (variable_imputation_instruction[source_col] == 4)
+	;//do nothing
+      else
+	PLERROR("In MeanMedianModeImputationVMatrix::getRow(%d, Vec) - "
+		"unknow variable_imputation_instruction value of %d",i,
+		variable_imputation_instruction[source_col] );
+  
 }
 
 void MeanMedianModeImputationVMatrix::getColumn(int i, Vec v) const
@@ -150,6 +176,13 @@
       if (variable_imputation_instruction[i] == 1) v[source_row] = variable_mean[i];
       else if (variable_imputation_instruction[i] == 2) v[source_row] = variable_median[i];
       else if (variable_imputation_instruction[i] == 3) v[source_row] = variable_mode[i];
+      else if (variable_imputation_instruction[i] == 4)
+	;//do nothing
+      else
+	PLERROR("In MeanMedianModeImputationVMatrix::getRow(%d, Vec) - "
+		"unknow variable_imputation_instruction value of %d",i,
+		variable_imputation_instruction[i] );
+
 }
 
 
@@ -164,31 +197,25 @@
     updateMtime(source);
 
     int train_length = train_set->length();
+    int train_width = train_set->width();
+
     if (number_of_train_samples_to_use > 0.0)
         if (number_of_train_samples_to_use < 1.0) train_length = (int) (number_of_train_samples_to_use * (real) train_length);
         else train_length = (int) number_of_train_samples_to_use;
     if (train_length > train_set->length()) train_length = train_set->length();
-    if(train_length < 1) PLERROR("In MeanMedianModeImputationVMatrix::length of the number of train samples to use must be at least 1, got: %i", train_length);
-    int train_width = train_set->width();
-    int train_targetsize = train_set->targetsize();
-    int train_weightsize = train_set->weightsize();
-    int train_inputsize = train_set->inputsize();
-    if(train_inputsize < 1) PLERROR("In MeanMedianModeImputationVMatrix::inputsize of the train vmat must be supplied, got : %i", train_inputsize);
-    int source_width = source->width();
-    int source_targetsize = source->targetsize();
-    int source_weightsize = source->weightsize();
-    int source_inputsize = source->inputsize();
-    if (train_width != source_width) PLERROR("In MeanMedianModeImputationVMatrix::train set and source width must agree, got : %i, %i", train_width, source_width);
-    if (train_targetsize != source_targetsize) PLERROR("In MeanMedianModeImputationVMatrix::train set and source targetsize must agree, got : %i, %i", train_targetsize, source_targetsize);
-    if (train_weightsize != source_weightsize) PLERROR("In MeanMedianModeImputationVMatrix::train set and source weightsize must agree, got : %i, %i", train_weightsize, source_weightsize);
-    if (train_inputsize != source_inputsize) PLERROR("In MeanMedianModeImputationVMatrix::train set and source inputsize must agree, got : %i, %i", train_inputsize, source_inputsize);
+    if(train_length < 1) 
+      PLERROR("In MeanMedianModeImputationVMatrix::length of the number of"
+	      " train samples to use must be at least 1, got: %i", train_length);
+
+    if(train_set->inputsize() < 1) 
+      PLERROR("In MeanMedianModeImputationVMatrix::inputsize of the train vmat"
+	      " must be supplied, got : %i", train_set->inputsize());
+    source->compatibleSizeError(train_set);
+    setMetaInfoFrom(source);
+
     train_field_names.resize(train_width);
     train_field_names = train_set->fieldNames();
-    length_ = source->length();
-    width_ = source_width;
-    inputsize_ = source_inputsize;
-    targetsize_ = source_targetsize;
-    weightsize_ = source_weightsize;
+
     declareFieldNames(train_field_names);
     variable_mean.resize(train_width);
     variable_median.resize(train_width);
@@ -210,7 +237,9 @@
         if (imputation_spec[spec_col].second == "mean") variable_imputation_instruction[train_col] = 1;
         else if (imputation_spec[spec_col].second == "median") variable_imputation_instruction[train_col] = 2;
         else if (imputation_spec[spec_col].second == "mode") variable_imputation_instruction[train_col] = 3;
-        else PLERROR("In MeanMedianModeImputationVMatrix: unsupported imputation instruction: %s : %s",
+        else if (imputation_spec[spec_col].second == "none") variable_imputation_instruction[train_col] = 4;
+        else
+	  PLERROR("In MeanMedianModeImputationVMatrix: unsupported imputation instruction: %s : %s",
 		     (imputation_spec[spec_col].first).c_str(), (imputation_spec[spec_col].second).c_str());
     }
     if(nofields.length()>0)
@@ -223,10 +252,11 @@
     if(no_instruction.size()>0)
       PLWARNING("In MeanMedianModeImputationVMatrix::build_() In the source VMatrix their is %d field(s) that do not have instruction: '%s'.",
 		no_instruction.size(),tostring(no_instruction).c_str());
+
     PPath train_metadata = train_set->getMetaDataDir();
     PPath mean_median_mode_file_name = train_metadata + "mean_median_mode_file.pmat";
     train_set->lockMetaDataDir();
-    if (!isfile(mean_median_mode_file_name))
+    if (!train_set->isFileUpToDate(mean_median_mode_file_name,true,true))
     {
         computeMeanMedianModeVectors();
         createMeanMedianModeFile(mean_median_mode_file_name);



From chrish at mail.berlios.de  Tue Mar  4 17:08:08 2008
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Tue, 4 Mar 2008 17:08:08 +0100
Subject: [Plearn-commits] r8623 - trunk/scripts
Message-ID: <200803041608.m24G88xb031758@sheep.berlios.de>

Author: chrish
Date: 2008-03-04 17:08:08 +0100 (Tue, 04 Mar 2008)
New Revision: 8623

Modified:
   trunk/scripts/checkheader
Log:
Make checkheader executable


Property changes on: trunk/scripts/checkheader
___________________________________________________________________
Name: svn:executable
   + *



From saintmlx at mail.berlios.de  Tue Mar  4 19:59:33 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 4 Mar 2008 19:59:33 +0100
Subject: [Plearn-commits] r8624 - trunk/python_modules/plearn/utilities
Message-ID: <200803041859.m24IxXow002936@sheep.berlios.de>

Author: saintmlx
Date: 2008-03-04 19:59:33 +0100 (Tue, 04 Mar 2008)
New Revision: 8624

Modified:
   trunk/python_modules/plearn/utilities/pldiff.py
Log:
- no progreess bars for pldiff



Modified: trunk/python_modules/plearn/utilities/pldiff.py
===================================================================
--- trunk/python_modules/plearn/utilities/pldiff.py	2008-03-04 16:08:08 UTC (rev 8623)
+++ trunk/python_modules/plearn/utilities/pldiff.py	2008-03-04 18:59:33 UTC (rev 8624)
@@ -37,7 +37,7 @@
 
 from plearn.math import floats_are_equal
 
-_plearn_cmd = "%s --no-version --verbosity VLEVEL_IMP"
+_plearn_cmd = "%s --no-progress --no-version --verbosity VLEVEL_IMP"
 _plearn_exec = "plearn_tests"
 def set_plearn_exec(plearn_exec):
     global _plearn_exec
@@ -165,7 +165,7 @@
 
     ## Actual comparison
     report = []
-    diff = toolkit.command_output(plearn_cmd("--no-progress diff %s %s %s") \
+    diff = toolkit.command_output(plearn_cmd("diff %s %s %s") \
                           % (former_rw, later_rw, precision))
     diff = "".join(diff)
     # diff = toldiff(former_rw, later_rw, precision)



From nouiz at mail.berlios.de  Tue Mar  4 20:16:46 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 4 Mar 2008 20:16:46 +0100
Subject: [Plearn-commits] r8625 - trunk/plearn/vmat
Message-ID: <200803041916.m24JGkc0005058@sheep.berlios.de>

Author: nouiz
Date: 2008-03-04 20:16:45 +0100 (Tue, 04 Mar 2008)
New Revision: 8625

Modified:
   trunk/plearn/vmat/FilteredVMatrix.cc
Log:
small refactoring


Modified: trunk/plearn/vmat/FilteredVMatrix.cc
===================================================================
--- trunk/plearn/vmat/FilteredVMatrix.cc	2008-03-04 18:59:33 UTC (rev 8624)
+++ trunk/plearn/vmat/FilteredVMatrix.cc	2008-03-04 19:16:45 UTC (rev 8625)
@@ -90,7 +90,7 @@
 
 
     lockMetaDataDir();
-    if(isfile(idxfname) && mtime(idxfname)>=getMtime())
+    if(isFileUpToDate(idxfname))
         indexes.open(idxfname);
     else  // let's (re)create the index
     {



From nouiz at mail.berlios.de  Tue Mar  4 20:18:09 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 4 Mar 2008 20:18:09 +0100
Subject: [Plearn-commits] r8626 - trunk/plearn/vmat
Message-ID: <200803041918.m24JI9Za005164@sheep.berlios.de>

Author: nouiz
Date: 2008-03-04 20:18:08 +0100 (Tue, 04 Mar 2008)
New Revision: 8626

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
better error msg


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-03-04 19:16:45 UTC (rev 8625)
+++ trunk/plearn/vmat/VMatrix.cc	2008-03-04 19:18:08 UTC (rev 8626)
@@ -1454,8 +1454,8 @@
                   path.absolute().c_str());
     if(warning_older && exist && !uptodate)
         PLWARNING("In VMatrix::isFileUpToDate - File '%s' is older than this "
-                  "VMat's last modification time, and cannot be re-used",
-                  path.absolute().c_str());
+                  "VMat's mtime of %d, and cannot be re-used",
+                  path.absolute().c_str(), getMtime());
 
     return exist && uptodate;
 }



From nouiz at mail.berlios.de  Tue Mar  4 20:20:32 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 4 Mar 2008 20:20:32 +0100
Subject: [Plearn-commits] r8627 - trunk/plearn/misc
Message-ID: <200803041920.m24JKWUT005631@sheep.berlios.de>

Author: nouiz
Date: 2008-03-04 20:20:32 +0100 (Tue, 04 Mar 2008)
New Revision: 8627

Modified:
   trunk/plearn/misc/vmatmain.cc
Log:
better printing of information


Modified: trunk/plearn/misc/vmatmain.cc
===================================================================
--- trunk/plearn/misc/vmatmain.cc	2008-03-04 19:18:08 UTC (rev 8626)
+++ trunk/plearn/misc/vmatmain.cc	2008-03-04 19:20:32 UTC (rev 8627)
@@ -1158,8 +1158,9 @@
                 threshold_fail++;
             }
         }
-        pout << "Their is "<<threshold_fail<<" variable that are under the threshold"<<endl;
-        pout <<"Kolmogorow Smirnow two sample test end"<<endl<<endl;
+        pout << "Their is "<<threshold_fail<<"/"<<m1->width()<<
+            " variable that are under the threshold"<<endl<<
+            " Kolmogorow Smirnow two sample test end"<<endl<<endl;
 
 
 //         real stderror_threshold = 1;



From nouiz at mail.berlios.de  Tue Mar  4 20:39:28 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 4 Mar 2008 20:39:28 +0100
Subject: [Plearn-commits] r8628 - trunk/plearn_learners/regressors
Message-ID: <200803041939.m24JdSxJ007144@sheep.berlios.de>

Author: nouiz
Date: 2008-03-04 20:39:28 +0100 (Tue, 04 Mar 2008)
New Revision: 8628

Modified:
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
   trunk/plearn_learners/regressors/RegressionTreeNode.h
Log:
added function RegressionTreeNode::getSplitCol(),
made some function const


Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-03-04 19:20:32 UTC (rev 8627)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-03-04 19:39:28 UTC (rev 8628)
@@ -344,13 +344,13 @@
     return split_col;
 }
 
-int RegressionTreeNode::getSplitBalance()
+int RegressionTreeNode::getSplitBalance()const
 {
     if (split_col < 0) return train_set->length();
     return split_balance;
 }
 
-real RegressionTreeNode::getErrorImprovment()
+real RegressionTreeNode::getErrorImprovment()const
 {
     if (split_col < 0) return -1.0;
     real err=leave_error[0] + leave_error[1] - after_split_error;
@@ -358,6 +358,11 @@
     return err;
 }
 
+intt RegressionTreeNode::getSplitCol()const
+{
+    return split_col;
+}
+
 TVec< PP<RegressionTreeNode> > RegressionTreeNode::getNodes()
 {
     TVec< PP<RegressionTreeNode> > return_value;

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.h	2008-03-04 19:20:32 UTC (rev 8627)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.h	2008-03-04 19:39:28 UTC (rev 8628)
@@ -98,8 +98,9 @@
     void         compareSplit(int col, real left_leave_last_feature, real right_leave_first_feature,
                               Vec left_error, Vec right_error, Vec missing_error);
     int          expandNode();
-    int          getSplitBalance();
-    real         getErrorImprovment();
+    int          getSplitBalance()const;
+    real         getErrorImprovment()const;
+    int          getSplitCol() const;
     TVec< PP<RegressionTreeNode> >  getNodes();
     void         computeOutput(const Vec& inputv, Vec& outputv);
     



From nouiz at mail.berlios.de  Tue Mar  4 20:43:35 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 4 Mar 2008 20:43:35 +0100
Subject: [Plearn-commits] r8629 - trunk/plearn_learners/regressors
Message-ID: <200803041943.m24JhZY6007656@sheep.berlios.de>

Author: nouiz
Date: 2008-03-04 20:43:35 +0100 (Tue, 04 Mar 2008)
New Revision: 8629

Modified:
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
Log:
typo


Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-03-04 19:39:28 UTC (rev 8628)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-03-04 19:43:35 UTC (rev 8629)
@@ -358,7 +358,7 @@
     return err;
 }
 
-intt RegressionTreeNode::getSplitCol()const
+int RegressionTreeNode::getSplitCol()const
 {
     return split_col;
 }



From larocheh at mail.berlios.de  Wed Mar  5 20:03:19 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 5 Mar 2008 20:03:19 +0100
Subject: [Plearn-commits] r8630 - trunk/plearn_learners/online
Message-ID: <200803051903.m25J3Jap027409@sheep.berlios.de>

Author: larocheh
Date: 2008-03-05 20:03:19 +0100 (Wed, 05 Mar 2008)
New Revision: 8630

Modified:
   trunk/plearn_learners/online/RBMWoodsLayer.cc
   trunk/plearn_learners/online/RBMWoodsLayer.h
Log:
Added an option to have samples in {-1,1}


Modified: trunk/plearn_learners/online/RBMWoodsLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMWoodsLayer.cc	2008-03-04 19:43:35 UTC (rev 8629)
+++ trunk/plearn_learners/online/RBMWoodsLayer.cc	2008-03-05 19:03:19 UTC (rev 8630)
@@ -53,7 +53,8 @@
 RBMWoodsLayer::RBMWoodsLayer( real the_learning_rate ) :
     inherited( the_learning_rate ),
     n_trees( 10 ),
-    tree_depth( 3 )
+    tree_depth( 3 ),
+    use_signed_samples( false )
 {
 }
 
@@ -68,7 +69,10 @@
     PLCHECK_MSG(expectation_is_up_to_date, "Expectation should be computed "
             "before calling generateSample()");
 
-    sample.clear();
+    if(use_signed_samples)
+        sample.fill(-1);
+    else
+        sample.clear();
 
     int n_nodes_per_tree = size / n_trees;    
     int node, depth, node_sample, sub_tree_size;
@@ -83,8 +87,11 @@
         {
             node_sample = random_gen->binomial_sample( 
                 local_node_expectation[ node + offset ] );
-            sample[node + offset] = node_sample;
-
+            if( use_signed_samples )
+                sample[node + offset] = 2*node_sample-1;
+            else
+                sample[node + offset] = node_sample;
+            
             // Descending in the tree
             sub_tree_size /= 2;
             if ( node_sample > 0.5 )
@@ -115,156 +122,12 @@
 
 void RBMWoodsLayer::computeProbabilisticClustering(Vec& prob_clusters)
 {
-    int n_nodes_per_tree = size / n_trees;    
-    int n_leaves = n_nodes_per_tree+1;
-    prob_clusters.resize( n_trees * n_leaves );
-    int node, depth, sub_tree_size, grand_parent;
+    computeExpectation();
     int offset = 0;
-    bool left_of_grand_parent;
-    real grand_parent_prob;
-
-    // Get local expectations at every node
-    
-    // Divide and conquer computation of local (conditional) free energies
+    int n_nodes_per_tree = size / n_trees;
+    prob_clusters.resize(n_trees*(n_nodes_per_tree+1));
     for( int t=0; t<n_trees; t++ )
     {
-        depth = tree_depth-1;
-        sub_tree_size = 0;
-
-        // Initialize last level
-        for( int n=sub_tree_size; n<n_nodes_per_tree; n += 2*sub_tree_size + 2 )
-        {
-            //on_free_energy[ n + offset ] = safeexp(activation[n+offset]);
-            //off_free_energy[ n + offset ] = 1;
-            // Now working in log-domain
-            on_free_energy[ n + offset ] = activation[n+offset];
-            off_free_energy[ n + offset ] = 0;
-            
-        }
-
-        depth = tree_depth-2;
-        sub_tree_size = 1;
-
-        while( depth >= 0 )
-        {
-            for( int n=sub_tree_size; n<n_nodes_per_tree; n += 2*sub_tree_size + 2 )
-            {
-                //on_free_energy[ n + offset ] = safeexp(activation[n+offset]) * 
-                //    ( on_free_energy[n + offset - sub_tree_size] + off_free_energy[n + offset - sub_tree_size] ) ;
-                //off_free_energy[ n + offset ] = 
-                //    ( on_free_energy[n + offset + sub_tree_size] + off_free_energy[n + offset + sub_tree_size] ) ;
-                // Now working in log-domain
-                on_free_energy[ n + offset ] = activation[n+offset] + 
-                    logadd( on_free_energy[n + offset - sub_tree_size],
-                            off_free_energy[n + offset - sub_tree_size] ) ;
-                off_free_energy[ n + offset ] = 
-                    logadd( on_free_energy[n + offset + sub_tree_size],
-                            off_free_energy[n + offset + sub_tree_size] ) ;
-
-            }
-            sub_tree_size = 2 * ( sub_tree_size + 1 ) - 1;
-            depth--;
-        }
-        offset += n_nodes_per_tree;
-    }    
-    
-    for( int i=0 ; i<size ; i++ )
-        //local_node_expectation[i] = on_free_energy[i] / ( on_free_energy[i] + off_free_energy[i] );
-        // Now working in log-domain
-        local_node_expectation[i] = safeexp(on_free_energy[i] 
-                                            - logadd(on_free_energy[i], off_free_energy[i]));
-
-    // Compute marginal expectations over clustering
-    offset = 0;
-    for( int t=0; t<n_trees; t++ )
-    {
-        // Initialize root        
-        node = n_nodes_per_tree / 2;
-        expectation[ node + offset ] = local_node_expectation[ node + offset ];
-        off_expectation[ node + offset ] = (1 - local_node_expectation[ node + offset ]);
-        sub_tree_size = node;
-
-        // First level nodes
-        depth = 1;
-        sub_tree_size /= 2;
-
-        // Left child
-        node = sub_tree_size;
-        expectation[ node + offset ] = local_node_expectation[ node + offset ]
-            * local_node_expectation[ node + offset + sub_tree_size + 1 ];
-        off_expectation[ node + offset ] = (1 - local_node_expectation[ node + offset ])
-            * local_node_expectation[ node + offset + sub_tree_size + 1 ];
-        
-        // Right child
-        node = 3*sub_tree_size+2;
-        expectation[ node + offset ] = local_node_expectation[ node + offset ]
-            * (1 - local_node_expectation[ node + offset - sub_tree_size - 1 ]);
-        off_expectation[ node + offset ] = (1 - local_node_expectation[ node + offset ])
-            * (1 - local_node_expectation[ node + offset - sub_tree_size - 1 ]);
-
-        // Set other nodes, level-wise
-        depth = 2;
-        sub_tree_size /= 2;
-        while( depth < tree_depth )
-        {
-            // Left child
-            left_of_grand_parent = true;
-            for( int n=sub_tree_size; n<n_nodes_per_tree; n += 4*sub_tree_size + 4 )
-            {
-                if( left_of_grand_parent )
-                {
-                    grand_parent = n + offset + 3*sub_tree_size + 3;
-                    grand_parent_prob = expectation[ grand_parent ];
-                    left_of_grand_parent = false;
-                }
-                else
-                {
-                    grand_parent = n + offset - sub_tree_size - 1;
-                    grand_parent_prob = off_expectation[ grand_parent ];
-                    left_of_grand_parent = true;
-                }
-
-                expectation[ n + offset ] = local_node_expectation[ n + offset ]
-                    * local_node_expectation[ n + offset + sub_tree_size + 1 ]
-                    * grand_parent_prob;
-                off_expectation[ n + offset ] = (1 - local_node_expectation[ n + offset ])
-                    * local_node_expectation[ n + offset + sub_tree_size + 1 ]
-                    * grand_parent_prob;
-            }
-
-            // Right child
-            left_of_grand_parent = true;
-            for( int n=3*sub_tree_size+2; n<n_nodes_per_tree; n += 4*sub_tree_size + 4 )
-            {
-                if( left_of_grand_parent )
-                {
-                    grand_parent = n + offset + sub_tree_size + 1;
-                    grand_parent_prob = expectation[ grand_parent ];
-                    left_of_grand_parent = false;
-                }
-                else
-                {
-                    grand_parent = n + offset - 3*sub_tree_size - 3;
-                    grand_parent_prob = off_expectation[ grand_parent ];
-                    left_of_grand_parent = true;
-                }
-
-                expectation[ n + offset ] = local_node_expectation[ n + offset ]
-                    * (1 - local_node_expectation[ n + offset - sub_tree_size - 1 ])
-                    * grand_parent_prob;
-                off_expectation[ n + offset ] = (1 - local_node_expectation[ n + offset ])
-                    * (1 - local_node_expectation[ n + offset - sub_tree_size - 1 ])
-                    * grand_parent_prob;
-            }
-            sub_tree_size /= 2;
-            depth++;
-        }
-        offset += n_nodes_per_tree;
-    }
-
-    offset = 0;
-    for( int t=0; t<n_trees; t++ )
-    {
         for( int i=0; i<n_nodes_per_tree; i = i+2)
             prob_clusters[i+offset+t] = expectation[i+offset];
         for( int i=0; i<n_nodes_per_tree; i = i+2)
@@ -302,8 +165,10 @@
             //off_free_energy[ n + offset ] = 1;
             // Now working in log-domain
             on_free_energy[ n + offset ] = activation[n+offset];
-            off_free_energy[ n + offset ] = 0;
-            
+            if( use_signed_samples )
+                off_free_energy[ n + offset ] = -activation[n+offset];
+            else
+                off_free_energy[ n + offset ] = 0;
         }
 
         depth = tree_depth-2;
@@ -321,10 +186,15 @@
                 on_free_energy[ n + offset ] = activation[n+offset] + 
                     logadd( on_free_energy[n + offset - (sub_tree_size/2+1)],
                             off_free_energy[n + offset - (sub_tree_size/2+1)] ) ;
-                off_free_energy[ n + offset ] = 
-                    logadd( on_free_energy[n + offset + (sub_tree_size/2+1)],
-                            off_free_energy[n + offset + (sub_tree_size/2+1)] ) ;
-
+                if( use_signed_samples )
+                    off_free_energy[ n + offset ] = -activation[n+offset] +
+                        logadd( on_free_energy[n + offset + (sub_tree_size/2+1)],
+                                off_free_energy[n + offset + (sub_tree_size/2+1)] ) ;
+                else
+                    off_free_energy[ n + offset ] = 
+                        logadd( on_free_energy[n + offset + (sub_tree_size/2+1)],
+                                off_free_energy[n + offset + (sub_tree_size/2+1)] ) ;
+                
             }
             sub_tree_size = 2 * ( sub_tree_size + 1 ) - 1;
             depth--;
@@ -472,7 +342,10 @@
             //off_free_energy[ n + offset ] = 1;
             // Now working in log-domain
             on_free_energy[ n + offset ] = input[n+offset] + bias[n+offset];
-            off_free_energy[ n + offset ] = 0;            
+            if( use_signed_samples )
+                off_free_energy[ n + offset ] = -(input[n+offset] + bias[n+offset]);
+            else
+                off_free_energy[ n + offset ] = 0;
         }
 
         depth = tree_depth-2;
@@ -490,9 +363,14 @@
                 on_free_energy[ n + offset ] = input[n+offset] + bias[n+offset] +
                     logadd( on_free_energy[n + offset - (sub_tree_size/2+1)], 
                             off_free_energy[n + offset - (sub_tree_size/2+1)] ) ;
-                off_free_energy[ n + offset ] = 
-                    logadd( on_free_energy[n + offset + (sub_tree_size/2+1)], 
-                            off_free_energy[n + offset + (sub_tree_size/2+1)] ) ;
+                if( use_signed_samples )
+                    off_free_energy[ n + offset ] = -(input[n+offset] + bias[n+offset]) +
+                        logadd( on_free_energy[n + offset + (sub_tree_size/2+1)], 
+                                off_free_energy[n + offset + (sub_tree_size/2+1)] ) ;
+                else
+                    off_free_energy[ n + offset ] = 
+                        logadd( on_free_energy[n + offset + (sub_tree_size/2+1)], 
+                                off_free_energy[n + offset + (sub_tree_size/2+1)] ) ;
             }
             sub_tree_size = 2 * ( sub_tree_size + 1 ) - 1;
             depth--;
@@ -826,6 +704,8 @@
 
                 out_grad = off_free_energy_gradient[ n + offset ];
                 node_exp = local_node_expectation[n + offset + (sub_tree_size/2+1)];
+                if( use_signed_samples )
+                    input_gradient[n+offset] -= out_grad;
                 on_free_energy_gradient[n + offset + (sub_tree_size/2+1)] += out_grad * node_exp; 
                 off_free_energy_gradient[n + offset + (sub_tree_size/2+1)] += 
                     out_grad * (1 - node_exp); 
@@ -838,7 +718,11 @@
         sub_tree_size = 0;
 
         for( int n=sub_tree_size; n<n_nodes_per_tree; n += 2*sub_tree_size + 2 )
+        {
             input_gradient[n+offset] += on_free_energy_gradient[ n + offset ];
+            if( use_signed_samples )
+                input_gradient[n+offset] -= off_free_energy_gradient[ n + offset ];                
+        }
 
         offset += n_nodes_per_tree;
     }
@@ -1084,6 +968,10 @@
                   "Depth of the trees in the woods (1 gives the ordinary "
                   "RBMBinomialLayer).");
 
+    declareOption(ol, "use_signed_samples", &RBMWoodsLayer::use_signed_samples,
+                  OptionBase::buildoption,
+                  "Indication that samples should be in {-1,1}, not {0,1}.");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }

Modified: trunk/plearn_learners/online/RBMWoodsLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMWoodsLayer.h	2008-03-04 19:43:35 UTC (rev 8629)
+++ trunk/plearn_learners/online/RBMWoodsLayer.h	2008-03-05 19:03:19 UTC (rev 8630)
@@ -62,6 +62,9 @@
     // Depth of the trees in the woods (1 gives the ordinary RBMBinomialLayer)
     int tree_depth;
 
+    // Indication that samples should be in {-1,1}, not {0,1}
+    bool use_signed_samples;
+
 public:
     //#####  Public Member Functions  #########################################
 



From tihocan at mail.berlios.de  Thu Mar  6 20:00:20 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 6 Mar 2008 20:00:20 +0100
Subject: [Plearn-commits] r8631 - trunk/plearn/vmat
Message-ID: <200803061900.m26J0KJk021486@sheep.berlios.de>

Author: tihocan
Date: 2008-03-06 20:00:20 +0100 (Thu, 06 Mar 2008)
New Revision: 8631

Modified:
   trunk/plearn/vmat/ProcessingVMatrix.cc
Log:
Added safety check to make sure sizes and width are compatible

Modified: trunk/plearn/vmat/ProcessingVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ProcessingVMatrix.cc	2008-03-05 19:03:19 UTC (rev 8630)
+++ trunk/plearn/vmat/ProcessingVMatrix.cc	2008-03-06 19:00:20 UTC (rev 8631)
@@ -156,6 +156,10 @@
             weightsize_ = 0;
         if(extrasize_<0)
             extrasize_ = 0;            
+        
+        PLCHECK_MSG(
+                width_ ==  inputsize_ + targetsize_ + weightsize_ + extrasize_,
+                "Width does not match sizes!");
     }
     else // use the *_prg
     {



From nouiz at mail.berlios.de  Thu Mar  6 21:01:53 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 6 Mar 2008 21:01:53 +0100
Subject: [Plearn-commits] r8632 - trunk/plearn/vmat
Message-ID: <200803062001.m26K1rxV026457@sheep.berlios.de>

Author: nouiz
Date: 2008-03-06 21:01:53 +0100 (Thu, 06 Mar 2008)
New Revision: 8632

Modified:
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
Added function: VMatrix::compute_missing_size_value()
      if only one of inputsize, targetsize, weightsize, extrasize
      is less unknow and width>0, we compute its value


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-03-06 19:00:20 UTC (rev 8631)
+++ trunk/plearn/vmat/VMatrix.cc	2008-03-06 20:01:53 UTC (rev 8632)
@@ -1251,7 +1251,7 @@
 }
 
 void VMatrix::compatibleSizeError(const VMat& m){
-#define MY_PRINT_ERROR_MST(NAME) PLERROR("In VMatrix::looksTheSameAsError The matrix are not compatible!\n m1."#NAME"=%d and m2."#NAME"=%d", this->NAME(), m->NAME());
+#define MY_PRINT_ERROR_MST(NAME) PLERROR("In VMatrix::compatibleSizeError - in class %s The matrix are not compatible!\n m1."#NAME"=%d and m2."#NAME"=%d", classname().c_str(), this->NAME(), m->NAME());
 
     if(this->width()      != m->width())
         MY_PRINT_ERROR_MST(width)
@@ -2041,6 +2041,40 @@
     return size_fieldnames;
 }
 
+////////////////////////////////
+// compute_missing_size_value //
+////////////////////////////////
+void VMatrix::compute_missing_size_value()
+{
+    PLCHECK(width_>0);
+    int v=min(inputsize_,0) + min(targetsize_,0)
+        + min(weightsize_,0) + min(extrasize_,0);
+    if(v<-1)
+        PLWARNING("In VMatrix::compute_missing_size_value() - in class %s"
+                  " more then one of"
+                  " inputsize(%d), targetsize(%d), weightsize(%d) and"
+                  " extrasize(%d) is unknow so we can't compute them.",
+                  classname().c_str(), inputsize_, targetsize_, weightsize_,
+                  extrasize_);
+    else if(v==0 && 
+            width_ != inputsize_ + targetsize_ + weightsize_ + extrasize_)
+        PLWARNING("In VMatrix::compute_missing_size_value() for class %s - "
+                  "inputsize_(%d) + targetsize_(%d) + weightsize_(%d) + "
+                  "extrasize_(%d) != width_(%d) !",
+                  classname().c_str(), inputsize_, targetsize_, weightsize_,
+                  extrasize_, width_);
+
+    else if(inputsize_<0)
+        inputsize_ = width_- targetsize_ - weightsize_ - extrasize_;
+    else if(targetsize_ < 0)
+        targetsize_ = width_- inputsize_ - weightsize_ - extrasize_;
+    else if(weightsize_ < 0)
+        weightsize_ = width_- inputsize_ - targetsize_ - extrasize_;
+    else if(extrasize_ < 0)
+        extrasize_  = width_- inputsize_ - targetsize_ - weightsize_;
+
+}
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-03-06 19:00:20 UTC (rev 8631)
+++ trunk/plearn/vmat/VMatrix.h	2008-03-06 20:01:53 UTC (rev 8632)
@@ -659,6 +659,11 @@
      */
     int max_fieldnames_size() const;
 
+    /** if only one of inputsize, targetsize, weightsize, extrasize
+     *  is less unknow and width>0, we compute its value
+     */
+    void compute_missing_size_value();
+
     /**
      *  Returns the bounding box of the data, as a vector of min:max pairs.  If
      *  extra_percent is non 0, then the box is enlarged in both ends of every



From larocheh at mail.berlios.de  Thu Mar  6 21:07:55 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 6 Mar 2008 21:07:55 +0100
Subject: [Plearn-commits] r8633 - trunk/plearn_learners/online
Message-ID: <200803062007.m26K7twQ026965@sheep.berlios.de>

Author: larocheh
Date: 2008-03-06 21:07:55 +0100 (Thu, 06 Mar 2008)
New Revision: 8633

Modified:
   trunk/plearn_learners/online/RBMGaussianLayer.cc
   trunk/plearn_learners/online/RBMGaussianLayer.h
Log:
Added an option to fix the quad_coeffs to a particular standard deviation.


Modified: trunk/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.cc	2008-03-06 20:01:53 UTC (rev 8632)
+++ trunk/plearn_learners/online/RBMGaussianLayer.cc	2008-03-06 20:07:55 UTC (rev 8633)
@@ -55,6 +55,7 @@
     min_quad_coeff( 0. ),
     share_quad_coeff( false ),
     size_quad_coeff( 0 ),
+    fixed_std_deviation( -1 ),
     sigma_is_up_to_date( false )
 {
 }
@@ -64,6 +65,7 @@
     min_quad_coeff( 0. ),
     share_quad_coeff( false ),
     size_quad_coeff( 0 ),
+    fixed_std_deviation( -1 ),
     quad_coeff( the_size, 1. ), // or 1./M_SQRT2 ?
     quad_coeff_pos_stats( the_size ),
     quad_coeff_neg_stats( the_size ),
@@ -83,6 +85,7 @@
                                     bool do_share_quad_coeff ) :
     inherited( the_learning_rate ),
     min_quad_coeff( 0. ),
+    fixed_std_deviation( -1 ),
     quad_coeff_pos_stats( the_size ),
     quad_coeff_neg_stats( the_size ),
     sigma_is_up_to_date( false )
@@ -326,7 +329,11 @@
 void RBMGaussianLayer::forget()
 {
     clearStats();
-    quad_coeff.fill( 1. );
+
+    if( fixed_std_deviation > 0 )
+        quad_coeff.fill( 1 / ( M_SQRT2 * fixed_std_deviation ) );
+    else
+        quad_coeff.fill( 1. );
     inherited::forget();
 }
 
@@ -346,7 +353,15 @@
                   "Suitable to avoid unstability (overfitting)  in cases where\n"
                   "all the units have the same 'meaning'  (pixels of an image)");
 
+    declareOption(ol, "fixed_std_deviation", &RBMGaussianLayer::fixed_std_deviation,
+                  OptionBase::buildoption,
+                  "Value for the usually learned standard deviation, "
+                  "if it should not be learned.\n"
+                  "This will fix the value of the quad coeffs to the "
+                  "appropriate value.\n"
+                  "If < 0, then this option is ignored.\n");
 
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -368,6 +383,15 @@
         needs_forget = true;
     }
 
+    if ( fixed_std_deviation > 0 && share_quad_coeff )
+    {
+        if( share_quad_coeff ) 
+            PLERROR("In RBMGaussianLayer::build_(): fixed_std_deviation should not "
+                    "be > 0 when share_quad_coeff is true.");
+        quad_coeff.fill( 1 / ( M_SQRT2 * fixed_std_deviation ) );
+    }
+
+
     quad_coeff_pos_stats.resize( size );
     quad_coeff_neg_stats.resize( size );
 
@@ -398,103 +422,112 @@
 
 void RBMGaussianLayer::accumulatePosStats( const Vec& pos_values )
 {
-    if (share_quad_coeff)
-       for( int i=0 ; i<size ; i++ )
-       {
+    if ( fixed_std_deviation <= 0 )
+    {
+        if (share_quad_coeff)
+            for( int i=0 ; i<size ; i++ )
+            {
            real x_i = pos_values[i];
            quad_coeff_pos_stats[i] += 2 * quad_coeff[0] * x_i * x_i;
-       }
-    else
-       for( int i=0 ; i<size ; i++ )
-       {
-           real x_i = pos_values[i];
-           quad_coeff_pos_stats[i] += 2 * quad_coeff[i] * x_i * x_i;
-       }
+            }
+        else
+            for( int i=0 ; i<size ; i++ )
+            {
+                real x_i = pos_values[i];
+                quad_coeff_pos_stats[i] += 2 * quad_coeff[i] * x_i * x_i;
+            }
+    }
 
     inherited::accumulatePosStats( pos_values );
 }
 
 void RBMGaussianLayer::accumulateNegStats( const Vec& neg_values )
 {
-    if (share_quad_coeff)
-        for( int i=0 ; i<size ; i++ )
-        {
-            real x_i = neg_values[i];
-            quad_coeff_neg_stats[i] += 2 * quad_coeff[0] * x_i * x_i;
-        }
-    else
-        for( int i=0 ; i<size ; i++ )
-        {
-            real x_i = neg_values[i];
-            quad_coeff_neg_stats[i] += 2 * quad_coeff[i] * x_i * x_i;
-        }
-    inherited::accumulateNegStats( neg_values );
-}
-
-void RBMGaussianLayer::update()
-{
-    // quad_coeff -= learning_rate * (quad_coeff_pos_stats/pos_count
-    //                                - quad_coeff_neg_stats/neg_count)
-    real pos_factor = -learning_rate / pos_count;
-    real neg_factor = learning_rate / neg_count;
-
-    real* a = quad_coeff.data();
-    real* aps = quad_coeff_pos_stats.data();
-    real* ans = quad_coeff_neg_stats.data();
-
-    if( momentum == 0. )
+    if ( fixed_std_deviation <= 0 )
     {
-        if(share_quad_coeff)
-        {
-            real update=0;
+        if (share_quad_coeff)
             for( int i=0 ; i<size ; i++ )
             {
-                update += pos_factor * aps[i] + neg_factor * ans[i];
+                real x_i = neg_values[i];
+                quad_coeff_neg_stats[i] += 2 * quad_coeff[0] * x_i * x_i;
             }
-            a[0] += update/(real)size;
-            if( a[0] < min_quad_coeff )
-                a[0] = min_quad_coeff;
-        }
         else
             for( int i=0 ; i<size ; i++ )
             {
-                a[i] += pos_factor * aps[i] + neg_factor * ans[i];
-                if( a[i] < min_quad_coeff )
-                    a[i] = min_quad_coeff;
+                real x_i = neg_values[i];
+                quad_coeff_neg_stats[i] += 2 * quad_coeff[i] * x_i * x_i;
             }
     }
-    else
+    inherited::accumulateNegStats( neg_values );
+}
+
+void RBMGaussianLayer::update()
+{
+    // quad_coeff -= learning_rate * (quad_coeff_pos_stats/pos_count
+    //                                - quad_coeff_neg_stats/neg_count)
+    if ( fixed_std_deviation <= 0 )
     {
-        if(share_quad_coeff)
+        real pos_factor = -learning_rate / pos_count;
+        real neg_factor = learning_rate / neg_count;
+        
+        real* a = quad_coeff.data();
+        real* aps = quad_coeff_pos_stats.data();
+        real* ans = quad_coeff_neg_stats.data();
+
+        if( momentum == 0. )
         {
-            quad_coeff_inc.resize( 1 );
-            real* ainc = quad_coeff_inc.data();
-            for( int i=0 ; i<size ; i++ )
+            if(share_quad_coeff)
             {
-                ainc[0] = momentum*ainc[0] + pos_factor*aps[i] + neg_factor*ans[i];
-                ainc[0] /= (real)size;
-                a[0] += ainc[0];
+                real update=0;
+                for( int i=0 ; i<size ; i++ )
+                {
+                    update += pos_factor * aps[i] + neg_factor * ans[i];
+                }
+                a[0] += update/(real)size;
+                if( a[0] < min_quad_coeff )
+                    a[0] = min_quad_coeff;
             }
-            if( a[0] < min_quad_coeff )
-                a[0] = min_quad_coeff;
+            else
+                for( int i=0 ; i<size ; i++ )
+                {
+                    a[i] += pos_factor * aps[i] + neg_factor * ans[i];
+                    if( a[i] < min_quad_coeff )
+                        a[i] = min_quad_coeff;
+                }
         }
         else
         {
-            quad_coeff_inc.resize( size );
-            real* ainc = quad_coeff_inc.data();
-            for( int i=0 ; i<size ; i++ )
+            if(share_quad_coeff)
             {
-                ainc[i] = momentum*ainc[i] + pos_factor*aps[i] + neg_factor*ans[i];
-                a[i] += ainc[i];
-                if( a[i] < min_quad_coeff )
-                    a[i] = min_quad_coeff;
+                quad_coeff_inc.resize( 1 );
+                real* ainc = quad_coeff_inc.data();
+                for( int i=0 ; i<size ; i++ )
+                {
+                    ainc[0] = momentum*ainc[0] + pos_factor*aps[i] + neg_factor*ans[i];
+                    ainc[0] /= (real)size;
+                    a[0] += ainc[0];
+                }
+                if( a[0] < min_quad_coeff )
+                    a[0] = min_quad_coeff;
             }
+            else
+            {
+                quad_coeff_inc.resize( size );
+                real* ainc = quad_coeff_inc.data();
+                for( int i=0 ; i<size ; i++ )
+                {
+                    ainc[i] = momentum*ainc[i] + pos_factor*aps[i] + neg_factor*ans[i];
+                    a[i] += ainc[i];
+                    if( a[i] < min_quad_coeff )
+                        a[i] = min_quad_coeff;
+                }
+            }
         }
+
+        // We will need to recompute sigma
+        sigma_is_up_to_date = false;
     }
-
-    // We will need to recompute sigma
-    sigma_is_up_to_date = false;
-
+    
     // will update the bias, and clear the statistics
     inherited::update();
 }
@@ -503,65 +536,68 @@
 {
     // quad_coeff[i] -= learning_rate * 2 * quad_coeff[i] * (pos_values[i]^2
     //                                                       - neg_values[i]^2)
-    real two_lr = 2 * learning_rate;
-    real* a = quad_coeff.data();
-    real* pv = pos_values.data();
-    real* nv = neg_values.data();
+    if ( fixed_std_deviation <= 0 )
+    {
+        real two_lr = 2 * learning_rate;
+        real* a = quad_coeff.data();
+        real* pv = pos_values.data();
+        real* nv = neg_values.data();
 
-    if( momentum == 0. )
-    {
-        if (share_quad_coeff)
+        if( momentum == 0. )
         {
-            real update=0;
-            for( int i=0 ; i<size ; i++ )
+            if (share_quad_coeff)
             {
-                update += two_lr * a[0] * (nv[i]*nv[i] - pv[i]*pv[i]);
+                real update=0;
+                for( int i=0 ; i<size ; i++ )
+                {
+                    update += two_lr * a[0] * (nv[i]*nv[i] - pv[i]*pv[i]);
+                }
+                a[0] += update/(real)size;
+                if( a[0] < min_quad_coeff )
+                    a[0] = min_quad_coeff;
             }
-            a[0] += update/(real)size;
-            if( a[0] < min_quad_coeff )
-                a[0] = min_quad_coeff;
+            else
+                for( int i=0 ; i<size ; i++ )
+                {
+                    a[i] += two_lr * a[i] * (nv[i]*nv[i] - pv[i]*pv[i]);
+                    if( a[i] < min_quad_coeff )
+                        a[i] = min_quad_coeff;
+                }
         }
         else
-            for( int i=0 ; i<size ; i++ )
-            {
-                a[i] += two_lr * a[i] * (nv[i]*nv[i] - pv[i]*pv[i]);
-                if( a[i] < min_quad_coeff )
-                    a[i] = min_quad_coeff;
-            }
-    }
-    else
-    {
-        real* ainc = quad_coeff_inc.data();
-        if(share_quad_coeff)
         {
-            quad_coeff_inc.resize( 1 );
-            for( int i=0 ; i<size ; i++ )
+            real* ainc = quad_coeff_inc.data();
+            if(share_quad_coeff)
             {
-                ainc[0] = momentum*ainc[0]
-                    + two_lr * a[0] * (nv[i]*nv[i] - pv[i]*pv[i]);
-                ainc[0] /= (real)size;
-                a[0] += ainc[0];
+                quad_coeff_inc.resize( 1 );
+                for( int i=0 ; i<size ; i++ )
+                {
+                    ainc[0] = momentum*ainc[0]
+                        + two_lr * a[0] * (nv[i]*nv[i] - pv[i]*pv[i]);
+                    ainc[0] /= (real)size;
+                    a[0] += ainc[0];
+                }
+                if( a[0] < min_quad_coeff )
+                    a[0] = min_quad_coeff;
             }
-            if( a[0] < min_quad_coeff )
-                a[0] = min_quad_coeff;
-        }
-        else
-        {
-            quad_coeff_inc.resize( size );
-            for( int i=0 ; i<size ; i++ )
+            else
             {
-                ainc[i] = momentum*ainc[i]
-                    + two_lr * a[i] * (nv[i]*nv[i] - pv[i]*pv[i]);
-                a[i] += ainc[i];
-                if( a[i] < min_quad_coeff )
-                    a[i] = min_quad_coeff;
+                quad_coeff_inc.resize( size );
+                for( int i=0 ; i<size ; i++ )
+                {
+                    ainc[i] = momentum*ainc[i]
+                        + two_lr * a[i] * (nv[i]*nv[i] - pv[i]*pv[i]);
+                    a[i] += ainc[i];
+                    if( a[i] < min_quad_coeff )
+                        a[i] = min_quad_coeff;
+                }
             }
         }
+
+        // We will need to recompute sigma
+        sigma_is_up_to_date = false;
     }
-
-    // We will need to recompute sigma
-    sigma_is_up_to_date = false;
-
+    
     // update the bias
     inherited::update( pos_values, neg_values );
 }
@@ -577,46 +613,48 @@
 
     // quad_coeff[i] -= learning_rate * 2 * quad_coeff[i] * (pos_values[i]^2
     //                                                       - neg_values[i]^2)
+    if ( fixed_std_deviation <= 0 )
+    {
+        real two_lr = 2 * learning_rate / batch_size;
+        real* a = quad_coeff.data();
 
-    real two_lr = 2 * learning_rate / batch_size;
-    real* a = quad_coeff.data();
-
-    if( momentum == 0. )
-    {
-        if (share_quad_coeff)
-            for( int k=0; k<batch_size; k++ )
-            {
-                real *pv_k = pos_values[k];
-                real *nv_k = neg_values[k];
-                real update=0;
-                for( int i=0; i<size; i++ )
+        if( momentum == 0. )
+        {
+            if (share_quad_coeff)
+                for( int k=0; k<batch_size; k++ )
                 {
-                    update += two_lr * a[0] * (nv_k[i]*nv_k[i] - pv_k[i]*pv_k[i]);
+                    real *pv_k = pos_values[k];
+                    real *nv_k = neg_values[k];
+                    real update=0;
+                    for( int i=0; i<size; i++ )
+                    {
+                        update += two_lr * a[0] * (nv_k[i]*nv_k[i] - pv_k[i]*pv_k[i]);
+                    }
+                    a[0] += update/(real)size;
+                    if( a[0] < min_quad_coeff )
+                        a[0] = min_quad_coeff;
                 }
-                a[0] += update/(real)size;
-                if( a[0] < min_quad_coeff )
-                    a[0] = min_quad_coeff;
-            }
-        else
-            for( int k=0; k<batch_size; k++ )
-            {
-                real *pv_k = pos_values[k];
-                real *nv_k = neg_values[k];
-                for( int i=0; i<size; i++ )
+            else
+                for( int k=0; k<batch_size; k++ )
                 {
-                    a[i] += two_lr * a[i] * (nv_k[i]*nv_k[i] - pv_k[i]*pv_k[i]);
-                    if( a[i] < min_quad_coeff )
-                        a[i] = min_quad_coeff;
+                    real *pv_k = pos_values[k];
+                    real *nv_k = neg_values[k];
+                    for( int i=0; i<size; i++ )
+                    {
+                        a[i] += two_lr * a[i] * (nv_k[i]*nv_k[i] - pv_k[i]*pv_k[i]);
+                        if( a[i] < min_quad_coeff )
+                            a[i] = min_quad_coeff;
+                    }
                 }
-            }
+        }
+        else
+            PLCHECK_MSG( false,
+                         "momentum and minibatch are not compatible yet" );
+
+        // We will need to recompute sigma
+        sigma_is_up_to_date = false;
     }
-    else
-        PLCHECK_MSG( false,
-                     "momentum and minibatch are not compatible yet" );
 
-    // We will need to recompute sigma
-    sigma_is_up_to_date = false;
-
     // Update the bias
     inherited::update( pos_values, neg_values );
 }

Modified: trunk/plearn_learners/online/RBMGaussianLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.h	2008-03-06 20:01:53 UTC (rev 8632)
+++ trunk/plearn_learners/online/RBMGaussianLayer.h	2008-03-06 20:07:55 UTC (rev 8633)
@@ -64,6 +64,10 @@
     //! or 1 when share_quad_coeff is True
     int size_quad_coeff;
 
+    //! Value for the usually learned standard deviation, if it should not be learned.
+    //! This will fix the value of the quad coeffs to the appropriate value.
+    //! If < 0, then this option is ignored.
+    real fixed_std_deviation;
 
 public:
     //#####  Public Member Functions  #########################################



From nouiz at mail.berlios.de  Thu Mar  6 21:22:18 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 6 Mar 2008 21:22:18 +0100
Subject: [Plearn-commits] r8634 - in trunk/plearn: misc vmat
Message-ID: <200803062022.m26KMIm8028315@sheep.berlios.de>

Author: nouiz
Date: 2008-03-06 21:22:18 +0100 (Thu, 06 Mar 2008)
New Revision: 8634

Modified:
   trunk/plearn/misc/vmatmain.cc
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
renemed function for standardisation
new name: VMatrix::computeMissingSizeValue() VMatrix::maxFieldNamesSize()




Modified: trunk/plearn/misc/vmatmain.cc
===================================================================
--- trunk/plearn/misc/vmatmain.cc	2008-03-06 20:07:55 UTC (rev 8633)
+++ trunk/plearn/misc/vmatmain.cc	2008-03-06 20:22:18 UTC (rev 8634)
@@ -1022,7 +1022,7 @@
         int pc_value_90=0;
         int pc_value_0=0;
 
-        uint size_fieldnames=m1->max_fieldnames_size();
+        uint size_fieldnames=m1->maxFieldNamesSize();
 
         Vec Ds(m1->width());
         Vec p_values(m1->width());
@@ -1108,7 +1108,7 @@
         Vec Ds(m1->width());
         Vec p_values(m1->width());
         Mat score(m1->width(),3);
-        uint size_fieldnames=m1->max_fieldnames_size();
+        uint size_fieldnames=m1->maxFieldNamesSize();
         if(mat_to_mem==true)
         {
             m1.precompute();
@@ -1187,7 +1187,7 @@
         TVec<StatsCollector> stats = 
             m1->getStats();//"stats_all.psave",-1,true);
         TVec<string> caracs;
-        uint size_fieldnames=m1->max_fieldnames_size();
+        uint size_fieldnames=m1->maxFieldNamesSize();
 
         for(int i=0;i<stats.size();i++)
         {

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-03-06 20:07:55 UTC (rev 8633)
+++ trunk/plearn/vmat/VMatrix.cc	2008-03-06 20:22:18 UTC (rev 8634)
@@ -2032,7 +2032,7 @@
 /////////////////////////
 // max_fieldnames_size //
 /////////////////////////
-int VMatrix::max_fieldnames_size() const
+int VMatrix::maxFieldNamesSize() const
 {
     uint size_fieldnames=0;
     for(int i=0;i<width();i++)
@@ -2044,7 +2044,7 @@
 ////////////////////////////////
 // compute_missing_size_value //
 ////////////////////////////////
-void VMatrix::compute_missing_size_value()
+void VMatrix::computeMissingSizeValue()
 {
     PLCHECK(width_>0);
     int v=min(inputsize_,0) + min(targetsize_,0)
@@ -2053,9 +2053,10 @@
         PLWARNING("In VMatrix::compute_missing_size_value() - in class %s"
                   " more then one of"
                   " inputsize(%d), targetsize(%d), weightsize(%d) and"
-                  " extrasize(%d) is unknow so we can't compute them.",
+                  " extrasize(%d) is unknow so we can't compute them with the"
+                  " width(%d)",
                   classname().c_str(), inputsize_, targetsize_, weightsize_,
-                  extrasize_);
+                  extrasize_, width_);
     else if(v==0 && 
             width_ != inputsize_ + targetsize_ + weightsize_ + extrasize_)
         PLWARNING("In VMatrix::compute_missing_size_value() for class %s - "

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-03-06 20:07:55 UTC (rev 8633)
+++ trunk/plearn/vmat/VMatrix.h	2008-03-06 20:22:18 UTC (rev 8634)
@@ -657,12 +657,12 @@
     /**
      * @return The size of the longest fieldname
      */
-    int max_fieldnames_size() const;
+    int maxFieldNamesSize() const;
 
     /** if only one of inputsize, targetsize, weightsize, extrasize
      *  is less unknow and width>0, we compute its value
      */
-    void compute_missing_size_value();
+    void computeMissingSizeValue();
 
     /**
      *  Returns the bounding box of the data, as a vector of min:max pairs.  If



From nouiz at mail.berlios.de  Thu Mar  6 21:27:29 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 6 Mar 2008 21:27:29 +0100
Subject: [Plearn-commits] r8635 - trunk/plearn_learners/regressors
Message-ID: <200803062027.m26KRT5c028874@sheep.berlios.de>

Author: nouiz
Date: 2008-03-06 21:27:29 +0100 (Thu, 06 Mar 2008)
New Revision: 8635

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
code refactoring to get the fieldnames


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2008-03-06 20:22:18 UTC (rev 8634)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2008-03-06 20:27:29 UTC (rev 8635)
@@ -111,15 +111,11 @@
 }
 
 void RegressionTreeRegisters::initRegisters(VMat the_train_set)
-{
+{   
     VMat tmp = VMat(new TransposeVMatrix(the_train_set));
     PP<MemoryVMatrixNoSave> tmp2 = new MemoryVMatrixNoSave(tmp);
     tsource = VMat(tmp2 );
-    length_ = the_train_set->length();
-    width_ = the_train_set->width();
-    inputsize_ = the_train_set->inputsize();
-    targetsize_ = the_train_set->targetsize();
-    weightsize_ = the_train_set->weightsize();
+    setMetaInfoFrom(the_train_set);
     leave_register.resize(length());
     sortRows();
 }



From laulysta at mail.berlios.de  Thu Mar  6 23:15:55 2008
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Thu, 6 Mar 2008 23:15:55 +0100
Subject: [Plearn-commits] r8636 - trunk/plearn_learners/online
Message-ID: <200803062215.m26MFtSx006736@sheep.berlios.de>

Author: laulysta
Date: 2008-03-06 23:15:55 +0100 (Thu, 06 Mar 2008)
New Revision: 8636

Modified:
   trunk/plearn_learners/online/RBMLayer.cc
   trunk/plearn_learners/online/RBMLayer.h
Log:


Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2008-03-06 20:27:29 UTC (rev 8635)
+++ trunk/plearn_learners/online/RBMLayer.cc	2008-03-06 22:15:55 UTC (rev 8636)
@@ -273,7 +273,12 @@
     expectations_are_up_to_date = false;
 }
 
+void RBMLayer::expectation_is_not_up_to_date()
+{
+    expectation_is_up_to_date = false;
+}
 
+
 /////////////////////
 // getExpectations //
 /////////////////////

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2008-03-06 20:27:29 UTC (rev 8635)
+++ trunk/plearn_learners/online/RBMLayer.h	2008-03-06 22:15:55 UTC (rev 8636)
@@ -165,6 +165,9 @@
     virtual void getAllActivations( PP<RBMConnection> rbmc, int offset = 0,
                                     bool minibatch = false);
 
+    //change the flag of expectation_is_up_to_date to false
+    virtual void expectation_is_not_up_to_date();
+
     //! generate a sample, and update the sample field
     virtual void generateSample() = 0 ;
 



From tihocan at mail.berlios.de  Fri Mar  7 16:50:51 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 7 Mar 2008 16:50:51 +0100
Subject: [Plearn-commits] r8637 - trunk/plearn_learners/generic
Message-ID: <200803071550.m27Fopo9026477@sheep.berlios.de>

Author: tihocan
Date: 2008-03-07 16:50:51 +0100 (Fri, 07 Mar 2008)
New Revision: 8637

Modified:
   trunk/plearn_learners/generic/NNet.cc
Log:
Fixed problem when the number of training bags was forgotten before training

Modified: trunk/plearn_learners/generic/NNet.cc
===================================================================
--- trunk/plearn_learners/generic/NNet.cc	2008-03-06 22:15:55 UTC (rev 8636)
+++ trunk/plearn_learners/generic/NNet.cc	2008-03-07 15:50:51 UTC (rev 8637)
@@ -513,25 +513,11 @@
 
     inherited::setTrainingSet(training_set, call_forget);
     //cout << "name = " << name << endl << "targetsize = " << targetsize_ << endl << "weightsize = " << weightsize_ << endl;
+
+    // Since the training set probably changed, it is safer to reset
+    // 'n_training_bags', just in case.
+    n_training_bags = -1;
     
-    if (operate_on_bags) {
-        // Compute the number of bags in the training set.
-        int n_train = training_set->length();
-        PP<ProgressBar> pb = 
-            report_progress ? new ProgressBar("Counting bags", n_train)
-                            : NULL;
-        Vec input, target;
-        real weight;
-        n_training_bags = 0;
-        for (int i = 0; i < n_train; i++) {
-            training_set->getExample(i, input, target, weight);
-            if (int(round(target.lastElement()))
-                & SumOverBagsVariable::TARGET_COLUMN_FIRST)
-                n_training_bags++;
-            if (pb)
-                pb->updateone();
-        }
-    }
 }
 
 ////////////////
@@ -1160,6 +1146,25 @@
 
     if(!train_set)
         PLERROR("In NNet::train - No training set available");
+
+    if (operate_on_bags && n_training_bags < 0) {
+        // Compute the number of bags in the training set.
+        int n_train = train_set->length();
+        PP<ProgressBar> pb = 
+            report_progress ? new ProgressBar("Counting bags", n_train)
+                            : NULL;
+        Vec input, target;
+        real weight;
+        n_training_bags = 0;
+        for (int i = 0; i < n_train; i++) {
+            train_set->getExample(i, input, target, weight);
+            if (int(round(target.lastElement()))
+                & SumOverBagsVariable::TARGET_COLUMN_FIRST)
+                n_training_bags++;
+            if (pb)
+                pb->updateone();
+        }
+    }
     
     if(!train_stats)
         setTrainStatsCollector(new VecStatsCollector());



From tihocan at mail.berlios.de  Fri Mar  7 16:51:54 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 7 Mar 2008 16:51:54 +0100
Subject: [Plearn-commits] r8638 - trunk/plearn_learners/classifiers
Message-ID: <200803071551.m27FpsfW026722@sheep.berlios.de>

Author: tihocan
Date: 2008-03-07 16:51:54 +0100 (Fri, 07 Mar 2008)
New Revision: 8638

Modified:
   trunk/plearn_learners/classifiers/ToBagClassifier.cc
   trunk/plearn_learners/classifiers/ToBagClassifier.h
Log:
Implemented computeOutputAndCosts

Modified: trunk/plearn_learners/classifiers/ToBagClassifier.cc
===================================================================
--- trunk/plearn_learners/classifiers/ToBagClassifier.cc	2008-03-07 15:50:51 UTC (rev 8637)
+++ trunk/plearn_learners/classifiers/ToBagClassifier.cc	2008-03-07 15:51:54 UTC (rev 8638)
@@ -38,6 +38,7 @@
 
 
 #include "ToBagClassifier.h"
+#include <plearn/var/SumOverBagsVariable.h>
 #include <plearn/vmat/SubVMatrix.h>
 
 namespace PLearn {
@@ -101,34 +102,33 @@
                                               const Vec& target,
                                               Vec& costs) const
 {
+    fillSubTarget(target);
+    inherited::computeCostsFromOutputs(input, output, sub_target, costs);
+    updateCostAndBagOutput(target, output, costs);
+}
+
+///////////////////////////
+// computeOutputAndCosts //
+///////////////////////////
+void ToBagClassifier::computeOutputAndCosts(const Vec& input,
+                                            const Vec& target,
+                                            Vec& output, Vec& costs) const
+{
+    fillSubTarget(target);
+    inherited::computeOutputAndCosts(input, sub_target, output, costs);
+    updateCostAndBagOutput(target, output, costs);
+}
+
+///////////////////
+// fillSubTarget //
+///////////////////
+void ToBagClassifier::fillSubTarget(const Vec& target) const
+{
     sub_target.resize(target.length() - 1);
     sub_target << target.subVec(0, sub_target.length());
-    inherited::computeCostsFromOutputs(input, output, sub_target, costs);
-    PLASSERT( is_equal(sum(output), 1) );   // Ensure probabilities sum to 1.
-    int bag_info = int(round(target.lastElement()));
-    if (bag_info % 2 == 1)
-        bag_output.resize(0, 0);
-    bag_output.appendRow(output);
-    costs.resize(nTestCosts());
-    costs.fill(MISSING_VALUE);
-    if (bag_info >= 2) {
-        // Perform majority vote.
-        votes.resize(bag_output.width());
-        columnSum(bag_output, votes);
-        int target_class = int(round(target[0]));
-        int prediction = argmax(votes);
-        if (prediction == target_class)
-            costs[0] = 0;
-        else
-            costs[0] = 1;
-        if (n_classes > 0) {
-            int i_start = 1 + target_class * n_classes;
-            costs.subVec(i_start, n_classes).fill(0);
-            costs[i_start + prediction] = 1;
-        }
-    }
 }
 
+
 //////////////////////
 // getTestCostNames //
 //////////////////////
@@ -187,8 +187,39 @@
     return learner_->targetsize() + 1;
 }
 
+////////////////////////////
+// updateCostAndBagOutput //
+////////////////////////////
+void ToBagClassifier::updateCostAndBagOutput(const Vec& target,
+                                             const Vec& output,
+                                             Vec& costs) const
+{
+    PLASSERT( is_equal(sum(output), 1) );   // Ensure probabilities sum to 1.
+    int bag_info = int(round(target.lastElement()));
+    if (bag_info & SumOverBagsVariable::TARGET_COLUMN_FIRST)
+        bag_output.resize(0, 0);
+    bag_output.appendRow(output);
+    costs.resize(nTestCosts());
+    costs.fill(MISSING_VALUE);
+    if (bag_info & SumOverBagsVariable::TARGET_COLUMN_LAST) {
+        // Perform majority vote.
+        votes.resize(bag_output.width());
+        columnSum(bag_output, votes);
+        int target_class = int(round(target[0]));
+        int prediction = argmax(votes);
+        if (prediction == target_class)
+            costs[0] = 0;
+        else
+            costs[0] = 1;
+        if (n_classes > 0) {
+            int i_start = 1 + target_class * n_classes;
+            costs.subVec(i_start, n_classes).fill(0);
+            costs[i_start + prediction] = 1;
+        }
+    }
+}
+
 } // end of namespace PLearn
-
 
 /*
   Local Variables:

Modified: trunk/plearn_learners/classifiers/ToBagClassifier.h
===================================================================
--- trunk/plearn_learners/classifiers/ToBagClassifier.h	2008-03-07 15:50:51 UTC (rev 8637)
+++ trunk/plearn_learners/classifiers/ToBagClassifier.h	2008-03-07 15:51:54 UTC (rev 8638)
@@ -83,12 +83,9 @@
     virtual void computeCostsFromOutputs(const Vec& input, const Vec& output,
                                          const Vec& target, Vec& costs) const;
 
-    //! Currently using PLearner's simple version for code simplicity.
+    //! Overridden to be able to use the sub-learner's corresponding method.
     virtual void computeOutputAndCosts(const Vec& input, const Vec& target,
-                                       Vec& output, Vec& costs) const
-    {
-        PLearner::computeOutputAndCosts(input, target, output, costs);
-    }
+                                       Vec& output, Vec& costs) const;
 
     //! Currently using PLearner's simple version for code simplicity.
     virtual void computeOutputsAndCosts(const Mat& input, const Mat& target,
@@ -134,9 +131,15 @@
     //#####  Protected Member Functions  ######################################
 
     //! Declares the class options.
-    // (PLEASE IMPLEMENT IN .cc)
     static void declareOptions(OptionList& ol);
 
+    //! TODO Document.
+    void updateCostAndBagOutput(const Vec& target, const Vec& output,
+                                Vec& costs) const;
+
+    //! Fill 'sub_target' with all elements of 'target' but the last one.
+    void fillSubTarget(const Vec& target) const;
+
 private:
     //#####  Private Member Functions  ########################################
 



From tihocan at mail.berlios.de  Fri Mar  7 16:54:11 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 7 Mar 2008 16:54:11 +0100
Subject: [Plearn-commits] r8639 - trunk/plearn_learners/classifiers
Message-ID: <200803071554.m27FsBAh026914@sheep.berlios.de>

Author: tihocan
Date: 2008-03-07 16:54:11 +0100 (Fri, 07 Mar 2008)
New Revision: 8639

Modified:
   trunk/plearn_learners/classifiers/ToBagClassifier.h
Log:
Added documentation of new method updateCostAndBagOutput

Modified: trunk/plearn_learners/classifiers/ToBagClassifier.h
===================================================================
--- trunk/plearn_learners/classifiers/ToBagClassifier.h	2008-03-07 15:51:54 UTC (rev 8638)
+++ trunk/plearn_learners/classifiers/ToBagClassifier.h	2008-03-07 15:54:11 UTC (rev 8639)
@@ -133,7 +133,9 @@
     //! Declares the class options.
     static void declareOptions(OptionList& ol);
 
-    //! TODO Document.
+    //! Given a target and the corresponding output, update the 'bag_output'
+    //! data matrix and compute the costs (which will be missing except for
+    //! the last sample in a bag).
     void updateCostAndBagOutput(const Vec& target, const Vec& output,
                                 Vec& costs) const;
 



From larocheh at mail.berlios.de  Fri Mar  7 17:15:10 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 7 Mar 2008 17:15:10 +0100
Subject: [Plearn-commits] r8640 - trunk/plearn_learners_experimental
Message-ID: <200803071615.m27GFAPZ030219@sheep.berlios.de>

Author: larocheh
Date: 2008-03-07 17:15:10 +0100 (Fri, 07 Mar 2008)
New Revision: 8640

Modified:
   trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc
Log:
Corrected some bugs...


Modified: trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc
===================================================================
--- trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc	2008-03-07 15:54:11 UTC (rev 8639)
+++ trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc	2008-03-07 16:15:10 UTC (rev 8640)
@@ -544,9 +544,9 @@
         reconstruction_expectation_gradients.resize(layers[i]->size);
 
         pos_down_val.resize(layers[i]->size);
-        pos_up_val.resize(layers[i]->size);
+        pos_up_val.resize(layers[i+1]->size);
         neg_down_val.resize(layers[i]->size);
-        neg_up_val.resize(layers[i]->size);
+        neg_up_val.resize(layers[i+1]->size);
 
         for( ; *this_stage<end_stage ; (*this_stage)++ )
         {
@@ -569,6 +569,7 @@
             MODULE_LOG << "Finding the nearest neighbors" << endl;
             // Find training nearest neighbors
             TVec<int> nearest_neighbors_indices_row;
+            nearest_neighbors_indices.resize(train_set->length(), k_neighbors);
             for(int k=0; k<n_classes; k++)
             {
                 for(int i=0; i<class_datasets[k]->length(); i++)
@@ -711,7 +712,7 @@
         
         // negative phase
         connections[index]->setAsDownInput( layers[index]->sample );
-        layers[index+1]->getAllActivations( connections[index+1] );
+        layers[index+1]->getAllActivations( connections[index] );
         layers[index+1]->computeExpectation();
         // accumulate negative stats
         // no need to deep-copy because the values won't change before update
@@ -784,7 +785,7 @@
     F << all_outputs.subVec(0,n_components * inputsize()).toMat(
         n_components, inputsize());
     mu << all_outputs.subVec(n_components * inputsize(),inputsize());
-    pre_sigma_noise << all_outputs.subVec( n_components * (inputsize() + 1), 1 );
+    pre_sigma_noise << all_outputs.subVec( (n_components+1) * inputsize(), 1 );
 
     F_copy.resize(F.length(),F.width());
     sm_svd.resize(n_components);
@@ -815,6 +816,8 @@
     real dotp = 0;
     real coef = 0;
     real n = inputsize();
+    z.resize(k_neighbors,inputsize());
+    temp_ncomp.resize(n_components);
     inv_Sigma_z.resize(k_neighbors,inputsize());
     inv_Sigma_z.clear();
     real tr_inv_Sigma = 0;
@@ -870,7 +873,8 @@
     }
 
     all_outputs_gradient.clear();
-    coef = 1/train_set->length();
+    coef = 1.0/train_set->length();
+    all_outputs_gradient.resize((n_components+1) * inputsize()+1);
     for(int neighbor=0; neighbor<k_neighbors; neighbor++)
     {
         // dNLL/dF
@@ -1101,10 +1105,12 @@
     }
     else
     {
-        if( ((int)round(output[0])) == ((int)round(target[0])) )
+        int target_class = ((int)round(target[0]));
+        if( ((int)round(output[0])) == target_class )
             costs[n_layers-1] = 0;
         else
             costs[n_layers-1] = 1;
+        costs[n_layers] = - test_votes[target_class]+pl_log(class_datasets[target_class]->length());
     }
 }
 
@@ -1158,6 +1164,7 @@
         cost_names.push_back("reconstruction_error_" + tostring(i+1));
     
     cost_names.append( "class_error" );
+    cost_names.append( "NLL" );
 
     return cost_names;
 }



From nouiz at mail.berlios.de  Fri Mar  7 17:51:23 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 7 Mar 2008 17:51:23 +0100
Subject: [Plearn-commits] r8641 - trunk/plearn/base
Message-ID: <200803071651.m27GpNUB002758@sheep.berlios.de>

Author: nouiz
Date: 2008-03-07 17:51:22 +0100 (Fri, 07 Mar 2008)
New Revision: 8641

Modified:
   trunk/plearn/base/general.cc
Log:
removed a dependency


Modified: trunk/plearn/base/general.cc
===================================================================
--- trunk/plearn/base/general.cc	2008-03-07 16:15:10 UTC (rev 8640)
+++ trunk/plearn/base/general.cc	2008-03-07 16:51:22 UTC (rev 8641)
@@ -45,7 +45,6 @@
 #include <sys/stat.h>
 #include <nspr/prsystem.h>
 #include <nspr/prenv.h>
-#include <plearn/base/tostring.h>
 #ifdef _MSC_VER
 #include <io.h>
 #endif
@@ -116,7 +115,7 @@
 {
     char tmp[1024];
     if(PR_GetSystemInfo(PR_SI_HOSTNAME,tmp,500)==PR_SUCCESS)
-        return tostring(tmp);
+        return string(tmp);
     else{
         const char* h = PR_GetEnv("HOSTNAME");
         if (!h)



From tihocan at mail.berlios.de  Fri Mar  7 18:29:15 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 7 Mar 2008 18:29:15 +0100
Subject: [Plearn-commits] r8642 - trunk/plearn/vmat
Message-ID: <200803071729.m27HTF4M007596@sheep.berlios.de>

Author: tihocan
Date: 2008-03-07 18:29:15 +0100 (Fri, 07 Mar 2008)
New Revision: 8642

Modified:
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
The Python interface of getFieldIndex now uses the version that allows one to control whether to crash or not when an unknown field name is asked for

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-03-07 16:51:22 UTC (rev 8641)
+++ trunk/plearn/vmat/VMatrix.cc	2008-03-07 17:29:15 UTC (rev 8642)
@@ -196,10 +196,12 @@
          RetDoc ("TVec of field names.\n")));
 
      declareMethod(
-        rmm, "getFieldIndex", &VMatrix::remote_getFieldIndex,
+        rmm, "getFieldIndex", &VMatrix::getFieldIndex,
         (BodyDoc("Returns the index of a field.\n"),
          ArgDoc ("fname_or_num",
              "Field name or index (as a string) of the field.\n"),
+         ArgDoc("throw_error",
+             "Whether to throw an error or return -1 for unknown fields"),
          RetDoc ("Index of the field.\n")));
     
     declareMethod(
@@ -411,7 +413,7 @@
 ///////////////////
 // getFieldIndex //
 ///////////////////
-int VMatrix::getFieldIndex(const string& fieldname_or_num, const bool error) const
+int VMatrix::getFieldIndex(const string& fieldname_or_num, bool error) const
 {
     int i = fieldIndex(fieldname_or_num);
     if(i==-1 && pl_islong(fieldname_or_num)) {

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-03-07 16:51:22 UTC (rev 8641)
+++ trunk/plearn/vmat/VMatrix.h	2008-03-07 17:29:15 UTC (rev 8642)
@@ -189,14 +189,13 @@
 
     /**
      *  This first calls fieldIndex to try and get the index corresponding to
-     *  the given string If this fails, the given string is assumed to hold the
-     *  numerical index, and its conversion to int will be returned (or a
-     *  PLERROR issued if this fails).
+     *  the given string. If this fails, the given string is assumed to hold
+     *  the numerical index, and its conversion to int will be returned (or a
+     *  PLERROR issued if this fails, unless 'throw_error' is set to false, in
+     *  which case -1 is returned instead).
      */
-    int getFieldIndex(const string& fieldname_or_num,const bool error=true) const;
-    int remote_getFieldIndex(const string& fieldname_or_num) const{
-        return getFieldIndex(fieldname_or_num);
-    }
+    int getFieldIndex(const string& fieldname_or_num,
+                      bool throw_error = true) const;
 
     /// Return the field name at a given index
     string fieldName(int fieldindex) const



From nouiz at mail.berlios.de  Fri Mar  7 19:27:33 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 7 Mar 2008 19:27:33 +0100
Subject: [Plearn-commits] r8643 - trunk/plearn/base
Message-ID: <200803071827.m27IRXfw032240@sheep.berlios.de>

Author: nouiz
Date: 2008-03-07 19:27:33 +0100 (Fri, 07 Mar 2008)
New Revision: 8643

Modified:
   trunk/plearn/base/stringutils.cc
   trunk/plearn/base/stringutils.h
Log:
Added function split_quoted_delimiter(const string& s, char delimiter, char double_quote) as an optimized version of split_quoted_delimiter(const string& s, char delimiter, const string& double_quote)


Modified: trunk/plearn/base/stringutils.cc
===================================================================
--- trunk/plearn/base/stringutils.cc	2008-03-07 17:29:15 UTC (rev 8642)
+++ trunk/plearn/base/stringutils.cc	2008-03-07 18:27:33 UTC (rev 8643)
@@ -364,12 +364,14 @@
     return res;
 }
 
-vector<string> split_quoted_delimiter(const string& s, char delimiter, string double_quote){
-    if(double_quote.length()==1)
-        PLASSERT(delimiter!=double_quote[0]);
+vector<string> split_quoted_delimiter(const string& s, char delimiter,
+                                      const string& double_quote){
+    int delim_size=double_quote.size();
+    if(delim_size==1)
+        return split_quoted_delimiter(s,delimiter,double_quote[0]);
+
     vector<string> ret = split(s, delimiter);
     vector<string> ret2;
-    int delim_size=double_quote.size();
     for(uint i=0; i<ret.size();i++){
         bool bw=string_begins_with(ret[i],double_quote);
         bool ew=string_ends_with(ret[i],double_quote);
@@ -395,6 +397,37 @@
     return ret2;
     
 }
+vector<string> split_quoted_delimiter(const string& s, char delimiter,
+                                      char double_quote){
+    PLASSERT(delimiter!=double_quote);
+    vector<string> ret = split(s, delimiter);
+    vector<string> ret2;
+    for(uint i=0; i<ret.size();i++){
+        string f = ret[i];
+        bool bw=f[0]==double_quote;
+        bool ew=f[f.size()-1]==double_quote;
+        if(bw && ew){
+            ret2.push_back(f.substr(1,f.size()-1)); 
+        }else if(bw){
+            string tmp=f.substr(1);
+            tmp+=delimiter;
+            for(uint j=i+1;j<ret.size();j++){
+                if(ret[j][ret[j].size()-1]==double_quote){
+                    tmp+=ret[j].substr(0,ret[j].size()-1);
+                    ret2.push_back(tmp);
+                    i=j;
+                    break;
+                }
+                tmp+=ret[j];
+                tmp+=delimiter;
+            }
+        }else
+            ret2.push_back(f);
+    }
+    return ret2;
+    
+}
+
 vector<string> split(const string& s, const string& delimiters, bool keep_delimiters)
 {
     vector<string> result;

Modified: trunk/plearn/base/stringutils.h
===================================================================
--- trunk/plearn/base/stringutils.h	2008-03-07 17:29:15 UTC (rev 8642)
+++ trunk/plearn/base/stringutils.h	2008-03-07 18:27:33 UTC (rev 8643)
@@ -178,8 +178,12 @@
   @double_quote a string that will surround a field if it containt delimiter caractere that should not consider generate a new field.
   @todo optimize...
 */
-vector<string> split_quoted_delimiter(const string& s, char delimiter, string double_quote);
+vector<string> split_quoted_delimiter(const string& s, char delimiter,
+                                      const string& double_quote);
 
+vector<string> split_quoted_delimiter(const string& s, char delimiter,
+                                      char double_quote);
+
 /*!     Split the string on the first occurence of a delimiter and returns 
   what was left of the delimitor and what was right of it.
   If no delimitor character is found, the original string is returned 



From tihocan at mail.berlios.de  Fri Mar  7 20:59:41 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 7 Mar 2008 20:59:41 +0100
Subject: [Plearn-commits] r8644 - trunk/plearn/vmat
Message-ID: <200803071959.m27JxfwY007493@sheep.berlios.de>

Author: tihocan
Date: 2008-03-07 20:59:41 +0100 (Fri, 07 Mar 2008)
New Revision: 8644

Modified:
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
Reverted back the remote version of getFieldIndex to its previous version, to keep existing code simple. Instead, a new remote method findFieldIndex has been added, that will not crash when a field is missing.

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-03-07 18:27:33 UTC (rev 8643)
+++ trunk/plearn/vmat/VMatrix.cc	2008-03-07 19:59:41 UTC (rev 8644)
@@ -196,12 +196,19 @@
          RetDoc ("TVec of field names.\n")));
 
      declareMethod(
-        rmm, "getFieldIndex", &VMatrix::getFieldIndex,
-        (BodyDoc("Returns the index of a field.\n"),
+        rmm, "findFieldIndex", &VMatrix::fieldIndex,
+        (BodyDoc("Returns the index of a field, or -1 if the field does not "
+                 "exist.\n"),
+         ArgDoc ("fname",
+             "Field name of the field.\n"),
+         RetDoc ("Index of the field (-1 if not found)\n")));
+
+      declareMethod(
+        rmm, "getFieldIndex", &VMatrix::remote_getFieldIndex,
+        (BodyDoc("Returns the index of a field. "
+                 "Throws an error if the field is not found.\n"),
          ArgDoc ("fname_or_num",
              "Field name or index (as a string) of the field.\n"),
-         ArgDoc("throw_error",
-             "Whether to throw an error or return -1 for unknown fields"),
          RetDoc ("Index of the field.\n")));
     
     declareMethod(

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-03-07 18:27:33 UTC (rev 8643)
+++ trunk/plearn/vmat/VMatrix.h	2008-03-07 19:59:41 UTC (rev 8644)
@@ -197,6 +197,12 @@
     int getFieldIndex(const string& fieldname_or_num,
                       bool throw_error = true) const;
 
+    //! Remote version of 'getFieldIndex'.
+    int remote_getFieldIndex(const string& fieldname_or_num) const
+    {
+        return getFieldIndex(fieldname_or_num);
+    }
+
     /// Return the field name at a given index
     string fieldName(int fieldindex) const
     {



From tihocan at mail.berlios.de  Fri Mar  7 21:07:34 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 7 Mar 2008 21:07:34 +0100
Subject: [Plearn-commits] r8645 - trunk/plearn/misc
Message-ID: <200803072007.m27K7YmL009037@sheep.berlios.de>

Author: tihocan
Date: 2008-03-07 21:07:34 +0100 (Fri, 07 Mar 2008)
New Revision: 8645

Modified:
   trunk/plearn/misc/vmatmain.cc
Log:
Fixed a few typos

Modified: trunk/plearn/misc/vmatmain.cc
===================================================================
--- trunk/plearn/misc/vmatmain.cc	2008-03-07 19:59:41 UTC (rev 8644)
+++ trunk/plearn/misc/vmatmain.cc	2008-03-07 20:07:34 UTC (rev 8645)
@@ -972,7 +972,7 @@
                 real lmissing = lstats.nmissing()/lstats.n();
                 pout<<i<<"("<<m1->fieldName(int(round(score(i,0))))<<")"
                     <<" The missing stats difference is "<< score(i,2)
-                    <<". Their is "<<lmissing<<" missing while target have "
+                    <<". There are "<<lmissing<<" missing while target has "
                     <<tmissing<<" missing."<<endl;
                 nbdiff++;
             }
@@ -1046,7 +1046,7 @@
         }
 
         sortRows(score,2,false);
-        pout <<"Kolmogorow Smirnow two sample test"<<endl<<endl;
+        pout <<"Kolmogorov Smirnov two sample test"<<endl<<endl;
         if(threshold<REAL_MAX)
             pout<<"Variables that are under the threshold"<<endl;
         pout<<"Sorted by p_value"<<endl;
@@ -1070,7 +1070,7 @@
             }
         }
         if(threshold<REAL_MAX)
-            pout << "Their is "<<threshold_fail<<" variable that are under the threshold"<<endl;
+            pout << "There are "<<threshold_fail<<" variables that are under the threshold"<<endl;
         if(threshold==REAL_MAX)
         {
             pout << "99% cutoff: "<<pc_value_99<<endl;
@@ -1078,7 +1078,7 @@
             pout << "90% cutoff: "<<pc_value_90<<endl;
             pout << "0-90% cutoff: "<<pc_value_0<<endl;
         }
-        pout <<"Kolmogorow Smirnow two sample test end"<<endl<<endl;
+        pout <<"Kolmogorov Smirnov two sample test end"<<endl<<endl;
     }
     else if(command=="compare_stats_desjardins")
     {      
@@ -1136,7 +1136,7 @@
         }
 
         sortRows(score,2,false);
-        pout <<"Kolmogorow Smirnow two sample test"<<endl<<endl;
+        pout <<"Kolmogorov Smirnov two sample test"<<endl<<endl;
         pout<<"Variables that are under the ks_threshold"<<endl;
         pout<<"Sorted by p_value"<<endl;
         cout << std::left << setw(8) << "# "
@@ -1158,9 +1158,9 @@
                 threshold_fail++;
             }
         }
-        pout << "Their is "<<threshold_fail<<"/"<<m1->width()<<
-            " variable that are under the threshold"<<endl<<
-            " Kolmogorow Smirnow two sample test end"<<endl<<endl;
+        pout << "There are "<<threshold_fail<<"/"<<m1->width()<<
+            " variables that are under the threshold"<<endl<<
+            " Kolmogorov Smirnov two sample test end"<<endl<<endl;
 
 
 //         real stderror_threshold = 1;



From nouiz at mail.berlios.de  Fri Mar  7 21:29:29 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 7 Mar 2008 21:29:29 +0100
Subject: [Plearn-commits] r8646 - trunk/plearn/vmat
Message-ID: <200803072029.m27KTTja010847@sheep.berlios.de>

Author: nouiz
Date: 2008-03-07 21:29:28 +0100 (Fri, 07 Mar 2008)
New Revision: 8646

Modified:
   trunk/plearn/vmat/ConcatRowsVMatrix.cc
Log:
optimisation, create the matrix only if needed.


Modified: trunk/plearn/vmat/ConcatRowsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ConcatRowsVMatrix.cc	2008-03-07 20:07:34 UTC (rev 8645)
+++ trunk/plearn/vmat/ConcatRowsVMatrix.cc	2008-03-07 20:29:28 UTC (rev 8646)
@@ -312,7 +312,11 @@
     // Now fill 'to_concat' with the corresponding VMats.
     to_concat.resize(sources.length());
     for (int i = 0; i < sources.length(); i++) {
-        to_concat[i] = new SelectColumnsVMatrix(sources[i], fnames, true);
+        TVec<string> source_fnames=sources[i]->fieldNames();
+        if(fnames!=source_fnames)
+            to_concat[i] = new SelectColumnsVMatrix(sources[i], fnames, true);
+        else
+            to_concat[i] = sources[i];
     }
 }
 



From nouiz at mail.berlios.de  Fri Mar  7 22:57:42 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 7 Mar 2008 22:57:42 +0100
Subject: [Plearn-commits] r8647 - trunk/plearn/vmat
Message-ID: <200803072157.m27LvgQX021037@sheep.berlios.de>

Author: nouiz
Date: 2008-03-07 22:57:42 +0100 (Fri, 07 Mar 2008)
New Revision: 8647

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
Log:
small code refactoring


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2008-03-07 20:29:28 UTC (rev 8646)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2008-03-07 21:57:42 UTC (rev 8647)
@@ -613,28 +613,25 @@
     if(fieldtype=="skip")
     {
         // do nothing, simply skip it
+        return;
     }
+    
+    if(strval=="")  // missing
+        dest[0] = MISSING_VALUE;
     else if(fieldtype=="auto")
     {
-        if(strval=="")  // missing
-            dest[0] = MISSING_VALUE;
-        else if(pl_isnumber(strval,&val))
+        if(pl_isnumber(strval,&val))
             dest[0] = real(val);
         else
             dest[0] = getMapping(k, strval);
     }
     else if(fieldtype=="char")
     {
-        if(strval=="")  // missing
-            dest[0] = MISSING_VALUE;
-        else
-            dest[0] = getMapping(k, strval);
+        dest[0] = getMapping(k, strval);
     }
     else if(fieldtype=="num")
     {
-        if(strval=="")  // missing
-            dest[0] = MISSING_VALUE;
-        else if(pl_isnumber(strval,&val))
+        if(pl_isnumber(strval,&val))
             dest[0] = real(val);
         else
             PLERROR("In TextFilesVMatrix::transformStringToValue - expedted a number as the value for field %d(%s). Got %s",k,fieldname.c_str(),strval.c_str());
@@ -642,23 +639,15 @@
     }
     else if(fieldtype=="date")
     {
-        if(strval=="")  // missing
-            dest[0] = MISSING_VALUE;
-        else
-            dest[0] = date_to_float(PDate(strval));
+        dest[0] = date_to_float(PDate(strval));
     }
-
     else if(fieldtype=="jdate")
     {
-        if(strval=="")  // missing
-            dest[0] = MISSING_VALUE;
-        else
-            dest[0] = PDate(strval).toJulianDay();
+        dest[0] = PDate(strval).toJulianDay();
     }
-
     else if(fieldtype=="sas_date")
     {
-        if(strval=="" || strval == "0")  // missing
+        if(strval == "0")  // missing
             dest[0] = MISSING_VALUE;
         else if(pl_isnumber(strval,&val)) {
             dest[0] = val;
@@ -669,15 +658,13 @@
         else
             PLERROR("Error while parsing a sas_date");
     }
-
     else if(fieldtype=="YYYYMM")
     {
-        if(strval=="" || !pl_isnumber(strval) || toint(strval)<197000)
+        if(!pl_isnumber(strval) || toint(strval)<197000)
             dest[0] = MISSING_VALUE;
         else
             dest[0] = PDate(strval+"01").toJulianDay();
     }
-
     else if(fieldtype=="postal")
     {
         dest[0] = getPostalEncoding(strval);
@@ -687,8 +674,6 @@
         char char_torm = ' ';
         if(fieldtype=="dollar-comma")
             char_torm = ',';
-        if(strval=="")  // missing
-            dest[0] = MISSING_VALUE;
         else if(strval[0]=='$')
         {
             string s = "";
@@ -705,10 +690,7 @@
             PLERROR("In TextFilesVMatrix::transformStringToValue - Got as value '%s' while expecting a value beggining with '$' while parsing field %d (%s) with fieldtype %s",strval.c_str(),k,fieldname.c_str(),fieldtype.c_str());
     }
     else if(fieldtype=="bell_range") {
-        if (strval == "") {
-            // Missing value.
-            dest[0] = MISSING_VALUE;
-        } else if (strval == "Negative Value") {
+        if (strval == "Negative Value") {
             // We put an arbitrary negative value since we don't have more info.
             dest[0] = -100;
         } else {
@@ -745,15 +727,12 @@
             if(strval[i]!=',')
                 s=s+strval[i];
         }
-        if(s=="")  // missing
-            dest[0] = MISSING_VALUE;
-        else if(pl_isnumber(s,&val))
+        if(pl_isnumber(s,&val))
             dest[0] = real(val);
         else
             PLERROR("In TextFilesVMatrix::transformStringToValue - expedted a number as the value for field %d(%s). Got %s",k,fieldname.c_str(),strval.c_str());
                 
     }
-
     else
     {
         PLERROR("TextFilesVMatrix::TextFilesVMatrix::transformStringToValue, Invalid field type specification for field %s: %s",fieldname.c_str(), fieldtype.c_str());



From nouiz at mail.berlios.de  Mon Mar 10 16:09:34 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 10 Mar 2008 16:09:34 +0100
Subject: [Plearn-commits] r8648 - in trunk: plearn/vmat
	plearn_learners/distributions/test
	plearn_learners/distributions/test/.pytest/PL_GaussMix_Generate/expected_results
Message-ID: <200803101509.m2AF9YgB000840@sheep.berlios.de>

Author: nouiz
Date: 2008-03-10 16:09:32 +0100 (Mon, 10 Mar 2008)
New Revision: 8648

Modified:
   trunk/plearn/vmat/ConcatRowsVMatrix.cc
   trunk/plearn/vmat/SelectColumnsVMatrix.cc
   trunk/plearn/vmat/SelectColumnsVMatrix.h
   trunk/plearn_learners/distributions/test/.pytest/PL_GaussMix_Generate/expected_results/RUN.log
   trunk/plearn_learners/distributions/test/gaussian_mixtures_generate.pymat
Log:
-Added warning in SelectColumnsVMatrix when we add a missing columns with missing value
-To remove not needed new warning, we have better handgling of missing value for inputsize, targetsize,... in SelectColumnsVMatrix and in ConcatRowsVMatrix
-updated test PL_GaussMix_Generate to reflect new warning


Modified: trunk/plearn/vmat/ConcatRowsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ConcatRowsVMatrix.cc	2008-03-07 21:57:42 UTC (rev 8647)
+++ trunk/plearn/vmat/ConcatRowsVMatrix.cc	2008-03-10 15:09:32 UTC (rev 8648)
@@ -162,6 +162,18 @@
     // Set length_ and width_.
     recomputeDimensions();
 
+    // Note that the 3 lines below will overwrite any provided sizes.
+    if(inputsize_<0 && targetsize_<0 && weightsize_<0){
+        inputsize_ = to_concat[0]->inputsize();
+        targetsize_ = to_concat[0]->targetsize();
+        weightsize_ = to_concat[0]->weightsize();
+        if(fieldinfos.size()!=to_concat[0].width())
+            PLERROR("In ConcatRowsVMatrix::build_() - We override "
+                      "inputsize, targetsize and weightsize with the value"
+                      " of sources[0] and this cause inconsistency");
+    }
+    computeMissingSizeValue();
+
     // Make sure mappings are correct.
     ensureMappingsConsistency();
     if (fully_check_mappings)
@@ -169,11 +181,6 @@
     if (need_fix_mappings && !fully_check_mappings)
         PLWARNING("In ConcatRowsVMatrix::build_ - Mappings need to be fixed, but you did not set 'fully_check_mappings' to true, this might be dangerous");
 
-    // TODO Note that the 3 lines below will overwrite any provided sizes.
-    inputsize_ = to_concat[0]->inputsize();
-    targetsize_ = to_concat[0]->targetsize();
-    weightsize_ = to_concat[0]->weightsize();
-
     // Reset last-used cache for completeness
     m_last_vmat_index = m_last_vmat_startrow = m_last_vmat_lastrow = -1;
 }
@@ -313,9 +320,15 @@
     to_concat.resize(sources.length());
     for (int i = 0; i < sources.length(); i++) {
         TVec<string> source_fnames=sources[i]->fieldNames();
-        if(fnames!=source_fnames)
-            to_concat[i] = new SelectColumnsVMatrix(sources[i], fnames, true);
-        else
+        if(fnames!=source_fnames){
+            SelectColumnsVMatrix* v = 
+                new SelectColumnsVMatrix(sources[i],fnames,true,false);
+            //to remove warning
+            if(inputsize_>=0 && targetsize_>=0 && weightsize_ >=0)
+                v->defineSizes(inputsize_, targetsize_, weightsize_,extrasize_);
+            v->build();
+            to_concat[i] = v;
+        } else
             to_concat[i] = sources[i];
     }
 }

Modified: trunk/plearn/vmat/SelectColumnsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-03-07 21:57:42 UTC (rev 8647)
+++ trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-03-10 15:09:32 UTC (rev 8648)
@@ -62,14 +62,18 @@
       inverse_fields_selection(false)
 {}
 
-SelectColumnsVMatrix::SelectColumnsVMatrix(VMat the_source, TVec<string> the_fields, bool the_extend_with_missing)
+SelectColumnsVMatrix::SelectColumnsVMatrix(VMat the_source,
+                                           TVec<string> the_fields,
+                                           bool the_extend_with_missing,
+                                           bool call_build_)
     : extend_with_missing(the_extend_with_missing),
       fields(the_fields),
       fields_partial_match(false),
       inverse_fields_selection(false)
 {
     source = the_source;
-    build_();
+    if (call_build_)
+        build_();
 }
 
 SelectColumnsVMatrix::SelectColumnsVMatrix(VMat the_source,
@@ -224,6 +228,12 @@
                                     "    (you may want to use the 'extend_with_missing' option)", the_field.c_str());
                     } else
                         indices.append(the_index);
+                    if(extend_with_missing && the_index == -1)
+                        PLWARNING("In SelectColumnsVMatrix::build_() - We are"
+                                  " extendind the source matrix with the"
+                                  " columns '%s' with missing value",
+                                  the_field.c_str());
+                    
                 }
             } else {
                 // We need to check whether or not we should add each field.
@@ -255,17 +265,18 @@
         }
         // Copy matrix dimensions
         width_ = indices.length();
+        if(!extend_with_missing)
+            PLCHECK(source->width()>width_);
         length_ = source->length();
 
-        // Check sizes are consistent with width.
-        if(inputsize_ >= 0 && targetsize_ >= 0 && weightsize_ >= 0 &&
-           extrasize_ >= 0  &&
-           (inputsize_ + targetsize_ + weightsize_ + extrasize_ != width_))
-        {
-            PLWARNING("In SelectColumnsVMatrix::build_() - inputsize_(%d) +"
-                      " targetsize_(%d) + weightsize_(%d) + extrasize_(%d) != width_(%d) !",
-                      inputsize_, targetsize_, weightsize_, extrasize_, width_);
-        }
+        //if we don't add new columns and the source columns type are emtpy,
+        //they should be empty in this matrix if they are not already set.
+        if(targetsize_<0 && !extend_with_missing && source->targetsize()==0)
+            targetsize_=0;
+        if(weightsize_<0 && !extend_with_missing && source->weightsize()==0)
+            weightsize_=0;
+        computeMissingSizeValue();
+
 #if 0
         // Disabled for now, since it gives way too many false positives in
         // some cases. Todo: figure out a way to warn in a useful way when

Modified: trunk/plearn/vmat/SelectColumnsVMatrix.h
===================================================================
--- trunk/plearn/vmat/SelectColumnsVMatrix.h	2008-03-07 21:57:42 UTC (rev 8647)
+++ trunk/plearn/vmat/SelectColumnsVMatrix.h	2008-03-10 15:09:32 UTC (rev 8648)
@@ -80,7 +80,9 @@
 
     //! The appropriate fieldinfos are copied upon construction.
     //! Here the indices will be shared for efficiency. But you should not modify them afterwards!
-    SelectColumnsVMatrix(VMat the_source, TVec<string> the_fields, bool the_extend_with_missing = false);
+    SelectColumnsVMatrix(VMat the_source, TVec<string> the_fields,
+                         bool the_extend_with_missing = false,
+                         bool call_build_ = true);
 
     //! The appropriate fieldinfos are copied upon construction
     //! Here the indices will be shared for efficiency. But you should not modify them afterwards!

Modified: trunk/plearn_learners/distributions/test/.pytest/PL_GaussMix_Generate/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/distributions/test/.pytest/PL_GaussMix_Generate/expected_results/RUN.log	2008-03-07 21:57:42 UTC (rev 8647)
+++ trunk/plearn_learners/distributions/test/.pytest/PL_GaussMix_Generate/expected_results/RUN.log	2008-03-10 15:09:32 UTC (rev 8648)
@@ -0,0 +1,3 @@
+ WARNING: In SelectColumnsVMatrix::build_() - We are extendind the source matrix with the columns '2' with missing value
+ WARNING: In SelectColumnsVMatrix::build_() - We are extendind the source matrix with the columns '2' with missing value
+ WARNING: In SelectColumnsVMatrix::build_() - We are extendind the source matrix with the columns '2' with missing value

Modified: trunk/plearn_learners/distributions/test/gaussian_mixtures_generate.pymat
===================================================================
--- trunk/plearn_learners/distributions/test/gaussian_mixtures_generate.pymat	2008-03-07 21:57:42 UTC (rev 8647)
+++ trunk/plearn_learners/distributions/test/gaussian_mixtures_generate.pymat	2008-03-10 15:09:32 UTC (rev 8648)
@@ -105,6 +105,9 @@
                 generate_vmat(diagonal(1), 20),
                 generate_vmat(general(0), 20),
                 generate_vmat(general(1), 20)
-            ]
+            ],
+	    inputsize=3,
+	    targetsize=0,
+	    weightsize=0
            )
 



From nouiz at mail.berlios.de  Mon Mar 10 20:11:32 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 10 Mar 2008 20:11:32 +0100
Subject: [Plearn-commits] r8649 - trunk/plearn/vmat
Message-ID: <200803101911.m2AJBWaX014481@sheep.berlios.de>

Author: nouiz
Date: 2008-03-10 20:11:32 +0100 (Mon, 10 Mar 2008)
New Revision: 8649

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
better error msg


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-03-10 15:09:32 UTC (rev 8648)
+++ trunk/plearn/vmat/VMatrix.cc	2008-03-10 19:11:32 UTC (rev 8649)
@@ -1457,14 +1457,16 @@
     if(exist)
         uptodate = getMtime() < mtime(path);
     if (warning_reuse && exist && uptodate && getMtime()==0)
-        PLWARNING("In VMatrix::isFileUpToDate - File '%s' will be used, but "
+        PLWARNING("In VMatrix::isFileUpToDate - for class '%s'"
+                  " File '%s' will be used, but "
                   "this VMat's last modification time is undefined: we cannot "
                   "be sure the file is up-to-date",
-                  path.absolute().c_str());
+                  classname().c_str(), path.absolute().c_str());
     if(warning_older && exist && !uptodate)
-        PLWARNING("In VMatrix::isFileUpToDate - File '%s' is older than this "
+        PLWARNING("In VMatrix::isFileUpToDate - for class '%s'"
+                  "File '%s' is older than this "
                   "VMat's mtime of %d, and cannot be re-used",
-                  path.absolute().c_str(), getMtime());
+                  classname().c_str(), path.absolute().c_str(), getMtime());
 
     return exist && uptodate;
 }



From nouiz at mail.berlios.de  Mon Mar 10 20:57:54 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 10 Mar 2008 20:57:54 +0100
Subject: [Plearn-commits] r8650 - trunk/plearn/vmat
Message-ID: <200803101957.m2AJvswn020558@sheep.berlios.de>

Author: nouiz
Date: 2008-03-10 20:57:54 +0100 (Mon, 10 Mar 2008)
New Revision: 8650

Modified:
   trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
   trunk/plearn/vmat/MeanMedianModeImputationVMatrix.h
Log:
better error msg


Modified: trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-03-10 19:11:32 UTC (rev 8649)
+++ trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-03-10 19:57:54 UTC (rev 8650)
@@ -256,7 +256,8 @@
     PPath train_metadata = train_set->getMetaDataDir();
     PPath mean_median_mode_file_name = train_metadata + "mean_median_mode_file.pmat";
     train_set->lockMetaDataDir();
-    if (!train_set->isFileUpToDate(mean_median_mode_file_name,true,true))
+    if (!train_set->isFileUpToDate(mean_median_mode_file_name,true,true)
+	||!source->isFileUpToDate(mean_median_mode_file_name,true,true))
     {
         computeMeanMedianModeVectors();
         createMeanMedianModeFile(mean_median_mode_file_name);
@@ -275,21 +276,13 @@
 
 void MeanMedianModeImputationVMatrix::loadMeanMedianModeFile(PPath file_name)
 {
+    isFileUpToDate(file_name,true,true);
+    source->isFileUpToDate(file_name,true,true);
+
     mean_median_mode_file = new FileVMatrix(file_name);
     mean_median_mode_file->getRow(0, variable_mean);
     mean_median_mode_file->getRow(1, variable_median);
     mean_median_mode_file->getRow(2, variable_mode);
-    time_t source_time = source->getMtime();
-    time_t stat_file_time = mean_median_mode_file->getMtime();
-    if(stat_file_time==0)
-      PLWARNING("In MeanMedianModeImputationVMatrix::loadMeanMedianModeFile() - "
-		"The precomputed stats file '%s'"
-		" have a modification time of 0",file_name.c_str());
-    else if(source_time>stat_file_time)
-            PLWARNING("In MeanMedianModeImputationVMatrix::loadMeanMedianModeFile()"
-		      " - The precomputed stats file '%s'"
-		      " was created before the source file. Delete it to have it recreated next time."
-		      ,file_name.c_str());
 }
 
 VMat MeanMedianModeImputationVMatrix::getMeanMedianModeFile()

Modified: trunk/plearn/vmat/MeanMedianModeImputationVMatrix.h
===================================================================
--- trunk/plearn/vmat/MeanMedianModeImputationVMatrix.h	2008-03-10 19:11:32 UTC (rev 8649)
+++ trunk/plearn/vmat/MeanMedianModeImputationVMatrix.h	2008-03-10 19:57:54 UTC (rev 8650)
@@ -70,7 +70,7 @@
   //! If greater or equal to 1, the integer portion is interpreted as the number of samples to use.
   real                          number_of_train_samples_to_use;
   
-  //! Pairs of instruction of the form field_name : mean | median | mode.
+  //! Pairs of instruction of the form field_name : mean | median | mode | none.
   TVec< pair<string, string> >  imputation_spec;
   
   //! The vector of variable means observed from the train set.
@@ -85,8 +85,6 @@
   //! The vector of coded instruction for each variables.
   TVec<int>                     variable_imputation_instruction;
   
-  //! Pairs of instruction of the form field_name : mean | median | mode.
-  
 
                         MeanMedianModeImputationVMatrix();
   virtual               ~MeanMedianModeImputationVMatrix();



From nouiz at mail.berlios.de  Mon Mar 10 20:58:15 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 10 Mar 2008 20:58:15 +0100
Subject: [Plearn-commits] r8651 - trunk/plearn/vmat
Message-ID: <200803101958.m2AJwFQO020667@sheep.berlios.de>

Author: nouiz
Date: 2008-03-10 20:58:15 +0100 (Mon, 10 Mar 2008)
New Revision: 8651

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
better error msg


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-03-10 19:57:54 UTC (rev 8650)
+++ trunk/plearn/vmat/VMatrix.cc	2008-03-10 19:58:15 UTC (rev 8651)
@@ -1460,12 +1460,12 @@
         PLWARNING("In VMatrix::isFileUpToDate - for class '%s'"
                   " File '%s' will be used, but "
                   "this VMat's last modification time is undefined: we cannot "
-                  "be sure the file is up-to-date",
+                  "be sure the file is up-to-date.",
                   classname().c_str(), path.absolute().c_str());
     if(warning_older && exist && !uptodate)
         PLWARNING("In VMatrix::isFileUpToDate - for class '%s'"
-                  "File '%s' is older than this "
-                  "VMat's mtime of %d, and cannot be re-used",
+                  " File '%s' is older than this "
+                  "VMat's mtime of %d, and cannot be re-used.",
                   classname().c_str(), path.absolute().c_str(), getMtime());
 
     return exist && uptodate;



From saintmlx at mail.berlios.de  Mon Mar 10 22:02:56 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Mon, 10 Mar 2008 22:02:56 +0100
Subject: [Plearn-commits] r8652 - trunk/plearn/vmat
Message-ID: <200803102102.m2AL2uxp027578@sheep.berlios.de>

Author: saintmlx
Date: 2008-03-10 22:02:55 +0100 (Mon, 10 Mar 2008)
New Revision: 8652

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
- don't try to lock metadatadir when there is none



Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-03-10 19:58:15 UTC (rev 8651)
+++ trunk/plearn/vmat/VMatrix.cc	2008-03-10 21:02:55 UTC (rev 8652)
@@ -1480,7 +1480,7 @@
     TVec<StatsCollector> stats;
     PPath metadatadir = getMetaDataDir();
     PPath statsfile =  metadatadir / filename;
-    lockMetaDataDir();
+    if(!metadatadir.isEmpty()) lockMetaDataDir(); // don't try to lock nothing
     bool uptodate= isFileUpToDate(statsfile, true, true);
     if (uptodate)
         PLearn::load(statsfile, stats);
@@ -1491,7 +1491,7 @@
         if(!metadatadir.isEmpty())
             PLearn::save(statsfile, stats);
     }
-    unlockMetaDataDir();
+    if(!metadatadir.isEmpty()) unlockMetaDataDir();// don't try to unlock nothing
     return stats;
 }
 



From nouiz at mail.berlios.de  Mon Mar 10 22:34:35 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 10 Mar 2008 22:34:35 +0100
Subject: [Plearn-commits] r8653 - trunk/plearn/vmat
Message-ID: <200803102134.m2ALYZ1p030516@sheep.berlios.de>

Author: nouiz
Date: 2008-03-10 22:34:34 +0100 (Mon, 10 Mar 2008)
New Revision: 8653

Modified:
   trunk/plearn/vmat/MeanMedianModeImputationVMatrix.h
Log:
made static some function


Modified: trunk/plearn/vmat/MeanMedianModeImputationVMatrix.h
===================================================================
--- trunk/plearn/vmat/MeanMedianModeImputationVMatrix.h	2008-03-10 21:02:55 UTC (rev 8652)
+++ trunk/plearn/vmat/MeanMedianModeImputationVMatrix.h	2008-03-10 21:34:34 UTC (rev 8653)
@@ -109,10 +109,10 @@
           void         createMeanMedianModeFile(PPath file_name); 
           void         loadMeanMedianModeFile(PPath file_name); 
           void         computeMeanMedianModeVectors();  
-          void         sortColumn(Vec input_vec, int start, int end);
-          void         sortSmallSubArray(Vec input_vec, int start_index, int end_index);
-          void         swapValues(Vec input_vec, int index_i, int index_j);
-          real         compare(real field1, real field2);
+  static  void         sortColumn(Vec input_vec, int start, int end);
+  static  void         sortSmallSubArray(Vec input_vec, int start_index, int end_index);
+  static  void         swapValues(Vec input_vec, int index_i, int index_j);
+  static  real         compare(real field1, real field2);
   
   PLEARN_DECLARE_OBJECT(MeanMedianModeImputationVMatrix);
 



From nouiz at mail.berlios.de  Mon Mar 10 22:38:20 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 10 Mar 2008 22:38:20 +0100
Subject: [Plearn-commits] r8654 - in trunk/plearn_learners: regressors
	testers
Message-ID: <200803102138.m2ALcKwi030793@sheep.berlios.de>

Author: nouiz
Date: 2008-03-10 22:38:19 +0100 (Mon, 10 Mar 2008)
New Revision: 8654

Modified:
   trunk/plearn_learners/regressors/RegressionTree.cc
   trunk/plearn_learners/regressors/RegressionTree.h
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
   trunk/plearn_learners/regressors/RegressionTreeNode.h
   trunk/plearn_learners/testers/PTester.cc
Log:
-Added costs in RegressionTree that give the relative weight of variable for the prediction.
-In PTester, made some modification as RegressionTree::getTestCostNames() need a train_set...


Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2008-03-10 21:34:34 UTC (rev 8653)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2008-03-10 21:38:19 UTC (rev 8654)
@@ -173,8 +173,8 @@
         sample_input.resize(inputsize);
         sample_target.resize(targetsize);
         sample_output.resize(outputsize());
+        sample_costs.resize(getTestCostNames().size());
     }
-    sample_costs.resize(getTestCostNames().size());
 
     if (loss_function_weight != 0.0)
     {
@@ -334,12 +334,29 @@
 
 TVec<string> RegressionTree::getTestCostNames() const
 { 
-    return getTrainCostNames();
+    TVec<string> costs=getTrainCostNames();
+    PP<VMatrix> the_train_set=train_set;
+    if(sorted_train_set)
+        the_train_set = sorted_train_set;
+
+    PLCHECK_MSG(the_train_set,"In RegressionTree::getTestCostNames() - "
+                "a train set is needed!");
+    for(int i=0;i<the_train_set->width();i++)
+    {
+        costs.append("SPLIT_VAR_"+the_train_set->fieldName(i));
+    }
+    return costs;
 }
 
 void RegressionTree::computeOutput(const Vec& inputv, Vec& outputv) const
 {
-    root->computeOutput(inputv, outputv);
+    computeOutputAndNodes(inputv,outputv);
+}
+
+void RegressionTree::computeOutputAndNodes(const Vec& inputv, Vec& outputv,
+                                           TVec<PP<RegressionTreeNode> >* nodes) const
+{
+    root->computeOutputAndNodes(inputv, outputv, nodes);
     if (multiclass_outputs.length() <= 0) return;
     real closest_value=multiclass_outputs[0];
     real margin_to_closest_value=abs(outputv[0] - multiclass_outputs[0]);
@@ -357,11 +374,21 @@
 
 void RegressionTree::computeCostsFromOutputs(const Vec& inputv, const Vec& outputv, const Vec& targetv, Vec& costsv) const
 {
+    PLASSERT(costsv.size()==nTestCosts());
+    costsv.clear();
     costsv[0] = pow((outputv[0] - targetv[0]), 2);
     costsv[1] = outputv[1];
     costsv[2] = 1.0 - (l2_loss_function_factor * costsv[0]);
     costsv[3] = 1.0 - (l1_loss_function_factor * abs(outputv[0] - targetv[0]));
     costsv[4] = !fast_is_equal(targetv[0],outputv[0]);
+    Vec tmp(outputv.size());
+    TVec<PP<RegressionTreeNode> > nodes;
+    root->computeOutputAndNodes(inputv, tmp, &nodes);
+    PLASSERT(outputv==tmp);
+    for(int i=0;i<nodes.length();i++)
+    {
+        costsv[5+nodes[i]->getSplitCol()]++;
+    }
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn_learners/regressors/RegressionTree.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.h	2008-03-10 21:34:34 UTC (rev 8653)
+++ trunk/plearn_learners/regressors/RegressionTree.h	2008-03-10 21:38:19 UTC (rev 8654)
@@ -106,6 +106,8 @@
     virtual TVec<string> getTrainCostNames() const;
     virtual TVec<string> getTestCostNames() const;
     virtual void         computeOutput(const Vec& input, Vec& output) const;
+    virtual void         computeOutputAndNodes(const Vec& input, Vec& output,
+                                               TVec<PP<RegressionTreeNode> >* nodes=0) const;
     virtual void         computeCostsFromOutputs(const Vec& input, const Vec& output, const Vec& target, Vec& costs) const;
     void         setSortedTrainSet(PP<RegressionTreeRegisters> the_sorted_train_set);
   

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-03-10 21:34:34 UTC (rev 8653)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-03-10 21:38:19 UTC (rev 8654)
@@ -380,8 +380,11 @@
     return return_value;
 }
 
-void RegressionTreeNode::computeOutput(const Vec& inputv, Vec& outputv)
+void RegressionTreeNode::computeOutputAndNodes(const Vec& inputv, Vec& outputv,
+                                       TVec<PP<RegressionTreeNode> >* nodes)
 {
+    if(nodes)
+        nodes->append(this);
     if (!left_node)
     {
         outputv[0] = leave_output[0];
@@ -392,7 +395,7 @@
     {
         if (missing_is_valid > 0)
         {
-            missing_node->computeOutput(inputv, outputv);
+            missing_node->computeOutputAndNodes(inputv, outputv, nodes);
         }
         else
         {
@@ -403,12 +406,12 @@
     }
     if (inputv[split_col] > split_feature_value)
     {
-        right_node->computeOutput(inputv, outputv);
+        right_node->computeOutputAndNodes(inputv, outputv, nodes);
         return;
     }
     else
     {
-        left_node->computeOutput(inputv, outputv);
+        left_node->computeOutputAndNodes(inputv, outputv, nodes);
         return;
     }
 }

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.h	2008-03-10 21:34:34 UTC (rev 8653)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.h	2008-03-10 21:38:19 UTC (rev 8654)
@@ -102,7 +102,12 @@
     real         getErrorImprovment()const;
     int          getSplitCol() const;
     TVec< PP<RegressionTreeNode> >  getNodes();
-    void         computeOutput(const Vec& inputv, Vec& outputv);
+    void         computeOutputAndNodes(const Vec& inputv, Vec& outputv, TVec<PP<RegressionTreeNode> >* nodes=0);
+    void         computeOutput(const Vec& inputv, Vec& outputv)
+    {
+        computeOutputAndNodes(inputv,outputv);
+    }
+
     
 private:
     void         build_();

Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2008-03-10 21:34:34 UTC (rev 8653)
+++ trunk/plearn_learners/testers/PTester.cc	2008-03-10 21:38:19 UTC (rev 8654)
@@ -1023,9 +1023,6 @@
     if (nsplits > 1)
         call_forget = true;
 
-    TVec<string> testcostnames = learner->getTestCostNames();
-    TVec<string> traincostnames = learner->getTrainCostNames();
-
     // Global stats collector
     PP<VecStatsCollector> global_statscol;
     if (global_template_stats_collector)
@@ -1059,6 +1056,13 @@
     if (expdir != "" && report_stats)
     {
         if(save_test_names){
+            //To work around the fact that RegressionTree need a
+            // train_set to generate the train/test costs names
+            if(!learner->getTrainingSet())
+                learner->setTrainingSet(splitter->getDataSet(), false);
+
+            TVec<string> testcostnames = learner->getTestCostNames();
+            TVec<string> traincostnames = learner->getTrainCostNames();
             saveStringInFile(expdir / "train_cost_names.txt", join(traincostnames, "\n") + "\n");
             saveStringInFile(expdir / "test_cost_names.txt", join(testcostnames, "\n") + "\n");
         }



From nouiz at mail.berlios.de  Mon Mar 10 22:43:19 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 10 Mar 2008 22:43:19 +0100
Subject: [Plearn-commits] r8655 - in
 trunk/plearn_learners/regressors/test/RegressionTree/.pytest:
 PL_RegressionTree/expected_results/expdir
 PL_RegressionTree/expected_results/expdir/Split0
 PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir
 PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0
 PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0
 PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1
 PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0
 PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2
 PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0
 PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat.metadata
 PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat.metadata
 PL_RegressionTree_MultiClass/expected_results/expdir
 PL_RegressionTree_MultiClass/expected_results/expdir/Split0 P!
 L_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir
 PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0
 PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0
 PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2
 PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0
 PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3
 PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0
 PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_costs.pmat.metadata
 PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_costs.pmat.metadata
Message-ID: <200803102143.m2ALhJsZ031246@sheep.berlios.de>

Author: nouiz
Date: 2008-03-10 22:43:17 +0100 (Mon, 10 Mar 2008)
New Revision: 8655

Modified:
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_outputs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_outputs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/test_cost_names.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_costs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_costs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/test_cost_names.txt
Log:
updated test following last change


Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave	2008-03-10 21:43:17 UTC (rev 8655)
@@ -1,23 +1,23 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+fieldnames = 11 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_0" "SPLIT_VAR_1" "SPLIT_VAR_2" "SPLIT_VAR_3" "SPLIT_VAR_4" "SPLIT_VAR_5" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 5 [ StatsCollector(
+stats = 11 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = 925509.521859630011 ;
+sum_ = 925509.521859630127 ;
 sumsquare_ = 25141454081.3727722 ;
-sumcube_ = 876439142897729.75 ;
-sumfourth_ = 4.10303450723266068e+19 ;
+sumcube_ = 876439142897729.5 ;
+sumfourth_ = 4.10303450723265987e+19 ;
 min_ = 0.0830570112682627176 ;
 max_ = 73642.6125143663958 ;
 agmemin_ = 10 ;
@@ -56,10 +56,10 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = -1851019.04371926002 ;
+sum_ = -1851019.04371926025 ;
 sumsquare_ = 100565816325.491089 ;
-sumcube_ = -7011513143181838 ;
-sumfourth_ = 6.5648552115722571e+20 ;
+sumcube_ = -7011513143181836 ;
+sumfourth_ = 6.56485521157225578e+20 ;
 min_ = -147284.225028732792 ;
 max_ = 0.833885977463474592 ;
 agmemin_ = 95 ;
@@ -77,10 +77,10 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = -7047.2320373333414 ;
-sumsquare_ = 2238180.95607663365 ;
+sum_ = -7047.2320373333423 ;
+sumsquare_ = 2238180.95607663412 ;
 sumcube_ = -435311323.447215438 ;
-sumfourth_ = 124843839496.819016 ;
+sumfourth_ = 124843839496.819046 ;
 min_ = -541.743447733333369 ;
 max_ = 0.423607733333408731 ;
 agmemin_ = 95 ;
@@ -112,6 +112,132 @@
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave	2008-03-10 21:43:17 UTC (rev 8655)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+fieldnames = 11 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_0" "SPLIT_VAR_1" "SPLIT_VAR_2" "SPLIT_VAR_3" "SPLIT_VAR_4" "SPLIT_VAR_5" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 5 [ StatsCollector(
+stats = 11 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -77,10 +77,10 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -3525.57301386666904 ;
-sumsquare_ = 959697.857134004356 ;
-sumcube_ = -237882864.922130883 ;
-sumfourth_ = 74259479439.1794434 ;
+sum_ = -3525.5730138666695 ;
+sumsquare_ = 959697.857134004473 ;
+sumcube_ = -237882864.922130942 ;
+sumfourth_ = 74259479439.1794739 ;
 min_ = -496.42831226666658 ;
 max_ = 0.638267733333410803 ;
 agmemin_ = 15 ;
@@ -112,6 +112,132 @@
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave	2008-03-10 21:43:17 UTC (rev 8655)
@@ -7,17 +7,17 @@
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 5 [ StatsCollector(
+stats = 11 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = 925509.521859630011 ;
+sum_ = 925509.521859630127 ;
 sumsquare_ = 25141454081.3727722 ;
-sumcube_ = 876439142897729.75 ;
-sumfourth_ = 4.10303450723266068e+19 ;
+sumcube_ = 876439142897729.5 ;
+sumfourth_ = 4.10303450723265987e+19 ;
 min_ = 0.0830570112682627176 ;
 max_ = 73642.6125143663958 ;
 agmemin_ = 10 ;
@@ -56,10 +56,10 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = -1851019.04371926002 ;
+sum_ = -1851019.04371926025 ;
 sumsquare_ = 100565816325.491089 ;
-sumcube_ = -7011513143181838 ;
-sumfourth_ = 6.5648552115722571e+20 ;
+sumcube_ = -7011513143181836 ;
+sumfourth_ = 6.56485521157225578e+20 ;
 min_ = -147284.225028732792 ;
 max_ = 0.833885977463474592 ;
 agmemin_ = 95 ;
@@ -77,10 +77,10 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = -7047.2320373333414 ;
-sumsquare_ = 2238180.95607663365 ;
+sum_ = -7047.2320373333423 ;
+sumsquare_ = 2238180.95607663412 ;
 sumcube_ = -435311323.447215438 ;
-sumfourth_ = 124843839496.819016 ;
+sumfourth_ = 124843839496.819046 ;
 min_ = -541.743447733333369 ;
 max_ = 0.423607733333408731 ;
 agmemin_ = 95 ;
@@ -112,6 +112,132 @@
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave	2008-03-10 21:43:17 UTC (rev 8655)
@@ -1,29 +1,29 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+fieldnames = 11 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_0" "SPLIT_VAR_1" "SPLIT_VAR_2" "SPLIT_VAR_3" "SPLIT_VAR_4" "SPLIT_VAR_5" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 5 [ StatsCollector(
+stats = 11 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = 8640.51363742984176 ;
-sumsquare_ = 1374774.31180404266 ;
-sumcube_ = 323112262.385960162 ;
-sumfourth_ = 93349681763.0307159 ;
+sum_ = 8640.51363742983631 ;
+sumsquare_ = 1374774.3118040422 ;
+sumcube_ = 323112262.385959983 ;
+sumfourth_ = 93349681763.0306549 ;
 min_ = 0 ;
-max_ = 434.308100803600041 ;
+max_ = 434.308100803599757 ;
 agmemin_ = 120 ;
 agemax_ = 102 ;
-first_ = 3.3058115703008113 ;
-last_ = 39.3579314881000286 ;
+first_ = 3.30581157030083705 ;
+last_ = 39.3579314881000712 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -56,16 +56,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = -17281.0272748596835 ;
-sumsquare_ = 5499097.24721617065 ;
-sumcube_ = -2584898099.08768129 ;
-sumfourth_ = 1493594908208.49146 ;
-min_ = -867.616201607200082 ;
+sum_ = -17281.0272748596726 ;
+sumsquare_ = 5499097.24721616879 ;
+sumcube_ = -2584898099.08767986 ;
+sumfourth_ = 1493594908208.49048 ;
+min_ = -867.616201607199514 ;
 max_ = 1 ;
 agmemin_ = 102 ;
 agemax_ = 120 ;
-first_ = -5.61162314060162259 ;
-last_ = -77.7158629762000572 ;
+first_ = -5.61162314060167411 ;
+last_ = -77.7158629762001425 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -77,16 +77,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = -1392.41750743656576 ;
-sumsquare_ = 24435.3412616714268 ;
-sumcube_ = -524149.89389633236 ;
-sumfourth_ = 13079901.8943236265 ;
-min_ = -40.6801200000000023 ;
+sum_ = -1392.41750743656257 ;
+sumsquare_ = 24435.3412616713867 ;
+sumcube_ = -524149.89389633137 ;
+sumfourth_ = 13079901.8943236005 ;
+min_ = -40.6801199999999881 ;
 max_ = 1 ;
 agmemin_ = 102 ;
 agemax_ = 120 ;
-first_ = -2.63637818181817352 ;
-last_ = -11.5471800000000044 ;
+first_ = -2.63637818181818773 ;
+last_ = -11.5471800000000115 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -98,20 +98,146 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = -2 ;
-sumsquare_ = 2 ;
-sumcube_ = -2 ;
-sumfourth_ = 2 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
 min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 22 ;
+sumsquare_ = 24 ;
+sumcube_ = 22 ;
+sumfourth_ = 24 ;
+min_ = 3 ;
+max_ = 5 ;
+agmemin_ = 95 ;
+agemax_ = 139 ;
+first_ = 4 ;
+last_ = 4 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = -99 ;
+sumsquare_ = 99 ;
+sumcube_ = -99 ;
+sumfourth_ = 99 ;
+min_ = 0 ;
 max_ = 1 ;
-agmemin_ = 120 ;
+agmemin_ = 148 ;
 agemax_ = 149 ;
 first_ = 1 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 131 ;
+sumsquare_ = 183 ;
+sumcube_ = 287 ;
+sumfourth_ = 495 ;
+min_ = 0 ;
+max_ = 2 ;
+agmemin_ = 149 ;
+agemax_ = 148 ;
+first_ = 0 ;
 last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 0 ;
+last_ = 0 ;
 binary_ = 1 ;
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave	2008-03-10 21:43:17 UTC (rev 8655)
@@ -1,28 +1,28 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+fieldnames = 11 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_0" "SPLIT_VAR_1" "SPLIT_VAR_2" "SPLIT_VAR_3" "SPLIT_VAR_4" "SPLIT_VAR_5" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 5 [ StatsCollector(
+stats = 11 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = 10321.7894884868656 ;
-sumsquare_ = 71200030.8763843179 ;
-sumcube_ = 534446154111.910217 ;
-sumfourth_ = 4273059163655376.5 ;
+sum_ = 10321.7894884868638 ;
+sumsquare_ = 71200030.8763843328 ;
+sumcube_ = 534446154111.910156 ;
+sumfourth_ = 4273059163655375.5 ;
 min_ = 0.296827275773477062 ;
 max_ = 8311.49117606439722 ;
 agmemin_ = 45 ;
 agemax_ = 35 ;
-first_ = 231.958382832400019 ;
+first_ = 231.958382832400133 ;
 last_ = 187.926810649600071 ;
 binary_ = 0 ;
 integer_ = 0 ;
@@ -56,15 +56,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -20643.5789769737312 ;
-sumsquare_ = 284800123.505537271 ;
-sumcube_ = -4275569232895.28174 ;
-sumfourth_ = 68368946618486024 ;
+sum_ = -20643.5789769737275 ;
+sumsquare_ = 284800123.505537331 ;
+sumcube_ = -4275569232895.28125 ;
+sumfourth_ = 68368946618486008 ;
 min_ = -16621.9823521287944 ;
 max_ = 0.406345448453045877 ;
 agmemin_ = 35 ;
 agemax_ = 45 ;
-first_ = -462.916765664800039 ;
+first_ = -462.916765664800266 ;
 last_ = -374.853621299200142 ;
 binary_ = 0 ;
 integer_ = 0 ;
@@ -77,15 +77,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -50.2688274199132863 ;
-sumsquare_ = 38224.7447939705817 ;
-sumcube_ = -3656520.3377262922 ;
-sumfourth_ = 551819990.898841023 ;
+sum_ = -50.2688274199130589 ;
+sumsquare_ = 38224.7447939705889 ;
+sumcube_ = -3656520.3377262908 ;
+sumfourth_ = 551819990.898840904 ;
 min_ = -181.33475999999996 ;
 max_ = -0.0896371428571569595 ;
 agmemin_ = 35 ;
 agemax_ = 45 ;
-first_ = -29.4603600000000014 ;
+first_ = -29.4603600000000085 ;
 last_ = -26.4172800000000052 ;
 binary_ = 0 ;
 integer_ = 0 ;
@@ -98,20 +98,146 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
+sum_ = 2 ;
+sumsquare_ = 2 ;
+sumcube_ = 2 ;
+sumfourth_ = 2 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 49 ;
+agemax_ = 35 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
 sum_ = 0 ;
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 1 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = 6 ;
+sumsquare_ = 8 ;
+sumcube_ = 6 ;
+sumfourth_ = 8 ;
+min_ = 3 ;
+max_ = 5 ;
+agmemin_ = 35 ;
+agemax_ = 45 ;
+first_ = 4 ;
+last_ = 5 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = 16 ;
+sumsquare_ = 16 ;
+sumcube_ = 16 ;
+sumfourth_ = 16 ;
+min_ = 0 ;
 max_ = 1 ;
 agmemin_ = 49 ;
-agemax_ = 49 ;
-first_ = 1 ;
+agemax_ = 46 ;
+first_ = 0 ;
 last_ = 1 ;
 binary_ = 1 ;
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = -4 ;
+sumsquare_ = 24 ;
+sumcube_ = -4 ;
+sumfourth_ = 24 ;
+min_ = 0 ;
+max_ = 2 ;
+agmemin_ = 46 ;
+agemax_ = 44 ;
+first_ = 1 ;
+last_ = 0 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave	2008-03-10 21:43:17 UTC (rev 8655)
@@ -7,23 +7,23 @@
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 5 [ StatsCollector(
+stats = 11 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = 8640.51363742984176 ;
-sumsquare_ = 1374774.31180404266 ;
-sumcube_ = 323112262.385960162 ;
-sumfourth_ = 93349681763.0307159 ;
+sum_ = 8640.51363742983631 ;
+sumsquare_ = 1374774.3118040422 ;
+sumcube_ = 323112262.385959983 ;
+sumfourth_ = 93349681763.0306549 ;
 min_ = 0 ;
-max_ = 434.308100803600041 ;
+max_ = 434.308100803599757 ;
 agmemin_ = 120 ;
 agemax_ = 102 ;
-first_ = 3.3058115703008113 ;
-last_ = 39.3579314881000286 ;
+first_ = 3.30581157030083705 ;
+last_ = 39.3579314881000712 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -56,16 +56,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = -17281.0272748596835 ;
-sumsquare_ = 5499097.24721617065 ;
-sumcube_ = -2584898099.08768129 ;
-sumfourth_ = 1493594908208.49146 ;
-min_ = -867.616201607200082 ;
+sum_ = -17281.0272748596726 ;
+sumsquare_ = 5499097.24721616879 ;
+sumcube_ = -2584898099.08767986 ;
+sumfourth_ = 1493594908208.49048 ;
+min_ = -867.616201607199514 ;
 max_ = 1 ;
 agmemin_ = 102 ;
 agemax_ = 120 ;
-first_ = -5.61162314060162259 ;
-last_ = -77.7158629762000572 ;
+first_ = -5.61162314060167411 ;
+last_ = -77.7158629762001425 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -77,16 +77,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = -1392.41750743656576 ;
-sumsquare_ = 24435.3412616714268 ;
-sumcube_ = -524149.89389633236 ;
-sumfourth_ = 13079901.8943236265 ;
-min_ = -40.6801200000000023 ;
+sum_ = -1392.41750743656257 ;
+sumsquare_ = 24435.3412616713867 ;
+sumcube_ = -524149.89389633137 ;
+sumfourth_ = 13079901.8943236005 ;
+min_ = -40.6801199999999881 ;
 max_ = 1 ;
 agmemin_ = 102 ;
 agemax_ = 120 ;
-first_ = -2.63637818181817352 ;
-last_ = -11.5471800000000044 ;
+first_ = -2.63637818181818773 ;
+last_ = -11.5471800000000115 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -98,20 +98,146 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = -2 ;
-sumsquare_ = 2 ;
-sumcube_ = -2 ;
-sumfourth_ = 2 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
 min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 22 ;
+sumsquare_ = 24 ;
+sumcube_ = 22 ;
+sumfourth_ = 24 ;
+min_ = 3 ;
+max_ = 5 ;
+agmemin_ = 95 ;
+agemax_ = 139 ;
+first_ = 4 ;
+last_ = 4 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = -99 ;
+sumsquare_ = 99 ;
+sumcube_ = -99 ;
+sumfourth_ = 99 ;
+min_ = 0 ;
 max_ = 1 ;
-agmemin_ = 120 ;
+agmemin_ = 148 ;
 agemax_ = 149 ;
 first_ = 1 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 131 ;
+sumsquare_ = 183 ;
+sumcube_ = 287 ;
+sumfourth_ = 495 ;
+min_ = 0 ;
+max_ = 2 ;
+agmemin_ = 149 ;
+agemax_ = 148 ;
+first_ = 0 ;
 last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 0 ;
+last_ = 0 ;
 binary_ = 1 ;
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave	2008-03-10 21:43:17 UTC (rev 8655)
@@ -1,28 +1,28 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+fieldnames = 11 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_0" "SPLIT_VAR_1" "SPLIT_VAR_2" "SPLIT_VAR_3" "SPLIT_VAR_4" "SPLIT_VAR_5" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 5 [ StatsCollector(
+stats = 11 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = 179.545344601616335 ;
-sumsquare_ = 58456.8760552968452 ;
-sumcube_ = 2844509.12695805216 ;
-sumfourth_ = 247849777.418584049 ;
+sum_ = 179.545344601608576 ;
+sumsquare_ = 58456.8760552967724 ;
+sumcube_ = 2844509.126958034 ;
+sumfourth_ = 247849777.418581784 ;
 min_ = 0 ;
-max_ = 128.059475553344555 ;
+max_ = 128.059475553344242 ;
 agmemin_ = 120 ;
 agemax_ = 16 ;
-first_ = 13.8781581156000211 ;
+first_ = 13.8781581156000726 ;
 last_ = 9.08582363289999861 ;
 binary_ = 0 ;
 integer_ = 0 ;
@@ -56,15 +56,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = -359.090689203232671 ;
-sumsquare_ = 233827.504221187381 ;
-sumcube_ = -22756073.0156644173 ;
-sumfourth_ = 3965596438.69734478 ;
-min_ = -255.11895110668911 ;
+sum_ = -359.090689203217153 ;
+sumsquare_ = 233827.50422118709 ;
+sumcube_ = -22756073.015664272 ;
+sumfourth_ = 3965596438.69730854 ;
+min_ = -255.118951106688485 ;
 max_ = 1 ;
 agmemin_ = 16 ;
 agemax_ = 120 ;
-first_ = -26.7563162312000422 ;
+first_ = -26.7563162312001452 ;
 last_ = -17.1716472657999972 ;
 binary_ = 0 ;
 integer_ = 0 ;
@@ -77,15 +77,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = 192.890804746032558 ;
-sumsquare_ = 3592.51670061680898 ;
-sumcube_ = 2141.61368714040918 ;
-sumfourth_ = 201415.693085262814 ;
-min_ = -21.6326733333333436 ;
+sum_ = 192.890804746034803 ;
+sumsquare_ = 3592.51670061681125 ;
+sumcube_ = 2141.61368714056789 ;
+sumfourth_ = 201415.693085262639 ;
+min_ = -21.6326733333333152 ;
 max_ = 1 ;
 agmemin_ = 16 ;
 agemax_ = 120 ;
-first_ = -6.45068000000000552 ;
+first_ = -6.45068000000001973 ;
 last_ = -5.02853999999999957 ;
 binary_ = 0 ;
 integer_ = 0 ;
@@ -98,13 +98,13 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = -6 ;
-sumsquare_ = 6 ;
-sumcube_ = -6 ;
-sumfourth_ = 6 ;
-min_ = 0 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
 max_ = 1 ;
-agmemin_ = 120 ;
+agmemin_ = 149 ;
 agemax_ = 149 ;
 first_ = 1 ;
 last_ = 1 ;
@@ -112,6 +112,132 @@
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 64 ;
+sumsquare_ = 66 ;
+sumcube_ = 64 ;
+sumfourth_ = 66 ;
+min_ = 3 ;
+max_ = 5 ;
+agmemin_ = 95 ;
+agemax_ = 148 ;
+first_ = 4 ;
+last_ = 5 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = -86 ;
+sumsquare_ = 112 ;
+sumcube_ = -86 ;
+sumfourth_ = 112 ;
+min_ = 0 ;
+max_ = 2 ;
+agmemin_ = 148 ;
+agemax_ = 139 ;
+first_ = 1 ;
+last_ = 0 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = -58 ;
+sumsquare_ = 122 ;
+sumcube_ = -172 ;
+sumfourth_ = 350 ;
+min_ = 0 ;
+max_ = 3 ;
+agmemin_ = 145 ;
+agemax_ = 144 ;
+first_ = 2 ;
+last_ = 2 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave	2008-03-10 21:43:17 UTC (rev 8655)
@@ -1,24 +1,24 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+fieldnames = 11 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_0" "SPLIT_VAR_1" "SPLIT_VAR_2" "SPLIT_VAR_3" "SPLIT_VAR_4" "SPLIT_VAR_5" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 5 [ StatsCollector(
+stats = 11 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = 11204.3270149540895 ;
+sum_ = 11204.3270149540876 ;
 sumsquare_ = 76349558.7474929094 ;
 sumcube_ = 569851029912.986938 ;
-sumfourth_ = 4522879206793045 ;
-min_ = 0.650915482711108107 ;
+sumfourth_ = 4522879206793046 ;
+min_ = 0.650915482711153959 ;
 max_ = 8311.49117606439722 ;
 agmemin_ = 17 ;
 agemax_ = 35 ;
@@ -56,12 +56,12 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -22408.6540299081789 ;
+sum_ = -22408.6540299081753 ;
 sumsquare_ = 305398234.989971638 ;
 sumcube_ = -4558808239303.89551 ;
-sumfourth_ = 72366067308688720 ;
+sumfourth_ = 72366067308688736 ;
 min_ = -16621.9823521287944 ;
-max_ = -0.301830965422216213 ;
+max_ = -0.301830965422307917 ;
 agmemin_ = 35 ;
 agemax_ = 17 ;
 first_ = -285.602978279199931 ;
@@ -77,12 +77,12 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -84.9389746746032586 ;
-sumsquare_ = 40750.1377623234584 ;
+sum_ = -84.938974674603287 ;
+sumsquare_ = 40750.1377623234439 ;
 sumcube_ = -4587593.35613886733 ;
-sumfourth_ = 688820550.749982357 ;
+sumfourth_ = 688820550.749982238 ;
 min_ = -181.33475999999996 ;
-max_ = -0.61358666666666295 ;
+max_ = -0.613586666666719793 ;
 agmemin_ = 35 ;
 agemax_ = 17 ;
 first_ = -22.9417199999999966 ;
@@ -98,20 +98,146 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
+sum_ = 5 ;
+sumsquare_ = 5 ;
+sumcube_ = 5 ;
+sumfourth_ = 5 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 49 ;
+agemax_ = 35 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
 sum_ = 0 ;
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 1 ;
-max_ = 1 ;
+min_ = 0 ;
+max_ = 0 ;
 agmemin_ = 49 ;
 agemax_ = 49 ;
-first_ = 1 ;
-last_ = 1 ;
+first_ = 0 ;
+last_ = 0 ;
 binary_ = 1 ;
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = -27 ;
+sumsquare_ = 29 ;
+sumcube_ = -33 ;
+sumfourth_ = 41 ;
+min_ = 3 ;
+max_ = 5 ;
+agmemin_ = 35 ;
+agemax_ = 49 ;
+first_ = 5 ;
+last_ = 5 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = 18 ;
+sumsquare_ = 22 ;
+sumcube_ = 30 ;
+sumfourth_ = 46 ;
+min_ = 0 ;
+max_ = 2 ;
+agmemin_ = 49 ;
+agemax_ = 45 ;
+first_ = 0 ;
+last_ = 2 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = -21 ;
+sumsquare_ = 39 ;
+sumcube_ = -51 ;
+sumfourth_ = 99 ;
+min_ = 0 ;
+max_ = 3 ;
+agmemin_ = 45 ;
+agemax_ = 44 ;
+first_ = 2 ;
+last_ = 0 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave	2008-03-10 21:43:17 UTC (rev 8655)
@@ -7,22 +7,22 @@
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 5 [ StatsCollector(
+stats = 11 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = 179.545344601616335 ;
-sumsquare_ = 58456.8760552968452 ;
-sumcube_ = 2844509.12695805216 ;
-sumfourth_ = 247849777.418584049 ;
+sum_ = 179.545344601608576 ;
+sumsquare_ = 58456.8760552967724 ;
+sumcube_ = 2844509.126958034 ;
+sumfourth_ = 247849777.418581784 ;
 min_ = 0 ;
-max_ = 128.059475553344555 ;
+max_ = 128.059475553344242 ;
 agmemin_ = 120 ;
 agemax_ = 16 ;
-first_ = 13.8781581156000211 ;
+first_ = 13.8781581156000726 ;
 last_ = 9.08582363289999861 ;
 binary_ = 0 ;
 integer_ = 0 ;
@@ -56,15 +56,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = -359.090689203232671 ;
-sumsquare_ = 233827.504221187381 ;
-sumcube_ = -22756073.0156644173 ;
-sumfourth_ = 3965596438.69734478 ;
-min_ = -255.11895110668911 ;
+sum_ = -359.090689203217153 ;
+sumsquare_ = 233827.50422118709 ;
+sumcube_ = -22756073.015664272 ;
+sumfourth_ = 3965596438.69730854 ;
+min_ = -255.118951106688485 ;
 max_ = 1 ;
 agmemin_ = 16 ;
 agemax_ = 120 ;
-first_ = -26.7563162312000422 ;
+first_ = -26.7563162312001452 ;
 last_ = -17.1716472657999972 ;
 binary_ = 0 ;
 integer_ = 0 ;
@@ -77,15 +77,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = 192.890804746032558 ;
-sumsquare_ = 3592.51670061680898 ;
-sumcube_ = 2141.61368714040918 ;
-sumfourth_ = 201415.693085262814 ;
-min_ = -21.6326733333333436 ;
+sum_ = 192.890804746034803 ;
+sumsquare_ = 3592.51670061681125 ;
+sumcube_ = 2141.61368714056789 ;
+sumfourth_ = 201415.693085262639 ;
+min_ = -21.6326733333333152 ;
 max_ = 1 ;
 agmemin_ = 16 ;
 agemax_ = 120 ;
-first_ = -6.45068000000000552 ;
+first_ = -6.45068000000001973 ;
 last_ = -5.02853999999999957 ;
 binary_ = 0 ;
 integer_ = 0 ;
@@ -98,13 +98,13 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = -6 ;
-sumsquare_ = 6 ;
-sumcube_ = -6 ;
-sumfourth_ = 6 ;
-min_ = 0 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
 max_ = 1 ;
-agmemin_ = 120 ;
+agmemin_ = 149 ;
 agemax_ = 149 ;
 first_ = 1 ;
 last_ = 1 ;
@@ -112,6 +112,132 @@
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 64 ;
+sumsquare_ = 66 ;
+sumcube_ = 64 ;
+sumfourth_ = 66 ;
+min_ = 3 ;
+max_ = 5 ;
+agmemin_ = 95 ;
+agemax_ = 148 ;
+first_ = 4 ;
+last_ = 5 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = -86 ;
+sumsquare_ = 112 ;
+sumcube_ = -86 ;
+sumfourth_ = 112 ;
+min_ = 0 ;
+max_ = 2 ;
+agmemin_ = 148 ;
+agemax_ = 139 ;
+first_ = 1 ;
+last_ = 0 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = -58 ;
+sumsquare_ = 122 ;
+sumcube_ = -172 ;
+sumfourth_ = 350 ;
+min_ = 0 ;
+max_ = 3 ;
+agmemin_ = 145 ;
+agemax_ = 144 ;
+first_ = 2 ;
+last_ = 2 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames	2008-03-10 21:43:17 UTC (rev 8655)
@@ -3,3 +3,9 @@
 base_reward_l2	0
 base_reward_l1	0
 class_error	0
+SPLIT_VAR_0	0
+SPLIT_VAR_1	0
+SPLIT_VAR_2	0
+SPLIT_VAR_3	0
+SPLIT_VAR_4	0
+SPLIT_VAR_5	0

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_outputs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_stats.psave	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_stats.psave	2008-03-10 21:43:17 UTC (rev 8655)
@@ -1,28 +1,28 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+fieldnames = 11 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_0" "SPLIT_VAR_1" "SPLIT_VAR_2" "SPLIT_VAR_3" "SPLIT_VAR_4" "SPLIT_VAR_5" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 5 [ StatsCollector(
+stats = 11 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = 179.545344601616335 ;
-sumsquare_ = 58456.8760552968452 ;
-sumcube_ = 2844509.12695805216 ;
-sumfourth_ = 247849777.418584049 ;
+sum_ = 179.545344601608576 ;
+sumsquare_ = 58456.8760552967724 ;
+sumcube_ = 2844509.126958034 ;
+sumfourth_ = 247849777.418581784 ;
 min_ = 0 ;
-max_ = 128.059475553344555 ;
+max_ = 128.059475553344242 ;
 agmemin_ = 120 ;
 agemax_ = 16 ;
-first_ = 13.8781581156000211 ;
+first_ = 13.8781581156000726 ;
 last_ = 9.08582363289999861 ;
 binary_ = 0 ;
 integer_ = 0 ;
@@ -56,15 +56,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = -359.090689203232671 ;
-sumsquare_ = 233827.504221187381 ;
-sumcube_ = -22756073.0156644173 ;
-sumfourth_ = 3965596438.69734478 ;
-min_ = -255.11895110668911 ;
+sum_ = -359.090689203217153 ;
+sumsquare_ = 233827.50422118709 ;
+sumcube_ = -22756073.015664272 ;
+sumfourth_ = 3965596438.69730854 ;
+min_ = -255.118951106688485 ;
 max_ = 1 ;
 agmemin_ = 16 ;
 agemax_ = 120 ;
-first_ = -26.7563162312000422 ;
+first_ = -26.7563162312001452 ;
 last_ = -17.1716472657999972 ;
 binary_ = 0 ;
 integer_ = 0 ;
@@ -77,15 +77,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = 192.890804746032558 ;
-sumsquare_ = 3592.51670061680898 ;
-sumcube_ = 2141.61368714040918 ;
-sumfourth_ = 201415.693085262814 ;
-min_ = -21.6326733333333436 ;
+sum_ = 192.890804746034803 ;
+sumsquare_ = 3592.51670061681125 ;
+sumcube_ = 2141.61368714056789 ;
+sumfourth_ = 201415.693085262639 ;
+min_ = -21.6326733333333152 ;
 max_ = 1 ;
 agmemin_ = 16 ;
 agemax_ = 120 ;
-first_ = -6.45068000000000552 ;
+first_ = -6.45068000000001973 ;
 last_ = -5.02853999999999957 ;
 binary_ = 0 ;
 integer_ = 0 ;
@@ -98,13 +98,13 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = -6 ;
-sumsquare_ = 6 ;
-sumcube_ = -6 ;
-sumfourth_ = 6 ;
-min_ = 0 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
 max_ = 1 ;
-agmemin_ = 120 ;
+agmemin_ = 149 ;
 agemax_ = 149 ;
 first_ = 1 ;
 last_ = 1 ;
@@ -112,6 +112,132 @@
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 64 ;
+sumsquare_ = 66 ;
+sumcube_ = 64 ;
+sumfourth_ = 66 ;
+min_ = 3 ;
+max_ = 5 ;
+agmemin_ = 95 ;
+agemax_ = 148 ;
+first_ = 4 ;
+last_ = 5 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = -86 ;
+sumsquare_ = 112 ;
+sumcube_ = -86 ;
+sumfourth_ = 112 ;
+min_ = 0 ;
+max_ = 2 ;
+agmemin_ = 148 ;
+agemax_ = 139 ;
+first_ = 1 ;
+last_ = 0 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = -58 ;
+sumsquare_ = 122 ;
+sumcube_ = -172 ;
+sumfourth_ = 350 ;
+min_ = 0 ;
+max_ = 3 ;
+agmemin_ = 145 ;
+agemax_ = 144 ;
+first_ = 2 ;
+last_ = 2 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames	2008-03-10 21:43:17 UTC (rev 8655)
@@ -3,3 +3,9 @@
 base_reward_l2	0
 base_reward_l1	0
 class_error	0
+SPLIT_VAR_0	0
+SPLIT_VAR_1	0
+SPLIT_VAR_2	0
+SPLIT_VAR_3	0
+SPLIT_VAR_4	0
+SPLIT_VAR_5	0

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_outputs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_stats.psave	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_stats.psave	2008-03-10 21:43:17 UTC (rev 8655)
@@ -1,24 +1,24 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+fieldnames = 11 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_0" "SPLIT_VAR_1" "SPLIT_VAR_2" "SPLIT_VAR_3" "SPLIT_VAR_4" "SPLIT_VAR_5" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 5 [ StatsCollector(
+stats = 11 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = 11204.3270149540895 ;
+sum_ = 11204.3270149540876 ;
 sumsquare_ = 76349558.7474929094 ;
 sumcube_ = 569851029912.986938 ;
-sumfourth_ = 4522879206793045 ;
-min_ = 0.650915482711108107 ;
+sumfourth_ = 4522879206793046 ;
+min_ = 0.650915482711153959 ;
 max_ = 8311.49117606439722 ;
 agmemin_ = 17 ;
 agemax_ = 35 ;
@@ -56,12 +56,12 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -22408.6540299081789 ;
+sum_ = -22408.6540299081753 ;
 sumsquare_ = 305398234.989971638 ;
 sumcube_ = -4558808239303.89551 ;
-sumfourth_ = 72366067308688720 ;
+sumfourth_ = 72366067308688736 ;
 min_ = -16621.9823521287944 ;
-max_ = -0.301830965422216213 ;
+max_ = -0.301830965422307917 ;
 agmemin_ = 35 ;
 agemax_ = 17 ;
 first_ = -285.602978279199931 ;
@@ -77,12 +77,12 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -84.9389746746032586 ;
-sumsquare_ = 40750.1377623234584 ;
+sum_ = -84.938974674603287 ;
+sumsquare_ = 40750.1377623234439 ;
 sumcube_ = -4587593.35613886733 ;
-sumfourth_ = 688820550.749982357 ;
+sumfourth_ = 688820550.749982238 ;
 min_ = -181.33475999999996 ;
-max_ = -0.61358666666666295 ;
+max_ = -0.613586666666719793 ;
 agmemin_ = 35 ;
 agemax_ = 17 ;
 first_ = -22.9417199999999966 ;
@@ -98,20 +98,146 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
+sum_ = 5 ;
+sumsquare_ = 5 ;
+sumcube_ = 5 ;
+sumfourth_ = 5 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 49 ;
+agemax_ = 35 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
 sum_ = 0 ;
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 1 ;
-max_ = 1 ;
+min_ = 0 ;
+max_ = 0 ;
 agmemin_ = 49 ;
 agemax_ = 49 ;
-first_ = 1 ;
-last_ = 1 ;
+first_ = 0 ;
+last_ = 0 ;
 binary_ = 1 ;
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = -27 ;
+sumsquare_ = 29 ;
+sumcube_ = -33 ;
+sumfourth_ = 41 ;
+min_ = 3 ;
+max_ = 5 ;
+agmemin_ = 35 ;
+agemax_ = 49 ;
+first_ = 5 ;
+last_ = 5 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = 18 ;
+sumsquare_ = 22 ;
+sumcube_ = 30 ;
+sumfourth_ = 46 ;
+min_ = 0 ;
+max_ = 2 ;
+agmemin_ = 49 ;
+agemax_ = 45 ;
+first_ = 0 ;
+last_ = 2 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = -21 ;
+sumsquare_ = 39 ;
+sumcube_ = -51 ;
+sumfourth_ = 99 ;
+min_ = 0 ;
+max_ = 3 ;
+agmemin_ = 45 ;
+agemax_ = 44 ;
+first_ = 2 ;
+last_ = 0 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/train_stats.psave	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/train_stats.psave	2008-03-10 21:43:17 UTC (rev 8655)
@@ -81,12 +81,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = -5.1647413016931214 ;
-max_ = -5.1647413016931214 ;
+min_ = -5.16474130169312051 ;
+max_ = -5.16474130169312051 ;
 agmemin_ = 0 ;
 agemax_ = 0 ;
-first_ = -5.1647413016931214 ;
-last_ = -5.1647413016931214 ;
+first_ = -5.16474130169312051 ;
+last_ = -5.16474130169312051 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -102,12 +102,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 367.38802943868177 ;
-max_ = 367.38802943868177 ;
+min_ = 367.388029438681713 ;
+max_ = 367.388029438681713 ;
 agmemin_ = 0 ;
 agemax_ = 0 ;
-first_ = 367.38802943868177 ;
-last_ = 367.38802943868177 ;
+first_ = 367.388029438681713 ;
+last_ = 367.388029438681713 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -144,12 +144,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = -733.776058877363539 ;
-max_ = -733.776058877363539 ;
+min_ = -733.776058877363425 ;
+max_ = -733.776058877363425 ;
 agmemin_ = 0 ;
 agemax_ = 0 ;
-first_ = -733.776058877363539 ;
-last_ = -733.776058877363539 ;
+first_ = -733.776058877363425 ;
+last_ = -733.776058877363425 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt	2008-03-10 21:43:17 UTC (rev 8655)
@@ -1,2 +1,2 @@
-__REVISION__ = "PL8480"
+__REVISION__ = "PL8649"
 data                                          = PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/test_cost_names.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/test_cost_names.txt	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/test_cost_names.txt	2008-03-10 21:43:17 UTC (rev 8655)
@@ -3,3 +3,9 @@
 base_reward_l2
 base_reward_l1
 class_error
+SPLIT_VAR_0
+SPLIT_VAR_1
+SPLIT_VAR_2
+SPLIT_VAR_3
+SPLIT_VAR_4
+SPLIT_VAR_5

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave	2008-03-10 21:43:17 UTC (rev 8655)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+fieldnames = 8 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" "SPLIT_VAR_class" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 5 [ StatsCollector(
+stats = 8 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -112,6 +112,69 @@
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave	2008-03-10 21:43:17 UTC (rev 8655)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+fieldnames = 8 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" "SPLIT_VAR_class" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 5 [ StatsCollector(
+stats = 8 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -112,6 +112,69 @@
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 6830 ;
+agemax_ = 6830 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 6830 ;
+agemax_ = 6830 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave	2008-03-10 21:43:17 UTC (rev 8655)
@@ -7,7 +7,7 @@
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 5 [ StatsCollector(
+stats = 8 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -112,6 +112,69 @@
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave	2008-03-10 21:43:17 UTC (rev 8655)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+fieldnames = 8 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" "SPLIT_VAR_class" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 5 [ StatsCollector(
+stats = 8 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -112,6 +112,69 @@
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave	2008-03-10 21:43:17 UTC (rev 8655)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+fieldnames = 8 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" "SPLIT_VAR_class" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 5 [ StatsCollector(
+stats = 8 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -112,6 +112,69 @@
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 6830 ;
+agemax_ = 6830 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave	2008-03-10 21:43:17 UTC (rev 8655)
@@ -7,7 +7,7 @@
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 5 [ StatsCollector(
+stats = 8 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -112,6 +112,69 @@
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave	2008-03-10 21:43:17 UTC (rev 8655)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+fieldnames = 8 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" "SPLIT_VAR_class" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 5 [ StatsCollector(
+stats = 8 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -98,10 +98,10 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = -145 ;
-sumsquare_ = 145 ;
-sumcube_ = -145 ;
-sumfourth_ = 145 ;
+sum_ = -144 ;
+sumsquare_ = 144 ;
+sumcube_ = -144 ;
+sumfourth_ = 144 ;
 min_ = 0 ;
 max_ = 1 ;
 agmemin_ = 198 ;
@@ -112,6 +112,69 @@
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 51 ;
+sumsquare_ = 51 ;
+sumcube_ = 51 ;
+sumfourth_ = 51 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 199 ;
+agemax_ = 198 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave	2008-03-10 21:43:17 UTC (rev 8655)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+fieldnames = 8 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" "SPLIT_VAR_class" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 5 [ StatsCollector(
+stats = 8 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -98,20 +98,83 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = 1507 ;
-sumsquare_ = 1507 ;
-sumcube_ = 1507 ;
-sumfourth_ = 1507 ;
+sum_ = 2109 ;
+sumsquare_ = 3169 ;
+sumcube_ = 5289 ;
+sumfourth_ = 9529 ;
 min_ = 0 ;
-max_ = 1 ;
+max_ = 2 ;
 agmemin_ = 6830 ;
 agemax_ = 6775 ;
 first_ = 0 ;
 last_ = 0 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = 1 ;
 binary_ = 1 ;
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -4466 ;
+sumsquare_ = 4466 ;
+sumcube_ = -4466 ;
+sumfourth_ = 4466 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 6775 ;
+agemax_ = 6830 ;
+first_ = 2 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 6830 ;
+agemax_ = 6830 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave	2008-03-10 21:43:17 UTC (rev 8655)
@@ -7,7 +7,7 @@
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 5 [ StatsCollector(
+stats = 8 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -98,10 +98,10 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = -145 ;
-sumsquare_ = 145 ;
-sumcube_ = -145 ;
-sumfourth_ = 145 ;
+sum_ = -144 ;
+sumsquare_ = 144 ;
+sumcube_ = -144 ;
+sumfourth_ = 144 ;
 min_ = 0 ;
 max_ = 1 ;
 agmemin_ = 198 ;
@@ -112,6 +112,69 @@
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 51 ;
+sumsquare_ = 51 ;
+sumcube_ = 51 ;
+sumfourth_ = 51 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 199 ;
+agemax_ = 198 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test1_stats.psave	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test1_stats.psave	2008-03-10 21:43:17 UTC (rev 8655)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+fieldnames = 8 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" "SPLIT_VAR_class" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 5 [ StatsCollector(
+stats = 8 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -98,10 +98,10 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = -146 ;
-sumsquare_ = 146 ;
-sumcube_ = -146 ;
-sumfourth_ = 146 ;
+sum_ = -144 ;
+sumsquare_ = 144 ;
+sumcube_ = -144 ;
+sumfourth_ = 144 ;
 min_ = 0 ;
 max_ = 1 ;
 agmemin_ = 198 ;
@@ -112,6 +112,69 @@
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -2 ;
+sumsquare_ = 2 ;
+sumcube_ = -2 ;
+sumfourth_ = 2 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 140 ;
+agemax_ = 199 ;
+first_ = 2 ;
+last_ = 2 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test2_stats.psave	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test2_stats.psave	2008-03-10 21:43:17 UTC (rev 8655)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+fieldnames = 8 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" "SPLIT_VAR_class" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 5 [ StatsCollector(
+stats = 8 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -98,20 +98,83 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = 1675 ;
-sumsquare_ = 1675 ;
-sumcube_ = 1675 ;
-sumfourth_ = 1675 ;
+sum_ = 2445 ;
+sumsquare_ = 3841 ;
+sumcube_ = 6633 ;
+sumfourth_ = 12217 ;
 min_ = 0 ;
-max_ = 1 ;
+max_ = 2 ;
 agmemin_ = 6830 ;
 agemax_ = 6775 ;
 first_ = 0 ;
+last_ = 2 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 6830 ;
+first_ = 1 ;
 last_ = 1 ;
 binary_ = 1 ;
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -770 ;
+sumsquare_ = 770 ;
+sumcube_ = -770 ;
+sumfourth_ = 770 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 6775 ;
+agemax_ = 6830 ;
+first_ = 2 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 6830 ;
+agemax_ = 6830 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/train_stats.psave	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/train_stats.psave	2008-03-10 21:43:17 UTC (rev 8655)
@@ -7,7 +7,7 @@
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 5 [ StatsCollector(
+stats = 8 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -98,10 +98,10 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = -146 ;
-sumsquare_ = 146 ;
-sumcube_ = -146 ;
-sumfourth_ = 146 ;
+sum_ = -144 ;
+sumsquare_ = 144 ;
+sumcube_ = -144 ;
+sumfourth_ = 144 ;
 min_ = 0 ;
 max_ = 1 ;
 agmemin_ = 198 ;
@@ -112,6 +112,69 @@
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -2 ;
+sumsquare_ = 2 ;
+sumcube_ = -2 ;
+sumfourth_ = 2 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 140 ;
+agemax_ = 199 ;
+first_ = 2 ;
+last_ = 2 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames	2008-03-10 21:43:17 UTC (rev 8655)
@@ -3,3 +3,6 @@
 base_reward_l2	0
 base_reward_l1	0
 class_error	0
+SPLIT_VAR_x1	0
+SPLIT_VAR_y2	0
+SPLIT_VAR_class	0

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_stats.psave	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_stats.psave	2008-03-10 21:43:17 UTC (rev 8655)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+fieldnames = 8 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" "SPLIT_VAR_class" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 5 [ StatsCollector(
+stats = 8 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -112,6 +112,69 @@
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames	2008-03-10 21:43:17 UTC (rev 8655)
@@ -3,3 +3,6 @@
 base_reward_l2	0
 base_reward_l1	0
 class_error	0
+SPLIT_VAR_x1	0
+SPLIT_VAR_y2	0
+SPLIT_VAR_class	0

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_stats.psave	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_stats.psave	2008-03-10 21:43:17 UTC (rev 8655)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+fieldnames = 8 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" "SPLIT_VAR_class" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 5 [ StatsCollector(
+stats = 8 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -112,6 +112,69 @@
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 6830 ;
+agemax_ = 6830 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt	2008-03-10 21:43:17 UTC (rev 8655)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL8480"
+__REVISION__ = "PL8649"
 datatest                                      = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat
 datatrain                                     = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/test_cost_names.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/test_cost_names.txt	2008-03-10 21:38:19 UTC (rev 8654)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/test_cost_names.txt	2008-03-10 21:43:17 UTC (rev 8655)
@@ -3,3 +3,6 @@
 base_reward_l2
 base_reward_l1
 class_error
+SPLIT_VAR_x1
+SPLIT_VAR_y2
+SPLIT_VAR_class



From nouiz at mail.berlios.de  Mon Mar 10 22:47:03 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 10 Mar 2008 22:47:03 +0100
Subject: [Plearn-commits] r8656 - trunk/plearn_learners/meta
Message-ID: <200803102147.m2ALl3IV031441@sheep.berlios.de>

Author: nouiz
Date: 2008-03-10 22:47:03 +0100 (Mon, 10 Mar 2008)
New Revision: 8656

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
changed the behavior of AdaBoost::forward_sub_learner_test_costs to add the weighted sum of the weaklearners cost.


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2008-03-10 21:43:17 UTC (rev 8655)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2008-03-10 21:47:03 UTC (rev 8656)
@@ -747,7 +747,9 @@
 void AdaBoost::computeCostsFromOutputs(const Vec& input, const Vec& output, 
                                        const Vec& target, Vec& costs) const
 {
+    PLASSERT(costs.size()==nTestCosts());
     costs.resize(5);
+    costs.clear();
 
     // First cost is negative log-likelihood...  output[0] is the likelihood
     // of the first class
@@ -764,26 +766,46 @@
                  "either 0 or 1; current target=%f", target[0]);
     costs[1] = exp(-1.0*sum_voting_weights*(2*output[0]-1)*(2*target[0]-1));
     costs[2] = costs[0];
-    costs[3] = train_stats->getStat("E[avg_weight_class_0]");
-    costs[4] = train_stats->getStat("E[avg_weight_class_1]");
+    if(train_stats){
+        costs[3] = train_stats->getStat("E[avg_weight_class_0]");
+        costs[4] = train_stats->getStat("E[avg_weight_class_1]");
+    }
+    else
+        costs[3]=costs[4]=MISSING_VALUE;
+
+    PP<VMatrix> the_train_set = train_set;
+    if(!train_set)
+        the_train_set=sorted_train_set;
+    PLASSERT(the_train_set);
+
     if(forward_sub_learner_test_costs){
-        //TODO: is this the good beavior to have?
-        //We can't reuse the output from parameter as it is not the same
-        //as the one from the sub learner
-        Vec tmp(weak_learner_template->nTestCosts());
-        weak_learners.last()->computeCostsOnly(input,target,tmp);
-        costs.append(tmp);
+        Vec weighted_costs(weak_learners[0]->nTestCosts());
+        Vec sum_weighted_costs(weak_learners[0]->nTestCosts());
+        sum_weighted_costs.clear();
+        for(int i=0;i<weak_learners.size();i++){
+            weak_learners[i]->computeCostsOnly(input, target, weighted_costs);
+            weighted_costs*=voting_weights[i];
+            sum_weighted_costs+=weighted_costs;
+        }
+        costs.append(sum_weighted_costs);
     }
+
+    PLASSERT(costs.size()==nTestCosts());
 }
 
 TVec<string> AdaBoost::getTestCostNames() const
 {
     TVec<string> costs=getTrainCostNames();
 
+    PP<VMatrix> the_train_set = train_set;
+    if(!train_set)
+        the_train_set=sorted_train_set;
+    PLASSERT(the_train_set);
+
     if(forward_sub_learner_test_costs){
-        TVec<string> subcosts=weak_learner_template->getTestCostNames();
+        TVec<string> subcosts=weak_learners[0]->getTestCostNames();
         for(int i=0;i<subcosts.length();i++){
-            subcosts[i]="weak_learner."+subcosts[i];
+            subcosts[i]="weighted_weak_learner."+subcosts[i];
         }
         costs.append(subcosts);
     }



From nouiz at mail.berlios.de  Mon Mar 10 22:48:58 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 10 Mar 2008 22:48:58 +0100
Subject: [Plearn-commits] r8657 - trunk/plearn_learners/meta
Message-ID: <200803102148.m2ALmwQG031579@sheep.berlios.de>

Author: nouiz
Date: 2008-03-10 22:48:57 +0100 (Mon, 10 Mar 2008)
New Revision: 8657

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
   trunk/plearn_learners/meta/MultiClassAdaBoost.h
Log:
Added an option MultiClassAdaBoost::forward_sub_learner_test_costs that will return the sum of the sublearner(AdaBoost) costs.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-03-10 21:47:03 UTC (rev 8656)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-03-10 21:48:57 UTC (rev 8657)
@@ -48,7 +48,8 @@
     "MULTI-LINE \nHELP");
 
 MultiClassAdaBoost::MultiClassAdaBoost():
-    nb_stage_to_use(-1)
+    nb_stage_to_use(-1),
+    forward_sub_learner_test_costs(false)
 /* ### Initialize all fields to their default value here */
 {
     // ...
@@ -92,6 +93,10 @@
     declareOption(ol, "learner2", &MultiClassAdaBoost::learner2,
                   OptionBase::buildoption,
                   "The sub learner to use.");
+    declareOption(ol, "forward_sub_learner_test_costs", 
+                  &MultiClassAdaBoost::forward_sub_learner_test_costs,
+                  OptionBase::buildoption,
+                  "Did we add the learner1 and learner2 costs to our costs.\n");
 
 }
 
@@ -228,8 +233,8 @@
 void MultiClassAdaBoost::computeCostsFromOutputs(const Vec& input, const Vec& output,
                                            const Vec& target, Vec& costs) const
 {
-// Compute the costs from *already* computed output.
-// ...
+    PLASSERT(costs.size()==nTestCosts());
+
     int out = int(round(output[0]));
     int pred = int(round(target[0]));
     costs[0]=int(out != pred);//class_error
@@ -243,11 +248,31 @@
     else
         costs[3]=0;
 
-    costs[4]=0;
-    costs[5]=0;
-    costs[6]=0;
+    costs[4]=costs[5]=costs[6]=0;
     costs[out+4]=1;
+    PLASSERT(nTestCosts()==costs.size());
+    if(forward_sub_learner_test_costs){
+        costs.resize(7);
+        Vec subcosts1(learner1.nTestCosts());
+        Vec subcosts2(learner1.nTestCosts());
+        Vec target1(0,1), target2(0,1);
+        if(fast_is_equal(target[0],0.)){
+            target1.append(0);
+            target2.append(0);
+        }else if(fast_is_equal(target[0],1.)){
+            target1.append(1);
+            target2.append(0);
+        }else if(fast_is_equal(target[0],2.)){
+            target1.append(1);
+            target2.append(1);
+        }
+        learner1.computeCostsOnly(input,target1,subcosts1);
+        learner2.computeCostsOnly(input,target2,subcosts2);
+        subcosts1+=subcosts2;
+        costs.append(subcosts1);
+    }
 
+    PLASSERT(costs.size()==nTestCosts());
 }
 
 TVec<string> MultiClassAdaBoost::getTestCostNames() const
@@ -264,6 +289,13 @@
     names.append("class0");
     names.append("class1");
     names.append("class2");
+    if(forward_sub_learner_test_costs){
+        TVec<string> subcosts=learner1.getTestCostNames();
+        for(int i=0;i<subcosts.length();i++){
+            subcosts[i]="sum_sublearner."+subcosts[i];
+        }
+        names.append(subcosts);
+    }
     return names;
 }
 

Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.h	2008-03-10 21:47:03 UTC (rev 8656)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.h	2008-03-10 21:48:57 UTC (rev 8657)
@@ -68,11 +68,13 @@
     //! The number of stage that will be used
     int nb_stage_to_use;
 
+    //! Did we add the learner1 and learner2 costs to our costs
+    bool forward_sub_learner_test_costs;
+
     //! The learner1 and learner2 must be trained!
     AdaBoost learner1;
     AdaBoost learner2;
 
-
 public:
     //#####  Public Member Functions  #########################################
 



From nouiz at mail.berlios.de  Tue Mar 11 15:21:37 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 11 Mar 2008 15:21:37 +0100
Subject: [Plearn-commits] r8658 - trunk/commands
Message-ID: <200803111421.m2BELbWT032428@sheep.berlios.de>

Author: nouiz
Date: 2008-03-11 15:21:37 +0100 (Tue, 11 Mar 2008)
New Revision: 8658

Modified:
   trunk/commands/
   trunk/commands/plearn_desjardins.cc
Log:
-added file to ignore
-added include



Property changes on: trunk/commands
___________________________________________________________________
Name: svn:ignore
   - OBJS
plearn
plearn_curses
plearn_full
plearn_lapack
plearn_light
plearn_noblas
plearn_python
plearn_tests
plearn.lnk
plearn_tests.lnk

   + OBJS
plearn
plearn_curses
plearn_full
plearn_lapack
plearn_light
plearn_noblas
plearn_python
plearn_tests
plearn.lnk
plearn_tests.lnk
plearn_exp-*
plearn_desjardins-*


Modified: trunk/commands/plearn_desjardins.cc
===================================================================
--- trunk/commands/plearn_desjardins.cc	2008-03-10 21:48:57 UTC (rev 8657)
+++ trunk/commands/plearn_desjardins.cc	2008-03-11 14:21:37 UTC (rev 8658)
@@ -71,6 +71,7 @@
  * Splitter *
  ************/
 #include <plearn/vmat/FractionSplitter.h>
+#include <plearn/vmat/ExplicitSplitter.h>
 
 /***********
  * VMatrix *



From nouiz at mail.berlios.de  Tue Mar 11 16:19:27 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 11 Mar 2008 16:19:27 +0100
Subject: [Plearn-commits] r8659 - trunk/plearn_learners/regressors
Message-ID: <200803111519.m2BFJRYd018404@sheep.berlios.de>

Author: nouiz
Date: 2008-03-11 16:19:26 +0100 (Tue, 11 Mar 2008)
New Revision: 8659

Modified:
   trunk/plearn_learners/regressors/RegressionTree.cc
Log:
implemented RegressionTree::computeOutputAndCosts for speedup


Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2008-03-11 14:21:37 UTC (rev 8658)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2008-03-11 15:19:26 UTC (rev 8659)
@@ -372,25 +372,47 @@
     outputv[0] = closest_value;
 }
 
-void RegressionTree::computeCostsFromOutputs(const Vec& inputv, const Vec& outputv, const Vec& targetv, Vec& costsv) const
+void RegressionTree::computeOutputAndCosts(const Vec& inputv,
+                                           const Vec& targetv,
+                                           Vec& outputv, Vec& costsv) const
 {
     PLASSERT(costsv.size()==nTestCosts());
+    TVec<PP<RegressionTreeNode> > nodes;
+    root->computeOutputAndNodes(inputv, outputv, &nodes);
+    if (multiclass_outputs.length() <= 0) return;
+    real closest_value=multiclass_outputs[0];
+    real margin_to_closest_value=abs(outputv[0] - multiclass_outputs[0]);
+    for (int value_ind = 1; value_ind < multiclass_outputs.length(); value_ind++)
+    {
+        real v=abs(outputv[0] - multiclass_outputs[value_ind]);
+        if (v < margin_to_closest_value)
+        {
+            closest_value = multiclass_outputs[value_ind];
+            margin_to_closest_value = v;
+        }
+    }
+    outputv[0] = closest_value;
     costsv.clear();
     costsv[0] = pow((outputv[0] - targetv[0]), 2);
     costsv[1] = outputv[1];
     costsv[2] = 1.0 - (l2_loss_function_factor * costsv[0]);
     costsv[3] = 1.0 - (l1_loss_function_factor * abs(outputv[0] - targetv[0]));
     costsv[4] = !fast_is_equal(targetv[0],outputv[0]);
-    Vec tmp(outputv.size());
-    TVec<PP<RegressionTreeNode> > nodes;
-    root->computeOutputAndNodes(inputv, tmp, &nodes);
-    PLASSERT(outputv==tmp);
     for(int i=0;i<nodes.length();i++)
-    {
         costsv[5+nodes[i]->getSplitCol()]++;
-    }
 }
 
+void RegressionTree::computeCostsFromOutputs(const Vec& input,
+                                             const Vec& output, 
+                                             const Vec& target,
+                                             Vec& costs) const
+{
+    Vec tmp(output.size());
+    computeOutputAndCosts(input, target, tmp, costs); 
+    PLASSERT(output==tmp);
+}
+
+
 } // end of namespace PLearn
 
 



From nouiz at mail.berlios.de  Tue Mar 11 16:21:59 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 11 Mar 2008 16:21:59 +0100
Subject: [Plearn-commits] r8660 - trunk/plearn_learners/regressors
Message-ID: <200803111521.m2BFLx4q018599@sheep.berlios.de>

Author: nouiz
Date: 2008-03-11 16:21:58 +0100 (Tue, 11 Mar 2008)
New Revision: 8660

Modified:
   trunk/plearn_learners/regressors/RegressionTree.h
Log:
forgetted in last commit


Modified: trunk/plearn_learners/regressors/RegressionTree.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.h	2008-03-11 15:19:26 UTC (rev 8659)
+++ trunk/plearn_learners/regressors/RegressionTree.h	2008-03-11 15:21:58 UTC (rev 8660)
@@ -106,6 +106,9 @@
     virtual TVec<string> getTrainCostNames() const;
     virtual TVec<string> getTestCostNames() const;
     virtual void         computeOutput(const Vec& input, Vec& output) const;
+    virtual void         computeOutputAndCosts(const Vec& input,
+                                               const Vec& target,
+                                               Vec& output, Vec& costs) const;
     virtual void         computeOutputAndNodes(const Vec& input, Vec& output,
                                                TVec<PP<RegressionTreeNode> >* nodes=0) const;
     virtual void         computeCostsFromOutputs(const Vec& input, const Vec& output, const Vec& target, Vec& costs) const;



From nouiz at mail.berlios.de  Tue Mar 11 16:27:43 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 11 Mar 2008 16:27:43 +0100
Subject: [Plearn-commits] r8661 - trunk/plearn_learners/testers
Message-ID: <200803111527.m2BFRhoi019076@sheep.berlios.de>

Author: nouiz
Date: 2008-03-11 16:27:43 +0100 (Tue, 11 Mar 2008)
New Revision: 8661

Modified:
   trunk/plearn_learners/testers/PTester.cc
Log:
The dataset is not always present. so we use the first split


Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2008-03-11 15:21:58 UTC (rev 8660)
+++ trunk/plearn_learners/testers/PTester.cc	2008-03-11 15:27:43 UTC (rev 8661)
@@ -1059,7 +1059,7 @@
             //To work around the fact that RegressionTree need a
             // train_set to generate the train/test costs names
             if(!learner->getTrainingSet())
-                learner->setTrainingSet(splitter->getDataSet(), false);
+                learner->setTrainingSet(splitter->getSplit(0)[0], false);
 
             TVec<string> testcostnames = learner->getTestCostNames();
             TVec<string> traincostnames = learner->getTrainCostNames();



From saintmlx at mail.berlios.de  Tue Mar 11 19:53:06 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 11 Mar 2008 19:53:06 +0100
Subject: [Plearn-commits] r8662 - trunk/plearn/vmat
Message-ID: <200803111853.m2BIr6DI030637@sheep.berlios.de>

Author: saintmlx
Date: 2008-03-11 19:53:06 +0100 (Tue, 11 Mar 2008)
New Revision: 8662

Modified:
   trunk/plearn/vmat/SelectColumnsVMatrix.cc
Log:
- remove check on width, since all cols can be selected



Modified: trunk/plearn/vmat/SelectColumnsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-03-11 15:27:43 UTC (rev 8661)
+++ trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-03-11 18:53:06 UTC (rev 8662)
@@ -265,8 +265,6 @@
         }
         // Copy matrix dimensions
         width_ = indices.length();
-        if(!extend_with_missing)
-            PLCHECK(source->width()>width_);
         length_ = source->length();
 
         //if we don't add new columns and the source columns type are emtpy,



From louradou at mail.berlios.de  Tue Mar 11 21:39:04 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Tue, 11 Mar 2008 21:39:04 +0100
Subject: [Plearn-commits] r8663 - trunk/python_modules/plearn/learners
Message-ID: <200803112039.m2BKd4aj010644@sheep.berlios.de>

Author: louradou
Date: 2008-03-11 21:39:03 +0100 (Tue, 11 Mar 2008)
New Revision: 8663

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:
white space removed, code cleaned, with a new format to output results in a file (the same as PLearn/scripts/makeresults and appendresults)



Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2008-03-11 18:53:06 UTC (rev 8662)
+++ trunk/python_modules/plearn/learners/SVM.py	2008-03-11 20:39:03 UTC (rev 8663)
@@ -1,5 +1,8 @@
 import sys, os, time
-from numarray import *
+try:
+    from numarray import *
+except:
+    from numpy.numarray import *
 from math import *
 from libsvm import *
 
@@ -23,7 +26,7 @@
           #if self.best_parameters != None:
           #   self.add_parameter_to_tried_list(self.getBestValue('C'), self.best_parameters[1:])
           self.error_rate       = 1.
-	  self.should_be_tuned_again = 1
+          self.should_be_tuned_again = 1
 
 
       def should_be_tuned_again():
@@ -79,7 +82,7 @@
           dim = len(samples[0])
           std = mean_std(samples)
           rho = sqrt(dim)*std
-          gamma0 = 1/(2*rho**2)
+          gamma0 = 90./(2*rho**2)
           gamma_base = self.init_gamma(gamma0)
           return SVM_expert.init_parameters( self, gamma_base )
           
@@ -98,22 +101,22 @@
       def should_be_tuned_again( self ):
           best_gamma = self.getBestValue('gamma') 
           tried_gamma = self.tried_parameters
-	  if tried_gamma.has_key(best_gamma):
-	     is_lower = False
-	     is_higher = False
+          if tried_gamma.has_key(best_gamma):
+             is_lower = False
+             is_higher = False
              for gamma in tried_gamma:
-	         if gamma<tried_gamma:
-		    is_lower = True
-	         elif gamma<tried_gamma:
-		    is_higher = True
-	     if is_lower and is_higher:
-	        best_C_for_this_gamma  = self.getBestValue('C') 
-		tried_C_for_this_gamma = self.tried_parameters[best_gamma]
-		return not ( best_C_for_this_gamma <> min(tried_C_for_this_gamma) and best_C_for_this_gamma <> max(tried_C_for_this_gamma) )
-	     else:
-	        return True
-	  else:
-	     return True
+                if gamma<tried_gamma:
+                    is_lower = True
+                elif gamma<tried_gamma:
+                    is_higher = True
+             if is_lower and is_higher:
+                best_C_for_this_gamma  = self.getBestValue('C') 
+                tried_C_for_this_gamma = self.tried_parameters[best_gamma]
+                return not ( best_C_for_this_gamma <> min(tried_C_for_this_gamma) and best_C_for_this_gamma <> max(tried_C_for_this_gamma) )
+             else:
+                return True
+          else:
+            return True
       
 class POLY_expert(SVM_expert):
       def __init__( self ):
@@ -138,16 +141,16 @@
                 return SVM_expert.choose_new_parameters(self, (self.best_parameters[1], self.best_parameters[2])  )
           else:
              return SVM_expert.init_parameters(self, self.init_degree(best_degree) )
-	     
+         
       def should_be_tuned_again( self ):
           best_degree = self.getBestValue('degree')
           tried_degrees = [prms[0] for prms in self.tried_parameters]
           if self.tried_parameters.has_key(best_degree):
              if best_degree <> max(tried_degrees):
-	        best_C_for_this_degree  = self.getBestValue('C') 
-		tried_C_for_this_degree = self.tried_parameters[(best_degree,self.best_parameters[2])]
-		return not ( best_C_for_this_degree <> min(tried_C_for_this_degree) and best_C_for_this_degree <> max(tried_C_for_this_degree) )
-	  return True
+                best_C_for_this_degree  = self.getBestValue('C') 
+                tried_C_for_this_degree = self.tried_parameters[(best_degree,self.best_parameters[2])]
+                return not ( best_C_for_this_degree <> min(tried_C_for_this_degree) and best_C_for_this_degree <> max(tried_C_for_this_degree) )
+          return True
       
 class LINEAR_expert(SVM_expert):
       def __init__( self ):
@@ -162,10 +165,10 @@
 
       def should_be_tuned_again( self ):
           if len(self.tried_parameters[None]) <= 3:
-	     return True
+            return True
           best_C = self.getBestValue('C') 
           tried_C = self.tried_parameters[None]
-	  return not (best_C <> min(tried_C) and best_C <> max(tried_C) )
+          return not (best_C <> min(tried_C) and best_C <> max(tried_C) )
 
 class SVM(object):
 
@@ -173,11 +176,13 @@
                         'valid_error_rate',
                         'best_parameters',
                         'tried_parameters',
-                        'save_filename',
-			'best_model',
-			'nr_fold'
+                        'results_filename',
+                        'preproc_optionnames',
+                        'preproc_optionvalues',
+                        'best_model',
+                        'nr_fold'
                         'result_list',
-			'automatically_decide_when_to_stop_tuning'
+                        'automatically_decide_when_to_stop_tuning'
                         ]
        
       def __init__( self ):
@@ -188,28 +193,52 @@
           self.LINEAR_expert  = LINEAR_expert()
           self.RBF_expert     = RBF_expert()
           self.POLY_expert    = POLY_expert()
+          self.all_experts    = [self.LINEAR_expert,
+                                 self.RBF_expert,
+                                 self.POLY_expert,
+                                ]
           
           self.best_parameters      = None  
           self.best_model           = None
           self.tried_parameters     = {}
           self.result_list          = {}
           
-          self.save_filename        = None
+          self.results_filename        = None
+          self.preproc_optionnames  = []
+          self.preproc_optionvalues = []
           
           # For cross-validation
           self.nr_fold        = 5
 
-	  self.automatically_decide_when_to_stop_tuning = False
+          self.automatically_decide_when_to_stop_tuning = False
 
+      def get_expert( self, kernel_type ):
+          return eval( 'self.'+kernel_type+'_expert' )
+
+      def get_parameters( self, kernel_type, parameters ):
+          prm_dict = {'kernel':kernel_type.lower()}
+
+          for expert in self.all_experts:
+              for prm_name in expert.parameters_names:
+                  prm_dict[prm_name]= None
+          expert = self.get_expert( kernel_type )
+          if len(expert.parameters_names) <> len(parameters):
+             raise IndexError, "in SVM.get_parameters(): "+\
+                   "parameters (length %s) should be of length %s for kernel " % \
+                   ( len(parameters), len(expert.parameters_names), kernel_type )
+
+          for prm_name, prm_value in zip(expert.parameters_names,
+                                         parameters):
+              prm_dict[prm_name]=prm_value
+          return prm_dict
+
+      
       def reset( self ):
-          self.LINEAR_expert.reset()
-          self.RBF_expert.reset()
-          self.POLY_expert.reset()
+          for expert in self.all_experts:
+              expert.reset()
           
           self.tried_parameters = {}
           self.result_list          = {}
-          #if self.best_parameters != None:
-          #   self.add_parameter_to_tried_list(self.best_parameters[0], self.best_parameters[1:])
           self.error_rate       = 1.
           self.valid_error_rate = 1.
 
@@ -224,50 +253,113 @@
              self.result_list[kernel]+= kernel_parameters, error_rate
           else:
              self.result_list[kernel] = kernel_parameters, error_rate
-             
+
+
+      def write_results( self, kernel_type, parameters,
+                        train_class_error,
+                        valid_class_error,
+                        test_class_error = None,
+                        cross_valid = False):
+          if cross_valid:
+                valid_name='crossvalid'
+          else:
+                valid_name='valid'
+
+          # the results_filename will not include the extension ".amat"
+          if self.results_filename[-5:]=='.amat':
+             self.results_filename = self.results_filename[:-5]
+
+          prm_dict = self.get_parameters( kernel_type, parameters )
+          prm_names_list = [ prm_name for prm_name in prm_dict ]
+          prm_names = ' '.join( prm_names_list )
+          prm_values = ' '.join( str(prm_dict[pn]) for pn in prm_names_list )
+          
+          if self.results_filename == None:
+             print "==============================="
+             for pn in prm_names_list:
+                 print "    ",pn,"=",prm_dict[pn]
+             if train_class_error <> None:
+                 print "E[train.E[class_error]]=",train_class_error
+             if valid_class_error <> None:
+                 print "E["+valid_name+".E[class_error]]=",valid_class_error
+             if test_class_error <> None:
+                 print "E[test.E[class_error]]=",test_class_error
+             return
+          
+          if type(self.preproc_optionnames)==str:
+             preproc_optionnames = self.preproc_optionnames
+          elif type(self.preproc_optionnames)==list:
+             preproc_optionnames = ' '.join(self.preproc_optionnames)
+          else:
+             raise TypeError, "preproc_optionnames must be of type str or list"
+          if type(self.preproc_optionvalues)==str:
+             preproc_optionvalues = self.preproc_optionvalues
+          elif type(self.preproc_optionvalues)==list:
+             preproc_optionvalues = ' '.join('%s' % v for v in self.preproc_optionvalues)
+          else:
+             raise TypeError, "preproc_optionvalues must be of type str or list"
+          
+          os.system('makeresults  %s  %s %s %s %s %s;' % \
+                                ( self.results_filename,
+                                  preproc_optionnames,
+                                  prm_names,
+                                  "E[train.E[class_error]]",
+                                  "E["+valid_name+".E[class_error]]",
+                                  "E[test.E[class_error]]"
+                                ) \
+                  + 'appendresults %s.amat  %s %s %s %s %s' % \
+                                ( self.results_filename,
+                                  preproc_optionvalues,
+                                  prm_values,
+                                  train_class_error,
+                                  valid_class_error,
+                                  test_class_error
+                                )
+                   )
+
       def train_and_test(self, samples_target_list):
           check_samples_target_list(samples_target_list)
 
-          best_expert = eval( 'self.'+self.best_parameters[0]+'_expert' )
+          best_expert = self.get_expert( self.best_parameters[0] )
           best_parameters = best_expert.best_parameters
           param = best_expert.get_svm_parameter( best_parameters )
           if len(samples_target_list) == 1: # cross-validation
-             costs = do_cross_validation(samples_target_list[0][0], samples_target_list[0][1], param, self.nr_fold)
+             #costs = do_cross_validation(samples_target_list[0][0], samples_target_list[0][1], param, self.nr_fold)
+             train_problem = svm_problem( samples_target_list[0][1] , samples_target_list[0][0] )
+             self.best_model = svm_model(train_problem, param)
+             costs = test_model(self.best_model, samples_target_list[0][0], samples_target_list[0][1])
           else:
-             costs = do_simple_validation(samples_target_list[0][0] , samples_target_list[0][1] , samples_target_list[1][0] , samples_target_list[1][1], param)
+             #costs = do_simple_validation(samples_target_list[0][0] , samples_target_list[0][1] , samples_target_list[1][0] , samples_target_list[1][1], param)                     
+             train_problem = svm_problem( samples_target_list[0][1] , samples_target_list[0][0] )
+             self.best_model = svm_model(train_problem, param)
+             costs = test_model(self.best_model, samples_target_list[1][0], samples_target_list[1][1])
           return costs
 
       def test(self, samples_target_list):
           check_samples_target_list([samples_target_list])
-          if self.save_filename != None:
-                        try:
-                           FID=open(self.save_filename,'a')
-                           #FID.write('------------\nTry with '+kernel_type+' kernel :( parameters : '+str(parameters)+' )\n')
-                           FID.write('\n'+'='*10+'\n ==> Test Error rate = '+str(test_error_rate)+'\n'+'='*10+'\n')
-                           FID.close()
-                        except:
-                           print "COULD not write in save_filename"
-          return test_model(self.best_model, [[x_i for x_i in x] for x in samples_target_list[0]], [float(l) for l in samples_target_list[1]])['error_rate']
+          test_class_error=test_model(self.best_model, [[x_i for x_i in x] for x in samples_target_list[0]], [float(l) for l in samples_target_list[1]])['error_rate']
+          self.write_results( best_parameters[0], best_parameters[1:],
+                         None,
+                         self.valid_error_rate,
+                         test_class_error)
+          return test_class_error
 
 
       def train_and_tune(self, kernel_type, samples_target_list):
+          kernel_type=kernel_type.upper()
           check_samples_target_list(samples_target_list)
+          cross_valid=False
           if len(samples_target_list) == 1:
-             print "\nCross-validation...\n"
-             train_error_name=str(self.nr_fold)+"-fold Cross-Valid Error Rate"
-             test_error_name=train_error_name
+             print str(self.nr_fold)+"-fold Cross-Validation"
+             cross_valid=True
           elif len(samples_target_list) == 2:
              print "\nSimple validation...\n"
-             train_error_name="Valid Error Rate"
-             test_error_name=train_error_name
           elif len(samples_target_list) == 3:
              print "\nValidation + test...\n"
-             train_error_name="Valid Error Rate"
-             test_error_name="Test Error Rate"
           else:
              raise TypeError, "last argument of train_and_tune() should be a list, with length between 1 and 3"
            
-          expert = eval( 'self.'+kernel_type+'_expert' )
+          expert = self.get_expert( kernel_type )
           
           if len(expert.tried_parameters) == 0:
              recompute_best = True
@@ -300,20 +392,14 @@
                   if error_rate < best_error_rate:
                      best_parameters = parameters
                      best_error_rate = error_rate
-		     if len(samples_target_list) <> 1: # in case of cross-validation, we will compute the model later
+                     if len(samples_target_list) <> 1: # in case of cross-validation, we will compute the model later
                         best_model   = model
 
-                  if self.save_filename != None:
-                     try:
-                        FID=open(self.save_filename,'a')
-                        FID.write('------------\nTry with '+kernel_type+' kernel :( parameters : '+str(parameters)+' )\n')
-                        FID.write(' --> Error rate = '+str(error_rate)+'\n')
-                        FID.close()
-                     except:
-                        print "COULD not write in save_filename"
-                  print '------------\nTry with '+kernel_type+' kernel :( parameters : '+str(parameters)
-                  print ' --> '+train_error_name+' = '+str(error_rate)+'\n'
-                  print '...'
+                  self.write_results( kernel_type, parameters,
+                                 None,
+                                 error_rate,
+                                 None,
+                                 cross_valid )
 
           if best_error_rate < expert.error_rate:
              expert.best_parameters = best_parameters
@@ -328,32 +414,15 @@
                    best_model = svm_model(train_problem, param)
                 self.best_model = best_model
                 if len(samples_target_list) == 3: # train-valid-test
-                   self.error_rate = test_model(self.best_model, samples_target_list[2][0], samples_target_list[2][1])['error_rate']
+                   self.error_rate = self.test( [samples_target_list[2]] )
                 else:
                    self.error_rate = self.valid_error_rate
           
-	  if self.automatically_decide_when_to_stop_tuning:
-	     if expert.should_be_tuned_again():
-	        self.train_and_tune(kernel_type, samples_target_list)
-	  
-          if self.save_filename != None:
-             try:
-                  FID=open(self.save_filename,'a')
-                  FID.write('==============================================\n')
-                  if len(samples_target_list) == 3: # train-valid-test
-                     FID.write(' --> Best '+train_error_name+' = '+str(self.valid_error_rate)+'\n')
-                  FID.write(' --> '+test_error_name+' = '+str(self.error_rate)+'\n')
-                  FID.write('     for prms: '+str(self.best_parameters)+'\n')
-                  FID.close()
-             except:
-                  print "COULD not write in save_filename"
+          if self.automatically_decide_when_to_stop_tuning:
+             if expert.should_be_tuned_again():
+                self.train_and_tune(kernel_type, samples_target_list)
 
-          print '=============================================='
-          print ' --> Best error rate = '+str(self.error_rate)
-          print '     for prms: '+str(self.best_parameters)
-
-
-	  return self.error_rate
+          return self.error_rate
           
 
 ##
@@ -392,20 +461,19 @@
         test_samples = samples_subsets[i]
         test_targets = targets_subsets[i]
         train_targets=[]
-	if arrayType:
+        if arrayType:
            train_samples=[]
-	else:
+        else:
            train_samples=[]
         for j in range(0,i)+range(i+1,nr_fold):
             train_targets += targets_subsets[j]
-	    if arrayType:
-	       L=len(train_samples)
-               train_samples=resize(samples,[L+len(samples_subsets[j]),len(samples_subsets[j][0])])
-	       train_samples[L:,:]=samples_subsets[j]
-	    else:
-               train_samples += samples_subsets[j]	       
+            if arrayType:
+                L=len(train_samples)
+                train_samples=resize(samples,[L+len(samples_subsets[j]),len(samples_subsets[j][0])])
+                train_samples[L:,:]=samples_subsets[j]
+            else:
+                train_samples += samples_subsets[j]           
         cum_error_rate += do_simple_validation(train_samples, train_targets, test_samples, test_targets, param)['error_rate']
-#        cum_error_rate += do_simple_validation(samples, targets, test_samples, test_targets, param)['error_rate']
     av_error_rate = cum_error_rate*1.0 / nr_fold
     print av_error_rate
     return {'error_rate':av_error_rate}
@@ -466,7 +534,7 @@
                           geom_mean([sorted_table[index+1],best_value]) ]
     return proposed_table
 
-def normalize(data,mean,std):
+def normalize_data(data,mean,std):
     if mean == None:
        mean=[]
        for i in range(len(data[0])):
@@ -552,7 +620,7 @@
 #<<<#
 #>>># To save the results (progressively) in a ASCII file
 
-    my_svm.save_filename = 'my_svm_results.txt'
+    my_svm.results_filename = 'my_svm_results.txt'
    
 #<<<#
 #>>># Pre-processing your data : it is better to normalize...
@@ -560,12 +628,12 @@
     # Get the mean and standard deviation on the training set
     # and normalize the training set (Mahalanobis)
     #
-    mean, std = normalize(train_samples, None, None)
+    mean, std = normalize_data(train_samples, None, None)
     #
     # DO NOT FORGET to apply the same normalization to other datasets
     #
-    normalize(valid_samples, mean, std)
-    normalize(test_samples, mean, std)
+    normalize_data(valid_samples, mean, std)
+    normalize_data(test_samples, mean, std)
     
 #<<<#
 #>>># Defining train / valid data



From tihocan at mail.berlios.de  Tue Mar 11 21:48:53 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 11 Mar 2008 21:48:53 +0100
Subject: [Plearn-commits] r8664 - trunk/plearn/vmat
Message-ID: <200803112048.m2BKmrQa011134@sheep.berlios.de>

Author: tihocan
Date: 2008-03-11 21:48:52 +0100 (Tue, 11 Mar 2008)
New Revision: 8664

Modified:
   trunk/plearn/vmat/ProcessingVMatrix.cc
   trunk/plearn/vmat/VMatrix.cc
Log:
Fixed a few situations where the sum of sizes could be different from the width

Modified: trunk/plearn/vmat/ProcessingVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ProcessingVMatrix.cc	2008-03-11 20:39:03 UTC (rev 8663)
+++ trunk/plearn/vmat/ProcessingVMatrix.cc	2008-03-11 20:48:52 UTC (rev 8664)
@@ -151,12 +151,14 @@
         if(inputsize_<0)
             inputsize_ = nfields;
         if(targetsize_<0)
-            targetsize_ = 0;            
+            targetsize_ = width_ - inputsize_ - max(0, weightsize_)
+                                              - max(0, extrasize_);
         if(weightsize_<0)
-            weightsize_ = 0;
+            weightsize_ = width_ - inputsize_ - targetsize_
+                                              - max(0, extrasize_);
         if(extrasize_<0)
-            extrasize_ = 0;            
-        
+            extrasize_ = width_ - inputsize_ - targetsize_ - weightsize_;
+
         PLCHECK_MSG(
                 width_ ==  inputsize_ + targetsize_ + weightsize_ + extrasize_,
                 "Width does not match sizes!");

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-03-11 20:39:03 UTC (rev 8663)
+++ trunk/plearn/vmat/VMatrix.cc	2008-03-11 20:48:52 UTC (rev 8664)
@@ -1200,15 +1200,27 @@
     if(weightsize_<0) {
         int ws = vm->weightsize();
         if (ws + current_w <= width_) {
-            weightsize_ = ws;
-            current_w += ws;
+            // We must also ensure the total sum of sizes (if available)
+            // will match the width. Otherwise we may end up with sizes
+            // conflicting with the width.
+            if (inputsize_ < 0 || targetsize_ < 0 || extrasize_ < 0 ||
+                inputsize_ + targetsize_ + extrasize_ + ws == width_)
+            {
+                weightsize_ = ws;
+                current_w += ws;
+            }
         }
     }
     if(extrasize_<=0) {
         int es = vm->extrasize();
         if (es + current_w <= width_) {
-            extrasize_ = es;
-            current_w += es;
+            // Same as above.
+            if (inputsize_ < 0 || targetsize_ < 0 || weightsize_ < 0 ||
+                inputsize_ + targetsize_ + weightsize_ + es == width_)
+            {
+                extrasize_ = es;
+                current_w += es;
+            }
         }
     }
 



From tihocan at mail.berlios.de  Wed Mar 12 16:29:06 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 12 Mar 2008 16:29:06 +0100
Subject: [Plearn-commits] r8665 - trunk/plearn_learners/testers
Message-ID: <200803121529.m2CFT63L020123@sheep.berlios.de>

Author: tihocan
Date: 2008-03-12 16:29:06 +0100 (Wed, 12 Mar 2008)
New Revision: 8665

Modified:
   trunk/plearn_learners/testers/PTester.cc
   trunk/plearn_learners/testers/PTester.h
Log:
- Removed deprecated 'oldperform' method
- Fixed problem introduced in r8654 caused by extra calls to setTrainingSet, which are not really needed: now the cost names are saved during the first split once the training set has been provided to the learner. This fixes test PL_DBN_SimpleRBM.


Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2008-03-11 20:48:52 UTC (rev 8664)
+++ trunk/plearn_learners/testers/PTester.cc	2008-03-12 15:29:06 UTC (rev 8665)
@@ -80,8 +80,12 @@
     return res;
 }
 
-PTester::PTester()
-    :  provide_learner_expdir(false),
+/////////////
+// PTester //
+/////////////
+PTester::PTester():
+       need_to_save_test_names(false),
+       provide_learner_expdir(false),
        report_stats(true),
        save_data_sets(false),
        save_initial_learners(false),
@@ -437,347 +441,9 @@
     expdir = the_expdir / "";
 }
 
-////////////////
-// oldperform //
-////////////////
-// DEPRECATED -- USE PTester::perform
-Vec PTester::oldperform(bool call_forget)
-{
-    if (!learner)
-        PLERROR("No learner specified for PTester.");
-    if (!splitter)
-        PLERROR("No splitter specified for PTester");
-
-    const int nstats = statnames_processed.length();
-    Vec global_result(nstats);
-
-    if (expdir != "")
-    {
-        if (pathexists(expdir) && enforce_clean_expdir)
-            PLERROR("Directory (or file) %s already exists.\n"
-                    "First move it out of the way.", expdir.c_str());
-        // This code looks like it's guaranteeing that we get an expdir that we
-        // create when enforce_clean_expdir is True, but this is not the case. 
-        // Let's say some other process (from a parallel dispatch of many
-        // PLearn processes, say) is trying to create an expdir with the same
-        // name, and that other process succeeds in creating it when we are
-        // here. Then, given that force_mkdir() returns true if the directory
-        // already exists, this is a textbook example of a race condition.
-        if (!force_mkdir(expdir))
-            PLERROR("In PTester Could not create experiment directory %s",expdir.c_str());
-        expdir = expdir.absolute() / "";
-
-        // Save this tester description in the expdir
-        if (save_initial_tester)
-            PLearn::save(expdir / "tester.psave", *this);
-    }
-
-    splitter->setDataSet(dataset);
-
-    const int nsplits = splitter->nsplits();
-    if (nsplits > 1)
-        call_forget = true;
-
-    TVec<string> testcostnames = learner->getTestCostNames();
-    TVec<string> traincostnames = learner->getTrainCostNames();
-
-    const int nsets = splitter->nSetsPerSplit();
-
-    // Stats collectors for individual sets of a split:
-    TVec< PP<VecStatsCollector> > stcol(nsets);
-    for (int setnum = 0; setnum < nsets; setnum++)
-    {
-        if (template_stats_collector)
-        {
-            CopiesMap copies;
-            stcol[setnum] = template_stats_collector->deepCopy(copies);
-        }
-        else
-            stcol[setnum] = new VecStatsCollector();
-
-        if (setnum == 0)
-            stcol[setnum]->setFieldNames(traincostnames);
-        else
-            stcol[setnum]->setFieldNames(testcostnames);
-
-        stcol[setnum]->build();
-        stcol[setnum]->forget();
-    }
-
-    PP<VecStatsCollector> train_stats = stcol[0];
-    learner->setTrainStatsCollector(train_stats);
-
-    // Global stats collector
-    PP<VecStatsCollector> global_statscol;
-    if (global_template_stats_collector)
-    {
-        CopiesMap copies;
-        global_statscol = global_template_stats_collector->deepCopy(copies);
-        global_statscol->build();
-        global_statscol->forget();
-    }
-    else
-        global_statscol = new VecStatsCollector();
-
-    // Stat specs
-    TVec<StatSpec> statspecs(nstats);
-    for(int k = 0; k < nstats; k++)
-    {
-        statspecs[k].init(statnames_processed[k]);
-    }
-        
-    // Hack to accumulate statistics over splits. We store in 'acc' the sets
-    // which need to accumulate statistics.
-    TVec<int> acc;
-    for (int k = 0; k < nstats; k++)
-        if (statspecs[k].extstat == "ACC")
-        {
-            if (statspecs[k].setnum == 0)
-                PLERROR("In PTester::perform - For now, you cannot accumulate train stats");
-            if (acc.find(statspecs[k].setnum) == -1)
-                acc.append(statspecs[k].setnum);
-        }
-        else if (acc.find(statspecs[k].setnum) != -1)
-            PLERROR("In PTester::perform - You can't have stats with and without 'ACC' for set %d", statspecs[k].setnum);
-
-    // The vmat in which to save global result stats specified in statnames
-    VMat global_stats_vm;
-    // The vmat in which to save per split result stats
-    VMat split_stats_vm;
-        
-    if (expdir != "" && report_stats)
-    {
-        if(save_test_names){
-            saveStringInFile(expdir / "train_cost_names.txt", join(traincostnames, "\n") + "\n");
-            saveStringInFile(expdir / "test_cost_names.txt", join(testcostnames, "\n") + "\n");
-        }
-        global_stats_vm = new FileVMatrix(expdir / "global_stats.pmat",
-                                          1, nstats);
-        for (int k = 0; k < nstats; k++)
-            global_stats_vm->declareField(k, statspecs[k].statName());
-        global_stats_vm->saveFieldInfos();
-
-        split_stats_vm = new FileVMatrix(expdir / "split_stats.pmat",
-                                         0, 1 + nstats);
-        split_stats_vm->declareField(0, "splitnum");
-        for (int k = 0; k < nstats; k++)
-            split_stats_vm->declareField(k+1, statspecs[k].setname + "." + statspecs[k].intstatname);
-        split_stats_vm->saveFieldInfos();
-    }
-
-    for (int splitnum = 0; splitnum < nsplits; splitnum++)
-    {
-        PPath splitdir;
-        bool is_splitdir = false;
-        if (!expdir.isEmpty())
-        {
-            splitdir = expdir / ("Split" + tostring(splitnum));
-            is_splitdir = true;
-        }
-
-        TVec<VMat> dsets = splitter->getSplit(splitnum);
-
-        if (should_train) {
-            VMat trainset = dsets[0];
-            if (is_splitdir && save_data_sets)
-                PLearn::save(splitdir / "training_set.vmat", trainset);
-            
-            if (provide_learner_expdir)
-            {
-                if (is_splitdir)
-                    learner->setExperimentDirectory(splitdir / "LearnerExpdir/");
-                else
-                    learner->setExperimentDirectory("");
-            }
-
-            learner->setTrainingSet(trainset, call_forget && should_train);
-            if (dsets.size() > 1)
-                learner->setValidationSet(dsets[1]);
-
-            if (is_splitdir && save_initial_learners)
-                PLearn::save(splitdir / "initial_learner.psave", learner);
-
-            train_stats->forget();
-            learner->train();
-            train_stats->finalize();
-
-            if (is_splitdir)
-            {
-                if (save_stat_collectors)
-                    PLearn::save(splitdir / "train_stats.psave", train_stats);
-                if (save_learners)
-                    PLearn::save(splitdir / "final_learner.psave", learner);
-            }
-        }
-        else
-            learner->build();
-
-        // This needs to be after the SetTrainingSet() / build() call to the
-        // learner.
-        const int outputsize = learner->outputsize();
-
-        // perf_eval_costs[setnum][perf_evaluator_name][costname] will contain value
-        // of the given cost returned by the given perf_evaluator on the given setnum
-        TVec< map<string, map<string, real> > > perf_eval_costs(dsets.length());
-
-        // Perform the test if required
-        if (should_test)
-        {
-            for (int setnum = 1; setnum < dsets.length(); setnum++)
-            {
-                VMat testset = dsets[setnum];
-                VMat test_outputs;
-                VMat test_costs;
-                VMat test_confidence;
-
-                PP<VecStatsCollector> test_stats = stcol[setnum];
-                const string setname = "test" + tostring(setnum);
-                if (is_splitdir && save_data_sets)
-                    PLearn::save(splitdir / (setname + "_set.vmat"), testset);
-
-                // QUESTION Why is this done so late? Can't it be moved
-                // somewhere earlier? At least before the save_data_sets?
-                if (is_splitdir)
-                    force_mkdir(splitdir);
-
-                if (is_splitdir && save_test_outputs)
-                    test_outputs = new FileVMatrix(splitdir / (setname + "_outputs.pmat"),
-                                                   0, learner->getOutputNames());
-                else if (!perf_evaluators.empty())
-                {
-                    // We don't want to save test outputs to disk, but we
-                    // need them for pef_evaluators. So let's store them in
-                    // a MemoryVMatrix
-                    Mat data(testset.length(), outputsize);
-                    data.resize(0, outputsize);
-                    test_outputs = new MemoryVMatrix(data);
-                    test_outputs->declareFieldNames(learner->getOutputNames());
-                }
-
-                if (is_splitdir)
-                {
-                    if (save_test_costs)
-                        test_costs = new FileVMatrix(splitdir / (setname + "_costs.pmat"),
-                                                     0, learner->getTestCostNames());
-                    if (save_test_confidence)
-                        test_confidence = new FileVMatrix(splitdir / (setname + "_confidence.pmat"),
-                                                          0, 2 * outputsize);
-                }
-
-                const bool reset_stats = (acc.find(setnum) == -1);
-                if (reset_stats)
-                    test_stats->forget();
-                    
-                if (testset->length() == 0)
-                    PLWARNING("PTester:: test set %s is of length 0, costs will be set to -1",
-                              setname.c_str());
-
-                // Before each test set, reset the internal state of the learner
-                learner->resetInternalState();
-
-                learner->test(testset, test_stats, test_outputs, test_costs);
-                if (reset_stats)
-                    test_stats->finalize();
-                if (is_splitdir && save_stat_collectors)
-                    PLearn::save(splitdir / (setname + "_stats.psave"), test_stats);
-
-                perf_evaluators_t::iterator it = perf_evaluators.begin();
-                const perf_evaluators_t::iterator itend = perf_evaluators.end();
-                while (it != itend)
-                {
-                    PPath perf_eval_dir;
-                    if (is_splitdir)
-                        perf_eval_dir = splitdir / setname / ("perfeval_" + it->first);
-                    Vec perf_costvals = it->second->evaluatePerformance(learner, testset, test_outputs, perf_eval_dir);
-                    TVec<string> perf_costnames = it->second->getCostNames();
-                    if (perf_costvals.length()!=perf_costnames.length())
-                        PLERROR("vector of costs returned by performance evaluator differ in size with its vector of costnames");
-                    map<string, real>& costmap = perf_eval_costs[setnum][it->first];
-                    for (int costi = 0; costi < perf_costnames.length(); costi++)
-                        costmap[perf_costnames[costi]] = perf_costvals[costi];
-                    ++it;
-                }
-                computeConfidence(testset, test_confidence);
-            }
-        }
-
-        Vec splitres(1 + nstats);
-        splitres[0] = splitnum;
-
-        for (int k = 0; k < nstats; k++)
-        {
-            // If we ask for a test-set that's beyond what's currently
-            // available, OR we are asking for test-statistics in
-            // train-only mode, then the statistic is MISSING_VALUE.
-            StatSpec& sp = statspecs[k];
-            if (sp.setnum>=stcol.length() ||
-                (! should_test && sp.setnum > 0))
-            {
-                splitres[k+1] = MISSING_VALUE;
-            }
-            else
-            {
-                if (acc.find(sp.setnum) == -1)
-                {
-                    string left, right;
-                    split_on_first(sp.intstatname, ".",left,right);
-                    if (right != "" && perf_evaluators.find(left) != perf_evaluators.end())
-                    {
-                        // looks like a cost from a performance evaluator
-                        map<string, real>& costmap = perf_eval_costs[sp.setnum][left];
-                        if (costmap.find(right) == costmap.end())
-                            PLERROR("No cost named %s appears to be returned by evaluator %s",
-                                    right.c_str(), left.c_str());
-                        splitres[k+1] = costmap[right];
-                    }
-                    else
-                        // must be a cost from a stats collector
-                        splitres[k+1] = stcol[sp.setnum]->getStat(sp.intstatname);
-                }
-                else
-                    splitres[k+1] = MISSING_VALUE;
-            }
-        }
-
-        if (split_stats_vm)
-        {
-            split_stats_vm->appendRow(splitres);
-            split_stats_vm->flush();
-        }
-
-        global_statscol->update(splitres.subVec(1, nstats));
-    }
-
-
-    global_statscol->finalize();
-    for (int k = 0; k < nstats; k++)
-    {
-        if (acc.find(statspecs[k].setnum) == -1)
-            global_result[k] = global_statscol->getStats(k).getStat(statspecs[k].extstat);
-        else
-        {
-            const int j = statspecs[k].setnum;
-            stcol[j]->finalize();
-            global_result[k] = stcol[j]->getStat(statspecs[k].intstatname);
-        }
-    }
-
-    if (global_stats_vm)
-        global_stats_vm->appendRow(global_result);
-
-#if USING_MPI
-    if (PLMPI::rank == 0)
-#endif
-    // Perform the final commands provided in final_commands.
-    for (int i = 0; i < final_commands.length(); i++)
-    {
-        system(final_commands[i].c_str());
-    }
-
-    return global_result;
-}
-
-
+///////////////////
+// perform1Split //
+///////////////////
 Vec PTester::perform1Split(int splitnum, bool call_forget)
 {
     if (!learner)
@@ -848,6 +514,17 @@
         }
 
         learner->setTrainingSet(trainset, call_forget);
+
+        if (need_to_save_test_names) {
+            // Now that the learner has a training set, we can be sure the
+            // cost names can be saved.
+            TVec<string> testcostnames = learner->getTestCostNames();
+            TVec<string> traincostnames = learner->getTrainCostNames();
+            saveStringInFile(expdir / "train_cost_names.txt", join(traincostnames, "\n") + "\n");
+            saveStringInFile(expdir / "test_cost_names.txt", join(testcostnames, "\n") + "\n");
+            need_to_save_test_names = false;
+        }
+
         if (dsets.size() > 1)
             learner->setValidationSet(dsets[1]);
 
@@ -992,7 +669,9 @@
     return splitres;
 }
 
-
+/////////////
+// perform //
+/////////////
 Vec PTester::perform(bool call_forget)
 {
     if (!learner)
@@ -1053,19 +732,10 @@
     // The vmat in which to save per split result stats
     VMat split_stats_vm;
         
-    if (expdir != "" && report_stats)
+    need_to_save_test_names = false; // Reset to default 'false' value.
+    if (!expdir.isEmpty() && report_stats)
     {
-        if(save_test_names){
-            //To work around the fact that RegressionTree need a
-            // train_set to generate the train/test costs names
-            if(!learner->getTrainingSet())
-                learner->setTrainingSet(splitter->getSplit(0)[0], false);
-
-            TVec<string> testcostnames = learner->getTestCostNames();
-            TVec<string> traincostnames = learner->getTrainCostNames();
-            saveStringInFile(expdir / "train_cost_names.txt", join(traincostnames, "\n") + "\n");
-            saveStringInFile(expdir / "test_cost_names.txt", join(testcostnames, "\n") + "\n");
-        }
+        need_to_save_test_names = save_test_names;
         global_stats_vm = new FileVMatrix(expdir / "global_stats.pmat",
                                           1, nstats);
         for (int k = 0; k < nstats; k++)

Modified: trunk/plearn_learners/testers/PTester.h
===================================================================
--- trunk/plearn_learners/testers/PTester.h	2008-03-11 20:48:52 UTC (rev 8664)
+++ trunk/plearn_learners/testers/PTester.h	2008-03-12 15:29:06 UTC (rev 8665)
@@ -67,6 +67,12 @@
     //! processing at build time, taking into account the 'statmask' option.
     TVec<string> statnames_processed;
 
+    //! Set to true in perform() when 'save_test_names' is true, in order to
+    //! remember to save the cost names after setting the learner's training
+    //! set (since some learners may not have these costs available until they
+    //! are provided with a training set).
+    bool need_to_save_test_names;
+
 public:
 
     // ************************
@@ -193,8 +199,6 @@
     Vec perform(bool call_forget=true);
     Vec perform1Split(int splitnum, bool call_forget=true);
 
-    Vec oldperform(bool call_forget=true);
-
     //! Transforms a shallow copy into a deep copy
     virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 };



From tihocan at mail.berlios.de  Wed Mar 12 16:37:10 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 12 Mar 2008 16:37:10 +0100
Subject: [Plearn-commits] r8666 - trunk/plearn/vmat
Message-ID: <200803121537.m2CFbAoq024292@sheep.berlios.de>

Author: tihocan
Date: 2008-03-12 16:37:09 +0100 (Wed, 12 Mar 2008)
New Revision: 8666

Modified:
   trunk/plearn/vmat/SelectColumnsVMatrix.cc
Log:
Fixed constructor to properly call the parent constructor

Modified: trunk/plearn/vmat/SelectColumnsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-03-12 15:29:06 UTC (rev 8665)
+++ trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-03-12 15:37:09 UTC (rev 8666)
@@ -65,13 +65,13 @@
 SelectColumnsVMatrix::SelectColumnsVMatrix(VMat the_source,
                                            TVec<string> the_fields,
                                            bool the_extend_with_missing,
-                                           bool call_build_)
-    : extend_with_missing(the_extend_with_missing),
-      fields(the_fields),
-      fields_partial_match(false),
-      inverse_fields_selection(false)
+                                           bool call_build_):
+    inherited(the_source, call_build_),
+    extend_with_missing(the_extend_with_missing),
+    fields(the_fields),
+    fields_partial_match(false),
+    inverse_fields_selection(false)
 {
-    source = the_source;
     if (call_build_)
         build_();
 }



From tihocan at mail.berlios.de  Wed Mar 12 16:42:43 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 12 Mar 2008 16:42:43 +0100
Subject: [Plearn-commits] r8667 - trunk/plearn/vmat
Message-ID: <200803121542.m2CFghBB026734@sheep.berlios.de>

Author: tihocan
Date: 2008-03-12 16:42:42 +0100 (Wed, 12 Mar 2008)
New Revision: 8667

Modified:
   trunk/plearn/vmat/SelectColumnsVMatrix.cc
Log:
Put back safety check removed in r8662, but with the (hopefully) correct loose inequality

Modified: trunk/plearn/vmat/SelectColumnsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-03-12 15:37:09 UTC (rev 8666)
+++ trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-03-12 15:42:42 UTC (rev 8667)
@@ -230,7 +230,7 @@
                         indices.append(the_index);
                     if(extend_with_missing && the_index == -1)
                         PLWARNING("In SelectColumnsVMatrix::build_() - We are"
-                                  " extendind the source matrix with the"
+                                  " extending the source matrix with the"
                                   " columns '%s' with missing value",
                                   the_field.c_str());
                     
@@ -267,6 +267,10 @@
         width_ = indices.length();
         length_ = source->length();
 
+        if(!extend_with_missing)
+            // The new width cannot exceed the old one.
+            PLCHECK(source->width() >= width_);
+
         //if we don't add new columns and the source columns type are emtpy,
         //they should be empty in this matrix if they are not already set.
         if(targetsize_<0 && !extend_with_missing && source->targetsize()==0)



From larocheh at mail.berlios.de  Wed Mar 12 17:38:03 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 12 Mar 2008 17:38:03 +0100
Subject: [Plearn-commits] r8668 - trunk/plearn_learners/online
Message-ID: <200803121638.m2CGc3pX001289@sheep.berlios.de>

Author: larocheh
Date: 2008-03-12 17:38:03 +0100 (Wed, 12 Mar 2008)
New Revision: 8668

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
Log:
- Added the possibility to do generative fine-tuning with the up-down algorithm
- Added the possibility to use mean-field contrastive divergence


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2008-03-12 15:42:42 UTC (rev 8667)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2008-03-12 16:38:03 UTC (rev 8668)
@@ -39,6 +39,7 @@
 
 #define PL_LOG_MODULE_NAME "DeepBeliefNet"
 #include "DeepBeliefNet.h"
+#include "RBMMatrixTransposeConnection.h"
 #include <plearn/io/pl_log.h>
 
 #define minibatch_hack 0 // Do we force the minibatch setting? (debug hack)
@@ -58,11 +59,14 @@
 DeepBeliefNet::DeepBeliefNet() :
     cd_learning_rate( 0. ),
     cd_decrease_ct( 0. ),
+    up_down_learning_rate( 0. ),
+    up_down_decrease_ct( 0. ),
     grad_learning_rate( 0. ),
     batch_size( 1 ),
     grad_decrease_ct( 0. ),
     // grad_weight_decay( 0. ),
     n_classes( -1 ),
+    up_down_nstages( 0 ),
     use_classification_cost( true ),
     reconstruct_layerwise( false ),
     i_output_layer( -1 ),
@@ -70,6 +74,7 @@
     online ( false ),
     background_gibbs_update_ratio(0),
     gibbs_chain_reinit_freq( INT_MAX ),
+    use_mean_field_contrastive_divergence( false ),
     minibatch_size( 0 ),
     initialize_gibbs_chain( false ),
     final_module_has_learning_rate( false ),
@@ -82,7 +87,8 @@
     cumulative_training_time_cost_index ( -1 ),
     cumulative_testing_time_cost_index ( -1 ),
     cumulative_training_time( 0 ),
-    cumulative_testing_time( 0 )
+    cumulative_testing_time( 0 ),
+    up_down_stage( 0 )
 {
     random_gen = new PRandom();
     n_layers = 0;
@@ -103,6 +109,18 @@
                   "The decrease constant of the learning rate used during"
                   " contrastive divergence");
 
+    declareOption(ol, "up_down_learning_rate", 
+                  &DeepBeliefNet::up_down_learning_rate,
+                  OptionBase::buildoption,
+                  "The learning rate used in the up-down algorithm during the\n"
+                  "unsupervised fine tuning gradient descent.\n");
+
+    declareOption(ol, "up_down_decrease_ct", &DeepBeliefNet::up_down_decrease_ct,
+                  OptionBase::buildoption,
+                  "The decrease constant of the learning rate used in the\n"
+                  "up-down algorithm during the unsupervised fine tuning\n"
+                  "gradient descent.\n");
+
     declareOption(ol, "grad_learning_rate", &DeepBeliefNet::grad_learning_rate,
                   OptionBase::buildoption,
                   "The learning rate used during gradient descent");
@@ -142,6 +160,14 @@
                   "should be [1000 1000 500], and nstages should be at least 2500.\n"
                   "When online = true, this vector is ignored and should be empty.\n");
 
+    declareOption(ol, "up_down_nstages", &DeepBeliefNet::up_down_nstages,
+                  OptionBase::buildoption,
+                  "Number of samples to use for unsupervised fine-tuning\n"
+                  "with the up-down algorithm. The unsupervised fine-tuning will\n"
+                  "be executed between the greedy layer-wise learning and the\n"
+                  "supervised fine-tuning. The up-down algorithm only works for\n"
+                  "RBMMatrixConnection connections.\n");
+
     declareOption(ol, "use_classification_cost",
                   &DeepBeliefNet::use_classification_cost,
                   OptionBase::buildoption,
@@ -254,6 +280,12 @@
                   "If == INT_MAX, the default value of this option, then NEVER\n"
                   "re-initialize except at the beginning, when stage==0.\n");
 
+    declareOption(ol, "use_mean_field_contrastive_divergence",
+                  &DeepBeliefNet::use_mean_field_contrastive_divergence,
+                  OptionBase::buildoption,
+                  "Indication that mean-field contrastive divergence\n"
+                  "should be used instead of standard contrastive divergence.\n");
+
     declareOption(ol, "top_layer_joint_cd", &DeepBeliefNet::top_layer_joint_cd,
                   OptionBase::buildoption,
                   "Wether we do a step of joint contrastive divergence on"
@@ -280,6 +312,18 @@
                   OptionBase::learntoption | OptionBase::nosave,
                   "Cumulative testing time since age=0, in seconds.\n");
 
+    declareOption(ol, "up_down_stage", &DeepBeliefNet::up_down_stage,
+                  OptionBase::learntoption,
+                  "Number of samples visited so far during unsupervised\n"
+                  "fine-tuning.\n");
+
+    declareOption(ol, "generative_connections", 
+                  &DeepBeliefNet::generative_connections,
+                  OptionBase::learntoption,
+                  "The untied generative weights of the connections"
+                  "between the layers\n"
+                  "for the up-down algorithm.\n");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -303,6 +347,28 @@
     if( i_output_layer < 0)
         i_output_layer = n_layers - 1;
 
+    if( online && up_down_nstages > 0)
+        PLERROR("In DeepBeliefNet::build_ - up-down algorithm not implemented "
+            "for online setting.");
+
+    if( batch_size != 1 && up_down_nstages > 0 )
+        PLERROR("In DeepBeliefNet::build_ - up-down algorithm not implemented "
+            "for minibatch setting.");
+
+    if( use_mean_field_contrastive_divergence && up_down_nstages > 0 )
+        PLERROR("In DeepBeliefNet::build_ - up-down algorithm not implemented "
+            "for mean-field CD.");
+
+    if( use_mean_field_contrastive_divergence &&
+        background_gibbs_update_ratio != 0 )
+        PLERROR("In DeepBeliefNet::build_ - mean-field CD cannot be used "
+                "with background_gibbs_update_ratio != 0.");
+
+    if( use_mean_field_contrastive_divergence &&
+        use_sample_for_up_layer )
+        PLERROR("In DeepBeliefNet::build_ - mean-field CD cannot be used "
+                "with use_sample_for_top_layer");
+
     if( !online )
     {
         if( training_schedule.length() != n_layers )
@@ -451,6 +517,12 @@
     expectations_gradients.resize( n_layers );
     gibbs_down_state.resize( n_layers-1 );
 
+    if( up_down_nstages > 0 )
+    {
+        up_sample.resize(n_layers);
+        down_sample.resize(n_layers);
+    }
+
     for( int i=0 ; i<n_layers-1 ; i++ )
     {
         if( layers[i]->size != connections[i]->down_size )
@@ -478,11 +550,22 @@
 
         activation_gradients[i].resize( layers[i]->size );
         expectation_gradients[i].resize( layers[i]->size );
+
+        if( up_down_nstages > 0 )
+        {
+            up_sample[i].resize(layers[i]->size);
+            down_sample[i].resize(layers[i]->size);
+        }
     }
     if( !(layers[n_layers-1]->random_gen) )
     {
         layers[n_layers-1]->random_gen = random_gen;
         layers[n_layers-1]->forget();
+        if( up_down_nstages > 0 )
+        {
+            up_sample[n_layers-1].resize(layers[n_layers-1]->size);
+            down_sample[n_layers-1].resize(layers[n_layers-1]->size);
+        }
     }
     int last_layer_size = layers[n_layers-1]->size;
     PLASSERT_MSG(last_layer_size >= 0,
@@ -684,6 +767,9 @@
     deepCopyField(cumulative_schedule,      copies);
     deepCopyField(layer_input,              copies);
     deepCopyField(layer_inputs,             copies);
+    deepCopyField(generative_connections,   copies);
+    deepCopyField(up_sample,                copies);
+    deepCopyField(down_sample,              copies);
 }
 
 
@@ -737,6 +823,7 @@
 
     cumulative_training_time = 0;
     cumulative_testing_time = 0;
+    up_down_stage = 0;
 }
 
 ///////////
@@ -946,6 +1033,58 @@
             }
         }
 
+        if( up_down_stage < up_down_nstages )
+        {
+
+            if( up_down_stage == 0 )
+            {
+                // Untie weights
+                generative_connections.resize(connections.length()-1);
+                PP<RBMMatrixConnection> w;
+                RBMMatrixTransposeConnection* wt;
+                for(int c=0; c<generative_connections.length(); c++)
+                {
+                    CopiesMap map;
+                    w = dynamic_cast<RBMMatrixConnection*>((RBMConnection*) connections[c]->deepCopy(map));
+                    wt = new RBMMatrixTransposeConnection();
+                    wt->rbm_matrix_connection = w;
+                    wt->build();
+                    generative_connections[c] = wt;
+                }
+            }
+            /***** up-down algorithm *****/
+            MODULE_LOG << "Up-down gradient descent algorithm" << endl;
+            MODULE_LOG << "  up_down_stage = " << up_down_stage << endl;
+            MODULE_LOG << "  up_down_nstages = " << up_down_nstages << endl;
+            MODULE_LOG << "  up_down_learning_rate = " << up_down_learning_rate << endl;
+            
+            int init_stage = up_down_stage;
+            if( report_progress )
+                pb = new ProgressBar( "Up-down gradient descent algorithm "
+                                      + classname(),
+                                      up_down_nstages - init_stage );
+            
+            setLearningRate( up_down_learning_rate );
+
+            train_stats->forget();
+            int sample_start;
+            for( ; up_down_stage<up_down_nstages ; up_down_stage++ )
+            {
+                sample_start = up_down_stage % nsamples;                
+                if( !fast_exact_is_equal( up_down_decrease_ct, 0. ) )
+                    setLearningRate( up_down_learning_rate
+                                     / (1. + up_down_decrease_ct * 
+                                        up_down_stage) );
+                
+                train_set->getExample( sample_start, input, target, weight );
+                upDownStep( input, target, train_costs );
+                train_stats->update( train_costs );
+                
+                if( pb )
+                    pb->update( up_down_stage - init_stage + 1 );
+            }
+        }
+
         /***** fine-tuning by gradient descent *****/
         end_stage = min(cumulative_schedule[n_layers], nstages);
         if( stage >= end_stage )
@@ -1726,6 +1865,81 @@
         layers[ n_layers-1 ], n_layers-2);
 }
 
+////////////////
+// upDownStep //
+////////////////
+void DeepBeliefNet::upDownStep( const Vec& input, const Vec& target,
+                                Vec& train_costs )
+{
+    // Up pass
+    up_sample[0] << input;
+    for( int i=0 ; i<n_layers-2 ; i++ )
+    {
+        connections[i]->setAsDownInput( up_sample[i] );
+        layers[i+1]->getAllActivations( connections[i] );
+        layers[i+1]->computeExpectation();
+        layers[i+1]->generateSample();
+        up_sample[i+1] << layers[i+1]->sample;
+    }
+
+    // Top RBM update
+    if( use_classification_cost )
+    {
+        Vec target_exp = classification_module->target_layer->expectation;
+        fill_one_hot( target_exp, (int) round(target[0]), real(0.), real(1.) );
+
+        contrastiveDivergenceStep(
+            get_pointer( joint_layer ),
+            get_pointer( classification_module->joint_connection ),
+            layers[ n_layers-1 ], n_layers-2,false);
+    }
+    else
+    {
+        contrastiveDivergenceStep( layers[ n_layers-2 ],
+                                   connections[ n_layers-2 ],
+                                   layers[ n_layers-1 ],
+                                   n_layers-2, false);
+    }
+    down_sample[n_layers-2] << layers[n_layers-2]->sample;
+
+    // Down pass
+    for( int i=n_layers-3 ; i>=0 ; i-- )
+    {
+        generative_connections[i]->setAsDownInput( down_sample[i+1] );
+        layers[i]->getAllActivations( generative_connections[i] );
+        layers[i]->computeExpectation();
+        layers[i]->generateSample();
+        down_sample[i] << layers[i]->sample;
+    }
+
+    // Updates
+    real nll;
+    for( int i=0 ; i<n_layers-2 ; i++ )
+    {
+        // Update recognition weights
+        connections[i]->setAsDownInput( down_sample[i] );
+        layers[i+1]->getAllActivations( connections[i] );
+        layers[i+1]->computeExpectation();
+        layers[i+1]->bpropNLL(down_sample[i+1], nll, activation_gradients[i+1]);
+        layers[i+1]->update( activation_gradients[i+1] );
+        connections[i]->bpropUpdate( down_sample[i],
+                                  layers[i+1]->activation,
+                                  activation_gradients[i],
+                                  activation_gradients[i+1]);
+
+        // Update generative weights
+        generative_connections[i]->setAsDownInput( up_sample[i+1] );
+        layers[i]->getAllActivations( generative_connections[i] );
+        layers[i]->computeExpectation();
+        layers[i]->bpropNLL(up_sample[i], nll, activation_gradients[i]);
+        layers[i]->update( activation_gradients[i] );
+        generative_connections[i]->bpropUpdate( up_sample[i+1],
+                                             layers[i]->activation,
+                                             activation_gradients[i+1],
+                                             activation_gradients[i]);
+    }
+}
+
 ////////////////////
 // fineTuningStep //
 ////////////////////
@@ -2009,30 +2223,45 @@
         if (background_gibbs_update_ratio<1)
             // then do some contrastive divergence, o/w only background Gibbs
         {
-            up_layer->generateSamples();
-            if( use_sample_for_up_layer )
-                pos_up_vals << up_layer->samples;
-            connection->setAsUpInputs( up_layer->samples );
-            down_layer->getAllActivations( connection, 0, true );
-            down_layer->computeExpectations();
-            down_layer->generateSamples();
-            // negative phase
-            connection->setAsDownInputs( down_layer->samples );
-            up_layer->getAllActivations( connection, 0, mbatch );
-            up_layer->computeExpectations();
-
-            // accumulate negative stats
-            // no need to deep-copy because the values won't change before update
-            Mat neg_down_vals = down_layer->samples;
+            Mat neg_down_vals;
             Mat neg_up_vals;
-            if( use_sample_for_up_layer)
+            if( use_mean_field_contrastive_divergence )
             {
-                up_layer->generateSamples();
-                neg_up_vals = up_layer->samples;
+                connection->setAsUpInputs( up_layer->getExpectations() );
+                down_layer->getAllActivations( connection, 0, true );
+                down_layer->computeExpectations();
+                // negative phase
+                connection->setAsDownInputs( down_layer->getExpectations() );
+                up_layer->getAllActivations( connection, 0, mbatch );
+                up_layer->computeExpectations();
+
+                neg_down_vals = down_layer->getExpectations();
+                neg_up_vals = up_layer->getExpectations();
             }
             else
-                neg_up_vals = up_layer->getExpectations();
+            {
+                up_layer->generateSamples();
+                if( use_sample_for_up_layer )
+                    pos_up_vals << up_layer->samples;
+                connection->setAsUpInputs( up_layer->samples );
+                down_layer->getAllActivations( connection, 0, true );
+                down_layer->computeExpectations();
+                down_layer->generateSamples();
+                // negative phase
+                connection->setAsDownInputs( down_layer->samples );
+                up_layer->getAllActivations( connection, 0, mbatch );
+                up_layer->computeExpectations();
 
+                neg_down_vals = down_layer->samples;
+                if( use_sample_for_up_layer)
+                {
+                    up_layer->generateSamples();
+                    neg_up_vals = up_layer->samples;
+                }
+                else
+                    neg_up_vals = up_layer->getExpectations();
+            }
+
             if (background_gibbs_update_ratio==0)
             // update here only if there is ONLY contrastive divergence
             {
@@ -2105,8 +2334,9 @@
             down_state << down_layer->samples;
         }
     } else {
-        up_layer->generateSample();
-
+        if( !use_mean_field_contrastive_divergence )
+            up_layer->generateSample();
+        
         // accumulate positive stats using the expectation
         // we deep-copy because the value will change during negative phase
         pos_down_val.resize( down_layer->size );
@@ -2119,19 +2349,30 @@
             pos_up_val << up_layer->expectation;
 
         // down propagation, starting from a sample of up_layer
-        connection->setAsUpInput( up_layer->sample );
+        if( use_mean_field_contrastive_divergence )
+            connection->setAsUpInput( up_layer->expectation );
+        else
+            connection->setAsUpInput( up_layer->sample );
 
         down_layer->getAllActivations( connection );
         down_layer->computeExpectation();
-        down_layer->generateSample();
+        if( !use_mean_field_contrastive_divergence )
+            down_layer->generateSample();
 
         // negative phase
-        connection->setAsDownInput( down_layer->sample );
+        if( use_mean_field_contrastive_divergence )
+            connection->setAsDownInput( down_layer->expectation );
+        else
+            connection->setAsDownInput( down_layer->sample );
         up_layer->getAllActivations( connection, 0, mbatch );
         up_layer->computeExpectation();
         // accumulate negative stats
         // no need to deep-copy because the values won't change before update
-        Vec neg_down_val = down_layer->sample;
+        Vec neg_down_val;
+        if( use_mean_field_contrastive_divergence )
+            neg_down_val = down_layer->expectation;
+        else
+            neg_down_val = down_layer->sample;
         Vec neg_up_val;
         if( use_sample_for_up_layer )
         {
@@ -2141,7 +2382,6 @@
         else
             neg_up_val = up_layer->expectation;
 
-
         // update
         down_layer->update( pos_down_val, neg_down_val );
         connection->update( pos_down_val, pos_up_val,
@@ -2466,6 +2706,9 @@
 
     if( final_cost )
         final_cost->setLearningRate( the_learning_rate );
+
+    for( int i=0 ; i<generative_connections.length() ; i++ )
+        generative_connections[i]->setLearningRate( the_learning_rate );
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2008-03-12 15:42:42 UTC (rev 8667)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2008-03-12 16:38:03 UTC (rev 8668)
@@ -72,6 +72,14 @@
     //! contrastive divergence learning
     real cd_decrease_ct;
 
+    //! The learning rate used in the up-down algorithm during the 
+    //! unsupervised fine tuning gradient descent
+    real up_down_learning_rate;
+    
+    //! The decrease constant of the learning rate used in the
+    //! up-down algorithm during the unsupervised fine tuning gradient descent
+    real up_down_decrease_ct;
+
     //! The learning rate used during the gradient descent
     real grad_learning_rate;
 
@@ -101,6 +109,12 @@
     //! When online = true, this vector is ignored and should be empty.
     TVec<int> training_schedule;
 
+    //! Number of samples to use for unsupervised fine-tuning
+    //! with the up-down algorithm. The unsupervised fine-tuning will
+    //! be executed between the greedy layer-wise learning and the
+    //! supervised fine-tuning.
+    int up_down_nstages;
+
     //! If the first cost function is the NLL in classification,
     //! pre-trained with CD, and using the last *two* layers to get a better
     //! approximation (undirected softmax) than layer-wise mean-field.
@@ -179,6 +193,10 @@
     //! stage==0)
     int gibbs_chain_reinit_freq;
 
+    //! Indication that mean-field contrastive divergence
+    //! should be used instead of standard contrastive divergence.
+    bool use_mean_field_contrastive_divergence;
+
     //#####  Not Options  #####################################################
 
     //! Timer for monitoring the speed
@@ -257,6 +275,9 @@
 
     void jointGreedyStep( const Vec& input, const Vec& target );
 
+    void upDownStep( const Vec& input, const Vec& target,
+                     Vec& train_costs );
+
     void fineTuningStep( const Vec& input, const Vec& target,
                          Vec& train_costs );
 
@@ -404,9 +425,19 @@
     //! Cumulative training schedule
     TVec<int> cumulative_schedule;
 
+    //! Number of samples visited so far during unsupervised fine-tuning
+    int up_down_stage;
+
     mutable Vec layer_input;
     mutable Mat layer_inputs;
 
+    //! The untied generative weights of the connections between the layers
+    //! for the up-down algorithm.
+    TVec< PP<RBMConnection> > generative_connections;
+
+    mutable TVec<Vec> up_sample;
+    mutable TVec<Vec> down_sample;
+
 protected:
     //#####  Protected Member Functions  ######################################
 



From tihocan at mail.berlios.de  Wed Mar 12 19:00:13 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 12 Mar 2008 19:00:13 +0100
Subject: [Plearn-commits] r8669 - trunk/plearn/vmat
Message-ID: <200803121800.m2CI0Dvm014610@sheep.berlios.de>

Author: tihocan
Date: 2008-03-12 19:00:10 +0100 (Wed, 12 Mar 2008)
New Revision: 8669

Modified:
   trunk/plearn/vmat/AutoVMatrix.cc
Log:
Use isEmpty() for PPath

Modified: trunk/plearn/vmat/AutoVMatrix.cc
===================================================================
--- trunk/plearn/vmat/AutoVMatrix.cc	2008-03-12 16:38:03 UTC (rev 8668)
+++ trunk/plearn/vmat/AutoVMatrix.cc	2008-03-12 18:00:10 UTC (rev 8669)
@@ -89,7 +89,7 @@
 
 void AutoVMatrix::build_()
 {
-    if(filename=="")
+    if(filename.isEmpty())
         setVMat(VMat());
     else if (load_data_in_memory)
     {



From tihocan at mail.berlios.de  Wed Mar 12 19:01:30 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 12 Mar 2008 19:01:30 +0100
Subject: [Plearn-commits] r8670 - trunk/plearn/vmat
Message-ID: <200803121801.m2CI1Uo4015250@sheep.berlios.de>

Author: tihocan
Date: 2008-03-12 19:01:29 +0100 (Wed, 12 Mar 2008)
New Revision: 8670

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
More sizes computed with warning displayed! This can be useful to spot errors in SourceVMatrix subclasses that do not properly set the sizes

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-03-12 18:00:10 UTC (rev 8669)
+++ trunk/plearn/vmat/VMatrix.cc	2008-03-12 18:01:29 UTC (rev 8670)
@@ -1224,18 +1224,10 @@
         }
     }
 
-    // Automatically find out inputsize if possible.
-    // Note that this could be done also with other sizes.
-    if (inputsize_ < 0 && width_ >= 0 && targetsize_ >= 0 &&
-            weightsize_ >= 0 && extrasize_ >= 0)
-    {
-        int new_is = width_ - targetsize_ - weightsize_ -
-            extrasize_;
-        if (new_is >= 0)
-            inputsize_ = new_is;
-    }
+    // Fill missing size if possible, also display warning when sizes are not
+    // compatible with the width.
+    computeMissingSizeValue();
 
-
     // Copy fieldnames from vm if not set and they look good.
     bool same_fields_as_source =
         (!hasFieldInfos() && (width() == vm->width()) && vm->hasFieldInfos());



From tihocan at mail.berlios.de  Wed Mar 12 19:02:07 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 12 Mar 2008 19:02:07 +0100
Subject: [Plearn-commits] r8671 - trunk/plearn_learners/testers
Message-ID: <200803121802.m2CI27lA015614@sheep.berlios.de>

Author: tihocan
Date: 2008-03-12 19:02:07 +0100 (Wed, 12 Mar 2008)
New Revision: 8671

Modified:
   trunk/plearn_learners/testers/PTester.cc
Log:
Moved some code around so that the train and test cost names are not called before the learner is provided with a training set

Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2008-03-12 18:01:29 UTC (rev 8670)
+++ trunk/plearn_learners/testers/PTester.cc	2008-03-12 18:02:07 UTC (rev 8671)
@@ -452,14 +452,11 @@
         PLERROR("PTester::perform1Split : No splitter specified for PTester");
 
     const int nstats = statnames_processed.length();
-
-    TVec<string> testcostnames = learner->getTestCostNames();
-    TVec<string> traincostnames = learner->getTrainCostNames();
-
     const int nsets = splitter->nSetsPerSplit();
 
     // Stats collectors for individual sets of a split:
     TVec< PP<VecStatsCollector> > stcol(nsets);
+
     for (int setnum = 0; setnum < nsets; setnum++)
     {
         if (template_stats_collector)
@@ -469,20 +466,9 @@
         }
         else
             stcol[setnum] = new VecStatsCollector();
-
-        if (setnum == 0)
-            stcol[setnum]->setFieldNames(traincostnames);
-        else
-            stcol[setnum]->setFieldNames(testcostnames);
-
-        stcol[setnum]->build();
-        stcol[setnum]->forget();
     }
 
-    PP<VecStatsCollector> train_stats = stcol[0];
-    learner->setTrainStatsCollector(train_stats);
 
-
     // Stat specs
     TVec<StatSpec> statspecs(nstats);
     for(int k = 0; k < nstats; k++)
@@ -500,6 +486,8 @@
 
     TVec<VMat> dsets = splitter->getSplit(splitnum);
 
+    TVec<string> testcostnames;
+
     if (should_train) {
         VMat trainset = dsets[0];
         if (is_splitdir && save_data_sets)
@@ -515,10 +503,19 @@
 
         learner->setTrainingSet(trainset, call_forget);
 
+        TVec<string> testcostnames = learner->getTestCostNames();
+        TVec<string> traincostnames = learner->getTrainCostNames();
+        PP<VecStatsCollector> train_stats = stcol[0];
+        train_stats->setFieldNames(traincostnames);
+        train_stats->build();
+        train_stats->forget();
+        learner->setTrainStatsCollector(train_stats);
+
+
         if (need_to_save_test_names) {
             // Now that the learner has a training set, we can be sure the
             // cost names can be saved.
-            TVec<string> testcostnames = learner->getTestCostNames();
+            testcostnames = learner->getTestCostNames();
             TVec<string> traincostnames = learner->getTrainCostNames();
             saveStringInFile(expdir / "train_cost_names.txt", join(traincostnames, "\n") + "\n");
             saveStringInFile(expdir / "test_cost_names.txt", join(testcostnames, "\n") + "\n");
@@ -554,6 +551,14 @@
     // of the given cost returned by the given perf_evaluator on the given setnum
     TVec< map<string, map<string, real> > > perf_eval_costs(dsets.length());
 
+    if (testcostnames.isEmpty())
+        testcostnames = learner->getTestCostNames();
+    for (int setnum = 1; setnum < nsets; setnum++) {
+        stcol[setnum]->setFieldNames(testcostnames);
+        stcol[setnum]->build();
+        stcol[setnum]->forget();
+    }
+
     // Perform the test if required
     if (should_test)
     {



From nouiz at mail.berlios.de  Wed Mar 12 19:50:46 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 12 Mar 2008 19:50:46 +0100
Subject: [Plearn-commits] r8672 - trunk/plearn_learners/testers
Message-ID: <200803121850.m2CIokov002372@sheep.berlios.de>

Author: nouiz
Date: 2008-03-12 19:50:46 +0100 (Wed, 12 Mar 2008)
New Revision: 8672

Modified:
   trunk/plearn_learners/testers/PTester.cc
Log:
removed duplicate code


Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2008-03-12 18:02:07 UTC (rev 8671)
+++ trunk/plearn_learners/testers/PTester.cc	2008-03-12 18:50:46 UTC (rev 8672)
@@ -503,7 +503,7 @@
 
         learner->setTrainingSet(trainset, call_forget);
 
-        TVec<string> testcostnames = learner->getTestCostNames();
+        testcostnames = learner->getTestCostNames();
         TVec<string> traincostnames = learner->getTrainCostNames();
         PP<VecStatsCollector> train_stats = stcol[0];
         train_stats->setFieldNames(traincostnames);
@@ -515,8 +515,6 @@
         if (need_to_save_test_names) {
             // Now that the learner has a training set, we can be sure the
             // cost names can be saved.
-            testcostnames = learner->getTestCostNames();
-            TVec<string> traincostnames = learner->getTrainCostNames();
             saveStringInFile(expdir / "train_cost_names.txt", join(traincostnames, "\n") + "\n");
             saveStringInFile(expdir / "test_cost_names.txt", join(testcostnames, "\n") + "\n");
             need_to_save_test_names = false;



From tihocan at mail.berlios.de  Wed Mar 12 20:30:06 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 12 Mar 2008 20:30:06 +0100
Subject: [Plearn-commits] r8673 -
	trunk/plearn_learners/distributions/test/.pytest/PL_GaussMix_Generate/expected_results
Message-ID: <200803121930.m2CJU6W7006137@sheep.berlios.de>

Author: tihocan
Date: 2008-03-12 20:30:05 +0100 (Wed, 12 Mar 2008)
New Revision: 8673

Modified:
   trunk/plearn_learners/distributions/test/.pytest/PL_GaussMix_Generate/expected_results/RUN.log
Log:
Fixed test PL_GaussMix_Generate

Modified: trunk/plearn_learners/distributions/test/.pytest/PL_GaussMix_Generate/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/distributions/test/.pytest/PL_GaussMix_Generate/expected_results/RUN.log	2008-03-12 18:50:46 UTC (rev 8672)
+++ trunk/plearn_learners/distributions/test/.pytest/PL_GaussMix_Generate/expected_results/RUN.log	2008-03-12 19:30:05 UTC (rev 8673)
@@ -1,3 +1,3 @@
- WARNING: In SelectColumnsVMatrix::build_() - We are extendind the source matrix with the columns '2' with missing value
- WARNING: In SelectColumnsVMatrix::build_() - We are extendind the source matrix with the columns '2' with missing value
- WARNING: In SelectColumnsVMatrix::build_() - We are extendind the source matrix with the columns '2' with missing value
+ WARNING: In SelectColumnsVMatrix::build_() - We are extending the source matrix with the columns '2' with missing value
+ WARNING: In SelectColumnsVMatrix::build_() - We are extending the source matrix with the columns '2' with missing value
+ WARNING: In SelectColumnsVMatrix::build_() - We are extending the source matrix with the columns '2' with missing value



From tihocan at mail.berlios.de  Wed Mar 12 20:31:19 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 12 Mar 2008 20:31:19 +0100
Subject: [Plearn-commits] r8674 - trunk/plearn/vmat
Message-ID: <200803121931.m2CJVJ38006400@sheep.berlios.de>

Author: tihocan
Date: 2008-03-12 20:31:18 +0100 (Wed, 12 Mar 2008)
New Revision: 8674

Modified:
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
We can now control whether to display warnings in computeMissingSizeValue

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-03-12 19:30:05 UTC (rev 8673)
+++ trunk/plearn/vmat/VMatrix.cc	2008-03-12 19:31:18 UTC (rev 8674)
@@ -2059,20 +2059,28 @@
 ////////////////////////////////
 // compute_missing_size_value //
 ////////////////////////////////
-void VMatrix::computeMissingSizeValue()
+void VMatrix::computeMissingSizeValue(bool warn_if_cannot_compute,
+                                      bool warn_if_size_mismatch)
 {
-    PLCHECK(width_>0);
     int v=min(inputsize_,0) + min(targetsize_,0)
         + min(weightsize_,0) + min(extrasize_,0);
-    if(v<-1)
+
+    if (width_ < 0 && v <= -1) {
+        if (warn_if_cannot_compute)
+            PLWARNING("In VMatrix::computeMissingSizeValue - Cannot compute "
+                      "the missing size value when the width is undefined");
+        return;
+    }
+
+    if(v < -1 && warn_if_cannot_compute)
         PLWARNING("In VMatrix::compute_missing_size_value() - in class %s"
                   " more then one of"
                   " inputsize(%d), targetsize(%d), weightsize(%d) and"
-                  " extrasize(%d) is unknow so we can't compute them with the"
+                  " extrasize(%d) is unknow so we cannot compute them with the"
                   " width(%d)",
                   classname().c_str(), inputsize_, targetsize_, weightsize_,
                   extrasize_, width_);
-    else if(v==0 && 
+    else if(v==0 && warn_if_size_mismatch && width_ >= 0 &&
             width_ != inputsize_ + targetsize_ + weightsize_ + extrasize_)
         PLWARNING("In VMatrix::compute_missing_size_value() for class %s - "
                   "inputsize_(%d) + targetsize_(%d) + weightsize_(%d) + "
@@ -2088,7 +2096,6 @@
         weightsize_ = width_- inputsize_ - targetsize_ - extrasize_;
     else if(extrasize_ < 0)
         extrasize_  = width_- inputsize_ - targetsize_ - weightsize_;
-
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-03-12 19:30:05 UTC (rev 8673)
+++ trunk/plearn/vmat/VMatrix.h	2008-03-12 19:31:18 UTC (rev 8674)
@@ -664,10 +664,17 @@
      */
     int maxFieldNamesSize() const;
 
-    /** if only one of inputsize, targetsize, weightsize, extrasize
-     *  is less unknow and width>0, we compute its value
+    /** If only one of inputsize, targetsize, weightsize, extrasize
+     *  is unknow while width>=0, we compute its value.
+     *  Two warnings may be issued in this method:
+     *      1. If 'warn_if_cannot_compute' is true, a warning is issued when
+     *         it is not possible to compute a missing size's value (for
+     *         instance when there are two missing sizes).
+     *      2. If 'warn_if_size_mismatch' is true, a warning is issued when
+     *         all sizes are defined but they do not match the width.
      */
-    void computeMissingSizeValue();
+    void computeMissingSizeValue(bool warn_if_cannot_compute = true,
+                                 bool warn_if_size_mismatch = true);
 
     /**
      *  Returns the bounding box of the data, as a vector of min:max pairs.  If



From tihocan at mail.berlios.de  Wed Mar 12 20:31:47 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 12 Mar 2008 20:31:47 +0100
Subject: [Plearn-commits] r8675 - trunk/plearn/vmat
Message-ID: <200803121931.m2CJVlZL006469@sheep.berlios.de>

Author: tihocan
Date: 2008-03-12 20:31:47 +0100 (Wed, 12 Mar 2008)
New Revision: 8675

Modified:
   trunk/plearn/vmat/SelectColumnsVMatrix.cc
Log:
Do not display warnings anymore when the sizes cannot be filled

Modified: trunk/plearn/vmat/SelectColumnsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-03-12 19:31:18 UTC (rev 8674)
+++ trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-03-12 19:31:47 UTC (rev 8675)
@@ -277,7 +277,7 @@
             targetsize_=0;
         if(weightsize_<0 && !extend_with_missing && source->weightsize()==0)
             weightsize_=0;
-        computeMissingSizeValue();
+        computeMissingSizeValue(false);
 
 #if 0
         // Disabled for now, since it gives way too many false positives in



From tihocan at mail.berlios.de  Wed Mar 12 20:32:16 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 12 Mar 2008 20:32:16 +0100
Subject: [Plearn-commits] r8676 - trunk/plearn/vmat
Message-ID: <200803121932.m2CJWGr3006539@sheep.berlios.de>

Author: tihocan
Date: 2008-03-12 20:32:16 +0100 (Wed, 12 Mar 2008)
New Revision: 8676

Modified:
   trunk/plearn/vmat/ConcatRowsVMatrix.cc
Log:
Simplified code since there should be no more warning when sizes are not defined

Modified: trunk/plearn/vmat/ConcatRowsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ConcatRowsVMatrix.cc	2008-03-12 19:31:47 UTC (rev 8675)
+++ trunk/plearn/vmat/ConcatRowsVMatrix.cc	2008-03-12 19:32:16 UTC (rev 8676)
@@ -321,13 +321,8 @@
     for (int i = 0; i < sources.length(); i++) {
         TVec<string> source_fnames=sources[i]->fieldNames();
         if(fnames!=source_fnames){
-            SelectColumnsVMatrix* v = 
-                new SelectColumnsVMatrix(sources[i],fnames,true,false);
-            //to remove warning
-            if(inputsize_>=0 && targetsize_>=0 && weightsize_ >=0)
-                v->defineSizes(inputsize_, targetsize_, weightsize_,extrasize_);
-            v->build();
-            to_concat[i] = v;
+            to_concat[i] =
+                new SelectColumnsVMatrix(sources[i], fnames, true);
         } else
             to_concat[i] = sources[i];
     }



From tihocan at mail.berlios.de  Wed Mar 12 20:33:23 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 12 Mar 2008 20:33:23 +0100
Subject: [Plearn-commits] r8677 -
	trunk/plearn_learners/regressors/test/RegressionTree
Message-ID: <200803121933.m2CJXNS7006627@sheep.berlios.de>

Author: tihocan
Date: 2008-03-12 20:33:23 +0100 (Wed, 12 Mar 2008)
New Revision: 8677

Modified:
   trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn
Log:
Fixed dataset mismatch between sizes and width

Modified: trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn	2008-03-12 19:32:16 UTC (rev 8676)
+++ trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn	2008-03-12 19:33:23 UTC (rev 8677)
@@ -3,12 +3,18 @@
 
 plarg_defaults.data    = "PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat"
 
-dataset = pl.AutoVMatrix(
-    specification = plargs.data,
-    inputsize = 4,
-    targetsize = 1,
-    weightsize = 0
-    )
+dataset = pl.SubVMatrix(
+        inputsize = 4,
+        targetsize = 1,
+        weightsize = 0,
+        source = pl.AutoVMatrix(
+            specification = plargs.data,
+            inputsize = 4,
+            targetsize = 2,
+            weightsize = 0
+            ),
+        width = 5
+        )
 
 learner = pl.HyperLearner(
     option_fields = [ "nstages" ],



From nouiz at mail.berlios.de  Wed Mar 12 21:05:48 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 12 Mar 2008 21:05:48 +0100
Subject: [Plearn-commits] r8678 - trunk/python_modules/plearn/pymake
Message-ID: <200803122005.m2CK5mq9009936@sheep.berlios.de>

Author: nouiz
Date: 2008-03-12 21:05:48 +0100 (Wed, 12 Mar 2008)
New Revision: 8678

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
print the number of process used for parallel compilation.


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-03-12 19:33:23 UTC (rev 8677)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-03-12 20:05:48 UTC (rev 8678)
@@ -761,8 +761,8 @@
     if hostspath_list:
         if distcc_list_of_hosts is not None:
             print '*** Overriding distcc settings. (Remove pymake *.hosts file to use distcc settings.)'
-        print '*** Parallel compilation using list of hosts from file(s): ' + string.join( hostspath_list, ', ' )
         (list_of_hosts, nice_values) = process_hostspath_list(hostspath_list, default_nice_value,myhostname)
+        print '*** Parallel compilation using',len(list_of_hosts),'process from file(s): ' + string.join( hostspath_list, ', ' )
     elif distcc_list_of_hosts is not None:
         print '*** Parallel compilation using distcc list of hosts (%d)' % len(distcc_list_of_hosts)
         list_of_hosts = distcc_list_of_hosts



From lamblin at mail.berlios.de  Thu Mar 13 04:20:49 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Thu, 13 Mar 2008 04:20:49 +0100
Subject: [Plearn-commits] r8679 - in trunk: commands plearn_learners/online
Message-ID: <200803130320.m2D3Knnq030162@sheep.berlios.de>

Author: lamblin
Date: 2008-03-13 04:20:48 +0100 (Thu, 13 Mar 2008)
New Revision: 8679

Modified:
   trunk/commands/plearn_noblas_inc.h
   trunk/plearn_learners/online/InferenceRBM.cc
   trunk/plearn_learners/online/InferenceRBM.h
Log:
Module that scales (or stops) the propagation of a gradient.


Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2008-03-12 20:05:48 UTC (rev 8678)
+++ trunk/commands/plearn_noblas_inc.h	2008-03-13 03:20:48 UTC (rev 8679)
@@ -238,6 +238,7 @@
 #include <plearn_learners/online/RBMMultinomialLayer.h>
 #include <plearn_learners/online/RBMTrainer.h>
 #include <plearn_learners/online/RBMTruncExpLayer.h>
+#include <plearn_learners/online/ScaleGradientModule.h>
 #include <plearn_learners/online/SoftmaxModule.h>
 #include <plearn_learners/online/SplitModule.h>
 #include <plearn_learners/online/SquaredErrorCostModule.h>

Modified: trunk/plearn_learners/online/InferenceRBM.cc
===================================================================
--- trunk/plearn_learners/online/InferenceRBM.cc	2008-03-12 20:05:48 UTC (rev 8678)
+++ trunk/plearn_learners/online/InferenceRBM.cc	2008-03-13 03:20:48 UTC (rev 8679)
@@ -78,11 +78,14 @@
     deepCopyField(input_layer, copies);
     deepCopyField(target_layer, copies);
     deepCopyField(hidden_layer, copies);
-    deepCopyField(visible_layer, copies);
-
     deepCopyField(input_to_hidden, copies);
     deepCopyField(target_to_hidden, copies);
+    deepCopyField(random_gen, copies);
+    deepCopyField(visible_layer, copies);
     deepCopyField(visible_to_hidden, copies);
+    deepCopyField(v0, copies);
+    deepCopyField(h0, copies);
+
 }
 
 void InferenceRBM::declareOptions(OptionList& ol)
@@ -128,7 +131,12 @@
                   OptionBase::buildoption,
                   "Random numbers generator");
 
+    declareOption(ol, "use_fast_approximations",
+                  &InferenceRBM::use_fast_approximations,
+                  OptionBase::buildoption,
+                  "Whether to use fast approximations in softplus computation");
 
+
     declareOption(ol, "visible_layer", &InferenceRBM::visible_layer,
                   OptionBase::learntoption,
                   "Visible layer (input+target)");
@@ -181,6 +189,14 @@
          ArgDoc ("input", "Input layer's values")));
 
     declareMethod(
+        rmm, "hiddenExpGivenInputTarget",
+        &InferenceRBM::hiddenExpGivenInputTarget,
+        (BodyDoc("Computes the hidden layer's expectation given the input\n"
+                 "and the target"),
+         ArgDoc ("input", "Input layer's values"),
+         ArgDoc ("target", "Target (as an index)")));
+
+    declareMethod(
         rmm, "targetExpGivenInput",
         &InferenceRBM::targetExpGivenInput,
         (BodyDoc("Computes the target layer's expectation given the input"),
@@ -201,11 +217,30 @@
          RetDoc ("Hidden layer's expectation")));
 
     declareMethod(
+        rmm, "getHiddenExpGivenInputTarget",
+        &InferenceRBM::getHiddenExpGivenInputTarget,
+        (BodyDoc("Computes the hidden layer's expectation given the input\n"
+                 "and the target"),
+         ArgDoc ("input", "Input layer's values"),
+         ArgDoc ("target", "Target (as an index)"),
+         RetDoc ("Hidden layer's expectation")));
+
+    declareMethod(
         rmm, "getTargetExpGivenInput",
         &InferenceRBM::getTargetExpGivenInput,
         (BodyDoc("Computes the target layer's expectation given the input"),
          ArgDoc ("input", "Input layer's values"),
          RetDoc ("Target layer's expectation")));
+
+    declareMethod(
+        rmm, "supCDStep", &InferenceRBM::supCDStep,
+        (BodyDoc("Performs one step of CD and updates the parameters"),
+         ArgDoc ("visible", "Visible layer's values")));
+
+    declareMethod(
+        rmm, "setLearningRate", &InferenceRBM::setLearningRate,
+        (BodyDoc("Sets the learning rate of underlying modules"),
+         ArgDoc ("the_learning_rate", "The learning rate")));
 }
 
 
@@ -300,6 +335,23 @@
     hidden_layer->computeExpectations();
 }
 
+void InferenceRBM::hiddenExpGivenInputTarget(const Mat& input,
+                                             const TVec<int>& target)
+{
+    int batch_size = input.length();
+    PLASSERT(input.width() == input_size);
+    PLASSERT(target.length() == batch_size);
+
+    input_to_hidden->setAsDownInputs(input);
+    hidden_layer->getAllActivations(get_pointer(input_to_hidden), 0, true);
+
+    for (int k=0; k<batch_size; k++)
+        hidden_layer->activations(k) += target_to_hidden->weights(target[k]);
+
+    hidden_layer->expectations_are_up_to_date = false;
+    hidden_layer->computeExpectations();
+}
+
 void InferenceRBM::targetExpGivenInput(const Mat& input)
 {
     PLASSERT(input.width() == input_size);
@@ -336,7 +388,11 @@
                 PLASSERT(*t_to_h_w_ji == t_to_h_w(j,i));
                 PLASSERT(*hidden_act_kj == hidden_act(k,j));
 
-                *target_act_ki += softplus(*t_to_h_w_ji + *hidden_act_kj);
+                if (use_fast_approximations)
+                    *target_act_ki +=
+                        tabulated_softplus(*t_to_h_w_ji + *hidden_act_kj);
+                else
+                    *target_act_ki += softplus(*t_to_h_w_ji + *hidden_act_kj);
             }
         }
     }
@@ -378,6 +434,13 @@
     return hidden_layer->getExpectations();
 }
 
+Mat InferenceRBM::getHiddenExpGivenInputTarget(const Mat& input,
+                                               const TVec<int>& target)
+{
+    hiddenExpGivenInputTarget(input, target);
+    return hidden_layer->getExpectations();
+}
+
 Mat InferenceRBM::getTargetExpGivenInput(const Mat& input)
 {
     targetExpGivenInput(input);
@@ -390,6 +453,47 @@
     return hidden_layer->getExpectations();
 }
 
+void InferenceRBM::supCDStep(const Mat& visible)
+{
+    PLASSERT(visible.width() == visible_size);
+    int batch_size = visible.length();
+
+    v0.resize(batch_size,visible_size);
+    v0 << visible;
+
+    // positive phase
+    hiddenExpGivenVisible(visible);
+    h0.resize(batch_size, hidden_size);
+    h0 << hidden_layer->getExpectations();
+
+    // Down propagation
+    visible_to_hidden->setAsUpInputs(h0);
+    visible_layer->getAllActivations(get_pointer(visible_to_hidden), 0, true);
+    visible_layer->computeExpectations();
+    visible_layer->generateSamples();
+
+    // Negative phase
+    hiddenExpGivenVisible(visible_layer->samples);
+
+    // Update
+    visible_layer->update(v0, visible_layer->samples);
+    visible_to_hidden->update(v0, h0, visible_layer->samples,
+                              hidden_layer->getExpectations());
+    hidden_layer->update(h0, hidden_layer->getExpectations());
+}
+
+void InferenceRBM::unsupCDStep(const Mat& input)
+{
+    PLCHECK_MSG(false, "Not implemented yet");
+}
+
+void InferenceRBM::setLearningRate(real the_learning_rate)
+{
+    visible_layer->setLearningRate(the_learning_rate);
+    visible_to_hidden->setLearningRate(the_learning_rate);
+    hidden_layer->setLearningRate(the_learning_rate);
+}
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/online/InferenceRBM.h
===================================================================
--- trunk/plearn_learners/online/InferenceRBM.h	2008-03-12 20:05:48 UTC (rev 8678)
+++ trunk/plearn_learners/online/InferenceRBM.h	2008-03-13 03:20:48 UTC (rev 8679)
@@ -92,8 +92,11 @@
     //! Random numbers generator
     PP<PRandom> random_gen;
 
+    //! Whether to use fast approximations in softplus computation
+    bool use_fast_approximations;
 
 
+
 public:
     //#####  Public Member Functions  #########################################
 
@@ -105,14 +108,20 @@
     // Your other public member functions go here
 
     void hiddenExpGivenVisible(const Mat& visible);
+    void hiddenExpGivenInputTarget(const Mat& input, const TVec<int>& target);
     void hiddenExpGivenInput(const Mat& input);
     void targetExpGivenInput(const Mat& input);
 
     Mat getHiddenExpGivenVisible(const Mat& visible);
+    Mat getHiddenExpGivenInputTarget(const Mat& input, const TVec<int>& target);
     Mat getHiddenExpGivenInput(const Mat& input);
     Mat getTargetExpGivenInput(const Mat& input);
 
+    void supCDStep(const Mat& visible);
+    void unsupCDStep(const Mat& input);
 
+    virtual void setLearningRate(real the_learning_rate);
+
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.
@@ -159,13 +168,16 @@
     //#####  Private Member Functions  ########################################
 
     //! This does the actual building.
-    // (PLEASE IMPLEMENT IN .cc)
     void build_();
 
+    void build_rbms();
+
 private:
     //#####  Private Data Members  ############################################
 
     // The rest of the private stuff goes here
+    mutable Mat v0;
+    mutable Mat h0;
 };
 
 // Declares a few other classes and functions related to this class



From lamblin at mail.berlios.de  Thu Mar 13 05:07:18 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Thu, 13 Mar 2008 05:07:18 +0100
Subject: [Plearn-commits] r8680 - trunk/plearn_learners/online
Message-ID: <200803130407.m2D47ITk000499@sheep.berlios.de>

Author: lamblin
Date: 2008-03-13 05:07:18 +0100 (Thu, 13 Mar 2008)
New Revision: 8680

Added:
   trunk/plearn_learners/online/ScaleGradientModule.cc
   trunk/plearn_learners/online/ScaleGradientModule.h
Log:
Oops, forgot to commit the right files.


Added: trunk/plearn_learners/online/ScaleGradientModule.cc
===================================================================
--- trunk/plearn_learners/online/ScaleGradientModule.cc	2008-03-13 03:20:48 UTC (rev 8679)
+++ trunk/plearn_learners/online/ScaleGradientModule.cc	2008-03-13 04:07:18 UTC (rev 8680)
@@ -0,0 +1,156 @@
+// -*- C++ -*-
+
+// ScaleGradientModule.cc
+//
+// Copyright (C) 2008 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file ScaleGradientModule.cc */
+
+
+
+#include "ScaleGradientModule.h"
+#include <plearn/math/TMat_maths.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    ScaleGradientModule,
+    "Scales (or suppress) the gradient that is backpropagated.",
+    ""
+);
+
+//////////////////
+// ScaleGradientModule //
+//////////////////
+ScaleGradientModule::ScaleGradientModule():
+    scale(0)
+{
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void ScaleGradientModule::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "scale", &ScaleGradientModule::scale,
+        OptionBase::buildoption,
+        "The scaling factor. If 0, no gradient will be backpropagated."
+        );
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+////////////
+// build_ //
+////////////
+void ScaleGradientModule::build_()
+{
+}
+
+///////////
+// build //
+///////////
+void ScaleGradientModule::build()
+{
+    inherited::build();
+    build_();
+}
+
+////////////
+// forget //
+////////////
+void ScaleGradientModule::forget()
+{
+}
+
+///////////
+// fprop //
+///////////
+void ScaleGradientModule::fprop(const Mat& inputs, Mat& outputs)
+{
+    outputs.resize(inputs.length(), inputs.width());
+    outputs << inputs;
+}
+
+/////////////////
+// bpropUpdate //
+/////////////////
+void ScaleGradientModule::bpropUpdate(const Mat& inputs, const Mat& outputs,
+                                      Mat& input_gradients,
+                                      const Mat& output_gradients,
+                                      bool accumulate)
+{
+    input_gradients.resize(output_gradients.length(),
+                           output_gradients.width());
+
+    if (accumulate)
+    {
+        if (scale == 0)
+            return;
+        else // input_gradients += scale * output_gradients
+            multiplyAcc(input_gradients, output_gradients, scale);
+    }
+    else
+    {
+        if (scale == 0)
+            input_gradients.clear();
+        else // input_gradients = scale * output_gradients
+            multiply(input_gradients, output_gradients, scale);
+    }
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void ScaleGradientModule::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+}
+
+}
+// end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/ScaleGradientModule.h
===================================================================
--- trunk/plearn_learners/online/ScaleGradientModule.h	2008-03-13 03:20:48 UTC (rev 8679)
+++ trunk/plearn_learners/online/ScaleGradientModule.h	2008-03-13 04:07:18 UTC (rev 8680)
@@ -0,0 +1,133 @@
+// -*- C++ -*-
+
+// ScaleGradientModule.h
+//
+// Copyright (C) 2008 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file ScaleGradientModule.h */
+
+
+#ifndef ScaleGradientModule_INC
+#define ScaleGradientModule_INC
+
+#include <plearn_learners/online/OnlineLearningModule.h>
+
+namespace PLearn {
+
+/**
+ * Scales (or suppress) the gradient that is backpropagated.
+ *
+ */
+class ScaleGradientModule : public OnlineLearningModule
+{
+    typedef OnlineLearningModule inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! Scaling factor
+    real scale;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    ScaleGradientModule();
+
+    // Your other public member functions go here
+
+    //! Given a batch of inputs, compute the outputs
+    //! SOON TO BE DEPRECATED, USE fprop(const TVec<Mat*>& ports_value)
+    virtual void fprop(const Mat& inputs, Mat& outputs);
+
+    virtual void bpropUpdate(const Mat& inputs, const Mat& outputs,
+                             Mat& input_gradients,
+                             const Mat& output_gradients,
+                             bool accumulate=false);
+
+    //! Reset the parameters to the state they would be BEFORE starting
+    //! training.  Note that this method is necessarily called from
+    //! build().
+    virtual void forget();
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(ScaleGradientModule);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(ScaleGradientModule);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From nouiz at mail.berlios.de  Thu Mar 13 14:22:38 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 13 Mar 2008 14:22:38 +0100
Subject: [Plearn-commits] r8681 - trunk/plearn_learners/regressors
Message-ID: <200803131322.m2DDMckc025392@sheep.berlios.de>

Author: nouiz
Date: 2008-03-13 14:22:38 +0100 (Thu, 13 Mar 2008)
New Revision: 8681

Modified:
   trunk/plearn_learners/regressors/RegressionTree.cc
Log:
bugfix for bug introduced in Rev 8659 that would make all the costs equal 0 in some case


Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2008-03-13 04:07:18 UTC (rev 8680)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2008-03-13 13:22:38 UTC (rev 8681)
@@ -377,29 +377,17 @@
                                            Vec& outputv, Vec& costsv) const
 {
     PLASSERT(costsv.size()==nTestCosts());
-    TVec<PP<RegressionTreeNode> > nodes;
-    root->computeOutputAndNodes(inputv, outputv, &nodes);
-    if (multiclass_outputs.length() <= 0) return;
-    real closest_value=multiclass_outputs[0];
-    real margin_to_closest_value=abs(outputv[0] - multiclass_outputs[0]);
-    for (int value_ind = 1; value_ind < multiclass_outputs.length(); value_ind++)
-    {
-        real v=abs(outputv[0] - multiclass_outputs[value_ind]);
-        if (v < margin_to_closest_value)
-        {
-            closest_value = multiclass_outputs[value_ind];
-            margin_to_closest_value = v;
-        }
-    }
-    outputv[0] = closest_value;
+    
+    TVec<PP<RegressionTreeNode> > *nodes = new TVec<PP<RegressionTreeNode> >();
+    root->computeOutputAndNodes(inputv, outputv, nodes);
     costsv.clear();
     costsv[0] = pow((outputv[0] - targetv[0]), 2);
     costsv[1] = outputv[1];
     costsv[2] = 1.0 - (l2_loss_function_factor * costsv[0]);
     costsv[3] = 1.0 - (l1_loss_function_factor * abs(outputv[0] - targetv[0]));
     costsv[4] = !fast_is_equal(targetv[0],outputv[0]);
-    for(int i=0;i<nodes.length();i++)
-        costsv[5+nodes[i]->getSplitCol()]++;
+    for(int i=0;i<nodes->length();i++)
+        costsv[5+(*nodes)[i]->getSplitCol()]++;
 }
 
 void RegressionTree::computeCostsFromOutputs(const Vec& input,



From nouiz at mail.berlios.de  Thu Mar 13 15:10:36 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 13 Mar 2008 15:10:36 +0100
Subject: [Plearn-commits] r8682 - in
	trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir:
	. Split0 Split0/LearnerExpdir/Strat0/Trials0/Split0
	Split0/LearnerExpdir/Strat0/Trials1/Split0
	Split0/LearnerExpdir/Strat0/Trials2/Split0
	Split0/test1_costs.pmat.metadata Split0/test2_costs.pmat.metadata
Message-ID: <200803131410.m2DEAam1031246@sheep.berlios.de>

Author: nouiz
Date: 2008-03-13 15:10:35 +0100 (Thu, 13 Mar 2008)
New Revision: 8682

Modified:
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/experiment.plearn
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/test_cost_names.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave
Log:
fix PL_RegressionTree test. The error was caused by the fact that width_ != inputsize_ + targetsize_ + weightsize_ + extrasize_ in the past


Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave	2008-03-13 13:22:38 UTC (rev 8681)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave	2008-03-13 14:10:35 UTC (rev 8682)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 11 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_0" "SPLIT_VAR_1" "SPLIT_VAR_2" "SPLIT_VAR_3" "SPLIT_VAR_4" "SPLIT_VAR_5" ] ;
+fieldnames = 10 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_0" "SPLIT_VAR_1" "SPLIT_VAR_2" "SPLIT_VAR_3" "SPLIT_VAR_4" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 11 [ StatsCollector(
+stats = 10 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -217,27 +217,6 @@
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 150 ;
-sumsquarew_ = 150 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 0 ;
-max_ = 0 ;
-agmemin_ = 149 ;
-agemax_ = 149 ;
-first_ = 0 ;
-last_ = 0 ;
-binary_ = 1 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave	2008-03-13 13:22:38 UTC (rev 8681)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave	2008-03-13 14:10:35 UTC (rev 8682)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 11 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_0" "SPLIT_VAR_1" "SPLIT_VAR_2" "SPLIT_VAR_3" "SPLIT_VAR_4" "SPLIT_VAR_5" ] ;
+fieldnames = 10 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_0" "SPLIT_VAR_1" "SPLIT_VAR_2" "SPLIT_VAR_3" "SPLIT_VAR_4" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 11 [ StatsCollector(
+stats = 10 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -217,27 +217,6 @@
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 50 ;
-sumsquarew_ = 50 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 0 ;
-max_ = 0 ;
-agmemin_ = 49 ;
-agemax_ = 49 ;
-first_ = 0 ;
-last_ = 0 ;
-binary_ = 1 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave	2008-03-13 13:22:38 UTC (rev 8681)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave	2008-03-13 14:10:35 UTC (rev 8682)
@@ -7,7 +7,7 @@
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 11 [ StatsCollector(
+stats = 10 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -217,27 +217,6 @@
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 150 ;
-sumsquarew_ = 150 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 0 ;
-max_ = 0 ;
-agmemin_ = 149 ;
-agemax_ = 149 ;
-first_ = 0 ;
-last_ = 0 ;
-binary_ = 1 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave	2008-03-13 13:22:38 UTC (rev 8681)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave	2008-03-13 14:10:35 UTC (rev 8682)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 11 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_0" "SPLIT_VAR_1" "SPLIT_VAR_2" "SPLIT_VAR_3" "SPLIT_VAR_4" "SPLIT_VAR_5" ] ;
+fieldnames = 10 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_0" "SPLIT_VAR_1" "SPLIT_VAR_2" "SPLIT_VAR_3" "SPLIT_VAR_4" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 11 [ StatsCollector(
+stats = 10 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -217,27 +217,6 @@
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 150 ;
-sumsquarew_ = 150 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 0 ;
-max_ = 0 ;
-agmemin_ = 149 ;
-agemax_ = 149 ;
-first_ = 0 ;
-last_ = 0 ;
-binary_ = 1 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave	2008-03-13 13:22:38 UTC (rev 8681)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave	2008-03-13 14:10:35 UTC (rev 8682)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 11 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_0" "SPLIT_VAR_1" "SPLIT_VAR_2" "SPLIT_VAR_3" "SPLIT_VAR_4" "SPLIT_VAR_5" ] ;
+fieldnames = 10 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_0" "SPLIT_VAR_1" "SPLIT_VAR_2" "SPLIT_VAR_3" "SPLIT_VAR_4" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 11 [ StatsCollector(
+stats = 10 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -217,27 +217,6 @@
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 50 ;
-sumsquarew_ = 50 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 0 ;
-max_ = 0 ;
-agmemin_ = 49 ;
-agemax_ = 49 ;
-first_ = 0 ;
-last_ = 0 ;
-binary_ = 1 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave	2008-03-13 13:22:38 UTC (rev 8681)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave	2008-03-13 14:10:35 UTC (rev 8682)
@@ -7,7 +7,7 @@
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 11 [ StatsCollector(
+stats = 10 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -217,27 +217,6 @@
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 150 ;
-sumsquarew_ = 150 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 0 ;
-max_ = 0 ;
-agmemin_ = 149 ;
-agemax_ = 149 ;
-first_ = 0 ;
-last_ = 0 ;
-binary_ = 1 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave	2008-03-13 13:22:38 UTC (rev 8681)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave	2008-03-13 14:10:35 UTC (rev 8682)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 11 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_0" "SPLIT_VAR_1" "SPLIT_VAR_2" "SPLIT_VAR_3" "SPLIT_VAR_4" "SPLIT_VAR_5" ] ;
+fieldnames = 10 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_0" "SPLIT_VAR_1" "SPLIT_VAR_2" "SPLIT_VAR_3" "SPLIT_VAR_4" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 11 [ StatsCollector(
+stats = 10 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -217,27 +217,6 @@
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 150 ;
-sumsquarew_ = 150 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 0 ;
-max_ = 0 ;
-agmemin_ = 149 ;
-agemax_ = 149 ;
-first_ = 0 ;
-last_ = 0 ;
-binary_ = 1 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave	2008-03-13 13:22:38 UTC (rev 8681)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave	2008-03-13 14:10:35 UTC (rev 8682)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 11 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_0" "SPLIT_VAR_1" "SPLIT_VAR_2" "SPLIT_VAR_3" "SPLIT_VAR_4" "SPLIT_VAR_5" ] ;
+fieldnames = 10 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_0" "SPLIT_VAR_1" "SPLIT_VAR_2" "SPLIT_VAR_3" "SPLIT_VAR_4" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 11 [ StatsCollector(
+stats = 10 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -217,27 +217,6 @@
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 50 ;
-sumsquarew_ = 50 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 0 ;
-max_ = 0 ;
-agmemin_ = 49 ;
-agemax_ = 49 ;
-first_ = 0 ;
-last_ = 0 ;
-binary_ = 1 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave	2008-03-13 13:22:38 UTC (rev 8681)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave	2008-03-13 14:10:35 UTC (rev 8682)
@@ -7,7 +7,7 @@
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 11 [ StatsCollector(
+stats = 10 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -217,27 +217,6 @@
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 150 ;
-sumsquarew_ = 150 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 0 ;
-max_ = 0 ;
-agmemin_ = 149 ;
-agemax_ = 149 ;
-first_ = 0 ;
-last_ = 0 ;
-binary_ = 1 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames	2008-03-13 13:22:38 UTC (rev 8681)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames	2008-03-13 14:10:35 UTC (rev 8682)
@@ -8,4 +8,3 @@
 SPLIT_VAR_2	0
 SPLIT_VAR_3	0
 SPLIT_VAR_4	0
-SPLIT_VAR_5	0

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_stats.psave	2008-03-13 13:22:38 UTC (rev 8681)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_stats.psave	2008-03-13 14:10:35 UTC (rev 8682)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 11 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_0" "SPLIT_VAR_1" "SPLIT_VAR_2" "SPLIT_VAR_3" "SPLIT_VAR_4" "SPLIT_VAR_5" ] ;
+fieldnames = 10 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_0" "SPLIT_VAR_1" "SPLIT_VAR_2" "SPLIT_VAR_3" "SPLIT_VAR_4" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 11 [ StatsCollector(
+stats = 10 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -217,27 +217,6 @@
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 150 ;
-sumsquarew_ = 150 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 0 ;
-max_ = 0 ;
-agmemin_ = 149 ;
-agemax_ = 149 ;
-first_ = 0 ;
-last_ = 0 ;
-binary_ = 1 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames	2008-03-13 13:22:38 UTC (rev 8681)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames	2008-03-13 14:10:35 UTC (rev 8682)
@@ -8,4 +8,3 @@
 SPLIT_VAR_2	0
 SPLIT_VAR_3	0
 SPLIT_VAR_4	0
-SPLIT_VAR_5	0

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_stats.psave	2008-03-13 13:22:38 UTC (rev 8681)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_stats.psave	2008-03-13 14:10:35 UTC (rev 8682)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 11 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_0" "SPLIT_VAR_1" "SPLIT_VAR_2" "SPLIT_VAR_3" "SPLIT_VAR_4" "SPLIT_VAR_5" ] ;
+fieldnames = 10 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_0" "SPLIT_VAR_1" "SPLIT_VAR_2" "SPLIT_VAR_3" "SPLIT_VAR_4" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 11 [ StatsCollector(
+stats = 10 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -217,27 +217,6 @@
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 50 ;
-sumsquarew_ = 50 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 0 ;
-max_ = 0 ;
-agmemin_ = 49 ;
-agemax_ = 49 ;
-first_ = 0 ;
-last_ = 0 ;
-binary_ = 1 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/experiment.plearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/experiment.plearn	2008-03-13 13:22:38 UTC (rev 8681)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/experiment.plearn	2008-03-13 14:10:35 UTC (rev 8682)
@@ -1,19 +1,25 @@
-*10 -> PTester(
-    dataset = *1 -> AutoVMatrix(
+*11 -> PTester(
+    dataset = *2 -> SubVMatrix(
         inputsize = 4,
-        specification = "PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat",
+        source = *1 -> AutoVMatrix(
+            inputsize = 4,
+            specification = "PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat",
+            targetsize = 2,
+            weightsize = 0
+            ),
         targetsize = 1,
-        weightsize = 0
+        weightsize = 0,
+        width = 5
         ),
     expdir = "expdir",
-    learner = *8 -> HyperLearner(
+    learner = *9 -> HyperLearner(
         dont_restart_upon_change = [ "nstages" ],
         forget_when_training_set_changes = 0,
-        learner = *3 -> RegressionTree(
+        learner = *4 -> RegressionTree(
             complexity_penalty_factor = 0.0,
             compute_train_stats = 1,
             forget_when_training_set_changes = 1,
-            leave_template = *2 -> RegressionTreeLeave( ),
+            leave_template = *3 -> RegressionTreeLeave( ),
             loss_function_weight = 1,
             maximum_number_of_nodes = 50,
             nstages = 10,
@@ -27,8 +33,8 @@
         report_progress = 1,
         save_final_learner = 0,
         strategy = [
-            *5 -> HyperOptimize(
-                oracle = *4 -> EarlyStoppingOracle(
+            *6 -> HyperOptimize(
+                oracle = *5 -> EarlyStoppingOracle(
                     max_degradation = 3.40282e+38,
                     max_degraded_steps = 120,
                     max_value = 3.40282e+38,
@@ -48,7 +54,7 @@
                 which_cost = "E[test2.E[mse]]"
                 )
             ],
-        tester = *7 -> PTester(
+        tester = *8 -> PTester(
             provide_learner_expdir = 1,
             report_stats = 1,
             save_data_sets = 0,
@@ -59,7 +65,7 @@
             save_test_costs = 0,
             save_test_names = 0,
             save_test_outputs = 0,
-            splitter = *6 -> FractionSplitter(
+            splitter = *7 -> FractionSplitter(
                 splits = 1 3 [
                         (0, 0.75),
                         (0, 0.75),
@@ -84,7 +90,7 @@
     save_test_confidence = 0,
     save_test_costs = 1,
     save_test_outputs = 1,
-    splitter = *9 -> FractionSplitter(
+    splitter = *10 -> FractionSplitter(
         splits = 1 3 [
                 (0, 1),
                 (0, 0.75),

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt	2008-03-13 13:22:38 UTC (rev 8681)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt	2008-03-13 14:10:35 UTC (rev 8682)
@@ -1,2 +1,2 @@
-__REVISION__ = "PL8649"
+__REVISION__ = "PL8677"
 data                                          = PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/test_cost_names.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/test_cost_names.txt	2008-03-13 13:22:38 UTC (rev 8681)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/test_cost_names.txt	2008-03-13 14:10:35 UTC (rev 8682)
@@ -8,4 +8,3 @@
 SPLIT_VAR_2
 SPLIT_VAR_3
 SPLIT_VAR_4
-SPLIT_VAR_5

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave	2008-03-13 13:22:38 UTC (rev 8681)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave	2008-03-13 14:10:35 UTC (rev 8682)
@@ -1,18 +1,33 @@
 PTester(
 expdir = "PYTEST__PL_RegressionTree__RESULTS:expdir/" ;
-dataset = *1 ->AutoVMatrix(
+dataset = *1 ->SubVMatrix(
+parent = *2 ->AutoVMatrix(
 filename = "PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat" ;
 load_in_memory = 0 ;
 writable = 0 ;
 length = 200 ;
 width = 6 ;
 inputsize = 4 ;
-targetsize = 1 ;
+targetsize = 2 ;
 weightsize = 0 ;
 extrasize = 0 ;
 metadatadir = "PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat.metadata/"  )
 ;
-splitter = *2 ->FractionSplitter(
+istart = 0 ;
+jstart = 0 ;
+fistart = -1 ;
+flength = -1 ;
+source = *2  ;
+writable = 0 ;
+length = 200 ;
+width = 5 ;
+inputsize = 4 ;
+targetsize = 1 ;
+weightsize = 0 ;
+extrasize = 0 ;
+metadatadir = ""  )
+;
+splitter = *3 ->FractionSplitter(
 round_to_closest = 0 ;
 splits = 1  3  [ 
 (0 , 1 )	(0 , 0.75 )	(0.75 , 1 )	
@@ -22,11 +37,11 @@
 statnames = 8 [ "E[test1.E[mse]]" "E[test1.E[base_confidence]]" "E[test1.E[base_reward_l2]]" "E[test1.E[base_reward_l1]]" "E[test2.E[mse]]" "E[test2.E[base_confidence]]" "E[test2.E[base_reward_l2]]" "E[test2.E[base_reward_l1]]" ] ;
 statmask = []
 ;
-learner = *3 ->HyperLearner(
-tester = *4 ->PTester(
+learner = *4 ->HyperLearner(
+tester = *5 ->PTester(
 expdir = "" ;
 dataset = *0 ;
-splitter = *5 ->FractionSplitter(
+splitter = *6 ->FractionSplitter(
 round_to_closest = 0 ;
 splits = 1  3  [ 
 (0 , 0.75 )	(0 , 0.75 )	(0.75 , 1 )	
@@ -36,7 +51,7 @@
 statnames = 8 [ "E[test1.E[mse]]" "E[test1.E[base_confidence]]" "E[test1.E[base_reward_l2]]" "E[test1.E[base_reward_l1]]" "E[test2.E[mse]]" "E[test2.E[base_confidence]]" "E[test2.E[base_reward_l2]]" "E[test2.E[base_reward_l1]]" ] ;
 statmask = []
 ;
-learner = *6 ->RegressionTree(
+learner = *7 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 50 ;
@@ -44,7 +59,7 @@
 complexity_penalty_factor = 0 ;
 multiclass_outputs = []
 ;
-leave_template = *7 ->RegressionTreeLeave(
+leave_template = *8 ->RegressionTreeLeave(
 id = -1 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
@@ -106,10 +121,10 @@
 ;
 option_fields = 1 [ "nstages" ] ;
 dont_restart_upon_change = 1 [ "nstages" ] ;
-strategy = 1 [ *8 ->HyperOptimize(
+strategy = 1 [ *9 ->HyperOptimize(
 which_cost = "E[test2.E[mse]]" ;
 min_n_trials = 0 ;
-oracle = *9 ->EarlyStoppingOracle(
+oracle = *10 ->EarlyStoppingOracle(
 option = "nstages" ;
 values = []
 ;
@@ -133,7 +148,7 @@
 ] ;
 provide_strategy_expdir = 1 ;
 save_final_learner = 0 ;
-learner = *6  ;
+learner = *7  ;
 provide_learner_expdir = 1 ;
 expdir_append = "" ;
 forward_nstages = 0 ;



From nouiz at mail.berlios.de  Thu Mar 13 20:14:52 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 13 Mar 2008 20:14:52 +0100
Subject: [Plearn-commits] r8683 - trunk/plearn/vmat
Message-ID: <200803131914.m2DJEqsZ015729@sheep.berlios.de>

Author: nouiz
Date: 2008-03-13 20:14:52 +0100 (Thu, 13 Mar 2008)
New Revision: 8683

Modified:
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
In function VMatrix::isFileUpToDate, changed parameter name and order.
The order of parameter in .cc and in .h was different


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-03-13 14:10:35 UTC (rev 8682)
+++ trunk/plearn/vmat/VMatrix.cc	2008-03-13 19:14:52 UTC (rev 8683)
@@ -1453,14 +1453,14 @@
 ////////////////////
 // isFileUpToDate //
 ////////////////////
-bool VMatrix::isFileUpToDate(const PPath& path, bool warning_reuse,
+bool VMatrix::isFileUpToDate(const PPath& path, bool warning_mtime0,
                              bool warning_older) const
 {
     bool exist = isfile(path);
     bool uptodate = false;
     if(exist)
         uptodate = getMtime() < mtime(path);
-    if (warning_reuse && exist && uptodate && getMtime()==0)
+    if (warning_mtime0 && exist && uptodate && getMtime()==0)
         PLWARNING("In VMatrix::isFileUpToDate - for class '%s'"
                   " File '%s' will be used, but "
                   "this VMat's last modification time is undefined: we cannot "

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-03-13 14:10:35 UTC (rev 8682)
+++ trunk/plearn/vmat/VMatrix.h	2008-03-13 19:14:52 UTC (rev 8683)
@@ -404,8 +404,8 @@
     //! file exists and it is older than this VMat's last modification time.
     //! If 'warning_reuse' is 'true', then a warning will be issued when the
     //! file exists and this VMat's last modification time is undefined.
-    bool isFileUpToDate(const PPath& file, bool warning_older = false,
-                        bool warning_reuse = true) const;
+    bool isFileUpToDate(const PPath& file, bool warning_mtime0 = true,
+                        bool warning_older = false) const;
 
     //#####  Matrix Sizes  ####################################################
 



From lamblin at mail.berlios.de  Fri Mar 14 03:54:26 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 14 Mar 2008 03:54:26 +0100
Subject: [Plearn-commits] r8684 - trunk/plearn_learners/online
Message-ID: <200803140254.m2E2sQj3016924@sheep.berlios.de>

Author: lamblin
Date: 2008-03-14 03:54:25 +0100 (Fri, 14 Mar 2008)
New Revision: 8684

Modified:
   trunk/plearn_learners/online/ScaleGradientModule.cc
   trunk/plearn_learners/online/ScaleGradientModule.h
Log:
Avoid warning


Modified: trunk/plearn_learners/online/ScaleGradientModule.cc
===================================================================
--- trunk/plearn_learners/online/ScaleGradientModule.cc	2008-03-13 19:14:52 UTC (rev 8683)
+++ trunk/plearn_learners/online/ScaleGradientModule.cc	2008-03-14 02:54:25 UTC (rev 8684)
@@ -131,6 +131,11 @@
     }
 }
 
+void ScaleGradientModule::setLearningRate(real the_learning_rate)
+{
+    // pass;
+}
+
 /////////////////////////////////
 // makeDeepCopyFromShallowCopy //
 /////////////////////////////////

Modified: trunk/plearn_learners/online/ScaleGradientModule.h
===================================================================
--- trunk/plearn_learners/online/ScaleGradientModule.h	2008-03-13 19:14:52 UTC (rev 8683)
+++ trunk/plearn_learners/online/ScaleGradientModule.h	2008-03-14 02:54:25 UTC (rev 8684)
@@ -80,6 +80,8 @@
     //! build().
     virtual void forget();
 
+    virtual void setLearningRate(real the_learning_rate);
+
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.



From lamblin at mail.berlios.de  Fri Mar 14 03:55:29 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 14 Mar 2008 03:55:29 +0100
Subject: [Plearn-commits] r8685 - trunk/plearn_learners/online
Message-ID: <200803140255.m2E2tT0q016976@sheep.berlios.de>

Author: lamblin
Date: 2008-03-14 03:55:28 +0100 (Fri, 14 Mar 2008)
New Revision: 8685

Modified:
   trunk/plearn_learners/online/ProcessInputCostModule.cc
   trunk/plearn_learners/online/ProcessInputCostModule.h
Log:
Add minibatch


Modified: trunk/plearn_learners/online/ProcessInputCostModule.cc
===================================================================
--- trunk/plearn_learners/online/ProcessInputCostModule.cc	2008-03-14 02:54:25 UTC (rev 8684)
+++ trunk/plearn_learners/online/ProcessInputCostModule.cc	2008-03-14 02:55:28 UTC (rev 8685)
@@ -126,13 +126,19 @@
 
     deepCopyField(processing_module, copies);
     deepCopyField(cost_module, copies);
+    deepCopyField(processed_value, copies);
+    deepCopyField(processed_values, copies);
+    deepCopyField(processed_gradient, copies);
+    deepCopyField(processed_gradients, copies);
+    deepCopyField(processed_diag_hessian, copies);
+    deepCopyField(processed_diag_hessians, copies);
 }
 
 ///////////
 // fprop //
 ///////////
 void ProcessInputCostModule::fprop(const Vec& input, const Vec& target,
-                                   Vec& cost) const
+                                   real& cost) const
 {
     PLASSERT( processing_module );
     PLASSERT( cost_module );
@@ -143,8 +149,21 @@
     cost_module->fprop( processed_value, target, cost );
 }
 
+void ProcessInputCostModule::fprop(const Mat& inputs, const Mat& targets,
+                                   Vec& costs )
+{
+    PLASSERT( processing_module );
+    PLASSERT( cost_module );
+    PLASSERT( inputs.width() == input_size );
+    PLASSERT( targets.width() == target_size );
+    PLASSERT( inputs.length() == targets.length() );
+
+    processing_module->fprop( inputs, processed_values );
+    cost_module->fprop( processed_values, targets, costs );
+}
+
 void ProcessInputCostModule::fprop(const Vec& input, const Vec& target,
-                                   real& cost) const
+                                   Vec& cost) const
 {
     PLASSERT( processing_module );
     PLASSERT( cost_module );
@@ -155,6 +174,21 @@
     cost_module->fprop( processed_value, target, cost );
 }
 
+void ProcessInputCostModule::fprop(const Mat& inputs, const Mat& targets,
+                                   Mat& costs ) const
+{
+    PLASSERT( processing_module );
+    PLASSERT( cost_module );
+    PLASSERT( inputs.width() == input_size );
+    PLASSERT( targets.width() == target_size );
+    PLASSERT( inputs.length() == targets.length() );
+
+    processing_module->fprop( inputs, processed_values );
+    cost_module->fprop( processed_values, targets, costs );
+}
+
+
+
 /////////////////
 // bpropUpdate //
 /////////////////
@@ -180,8 +214,32 @@
                                     accumulate );
 }
 
+void ProcessInputCostModule::bpropUpdate(const Mat& inputs, const Mat& targets,
+                                         const Vec& costs, Mat& input_gradients,
+                                         bool accumulate)
+{
+    PLASSERT( processing_module );
+    PLASSERT( cost_module );
+    PLASSERT( inputs.width() == input_size );
+    PLASSERT( targets.width() == target_size );
+    PLASSERT( inputs.length() == targets.length() );
+    PLASSERT( inputs.length() == costs.size() );
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradients.width() == input_size
+                      && input_gradients.length() == inputs.length(),
+                      "Cannot resize input_gradient AND accumulate into it" );
+    }
 
+    cost_module->bpropUpdate( processed_values, targets, costs,
+                              processed_gradients );
+    processing_module->bpropUpdate( inputs, processed_values,
+                                    input_gradients, processed_gradients,
+                                    accumulate );
+}
+
+
 /////////////////
 // bbpropUpdate //
 /////////////////
@@ -219,6 +277,9 @@
 ////////////
 void ProcessInputCostModule::forget()
 {
+    PLASSERT( processing_module );
+    PLASSERT( cost_module );
+
     processed_value.clear();
     processed_gradient.clear();
     processed_diag_hessian.clear();

Modified: trunk/plearn_learners/online/ProcessInputCostModule.h
===================================================================
--- trunk/plearn_learners/online/ProcessInputCostModule.h	2008-03-14 02:54:25 UTC (rev 8684)
+++ trunk/plearn_learners/online/ProcessInputCostModule.h	2008-03-14 02:55:28 UTC (rev 8685)
@@ -76,18 +76,30 @@
     //! Default constructor
     ProcessInputCostModule();
 
+    //! Given the input and the target, compute only the first cost
+    //! (of which we will compute the gradient)
+    virtual void fprop(const Vec& input, const Vec& target, real& cost) const;
+
+    //! Minibatch version
+    virtual void fprop(const Mat& inputs, const Mat& targets, Vec& costs );
+
     //! Given the input and the target, compute a vector of costs
     //! (possibly resize it appropriately)
     virtual void fprop(const Vec& input, const Vec& target, Vec& cost) const;
 
-    //! Given the input and the target, compute only the first cost
-    //! (of which we will compute the gradient)
-    virtual void fprop(const Vec& input, const Vec& target, real& cost) const;
+    //! Minibatch version
+    virtual void fprop(const Mat& inputs, const Mat& targets, Mat& costs )
+        const;
 
     //! Adapt based on the cost, and compute input gradient to backpropagate.
     virtual void bpropUpdate(const Vec& input, const Vec& target, real cost,
                              Vec& input_gradient, bool accumulate=false);
 
+    //! Minibatch version
+    virtual void bpropUpdate(const Mat& inputs, const Mat& targets,
+                             const Vec& costs, Mat& input_gradients,
+                             bool accumulate=false);
+
     /* Optional
        N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
        JUST CALLS
@@ -167,8 +179,11 @@
     // The rest of the private stuff goes here
     //#####  Not Options  #####################################################
     mutable Vec processed_value;
+    mutable Mat processed_values;
     mutable Vec processed_gradient;
+    mutable Mat processed_gradients;
     mutable Vec processed_diag_hessian;
+    mutable Mat processed_diag_hessians;
 };
 
 // Declares a few other classes and functions related to this class



From nouiz at mail.berlios.de  Fri Mar 14 15:36:32 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 14 Mar 2008 15:36:32 +0100
Subject: [Plearn-commits] r8687 - trunk/plearn_learners/regressors
Message-ID: <200803141436.m2EEaWQ4014474@sheep.berlios.de>

Author: nouiz
Date: 2008-03-14 15:36:32 +0100 (Fri, 14 Mar 2008)
New Revision: 8687

Modified:
   trunk/plearn_learners/regressors/RegressionTree.cc
   trunk/plearn_learners/regressors/RegressionTree.h
Log:
code cleanup: localised some variable
code speedup: reuse the variables nodes for efficiency


Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2008-03-14 14:27:05 UTC (rev 8686)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2008-03-14 14:36:32 UTC (rev 8687)
@@ -132,6 +132,7 @@
     deepCopyField(first_leave, copies);
     deepCopyField(split_cols, copies);
     deepCopyField(tmp_vec, copies);
+    
 }
 
 void RegressionTree::build()
@@ -170,10 +171,7 @@
         if (weightsize != 1 && weightsize != 0)
             PLERROR("RegressionTree: expected weightsize to be 1 or 0, got %d",
                     weightsize);
-        sample_input.resize(inputsize);
-        sample_target.resize(targetsize);
-        sample_output.resize(outputsize());
-        sample_costs.resize(getTestCostNames().size());
+        nodes = new TVec<PP<RegressionTreeNode> >();
     }
 
     if (loss_function_weight != 0.0)
@@ -212,8 +210,13 @@
         pb = new ProgressBar("RegressionTree : computing the statistics: ", length);
     } 
     train_stats->forget();
+
     real sample_weight;
-        
+    Vec sample_input(sorted_train_set->inputsize());
+    Vec sample_output(outputsize());
+    Vec sample_target(sorted_train_set->targetsize());
+    Vec sample_costs(getTestCostNames().size());
+
     for (int train_sample_index = 0; train_sample_index < length;
          train_sample_index++)
     {  
@@ -377,8 +380,8 @@
                                            Vec& outputv, Vec& costsv) const
 {
     PLASSERT(costsv.size()==nTestCosts());
-    
-    TVec<PP<RegressionTreeNode> > *nodes = new TVec<PP<RegressionTreeNode> >();
+    PLASSERT(nodes);
+    nodes->resize(0);
     root->computeOutputAndNodes(inputv, outputv, nodes);
     costsv.clear();
     costsv[0] = pow((outputv[0] - targetv[0]), 2);

Modified: trunk/plearn_learners/regressors/RegressionTree.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.h	2008-03-14 14:27:05 UTC (rev 8686)
+++ trunk/plearn_learners/regressors/RegressionTree.h	2008-03-14 14:36:32 UTC (rev 8687)
@@ -84,11 +84,8 @@
     int length;
     real l2_loss_function_factor;
     real l1_loss_function_factor;
-    Vec sample_input;
-    Vec sample_output;
-    Vec sample_target;
-    Vec sample_costs;
     TVec<int> split_cols;
+    TVec<PP<RegressionTreeNode> > *nodes;
 
     Vec tmp_vec;
 public:



From nouiz at mail.berlios.de  Fri Mar 14 15:27:05 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 14 Mar 2008 15:27:05 +0100
Subject: [Plearn-commits] r8686 - trunk/plearn/vmat
Message-ID: <200803141427.m2EER5X8013219@sheep.berlios.de>

Author: nouiz
Date: 2008-03-14 15:27:05 +0100 (Fri, 14 Mar 2008)
New Revision: 8686

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
Log:
fix a bug introduced in revision 8647


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2008-03-14 02:55:28 UTC (rev 8685)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2008-03-14 14:27:05 UTC (rev 8686)
@@ -674,7 +674,7 @@
         char char_torm = ' ';
         if(fieldtype=="dollar-comma")
             char_torm = ',';
-        else if(strval[0]=='$')
+        if(strval[0]=='$')
         {
             string s = "";
             for(unsigned int pos=1; pos<strval.size(); pos++)



From tihocan at mail.berlios.de  Fri Mar 14 19:57:16 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 14 Mar 2008 19:57:16 +0100
Subject: [Plearn-commits] r8688 - in trunk/python_modules/plearn: . pyext
Message-ID: <200803141857.m2EIvG8m001300@sheep.berlios.de>

Author: tihocan
Date: 2008-03-14 19:57:16 +0100 (Fri, 14 Mar 2008)
New Revision: 8688

Modified:
   trunk/python_modules/plearn/__init__.py
   trunk/python_modules/plearn/pyext/__init__.py
Log:
Added mechanism to allow one to change the PLearn C++ library before importing plearn.pyext

Modified: trunk/python_modules/plearn/__init__.py
===================================================================
--- trunk/python_modules/plearn/__init__.py	2008-03-14 14:36:32 UTC (rev 8687)
+++ trunk/python_modules/plearn/__init__.py	2008-03-14 18:57:16 UTC (rev 8688)
@@ -12,3 +12,17 @@
     else:
         return condp
 
+# Mechanism to allow the user to choose which PLearn library should be used.
+def getLib():
+    try:
+        return pl_lib_dir, pl_lib_name
+    except:
+        # Return default library.
+        return 'plearn.pyext', 'plext'
+
+def setLib(dir, name):
+    global pl_lib_dir
+    global pl_lib_name
+    pl_lib_dir = dir
+    pl_lib_name = name
+

Modified: trunk/python_modules/plearn/pyext/__init__.py
===================================================================
--- trunk/python_modules/plearn/pyext/__init__.py	2008-03-14 14:36:32 UTC (rev 8687)
+++ trunk/python_modules/plearn/pyext/__init__.py	2008-03-14 18:57:16 UTC (rev 8688)
@@ -30,9 +30,12 @@
 #  This file is part of the PLearn library. For more information on the PLearn
 #  library, go to the PLearn Web site at www.plearn.org
 
-from plearn.pyext.plext import *
-from plearn.pyext import plext as pl
+from plearn import getLib
 
+pl_lib_dir, pl_lib_name = getLib()
+exec 'from %s.%s import *' % (pl_lib_dir, pl_lib_name)
+exec 'from %s import %s as pl' % (pl_lib_dir, pl_lib_name)
+
 from plearn.pyplearn.plargs import *
 import os
 import cgitb



From saintmlx at mail.berlios.de  Mon Mar 17 17:07:04 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Mon, 17 Mar 2008 17:07:04 +0100
Subject: [Plearn-commits] r8689 - trunk/scripts
Message-ID: <200803171607.m2HG74dr025709@sheep.berlios.de>

Author: saintmlx
Date: 2008-03-17 17:07:04 +0100 (Mon, 17 Mar 2008)
New Revision: 8689

Modified:
   trunk/scripts/make_plearn_python_ext
Log:
- add warning (deprecated script)



Modified: trunk/scripts/make_plearn_python_ext
===================================================================
--- trunk/scripts/make_plearn_python_ext	2008-03-14 18:57:16 UTC (rev 8688)
+++ trunk/scripts/make_plearn_python_ext	2008-03-17 16:07:04 UTC (rev 8689)
@@ -31,7 +31,14 @@
 #  This file is part of the PLearn library. For more information on the PLearn
 #  library, go to the PLearn Web site at www.plearn.org
 
+echo
+echo "******************************************"
+echo "** WARNING ** : THIS SCRIPT IS DEPRECATED! To compile a PLearn python extension, you should use pymake directly with the -pyso option."
+echo "******************************************"
+echo;echo
+
 pushd $PLEARNDIR/python_modules/plearn/pyext
-pymake -so $* plext.cc
-ln -fs libplext.so plext.so
+pymake -pyso $* plext.cc
+#pymake -so $* plext.cc
+#ln -fs libplext.so plext.so
 popd



From nouiz at mail.berlios.de  Mon Mar 17 17:16:54 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 17 Mar 2008 17:16:54 +0100
Subject: [Plearn-commits] r8690 - trunk/plearn/vmat
Message-ID: <200803171616.m2HGGso7026601@sheep.berlios.de>

Author: nouiz
Date: 2008-03-17 17:16:53 +0100 (Mon, 17 Mar 2008)
New Revision: 8690

Modified:
   trunk/plearn/vmat/FilteredVMatrix.cc
Log:
Added assert


Modified: trunk/plearn/vmat/FilteredVMatrix.cc
===================================================================
--- trunk/plearn/vmat/FilteredVMatrix.cc	2008-03-17 16:07:04 UTC (rev 8689)
+++ trunk/plearn/vmat/FilteredVMatrix.cc	2008-03-17 16:16:53 UTC (rev 8690)
@@ -84,6 +84,7 @@
 
 void FilteredVMatrix::openIndex()
 {
+    PLASSERT(hasMetaDataDir());
     string idxfname = getMetaDataDir() / "filtered.idx";
     if(!force_mkdir(getMetaDataDir()))
         PLERROR("In FilteredVMatrix::openIndex could not create directory %s",getMetaDataDir().absolute().c_str());



From nouiz at mail.berlios.de  Mon Mar 17 17:23:19 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 17 Mar 2008 17:23:19 +0100
Subject: [Plearn-commits] r8691 - trunk/plearn/vmat
Message-ID: <200803171623.m2HGNJfq027205@sheep.berlios.de>

Author: nouiz
Date: 2008-03-17 17:23:19 +0100 (Mon, 17 Mar 2008)
New Revision: 8691

Modified:
   trunk/plearn/vmat/PrecomputedVMatrix.cc
Log:
In class PrecomputedVMatrix, only call setMetaInfoFromSource() when the metadatadir is gived. This remove warning, as FilteredVMatrix is completly built only when the metadatadir is set.


Modified: trunk/plearn/vmat/PrecomputedVMatrix.cc
===================================================================
--- trunk/plearn/vmat/PrecomputedVMatrix.cc	2008-03-17 16:16:53 UTC (rev 8690)
+++ trunk/plearn/vmat/PrecomputedVMatrix.cc	2008-03-17 16:23:19 UTC (rev 8691)
@@ -119,7 +119,10 @@
 {
     inherited::setMetaDataDir(the_metadatadir);
     if ( hasMetaDataDir() ) // don't do anything if the meta-data-dir is not yet set.
+    {
+        setMetaInfoFromSource();
         usePrecomputed();
+    }
 }
 
 void PrecomputedVMatrix::usePrecomputed()
@@ -158,7 +161,7 @@
         {
             precomp_source = temporary ? new TemporaryFileVMatrix(pmatfile)
                                        : new FileVMatrix(pmatfile);
-            if(precomp_source->getMtime() >= source->getMtime())
+            if(isFileUpToDate(pmatfile))
                 recompute = false;
         }
 
@@ -178,7 +181,10 @@
 
 void PrecomputedVMatrix::build_()
 {
-    setMetaInfoFromSource();
+    //We only call it their, as some matrix(FilteredVMatrix) are only completly
+    //set if they have a metadatadir.
+    if(hasMetaDataDir())
+        setMetaInfoFromSource();
 }
 
 // ### Nothing to add here, simply calls build_



From nouiz at mail.berlios.de  Mon Mar 17 17:55:55 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 17 Mar 2008 17:55:55 +0100
Subject: [Plearn-commits] r8692 - trunk/plearn/vmat
Message-ID: <200803171655.m2HGttjQ030699@sheep.berlios.de>

Author: nouiz
Date: 2008-03-17 17:55:53 +0100 (Mon, 17 Mar 2008)
New Revision: 8692

Modified:
   trunk/plearn/vmat/FilteredVMatrix.cc
   trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
   trunk/plearn/vmat/PrecomputedVMatrix.cc
   trunk/plearn/vmat/TextFilesVMatrix.cc
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
renamed VMatrix::isFileUpToDate to VMatrix::isUpToDate.
Created VMatrix::isUpToDate(VMat,...)


Modified: trunk/plearn/vmat/FilteredVMatrix.cc
===================================================================
--- trunk/plearn/vmat/FilteredVMatrix.cc	2008-03-17 16:23:19 UTC (rev 8691)
+++ trunk/plearn/vmat/FilteredVMatrix.cc	2008-03-17 16:55:53 UTC (rev 8692)
@@ -91,7 +91,7 @@
 
 
     lockMetaDataDir();
-    if(isFileUpToDate(idxfname))
+    if(isUpToDate(idxfname))
         indexes.open(idxfname);
     else  // let's (re)create the index
     {

Modified: trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-03-17 16:23:19 UTC (rev 8691)
+++ trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-03-17 16:55:53 UTC (rev 8692)
@@ -256,8 +256,8 @@
     PPath train_metadata = train_set->getMetaDataDir();
     PPath mean_median_mode_file_name = train_metadata + "mean_median_mode_file.pmat";
     train_set->lockMetaDataDir();
-    if (!train_set->isFileUpToDate(mean_median_mode_file_name,true,true)
-	||!source->isFileUpToDate(mean_median_mode_file_name,true,true))
+    if (!train_set->isUpToDate(mean_median_mode_file_name,true,true)
+	||!source->isUpToDate(mean_median_mode_file_name,true,true))
     {
         computeMeanMedianModeVectors();
         createMeanMedianModeFile(mean_median_mode_file_name);
@@ -276,8 +276,8 @@
 
 void MeanMedianModeImputationVMatrix::loadMeanMedianModeFile(PPath file_name)
 {
-    isFileUpToDate(file_name,true,true);
-    source->isFileUpToDate(file_name,true,true);
+    isUpToDate(file_name,true,true);
+    source->isUpToDate(file_name,true,true);
 
     mean_median_mode_file = new FileVMatrix(file_name);
     mean_median_mode_file->getRow(0, variable_mean);

Modified: trunk/plearn/vmat/PrecomputedVMatrix.cc
===================================================================
--- trunk/plearn/vmat/PrecomputedVMatrix.cc	2008-03-17 16:23:19 UTC (rev 8691)
+++ trunk/plearn/vmat/PrecomputedVMatrix.cc	2008-03-17 16:55:53 UTC (rev 8692)
@@ -138,7 +138,7 @@
         {
             precomp_source = temporary ? new TemporaryDiskVMatrix(dmatdir)
                                        : new DiskVMatrix(dmatdir);
-            if(precomp_source->getMtime() >= source->getMtime())
+            if(isUpToDate(precomp_source))
                 recompute = false;
         }
 
@@ -161,7 +161,7 @@
         {
             precomp_source = temporary ? new TemporaryFileVMatrix(pmatfile)
                                        : new FileVMatrix(pmatfile);
-            if(isFileUpToDate(pmatfile))
+            if(isUpToDate(pmatfile))
                 recompute = false;
         }
 

Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2008-03-17 16:23:19 UTC (rev 8691)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2008-03-17 16:55:53 UTC (rev 8692)
@@ -327,7 +327,7 @@
     setColumnNamesAndWidth();
 
     // open the index file
-    if(!isFileUpToDate(idxfname))
+    if(!isUpToDate(idxfname))
         buildIdx(); // (re)build it first!
     idxfile = fopen(idxfname.c_str(),"rb");
     if(fgetc(idxfile) != byte_order())

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-03-17 16:23:19 UTC (rev 8691)
+++ trunk/plearn/vmat/VMatrix.cc	2008-03-17 16:55:53 UTC (rev 8692)
@@ -1450,31 +1450,51 @@
 
 void VMatrix::updateMtime(VMat v){if(v)updateMtime(v->getMtime());}
 
-////////////////////
-// isFileUpToDate //
-////////////////////
-bool VMatrix::isFileUpToDate(const PPath& path, bool warning_mtime0,
-                             bool warning_older) const
+////////////////
+// isUpToDate //
+////////////////
+bool VMatrix::isUpToDate(const PPath& path, bool warning_mtime0,
+                         bool warning_older) const
 {
     bool exist = isfile(path);
     bool uptodate = false;
     if(exist)
         uptodate = getMtime() < mtime(path);
     if (warning_mtime0 && exist && uptodate && getMtime()==0)
-        PLWARNING("In VMatrix::isFileUpToDate - for class '%s'"
+        PLWARNING("In VMatrix::isUpToDate - for class '%s'"
                   " File '%s' will be used, but "
                   "this VMat's last modification time is undefined: we cannot "
                   "be sure the file is up-to-date.",
                   classname().c_str(), path.absolute().c_str());
     if(warning_older && exist && !uptodate)
-        PLWARNING("In VMatrix::isFileUpToDate - for class '%s'"
+        PLWARNING("In VMatrix::isUpToDate - for class '%s'"
                   " File '%s' is older than this "
-                  "VMat's mtime of %d, and cannot be re-used.",
+                  "VMat's mtime of %d, and should not be re-used.",
                   classname().c_str(), path.absolute().c_str(), getMtime());
 
     return exist && uptodate;
 }
+////////////////
+// isUpToDate //
+////////////////
+bool VMatrix::isUpToDate(const VMat& vm, bool warning_mtime0,
+                         bool warning_older) const
+{
+    bool uptodate = getMtime() < vm->getMtime();
+    if (warning_mtime0 && uptodate && getMtime()==0)
+        PLWARNING("In VMatrix::isUpToDate - for class '%s'"
+                  " One VMat will be used, but "
+                  "this VMat's last modification time is undefined: we cannot "
+                  "be sure the file is up-to-date.",
+                  classname().c_str());
+    if(warning_older && !uptodate)
+        PLWARNING("In VMatrix::isUpToDate - for class '%s'"
+                  " One VMat with mtime of %d is older than this "
+                  "VMat's with mtime of %d, and should not be re-used.",
+                  classname().c_str(), vm->getMtime(), getMtime());
 
+    return uptodate;
+}
 /////////////////////////////////
 // getPrecomputedStatsFromFile //
 /////////////////////////////////
@@ -1485,7 +1505,7 @@
     PPath metadatadir = getMetaDataDir();
     PPath statsfile =  metadatadir / filename;
     if(!metadatadir.isEmpty()) lockMetaDataDir(); // don't try to lock nothing
-    bool uptodate= isFileUpToDate(statsfile, true, true);
+    bool uptodate= isUpToDate(statsfile, true, true);
     if (uptodate)
         PLearn::load(statsfile, stats);
     else

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-03-17 16:23:19 UTC (rev 8691)
+++ trunk/plearn/vmat/VMatrix.h	2008-03-17 16:55:53 UTC (rev 8692)
@@ -404,9 +404,10 @@
     //! file exists and it is older than this VMat's last modification time.
     //! If 'warning_reuse' is 'true', then a warning will be issued when the
     //! file exists and this VMat's last modification time is undefined.
-    bool isFileUpToDate(const PPath& file, bool warning_mtime0 = true,
-                        bool warning_older = false) const;
-
+    bool isUpToDate(const PPath& file, bool warning_mtime0 = true,
+                    bool warning_older = false) const;
+    bool isUpToDate(const VMat& vm, bool warning_mtime0 = true,
+                    bool warning_older = false) const;
     //#####  Matrix Sizes  ####################################################
 
     /// Return the number of columns in the VMatrix



From tihocan at mail.berlios.de  Mon Mar 17 19:34:43 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 17 Mar 2008 19:34:43 +0100
Subject: [Plearn-commits] r8693 - trunk/plearn/vmat
Message-ID: <200803171834.m2HIYhhV026601@sheep.berlios.de>

Author: tihocan
Date: 2008-03-17 19:34:43 +0100 (Mon, 17 Mar 2008)
New Revision: 8693

Modified:
   trunk/plearn/vmat/VMatrix.h
Log:
Updated comment to reflect change in parameter name. Yes, it means you need to recompile 95% of PLearn!

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-03-17 16:55:53 UTC (rev 8692)
+++ trunk/plearn/vmat/VMatrix.h	2008-03-17 18:34:43 UTC (rev 8693)
@@ -400,10 +400,10 @@
 
     //! Return 'true' iff 'file' was last modified after this VMat, or this
     //! VMat's last modification time is undefined (set to 0).
+    //! If 'warning_mtime0' is 'true', then a warning will be issued when the
+    //! file exists and this VMat's last modification time is undefined.
     //! If 'warning_older' is 'true', then a warning will be issued when the
     //! file exists and it is older than this VMat's last modification time.
-    //! If 'warning_reuse' is 'true', then a warning will be issued when the
-    //! file exists and this VMat's last modification time is undefined.
     bool isUpToDate(const PPath& file, bool warning_mtime0 = true,
                     bool warning_older = false) const;
     bool isUpToDate(const VMat& vm, bool warning_mtime0 = true,



From tihocan at mail.berlios.de  Mon Mar 17 20:01:32 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 17 Mar 2008 20:01:32 +0100
Subject: [Plearn-commits] r8694 - trunk/plearn/vmat
Message-ID: <200803171901.m2HJ1Wjg029759@sheep.berlios.de>

Author: tihocan
Date: 2008-03-17 20:01:30 +0100 (Mon, 17 Mar 2008)
New Revision: 8694

Modified:
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
Added comment for VMatrix::isUpToDate, and modified implementation to return true and display a warning when some modification date is undefined, to be coherent with the file implementation


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-03-17 18:34:43 UTC (rev 8693)
+++ trunk/plearn/vmat/VMatrix.cc	2008-03-17 19:01:30 UTC (rev 8694)
@@ -1474,27 +1474,33 @@
 
     return exist && uptodate;
 }
+
 ////////////////
 // isUpToDate //
 ////////////////
-bool VMatrix::isUpToDate(const VMat& vm, bool warning_mtime0,
+bool VMatrix::isUpToDate(VMat vm, bool warning_mtime0,
                          bool warning_older) const
 {
-    bool uptodate = getMtime() < vm->getMtime();
-    if (warning_mtime0 && uptodate && getMtime()==0)
+    time_t my_time = getMtime();
+    time_t vm_time = vm->getMtime();
+    bool uptodate = getMtime() < vm->getMtime() ||
+                    my_time == 0                ||
+                    vm_time == 0;
+    if (warning_mtime0 && uptodate && (my_time == 0 || vm_time == 0))
         PLWARNING("In VMatrix::isUpToDate - for class '%s'"
-                  " One VMat will be used, but "
-                  "this VMat's last modification time is undefined: we cannot "
-                  "be sure the file is up-to-date.",
+                  " When comparing the VMats' last modification times, at "
+                  "least one was found to be undefined: we cannot be sure "
+                  "the VMat is up-to-date.",
                   classname().c_str());
     if(warning_older && !uptodate)
         PLWARNING("In VMatrix::isUpToDate - for class '%s'"
-                  " One VMat with mtime of %d is older than this "
+                  " The VMat with mtime of %d is older than this "
                   "VMat's with mtime of %d, and should not be re-used.",
                   classname().c_str(), vm->getMtime(), getMtime());
 
     return uptodate;
 }
+
 /////////////////////////////////
 // getPrecomputedStatsFromFile //
 /////////////////////////////////

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-03-17 18:34:43 UTC (rev 8693)
+++ trunk/plearn/vmat/VMatrix.h	2008-03-17 19:01:30 UTC (rev 8694)
@@ -406,8 +406,17 @@
     //! file exists and it is older than this VMat's last modification time.
     bool isUpToDate(const PPath& file, bool warning_mtime0 = true,
                     bool warning_older = false) const;
-    bool isUpToDate(const VMat& vm, bool warning_mtime0 = true,
+
+    //! Return 'true' iff 'vm' was last modified after this VMat, this VMat's
+    //! last modification time is undefined (set to 0), or vm's last
+    //! modification time is undefined.
+    //! If 'warning_mtime0' is 'true', then a warning will be issued when one
+    //! of the VMats' last modification time is undefined.
+    //! If 'warning_older' is 'true', then a warning will be issued when 'vm'
+    //! is older than this VMat.
+    bool isUpToDate(VMat vm, bool warning_mtime0 = true,
                     bool warning_older = false) const;
+
     //#####  Matrix Sizes  ####################################################
 
     /// Return the number of columns in the VMatrix



From nouiz at mail.berlios.de  Mon Mar 17 20:07:59 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 17 Mar 2008 20:07:59 +0100
Subject: [Plearn-commits] r8695 - trunk/plearn/vmat
Message-ID: <200803171907.m2HJ7xpu030649@sheep.berlios.de>

Author: nouiz
Date: 2008-03-17 20:07:59 +0100 (Mon, 17 Mar 2008)
New Revision: 8695

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
very small optimisation


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-03-17 19:01:30 UTC (rev 8694)
+++ trunk/plearn/vmat/VMatrix.cc	2008-03-17 19:07:59 UTC (rev 8695)
@@ -1483,8 +1483,8 @@
 {
     time_t my_time = getMtime();
     time_t vm_time = vm->getMtime();
-    bool uptodate = getMtime() < vm->getMtime() ||
-                    my_time == 0                ||
+    bool uptodate = my_time < vm_time ||
+                    my_time == 0      ||
                     vm_time == 0;
     if (warning_mtime0 && uptodate && (my_time == 0 || vm_time == 0))
         PLWARNING("In VMatrix::isUpToDate - for class '%s'"



From saintmlx at mail.berlios.de  Mon Mar 17 20:43:51 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Mon, 17 Mar 2008 20:43:51 +0100
Subject: [Plearn-commits] r8696 - trunk/python_modules/plearn/pyext
Message-ID: <200803171943.m2HJhpm9000700@sheep.berlios.de>

Author: saintmlx
Date: 2008-03-17 20:43:50 +0100 (Mon, 17 Mar 2008)
New Revision: 8696

Modified:
   trunk/python_modules/plearn/pyext/__init__.py
Log:
- don't automatically handle +gui option in pyext; should be called explicitly when needed (i.e. after declaring plnamespaces, which can be declared after importing pyext)



Modified: trunk/python_modules/plearn/pyext/__init__.py
===================================================================
--- trunk/python_modules/plearn/pyext/__init__.py	2008-03-17 19:07:59 UTC (rev 8695)
+++ trunk/python_modules/plearn/pyext/__init__.py	2008-03-17 19:43:50 UTC (rev 8696)
@@ -75,20 +75,21 @@
         return [ content[i*ncols:(i+1)*ncols] for i in range(nrows) ]
 
 
-from plearn.utilities.options_dialog import *
-verb, logs, namespaces, use_gui= getGuiInfo(sys.argv)
+def optDialog():
+    from plearn.utilities import options_dialog as od
+    verb, logs, namespaces, use_gui= od.getGuiInfo(sys.argv)
 
-# Enact the use of plargs: the current behavior is to consider as a plargs
-# any command-line argument that contains a '=' char and to neglect all
-# others
-plargs.parse([ arg for arg in sys.argv if arg.find('=') != -1 ])
+    # Enact the use of plargs: the current behavior is to consider as a plargs
+    # any command-line argument that contains a '=' char and to neglect all
+    # others
+    plargs.parse([ arg for arg in sys.argv if arg.find('=') != -1 ])
 
-if use_gui:
-    runit, verb, logs= optionsDialog(sys.argv[0], plargs.expdir,
-                                     verb, logs, namespaces)
-    if not runit:
-        sys.exit()
-    loggingControl(verb, logs)
+    if use_gui:
+        runit, verb, logs= od.optionsDialog(sys.argv[0], plargs.expdir,
+                                            verb, logs, namespaces)
+        if not runit:
+            sys.exit()
+        loggingControl(verb, logs)
 
 pl.AutoVMatrix()
 AutoVMatrix.__len__ = lambda self: self.length



From nouiz at mail.berlios.de  Mon Mar 17 20:58:24 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 17 Mar 2008 20:58:24 +0100
Subject: [Plearn-commits] r8697 - trunk/plearn/vmat
Message-ID: <200803171958.m2HJwOrJ002396@sheep.berlios.de>

Author: nouiz
Date: 2008-03-17 20:58:23 +0100 (Mon, 17 Mar 2008)
New Revision: 8697

Modified:
   trunk/plearn/vmat/GaussianizeVMatrix.cc
   trunk/plearn/vmat/MissingInstructionVMatrix.cc
Log:
implemented makeDeepCopyFromShallowCopy


Modified: trunk/plearn/vmat/GaussianizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-03-17 19:43:50 UTC (rev 8696)
+++ trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-03-17 19:58:23 UTC (rev 8697)
@@ -284,15 +284,7 @@
 void GaussianizeVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
-
-    // ### Call deepCopyField on all "pointer-like" fields
-    // ### that you wish to be deepCopied rather than
-    // ### shallow-copied.
-    // ### ex:
-    // deepCopyField(trainvec, copies);
-
-    // ### Remove this line when you have fully implemented this method.
-    PLERROR("GaussianizeVMatrix::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
+    deepCopyField(train_source, copies);
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn/vmat/MissingInstructionVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MissingInstructionVMatrix.cc	2008-03-17 19:43:50 UTC (rev 8696)
+++ trunk/plearn/vmat/MissingInstructionVMatrix.cc	2008-03-17 19:58:23 UTC (rev 8697)
@@ -231,15 +231,7 @@
 void MissingInstructionVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
-
-    // ### Call deepCopyField on all "pointer-like" fields
-    // ### that you wish to be deepCopied rather than
-    // ### shallow-copied.
-    // ### ex:
-    // deepCopyField(trainvec, copies);
-
-    // ### Remove this line when you have fully implemented this method.
-    PLERROR("MissingInstructionVMatrix::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
+    deepCopyField(missing_instructions, copies);
 }
 
 } // end of namespace PLearn



From lamblin at mail.berlios.de  Mon Mar 17 22:09:56 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Mon, 17 Mar 2008 22:09:56 +0100
Subject: [Plearn-commits] r8698 - trunk/plearn_learners/online
Message-ID: <200803172109.m2HL9uOq008957@sheep.berlios.de>

Author: lamblin
Date: 2008-03-17 22:09:56 +0100 (Mon, 17 Mar 2008)
New Revision: 8698

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
Log:
Change the way the training stats are computed. You can now use
"train_stats_window" to choose on how many training samples they will be
computed. The default behaviour is the number of samples in the training
set.


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2008-03-17 19:58:23 UTC (rev 8697)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2008-03-17 21:09:56 UTC (rev 8698)
@@ -75,6 +75,7 @@
     background_gibbs_update_ratio(0),
     gibbs_chain_reinit_freq( INT_MAX ),
     use_mean_field_contrastive_divergence( false ),
+    train_stats_window( -1 ),
     minibatch_size( 0 ),
     initialize_gibbs_chain( false ),
     final_module_has_learning_rate( false ),
@@ -286,6 +287,12 @@
                   "Indication that mean-field contrastive divergence\n"
                   "should be used instead of standard contrastive divergence.\n");
 
+    declareOption(ol, "train_stats_window",
+                  &DeepBeliefNet::train_stats_window,
+                  OptionBase::buildoption,
+                  "The number of samples to use to compute training stats.\n"
+                  "-1 (default) means the number of training samples.\n");
+
     declareOption(ol, "top_layer_joint_cd", &DeepBeliefNet::top_layer_joint_cd,
                   OptionBase::buildoption,
                   "Wether we do a step of joint contrastive divergence on"
@@ -848,6 +855,10 @@
         << ", target nstages = " << nstages << endl;
 
     PLASSERT( train_set );
+    int n_train_stats_samples = (train_stats_window >= 0)
+        ? train_stats_window
+        : train_set->length();
+
     if (stage == 0) {
         // Training set-dependent initialization.
         minibatch_size = batch_size > 0 ? batch_size : train_set->length();
@@ -901,27 +912,43 @@
                                   nstages - stage );
 
         setLearningRate( grad_learning_rate );
-        for( ; stage<nstages; stage++)
+        train_stats->forget();
+
+        for( ; stage<nstages; stage+=minibatch_size)
         {
             initialize_gibbs_chain=(stage%gibbs_chain_reinit_freq==0);
+
             // Do a step every 'minibatch_size' examples.
-            if (stage % minibatch_size == 0) {
-                int sample_start = stage % nsamples;
-                if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
-                    setLearningRate( grad_learning_rate
-                                     / (1. + grad_decrease_ct * stage ));
-                if (batch_size > 1 || minibatch_hack) {
-                    train_set->getExamples(sample_start, minibatch_size,
-                                           inputs, targets, weights, NULL, true);
-                    train_costs_m.fill(MISSING_VALUE);
-                    if (reconstruct_layerwise)
-                        train_costs_m.column(reconstruction_cost_index).clear();
-                    onlineStep( inputs, targets, train_costs_m );
-                } else {
-                    train_set->getExample(sample_start, input, target, weight);
-                    onlineStep( input, target, train_costs );
-                }
+            int sample_start = stage % nsamples;
+            if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
+                setLearningRate( grad_learning_rate
+                                 / (1. + grad_decrease_ct * stage ));
+
+            if (batch_size > 1 || minibatch_hack)
+            {
+                train_set->getExamples(sample_start, minibatch_size,
+                                       inputs, targets, weights, NULL, true);
+                train_costs_m.fill(MISSING_VALUE);
+
+                if (reconstruct_layerwise)
+                    train_costs_m.column(reconstruction_cost_index).clear();
+
+                onlineStep( inputs, targets, train_costs_m );
             }
+            else
+            {
+                train_set->getExample(sample_start, input, target, weight);
+                onlineStep( input, target, train_costs );
+            }
+
+            // Update stats if we are in the last n_train_stats_samples
+            if (stage >= nstages - n_train_stats_samples)
+                if (minibatch_size > 1 || minibatch_hack)
+                    for (int k = 0; k < minibatch_size; k++)
+                        train_stats->update(train_costs_m(k));
+                else
+                    train_stats->update(train_costs);
+
             if( pb )
                 pb->update( stage + 1 );
         }
@@ -955,17 +982,17 @@
 
             for( ; stage<end_stage ; stage++ )
             {
-                 if( !fast_exact_is_equal( cd_decrease_ct, 0. ) )
-                 {
-                     real lr = cd_learning_rate 
-                         / (1. + cd_decrease_ct * 
-                            (stage - cumulative_schedule[i]));
-                     
-                     layers[i]->setLearningRate( lr );
-                     connections[i]->setLearningRate( lr );
-                     layers[i+1]->setLearningRate( lr );
-                 }
-                 
+                if( !fast_exact_is_equal( cd_decrease_ct, 0. ) )
+                {
+                    real lr = cd_learning_rate
+                        / (1. + cd_decrease_ct *
+                           (stage - cumulative_schedule[i]));
+
+                    layers[i]->setLearningRate( lr );
+                    connections[i]->setLearningRate( lr );
+                    layers[i+1]->setLearningRate( lr );
+                }
+
                 initialize_gibbs_chain=(stage%gibbs_chain_reinit_freq==0);
                 // Do a step every 'minibatch_size' examples.
                 if (stage % minibatch_size == 0) {
@@ -1016,8 +1043,8 @@
             {
                 if( !fast_exact_is_equal( cd_decrease_ct, 0. ) )
                 {
-                    real lr = cd_learning_rate / 
-                        (1. + cd_decrease_ct * 
+                    real lr = cd_learning_rate /
+                        (1. + cd_decrease_ct *
                          (stage - cumulative_schedule[n_layers-2]));
                     joint_layer->setLearningRate( lr );
                     classification_module->joint_connection->setLearningRate( lr );
@@ -1057,29 +1084,29 @@
             MODULE_LOG << "  up_down_stage = " << up_down_stage << endl;
             MODULE_LOG << "  up_down_nstages = " << up_down_nstages << endl;
             MODULE_LOG << "  up_down_learning_rate = " << up_down_learning_rate << endl;
-            
+
             int init_stage = up_down_stage;
             if( report_progress )
                 pb = new ProgressBar( "Up-down gradient descent algorithm "
                                       + classname(),
                                       up_down_nstages - init_stage );
-            
+
             setLearningRate( up_down_learning_rate );
 
             train_stats->forget();
             int sample_start;
             for( ; up_down_stage<up_down_nstages ; up_down_stage++ )
             {
-                sample_start = up_down_stage % nsamples;                
+                sample_start = up_down_stage % nsamples;
                 if( !fast_exact_is_equal( up_down_decrease_ct, 0. ) )
                     setLearningRate( up_down_learning_rate
-                                     / (1. + up_down_decrease_ct * 
+                                     / (1. + up_down_decrease_ct *
                                         up_down_stage) );
-                
+
                 train_set->getExample( sample_start, input, target, weight );
                 upDownStep( input, target, train_costs );
                 train_stats->update( train_costs );
-                
+
                 if( pb )
                     pb->update( up_down_stage - init_stage + 1 );
             }
@@ -1101,41 +1128,38 @@
                                   end_stage - init_stage );
 
         setLearningRate( grad_learning_rate );
+        train_stats->forget();
 
-        train_stats->forget();
-        bool update_stats = false;
-        for( ; stage<end_stage ; stage++ )
+        for( ; stage<end_stage ; stage+=minibatch_size )
         {
+            int sample_start = stage % nsamples;
 
-            // Update every 'minibatch_size' samples.
-            if (stage % minibatch_size == 0) {
-                int sample_start = stage % nsamples;
-                // Only update train statistics for the last 'epoch', i.e. last
-                // 'nsamples' seen.
-                update_stats = update_stats || stage >= end_stage - nsamples;
+            if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
+                setLearningRate( grad_learning_rate
+                        / (1. + grad_decrease_ct *
+                           (stage - cumulative_schedule[n_layers-1])) );
 
-                if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
-                    setLearningRate( grad_learning_rate
-                            / (1. + grad_decrease_ct * 
-                               (stage - cumulative_schedule[n_layers-1])) );
+            if (minibatch_size > 1 || minibatch_hack)
+            {
+                train_set->getExamples(sample_start, minibatch_size, inputs,
+                        targets, weights, NULL, true);
+                train_costs_m.fill(MISSING_VALUE);
+                fineTuningStep(inputs, targets, train_costs_m);
+            }
+            else
+            {
+                train_set->getExample( sample_start, input, target, weight );
+                fineTuningStep( input, target, train_costs );
+            }
 
-                if (minibatch_size > 1 || minibatch_hack) {
-                    train_set->getExamples(sample_start, minibatch_size, inputs,
-                            targets, weights, NULL, true);
-                    train_costs_m.fill(MISSING_VALUE);
-                    fineTuningStep(inputs, targets, train_costs_m);
-                } else {
-                    train_set->getExample( sample_start, input, target, weight );
-                    fineTuningStep( input, target, train_costs );
-                }
-                if (update_stats)
-                    if (minibatch_size > 1 || minibatch_hack)
-                        for (int k = 0; k < minibatch_size; k++)
-                            train_stats->update(train_costs_m(k));
-                    else
-                        train_stats->update( train_costs );
+            // Update stats if we are in the last n_train_stats_samples samples
+            if (stage >= end_stage - n_train_stats_samples)
+                if (minibatch_size > 1 || minibatch_hack)
+                    for (int k = 0; k < minibatch_size; k++)
+                        train_stats->update(train_costs_m(k));
+                else
+                    train_stats->update(train_costs);
 
-            }
             if( pb )
                 pb->update( stage - init_stage + 1 );
         }
@@ -1154,11 +1178,11 @@
     if (verbosity > 1)
         cout << "The cumulative time spent in train() up until now is " << cumulative_training_time << " cpu seconds" << endl;
 
-    train_costs_m.column(training_cpu_time_cost_index).fill(cpu_time);
-    train_costs_m.column(cumulative_training_time_cost_index).fill(cumulative_training_time);
-    train_stats->update( train_costs_m );
+    train_costs.fill(MISSING_VALUE);
+    train_costs[training_cpu_time_cost_index] = cpu_time;
+    train_costs[cumulative_training_time_cost_index] = cumulative_training_time;
+    train_stats->update( train_costs );
     train_stats->finalize();
-
 }
 
 ////////////////

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2008-03-17 19:58:23 UTC (rev 8697)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2008-03-17 21:09:56 UTC (rev 8698)
@@ -75,7 +75,7 @@
     //! The learning rate used in the up-down algorithm during the 
     //! unsupervised fine tuning gradient descent
     real up_down_learning_rate;
-    
+
     //! The decrease constant of the learning rate used in the
     //! up-down algorithm during the unsupervised fine tuning gradient descent
     real up_down_decrease_ct;
@@ -197,6 +197,10 @@
     //! should be used instead of standard contrastive divergence.
     bool use_mean_field_contrastive_divergence;
 
+    //! The number of samples to use to compute training stats.
+    //! -1 (default) means the number of training samples.
+    int train_stats_window;
+
     //#####  Not Options  #####################################################
 
     //! Timer for monitoring the speed



From lamblin at mail.berlios.de  Tue Mar 18 05:26:41 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 18 Mar 2008 05:26:41 +0100
Subject: [Plearn-commits] r8699 - trunk/plearn_learners/online
Message-ID: <200803180426.m2I4Qfg3031314@sheep.berlios.de>

Author: lamblin
Date: 2008-03-18 05:26:39 +0100 (Tue, 18 Mar 2008)
New Revision: 8699

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
Log:
Makes the test pass again.


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2008-03-17 21:09:56 UTC (rev 8698)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2008-03-18 04:26:39 UTC (rev 8699)
@@ -914,41 +914,44 @@
         setLearningRate( grad_learning_rate );
         train_stats->forget();
 
-        for( ; stage<nstages; stage+=minibatch_size)
+        for( ; stage < nstages; stage++)
         {
             initialize_gibbs_chain=(stage%gibbs_chain_reinit_freq==0);
 
             // Do a step every 'minibatch_size' examples.
-            int sample_start = stage % nsamples;
-            if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
-                setLearningRate( grad_learning_rate
-                                 / (1. + grad_decrease_ct * stage ));
-
-            if (batch_size > 1 || minibatch_hack)
+            if (stage % minibatch_size == 0)
             {
-                train_set->getExamples(sample_start, minibatch_size,
-                                       inputs, targets, weights, NULL, true);
-                train_costs_m.fill(MISSING_VALUE);
+                int sample_start = stage % nsamples;
+                if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
+                    setLearningRate( grad_learning_rate
+                                     / (1. + grad_decrease_ct * stage ));
 
-                if (reconstruct_layerwise)
-                    train_costs_m.column(reconstruction_cost_index).clear();
+                if (batch_size > 1 || minibatch_hack)
+                {
+                    train_set->getExamples(sample_start, minibatch_size,
+                                           inputs, targets, weights, NULL, true);
+                    train_costs_m.fill(MISSING_VALUE);
 
-                onlineStep( inputs, targets, train_costs_m );
-            }
-            else
-            {
-                train_set->getExample(sample_start, input, target, weight);
-                onlineStep( input, target, train_costs );
-            }
+                    if (reconstruct_layerwise)
+                        train_costs_m.column(reconstruction_cost_index).clear();
 
-            // Update stats if we are in the last n_train_stats_samples
-            if (stage >= nstages - n_train_stats_samples)
-                if (minibatch_size > 1 || minibatch_hack)
-                    for (int k = 0; k < minibatch_size; k++)
-                        train_stats->update(train_costs_m(k));
+                    onlineStep( inputs, targets, train_costs_m );
+                }
                 else
-                    train_stats->update(train_costs);
+                {
+                    train_set->getExample(sample_start, input, target, weight);
+                    onlineStep( input, target, train_costs );
+                }
 
+                // Update stats if we are in the last n_train_stats_samples
+                if (stage >= nstages - n_train_stats_samples)
+                    if (minibatch_size > 1 || minibatch_hack)
+                        for (int k = 0; k < minibatch_size; k++)
+                            train_stats->update(train_costs_m(k));
+                    else
+                        train_stats->update(train_costs);
+            }
+
             if( pb )
                 pb->update( stage + 1 );
         }
@@ -1130,36 +1133,39 @@
         setLearningRate( grad_learning_rate );
         train_stats->forget();
 
-        for( ; stage<end_stage ; stage+=minibatch_size )
+        for( ; stage < end_stage; stage++)
         {
-            int sample_start = stage % nsamples;
+            if (stage % minibatch_size == 0)
+            {
+                int sample_start = stage % nsamples;
 
-            if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
-                setLearningRate( grad_learning_rate
-                        / (1. + grad_decrease_ct *
-                           (stage - cumulative_schedule[n_layers-1])) );
+                if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
+                    setLearningRate( grad_learning_rate
+                            / (1. + grad_decrease_ct *
+                               (stage - cumulative_schedule[n_layers-1])) );
 
-            if (minibatch_size > 1 || minibatch_hack)
-            {
-                train_set->getExamples(sample_start, minibatch_size, inputs,
-                        targets, weights, NULL, true);
-                train_costs_m.fill(MISSING_VALUE);
-                fineTuningStep(inputs, targets, train_costs_m);
-            }
-            else
-            {
-                train_set->getExample( sample_start, input, target, weight );
-                fineTuningStep( input, target, train_costs );
-            }
-
-            // Update stats if we are in the last n_train_stats_samples samples
-            if (stage >= end_stage - n_train_stats_samples)
                 if (minibatch_size > 1 || minibatch_hack)
-                    for (int k = 0; k < minibatch_size; k++)
-                        train_stats->update(train_costs_m(k));
+                {
+                    train_set->getExamples(sample_start, minibatch_size, inputs,
+                            targets, weights, NULL, true);
+                    train_costs_m.fill(MISSING_VALUE);
+                    fineTuningStep(inputs, targets, train_costs_m);
+                }
                 else
-                    train_stats->update(train_costs);
+                {
+                    train_set->getExample( sample_start, input, target, weight );
+                    fineTuningStep( input, target, train_costs );
+                }
 
+                // Update stats if we are in the last n_train_stats_samples samples
+                if (stage >= end_stage - n_train_stats_samples)
+                    if (minibatch_size > 1 || minibatch_hack)
+                        for (int k = 0; k < minibatch_size; k++)
+                            train_stats->update(train_costs_m(k));
+                    else
+                        train_stats->update(train_costs);
+                }
+
             if( pb )
                 pb->update( stage - init_stage + 1 );
         }



From nouiz at mail.berlios.de  Tue Mar 18 16:21:30 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 18 Mar 2008 16:21:30 +0100
Subject: [Plearn-commits] r8700 - trunk/plearn/vmat
Message-ID: <200803181521.m2IFLUMF000435@sheep.berlios.de>

Author: nouiz
Date: 2008-03-18 16:21:30 +0100 (Tue, 18 Mar 2008)
New Revision: 8700

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
Log:
In TextFilesVMatrix, we do not deep copy the FILE object, instead, we recreate them. We need to do this as, we can't deep copy a FILE object, nor we should share it between object.


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2008-03-18 04:26:39 UTC (rev 8699)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2008-03-18 15:21:30 UTC (rev 8700)
@@ -82,6 +82,7 @@
 
 void TextFilesVMatrix::getFileAndPos(int i, unsigned char& fileno, int& pos) const
 {
+    PLASSERT(idxfile!=0);
     if(i<0 || i>=length())
         PLERROR("TextFilesVMatrix::getFileAndPos out of range row %d (only %d rows)", i, length());
     fseek(idxfile, 5+i*5, SEEK_SET);
@@ -862,6 +863,11 @@
 void TextFilesVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
+    idxfile=0;
+    txtfiles.resize(0);
+    //should be already build.
+    auto_build_map=false;
+    build();
 }
 
 } // end of namespace PLearn



From tihocan at mail.berlios.de  Tue Mar 18 17:48:51 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 18 Mar 2008 17:48:51 +0100
Subject: [Plearn-commits] r8701 - trunk/python_modules/plearn/pymake
Message-ID: <200803181648.m2IGmpEM009696@sheep.berlios.de>

Author: tihocan
Date: 2008-03-18 17:48:50 +0100 (Tue, 18 Mar 2008)
New Revision: 8701

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
Fixed support for multi-file compilation, that had been broken at some point it seems

Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-03-18 15:21:30 UTC (rev 8700)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-03-18 16:48:50 UTC (rev 8701)
@@ -1042,13 +1042,15 @@
 def sequential_link(executables_to_link, linkname):
     """executables_to_link is a list of FileInfo of .cc files that contain a main()
     and whose corresponding executable should be made"""
+    link_exit_code = 0
     for ccfile in executables_to_link:
         failures =  ccfile.failed_ccfiles_to_link()
         if failures:
             print '[ Executable target',ccfile.filebase,'not remade because of previous compilation errors. ]'
             print '   Errors were while compiling files:'
             print '   '+string.join(failures,'\n   ')
-            return 1
+            if link_exit_code == 0:
+                link_exit_code = 1
         else:
             if not local_compilation:
                 print 'Waiting for NFS to catch up...'
@@ -1057,13 +1059,15 @@
                 copy_ofiles_locally(executables_to_link)
             if verbose>=2:
                 print '[ LINKING',ccfile.filebase,']'
-            link_exit_code = ccfile.launch_linking()
+            new_link_exit_code = ccfile.launch_linking()
+            if link_exit_code == 0:
+                link_exit_code = new_link_exit_code
             if create_so or create_pyso:
                 so_filename = os.path.basename(ccfile.corresponding_output)
                 ccfile.make_symbolic_link(so_filename, so_filename)
             else:
                 ccfile.make_symbolic_link(linkname)
-            return link_exit_code
+    return link_exit_code
 
 def sequential_dll(target_file_info):
     """target_file_info is a FileInfo of .cc file whose corresponding dll should be made"""
@@ -2759,6 +2763,8 @@
 
     ######  The compilation and linking
 
+    return_code = 0
+
     for target in otherargs:
         configpath = get_config_path(target)
         execfile( configpath, globals() )
@@ -2814,15 +2820,20 @@
                 print '++++ Linking', string.join(map(lambda x: x.filebase, executables_to_link.keys()))
                 ret = sequential_link(executables_to_link.keys(),linkname)
                 if ret != 0:
-                    sys.exit(ret)
+                    return_code = ret
+                    continue    # Move on to next file.
 
             if create_dll:
                 print '++++ Creating DLL of', target
                 ret = sequential_dll(file_info(target))
                 if ret != 0:
-                    sys.exit(ret)
+                    return_code = ret
+                    continue    # Move on to next file.
 
             if temp_objs:
                 os.system('chmod 777 --silent '+objsdir+'/*')
                 mychmod(objsdir,33279)
 
+    if return_code != 0:
+        sys.exit(return_code)
+



From nouiz at mail.berlios.de  Tue Mar 18 20:25:09 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 18 Mar 2008 20:25:09 +0100
Subject: [Plearn-commits] r8702 - trunk/plearn_learners/regressors
Message-ID: <200803181925.m2IJP92L016401@sheep.berlios.de>

Author: nouiz
Date: 2008-03-18 20:25:08 +0100 (Tue, 18 Mar 2008)
New Revision: 8702

Modified:
   trunk/plearn_learners/regressors/RegressionTree.cc
Log:
bugfix, in case we have multiclass_output.size()>0, it was not rounding the output


Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2008-03-18 16:48:50 UTC (rev 8701)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2008-03-18 19:25:08 UTC (rev 8702)
@@ -382,7 +382,7 @@
     PLASSERT(costsv.size()==nTestCosts());
     PLASSERT(nodes);
     nodes->resize(0);
-    root->computeOutputAndNodes(inputv, outputv, nodes);
+    computeOutputAndNodes(inputv, outputv, nodes);
     costsv.clear();
     costsv[0] = pow((outputv[0] - targetv[0]), 2);
     costsv[1] = outputv[1];



From nouiz at mail.berlios.de  Tue Mar 18 20:49:39 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 18 Mar 2008 20:49:39 +0100
Subject: [Plearn-commits] r8703 - trunk/plearn/vmat
Message-ID: <200803181949.m2IJndIq021024@sheep.berlios.de>

Author: nouiz
Date: 2008-03-18 20:49:38 +0100 (Tue, 18 Mar 2008)
New Revision: 8703

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
better PLERROR() message


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-03-18 19:25:08 UTC (rev 8702)
+++ trunk/plearn/vmat/VMatrix.cc	2008-03-18 19:49:38 UTC (rev 8703)
@@ -789,7 +789,8 @@
         case 2: current_fieldinfos[i] = VMField(v[0], VMField::FieldType(toint(v[1]))); break;
         default: PLERROR("In VMatrix::getSavedFieldInfos Format not recognized in file %s.\n"
                          "Each line should be '<name> {<type>}'.\n"
-                         "Got: '%s'",filename.absolute().c_str(),line.c_str());
+                         "Got: '%s'. Check for a space in <name>",
+                         filename.absolute().c_str(),line.c_str());
         }
     }
     return current_fieldinfos;



From nouiz at mail.berlios.de  Wed Mar 19 18:23:31 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 19 Mar 2008 18:23:31 +0100
Subject: [Plearn-commits] r8704 - trunk/plearn/vmat
Message-ID: <200803191723.m2JHNVSV002494@sheep.berlios.de>

Author: nouiz
Date: 2008-03-19 18:23:28 +0100 (Wed, 19 Mar 2008)
New Revision: 8704

Modified:
   trunk/plearn/vmat/GaussianizeVMatrix.cc
Log:
optimisation


Modified: trunk/plearn/vmat/GaussianizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-03-18 19:49:38 UTC (rev 8703)
+++ trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-03-19 17:23:28 UTC (rev 8704)
@@ -179,6 +179,10 @@
                 TVec<int>(col, col + the_source->extrasize() - 1, 1));
     col += the_source->extrasize();
 
+    //to save the stats their must be a metadatadir
+    if(!the_source->hasMetaDataDir() && hasMetaDataDir())
+        the_source->setMetaDataDir(getMetaDataDir()+"train_source");
+
     TVec<StatsCollector> stats = the_source->
         getPrecomputedStatsFromFile("stats_gaussianizeVMatrix.psave", -1, true);
 



From lamblin at mail.berlios.de  Thu Mar 20 01:34:38 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Thu, 20 Mar 2008 01:34:38 +0100
Subject: [Plearn-commits] r8705 - trunk/plearn_learners/online
Message-ID: <200803200034.m2K0YcJ1020297@sheep.berlios.de>

Author: lamblin
Date: 2008-03-20 01:34:37 +0100 (Thu, 20 Mar 2008)
New Revision: 8705

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/ScaleGradientModule.cc
   trunk/plearn_learners/online/ScaleGradientModule.h
Log:
Support of stochastic (non-minibatch) fprop and bpropUpdate


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2008-03-19 17:23:28 UTC (rev 8704)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2008-03-20 00:34:37 UTC (rev 8705)
@@ -2461,15 +2461,15 @@
     if( i_output_layer>=n_layers-2 && (!use_classification_cost && !final_module))
     {
         //! We haven't computed the expectations of the top layer
-	if(i_output_layer==n_layers-1)
-	{
+        if(i_output_layer==n_layers-1)
+        {
             connections[ n_layers-2 ]->setAsDownInput(layers[ n_layers-2 ]->expectation );
             layers[ n_layers-1 ]->getAllActivations( connections[ n_layers-2 ] );
             layers[ n_layers-1 ]->computeExpectation();
         }
         output.resize(outputsize());
         output << layers[ i_output_layer ]->expectation;
-    }	    
+    }
 
     if( use_classification_cost )
         classification_module->fprop( layers[ n_layers-2 ]->expectation,

Modified: trunk/plearn_learners/online/ScaleGradientModule.cc
===================================================================
--- trunk/plearn_learners/online/ScaleGradientModule.cc	2008-03-19 17:23:28 UTC (rev 8704)
+++ trunk/plearn_learners/online/ScaleGradientModule.cc	2008-03-20 00:34:37 UTC (rev 8705)
@@ -104,6 +104,12 @@
     outputs << inputs;
 }
 
+void ScaleGradientModule::fprop(const Vec& input, Vec& output) const
+{
+    output.resize(input.size());
+    output << input;
+}
+
 /////////////////
 // bpropUpdate //
 /////////////////
@@ -112,11 +118,12 @@
                                       const Mat& output_gradients,
                                       bool accumulate)
 {
-    input_gradients.resize(output_gradients.length(),
-                           output_gradients.width());
-
     if (accumulate)
     {
+        PLASSERT_MSG( input_gradients.length() == output_gradients.length()
+                      && input_gradients.width() == output_gradients.width(),
+                      "Cannot accumulate into input_gradients and resize it" );
+
         if (scale == 0)
             return;
         else // input_gradients += scale * output_gradients
@@ -124,6 +131,9 @@
     }
     else
     {
+        input_gradients.resize(output_gradients.length(),
+                               output_gradients.width());
+
         if (scale == 0)
             input_gradients.clear();
         else // input_gradients = scale * output_gradients
@@ -131,6 +141,30 @@
     }
 }
 
+void ScaleGradientModule::bpropUpdate(const Vec& input, const Vec& output,
+                                      Vec& input_gradient,
+                                      const Vec& output_gradient,
+                                      bool accumulate)
+{
+    if (accumulate)
+    {
+        PLASSERT_MSG( input_gradient.size() == output_gradient.size(),
+                      "Cannot accumulate into input_gradient and resize it" );
+        if (scale == 0)
+            return;
+        else // input_gradient += scale * output_gradients
+            multiplyAcc(input_gradient, output_gradient, scale);
+    }
+    else
+    {
+        input_gradient.resize(output_gradient.size());
+        if (scale == 0)
+            input_gradient.clear();
+        else // input_gradient = scale * output_gradients
+            multiply(output_gradient, scale, input_gradient);
+    }
+}
+
 void ScaleGradientModule::setLearningRate(real the_learning_rate)
 {
     // pass;

Modified: trunk/plearn_learners/online/ScaleGradientModule.h
===================================================================
--- trunk/plearn_learners/online/ScaleGradientModule.h	2008-03-19 17:23:28 UTC (rev 8704)
+++ trunk/plearn_learners/online/ScaleGradientModule.h	2008-03-20 00:34:37 UTC (rev 8705)
@@ -67,14 +67,19 @@
     // Your other public member functions go here
 
     //! Given a batch of inputs, compute the outputs
-    //! SOON TO BE DEPRECATED, USE fprop(const TVec<Mat*>& ports_value)
     virtual void fprop(const Mat& inputs, Mat& outputs);
+    virtual void fprop(const Vec& input, Vec& output) const;
 
     virtual void bpropUpdate(const Mat& inputs, const Mat& outputs,
                              Mat& input_gradients,
                              const Mat& output_gradients,
                              bool accumulate=false);
 
+    virtual void bpropUpdate(const Vec& input, const Vec& output,
+                             Vec& input_gradient,
+                             const Vec& output_gradient,
+                             bool accumulate=false);
+
     //! Reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from
     //! build().



From louradou at mail.berlios.de  Thu Mar 20 03:13:01 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Thu, 20 Mar 2008 03:13:01 +0100
Subject: [Plearn-commits] r8706 - trunk/python_modules/plearn/learners
Message-ID: <200803200213.m2K2D1wc028262@sheep.berlios.de>

Author: louradou
Date: 2008-03-20 03:12:59 +0100 (Thu, 20 Mar 2008)
New Revision: 8706

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:
A new version of this code, more clean and easy
to use with PLearn types (VMatrix, VecStatsCollector)




Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2008-03-20 00:34:37 UTC (rev 8705)
+++ trunk/python_modules/plearn/learners/SVM.py	2008-03-20 02:12:59 UTC (rev 8706)
@@ -1,561 +1,1361 @@
-import sys, os, time
-try:
-    from numarray import *
-except:
-    from numpy.numarray import *
-from math import *
+import os
+
+#import plearn.utilities
 from libsvm import *
 
-             
-class SVM_expert(object):
-      __attributes__ = ['kernel_type',
-                        'parameters_names',
-                        'tried_parameters',
-                        'best_parameters',
-                        'error_rate',
-                        ]
-                        
-      def __init__( self ):
-          self.parameters_names = ['C']
-          self.tried_parameters = {}
-          self.best_parameters  = None
-          self.error_rate       = 1.
+from plearn.pyext import *
+from numpy.numarray import *
+from math import *
+import random
 
-      def reset(self):
-          self.tried_parameters = {}
-          #if self.best_parameters != None:
-          #   self.add_parameter_to_tried_list(self.getBestValue('C'), self.best_parameters[1:])
-          self.error_rate       = 1.
-          self.should_be_tuned_again = 1
+class SVMHyperParamOracle__kernel(object):
+    """ An oracle that gives values of hyperparameters      
+        to train a SVM on vectors, given a popular kernel.
+        
+        Most of functions defined here deal with the choice
+        of the hyperparameter 'C' (that controls the
+        bias/variance trade-off in SVM training).
+        These functions are used by subclasses, in functions
+        that propose sets of hyperparameters (including
+        kernel hyperparameters).
 
+        List of attributes:    
+        -------------------    
 
-      def should_be_tuned_again():
-          raise StandardError, "should_be_tuned_again is nor implemented for this class"
-          
-      def get_svm_parameter( self, parameters ):
-          s= ', '.join([ self.parameters_names[i]+' = '+str(parameters[i]) for i in range(len(self.parameters_names)) ])
-          return eval('svm_parameter( svm_type = C_SVC, kernel_type = '+self.kernel_type+', '+s+')')
+    public: buildoption
 
-      def add_parameter_to_tried_list(self, C, kernel_parameters):
-          if self.tried_parameters.has_key(kernel_parameters):
-             self.tried_parameters[kernel_parameters]+=[C]
-          else:
-             self.tried_parameters[kernel_parameters] =[C] 
-             
-      def init_C(self, C_value):
-          return [C_value/10., C_value, C_value*10.]
 
-      def init_parameters(self, kernel_parameters_list):
-          C_base = self.init_C(10.)
-          table  = []
-          for C in C_base:
-              for prm in kernel_parameters_list:
-                  table.append( parameters2list(C, prm) )
-                  self.add_parameter_to_tried_list(C, prm)
-          return table
+        'C_initvalue': <float> Default value for 'C'.
 
-      def choose_new_parameters(self, kernel_parameters):
-          if self.tried_parameters.has_key(kernel_parameters):
-             C_table = choose_new_parameters_geom( self.tried_parameters[kernel_parameters], self.getBestValue('C') )
-          else:
-             C_table = self.init_C( self.best_parameters[0] )
-          table  = []
-          for C in C_table:
-                  table.append( parameters2list(C, kernel_parameters) )
-                  self.add_parameter_to_tried_list(C, kernel_parameters)
-          return table
-          
-      def getBestValue(self, name_prm):
-          return self.best_parameters[ self.parameters_names.index(name_prm) ]
-          
+        'verbosity': <int> Level of verbosity.
 
-class RBF_expert(SVM_expert):
-      def __init__( self ):
-          SVM_expert.__init__( self )
-          self.kernel_type       = 'RBF'
-          self.parameters_names += ['gamma']
+    public: learntoption
 
-      def init_gamma(self, gamma):
-          return [gamma/9., gamma, gamma*9.]
-          
-      def init_parameters( self, samples ):
-          dim = len(samples[0])
-          std = mean_std(samples)
-          rho = sqrt(dim)*std
-          gamma0 = 90./(2*rho**2)
-          gamma_base = self.init_gamma(gamma0)
-          return SVM_expert.init_parameters( self, gamma_base )
-          
-      def choose_new_parameters( self ):
-          best_gamma = self.getBestValue('gamma') 
-          tried_gamma = self.tried_parameters
-          if tried_gamma.has_key(best_gamma):
-             if self.getBestValue('C') == self.tried_parameters[best_gamma][0] or self.getBestValue('C') == self.tried_parameters[best_gamma][len(self.tried_parameters[best_gamma])-1]:
-                return SVM_expert.choose_new_parameters(self, best_gamma)
-             else:
-                proposed_gammas = choose_new_parameters_geom( tried_gamma, best_gamma)
-          else:
-             proposed_gammas = self.init_gamma(best_gamma)
-          return SVM_expert.init_parameters(self, proposed_gammas)
+        'best_param': <dict> of hyperparameters'values      
+                      which gave the best performance by    
+                      simple or cross- validation.
+                      (minimum valid_cost)
 
-      def should_be_tuned_again( self ):
-          best_gamma = self.getBestValue('gamma') 
-          tried_gamma = self.tried_parameters
-          if tried_gamma.has_key(best_gamma):
-             is_lower = False
-             is_higher = False
-             for gamma in tried_gamma:
-                if gamma<tried_gamma:
-                    is_lower = True
-                elif gamma<tried_gamma:
-                    is_higher = True
-             if is_lower and is_higher:
-                best_C_for_this_gamma  = self.getBestValue('C') 
-                tried_C_for_this_gamma = self.tried_parameters[best_gamma]
-                return not ( best_C_for_this_gamma <> min(tried_C_for_this_gamma) and best_C_for_this_gamma <> max(tried_C_for_this_gamma) )
-             else:
-                return True
-          else:
+        'best_cost': <float> Best cost obtained
+
+        'input_std': <float> the average standard deviation
+                          of the input data. It is estimated on
+                          the train set, and is used to choose first
+                          hyperparameter values.
+
+        'inputsize': <int> input dimension. Used also sometimes
+                     to choose the first hyperparameter values.
+
+    protected:
+
+        'kernel_type': <str> corresponding to the kernel.    
+                       in ['gaussian','linear','poly'].
+
+        'param_names': <list> of all hyperparameter names. It    
+                       always includes at first the positive     
+                       constant 'C' (trade-off bias/variance).   
+
+    private:
+
+        'trials_param_list': <list> of <dict> hyperparameter values
+                            that have been already tried.
+
+        'trials_cost_list': <list> or valid main costs corresponding to
+                                hyperparameter values in 'trials_param_list'
+
+    """
+    __attributes__ = [  'C_initvalue',
+                        'verbosity',
+                        'best_param',
+                        'best_cost'
+                        'kernel_type',
+                        'param_names',
+                        'trials_param_list',
+                        'trials_cost_list',
+                        'input_std',
+                        'inputsize'
+                      ]                  
+
+    def __init__(self):
+        self.param_names    = ['kernel_type','C']
+        self.trials_param_list  = []
+        self.trials_cost_list   = []
+        self.best_param     = None
+        self.best_cost      = None
+        self.C_initvalue    = 1.
+        self.stats_are_uptodate = False
+        self.input_std      = None
+        self.inputsize      = None
+        self.verbosity      = 1
+
+    """ To be called when training set change 'a bit'    
+        (same kind of data, but different statistics, 
+        so that we can assume new best parameters are
+        close to previous ones.
+    """
+    def forget(self):
+        self.trials_param_list  = []
+        self.trials_cost_list = []
+        self.stats_are_uptodate = False
+        # Note: when we forget, we keep the value of
+        #       'self.best_param'. This allow to initialize
+        #       a new search (when data changed a bit) to
+        #       a good candidate
+
+
+    def set_input_stats( self, inputsize, input_std ):
+        self.stats_are_uptodate = True
+        self.inputsize = int(inputsize)
+        self.input_std = input_std
+
+    def get_input_stats( self, samples=None ):
+        if self.stats_are_uptodate:
+            return ( self.inputsize, self.input_std )
+        if samples == None:
+            if self.verbosity > 0:
+                print "WARNING: get_data_stats() takes default value for input stats."
+            return ( 2, 1. )
+
+        if self.verbosity > 0:
+            print "  (computing input stats)"
+        
+        self.inputsize = len(samples[0])
+        self.input_std, std__std_per_component = mean_std(samples)
+        if( self.verbosity > 0 ): 
+            if( self.input_std < 0.5
+            or  self.input_std > 10.
+            or  std__std_per_component/self.input_std > 0.1 ):
+                print "Warning in SVMHyperParamOracle__kernel::get_input_stats() " + \
+                    "\n\tYour data does not seem to be normalized: " + \
+                    "\n\t(E[std_comp] = %.2f, std[std_comp] = %.2f)" % \
+                    ( self.input_std, std__std_per_component )
+
+        self.stats_are_uptodate = True
+        return (self.inputsize, self.input_std)
+
+    """ Return a list of values to try for hyperparameter 'C'
+        centered on a specified value 'C_value'.
+        This is an internal function (should not be called outside this class def).
+    """
+    def init_C(self, C_value=None):
+        if C_value==None:
+            C_value = self.C_initvalue
+        return [ C_value, C_value/10., C_value*10.]
+
+    """ Return <list> of <dict>: which hyperparameter values to try FIRST
+        - 'kernel_param_list': <list> of <dict> kernel hyperparameters name->value """
+    def choose_first_C_param(self, kernel_param_list=None,
+                                   C_value=None):
+        # Input check
+        if kernel_param_list==None:
+            kernel_param_list=[{}]
+        elif( type(kernel_param_list) <> list
+           or len(kernel_param_list) == 0
+           or type(kernel_param_list[0]) <> dict ):
+            raise TypeError,"SVMHyperParamOracle__kernel::choose_first_C_param(), " + \
+                            "last argument (%s) must be a list of dictionary" % \
+                            (kernel_param_list)
+
+        C_init_list = self.init_C(C_value)
+        table  = []
+        for C in C_init_list:
+            for pdict in kernel_param_list:
+                pdict.update({'C':C,'kernel_type':self.kernel_type})
+                if pdict not in table:
+                    table.append( pdict.copy() )
+        return table
+
+    """ For a given hyperparameter name (default:'C'),
+        return a list of values of this hyperparameter that have been tried
+        (for a given sub-setting for other hyperparameters).
+        Values are SORTED w.r.t to respective costs: the first one gave
+        the best valid performance.
+        - 'param_name': <string> generic name of the hyperparameter.
+        - 'other_param': <dict> hyperparameters name->value. """
+    def get_trials_oneparam_list(self, paramname='C', other_param=None):
+        # Input check
+        if other_param==None:
+            other_param={}
+        if paramname in other_param:
+            raise ValueError,"in SVMHyperParamOracle__kernel::get_trials_oneparam_list(), " + \
+                             "hyperparameter %s must not be included in other_param (%s) " % \
+                             (paramname, other_param)
+        if paramname not in self.param_names:
+            raise ValueError,"in SVMHyperParamOracle__kernel::get_trials_oneparam_list(), " + \
+                             "hyperparameter %s not in self.param_names (%s) " % \
+                             (paramname, self.param_names)
+
+        if not len(self.trials_param_list):
+            return None
+
+        param_list=[]
+        cost_list=[]
+        for (param, cost) in zip(self.trials_param_list,
+                                 self.trials_cost_list):
+            do_match= True
+            for pn in other_param:
+                if param[pn] <> other_param[pn]:
+                    do_match= False
+                    break
+            the_param_value = param[paramname]
+            if do_match:
+                # We avoid rendundancy in the returned list here
+                if the_param_value in param_list:
+                    if cost_list[ param_list.index(the_param_value) ] > cost:
+                        cost_list[ param_list.index(the_param_value) ] = cost
+                else:
+                    param_list.append(the_param_value)
+                    cost_list.append(cost)
+        sorted_cost_list = sorted(cost_list)
+        sorted_param_list = [None]*len(param_list)
+        for cost, j in zip(sorted_cost_list, range(len(sorted_cost_list)) ):
+            i = cost_list.index(cost)
+            cost_list[i]  = None
+            sorted_param_list[j] = param_list[i]
+        return sorted_param_list
+
+
+    """ A utility to give new hyperparameter values given the best value among a list
+        when the hyperparameter is multiplicative and positive (so the geometrical mean
+        is more suitable than the arithmetic mean).
+        - 'allow_to_return_already_tried_ones': <bool> Can return already tried values?
+                It is recommended to be True if and only if :
+                1. there are other hyperparams to tune, and
+                2. the hyperoptimization problem is not necessarily convex.
+    """
+    def choose_new_param_geom( self,
+                               crescentcosts_list,
+                               allow_to_return_already_tried_ones = True ):
+        best_value = crescentcosts_list[0]
+        crescentvalues_list = sorted(crescentcosts_list)
+
+        # smallest value
+        if best_value == crescentvalues_list[0]:
+            ratio = crescentvalues_list[0]/crescentvalues_list[1] # < 1
+            in_between = geom_mean([crescentvalues_list[1],best_value])
+            if allow_to_return_already_tried_ones:
+                return [ best_value * ratio,
+                         best_value,
+                         in_between ]
+            else:
+                return [ in_between * ratio,
+                         in_between,
+                         in_between * ratio*ratio ]
+
+        # largest value
+        if best_value == crescentvalues_list[-1]:
+            ratio = crescentvalues_list[-1]/crescentvalues_list[-2] # > 1
+            in_between = geom_mean([crescentvalues_list[-2],best_value])
+            if allow_to_return_already_tried_ones:
+                return [ best_value *ratio,
+                         best_value,
+                         in_between ]
+            else:
+                return [ in_between * ratio,
+                         in_between,
+                         in_between * ratio*ratio ]
+
+        #else:
+        # middle value (best case: dichotomie)
+
+        i = crescentvalues_list.index(best_value)
+        L = len(crescentvalues_list)
+
+        # Case when the 3 best hyperparam values for X are returned
+        #  (assuming that the system will choose new values for Y and/or Z and/or... given X)
+        # 
+        if allow_to_return_already_tried_ones:
+            if ( L > 4
+             and i > 1
+             and i < L-2
+             and random.random() < 0.5):
+                return crescentcosts_list[:3]
+            else:
+                return [  best_value,
+                          geom_mean([crescentvalues_list[i-1],best_value]),
+                          geom_mean([crescentvalues_list[i+1],best_value]) ]
+
+        return [  geom_mean([crescentvalues_list[i-1],best_value]),
+                  geom_mean([crescentvalues_list[i+1],best_value]) ]
+
+
+
+    """ Return <list> of <dict>: which hyperparameter values to try NEXT.
+        - 'kernel_param_list': <list> of <dict> kernel hyperparameters name->value. """
+    def choose_new_C_param(self, kernel_param_list=None):
+        # Input check
+        if kernel_param_list==None:
+            kernel_param_list=[{}]
+        elif( type(kernel_param_list) <> list
+          or  len(kernel_param_list) == 0
+          or  type(kernel_param_list[0]) <> dict ):
+            raise TypeError, "SVMHyperParamOracle__kernel::choose_new_C_param(), last argument (%s) must be a list of dictionary" % \
+                             (kernel_param_list)
+
+        new_param_list  = []
+        for kprm in kernel_param_list:
+            trials_C_list = self.get_trials_oneparam_list('C',kprm)
+            # if we have already tried some 'C' for this kernel
+            # parametrization, we try others
+            if trials_C_list and len( trials_C_list ) > 2:
+                C_table = self.choose_new_param_geom( trials_C_list, False)
+            # if this kernel parametrization is new, or the C has been really tuned ont it
+            # we give C values around the best one
+            # (obtained with another parametrization)
+            else:
+                C_table = self.init_C( self.best_param['C'] )
+
+            for C in C_table:
+                kprm.update({'C':C,'kernel_type':self.kernel_type})
+                if kprm not in self.trials_param_list:
+                    new_param_list.append( kprm.copy() )
+        return new_param_list
+
+
+    """ Return <bool>: whether or not we should try other 'C' for
+                       a given set of kernel hyperparameters.
+        - 'kernel_param': <dict> kernel parameters name->value.
+    """
+    def should_be_tuned_again(self, kernel_param_list=None):
+        if kernel_param_list==None:
+            kernel_param_list={}
+        trials_C_list = self.get_trials_oneparam_list('C',kernel_param_list)
+        if len(trials_C_list) <= 3:
             return True
-      
-class POLY_expert(SVM_expert):
-      def __init__( self ):
-          SVM_expert.__init__( self )
-          self.kernel_type       = 'POLY'
-          self.parameters_names += ['degree','coef0']
+        best_C = self.best_param['C']
+        return ( best_C == min(trials_C_list) or best_C == max(trials_C_list) )
 
-      def init_degree(self, degree):
-          return [  (degree-1,1), (degree,1), (degree+1,1) ]
+    """ Updates the statistics and internal trials history given new performances,
+        and returns if the trial was the best one.    
+        - 'param' is the <dict> of parameter values.
+        - the 'costs' are <dict> of corresponding performance (with None wherever missing).
+    """
+    def update_trials(self, param, cost):
+        if 'C' not in param:
+            raise IndexError,"in SVMHyperParamOracle__kernel::update_trials(), " + \
+                             "2nd arg (param=%s) must include 'C'" % (param)
 
-      def init_parameters( self, samples ):
-          #return SVM_expert.init_parameters(self, self.init_degree(3) )
-          return SVM_expert.init_parameters(self, [ (2,1), (3,1), (4,1) ] )
-          
-      def choose_new_parameters( self ):
-          best_degree = self.getBestValue('degree')
-          tried_degrees = [prms[0] for prms in self.tried_parameters]
-          if self.tried_parameters.has_key(best_degree):
-             if best_degree == max(tried_degrees):
-                return SVM_expert.init_parameters(self, [(best_degree+1,1)])
-             else:
-                return SVM_expert.choose_new_parameters(self, (self.best_parameters[1], self.best_parameters[2])  )
-          else:
-             return SVM_expert.init_parameters(self, self.init_degree(best_degree) )
-         
-      def should_be_tuned_again( self ):
-          best_degree = self.getBestValue('degree')
-          tried_degrees = [prms[0] for prms in self.tried_parameters]
-          if self.tried_parameters.has_key(best_degree):
-             if best_degree <> max(tried_degrees):
-                best_C_for_this_degree  = self.getBestValue('C') 
-                tried_C_for_this_degree = self.tried_parameters[(best_degree,self.best_parameters[2])]
-                return not ( best_C_for_this_degree <> min(tried_C_for_this_degree) and best_C_for_this_degree <> max(tried_C_for_this_degree) )
-          return True
-      
-class LINEAR_expert(SVM_expert):
-      def __init__( self ):
-          SVM_expert.__init__( self )
-          self.kernel_type = 'LINEAR'
+        if param not in self.trials_param_list:
+            self.trials_param_list.append(param)
+            self.trials_cost_list.append(cost)
+        else:
+            i = self.trials_param_list.index(param)
+            if cost <= self.trials_cost_list[i]:
+                self.trials_cost_list[i] = cost
 
-      def init_parameters( self, samples ):
-          return SVM_expert.init_parameters(self, [None])
+        if( self.best_cost == None or cost <= self.best_cost):
+        # TODO: what if cost == bestcost and param <> self.best_param?
+        #       -> best param should be a list...
+            self.best_param = param
+            self.best_cost  = cost
+            return True
 
-      def choose_new_parameters( self ):
-          return SVM_expert.choose_new_parameters(self, None)
+        return False
 
-      def should_be_tuned_again( self ):
-          if len(self.tried_parameters[None]) <= 3:
+
+class SVMHyperParamOracle__linear(SVMHyperParamOracle__kernel):
+    """ Sub-class of SVMHyperParamOracle__kernel, with a linear kernel.
+    """
+    def __init__(self):
+        SVMHyperParamOracle__kernel.__init__(self)
+        self.kernel_type = 'linear'
+
+    """ Return <list> of <dict>: which hyperparameter values to try FIRST
+        - 'samples': <array>(n_samples,dim) of (train/valid) samples.
+    """
+    def choose_first_param(self, samples=None ):
+        # Invariance w.r.t scaling input
+        d, std = self.get_input_stats( samples )
+        # Note:  default C=1   [for default std=1.]
+        self.C_initvalue = 1./(std*std)
+        return self.choose_first_C_param()
+
+    """ Return <list> of <dict>: which hyperparameter values to try NEXT.
+    """
+    def choose_new_param(self):
+        return self.choose_new_C_param()
+
+    """ Return <bool>: whether or not we should try other hyperparameter values """
+    #def should_be_tuned_again(self):        
+    #    return SVMHyperParamOracle__kernel.should_be_tuned_again(self)
+
+
+class SVMHyperParamOracle__rbf(SVMHyperParamOracle__kernel):
+    """ Sub-class of SVMHyperParamOracle__kernel, with a Gaussian kernel ('rbf').
+    """
+    def __init__(self):
+        SVMHyperParamOracle__kernel.__init__(self)
+        self.kernel_type  = 'rbf'
+        self.param_names += ['gamma']
+
+    def init_gamma(self, gamma_value=0.5):
+        return [ gamma_value, gamma_value/9., gamma_value*9. ]
+
+    """ Return <list> of <dict>: which hyperparameter values to try FIRST.
+        - 'samples': <array>(n_samples,dim) of (train/valid) samples.
+    """
+    def choose_first_param(self, samples=None ):
+        d, std = self.get_input_stats(samples)
+        # Note:  default gamma=1/4   [for default d=2, std=1.]
+        gamma0 = 1./(2*self.inputsize*std*std)
+        gamma_list = self.init_gamma(gamma0)
+        kernel_param_list=[]
+        for g in gamma_list:
+            kernel_param_list.append({'gamma':g})
+        return self.choose_first_C_param( kernel_param_list )
+        
+    """ Return <list> of <dict>: which hyperparameter values to try NEXT.
+    """
+    def choose_new_param(self):
+        best_gamma = self.best_param['gamma']
+        tried_gamma = self.get_trials_oneparam_list('gamma')
+        if best_gamma in tried_gamma:
+            if best_gamma <> tried_gamma[0]:
+                raise ValueError,"in SVMHyperParamOracle__rbf::choose_new_param() " + \
+                                 "best gamma %s is not the 1st of tried_gamma %s." % \
+                                 (best_gamma, tried_gamma ) + \
+                                 "\nThis list should be ordered w.r.t. costs."
+            gamma_list = self.choose_new_param_geom( tried_gamma )
+        else:
+            gamma_list = self.init_gamma(best_gamma)
+        return self.choose_new_C_param( [{'gamma':g}  for g in gamma_list] )
+
+    """ Return <bool>: whether or not we should try other hyperparameter values.
+    """
+    def should_be_tuned_again(self):
+        best_gamma = self.best_param['gamma']
+        tried_gamma = self.get_trials_oneparam_list('gamma')
+        if best_gamma not in tried_gamma:
             return True
-          best_C = self.getBestValue('C') 
-          tried_C = self.tried_parameters[None]
-          return not (best_C <> min(tried_C) and best_C <> max(tried_C) )
+        if( best_gamma == min(tried_gamma)
+        or  best_gamma == max(tried_gamma) ):
+            return True
+        return SVMHyperParamOracle__kernel.should_be_tuned_again(self, {'gamma':best_gamma})
 
+    
+class SVMHyperParamOracle__poly(SVMHyperParamOracle__kernel):
+    """ Sub-class of SVMHyperParamOracle__kernel, with a polynomial kernel.
+    """
+    def __init__(self):
+        SVMHyperParamOracle__kernel.__init__(self)
+        self.kernel_type     = 'poly'
+        self.param_names    += ['degree','coef0']
+        self.coef0_initvalue   = 1.
+
+    def init_degree(self, degree=3):
+        if degree < 3: degree = 3
+        return [  degree, degree-1, degree+1 ]
+
+    def init_coef0(self,
+                   coef0= None):
+        if not coef0:
+            coef0 = self.coef0_initvalue
+        return [  coef0, coef0/100., coef0*100. ]
+
+    """ Return <list> of <dict>: which hyperparameter values to try FIRST.
+        - 'samples': <array>(n_samples,dim) of (train/valid) samples.
+    """
+    def choose_first_param(self, samples=None ):
+        degree_list  = self.init_degree()
+        if samples <> None:
+            d, std = self.get_input_stats(samples)
+            self.coef0_initvalue = std*std
+            first_param_list = []
+            # We choose a table of C different for each d, for scaling invariance.
+            # We also hope there won't be any numerical problem after...
+            for d in degree_list:
+                self.C_initvalue = 1./(std**int(2*d))
+                first_param_list += self.choose_first_C_param( \
+                                         [{'degree':d,'coef0':self.coef0_initvalue}] )
+            return first_param_list
+        return self.choose_first_C_param( [ {'degree':d,'coef0':self.coef0_initvalue} for d in degree_list ] )        
+
+    """ Return <list> of <dict>: which hyperparameter values to try NEXT.
+    """
+    def choose_new_param(self):
+        best_degree = self.best_param['degree']
+        tried_degrees = self.get_trials_oneparam_list('degree')
+        if best_degree in tried_degrees:
+            if best_degree == max(tried_degrees):
+               degree_list = self.init_degree(best_degree+1)
+            elif best_degree == min(tried_degrees):
+               degree_list = self.init_degree(best_degree-1)
+            else:
+               degree_list = self.get_trials_oneparam_list('degree')[:3]
+        else:
+            degree_list = self.init_degree(best_degree)
+
+        new_param_list=[]
+        for d in degree_list:
+            tried_coef0 = self.get_trials_oneparam_list('coef0',{'degree':d})
+            tried_C = self.get_trials_oneparam_list('C',{'degree':d})
+
+            if not tried_C:
+                new_param_list += self.choose_first_C_param( \
+                                        [ {'degree':d, 'coef0':self.coef0_initvalue} ], \
+                                        self.best_param['C'] )
+            
+            elif( tried_C[0] == min(tried_C)
+               or tried_C[0] == max(tried_C) ):
+                new_param_list += self.choose_new_C_param( [ {'degree':d, 'coef0':tried_coef0[0]} ] )
+
+            # new degree tried: one value of 'coef0' tried and several for 'C' first
+            elif len(tried_coef0) == 0:
+                new_param_list += self.choose_first_C_param( \
+                                       [ {'degree':d,'coef0':self.coef0_initvalue} ], \
+                                       tried_C[0] )
+
+            # this degree was tried once with several 'C' but one (or two?) coef0
+            # we try others coef0
+            elif len(tried_coef0) < 3:
+                new_param_list += self.choose_first_C_param( \
+                                       [ {'degree':d, 'coef0':c} for c in self.init_coef0(tried_coef0[0]) ], \
+                                       tried_C[0] )
+
+            # coef0 is critical, we want to try again
+            elif( tried_coef0[0] == min(tried_coef0)
+               or tried_coef0[0] == max(tried_coef0) ):
+                new_param_list += self.choose_first_C_param( \
+                                       [ {'degree':d, 'coef0':c} for c in self.choose_new_param_geom( tried_coef0 ) ], \
+                                       tried_C[0] )
+                                       
+            # we try other 'C'
+            else:
+                new_param_list += self.choose_new_C_param( [ {'degree':d, 'coef0':tried_coef0[0]} ] )
+                                    
+        return new_param_list
+
+    """ Return <bool>: whether or not we should try other hyperparameter values.
+    """
+    def should_be_tuned_again(self):
+        best_degree = self.best_param['degree']
+        tried_degrees = self.get_trials_oneparam_list('degree')
+        if( best_degree in tried_degrees
+        and best_degree <> max(tried_degrees)
+        and ( best_degree <> min(tried_degrees) or best_degree == 2 ) ):
+                tried_C = self.get_trials_oneparam_list('C',{'degree':best_degree})
+                tried_coef0 = self.get_trials_oneparam_list('coef0',{'degree':best_degree})
+                return (  tried_C[0] == min(tried_C)
+                       or tried_C[0] == max(tried_C)
+                       or tried_coef0[0] == min(tried_coef0)
+                       or tried_coef0[0] == max(tried_coef0) )
+        return True
+    
+
+
 class SVM(object):
+    """ An expert driven by different oracles (1 oracle = 1 kernel)
+        to try values of hyperparameters to train a SVM.
+        The user may want to try one or several kernels.
+        Results are stored per kernel type.
 
-      __attributes__ = ['error_rate',
-                        'valid_error_rate',
-                        'best_parameters',
-                        'tried_parameters',
+        List of attributes:    
+        -------------------    
+
+    public: buildoption
+
+        'costnames': <list> of cost names. By default, ['class_error'].
+                 All cost names implemented are:
+                 - 'class_error': classification error.         
+                 - 'confusion_matrix': confusion matrix. It will     
+                                  output several stats on the results.amat file.         
+                                  e.g. 'cm_1_0' is the % of time the system
+                                  returned 0 when the truth was 1.
+
+                 - 'norm_ce': normalized classification error. It is computed
+                              by weighting each classification error by its cost
+                              given in 'errorcosts'.
+                              
+                 - 'n_sv': <int> number of Support Vectors.
+
+        'errorcosts': <array> of shape (nclasses, nclasses) that
+                      specifies the costs of each type of misclassification,
+                      to compute 'norm_ce'.
+                      errorcosts[c,c] should be 0. errorcosts[c,r] is the cost
+                      assigned when the truth is 'class c' and the system
+                      returns 'class r'.
+                      By default, if 'norm_ce' is asked, 'errorcosts' will be
+                      set w.r.t classes frequencies.
+
+        'maincost_name': <str> Cost that leads the choice of hyperparameters.
+                         The main cost is the cost to minimize it by validation.
+                         By default (if set to None), the first cost of costnames
+                         is taken. 
+
+        'validtype': <str> type of validation: 'simple' or 'cross'.
+        
+        'n_fold': <int> Number of folds in the cross-validation
+
+        'balanceC': <bool> Whether to use a different 'C' for each class,
+                         class SVM(    based on the frequency of each class in the training
+                             set. Useful when: classes are unbalanced and the cost
+                             of interest is normalized w.r.t class prior probas.
+                                     
+        'results_filename'
+        'preproc_optionnames'        
+        'preproc_optionvalues'
+
+
+
+        - 'retrain_on_valid': <bool> Use the best hyperparameters (found on valid)
+                             to re-train a bigger model on {train, valid}.
+                             Note: this option is obsolete for cross-validation
+                             (valid_samples = None), were the model is re-trained
+                             in any case.
+
+        - 'retrain_until_local_optimum_is_found': <bool> Should train_and_tune()
+                         class SVM(continue to tune hyperparameters until a local optimum
+                         is found. If False, you can re-run train_and_tune()
+                         several times. If True, you can also re-run to possibly
+                         find better performance.
+
+        - 'test_on_train': <bool> Should we test best models on {test, train} (1)
+
+
+        - 'verbosity': <int> Level of verbosity.
+
+    public: learntoption
+
+        'best_model':
+
+        'best_param': <dict> hyperparameters name->value
+                      which gave the best performance by    
+                      simple or cross- validation.
+                      (minimum valid_cost)
+
+        'valid_stats': <VecStatsCollector> costs corresponding to
+                       the best validation main cost.
+
+        'test_stats': <VecStatsCollector> test costs corresponding to
+                      the best validation main cost.
+
+        'train_stats': <VecStatsCollector> train costs corresponding to
+                       the best validation main cost.
+
+        
+        'nclasses': <int> number of classes.
+
+        'class_priors': <list> frequency for each class in the train set.
+                        This stat is used when the 'balancedC' option is ON.
+
+        'inputsize': <int> input size
+
+        'input_std': <float> input std
+
+       
+    protected:
+
+        'param_names': <list> of all hyperparameter names. It    
+                       always includes at first the positive     
+                       constant 'C' (trade-off bias/variance).   
+
+        'stats_are_uptodate': <bool> Are the statistics computed
+                        on the train set up to date?
+
+
+    """
+
+
+    __attributes__ = [  'name',
+                        'costnames',
+                        'errorcosts',
+                        'maincost_name',
+                        'validtype',
+                        'n_fold',
+                        'balanceC',
                         'results_filename',
                         'preproc_optionnames',
                         'preproc_optionvalues',
+                        'verbosity',
+                        'param',
+                        'best_param',
+                        'model',
                         'best_model',
-                        'nr_fold'
-                        'result_list',
-                        'automatically_decide_when_to_stop_tuning'
-                        ]
-       
-      def __init__( self ):
-      
-          self.error_rate       = 1.
-          self.valid_error_rate = 1.
-          
-          self.LINEAR_expert  = LINEAR_expert()
-          self.RBF_expert     = RBF_expert()
-          self.POLY_expert    = POLY_expert()
-          self.all_experts    = [self.LINEAR_expert,
-                                 self.RBF_expert,
-                                 self.POLY_expert,
-                                ]
-          
-          self.best_parameters      = None  
-          self.best_model           = None
-          self.tried_parameters     = {}
-          self.result_list          = {}
-          
-          self.results_filename        = None
-          self.preproc_optionnames  = []
-          self.preproc_optionvalues = []
-          
-          # For cross-validation
-          self.nr_fold        = 5
+                        'valid_stats',
+                        'test_stats',
+                        'train_stats',
+                        'param_names',
+                        'nclasses',
+                        'class_priors',
+                        'inputsize',
+                        'input_std',
+                        'stats_are_uptodate',
+                        'get_datalist'
+                     ]
+     
+    def __init__(self):
+        
+        self.name = 'svm'
+        
+        self.costnames       = ['class_error', 'confusion_matrix']
+        self.errorcosts      = None
+        self.maincost_name   = None
+        self.valid_stats     = None
+        self.test_stats      = None
+        self.train_stats     = None
+        self.validtype       = 'simple'
 
-          self.automatically_decide_when_to_stop_tuning = False
+        self.n_fold   = 5
+        self.balanceC = False
 
-      def get_expert( self, kernel_type ):
-          return eval( 'self.'+kernel_type+'_expert' )
+        self.HyperParamOracle__linear  = SVMHyperParamOracle__linear()
+        self.HyperParamOracle__rbf     = SVMHyperParamOracle__rbf()
+        self.HyperParamOracle__poly    = SVMHyperParamOracle__poly()
+        self.all_experts        = [self.HyperParamOracle__linear,
+                                   self.HyperParamOracle__rbf,
+                                   self.HyperParamOracle__poly,
+                                  ]
+        self.param_names =['validtype','kernel_type','balanceC']
+        for expert in self.all_experts:
+            for pn in expert.param_names:
+                if pn not in self.param_names:
+                    self.param_names.append(pn)
+        
+        self.kernel_type = 'linear'
+        self.param = None
+        self.best_param  = None
+        self.model = None
+        self.best_model = None
+ 
+        self.stats_are_uptodate = False
+        self.inputsize = None
+        self.input_std = None
+        self.nclasses = None
+        self.class_priors = None
+        self.weight = None
+        self.labels = None
+        
+        
+        self.results_filename      = None
+        self.preproc_optionnames  = []
+        self.preproc_optionvalues = []
 
-      def get_parameters( self, kernel_type, parameters ):
-          prm_dict = {'kernel':kernel_type.lower()}
+        self.retrain_on_valid = True
+        self.retrain_until_local_optimum_is_found = True
+        self.test_on_train = True
 
-          for expert in self.all_experts:
-              for prm_name in expert.parameters_names:
-                  prm_dict[prm_name]= None
-          expert = self.get_expert( kernel_type )
-          if len(expert.parameters_names) <> len(parameters):
-             raise IndexError, "in SVM.get_parameters(): "+\
-                   "parameters (length %s) should be of length %s for kernel " % \
-                   ( len(parameters), len(expert.parameters_names), kernel_type )
+        
+        self.verbosity = 0
+        
+        self.trainset_key = 'trainset'
+        self.validset_key = 'validset'
+        self.testset_key  = 'testset'
 
-          for prm_name, prm_value in zip(expert.parameters_names,
-                                         parameters):
-              prm_dict[prm_name]=prm_value
-          return prm_dict
+    def forget(self):
+        for expert in self.all_experts:
+             expert.forget()
+        self.valid_stats     = None
+        self.test_stats      = None
+        self.train_stats     = None
+        self.stats_are_uptodate = False
+        # Note: when we forget, we keep the value of
+        #       'self.best_param'. This allow to initialize
+        #       a new search (when data changed a bit) to
+        #       a good candidate
 
-      
-      def reset( self ):
-          for expert in self.all_experts:
-              expert.reset()
-          
-          self.tried_parameters = {}
-          self.result_list          = {}
-          self.error_rate       = 1.
-          self.valid_error_rate = 1.
+    def train_inputspec(self, dataspec):
+        if self.trainset_key not in dataspec:
+            return None
+        return dataspec[ self.trainset_key ]
+    def valid_inputspec(self, dataspec):
+        if self.validset_key not in dataspec:
+            return None
+        return dataspec[ self.validset_key ]
+    def test_inputspec(self, dataspec):
+        if self.testset_key not in dataspec:
+            return None
+        return dataspec[ self.testset_key ]
 
-      def add_parameter_to_tried_list(self, kernel, kernel_parameters):
-          if self.tried_parameters.has_key(kernel):
-             self.tried_parameters[kernel]+=[kernel_parameters]
-          else:
-             self.tried_parameters[kernel] =[kernel_parameters]
+    ## specific to libsvm
+    """ Return samples and targets in the format required
+        by libsvm, i.e. lists of float.
+        This is only a default function, so that the user can
+        define another method for any sophisticated VMatrix
+        (this can be done by changing the attribute:
+                                SVM.get_datalist ).
+        - 'input_vmat': <VMatrix> Must have the function getMat()
+                        that returns the corresponding array,
+                        as well as attributes:
+                        inputsize, targetsize, length.
+                        
+    """
+    def get_datalist( input_vmat ):
+        data_array = input_vmat.getMat()
+        inputsize = input_vmat.inputsize
+        targetsize = input_vmat.targetsize
+        nsamples = input_vmat.length
+        assert shape(data_array)[0] == nsamples
+        samples   = [ [ float(x_t_i)    for x_t_i in x_t ]
+                                        for x_t in data_array[:,:inputsize] ]
+        assert targetsize == 1
+        targets   = [ float(t) for t in data_array[:,inputsize] ]
+        return samples, targets
 
-      def add_result_to_result_list(self, kernel, kernel_parameters, error_rate):
-          if self.result_list.has_key(kernel):
-             self.result_list[kernel]+= kernel_parameters, error_rate
-          else:
-             self.result_list[kernel] = kernel_parameters, error_rate
+    """ Return a dictionary class label -> class frenquency,
+        where frequencies are estimated from 'targets', a list of class labels
+    """
+    def get_class_priors(self, targets ):
+        class_priors = {}
+        for label in targets:
+            if label not in class_priors:
+                class_priors[label] = 0.
+            class_priors[ label ] += 1.
+        nsamples = sum( class_priors.values() )
+        for label in class_priors:
+            class_priors[ label ] *= 1./nsamples
+        return class_priors
+           
+    """ Return the input/target dimensions and stats estimated on a VMatrix 'vmat'.
+    """
+    def get_data_stats(self, vmat=None):
+        if self.stats_are_uptodate:
+            return ( self.nclasses, self.class_priors,
+                     self.inputsize, self.input_std )
+        assert vmat <> None
 
+        samples, targets = self.get_datalist( vmat )
 
-      def write_results( self, kernel_type, parameters,
-                        train_class_error,
-                        valid_class_error,
-                        test_class_error = None,
-                        cross_valid = False):
-          if cross_valid:
-                valid_name='crossvalid'
-          else:
-                valid_name='valid'
+        self.inputsize, self.input_std = self.all_experts[0].get_input_stats(samples)
+        for expert in self.all_experts[1:]:
+            expert.set_input_stats( self.inputsize, self.input_std )
 
-          # the results_filename will not include the extension ".amat"
-          if self.results_filename[-5:]=='.amat':
-             self.results_filename = self.results_filename[:-5]
+        if targets == None:
+            if self.verbosity > 1:
+                print "WARNING: get_data_stats() takes default value for class info."
+            return ( 2, [ .5, .5 ],
+                 self.inputsize, self.input_std )
 
-          prm_dict = self.get_parameters( kernel_type, parameters )
-          prm_names_list = [ prm_name for prm_name in prm_dict ]
-          prm_names = ' '.join( prm_names_list )
-          prm_values = ' '.join( str(prm_dict[pn]) for pn in prm_names_list )
-          
-          if self.results_filename == None:
-             print "==============================="
-             for pn in prm_names_list:
-                 print "    ",pn,"=",prm_dict[pn]
-             if train_class_error <> None:
-                 print "E[train.E[class_error]]=",train_class_error
-             if valid_class_error <> None:
-                 print "E["+valid_name+".E[class_error]]=",valid_class_error
-             if test_class_error <> None:
-                 print "E[test.E[class_error]]=",test_class_error
-             return
-          
-          if type(self.preproc_optionnames)==str:
-             preproc_optionnames = self.preproc_optionnames
-          elif type(self.preproc_optionnames)==list:
-             preproc_optionnames = ' '.join(self.preproc_optionnames)
-          else:
-             raise TypeError, "preproc_optionnames must be of type str or list"
-          if type(self.preproc_optionvalues)==str:
-             preproc_optionvalues = self.preproc_optionvalues
-          elif type(self.preproc_optionvalues)==list:
-             preproc_optionvalues = ' '.join('%s' % v for v in self.preproc_optionvalues)
-          else:
-             raise TypeError, "preproc_optionvalues must be of type str or list"
-          
-          os.system('makeresults  %s  %s %s %s %s %s;' % \
-                                ( self.results_filename,
-                                  preproc_optionnames,
-                                  prm_names,
-                                  "E[train.E[class_error]]",
-                                  "E["+valid_name+".E[class_error]]",
-                                  "E[test.E[class_error]]"
-                                ) \
-                  + 'appendresults %s.amat  %s %s %s %s %s' % \
-                                ( self.results_filename,
-                                  preproc_optionvalues,
-                                  prm_values,
-                                  train_class_error,
-                                  valid_class_error,
-                                  test_class_error
-                                )
-                   )
+        class_priors = self.get_class_priors( targets )
+        self.nclasses = len( class_priors )
+        self.class_priors = class_priors
 
-      def train_and_test(self, samples_target_list):
-          check_samples_target_list(samples_target_list)
+        if self.verbosity > 0:
+            print "  ( class priors: %s  -- %d samples)" % \
+                     ( class_priors, len(targets) )
 
-          best_expert = self.get_expert( self.best_parameters[0] )
-          best_parameters = best_expert.best_parameters
-          param = best_expert.get_svm_parameter( best_parameters )
-          if len(samples_target_list) == 1: # cross-validation
-             #costs = do_cross_validation(samples_target_list[0][0], samples_target_list[0][1], param, self.nr_fold)
-             train_problem = svm_problem( samples_target_list[0][1] , samples_target_list[0][0] )
-             self.best_model = svm_model(train_problem, param)
-             costs = test_model(self.best_model, samples_target_list[0][0], samples_target_list[0][1])
-          else:
-             #costs = do_simple_validation(samples_target_list[0][0] , samples_target_list[0][1] , samples_target_list[1][0] , samples_target_list[1][1], param)                     
-             train_problem = svm_problem( samples_target_list[0][1] , samples_target_list[0][0] )
-             self.best_model = svm_model(train_problem, param)
-             costs = test_model(self.best_model, samples_target_list[1][0], samples_target_list[1][1])
-          return costs
+        if self.balanceC:
+            weight = [ 1./p for p in class_priors.values() ]
+            # i think it is better to have integers with libsvm
+            # but we can generalize this to whatever representation
+            #           (maybe not so safe with PLearn...)
+            self.labels = [ int(l) for l in class_priors.keys() ]
+            
+            # average weight = 1
+            S = sum(weight)
+            self.weight = [ w*self.nclasses*1./S for w in weight ]
+            if self.verbosity > 0:
+                print "  (to compensate for class priors: weight C by %s)" % \
+                         ( self.weight )
 
-      def test(self, samples_target_list):
-          check_samples_target_list([samples_target_list])
-          test_class_error=test_model(self.best_model, [[x_i for x_i in x] for x in samples_target_list[0]], [float(l) for l in samples_target_list[1]])['error_rate']
-          self.write_results( best_parameters[0], best_parameters[1:],
-                         None,
-                         self.valid_error_rate,
-                         test_class_error)
-          return test_class_error
+        self.stats_are_uptodate = True
+        return ( self.nclasses, self.class_priors,
+                 self.inputsize, self.input_std )
 
+    def get_expert(self, kernel_type ):
+        return eval( 'self.HyperParamOracle__'+kernel_type.lower() )
+    
+    """ The following 3 functions simply give access to costs
+        from a VecStatsCollector 'stat'
+    """
+    def get_cost(self, stats, cost_name):
+        """ cost_name is a string """
+        if stats == None:
+            return None
+        if cost_name not in stats.getFieldNames():
+            raise KeyError, "in SVM::main_cost(), " + \
+                            "stats does not have field %s" % \
+                            ( cost_name )
+        return stats.getStat('E[%s]' % cost_name )
+    def get_maincost(self, stats):
+        if self.maincost_name:
+            maincost_name= self.maincost_name
+        else:
+            maincost_name= self.costnames[0]
+        return self.get_cost( stats, maincost_name )
+    def get_all_costs(self, stats):
+        allcosts={}
+        for cost_name in self.costnames:
+            allcosts[cost_name] = self.get_cost(stats, cost_name)
+        return allcosts
 
-      def train_and_tune(self, kernel_type, samples_target_list):
-          kernel_type=kernel_type.upper()
-          check_samples_target_list(samples_target_list)
-          cross_valid=False
-          if len(samples_target_list) == 1:
-             print str(self.nr_fold)+"-fold Cross-Validation"
-             cross_valid=True
-          elif len(samples_target_list) == 2:
-             print "\nSimple validation...\n"
-          elif len(samples_target_list) == 3:
-             print "\nValidation + test...\n"
-          else:
-             raise TypeError, "last argument of train_and_tune() should be a list, with length between 1 and 3"
-           
-          expert = self.get_expert( kernel_type )
-          
-          if len(expert.tried_parameters) == 0:
-             recompute_best = True
-          else:
-             recompute_best = False
+    """ Return a svm_parameter in the format for libsvm.
+        - 'param': <dict> parameters name->value.
+    """
+    ## specific to libsvm
+    def get_libsvm_param(self, param ):
+        param = param.copy()
+        for pn in param:
+            if param[pn] == None:
+                param.pop(pn)
+            elif pn == 'kernel_type':
+                # libsvm wants upper case for kernel_type
+                param[pn] = param[pn].upper()
+        s= ', '.join( pn+' = '+str(param[pn])
+                      for pn in param )
+        if len(s)>0:s=', '+s
 
-          if expert.best_parameters  == None:
-             parameters_to_try = expert.init_parameters( samples_target_list[0][0] )
-          else:
-             parameters_to_try = expert.choose_new_parameters()
-          
-          best_parameters   = expert.best_parameters
-          best_error_rate   = expert.error_rate
-          best_model        = None
+        # Note: 'svm_type = C_SVC' stands for classification
+        return eval('svm_parameter( svm_type = C_SVC '+s+')' )
+    
 
-          for parameters in parameters_to_try:
-              if parameters != expert.best_parameters or recompute_best:
-              
-                  self.add_parameter_to_tried_list(kernel_type, parameters)
-                  param = expert.get_svm_parameter( parameters )
-                  
-                  if len(samples_target_list) == 1: # cross-validation
-                     error_rate = do_cross_validation(samples_target_list[0][0], samples_target_list[0][1], param, self.nr_fold)['error_rate']
-                  else:
-                     train_problem = svm_problem( samples_target_list[0][1] , samples_target_list[0][0] )
-                     model = svm_model(train_problem, param)
-                     error_rate = test_model(model, samples_target_list[1][0], samples_target_list[1][1])['error_rate']
-                     
-                  self.add_result_to_result_list(kernel_type, parameters, self.error_rate)
-                  if error_rate < best_error_rate:
-                     best_parameters = parameters
-                     best_error_rate = error_rate
-                     if len(samples_target_list) <> 1: # in case of cross-validation, we will compute the model later
-                        best_model   = model
+    """ Write given results with corresponding parameters
+        In a PLearn format (.amat).
+    """
+    def write_results(  self, param,
+                        valid_stats,
+                        test_stats = None,
+                        train_stats= None ):
+        if valid_stats==None and param == self.best_param:
+                valid_stats = self.valid_stats
+        if test_stats==None and param == self.best_param:
+                test_stats = self.test_stats
+        if train_stats==None and param == self.best_param:
+                train_stats = self.train_stats
+        valid_costs = self.get_all_costs(valid_stats)
+        test_costs = self.get_all_costs(test_stats)
+        train_costs = self.get_all_costs(train_stats)
+        
+        # If no file specified, print on stdout in a readable format
+        if self.results_filename == None or self.verbosity > 0:
+            print "\n -- Trial with parameters"
+            for pn in param:
+                print "    ",pn," = ",param[pn]
+            if train_costs <> None:
+                print " -- train costs: ", train_costs
+            if valid_costs <> None:
+                print " -- valid costs: ", valid_costs
+            if test_costs <> None:
+                print " -- test costs: ",  test_costs
+            if self.results_filename == None:
+                return
 
-                  self.write_results( kernel_type, parameters,
-                                 None,
-                                 error_rate,
-                                 None,
-                                 cross_valid )
 
-          if best_error_rate < expert.error_rate:
-             expert.best_parameters = best_parameters
-             expert.error_rate = best_error_rate
-             
-             if best_error_rate < self.valid_error_rate:
-                self.best_parameters = [kernel_type, best_parameters]
-                self.valid_error_rate = best_error_rate
-                if len(samples_target_list) == 1: # compute the best model in the case of cross-validation
-                   train_problem = svm_problem( [float(l) for l in samples_target_list[0][1]] , [[float(x_i) for x_i in x] for x in samples_target_list[0][0]] )
-                   param = expert.get_svm_parameter( best_parameters )
-                   best_model = svm_model(train_problem, param)
-                self.best_model = best_model
-                if len(samples_target_list) == 3: # train-valid-test
-                   self.error_rate = self.test( [samples_target_list[2]] )
+        # Format 'results_filename' (NO extension ".amat")
+        if self.results_filename[-5:]=='.amat':
+            self.results_filename = self.results_filename[:-5]
+        
+        # Format preprocessing option names to obtain one string
+        if type(self.preproc_optionnames)==str:
+            preproc_optionnames = self.preproc_optionnames
+        elif type(self.preproc_optionnames)==list:
+            preproc_optionnames = ' '.join(self.preproc_optionnames)
+        else:
+            raise TypeError, "preproc_optionnames must be of type str or list"
+
+        # Format preprocessing option values to obtain one string
+        if type(self.preproc_optionvalues)==str:
+            preproc_optionvalues = self.preproc_optionvalues
+        elif type(self.preproc_optionvalues)==list:
+            preproc_optionvalues = ' '.join('%s' % v for v in self.preproc_optionvalues)
+        else:
+            raise TypeError, "preproc_optionvalues must be of type str or list"
+        
+        param_values=[]
+        for pn in self.param_names:
+            if pn in param:
+                param_values.append(param[pn])
+            elif pn in self.__attributes__:
+                param_values.append( eval('self.'+pn) )
+            else:
+                param_values.append(None)
+        
+        costnames_string = ""
+        costvalues_string = ""
+        for cn in self.costnames:
+            for dataset in ['train','valid','test']:
+                costs = eval(dataset+'_costs')
+                
+                # Special processing for the confusion matrix
+                if cn == 'confusion_matrix':
+                    if cn in costs:
+                        confusion_matrix=costs['confusion_matrix']
+                    else:
+                        confusion_matrix=None
+                    for cli in range(self.nclasses):
+                        for clj in range(self.nclasses):
+                            costnames_string += "E[%s.E[cm_%d_%d]] " % \
+                                                (dataset, cli, clj)
+                            if confusion_matrix <> None:
+                                costvalues_string += "%s " % confusion_matrix[cli,clj]
+                            else:
+                                costvalues_string += "None "                                
+                    continue
+
+                costnames_string += "E[%s.E[%s]] " % (dataset, cn)
+                if cn in costs:
+                    costvalues_string += "%s " % costs[cn]
                 else:
-                   self.error_rate = self.valid_error_rate
-          
-          if self.automatically_decide_when_to_stop_tuning:
-             if expert.should_be_tuned_again():
-                self.train_and_tune(kernel_type, samples_target_list)
+                    costvalues_string += "None "
+        
+        # Write the result in the file specified by 'results_filename'
+        os.system('makeresults  %s %s %s %s;' % \
+                            (self.results_filename,
+                              preproc_optionnames,
+                              ' '.join(self.param_names),
+                              costnames_string
+                            ) \
+               + 'appendresults %s.amat %s %s %s' % \
+                            (self.results_filename,
+                              preproc_optionvalues,
+                              ' '.join(str(v) for v in param_values),
+                              costvalues_string
+                            )
+                )
 
-          return self.error_rate
-          
+    """ Updates the statistics and internal trials history given new performances,
+        and returns if the trial was the best one.
+        - 'param' is the <dict> of parameter values
+        - the 'costs' are <dict> of corresponding performance (with None wherever missing) """
+    def update_trials(self, param,
+                      valid_stats,
+                      test_stats = None,
+                      train_stats= None):
 
-##
-## Using the cross validation of libSVM
-## (problem: the folds are chosen randomly, so things are not rigorously compared)
-##
-#def do_cross_validation(samples, targets, param, nr_fold):
-#        N = len(targets)
-#        total_correct = 0
-#        prob = svm_problem(targets, samples)
-#            outputs = cross_validation(prob, param, nr_fold)
-#        for i in range(N):
-#            if outputs[i] == targets[i]:
-#               total_correct = total_correct + 1 
-#        return ( (N - total_correct) / N)
+        # Check input: enforce to give a valid cost (and not test cost)
+        # or else the valid has been done and we try with the best hyperparameters
+        if valid_stats==None:
+            if param == self.best_param:
+                if test_stats <> None:
+                    self.test_stats = test_stats
+                if train_stats <> None:
+                    self.train_stats = train_stats
+                return True
+            else:
+                raise KeyError,"in SVM::update_trials(), " + \
+                                "3rd arg (valid_stats=%s) must be a <dict> with key '%s' pointing to a float" % \
+                                (valid_stats, maincost_name)
 
-##
-## Doing cross validation on samples
-## - divide the set of samples in nr_fold subsets
-## - (nr_fold - 1) subsets serve as training set / 1 subset as test set
-##   for each step.
-## - Then the error rate is simply the average error rate...
-##
-def do_cross_validation(samples, targets, param, nr_fold):
-    arrayType=False
-    if 'array' in str(type(samples)):
-       arrayType=True
-    targets_subsets=[]
-    samples_subsets=[]
-    N=len(samples)
-    for i in range(nr_fold):
-        samples_subsets.append(samples[i:N:nr_fold,:])
-        targets_subsets.append(targets[i:N:nr_fold])
-    cum_error_rate=0.
-    for i in range(nr_fold):
-        test_samples = samples_subsets[i]
-        test_targets = targets_subsets[i]
-        train_targets=[]
-        if arrayType:
-           train_samples=[]
+        kernel_type = param['kernel_type']
+
+        maincost=self.get_maincost(valid_stats)
+        bestcost=self.get_maincost(self.valid_stats)
+
+        expert = self.get_expert(kernel_type)
+        kernel_param = {}
+        for pn in expert.param_names:
+            kernel_param[pn] = param[pn]
+        expert.update_trials(kernel_param, maincost)
+
+        if( bestcost == None or maincost <= bestcost):
+            self.best_param = param
+            self.valid_stats = valid_stats
+            self.test_stats  = test_stats
+            self.train_stats = train_stats
+            return True
+        return False
+
+
+    """ Return a libSVM model trained for a given set
+        of hyperparameters 'param' and some dataset
+    """
+    def train( self,
+               dataspec,
+               param = None
+             ):
+        if self.verbosity > 3:
+            print "SVM::train() called ", dataspec.keys()
+        
+        if param == None:
+            if not self.best_param:
+                return self.train_and_tune(dataspec)
+            param = self.best_param.copy()
+        
+        trainset = self.train_inputspec(dataspec)
+        if self.balanceC:
+            self.get_data_stats( trainset )
+            param.update({'weight':self.weight,
+                          'nr_weight':len(self.weight),
+                          'weight_label':self.labels})
+        
+        train_samples, train_targets = self.get_datalist( trainset )
+        train_problem = svm_problem( train_targets ,
+                                     train_samples )
+        if self.verbosity > 1:
+            print "launching libsvm with param %s " % param    
+        model = svm_model(train_problem, self.get_libsvm_param( param ) )
+        
+        if param == self.best_param:
+            self.best_model = model
+        self.model = model
+        self.param = param
+        
+        return dataspec
+        
+    """ This function can be changed by the user to take another
+        decision than the standard ones, for instance when
+        classifying bags of data (where each bag correspond to a target)
+    """
+    def update_predictions_targets(predictions, targets, vmat):
+        return predictions, targets
+
+    """ Return the costs obtained by a libSVM model
+        on a given dataset
+    """
+    def test( self,
+              testset,
+              teststats = None
+             ):
+        assert testset <> None
+        if self.verbosity > 3:
+            print "SVM::test() called"
+        # By default take the LAST model trained
+        if self.model <> None:
+            model = self.model
+        # unless self.model is None
         else:
-           train_samples=[]
-        for j in range(0,i)+range(i+1,nr_fold):
-            train_targets += targets_subsets[j]
-            if arrayType:
-                L=len(train_samples)
-                train_samples=resize(samples,[L+len(samples_subsets[j]),len(samples_subsets[j][0])])
-                train_samples[L:,:]=samples_subsets[j]
+            assert self.best_model <> None
+            model = self.best_model
+
+        samples, targets = self.get_datalist( testset )
+        nclasses = self.nclasses
+        nsamples = len(samples)
+
+        costnames = self.costnames
+        # Translation of the cost 'confusion_matrix'
+        # to several costs (one per matrix component)
+        for cn, icost in zip(costnames, range(len(costnames)) ):
+            if cn == 'confusion_matrix':
+                costnames = costnames[:icost] \
+                            + [ 'cm_%s%s' % (i,j) for i in range(nclasses)
+                                                  for j in range(nclasses) ] \
+                            + costnames[icost+1:]                
+                self.costnames = costnames
+                break
+        if teststats == None:
+            teststats= VecStatsCollector()        
+            teststats.setFieldNames( costnames )
+
+        predictions=[]
+        for x in samples:
+            ## specific to libsvm
+            predictions.append( int(model.predict(x)) )
+        predictions, targets = self.update_predictions_targets( predictions, targets, testset)
+
+        # Computing misclassification costs for the default normalized
+        # classification error (= class error weighted w.r.t class priors)
+        
+        class_priors = self.get_class_priors( targets )
+                                            # [int(t) for t in targets ]
+        cm_weights = [ 1./class_priors[t] for t in range(nclasses) ]
+        if 'norm_ce' in self.costnames:
+            if self.errorcosts == None:        
+                errorcosts = zeros((nclasses,nclasses))
+                for classe in range(nclasses):
+                    for prediction in range(classe)+range(classe+1,nclasses):
+                        errorcosts[classe,prediction] = 1./ ( nclasses * class_priors[classe] )
             else:
-                train_samples += samples_subsets[j]           
-        cum_error_rate += do_simple_validation(train_samples, train_targets, test_samples, test_targets, param)['error_rate']
-    av_error_rate = cum_error_rate*1.0 / nr_fold
-    print av_error_rate
-    return {'error_rate':av_error_rate}
+                errorcosts = self.errorcosts
+
+        for prediction, t in zip( predictions, targets ):
+            truth = int(t)
+            statVec = []
+            for cn in costnames:
+                if cn == 'class_error':
+                    if truth == prediction:
+                        statVec.append(0.)
+                    else:
+                        statVec.append(1.)
+                elif cn[:3] == 'cm_':
+                    c1 = int(cn[3])
+                    c2 = int(cn[4])
+                    if c1 == truth and c2 == prediction:
+                        statVec.append(cm_weights[truth])
+                    else:
+                        statVec.append(0.)
+                elif cn == 'norm_ce':
+                    statVec.append( errorcosts[truth, prediction] )
+                #elif cn == 'n_sv': # number of support vector (not done)
+                #    statVec.append( svmc.svm_get_nr_sv(model) )
+                else:
+                    raise ValueError, "computation of cost %s not implemented in SVM::test()" % cn
+            teststats.update(statVec,1.)
         
-def do_simple_validation(train_samples, train_targets, test_samples, test_targets, param):    
-    train_samples = [[float(x_i) for x_i in x] for x in train_samples]
-    test_samples = [[float(x_i) for x_i in x] for x in test_samples]
-    train_problem = svm_problem( [float(l) for l in train_targets], train_samples )
-    model = svm_model(train_problem, param)
-    return test_model(model,test_samples,[float(l) for l in test_targets])
+        return teststats #, outputs, costs
 
-def test_model(model, samples, targets):
-    N = len(samples)
-    diffs = {}
-    for i in range(N):
-          diff = abs(model.predict([float(x_i) for x_i in samples[i]]) - float(targets[i]))
-          #print "prediction: ",model.predict([float(x_i) for x_i in samples[i]]),"  -  real: ",float(targets[i]),"  ( ",diff," )"
-          if diffs.has_key(diff):
-                diffs[diff] += 1
-          else:
-                diffs[diff] = 1
-    error_rate = 0
-    linear_class_error = 0
-    square_class_error = 0
-    for diff, nbdiff in diffs.iteritems():
-          if abs(diff) > 0.001:
-                error_rate += 1*nbdiff
-          linear_class_error += diff*nbdiff
-          square_class_error += (diff*diff)*nbdiff
-    error_rate = float(error_rate) / N
-    linear_class_error = float(linear_class_error) / N
-    square_class_error = float(square_class_error) / N
-    return {'error_rate':error_rate, 'linear_class_error':linear_class_error, 'square_class_error':square_class_error }
 
-#
-# Some useful functions
-#
+    def valid( self,
+               dataspec,
+               param= None):
+        if self.validtype[:6]=='simple' and self.validset_key in dataspec:
+            return self.simplevalid(dataspec, param)
+        else:
+            assert self.validset_key in dataspec
+            return self.crossvalid(dataspec, param)
 
-def choose_new_parameters_geom( table, best_value ):
-    sorted_table = sorted(table)
-    if best_value == sorted_table[0]:
-       # smallest value
-       proposed_table = [ best_value*1.1*best_value/sorted_table[1],
-                          #best_value,
-                          geom_mean([sorted_table[1],best_value]) ]
-    elif best_value == sorted_table[len(table)-1]:
-       # largest value
-       proposed_table = [ geom_mean([sorted_table[len(table)-2],best_value]), 
-                          #best_value,
-                          best_value*0.9*best_value/sorted_table[len(table)-2] ]
-    else:
-       # middle value (best case: dichotomie)
-       if best_value not in sorted_table:
-          raise TypeError, "in RBF.choose_new_parameters: "+str(best_value)+" not found in tried_parameters"
-       index = sorted_table.index(best_value)
-       proposed_table = [ geom_mean([sorted_table[index-1],best_value]),
-                          #best_value,
-                          geom_mean([sorted_table[index+1],best_value]) ]
-    return proposed_table
+    """ Return the costs obtained by (simple) validation
+        for a given set of hyperparameter.
+    """
+    def simplevalid( self,
+                     dataspec,
+                     param= None,
+                     validstats = None ):
+        self.validtype = 'simple'
+        if not param:
+            param = self.best_param
+        if self.verbosity > 0:
+            print "\n** Simple Validation"
+            print "   with param %s" % param
+        self.train(dataspec, param)
+        validset = self.valid_inputspec(dataspec)
+        return self.test( validset, validstats )
 
+
+    """ Return the costs obtained by cross-validation
+        for a given set of hyperparameter.
+    """
+    def crossvalid( self,
+                    dataspec,
+                    param = None ):
+        self.validtype = 'cross'
+        n_fold = self.n_fold
+        if not param:
+            param = self.best_param
+
+        if self.verbosity > 0:
+            print "\n** %d-fold Cross Validation" % n_fold
+            print "   with param %s" % param
+        
+        trainset = self.train_inputspec(dataspec)
+        N=trainset.length
+        
+        validstats = None
+        for i in range(n_fold):
+            sub_testset = SelectRowsVMatrix(
+                                source = trainset,
+                                indices = range(i,N,n_fold)
+                            )
+            indices = []
+            for j in range(0,i)+range(i+1,n_fold):
+                indices += range(j,N,n_fold)
+            sub_trainset = SelectRowsVMatrix(
+                                source = trainset,
+                                indices = indices
+                            )
+            validstats = self.simplevalid({self.trainset_key:sub_trainset,
+                                           self.validset_key:sub_testset},
+                                            param,
+                                            validstats)
+
+        return validstats
+
+
+    """ THE interesting function of the class
+# TODO: document the function
+    """
+    def train_and_tune(self, dataspec):
+
+        if self.verbosity > 3:
+            print "SVM::train_and_tune() called ", dataspec.keys()
+
+        trainset = self.train_inputspec(dataspec)
+        validset = self.valid_inputspec(dataspec)
+        testset  = self.test_inputspec(dataspec)
+
+        
+        self.get_data_stats( trainset )
+
+        expert = self.get_expert( self.kernel_type )
+        expert.verbosity = self.verbosity
+
+        # HyperParamOracle__kernel.best_param is None just at the __init__
+        if expert.best_param  == None:
+            param_to_try = expert.choose_first_param()
+        else:
+            param_to_try = expert.choose_new_param()
+        
+        for param in param_to_try:
+
+            valid_stats = self.valid(dataspec, param)
+
+            # Better valid cost is obtained!
+            if self.update_trials( param, valid_stats ):
+
+                # Cross Validation
+                if self.validtype == 'cross':
+                    self.train( dataspec )
+
+                # Simple Validation
+                else:
+                    self.best_model = self.model
+                    if self.retrain_on_valid:
+
+                        # TODO: remove this part
+
+                        self.validtype = 'simple'
+
+                        train_stats = None
+                        test_stats = None
+                        if self.test_on_train:
+                            if self.verbosity > 2:
+                                print "\n** (testing simple valid on "+self.trainset_key+")"
+                            train_stats = self.test( trainset )
+                        if testset <> None:
+                            if self.verbosity > 2:
+                                print "\n** (testing simple valid on "+self.testset_key+")"
+                        test_stats = self.test( testset  )
+
+                        self.write_results( self.best_param,
+                                            valid_stats, test_stats, train_stats )
+
+                        self.validtype = 'simple+retrain'
+
+                        tv_set = ConcatRowsVMatrix(
+                                    sources = [ trainset,
+                                                validset
+                                                ],
+                                    fully_check_mappings = False,
+                                )
+                        if self.verbosity > 0:
+                            print "\n** re-training model on { train + valid } "
+                        self.train( {self.trainset_key: tv_set} )
+
+                train_stats = None
+                test_stats = None
+                if self.test_on_train:
+                    if self.verbosity > 2:
+                        print "\n** (testing on "+self.trainset_key+")"
+                    train_stats = self.test( trainset )
+                if testset <> None:
+                    if self.verbosity > 2:
+                        print "\n** (testing on "+self.testset_key+")"
+                    test_stats = self.test( testset )
+                self.update_trials( self.best_param,
+                                    None, test_stats, train_stats )
+                self.write_results( self.best_param,
+                                    self.valid_stats, self.test_stats, self.train_stats )
+
+            elif self.verbosity > 0:
+                print " -- valid costs: ", self.get_all_costs( valid_stats )
+
+        if self.retrain_until_local_optimum_is_found and expert.should_be_tuned_again():
+           self.train_and_tune( dataspec )
+
+        return dataspec
+
+
 def normalize_data(data,mean,std):
     if mean == None:
-       mean=[]
-       for i in range(len(data[0])):
-           mean.append( get_mean_cmp(data,i) )
+        mean=[]
+        for i in range(len(data[0])):
+            mean.append( get_mean_cmp(data,i) )
     if std == None:
-       std=[]
-       for i in range(len(data[0])):
-           std_tmp=get_std_cmp(data,i)
-           if std_tmp == 0.:
-              print "WARNING : standard deviation is 0 on component "+str(i)
-              std.append( 1. )
-           else:
-              std.append( std_tmp )
+        std=[]
+        for i in range(len(data[0])):
+            std_tmp=get_std_cmp(data,i)
+            if std_tmp == 0. and self.verbosity > 0:
+                print "WARNING : standard deviation is 0 on component "+str(i)
+                std.append( 1. )
+            else:
+                std.append( std_tmp )
     for i in range(len(data[0])):
         for j in range(len(data)):
             data[j][i]=(data[j][i]-mean[i])/std[i]
     return mean, std
 
 def mean_std(data):
-    stds=[get_std_cmp(data,i) for i in range(len(data[0]))]
-    return sum(stds)/len(stds)
+    stds=array([get_std_cmp(data,i) for i in range(len(data[0]))])
+    return stds.mean(), stds.std()
+
 def get_std_cmp(data,i):
     values=[float(vec[i]) for vec in data]
     tot = sum(values)
@@ -568,159 +1368,25 @@
 
 def arithm_mean(data):
     if type(data[0]) == list:
-       return [sum( [data[i][coor] for i in range(len(data))] )*1.0/len(data) for coor in range(len(data[0]))]
+     return [sum( [data[i][coor] for i in range(len(data))] )*1.0/len(data) for coor in range(len(data[0]))]
     else:
-       return sum(data)*1.0/len(data)
+     return sum(data)*1.0/len(data)
 def geom_mean(data):
     if type(data[0]) == list:
-       res=[]
-       for coor in range(len(data[0])):
-           res.append( geom_mean( [data[i][coor] for i in range(len(data))] ) )
-       return res
+        res=[]
+        for coor in range(len(data[0])):
+            res.append( geom_mean( [data[i][coor] for i in range(len(data))] ) )
+        return res
     else:
-       prod = 1.0
-       for value in data:
-           prod *= value
-       return prod**(1.0/len(data))
+        prod = 1.0
+        for value in data:
+            prod *= value
+        return prod**(1./len(data))
 
-def check_samples_target_list(samples_target_list):
-    if type(samples_target_list) != list or len(samples_target_list) == 0 or type(samples_target_list[0]) != list:
-       raise TypeError, "ERROR: samples_target_list must be a list of list (of arrays)"
-    else:
-       for samples_target in samples_target_list:
-           if len(samples_target) != 2:
-              raise TypeError, "ERROR: samples_target_list has an element with length "+str(len(samples_target))+" (instead of 2)"
-           if len(samples_target[0]) == 0 or len(samples_target[1]) == 0:
-              raise ValueError, "ERROR: samples_target_list has an element that has an element with an empty length"
-           if len(samples_target[0]) != len(samples_target[1]):
-              raise ValueError, "ERROR: samples_target_list has an element that has an elements with different len. Len are: " + len(samples_target[0])+" and " + len(samples_target[1])
-    if len(samples_target_list) not in [1,2,3]:
-       raise TypeError, "ERROR: samples_target_list have length "+str(len(samples_target_list))+" (not in [1,2,3])\n"+"samples_target_list has to be a list of [sample, target] arrays\n"+"for example :\n\t[[TrainSet, TrainLabels]]\n"+"\tor [[TrainSamples, TrainLabels], [ValidSamples, ValidLabels]]\n"+"\tor [[TrainSamples, TrainLabels], [ValidSamples, ValidLabels], [TestSamples, TestLabels]]\n"
-
-def parameters2list(C, kernel_parameters):
-          if kernel_parameters == None:
-             return [C]
-          elif type(kernel_parameters) == list:
-             return [C]+kernel_parameters
-          elif type(kernel_parameters) == tuple:
-             return [C]+[prm for prm in kernel_parameters]
-          else:
-             return [C, kernel_parameters ]
-
-
-
 if __name__ == '__main__':
 
     # an EXAMPLE to use the class...
 
-#>>># Initialization 
 
-    my_svm=SVM()
+    svm=SVM()
 
-#<<<#
-#>>># To save the results (progressively) in a ASCII file
-
-    my_svm.results_filename = 'my_svm_results.txt'
-   
-#<<<#
-#>>># Pre-processing your data : it is better to normalize...
-    
-    # Get the mean and standard deviation on the training set
-    # and normalize the training set (Mahalanobis)
-    #
-    mean, std = normalize_data(train_samples, None, None)
-    #
-    # DO NOT FORGET to apply the same normalization to other datasets
-    #
-    normalize_data(valid_samples, mean, std)
-    normalize_data(test_samples, mean, std)
-    
-#<<<#
-#>>># Defining train / valid data
-    # - CROSS-VALIDATION
-    
-    DATA = [ [train_samples , train_targets] ]
-    svm.nr_fold = 5  #(will train on 4/5 of the data and test on 1/5: this will be done 5 times)
-
-    # or...
-    # - SIMPLE VALIDATION
-    
-    DATA = [ [train_samples , train_targets] , [valid_samples , valid_targets] ]
-    
-    # Note:
-    # You can also do
-    #
-    # DATA = [ [train_samples , train_targets] , [valid_samples , valid_targets] , [test_samples , test_targets] ]
-    #
-    # (my_svm.error_rate will be the statistic on test set, and my_svm.valid_error_rate the statistics on the validation set)
-    #
-    # But it is not the most efficient way to do :
-    #     You'd better tune your model with validation,
-    #     and then test when you have the best model
-    
-    
-#<<<#
-#>>># Train several models with different sets of parameters and choosing the best set ("tuning"/"twicking")
-    # - my_svm.error_rate indicates the current error rates.
-    # - This error rate can only decrease while you run "train_and_tune"
-    #   (as you are tuning parameters so as to improve the results)
-    # So one should run train_and_tune several times (as long as he can wait), at least for the LINEAR and RBF kernel
-   
-    my_svm.train_and_tune( 'LINEAR' ,  DATA )
-    my_svm.train_and_tune( 'LINEAR' ,  DATA )
-    my_svm.train_and_tune( 'RBF' ,     DATA )
-    my_svm.train_and_tune( 'RBF' ,     DATA )
-    my_svm.train_and_tune( 'POLY' ,    DATA )
-    my_svm.train_and_tune( 'POLY' ,    DATA )
-    #[..]
-
-    valid_error_rate =  my_svm.valid_error_rate
-    print valid_error_rate
-    print my_svm.best_parameters
-    print my_svm.tried_parameters
-    
-#<<<#
-#>>># When training with new data (closed to the previous one)
-    # ones can need to forget the explored tables of parameters and accuracies
-    # while reminding the best set of parameters
-
-    my_svm.reset()
-   
-    # Cross-validation
-    NEW_DATA =  [ [train_samples , train_targets] ]
-    #
-    # or
-    #
-    # Simple validation
-    NEW_DATA =  [ [train_samples , train_targets], [valid_samples , valid_targets]  ]
-   
-    # If you want to tune again on the new data
-    my_svm.train_and_tune( 'RBF' , NEW_DATA )
-    #
-    # or
-    #
-    # If you want to try what give the best parameters
-    # (retrain the model on new train data, but no search for better parameters)
-    my_svm.train_and_test( NEW_DATA )
-    
-    valid_error_rate = my_svm.valid_error_rate
-
-#<<<#
-#>>># To try the best trained model with new data (and obtain "fair" error rates)
-   
-    TEST_DATA=[test_samples , test_targets]
-    
-    test_error_rate = my_svm.test( TEST_DATA )
-    
-    print test_error_rate
-
-#<<<#
-#>>># Doing all this amounts to the same as the following
-
-    ALL_DATA = [ [train_samples , train_targets], [valid_samples , valid_targets], [test_samples , test_targets]  ]
-    my_svm.train_and_tune( 'RBF' , ALL_DATA )
-    # [...]
-    
-    valid_error_rate = my_svm.valid_error_rate
-    test_error_rate  = my_svm.error_rate
-    



From chapados at mail.berlios.de  Thu Mar 20 16:42:21 2008
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Thu, 20 Mar 2008 16:42:21 +0100
Subject: [Plearn-commits] r8708 - trunk/plearn_learners/generic
Message-ID: <200803201542.m2KFgLjn031818@sheep.berlios.de>

Author: chapados
Date: 2008-03-20 16:42:20 +0100 (Thu, 20 Mar 2008)
New Revision: 8708

Modified:
   trunk/plearn_learners/generic/PLearner.cc
Log:
Added remote outputsize() function

Modified: trunk/plearn_learners/generic/PLearner.cc
===================================================================
--- trunk/plearn_learners/generic/PLearner.cc	2008-03-20 15:33:51 UTC (rev 8707)
+++ trunk/plearn_learners/generic/PLearner.cc	2008-03-20 15:42:20 UTC (rev 8708)
@@ -302,6 +302,10 @@
          RetDoc ("Current experiment directory")));
 
     declareMethod(
+        rmm, "outputsize", &PLearner::outputsize,
+        (BodyDoc("Return the learner outputsize")));
+    
+    declareMethod(
         rmm, "setTrainStatsCollector", &PLearner::setTrainStatsCollector,
         (BodyDoc("Sets the statistics collector whose update() method will be called\n"
                  "during training.\n."),



From nouiz at mail.berlios.de  Thu Mar 20 17:45:59 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 20 Mar 2008 17:45:59 +0100
Subject: [Plearn-commits] r8711 - trunk/plearn_learners/meta
Message-ID: <200803201645.m2KGjxot004433@sheep.berlios.de>

Author: nouiz
Date: 2008-03-20 17:45:59 +0100 (Thu, 20 Mar 2008)
New Revision: 8711

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
bugfix for computing train stats


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2008-03-20 16:43:28 UTC (rev 8710)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2008-03-20 16:45:59 UTC (rev 8711)
@@ -729,7 +729,12 @@
 void AdaBoost::computeCostsFromOutputs(const Vec& input, const Vec& output, 
                                        const Vec& target, Vec& costs) const
 {
-    PLASSERT(costs.size()==nTestCosts());
+    //when computing train stats, costs==nTrainCosts() 
+    //  and forward_sub_learner_test_costs==false
+    if(forward_sub_learner_test_costs)
+        PLASSERT(costs.size()==nTestCosts());
+    else
+        PLASSERT(costs.size()==nTrainCosts()||costs.size()==nTestCosts());
     costs.resize(5);
     costs.clear();
 
@@ -767,7 +772,7 @@
         costs.append(sum_weighted_costs);
     }
 
-    PLASSERT(costs.size()==nTestCosts());
+    PLASSERT(costs.size()==nTrainCosts()||costs.size()==nTestCosts());
 }
 
 TVec<string> AdaBoost::getTestCostNames() const



From nouiz at mail.berlios.de  Thu Mar 20 17:39:48 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 20 Mar 2008 17:39:48 +0100
Subject: [Plearn-commits] r8709 - trunk/plearn/vmat
Message-ID: <200803201639.m2KGdmvW003911@sheep.berlios.de>

Author: nouiz
Date: 2008-03-20 17:39:48 +0100 (Thu, 20 Mar 2008)
New Revision: 8709

Modified:
   trunk/plearn/vmat/SourceVMatrix.cc
   trunk/plearn/vmat/SourceVMatrix.h
Log:
Added a parameter in SourceVMatrix called deep_copy_source. It is initialized to true. Iff deep_copy_source==true, we make the deep copy of the source.

This is usefull to remove deep copy of matrix that do not change.


Modified: trunk/plearn/vmat/SourceVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SourceVMatrix.cc	2008-03-20 15:42:20 UTC (rev 8708)
+++ trunk/plearn/vmat/SourceVMatrix.cc	2008-03-20 16:39:48 UTC (rev 8709)
@@ -50,7 +50,8 @@
 
 
 SourceVMatrix::SourceVMatrix(bool call_build_)
-    : inherited(call_build_)
+    : inherited(call_build_),
+      deep_copy_source(true)
 {
     if( call_build_ )
         build_();
@@ -58,7 +59,8 @@
 
 SourceVMatrix::SourceVMatrix(VMat the_source, bool call_build_)
     : inherited(call_build_),
-      source(the_source)
+      source(the_source),
+      deep_copy_source(true)
 {
     if( call_build_ )
         build_();
@@ -67,7 +69,8 @@
 SourceVMatrix::SourceVMatrix(VMat the_source, int the_length, int the_width,
                              bool call_build_)
     : inherited(the_length, the_width, call_build_),
-      source(the_source)
+      source(the_source),
+      deep_copy_source(true)
 {
     if( call_build_ )
         build_();
@@ -111,6 +114,9 @@
     declareOption(ol, "source", &SourceVMatrix::source, OptionBase::buildoption,
                   "The source VMatrix");
 
+    declareOption(ol, "deep_copy_source", &SourceVMatrix::deep_copy_source,
+                  OptionBase::buildoption,
+                  "If True, we make a deep copy of the source.");
     /*
       declareOption(ol, "dependencies", &SourceVMatrix::dependencies, OptionBase::buildoption,
       "a list of paths to files that this VMat depends on. \n"
@@ -151,7 +157,8 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(sourcerow, copies);
-    deepCopyField(source, copies);
+    if(deep_copy_source)
+        deepCopyField(source, copies);
 }
 
 ///////////////

Modified: trunk/plearn/vmat/SourceVMatrix.h
===================================================================
--- trunk/plearn/vmat/SourceVMatrix.h	2008-03-20 15:42:20 UTC (rev 8708)
+++ trunk/plearn/vmat/SourceVMatrix.h	2008-03-20 16:39:48 UTC (rev 8709)
@@ -84,6 +84,8 @@
     // ************************
 
     VMat source;
+    
+    bool deep_copy_source;//Default true, if true, we deep copy the source
 
     //  TVec<string> dependencies; // a list of paths to files that this VMat depends on
 



From nouiz at mail.berlios.de  Thu Mar 20 17:49:20 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 20 Mar 2008 17:49:20 +0100
Subject: [Plearn-commits] r8712 - trunk/plearn_learners/meta
Message-ID: <200803201649.m2KGnKoG004779@sheep.berlios.de>

Author: nouiz
Date: 2008-03-20 17:49:19 +0100 (Thu, 20 Mar 2008)
New Revision: 8712

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
   trunk/plearn_learners/meta/MultiClassAdaBoost.h
Log:
implemented MultiClassAdaBoost::computeOutputAndCosts for performance


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-03-20 16:45:59 UTC (rev 8711)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-03-20 16:49:19 UTC (rev 8712)
@@ -200,7 +200,6 @@
     PLWARNING("In MultiClassAdaBoost::train() - not implemented, should be already trained");
 }
 
-
 void MultiClassAdaBoost::computeOutput(const Vec& input, Vec& output) const
 {
     output.resize(outputsize());
@@ -229,7 +228,76 @@
     output[1]=tmp1[0];
     output[2]=tmp2[0];
 }
+void MultiClassAdaBoost::computeOutputAndCosts(const Vec& input,
+                                               const Vec& target,
+                                               Vec& output, Vec& costs) const
+{
+    PLASSERT(costs.size()==nTestCosts());
 
+    output.resize(outputsize());
+
+    Vec output1(learner1.outputsize());
+    Vec output2(learner2.outputsize());
+    Vec subcosts1(learner1.nTestCosts());
+    Vec subcosts2(learner1.nTestCosts());
+
+    learner1.computeOutputAndCosts(input, target, output1, subcosts1);
+    learner2.computeOutputAndCosts(input, target, output2, subcosts2);
+
+    int ind1=int(round(output1[0]));
+    int ind2=int(round(output2[0]));
+    int ind=-1;
+    if(ind1==0 && ind2==0)
+        ind=0;
+    else if(ind1==1 && ind2==0)
+        ind=1;
+    else if(ind1==1 && ind2==1)
+        ind=2;
+    else
+        ind=1;//TODOself.confusion_target;
+    output[0]=ind;
+    output[1]=output1[0];
+    output[2]=output2[0];
+
+    int out = ind;
+    int pred = int(round(target[0]));
+    costs[0]=int(out != pred);//class_error
+    costs[1]=abs(out-pred);//linear_class_error
+    costs[2]=pow(real(abs(out-pred)),2);//square_class_error
+    
+    //append conflict cost
+    if(fast_is_equal(round(output[1]),0) 
+       && fast_is_equal(round(output[2]),1))
+        costs[3]=1;
+    else
+        costs[3]=0;
+
+    costs[4]=costs[5]=costs[6]=0;
+    costs[out+4]=1;
+
+    if(forward_sub_learner_test_costs){
+        costs.resize(7);
+        Vec subcosts1(learner1.nTestCosts());
+        Vec subcosts2(learner1.nTestCosts());
+        Vec target1(0,1), target2(0,1);
+        if(fast_is_equal(target[0],0.)){
+            target1.append(0);
+            target2.append(0);
+        }else if(fast_is_equal(target[0],1.)){
+            target1.append(1);
+            target2.append(0);
+        }else if(fast_is_equal(target[0],2.)){
+            target1.append(1);
+            target2.append(1);
+        }
+        subcosts1+=subcosts2;
+        costs.append(subcosts1);
+    }
+
+    PLASSERT(costs.size()==nTestCosts());
+
+}
+
 void MultiClassAdaBoost::computeCostsFromOutputs(const Vec& input, const Vec& output,
                                            const Vec& target, Vec& costs) const
 {

Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.h	2008-03-20 16:45:59 UTC (rev 8711)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.h	2008-03-20 16:49:19 UTC (rev 8712)
@@ -59,6 +59,9 @@
 {
     typedef PLearner inherited;
 
+    Vec tmp_costs;
+    Vec tmp_target;
+
 public:
     //#####  Public Build Options  ############################################
 
@@ -127,8 +130,8 @@
     // While in general not necessary, in case of particular needs
     // (efficiency concerns for ex) you may also want to overload
     // some of the following methods:
-    // virtual void computeOutputAndCosts(const Vec& input, const Vec& target,
-    //                                    Vec& output, Vec& costs) const;
+    virtual void computeOutputAndCosts(const Vec& input, const Vec& target,
+                                       Vec& output, Vec& costs) const;
     // virtual void computeCostsOnly(const Vec& input, const Vec& target,
     //                               Vec& costs) const;
     // virtual void test(VMat testset, PP<VecStatsCollector> test_stats,



From tihocan at mail.berlios.de  Thu Mar 20 16:33:52 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 20 Mar 2008 16:33:52 +0100
Subject: [Plearn-commits] r8707 - trunk/python_modules/plearn/pyext
Message-ID: <200803201533.m2KFXqWT031316@sheep.berlios.de>

Author: tihocan
Date: 2008-03-20 16:33:51 +0100 (Thu, 20 Mar 2008)
New Revision: 8707

Modified:
   trunk/python_modules/plearn/pyext/__init__.py
Log:
Reverted to previous version since it broke option parsing in Python scripts

Modified: trunk/python_modules/plearn/pyext/__init__.py
===================================================================
--- trunk/python_modules/plearn/pyext/__init__.py	2008-03-20 02:12:59 UTC (rev 8706)
+++ trunk/python_modules/plearn/pyext/__init__.py	2008-03-20 15:33:51 UTC (rev 8707)
@@ -75,21 +75,20 @@
         return [ content[i*ncols:(i+1)*ncols] for i in range(nrows) ]
 
 
-def optDialog():
-    from plearn.utilities import options_dialog as od
-    verb, logs, namespaces, use_gui= od.getGuiInfo(sys.argv)
+from plearn.utilities.options_dialog import *
+verb, logs, namespaces, use_gui= getGuiInfo(sys.argv)
 
-    # Enact the use of plargs: the current behavior is to consider as a plargs
-    # any command-line argument that contains a '=' char and to neglect all
-    # others
-    plargs.parse([ arg for arg in sys.argv if arg.find('=') != -1 ])
+# Enact the use of plargs: the current behavior is to consider as a plargs
+# any command-line argument that contains a '=' char and to neglect all
+# others
+plargs.parse([ arg for arg in sys.argv if arg.find('=') != -1 ])
 
-    if use_gui:
-        runit, verb, logs= od.optionsDialog(sys.argv[0], plargs.expdir,
-                                            verb, logs, namespaces)
-        if not runit:
-            sys.exit()
-        loggingControl(verb, logs)
+if use_gui:
+    runit, verb, logs= optionsDialog(sys.argv[0], plargs.expdir,
+                                     verb, logs, namespaces)
+    if not runit:
+        sys.exit()
+    loggingControl(verb, logs)
 
 pl.AutoVMatrix()
 AutoVMatrix.__len__ = lambda self: self.length



From nouiz at mail.berlios.de  Thu Mar 20 17:43:29 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 20 Mar 2008 17:43:29 +0100
Subject: [Plearn-commits] r8710 - trunk/plearn_learners/meta
Message-ID: <200803201643.m2KGhTbR004134@sheep.berlios.de>

Author: nouiz
Date: 2008-03-20 17:43:28 +0100 (Thu, 20 Mar 2008)
New Revision: 8710

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
   trunk/plearn_learners/meta/AdaBoost.h
Log:
can revert to an old state. Currently we can't continue to learn, but this can be added.


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2008-03-20 16:39:48 UTC (rev 8709)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2008-03-20 16:43:28 UTC (rev 8710)
@@ -280,6 +280,7 @@
     weak_learners.resize(0, nstages);
     voting_weights.resize(0, nstages);
     sum_voting_weights = 0;
+    found_zero_error_weak_learner=false;
     if (seed_ >= 0)
         manual_seed(seed_);
     else
@@ -298,9 +299,29 @@
         PLERROR("In AdaBoost::train, targetsize should be 1, found %d", 
                 train_set->targetsize());
 
-    if (nstages < stage)        //!< Asking to revert to previous stage
-        forget();
+    if (nstages < stage){        //!< Asking to revert to previous stage
+        PLCHECK(nstages>0); // should use forget
+        cout<<"In AdaBoost::train() - reverting to an old stage "<<stage<<" with nstages "<<nstages<<endl;
+        stage = nstages;
+        PLCHECK(learners_error.size()>=stage);
+        PLCHECK(weak_learners.size()>=stage);
+        PLCHECK(voting_weights.size()>=stage);
+        PLCHECK(nstages>0);
+        learners_error.resize(stage);
+        weak_learners.resize(stage);
+        voting_weights.resize(stage);
+        sum_voting_weights = sum(voting_weights);
+        found_zero_error_weak_learner=false;
 
+        example_weights.resize(0);
+        return;
+        //need examples_weights
+        //computeTrainingError();
+
+    }else if(nstages>0 && stage>0 && example_weights.size()==0){
+        PLERROR("In AdaBoost::train() -  we can't retrain a reverted learner...");
+    }
+    
     if(found_zero_error_weak_learner) // Training is over...
         return;
 
@@ -636,49 +657,8 @@
         }
         example_weights *= real(1.0)/sum_w;
 
-        if (compute_training_error)
-        {
-            {
-                PP<ProgressBar> pb;
-                if(report_progress) pb = new ProgressBar("computing weighted training error of whole model",n);
-                train_stats->forget();
-                Vec err(nTrainCosts());
-                int nb_class_0=0;
-                int nb_class_1=0;
-                real cum_weights_0=0;
-                real cum_weights_1=0;
+        computeTrainingError(input, target);
 
-                bool save_forward_sub_learner_test_costs = 
-                    forward_sub_learner_test_costs;
-                forward_sub_learner_test_costs=false;
-                for (int i=0;i<n;i++)
-                {
-                    if(report_progress) pb->update(i);
-                    train_set->getExample(i, input, target, weight);
-                    computeCostsOnly(input,target,err);
-                    if(fast_is_equal(target[0],0.)){
-                        cum_weights_0 += example_weights[i];
-                        nb_class_0++;
-                    }else{
-                        cum_weights_1 += example_weights[i];
-                        nb_class_1++;
-                    }
-                    err[3]=cum_weights_0/nb_class_0;
-                    err[4]=cum_weights_1/nb_class_1;
-                    train_stats->update(err);
-                }
-                train_stats->finalize();
-                forward_sub_learner_test_costs = 
-                    save_forward_sub_learner_test_costs;
-
-            }
-            if (verbosity>2)
-                cout << "At stage " << stage << 
-                    " boosted (weighted) classification error on training set = " 
-                     << train_stats->getMean() << endl;
-     
-        }
-
         if(fast_exact_is_equal(learners_error[stage], 0))
         {
             cout << "AdaBoost::train found weak learner with 0 training "
@@ -716,6 +696,8 @@
 }
 void AdaBoost::computeOutput(const Vec& input, Vec& output, int nb_learner) const
 {
+    if(nb_learner<0)
+        nb_learner=weak_learners.size();
     PLASSERT(nb_learner>0);
     real local_sum_weight = sum_voting_weights;
     if (nb_learner>voting_weights.length() and not found_zero_error_weak_learner){
@@ -773,11 +755,6 @@
     else
         costs[3]=costs[4]=MISSING_VALUE;
 
-    PP<VMatrix> the_train_set = train_set;
-    if(!train_set)
-        the_train_set=sorted_train_set;
-    PLASSERT(the_train_set);
-
     if(forward_sub_learner_test_costs){
         Vec weighted_costs(weak_learners[0]->nTestCosts());
         Vec sum_weighted_costs(weak_learners[0]->nTestCosts());
@@ -832,6 +809,57 @@
     return tmp_output2;
 }
 
+
+void AdaBoost::computeTrainingError(Vec input, Vec target)
+{
+    if (compute_training_error)
+    {
+        PP<VMatrix> the_train_set = train_set;
+        if(!train_set)
+            the_train_set=sorted_train_set;
+        PLASSERT(the_train_set);
+        int n=the_train_set->length();
+        PP<ProgressBar> pb;
+        if(report_progress) pb = new ProgressBar("computing weighted training error of whole model",n);
+        train_stats->forget();
+        Vec err(nTrainCosts());
+        int nb_class_0=0;
+        int nb_class_1=0;
+        real cum_weights_0=0;
+        real cum_weights_1=0;
+
+        bool save_forward_sub_learner_test_costs = 
+            forward_sub_learner_test_costs;
+        forward_sub_learner_test_costs=false;
+        real weight;
+        for (int i=0;i<n;i++)
+        {
+            if(report_progress) pb->update(i);
+            train_set->getExample(i, input, target, weight);
+            computeCostsOnly(input,target,err);
+            if(fast_is_equal(target[0],0.)){
+                cum_weights_0 += example_weights[i];
+                nb_class_0++;
+            }else{
+                cum_weights_1 += example_weights[i];
+                nb_class_1++;
+            }
+            err[3]=cum_weights_0/nb_class_0;
+            err[4]=cum_weights_1/nb_class_1;
+            train_stats->update(err);
+        }
+        train_stats->finalize();
+        forward_sub_learner_test_costs = 
+            save_forward_sub_learner_test_costs;
+
+        if (verbosity>2)
+            cout << "At stage " << stage << 
+                " boosted (weighted) classification error on training set = " 
+                 << train_stats->getMean() << endl;
+     
+    }
+}
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/meta/AdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.h	2008-03-20 16:39:48 UTC (rev 8709)
+++ trunk/plearn_learners/meta/AdaBoost.h	2008-03-20 16:43:28 UTC (rev 8710)
@@ -149,6 +149,7 @@
     //! be modified by subsequent use of this object, and thus should be copied
     //! if it needs to be stored safely.
     Vec remote_computeOutput_at_stage(const Vec& input, const int stage) const;
+    void computeTrainingError(Vec input, Vec target);
 
 protected: 
     //! Declares this class' options



From nouiz at mail.berlios.de  Thu Mar 20 18:46:45 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 20 Mar 2008 18:46:45 +0100
Subject: [Plearn-commits] r8713 - trunk/python_modules/plearn/parallel
Message-ID: <200803201746.m2KHkjIS004108@sheep.berlios.de>

Author: nouiz
Date: 2008-03-20 18:46:44 +0100 (Thu, 20 Mar 2008)
New Revision: 8713

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
allow passing of shell command


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-03-20 16:49:19 UTC (rev 8712)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-03-20 17:46:44 UTC (rev 8713)
@@ -677,7 +677,11 @@
                 c2=""
 
             # We use the absolute path so that we don't have corner case as with ./
-            c = os.path.normpath(os.path.join(os.getcwd(), c))
+            shellcommand=False
+            if c=="touch" or c=="echo":
+                shellcommand=True
+            else:
+                c = os.path.normpath(os.path.join(os.getcwd(), c))
             command = "".join([c,c2])
 
                 # We will execute the command on the specified architecture
@@ -725,8 +729,10 @@
                 self.targetcondorplatform=self.cplat
                 newcommand=command
 
-            if not os.path.exists(c):
-                raise Exception("The command '"+c+"' do not exist!")
+            if shellcommand:
+                pass
+            elif not os.path.exists(c):
+                raise Exception("The command '"+c+"' do not exist! You must provide the full path to the executable")
             elif not os.access(c, os.X_OK):
                 raise Exception("The command '"+c+"' do not have execution permission!")
 



From louradou at mail.berlios.de  Thu Mar 20 22:29:07 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Thu, 20 Mar 2008 22:29:07 +0100
Subject: [Plearn-commits] r8714 - trunk/plearn_learners/distributions
Message-ID: <200803202129.m2KLT72g018580@sheep.berlios.de>

Author: louradou
Date: 2008-03-20 22:29:06 +0100 (Thu, 20 Mar 2008)
New Revision: 8714

Modified:
   trunk/plearn_learners/distributions/GaussianDistribution.cc
   trunk/plearn_learners/distributions/GaussianDistribution.h
Log:
added an option to keep the covariance matrix fixed.
This is useful when a covariance is shared among
several Gaussian, such as in LDA.




Modified: trunk/plearn_learners/distributions/GaussianDistribution.cc
===================================================================
--- trunk/plearn_learners/distributions/GaussianDistribution.cc	2008-03-20 17:46:44 UTC (rev 8713)
+++ trunk/plearn_learners/distributions/GaussianDistribution.cc	2008-03-20 21:29:06 UTC (rev 8714)
@@ -121,6 +121,9 @@
     declareOption(ol, "given_mu", &GaussianDistribution::given_mu, OptionBase::buildoption,
                   "If this is set (i.e. not an empty vec), then train will not learn mu from the data, but simply copy its value given here.");
 
+    declareOption(ol, "given_covarmat", &GaussianDistribution::given_covarmat, OptionBase::buildoption,
+                  "If this is set (i.e. not an empty mat), then train will not learn covar from the data, but simply copy its value given here.");
+
     // Learnt options
     declareOption(ol, "mu", &GaussianDistribution::mu, OptionBase::learntoption, "");
     declareOption(ol, "covarmat", &GaussianDistribution::covarmat, OptionBase::learntoption, "");
@@ -184,6 +187,7 @@
     // First get mean and covariance
     if(given_mu.length()>0)
     { // we have a fixed given_mu
+        PLASSERT(given_covarmat.length()==0);
         d = given_mu.length();
         mu.resize(d);
         mu << given_mu;
@@ -194,6 +198,20 @@
         else
             PLERROR("In GaussianDistribution, weightsize can only be 0 or 1");
     }
+    else if(given_covarmat.length()>0)
+    {
+        d=given_covarmat.length();
+        PLASSERT(d==given_covarmat.width());
+        covarmat.resize(d,d);
+        covarmat << given_covarmat;
+        if(ws==0)
+            computeMean(training_set, mu);
+        else if(ws==1)
+            computeInputMean(training_set, mu);
+        else
+            PLERROR("In GaussianDistribution, weightsize can only be 0 or 1");
+       
+    }
     else
     {
         if(ws==0)

Modified: trunk/plearn_learners/distributions/GaussianDistribution.h
===================================================================
--- trunk/plearn_learners/distributions/GaussianDistribution.h	2008-03-20 17:46:44 UTC (rev 8713)
+++ trunk/plearn_learners/distributions/GaussianDistribution.h	2008-03-20 21:29:06 UTC (rev 8714)
@@ -74,6 +74,7 @@
     bool use_last_eig;
     float ignore_weights_below; //!< When doing a weighted fitting (weightsize==1), points with a weight below this value will be ignored
     Vec given_mu;
+    Mat given_covarmat;
 
 public:
     GaussianDistribution();



From louradou at mail.berlios.de  Thu Mar 20 22:30:59 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Thu, 20 Mar 2008 22:30:59 +0100
Subject: [Plearn-commits] r8715 - trunk/plearn_learners/classifiers
Message-ID: <200803202130.m2KLUxLF018884@sheep.berlios.de>

Author: louradou
Date: 2008-03-20 22:30:59 +0100 (Thu, 20 Mar 2008)
New Revision: 8715

Modified:
   trunk/plearn_learners/classifiers/ToBagClassifier.cc
Log:
a PLASSER in updateCostAndBagOutput was too strong
(numerical imprecision). I made it weaker.



Modified: trunk/plearn_learners/classifiers/ToBagClassifier.cc
===================================================================
--- trunk/plearn_learners/classifiers/ToBagClassifier.cc	2008-03-20 21:29:06 UTC (rev 8714)
+++ trunk/plearn_learners/classifiers/ToBagClassifier.cc	2008-03-20 21:30:59 UTC (rev 8715)
@@ -194,7 +194,7 @@
                                              const Vec& output,
                                              Vec& costs) const
 {
-    PLASSERT( is_equal(sum(output), 1) );   // Ensure probabilities sum to 1.
+    PLASSERT( sum(output) < 1.001 & sum(output) > 0.999);
     int bag_info = int(round(target.lastElement()));
     if (bag_info & SumOverBagsVariable::TARGET_COLUMN_FIRST)
         bag_output.resize(0, 0);



From tihocan at mail.berlios.de  Tue Mar 25 19:44:26 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 25 Mar 2008 19:44:26 +0100
Subject: [Plearn-commits] r8716 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200803251844.m2PIiQRM000546@sheep.berlios.de>

Author: tihocan
Date: 2008-03-25 19:44:26 +0100 (Tue, 25 Mar 2008)
New Revision: 8716

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
Log:
Added mechanism to display the effective mini-batch size

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2008-03-20 21:30:59 UTC (rev 8715)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2008-03-25 18:44:26 UTC (rev 8716)
@@ -869,6 +869,10 @@
                                                : stage_incr_per_cpu + 1;
 
     PP<PTimer> ptimer;
+    // Number of mini-batches that have been processed before one update.
+    int n_minibatches_per_update = 0;
+    StatsCollector nmbpu_stats;
+
     if (iam == 0) {
         //tmp_log << "Starting loop" << endl;
         //tmp_log.flush();
@@ -909,6 +913,7 @@
                     iam, cur_stage, samples_str.c_str());
                     */
             onlineStep(cur_stage, targets, train_costs, example_weights );
+            n_minibatches_per_update++;
             /*
             sleep(iam);
             string update = tostring(params_update);
@@ -947,6 +952,8 @@
                     all_params += params_update;
                     //params_update += all_params;
                     params_update.clear();
+                    nmbpu_stats.update(real(n_minibatches_per_update));
+                    n_minibatches_per_update = 0;
                     performed_update = true;
                 }
                 if (nsteps > 0) {
@@ -1043,6 +1050,15 @@
         int sem_value = semctl(semaphore_id, 0, GETVAL);
         if (sem_value == iam || iam == 0) {
             if (sem_value == iam && wait_for_final_update) {
+
+                // Display statistics for effective sizes of mini-batches.
+                /*
+                pout << "CPU " << iam << ": " << endl
+                    << " - mean  : " << nmbpu_stats.mean() << endl
+                    << " - stderr: " << nmbpu_stats.stderror() << endl
+                    << " - median: " << nmbpu_stats.pseudo_quantile(0.5) << endl;
+                */
+
                 if (nsteps >  0) {
                     //printf("CPU %d final updating (nsteps =%d)\n", iam, nsteps);
                     if (delayed_update) {



From nouiz at mail.berlios.de  Tue Mar 25 20:42:08 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 25 Mar 2008 20:42:08 +0100
Subject: [Plearn-commits] r8717 - trunk/python_modules/plearn/parallel
Message-ID: <200803251942.m2PJg8Yo005815@sheep.berlios.de>

Author: nouiz
Date: 2008-03-25 20:42:07 +0100 (Tue, 25 Mar 2008)
New Revision: 8717

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
at DIRO, allow to run in only a specified directory. Otherwise, their is bug/missinf feature in condor...


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-03-25 18:44:26 UTC (rev 8716)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-03-25 19:42:07 UTC (rev 8717)
@@ -652,6 +652,9 @@
 class DBICondor(DBIBase):
 
     def __init__( self, commands, **args ):
+        from socket import gethostname
+        if (not os.path.abspath(os.path.curdir).startswith("/home/fringant2/")) and gethostname().endswith(".iro.umontreal.ca"):
+            raise Exception("You must be in a subfolder of /home/fringant2/")
         DBIBase.__init__(self, commands, **args)
         if not os.path.exists(self.log_dir):
             os.mkdir(self.log_dir) # condor log are always generated
@@ -678,7 +681,8 @@
 
             # We use the absolute path so that we don't have corner case as with ./
             shellcommand=False
-            if c=="touch" or c=="echo":
+            autorized_shell_command=[ "touch", "echo"]
+            if c in autorized_shell_command:
                 shellcommand=True
             else:
                 c = os.path.normpath(os.path.join(os.getcwd(), c))



From nouiz at mail.berlios.de  Tue Mar 25 20:43:05 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 25 Mar 2008 20:43:05 +0100
Subject: [Plearn-commits] r8718 - trunk/python_modules/plearn/learners
Message-ID: <200803251943.m2PJh5KV005933@sheep.berlios.de>

Author: nouiz
Date: 2008-03-25 20:43:05 +0100 (Tue, 25 Mar 2008)
New Revision: 8718

Modified:
   trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
Log:
bugfix caused by other commit


Modified: trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
===================================================================
--- trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2008-03-25 19:42:07 UTC (rev 8717)
+++ trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2008-03-25 19:43:05 UTC (rev 8718)
@@ -183,8 +183,8 @@
         costs=[]
         #calculate stats, outputs, costs
         for i in range(len(testMat)):
-            out1=testoutputs1[i][0]
-            out2=testoutputs2[i][0]
+            out1=testoutputs1.getRow(i)[0]
+            out2=testoutputs2.getRow(i)[0]
             ind1=int(round(out1))
             ind2=int(round(out2))
             if ind1==ind2==0:
@@ -202,8 +202,8 @@
             target=testMat[i][-1]
             cost=self.computeCostsFromOutput(input,output,target,
                                              forward_sub_learner_costs=False)
-            cost.extend(testcosts1[i])
-            cost.extend(testcosts2[i])
+            cost.extend(testcosts1.getRow(i))
+            cost.extend(testcosts2.getRow(i))
             test_stats.update(cost,1)
             if return_costs:
                 costs.append(cost)



From tihocan at mail.berlios.de  Tue Mar 25 21:53:50 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 25 Mar 2008 21:53:50 +0100
Subject: [Plearn-commits] r8719 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200803252053.m2PKro4E019169@sheep.berlios.de>

Author: tihocan
Date: 2008-03-25 21:53:50 +0100 (Tue, 25 Mar 2008)
New Revision: 8719

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
Log:
Fixed serious bug that was totally fucking up the whole thing

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2008-03-25 19:43:05 UTC (rev 8718)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2008-03-25 20:53:50 UTC (rev 8719)
@@ -871,7 +871,7 @@
     PP<PTimer> ptimer;
     // Number of mini-batches that have been processed before one update.
     int n_minibatches_per_update = 0;
-    StatsCollector nmbpu_stats;
+    StatsCollector nmbpu_stats(/*-1*/);
 
     if (iam == 0) {
         //tmp_log << "Starting loop" << endl;
@@ -915,6 +915,10 @@
             onlineStep(cur_stage, targets, train_costs, example_weights );
             n_minibatches_per_update++;
             /*
+            pout << "CPU " << iam << ": n_minibatches_per_update = "
+                 << n_minibatches_per_update << endl;
+                 */
+            /*
             sleep(iam);
             string update = tostring(params_update);
             printf("\nCPU %d's current update: %s\n", iam, update.c_str());
@@ -986,10 +990,16 @@
                     // If 'synchronize_update' is true this means all CPUs have
                     // updated the parameters.
                     break;
-            } else if (performed_update) {
-                // TODO We could break here by checking the 'n_ready'
-                // semaphore: once it is reset to zero everyone can exit at
-                // once without necessarily doing it in turn.
+            } else {
+                if (!synchronize_update)
+                    // We do not wait our turn: instead we move on to the next
+                    // minibatch.
+                    break;
+                if (performed_update) {
+                    // TODO We could break here by checking the 'n_ready'
+                    // semaphore: once it is reset to zero everyone can exit at
+                    // once without necessarily doing it in turn.
+                }
             }
             }
         }
@@ -1046,18 +1056,20 @@
     //tmp_log.flush();
 
     // Wait until it is our turn.
+    bool displayed_stats = false;
     while (true) {
         int sem_value = semctl(semaphore_id, 0, GETVAL);
         if (sem_value == iam || iam == 0) {
             if (sem_value == iam && wait_for_final_update) {
 
                 // Display statistics for effective sizes of mini-batches.
-                /*
-                pout << "CPU " << iam << ": " << endl
-                    << " - mean  : " << nmbpu_stats.mean() << endl
-                    << " - stderr: " << nmbpu_stats.stderror() << endl
-                    << " - median: " << nmbpu_stats.pseudo_quantile(0.5) << endl;
-                */
+                if (!displayed_stats) {
+                    pout << "CPU " << iam << ": " << endl
+                        << " - mean  : " << nmbpu_stats.mean() << endl
+                        << " - stderr: " << nmbpu_stats.stderror() << endl
+                        << " - median: " << nmbpu_stats.pseudo_quantile(0.5) << endl;
+                    displayed_stats = true;
+                }
 
                 if (nsteps >  0) {
                     //printf("CPU %d final updating (nsteps =%d)\n", iam, nsteps);



From nouiz at mail.berlios.de  Wed Mar 26 17:23:18 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 26 Mar 2008 17:23:18 +0100
Subject: [Plearn-commits] r8720 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200803261623.m2QGNIn6001897@sheep.berlios.de>

Author: nouiz
Date: 2008-03-26 17:23:17 +0100 (Wed, 26 Mar 2008)
New Revision: 8720

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
Added the option --force --interruptible and --cpu=X that are forwarded to cluster.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-03-25 20:53:50 UTC (rev 8719)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-03-26 16:23:17 UTC (rev 8720)
@@ -389,6 +389,9 @@
         self.duree=None
         self.arch=None
         self.cluster_wait=True
+        self.force=False
+        self.interruptible=False
+        self.cpu=1
         self.threads=[]
         self.started=0
         self.nb_proc=50
@@ -447,6 +450,12 @@
             command += " --wait"
         if self.mem:
             command += " --memoire "+self.mem
+        if self.force:
+            command += " --force"
+        if self.interruptible:
+            command += " --interruptible"
+        if self.cpu!=1:
+            command += " --cpu "+self.cpu
         if self.os:
             command += " --os "+self.os
         command += " --execute '"+ filename + "'"

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-03-25 20:53:50 UTC (rev 8719)
+++ trunk/scripts/dbidispatch	2008-03-26 16:23:17 UTC (rev 8720)
@@ -3,7 +3,7 @@
 from plearn.utilities.toolkit import search_file
 
 ScriptName="launchdbi.py"
-ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|*--nodbilog] [--condor|--bqtools[=nb_process]|--cluster[=nb_process]|--local[=nb_process]|--ssh[=nb_process]] [--nb_proc=nb_process] [--mem=X] [--os=X] [--test|--notest] [--long] [--micro[=nb_batch]] [--duree=X] [--wait|--nowait] [--req="CONDOR_REQUIREMENT"] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- signals the default option value.'
+ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|*--nodbilog] [--condor|--bqtools[=nb_process]|--cluster[=nb_process]|--local[=nb_process]|--ssh[=nb_process]] [--nb_proc=nb_process] [--mem=X] [--os=X] [--test|--notest] [--long] [--micro[=nb_batch]] [--duree=X] [--wait|--nowait] [--req="CONDOR_REQUIREMENT"] [--force] [--interruptible] [--cpu=nb_cpu_per_node] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- signals the default option value.'
 LongHelp="""Dispatches jobs with dbi.py. dbi can dispatch jobs on condor, bqtools, cluster, local and ssh. If no system is selected on the command line, we try them in the previous order. ssh is never automaticaly selected.
 
 %(ShortHelp)s
@@ -51,7 +51,10 @@
   The '--nowait' means the --wait option is not given to the cluster command, as in the default.
   The '--mem=X' speficify the number of meg the program need to execute.
   The '--os=X' speficify the os of the server: fc4 or fc7. Default: fc4
-  
+  The '--force' option is passed to cluster
+  The '--interruptible' option is passed to cluster
+  The '--cpu=nb_cpu_per_node' option is passed to cluster
+
 condor only options:
   The '--req=\"CONDOR_REQUIREMENT\"' option makes dbidispatch send additional option to DBI that will be used to generate additional requirement for condor. CONDOR_REQUIREMENT must follow the syntax of requirement for condor with one exception. The symbol '\"' must be escaped 3 times! So the requirement (Machine == \"computer.example.com\") must be writen in the following way:
 
@@ -180,6 +183,12 @@
         if len(argv)>7:
             assert(argv[7]=="=")
             dbi_param["micro"]=argv[8:]
+    elif argv == "--force":
+        dbi_param["force"]=True
+    elif argv == "--interruptible":
+        dbi_param["interruptible"]=True
+    elif argv.startswith("--cpu="):
+        dbi_param["cpu"]=argv[6:]
     elif argv[0:1] == '-':
 	print "Unknow option (%s)",argv
 	print ShortHelp



From nouiz at mail.berlios.de  Wed Mar 26 17:47:05 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 26 Mar 2008 17:47:05 +0100
Subject: [Plearn-commits] r8721 - trunk/scripts
Message-ID: <200803261647.m2QGl53U004794@sheep.berlios.de>

Author: nouiz
Date: 2008-03-26 17:47:05 +0100 (Wed, 26 Mar 2008)
New Revision: 8721

Modified:
   trunk/scripts/dbidispatch
Log:
refactored the parsing of parameter


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-03-26 16:23:17 UTC (rev 8720)
+++ trunk/scripts/dbidispatch	2008-03-26 16:47:05 UTC (rev 8721)
@@ -3,7 +3,7 @@
 from plearn.utilities.toolkit import search_file
 
 ScriptName="launchdbi.py"
-ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|*--nodbilog] [--condor|--bqtools[=nb_process]|--cluster[=nb_process]|--local[=nb_process]|--ssh[=nb_process]] [--nb_proc=nb_process] [--mem=X] [--os=X] [--test|--notest] [--long] [--micro[=nb_batch]] [--duree=X] [--wait|--nowait] [--req="CONDOR_REQUIREMENT"] [--force] [--interruptible] [--cpu=nb_cpu_per_node] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- signals the default option value.'
+ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|*--nodbilog] [--condor|--bqtools[=nb_process]|--cluster[=nb_process]|--local[=nb_process]|--ssh[=nb_process]] [--nb_proc=nb_process] [--mem=X] [--os=X] [--test|--no_test] [--long] [--micro[=nb_batch]] [--duree=X] [--wait|--nowait] [--req="CONDOR_REQUIREMENT"] [--force] [--interruptible] [--cpu=nb_cpu_per_node] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- signals the default option value.'
 LongHelp="""Dispatches jobs with dbi.py. dbi can dispatch jobs on condor, bqtools, cluster, local and ssh. If no system is selected on the command line, we try them in the previous order. ssh is never automaticaly selected.
 
 %(ShortHelp)s
@@ -13,7 +13,7 @@
   The --condor, --bqtools, --cluster, --local or --ssh option specify on which system the jobs will be sent. If not present, we will use the first available in the previously given order. ssh is never automaticaly selected.
   The --dbilog (--nodbilog) tells dbi to generate (or not) an additional log
   The '--test' option makes dbidispatch generate the file %(ScriptName)s, without executing it. That way you can see what dbidispatch generates. Also, this file calls dbi in test mode, so dbi executes everything in the script except the experiment in %(ScriptName)s (so you can check the script).
-  The '--notest' option cancel the '--test' option.
+  The '--no_test' option cancel the '--test' option.
   The --file=FILEPATH specifies a file containing the jobs to execute, one per line. This is instead of specifying only one job on the command line.
 
 dbidispatch --test --file=tests
@@ -128,31 +128,10 @@
         dbi_param["dolog"]=False
     elif argv == "--dbilog":
         dbi_param["dolog"]=True
-    elif argv.startswith("--bqtools"):
-        launch_cmd = "bqtools"
-        if len(argv)>9:
-            assert(argv[9]=="=")
-            dbi_param["nb_proc"]=argv[10:]
-    elif argv.startswith("--cluster"):
-        launch_cmd = "Cluster"
-        if len(argv)>9:
-            assert(argv[9]=="=")
-            dbi_param["nb_proc"]=argv[10:]
-    elif argv == "--condor":
-        launch_cmd = "Condor"
-    elif argv.startswith("--duree="):
-        dbi_param["duree"]=argv[8:]
-    elif argv.startswith("--mem="):
-        dbi_param["mem"]=argv[6:]
-    elif argv.startswith("--os="):
-        dbi_param["os"]=argv[5:]
-    elif argv.startswith("--local"):
-        launch_cmd = "Local"
-        if len(argv)>7:
-            assert(argv[7]=="=")
-            dbi_param["nb_proc"]=argv[8:]
-    elif argv.startswith("--nb_proc="):
-        dbi_param["nb_proc"]=argv[10:]
+    elif argv.split('=')[0] in ["--bqtools","--cluster","--local","--condor"]:
+        launch_cmd = argv[2].upper()+argv.split('=')[0][3:]
+        if len(argv.split('='))>1:
+            dbi_param["nb_proc"]=argv.split('=')[1]
     elif argv.startswith("--ssh"):
         launch_cmd = "Ssh"
         if len(argv)>5:
@@ -160,13 +139,9 @@
             dbi_param["nb_proc"]=argv[6:]
         dbi_param["file_redirect_stdout"]=False
         dbi_param["file_redirect_stderr"]=False
-    elif argv == "--test":
-        dbi_param["test"]=True
-    elif argv == "--notest":
-        dbi_param["test"]=False
     elif argv.startswith("--file="):
         FILE = argv[7:]
-    elif argv == "--32"  or argv == "--64" or argv == "--3264":
+    elif argv in ["--32","--64","--3264"]:
         dbi_param["arch"]=argv[2:]
     elif argv == "--wait":
         dbi_param["cluster_wait"]=True
@@ -174,21 +149,17 @@
         dbi_param["cluster_wait"]=False
     elif argv[0:6] == "--req=":
         dbi_param["requirements"]="\"%s\""%argv[6:]
-    elif argv == "--no_clean_up":
-        dbi_param["clean_up"]=False
-    elif argv == "long":
-        dbi_param["long"] = True
     elif argv.startswith("--micro"):
         dbi_param["micro"]=20
         if len(argv)>7:
             assert(argv[7]=="=")
             dbi_param["micro"]=argv[8:]
-    elif argv == "--force":
-        dbi_param["force"]=True
-    elif argv == "--interruptible":
-        dbi_param["interruptible"]=True
-    elif argv.startswith("--cpu="):
-        dbi_param["cpu"]=argv[6:]
+    elif argv in  ["--force", "--interruptible", "--long", "--test"]:
+        dbi_param[argv[2:]]=True
+    elif argv in ["--no_test","--no_clean_up"]:
+        dbi_param[argv[5:]]=False
+    elif argv.split('=')[0] in ["--duree","--cpu","--mem","--os","--nb_proc"]:
+        dbi_param[argv.split('=')[0][2:]]=argv.split('=')[1]
     elif argv[0:1] == '-':
 	print "Unknow option (%s)",argv
 	print ShortHelp



From nouiz at mail.berlios.de  Wed Mar 26 19:35:37 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 26 Mar 2008 19:35:37 +0100
Subject: [Plearn-commits] r8722 - trunk/commands/PLearnCommands
Message-ID: <200803261835.m2QIZbOV005766@sheep.berlios.de>

Author: nouiz
Date: 2008-03-26 19:35:37 +0100 (Wed, 26 Mar 2008)
New Revision: 8722

Modified:
   trunk/commands/PLearnCommands/LearnerCommand.cc
Log:
Added the name to the output pmat file for plearn learner test


Modified: trunk/commands/PLearnCommands/LearnerCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/LearnerCommand.cc	2008-03-26 16:47:05 UTC (rev 8721)
+++ trunk/commands/PLearnCommands/LearnerCommand.cc	2008-03-26 18:35:37 UTC (rev 8722)
@@ -134,7 +134,7 @@
         testoutputs = new FileVMatrix(outputs_file,l,learner->outputsize());
     VMat testcosts;
     if(costs_file!="")
-        testcosts = new FileVMatrix(costs_file,l,learner->nTestCosts());
+        testcosts = new FileVMatrix(costs_file,l,learner->getTestCostNames());
 
     PP<VecStatsCollector> test_stats = new VecStatsCollector;
     test_stats->build();



From nouiz at mail.berlios.de  Wed Mar 26 21:44:01 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 26 Mar 2008 21:44:01 +0100
Subject: [Plearn-commits] r8724 - trunk/python_modules/plearn/parallel
Message-ID: <200803262044.m2QKi1dw022121@sheep.berlios.de>

Author: nouiz
Date: 2008-03-26 21:44:00 +0100 (Wed, 26 Mar 2008)
New Revision: 8724

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
create the file in the log directory


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-03-26 20:18:00 UTC (rev 8723)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-03-26 20:44:00 UTC (rev 8724)
@@ -777,7 +777,7 @@
                 param_dat.close()
 
 
-        condor_file = os.path.join(self.tmp_dir, self.unique_id + ".condor")
+        condor_file = os.path.join(self.log_dir, "submit_file.condor")
         self.temp_files.append(condor_file)
         condor_dat = open( condor_file, 'w' )
 



From nouiz at mail.berlios.de  Wed Mar 26 21:18:00 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 26 Mar 2008 21:18:00 +0100
Subject: [Plearn-commits] r8723 - trunk/plearn_learners/meta
Message-ID: <200803262018.m2QKI0Jr018099@sheep.berlios.de>

Author: nouiz
Date: 2008-03-26 21:18:00 +0100 (Wed, 26 Mar 2008)
New Revision: 8723

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
   trunk/plearn_learners/meta/AdaBoost.h
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
   trunk/plearn_learners/meta/MultiClassAdaBoost.h
Log:
-Adaboost now compute output as in the standard way of plearn.
-To compute output at an earlier stage, we must retrain with the wanted nstages. AdaBoost will revert to this stage.
  
This was done to simplify other part that was complicated by the fact that it was not done in the normal plearn way.


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2008-03-26 18:35:37 UTC (rev 8722)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2008-03-26 20:18:00 UTC (rev 8723)
@@ -220,25 +220,6 @@
     inherited::declareOptions(ol);
 }
 
-////////////////////
-// declareMethods //
-////////////////////
-void AdaBoost::declareMethods(RemoteMethodMap& rmm)
-{
-    // Insert a backpointer to remote methods; note that this
-    // different than for declareOptions()
-    rmm.inherited(inherited::_getRemoteMethodMap_());
-
-    declareMethod(
-        rmm, "computeOutput_at_stage", &AdaBoost::remote_computeOutput_at_stage,
-        (BodyDoc("On a trained learner, this computes the output from the "
-                 "input with the first stage weak learner. There must be "
-                 "enough weak learners that have been trained"),
-         ArgDoc ("input", "Input vector (should have width inputsize)"),
-         ArgDoc ("stage", "The number of stage to use to compute the output"),
-         RetDoc ("Computed output (will have width outputsize)")));
-
-}
 void AdaBoost::build_()
 {
     if(conf_rated_adaboost && pseudo_loss_adaboost)
@@ -289,19 +270,10 @@
 
 void AdaBoost::train()
 {
-    if(!train_set)
-        PLERROR("In AdaBoost::train, you did not setTrainingSet");
-    
-    if(!train_stats && compute_training_error)
-        PLERROR("In AdaBoost::train, you did not setTrainStatsCollector");
-
-    if (train_set->targetsize()!=1)
-        PLERROR("In AdaBoost::train, targetsize should be 1, found %d", 
-                train_set->targetsize());
-
     if (nstages < stage){        //!< Asking to revert to previous stage
         PLCHECK(nstages>0); // should use forget
-        cout<<"In AdaBoost::train() - reverting to an old stage "<<stage<<" with nstages "<<nstages<<endl;
+        cout<<"In AdaBoost::train() - reverting from stage "<<stage
+            <<" to stage "<<nstages<<endl;
         stage = nstages;
         PLCHECK(learners_error.size()>=stage);
         PLCHECK(weak_learners.size()>=stage);
@@ -322,6 +294,16 @@
         PLERROR("In AdaBoost::train() -  we can't retrain a reverted learner...");
     }
     
+    if(!train_set)
+        PLERROR("In AdaBoost::train, you did not setTrainingSet");
+    
+    if(!train_stats && compute_training_error)
+        PLERROR("In AdaBoost::train, you did not setTrainStatsCollector");
+
+    if (train_set->targetsize()!=1)
+        PLERROR("In AdaBoost::train, targetsize should be 1, found %d", 
+                train_set->targetsize());
+
     if(found_zero_error_weak_learner) // Training is over...
         return;
 
@@ -692,28 +674,11 @@
 
 void AdaBoost::computeOutput(const Vec& input, Vec& output) const
 {
-    computeOutput(input,output,voting_weights.length());
-}
-void AdaBoost::computeOutput(const Vec& input, Vec& output, int nb_learner) const
-{
-    if(nb_learner<0)
-        nb_learner=weak_learners.size();
-    PLASSERT(nb_learner>0);
-    real local_sum_weight = sum_voting_weights;
-    if (nb_learner>voting_weights.length() and not found_zero_error_weak_learner){
-        PLERROR("AdaBoost::computeOutput - Asked to compute the output with more learner(%d) then currently learned %d",
-                nb_learner,voting_weights.length());
-    }else if(nb_learner>voting_weights.length()){
-        nb_learner=voting_weights.length();
-    }else if(nb_learner != voting_weights.length()){
-        local_sum_weight = 0;
-        for (int i=0;i<nb_learner;i++)
-            local_sum_weight += voting_weights[i];
-    }
-    output.resize(weak_learner_template->outputsize());
+    PLASSERT(weak_learners.size()>0);
+    
     real sum_out=0;
-    weak_learner_output.resize(output.size());
-    for (int i=0;i<nb_learner;i++)
+    weak_learner_output.resize(weak_learners[0]->outputsize());
+    for (int i=0;i<weak_learners.size();i++)
     {
         weak_learners[i]->computeOutput(input,weak_learner_output);
         if(!pseudo_loss_adaboost && !conf_rated_adaboost)
@@ -722,7 +687,7 @@
         else
             sum_out += weak_learner_output[0]*voting_weights[i];
     }
-    output[0] = sum_out/local_sum_weight;
+    output[0] = sum_out/sum_voting_weights;
     output.resize(1);
 }
 
@@ -805,16 +770,6 @@
     return costs;
 }
 
-//! Version of computeOutput that returns a result by value
-Vec AdaBoost::remote_computeOutput_at_stage(const Vec& input,
-                                            const int stage) const
-{
-    tmp_output2.resize(outputsize());
-    computeOutput(input, tmp_output2, stage);
-    return tmp_output2;
-}
-
-
 void AdaBoost::computeTrainingError(Vec input, Vec target)
 {
     if (compute_training_error)

Modified: trunk/plearn_learners/meta/AdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.h	2008-03-26 18:35:37 UTC (rev 8722)
+++ trunk/plearn_learners/meta/AdaBoost.h	2008-03-26 20:18:00 UTC (rev 8723)
@@ -142,13 +142,6 @@
     //! This does the actual building. 
     void build_();
 
-    // List of methods that are called by Remote Method Invocation.  Our
-    // convention is to have them start with the remote_ prefix.
-
-    //! Compute output at a given stage. Note that the returned vector may
-    //! be modified by subsequent use of this object, and thus should be copied
-    //! if it needs to be stored safely.
-    Vec remote_computeOutput_at_stage(const Vec& input, const int stage) const;
     void computeTrainingError(Vec input, Vec target);
 
 protected: 
@@ -156,9 +149,6 @@
     // (Please implement in .cc)
     static void declareOptions(OptionList& ol);
 
-    //! Declare the methods that are remote-callable
-    static void declareMethods(RemoteMethodMap& rmm);
-
 public:
 
     // ************************
@@ -200,10 +190,6 @@
     //! Computes the output from the input
     virtual void computeOutput(const Vec& input, Vec& output) const;
 
-    //! Computes the output from the input with a specific number of learner
-    //! This way we don't need to save the learner at each stage, we can save just the last one
-    void computeOutput(const Vec& input, Vec& output, int nb_learner) const;
-
     //! Computes the costs from already computed output. 
     virtual void computeCostsFromOutputs(const Vec& input, const Vec& output, 
                                          const Vec& target, Vec& costs) const;

Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-03-26 18:35:37 UTC (rev 8722)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-03-26 20:18:00 UTC (rev 8723)
@@ -112,6 +112,9 @@
     // ###    options have been modified.
     // ### You should assume that the parent class' build_() has already been
     // ### called.
+    sub_target_tmp.resize(2);
+    for(int i=0;i<sub_target_tmp.size();i++)
+        sub_target_tmp[i].resize(1);
 }
 
 // ### Nothing to add here, simply calls build_
@@ -198,23 +201,32 @@
     }
     */
     PLWARNING("In MultiClassAdaBoost::train() - not implemented, should be already trained");
+    int stage1=learner1.stage;
+    int stage2=learner2.stage;
+
+    if(stage1>0 && stage1<nb_stage_to_use)
+        PLERROR("In  MultiClassAdaBoost::train() - asked to use more stage then already trained for learner1");
+    if(stage2>0 && stage2<nb_stage_to_use)
+        PLERROR("In  MultiClassAdaBoost::train() - asked to use more stage then already trained for learner1");
+    if(nb_stage_to_use>0){
+        learner1.nstages=nb_stage_to_use;
+        learner1.train();
+    }
+    if(nb_stage_to_use>0){
+        learner2.nstages=nb_stage_to_use;
+        learner2.train();
+    }
 }
 
 void MultiClassAdaBoost::computeOutput(const Vec& input, Vec& output) const
 {
-    output.resize(outputsize());
-
     Vec tmp1(learner1.outputsize());
     Vec tmp2(learner2.outputsize());
-    if(nb_stage_to_use!=-1){
-        learner1.computeOutput(input,tmp1,nb_stage_to_use);
-        learner2.computeOutput(input,tmp2,nb_stage_to_use);
-    }else{
-        learner1.computeOutput(input,tmp1);
-        learner2.computeOutput(input,tmp2);
-    }
+    learner1.computeOutput(input,tmp1);
+    learner2.computeOutput(input,tmp2);
     int ind1=int(round(tmp1[0]));
     int ind2=int(round(tmp2[0]));
+
     int ind=-1;
     if(ind1==0 && ind2==0)
         ind=0;
@@ -240,9 +252,12 @@
     Vec output2(learner2.outputsize());
     Vec subcosts1(learner1.nTestCosts());
     Vec subcosts2(learner1.nTestCosts());
+    getSubLearnerTarget(target, sub_target_tmp);
 
-    learner1.computeOutputAndCosts(input, target, output1, subcosts1);
-    learner2.computeOutputAndCosts(input, target, output2, subcosts2);
+    learner1.computeOutputAndCosts(input, sub_target_tmp[0],
+                                   output1, subcosts1);
+    learner2.computeOutputAndCosts(input, sub_target_tmp[1],
+                                   output2, subcosts2);
 
     int ind1=int(round(output1[0]));
     int ind2=int(round(output2[0]));
@@ -277,19 +292,6 @@
 
     if(forward_sub_learner_test_costs){
         costs.resize(7);
-        Vec subcosts1(learner1.nTestCosts());
-        Vec subcosts2(learner1.nTestCosts());
-        Vec target1(0,1), target2(0,1);
-        if(fast_is_equal(target[0],0.)){
-            target1.append(0);
-            target2.append(0);
-        }else if(fast_is_equal(target[0],1.)){
-            target1.append(1);
-            target2.append(0);
-        }else if(fast_is_equal(target[0],2.)){
-            target1.append(1);
-            target2.append(1);
-        }
         subcosts1+=subcosts2;
         costs.append(subcosts1);
     }
@@ -323,19 +325,10 @@
         costs.resize(7);
         Vec subcosts1(learner1.nTestCosts());
         Vec subcosts2(learner1.nTestCosts());
-        Vec target1(0,1), target2(0,1);
-        if(fast_is_equal(target[0],0.)){
-            target1.append(0);
-            target2.append(0);
-        }else if(fast_is_equal(target[0],1.)){
-            target1.append(1);
-            target2.append(0);
-        }else if(fast_is_equal(target[0],2.)){
-            target1.append(1);
-            target2.append(1);
-        }
-        learner1.computeCostsOnly(input,target1,subcosts1);
-        learner2.computeCostsOnly(input,target2,subcosts2);
+        getSubLearnerTarget(target, sub_target_tmp);
+
+        learner1.computeCostsOnly(input,sub_target_tmp[0],subcosts1);
+        learner2.computeCostsOnly(input,sub_target_tmp[1],subcosts2);
         subcosts1+=subcosts2;
         costs.append(subcosts1);
     }
@@ -379,7 +372,28 @@
     return names;
 }
 
-
+void MultiClassAdaBoost::getSubLearnerTarget(Vec target,
+                                             TVec<Vec> sub_target) 
+{
+    if(fast_is_equal(target[0],0.)){
+        sub_target[0]=0;
+        sub_target[1]=0;
+    }else if(fast_is_equal(target[0],1.)){
+        sub_target[0]=1;
+        sub_target[1]=0;
+    }else if(fast_is_equal(target[0],2.)){
+        sub_target[0]=1;
+        sub_target[1]=1;
+    }else if(target[0]>2){
+        PLWARNING("In MultiClassAdaBoost::getSubLearnerTarget - "
+                  "We only support target 0/1/2. We got %f. We transform "
+                  "it to a target of 2.", target[0]);
+        sub_target[0]=1;
+        sub_target[1]=1;
+    }else
+        PLERROR("In MultiClassAdaBoost::getSubLearnerTarget - "
+                  "We only support target 0/1/2. We got %f.", target[0]); 
+}
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.h	2008-03-26 18:35:37 UTC (rev 8722)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.h	2008-03-26 20:18:00 UTC (rev 8723)
@@ -156,6 +156,9 @@
     // (PLEASE IMPLEMENT IN .cc)
     virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
+
+//    virtual void setTrainingSet(VMat training_set, bool call_forget=true);
+
 protected:
     //#####  Protected Options  ###############################################
 
@@ -176,9 +179,10 @@
     // (PLEASE IMPLEMENT IN .cc)
     void build_();
 
+    static void getSubLearnerTarget(Vec target, TVec<Vec> sub_target);
 private:
     //#####  Private Data Members  ############################################
-
+    TVec<Vec> sub_target_tmp;
 };
 
 // Declares a few other classes and functions related to this class



From louradou at mail.berlios.de  Wed Mar 26 22:23:45 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Wed, 26 Mar 2008 22:23:45 +0100
Subject: [Plearn-commits] r8725 - trunk/python_modules/plearn/learners
Message-ID: <200803262123.m2QLNjEs025799@sheep.berlios.de>

Author: louradou
Date: 2008-03-26 22:23:44 +0100 (Wed, 26 Mar 2008)
New Revision: 8725

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:


Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2008-03-26 20:44:00 UTC (rev 8724)
+++ trunk/python_modules/plearn/learners/SVM.py	2008-03-26 21:23:44 UTC (rev 8725)
@@ -1096,8 +1096,6 @@
               teststats = None
              ):
         assert testset <> None
-        if self.verbosity > 3:
-            print "SVM::test() called"
         # By default take the LAST model trained
         if self.model <> None:
             model = self.model
@@ -1109,6 +1107,8 @@
         samples, targets = self.get_datalist( testset )
         nclasses = self.nclasses
         nsamples = len(samples)
+        if self.verbosity > 3:
+            print "SVM::test() called on ",nsamples," samples"
 
         costnames = self.costnames
         # Translation of the cost 'confusion_matrix'
@@ -1170,6 +1170,8 @@
                     raise ValueError, "computation of cost %s not implemented in SVM::test()" % cn
             teststats.update(statVec,1.)
         
+        if self.verbosity > 3:
+            print "SVM::test() computed:", teststats
         return teststats #, outputs, costs
 
 
@@ -1197,6 +1199,7 @@
             print "   with param %s" % param
         self.train(dataspec, param)
         validset = self.valid_inputspec(dataspec)
+        self.validset = validset
         return self.test( validset, validstats )
 
 
@@ -1268,8 +1271,14 @@
             valid_stats = self.valid(dataspec, param)
 
             # Better valid cost is obtained!
-            if self.update_trials( param, valid_stats ):
+            if not self.update_trials( param, valid_stats ):
+                if self.verbosity > 0:
+                    print " -- valid costs: ", self.get_all_costs( valid_stats )
 
+                # We reject the model (to avoid testing on it)
+                self.model = None
+            else:
+
                 # Cross Validation
                 if self.validtype == 'cross':
                     self.train( dataspec )
@@ -1324,9 +1333,6 @@
                 self.write_results( self.best_param,
                                     self.valid_stats, self.test_stats, self.train_stats )
 
-            elif self.verbosity > 0:
-                print " -- valid costs: ", self.get_all_costs( valid_stats )
-
         if self.retrain_until_local_optimum_is_found and expert.should_be_tuned_again():
            self.train_and_tune( dataspec )
 



From tihocan at mail.berlios.de  Thu Mar 27 04:15:41 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 27 Mar 2008 04:15:41 +0100
Subject: [Plearn-commits] r8726 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200803270315.m2R3FfQO018742@sheep.berlios.de>

Author: tihocan
Date: 2008-03-27 04:15:38 +0100 (Thu, 27 Mar 2008)
New Revision: 8726

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
Log:
Disabled debug output

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2008-03-26 21:23:44 UTC (rev 8725)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2008-03-27 03:15:38 UTC (rev 8726)
@@ -1056,7 +1056,7 @@
     //tmp_log.flush();
 
     // Wait until it is our turn.
-    bool displayed_stats = false;
+    bool displayed_stats = true;
     while (true) {
         int sem_value = semctl(semaphore_id, 0, GETVAL);
         if (sem_value == iam || iam == 0) {



From tihocan at mail.berlios.de  Thu Mar 27 04:23:45 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 27 Mar 2008 04:23:45 +0100
Subject: [Plearn-commits] r8727 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200803270323.m2R3NjZq019667@sheep.berlios.de>

Author: tihocan
Date: 2008-03-27 04:23:44 +0100 (Thu, 27 Mar 2008)
New Revision: 8727

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
Log:
Fixed compilation issue

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2008-03-27 03:15:38 UTC (rev 8726)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2008-03-27 03:23:44 UTC (rev 8727)
@@ -871,7 +871,7 @@
     PP<PTimer> ptimer;
     // Number of mini-batches that have been processed before one update.
     int n_minibatches_per_update = 0;
-    StatsCollector nmbpu_stats(/*-1*/);
+    StatsCollector nmbpu_stats; // Use -1 in constructor if you want the median.
 
     if (iam == 0) {
         //tmp_log << "Starting loop" << endl;



From tihocan at mail.berlios.de  Thu Mar 27 17:33:48 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 27 Mar 2008 17:33:48 +0100
Subject: [Plearn-commits] r8728 - trunk/plearn_learners/classifiers
Message-ID: <200803271633.m2RGXmL5004274@sheep.berlios.de>

Author: tihocan
Date: 2008-03-27 17:33:47 +0100 (Thu, 27 Mar 2008)
New Revision: 8728

Modified:
   trunk/plearn_learners/classifiers/ToBagClassifier.cc
Log:
Can now handle missing outputs in the underlying learner

Modified: trunk/plearn_learners/classifiers/ToBagClassifier.cc
===================================================================
--- trunk/plearn_learners/classifiers/ToBagClassifier.cc	2008-03-27 03:23:44 UTC (rev 8727)
+++ trunk/plearn_learners/classifiers/ToBagClassifier.cc	2008-03-27 16:33:47 UTC (rev 8728)
@@ -55,7 +55,10 @@
     "This learner can also compute the confusion matrix as a test cost, in\n"
     "addition to classification error. Each element of the confusion matrix\n"
     "is named 'cm_ij' with i the index of the true class, and j the index of\n"
-    "the predicted class.");
+    "the predicted class.\n"
+    "The underlying classifier may choose to not make any prediction on some\n"
+    "of the elements in the bag, in which case it should just return as\n"
+    "output a vector of missing values.");
 
 /////////////////////
 // ToBagClassifier //
@@ -194,13 +197,22 @@
                                              const Vec& output,
                                              Vec& costs) const
 {
+    costs.resize(nTestCosts());
+    costs.fill(MISSING_VALUE);
+    if (is_missing(output[0])) {
+        // Ignore missing outputs from learner.
+#ifdef BOUNDCHECK
+        for (int i = 1; i < output.length(); i++) {
+            PLASSERT( is_missing(output[i]) );
+        }
+        return;
+#endif
+    }
     PLASSERT( sum(output) < 1.001 & sum(output) > 0.999);
     int bag_info = int(round(target.lastElement()));
     if (bag_info & SumOverBagsVariable::TARGET_COLUMN_FIRST)
         bag_output.resize(0, 0);
     bag_output.appendRow(output);
-    costs.resize(nTestCosts());
-    costs.fill(MISSING_VALUE);
     if (bag_info & SumOverBagsVariable::TARGET_COLUMN_LAST) {
         // Perform majority vote.
         votes.resize(bag_output.width());



From nouiz at mail.berlios.de  Fri Mar 28 15:20:00 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 28 Mar 2008 15:20:00 +0100
Subject: [Plearn-commits] r8729 - trunk/plearn/vmat
Message-ID: <200803281420.m2SEK0uC018674@sheep.berlios.de>

Author: nouiz
Date: 2008-03-28 15:19:59 +0100 (Fri, 28 Mar 2008)
New Revision: 8729

Modified:
   trunk/plearn/vmat/ConcatColumnsVMatrix.cc
   trunk/plearn/vmat/SubVMatrix.cc
Log:
compute missing size when possible.


Modified: trunk/plearn/vmat/ConcatColumnsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ConcatColumnsVMatrix.cc	2008-03-27 16:33:47 UTC (rev 8728)
+++ trunk/plearn/vmat/ConcatColumnsVMatrix.cc	2008-03-28 14:19:59 UTC (rev 8729)
@@ -145,6 +145,8 @@
     if (no_duplicate_fieldnames) {
         unduplicateFieldNames();
     }
+
+    computeMissingSizeValue(false);
 }
 
 void ConcatColumnsVMatrix::getNewRow(int i, const Vec& samplevec) const

Modified: trunk/plearn/vmat/SubVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SubVMatrix.cc	2008-03-27 16:33:47 UTC (rev 8728)
+++ trunk/plearn/vmat/SubVMatrix.cc	2008-03-28 14:19:59 UTC (rev 8729)
@@ -172,6 +172,11 @@
     }
 
     // determine sizes
+    if(jstart==0 && width()==source->width() &&
+       inputsize()<0 && targetsize()<0 && weightsize()<0)
+        //We don't change the columns, so we can copy their size
+        copySizesFrom(source);
+    computeMissingSizeValue(false);
 /*
     if (width_ == source->width())
     {



From nouiz at mail.berlios.de  Fri Mar 28 15:41:05 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 28 Mar 2008 15:41:05 +0100
Subject: [Plearn-commits] r8730 - trunk/plearn_learners/generic
Message-ID: <200803281441.m2SEf5mP021537@sheep.berlios.de>

Author: nouiz
Date: 2008-03-28 15:41:05 +0100 (Fri, 28 Mar 2008)
New Revision: 8730

Modified:
   trunk/plearn_learners/generic/AddCostToLearner.cc
Log:
changed cost name to be more explicit


Modified: trunk/plearn_learners/generic/AddCostToLearner.cc
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.cc	2008-03-28 14:19:59 UTC (rev 8729)
+++ trunk/plearn_learners/generic/AddCostToLearner.cc	2008-03-28 14:41:05 UTC (rev 8730)
@@ -722,7 +722,7 @@
         if(costs[i] == "confusion_matrix")
             for(int conf_i=0; conf_i< n_classes;conf_i++)
                 for(int conf_j=0; conf_j<n_classes;conf_j++){
-                    string s = "confusion_matrix_i"+tostring(conf_i)+"_j"+tostring(conf_j);
+                    string s = "confusion_matrix_target"+tostring(conf_i)+"_pred"+tostring(conf_j);
                     sub_costs.append(s);
                 }
         else



From nouiz at mail.berlios.de  Fri Mar 28 15:47:49 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 28 Mar 2008 15:47:49 +0100
Subject: [Plearn-commits] r8731 - in trunk: commands plearn_learners/meta
	plearn_learners/regressors
Message-ID: <200803281447.m2SElnX5022390@sheep.berlios.de>

Author: nouiz
Date: 2008-03-28 15:47:48 +0100 (Fri, 28 Mar 2008)
New Revision: 8731

Modified:
   trunk/commands/plearn_noblas_inc.h
   trunk/plearn_learners/meta/AdaBoost.cc
   trunk/plearn_learners/meta/AdaBoost.h
   trunk/plearn_learners/regressors/BaseRegressorWrapper.cc
   trunk/plearn_learners/regressors/LocalMedBoost.cc
   trunk/plearn_learners/regressors/RegressionTree.cc
   trunk/plearn_learners/regressors/RegressionTree.h
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.h
Log:
Modified AdaBoost to do not have any knowledge of RegressionTree. To don't sort the RegressionTreeRegisters at each iteration I added an option modif_train_set_weights that modify the train_set weight directly.


Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2008-03-28 14:41:05 UTC (rev 8730)
+++ trunk/commands/plearn_noblas_inc.h	2008-03-28 14:47:48 UTC (rev 8731)
@@ -181,6 +181,7 @@
 #include <plearn_learners/regressors/KNNRegressor.h>
 #include <plearn_learners/regressors/RankLearner.h>
 #include <plearn_learners/regressors/RegressorFromDistribution.h>
+#include <plearn_learners/regressors/RegressionTree.h>
 // Unsupervised
 #include <plearn_learners/unsupervised/UniformizeLearner.h>
 

Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2008-03-28 14:41:05 UTC (rev 8730)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2008-03-28 14:47:48 UTC (rev 8731)
@@ -48,7 +48,6 @@
 #include <plearn/math/random.h>
 #include <plearn/io/load_and_save.h>
 #include <plearn/base/stringutils.h>
-#include <plearn_learners/regressors/RegressionTree.h>
 
 namespace PLearn {
 using namespace std;
@@ -66,7 +65,8 @@
       weight_by_resampling(1), 
       early_stopping(1),
       save_often(0),
-      forward_sub_learner_test_costs(false)
+      forward_sub_learner_test_costs(false),
+      modif_train_set_weights(false)
 { }
 
 PLEARN_IMPLEMENT_OBJECT(
@@ -200,17 +200,16 @@
                   &AdaBoost::forward_sub_learner_test_costs, OptionBase::buildoption,
                   "Did we add the sub_learner_costs to our costs.\n");
 
+    declareOption(ol, "modif_train_set_weights", 
+                  &AdaBoost::modif_train_set_weights, OptionBase::buildoption,
+                  "Did we modif directly the train_set weights?\n");
+
     declareOption(ol, "found_zero_error_weak_learner", 
                   &AdaBoost::found_zero_error_weak_learner, 
                   OptionBase::learntoption,
                   "Indication that a weak learner with 0 training error"
                   "has been found.\n");
 
-    declareOption(ol, "sorted_train_set",
-                  &AdaBoost::sorted_train_set,
-                  OptionBase::learntoption,
-                  "A sorted train set when using the class RegressionTree as a base regressor\n");
-
     declareOption(ol, "weak_learner_output",
                   &AdaBoost::weak_learner_output,
                   OptionBase::nosave,
@@ -304,6 +303,13 @@
         PLERROR("In AdaBoost::train, targetsize should be 1, found %d", 
                 train_set->targetsize());
 
+    if(modif_train_set_weights && train_set->weightsize()!=1)
+        PLERROR("In AdaBoost::train, when modif_train_set_weights is true"
+                " the weightsize of the trainset must be one.");
+    
+    PLCHECK_MSG(train_set->inputsize()>0, "In AdaBoost::train, the inputsize"
+                " of the train_set must be know.");
+
     if(found_zero_error_weak_learner) // Training is over...
         return;
 
@@ -349,22 +355,6 @@
         sum_voting_weights = 0;
         voting_weights.resize(0,nstages);
 
-        if (weak_learner_template->classname()=="RegressionTree" && ! weight_by_resampling)
-        {
-            if(train_set->weightsize()<=0)
-            {
-                Mat* m = new Mat(train_set->length(),1);
-                m->fill(1.0/m->length());
-                VMat data_weights = VMat(*m);
-                VMat new_train_set = new ConcatColumnsVMatrix(train_set,data_weights);
-                new_train_set->defineSizes(train_set->inputsize(),train_set->targetsize(),1);
-                train_set = new_train_set;
-            }
-            sorted_train_set = new RegressionTreeRegisters();
-            sorted_train_set->setOption("report_progress", tostring(report_progress));
-            sorted_train_set->setOption("verbosity", tostring(verbosity));
-            sorted_train_set->initRegisters(train_set);
-        }
     }
 
     VMat unweighted_data = train_set.subMatColumns(0, inputsize()+1);
@@ -373,7 +363,6 @@
     for ( ; stage < nstages ; ++stage)
     {
         VMat weak_learner_training_set;
-        PP<RegressionTreeRegisters> weak_learner_sorted_training_set;
         { 
             PP<ProgressBar> pb;
             if(report_progress) pb = new ProgressBar(
@@ -415,13 +404,14 @@
                     new SelectRowsVMatrix(unweighted_data, train_indices);
                 weak_learner_training_set->defineSizes(inputsize(), 1, 0);
             }
-            else if(weak_learner_template->classname()=="RegressionTree" && ! weight_by_resampling)
+            else if(modif_train_set_weights)
             {
                 //No Need for deep copy of the sorted_train_set as after the train it is not used anymore
                 // and the data are not modofied, but we need to change the weight
-                weak_learner_sorted_training_set = sorted_train_set;
-                for(int i=0;i<example_weights.size();i++)
-                    weak_learner_sorted_training_set->setWeight(i,example_weights[i]);
+                weak_learner_training_set = train_set;
+                int weight_col=train_set->inputsize()+train_set->targetsize();
+                for(int i=0;i<train_set->length();i++)
+                    train_set->put(i,weight_col,example_weights[i]);
             }
             else
             {
@@ -438,10 +428,7 @@
 
         // Create new weak-learner and train it
         PP<PLearner> new_weak_learner = ::PLearn::deepCopy(weak_learner_template);
-        if(weak_learner_template->classname()=="RegressionTree" && ! weight_by_resampling)
-            ((PP<RegressionTree>)(new_weak_learner))->setSortedTrainSet(weak_learner_sorted_training_set);
-        else
-            new_weak_learner->setTrainingSet(weak_learner_training_set);
+        new_weak_learner->setTrainingSet(weak_learner_training_set);
         new_weak_learner->setTrainStatsCollector(new VecStatsCollector);
         if(expdir!="" && provide_learner_expdir)
             new_weak_learner->setExperimentDirectory( expdir / ("WeakLearner"+tostring(stage)+"Expdir") );
@@ -744,11 +731,6 @@
 {
     TVec<string> costs=getTrainCostNames();
 
-    PP<VMatrix> the_train_set = train_set;
-    if(!train_set)
-        the_train_set=sorted_train_set;
-    PLASSERT(the_train_set);
-
     if(forward_sub_learner_test_costs){
         TVec<string> subcosts=weak_learners[0]->getTestCostNames();
         for(int i=0;i<subcosts.length();i++){
@@ -774,11 +756,8 @@
 {
     if (compute_training_error)
     {
-        PP<VMatrix> the_train_set = train_set;
-        if(!train_set)
-            the_train_set=sorted_train_set;
-        PLASSERT(the_train_set);
-        int n=the_train_set->length();
+        PLASSERT(train_set);
+        int n=train_set->length();
         PP<ProgressBar> pb;
         if(report_progress) pb = new ProgressBar("computing weighted training error of whole model",n);
         train_stats->forget();

Modified: trunk/plearn_learners/meta/AdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.h	2008-03-28 14:41:05 UTC (rev 8730)
+++ trunk/plearn_learners/meta/AdaBoost.h	2008-03-28 14:47:48 UTC (rev 8731)
@@ -44,7 +44,6 @@
 #define AdaBoost_INC
 
 #include <plearn_learners/generic/PLearner.h>
-#include <plearn_learners/regressors/RegressionTreeRegisters.h>
 
 namespace PLearn {
 using namespace std;
@@ -81,8 +80,6 @@
     //! Indication that a weak learner with 0 training error has been found
     bool found_zero_error_weak_learner;
 
-    //! a sorted train set when using a tree as a base regressor  
-    PP<RegressionTreeRegisters> sorted_train_set;
 public:
 
     // ************************
@@ -125,6 +122,8 @@
     // Did we add the sub_learner_costs to our costs
     bool forward_sub_learner_test_costs;
 
+    // Did we modif directly the train_set weights?
+    bool modif_train_set_weights;
     // ****************
     // * Constructors *
     // ****************

Modified: trunk/plearn_learners/regressors/BaseRegressorWrapper.cc
===================================================================
--- trunk/plearn_learners/regressors/BaseRegressorWrapper.cc	2008-03-28 14:41:05 UTC (rev 8730)
+++ trunk/plearn_learners/regressors/BaseRegressorWrapper.cc	2008-03-28 14:47:48 UTC (rev 8731)
@@ -143,7 +143,7 @@
         if (regression_tree > 0)
         {
             tree_regressor = ::PLearn::deepCopy(tree_regressor_template);
-            tree_regressor->setSortedTrainSet(sorted_train_set);
+            tree_regressor->setTrainingSet(VMat(sorted_train_set));
             tree_regressor->setOption("loss_function_weight", tostring(loss_function_weight));
             base_regressor = tree_regressor;
         }

Modified: trunk/plearn_learners/regressors/LocalMedBoost.cc
===================================================================
--- trunk/plearn_learners/regressors/LocalMedBoost.cc	2008-03-28 14:41:05 UTC (rev 8730)
+++ trunk/plearn_learners/regressors/LocalMedBoost.cc	2008-03-28 14:47:48 UTC (rev 8731)
@@ -197,7 +197,7 @@
             if (regression_tree == 1)
             {
                 tree_regressors[stage] = ::PLearn::deepCopy(tree_regressor_template);
-                tree_regressors[stage]->setSortedTrainSet(sorted_train_set);
+                tree_regressors[stage]->setTrainingSet(VMat(sorted_train_set));
                 base_regressors[stage] = tree_regressors[stage];
             }
             else

Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2008-03-28 14:41:05 UTC (rev 8730)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2008-03-28 14:47:48 UTC (rev 8731)
@@ -243,8 +243,13 @@
 
 void RegressionTree::initialiseTree()
 {
-    if (!sorted_train_set)
+    if (!sorted_train_set && train_set->classname()=="RegressionTreeRegisters")
     {
+        sorted_train_set=(PP<RegressionTreeRegisters>)train_set;
+        sorted_train_set->reinitRegisters();
+    }
+    else if(!sorted_train_set)
+    {
         sorted_train_set = new RegressionTreeRegisters();
         sorted_train_set->setOption("report_progress", tostring(report_progress));
         sorted_train_set->setOption("verbosity", tostring(verbosity));
@@ -313,12 +318,6 @@
     return split_col; 
 }
 
-void RegressionTree::setSortedTrainSet(PP<RegressionTreeRegisters> the_sorted_train_set)
-{
-    sorted_train_set = the_sorted_train_set;
-    build();
-}
-
 int RegressionTree::outputsize() const
 {
     return 2;

Modified: trunk/plearn_learners/regressors/RegressionTree.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.h	2008-03-28 14:41:05 UTC (rev 8730)
+++ trunk/plearn_learners/regressors/RegressionTree.h	2008-03-28 14:47:48 UTC (rev 8731)
@@ -109,7 +109,6 @@
     virtual void         computeOutputAndNodes(const Vec& input, Vec& output,
                                                TVec<PP<RegressionTreeNode> >* nodes=0) const;
     virtual void         computeCostsFromOutputs(const Vec& input, const Vec& output, const Vec& target, Vec& costs) const;
-    void         setSortedTrainSet(PP<RegressionTreeRegisters> the_sorted_train_set);
   
 private:
     void         build_();

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2008-03-28 14:41:05 UTC (rev 8730)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2008-03-28 14:47:48 UTC (rev 8731)
@@ -108,6 +108,8 @@
 
 void RegressionTreeRegisters::build_()
 {
+    if(source)
+        initRegisters(source);
 }
 
 void RegressionTreeRegisters::initRegisters(VMat the_train_set)
@@ -151,10 +153,20 @@
 
 void RegressionTreeRegisters::setWeight(int row, real val)
 {
+    PLASSERT(inputsize()>0&&targetsize()>0);
     PLASSERT(weightsize() > 0);
     tsource->put( inputsize() + targetsize(), row, val );
 }
 
+void RegressionTreeRegisters::put(int i, int j, real value)
+{
+    PLASSERT(inputsize()>0&&targetsize()>0);
+    if(j!=inputsize()+targetsize())
+        PLERROR("In RegressionTreeRegisters::put - implemented the put of "
+                "the weightsize only");
+    setWeight(i,value);
+}
+
 int RegressionTreeRegisters::getNextId()
 {
     next_id += 1;

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2008-03-28 14:41:05 UTC (rev 8730)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2008-03-28 14:47:48 UTC (rev 8731)
@@ -96,6 +96,7 @@
     void         sortRows();
     void         printRegisters();
     void         getExample(int i, Vec& input, Vec& target, real& weight);
+    virtual void put(int i, int j, real value);
 //    virtual void getNewRow(int i, const Vec& v) const;
     VMat source;
 



From nouiz at mail.berlios.de  Fri Mar 28 16:37:13 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 28 Mar 2008 16:37:13 +0100
Subject: [Plearn-commits] r8732 - trunk/plearn_learners/regressors
Message-ID: <200803281537.m2SFbDI5027965@sheep.berlios.de>

Author: nouiz
Date: 2008-03-28 16:37:12 +0100 (Fri, 28 Mar 2008)
New Revision: 8732

Modified:
   trunk/plearn_learners/regressors/BaseRegressorWrapper.cc
   trunk/plearn_learners/regressors/BaseRegressorWrapper.h
   trunk/plearn_learners/regressors/LocalMedBoost.cc
   trunk/plearn_learners/regressors/LocalMedBoost.h
   trunk/plearn_learners/regressors/RegressionTree.cc
   trunk/plearn_learners/regressors/RegressionTree.h
   trunk/plearn_learners/regressors/RegressionTreeLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeLeave.h
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
   trunk/plearn_learners/regressors/RegressionTreeNode.h
   trunk/plearn_learners/regressors/RegressionTreeQueue.cc
   trunk/plearn_learners/regressors/RegressionTreeQueue.h
   trunk/plearn_learners/regressors/RegressionTreeRegisters.h
Log:
flattened the dependency graph for the class that are in relation to RegressionTree


Modified: trunk/plearn_learners/regressors/BaseRegressorWrapper.cc
===================================================================
--- trunk/plearn_learners/regressors/BaseRegressorWrapper.cc	2008-03-28 14:47:48 UTC (rev 8731)
+++ trunk/plearn_learners/regressors/BaseRegressorWrapper.cc	2008-03-28 15:37:12 UTC (rev 8732)
@@ -40,6 +40,11 @@
  ******************************************************************************** */
 
 #include "BaseRegressorWrapper.h"
+#include "BaseRegressorConfidence.h"
+#include "RegressionTree.h"
+#include <plearn/vmat/MeanImputationVMatrix.h>
+#include <plearn/base/stringutils.h>
+#include "RegressionTreeRegisters.h"
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn_learners/regressors/BaseRegressorWrapper.h
===================================================================
--- trunk/plearn_learners/regressors/BaseRegressorWrapper.h	2008-03-28 14:47:48 UTC (rev 8731)
+++ trunk/plearn_learners/regressors/BaseRegressorWrapper.h	2008-03-28 15:37:12 UTC (rev 8732)
@@ -45,13 +45,12 @@
 #define BaseRegressorWrapper_INC
 
 #include <plearn_learners/generic/PLearner.h>
-#include <plearn/vmat/MeanImputationVMatrix.h>
-#include <plearn/base/stringutils.h>
-#include "BaseRegressorConfidence.h"
-#include "RegressionTree.h"
 
 namespace PLearn {
 using namespace std;
+class RegressionTree;
+class BaseRegressorConfidence;
+class RegressionTreeRegisters;
 
 class BaseRegressorWrapper: public PLearner
 {

Modified: trunk/plearn_learners/regressors/LocalMedBoost.cc
===================================================================
--- trunk/plearn_learners/regressors/LocalMedBoost.cc	2008-03-28 14:47:48 UTC (rev 8731)
+++ trunk/plearn_learners/regressors/LocalMedBoost.cc	2008-03-28 15:37:12 UTC (rev 8732)
@@ -39,6 +39,9 @@
  ******************************************************************************** */
 
 #include "LocalMedBoost.h"
+#include "RegressionTree.h"
+#include "RegressionTreeRegisters.h"
+#include "BaseRegressorWrapper.h"
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn_learners/regressors/LocalMedBoost.h
===================================================================
--- trunk/plearn_learners/regressors/LocalMedBoost.h	2008-03-28 14:47:48 UTC (rev 8731)
+++ trunk/plearn_learners/regressors/LocalMedBoost.h	2008-03-28 15:37:12 UTC (rev 8732)
@@ -41,11 +41,13 @@
 #ifndef LocalMedBoost_INC
 #define LocalMedBoost_INC
 
-#include "RegressionTree.h"
-#include "BaseRegressorWrapper.h"
+#include <plearn_learners/generic/PLearner.h>
 
 namespace PLearn {
 using namespace std;
+class RegressionTree;
+class BaseRegressorWrapper;
+class RegressionTreeRegisters;
 
 class LocalMedBoost: public PLearner
 {

Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2008-03-28 14:47:48 UTC (rev 8731)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2008-03-28 15:37:12 UTC (rev 8732)
@@ -40,6 +40,11 @@
  ******************************************************************************** */
 
 #include "RegressionTree.h"
+#include "RegressionTreeQueue.h"
+#include "RegressionTreeLeave.h"
+#include "RegressionTreeMulticlassLeave.h"
+#include "RegressionTreeRegisters.h"
+#include "RegressionTreeNode.h"
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn_learners/regressors/RegressionTree.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.h	2008-03-28 14:47:48 UTC (rev 8731)
+++ trunk/plearn_learners/regressors/RegressionTree.h	2008-03-28 15:37:12 UTC (rev 8732)
@@ -45,10 +45,13 @@
 #define RegressionTree_INC
 
 #include <plearn_learners/generic/PLearner.h>
-#include "RegressionTreeQueue.h"
 
 namespace PLearn {
 using namespace std;
+class RegressionTreeQueue;
+class RegressionTreeLeave;
+class RegressionTreeRegisters;
+class RegressionTreeNode;
 
 class RegressionTree: public PLearner
 {

Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2008-03-28 14:47:48 UTC (rev 8731)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2008-03-28 15:37:12 UTC (rev 8732)
@@ -40,6 +40,7 @@
  ******************************************************************************** */
 
 #include "RegressionTreeLeave.h"
+#include "RegressionTreeRegisters.h"
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.h	2008-03-28 14:47:48 UTC (rev 8731)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.h	2008-03-28 15:37:12 UTC (rev 8732)
@@ -42,12 +42,12 @@
 #ifndef RegressionTreeLeave_INC
 #define RegressionTreeLeave_INC
 
-#include "RegressionTreeRegisters.h"
+#include <plearn/base/Object.h>
 
 namespace PLearn {
 using namespace std;
+class RegressionTreeRegisters;
 
-
 class RegressionTreeLeave: public Object
 {
     typedef Object inherited;

Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2008-03-28 14:47:48 UTC (rev 8731)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2008-03-28 15:37:12 UTC (rev 8732)
@@ -40,6 +40,7 @@
  ******************************************************************************** */
 
 #include "RegressionTreeMulticlassLeave.h"
+#include "RegressionTreeRegisters.h"
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-03-28 14:47:48 UTC (rev 8731)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-03-28 15:37:12 UTC (rev 8732)
@@ -40,6 +40,8 @@
  ******************************************************************************** */
 
 #include "RegressionTreeNode.h"
+#include "RegressionTreeRegisters.h"
+#include "RegressionTreeLeave.h"
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.h	2008-03-28 14:47:48 UTC (rev 8731)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.h	2008-03-28 15:37:12 UTC (rev 8732)
@@ -42,11 +42,15 @@
 #ifndef RegressionTreeNode_INC
 #define RegressionTreeNode_INC
 
-#include "RegressionTreeLeave.h"
-#include "RegressionTreeMulticlassLeave.h"
+#include <plearn/base/Object.h>
+#include <plearn/base/PP.h>
+#include <plearn/math/TVec.h>
 
 namespace PLearn {
 using namespace std;
+class RegressionTreeRegisters;
+class RegressionTreeLeave;
+class RegressionTreeNode;
 
 class RegressionTreeNode: public Object
 {

Modified: trunk/plearn_learners/regressors/RegressionTreeQueue.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeQueue.cc	2008-03-28 14:47:48 UTC (rev 8731)
+++ trunk/plearn_learners/regressors/RegressionTreeQueue.cc	2008-03-28 15:37:12 UTC (rev 8732)
@@ -40,6 +40,7 @@
  ******************************************************************************** */
 
 #include "RegressionTreeQueue.h"
+#include "RegressionTreeNode.h"
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn_learners/regressors/RegressionTreeQueue.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeQueue.h	2008-03-28 14:47:48 UTC (rev 8731)
+++ trunk/plearn_learners/regressors/RegressionTreeQueue.h	2008-03-28 15:37:12 UTC (rev 8732)
@@ -42,10 +42,11 @@
 #ifndef RegressionTreeQueue_INC
 #define RegressionTreeQueue_INC
 
-#include "RegressionTreeNode.h"
+#include <plearn/base/Object.h>
 
 namespace PLearn {
 using namespace std;
+class RegressionTreeNode;
 
 class RegressionTreeQueue: public Object
 {

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2008-03-28 14:47:48 UTC (rev 8731)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2008-03-28 15:37:12 UTC (rev 8732)
@@ -42,7 +42,7 @@
 #ifndef RegressionTreeRegisters_INC
 #define RegressionTreeRegisters_INC
 
-#include <plearn/vmat/SourceVMatrix.h>
+#include <plearn/vmat/VMatrix.h>
 #include <plearn/base/stringutils.h>
 #include <plearn/math/TMat.h>
 #include <plearn/vmat/VMat.h>



From nouiz at mail.berlios.de  Fri Mar 28 16:41:37 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 28 Mar 2008 16:41:37 +0100
Subject: [Plearn-commits] r8733 - trunk/plearn/vmat
Message-ID: <200803281541.m2SFfb0L029062@sheep.berlios.de>

Author: nouiz
Date: 2008-03-28 16:41:36 +0100 (Fri, 28 Mar 2008)
New Revision: 8733

Modified:
   trunk/plearn/vmat/ConstantVMatrix.cc
Log:
added the computation of missing size


Modified: trunk/plearn/vmat/ConstantVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ConstantVMatrix.cc	2008-03-28 15:37:12 UTC (rev 8732)
+++ trunk/plearn/vmat/ConstantVMatrix.cc	2008-03-28 15:41:36 UTC (rev 8733)
@@ -86,6 +86,7 @@
 ////////////
 void ConstantVMatrix::build_()
 {
+    computeMissingSizeValue(false);
 }
 
 /////////



From nouiz at mail.berlios.de  Fri Mar 28 18:26:51 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 28 Mar 2008 18:26:51 +0100
Subject: [Plearn-commits] r8734 - trunk/plearn/vmat
Message-ID: <200803281726.m2SHQpna010364@sheep.berlios.de>

Author: nouiz
Date: 2008-03-28 18:26:50 +0100 (Fri, 28 Mar 2008)
New Revision: 8734

Modified:
   trunk/plearn/vmat/TransposeVMatrix.cc
   trunk/plearn/vmat/TransposeVMatrix.h
Log:
-Implemented TransposeVMatrix::getMat() for optimisation.
  So if we make a MemoryVMatrix from a TransposeVMatrix, we will read the source VMat in the row order


Modified: trunk/plearn/vmat/TransposeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TransposeVMatrix.cc	2008-03-28 15:41:36 UTC (rev 8733)
+++ trunk/plearn/vmat/TransposeVMatrix.cc	2008-03-28 17:26:50 UTC (rev 8734)
@@ -128,6 +128,24 @@
     source->getRow(j, v);
 }
 
+////////////
+// getMat //
+////////////
+void TransposeVMatrix::getMat(int i, int j, Mat m) const
+{
+#ifdef BOUNDCHECK
+    if(i<0 || j<0 || i+m.length()>length() || j+m.width()>width())
+        PLERROR("In VMatrix::getMat(i,j,m) OUT OF BOUNDS");
+#endif
+    Vec tmp(m.length());
+    for(int ii=0; ii<m.width(); ii++)
+    {
+        source->getSubRow(i+ii, j,tmp);
+        for(int k=0;k<m.length();k++)
+            m(k,ii)=tmp[k];
+    }
+}
+
 /////////////////////////////////
 // makeDeepCopyFromShallowCopy //
 /////////////////////////////////

Modified: trunk/plearn/vmat/TransposeVMatrix.h
===================================================================
--- trunk/plearn/vmat/TransposeVMatrix.h	2008-03-28 15:41:36 UTC (rev 8733)
+++ trunk/plearn/vmat/TransposeVMatrix.h	2008-03-28 17:26:50 UTC (rev 8734)
@@ -101,6 +101,7 @@
     //! v is assumed to be the right size.
     virtual void getNewRow(int i, const Vec& v) const;
     virtual void getColumn(int j, Vec v) const;
+    virtual void getMat(int i, int j, Mat m) const;
 
 public:
 



From nouiz at mail.berlios.de  Fri Mar 28 19:19:39 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 28 Mar 2008 19:19:39 +0100
Subject: [Plearn-commits] r8735 - trunk/python_modules/plearn/learners
Message-ID: <200803281819.m2SIJdhU006927@sheep.berlios.de>

Author: nouiz
Date: 2008-03-28 19:19:38 +0100 (Fri, 28 Mar 2008)
New Revision: 8735

Modified:
   trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
Log:
-use the option modif_train_set_weights from AdaBoost
-the test function take only a VMat. It don't need anymore a python version


Modified: trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
===================================================================
--- trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2008-03-28 17:26:50 UTC (rev 8734)
+++ trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2008-03-28 18:19:38 UTC (rev 8735)
@@ -33,6 +33,7 @@
         l.weak_learner_template=sublearner
         l.pseudo_loss_adaboost=plargs.pseudo_loss_adaboost
         l.weight_by_resampling=plargs.weight_by_resampling
+        l.modif_train_set_weights=plargs.modif_train_set_weights
         l.setTrainingSet(trainSet,True)
         l.early_stopping=False
         l.compute_training_error=True
@@ -165,7 +166,7 @@
         costs=self.computeCostsFromOutput(input,output,target)
         return (output,costs)
 
-    def test(self,testSet,testMat,test_stats,return_outputs,return_costs):
+    def test(self,testSet,test_stats,return_outputs,return_costs):
         testSet1=pl.ProcessingVMatrix(source=testSet,
                                prg = "[%0:%"+str(testSet.inputsize-1)+"] @CLASSE_REEL 1 0 ifelse :CLASSE_REEL")
         testSet2=pl.ProcessingVMatrix(source=testSet,
@@ -182,7 +183,7 @@
         outputs=[]
         costs=[]
         #calculate stats, outputs, costs
-        for i in range(len(testMat)):
+        for i in range(testSet.length):
             out1=testoutputs1.getRow(i)[0]
             out2=testoutputs2.getRow(i)[0]
             ind1=int(round(out1))
@@ -198,8 +199,9 @@
             output=[ind,out1,out2]
             if return_outputs:
                 outputs.append(output)
-            input=testMat[i][:-1]
-            target=testMat[i][-1]
+            row=testSet.getRow(i)
+            input=row[:-1]
+            target=row[-1]
             cost=self.computeCostsFromOutput(input,output,target,
                                              forward_sub_learner_costs=False)
             cost.extend(testcosts1.getRow(i))



From louradou at mail.berlios.de  Fri Mar 28 20:21:43 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Fri, 28 Mar 2008 20:21:43 +0100
Subject: [Plearn-commits] r8736 - trunk/python_modules/plearn/learners
Message-ID: <200803281921.m2SJLh4r009011@sheep.berlios.de>

Author: louradou
Date: 2008-03-28 20:21:42 +0100 (Fri, 28 Mar 2008)
New Revision: 8736

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:


Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2008-03-28 18:19:38 UTC (rev 8735)
+++ trunk/python_modules/plearn/learners/SVM.py	2008-03-28 19:21:42 UTC (rev 8736)
@@ -38,14 +38,18 @@
 
         'best_cost': <float> Best cost obtained
 
-        'input_std': <float> the average standard deviation
+        'inputsize': <int> input dimension. Used also sometimes
+                     to choose the first hyperparameter values.
+
+        'input_avgstd': <float> the average standard deviation
                           of the input data. It is estimated on
                           the train set, and is used to choose first
                           hyperparameter values.
 
-        'inputsize': <int> input dimension. Used also sometimes
-                     to choose the first hyperparameter values.
+        'input_means': <list> of mean values for each input component.
 
+        'input_stds': <list> of standard deviation values for each input component.
+
     protected:
 
         'kernel_type': <str> corresponding to the kernel.    
@@ -72,7 +76,7 @@
                         'param_names',
                         'trials_param_list',
                         'trials_cost_list',
-                        'input_std',
+                        'input_avgstd',
                         'inputsize'
                       ]                  
 
@@ -84,7 +88,9 @@
         self.best_cost      = None
         self.C_initvalue    = 1.
         self.stats_are_uptodate = False
-        self.input_std      = None
+        self.input_avgstd      = None
+        self.input_means = None
+        self.input_stds = None
         self.inputsize      = None
         self.verbosity      = 1
 
@@ -103,14 +109,14 @@
         #       a good candidate
 
 
-    def set_input_stats( self, inputsize, input_std ):
+    def set_input_stats( self, inputsize, input_avgstd ):
         self.stats_are_uptodate = True
         self.inputsize = int(inputsize)
-        self.input_std = input_std
+        self.input_avgstd = input_avgstd
 
     def get_input_stats( self, samples=None ):
         if self.stats_are_uptodate:
-            return ( self.inputsize, self.input_std )
+            return ( self.inputsize, self.input_avgstd )
         if samples == None:
             if self.verbosity > 0:
                 print "WARNING: get_data_stats() takes default value for input stats."
@@ -120,18 +126,18 @@
             print "  (computing input stats)"
         
         self.inputsize = len(samples[0])
-        self.input_std, std__std_per_component = mean_std(samples)
+        self.input_avgstd, std__std_per_component = mean_std(samples)
         if( self.verbosity > 0 ): 
-            if( self.input_std < 0.5
-            or  self.input_std > 10.
-            or  std__std_per_component/self.input_std > 0.1 ):
+            if( self.input_avgstd < 0.5
+            or  self.input_avgstd > 10.
+            or  std__std_per_component/self.input_avgstd > 0.1 ):
                 print "Warning in SVMHyperParamOracle__kernel::get_input_stats() " + \
                     "\n\tYour data does not seem to be normalized: " + \
                     "\n\t(E[std_comp] = %.2f, std[std_comp] = %.2f)" % \
-                    ( self.input_std, std__std_per_component )
+                    ( self.input_avgstd, std__std_per_component )
 
         self.stats_are_uptodate = True
-        return (self.inputsize, self.input_std)
+        return (self.inputsize, self.input_avgstd)
 
     """ Return a list of values to try for hyperparameter 'C'
         centered on a specified value 'C_value'.
@@ -640,7 +646,7 @@
 
         'inputsize': <int> input size
 
-        'input_std': <float> input std
+        'input_avgstd': <float> input std
 
        
     protected:
@@ -663,6 +669,7 @@
                         'validtype',
                         'n_fold',
                         'balanceC',
+                        'normalize_inputs',
                         'results_filename',
                         'preproc_optionnames',
                         'preproc_optionvalues',
@@ -678,7 +685,7 @@
                         'nclasses',
                         'class_priors',
                         'inputsize',
-                        'input_std',
+                        'input_avgstd',
                         'stats_are_uptodate',
                         'get_datalist'
                      ]
@@ -696,7 +703,8 @@
         self.validtype       = 'simple'
 
         self.n_fold   = 5
-        self.balanceC = False
+        self.balanceC = True
+        self.normalize_inputs = False
 
         self.HyperParamOracle__linear  = SVMHyperParamOracle__linear()
         self.HyperParamOracle__rbf     = SVMHyperParamOracle__rbf()
@@ -705,7 +713,7 @@
                                    self.HyperParamOracle__rbf,
                                    self.HyperParamOracle__poly,
                                   ]
-        self.param_names =['validtype','kernel_type','balanceC']
+        self.param_names =['validtype','normalize_inputs','kernel_type','balanceC']
         for expert in self.all_experts:
             for pn in expert.param_names:
                 if pn not in self.param_names:
@@ -719,7 +727,7 @@
  
         self.stats_are_uptodate = False
         self.inputsize = None
-        self.input_std = None
+        self.input_avgstd = None
         self.nclasses = None
         self.class_priors = None
         self.weight = None
@@ -779,7 +787,7 @@
                         inputsize, targetsize, length.
                         
     """
-    def get_datalist( input_vmat ):
+    def get_datalist(self, input_vmat ):
         data_array = input_vmat.getMat()
         inputsize = input_vmat.inputsize
         targetsize = input_vmat.targetsize
@@ -788,9 +796,20 @@
         samples   = [ [ float(x_t_i)    for x_t_i in x_t ]
                                         for x_t in data_array[:,:inputsize] ]
         assert targetsize == 1
-        targets   = [ float(t) for t in data_array[:,inputsize] ]
+        targets   = [ float(t) for t in data_array[:,inputsize] ]       
         return samples, targets
 
+    def get_svminputlist(self, input_vmat, isTrain=False):
+        samples, targets = self.get_datalist( input_vmat )
+        if self.normalize_inputs:
+            if isTrain:
+                self.input_means, self.input_stds = normalize_data(samples)
+            else:
+                assert self.input_means
+                normalize_data(samples, self.input_means, self.input_stds)
+        return samples, targets
+        
+
     """ Return a dictionary class label -> class frenquency,
         where frequencies are estimated from 'targets', a list of class labels
     """
@@ -810,20 +829,20 @@
     def get_data_stats(self, vmat=None):
         if self.stats_are_uptodate:
             return ( self.nclasses, self.class_priors,
-                     self.inputsize, self.input_std )
+                     self.inputsize, self.input_avgstd )
         assert vmat <> None
 
-        samples, targets = self.get_datalist( vmat )
+        samples, targets = self.get_svminputlist( vmat, True )
 
-        self.inputsize, self.input_std = self.all_experts[0].get_input_stats(samples)
+        self.inputsize, self.input_avgstd = self.all_experts[0].get_input_stats(samples)
         for expert in self.all_experts[1:]:
-            expert.set_input_stats( self.inputsize, self.input_std )
+            expert.set_input_stats( self.inputsize, self.input_avgstd )
 
         if targets == None:
             if self.verbosity > 1:
                 print "WARNING: get_data_stats() takes default value for class info."
             return ( 2, [ .5, .5 ],
-                 self.inputsize, self.input_std )
+                 self.inputsize, self.input_avgstd )
 
         class_priors = self.get_class_priors( targets )
         self.nclasses = len( class_priors )
@@ -849,7 +868,7 @@
 
         self.stats_are_uptodate = True
         return ( self.nclasses, self.class_priors,
-                 self.inputsize, self.input_std )
+                 self.inputsize, self.input_avgstd )
 
     def get_expert(self, kernel_type ):
         return eval( 'self.HyperParamOracle__'+kernel_type.lower() )
@@ -899,7 +918,7 @@
     
 
     """ Write given results with corresponding parameters
-        In a PLearn format (.amat).
+        In a PLearn format (.amat).True
     """
     def write_results(  self, param,
                         valid_stats,
@@ -950,8 +969,10 @@
         else:
             raise TypeError, "preproc_optionvalues must be of type str or list"
         
+        
+        param_names=self.param_names
         param_values=[]
-        for pn in self.param_names:
+        for pn in param_names:
             if pn in param:
                 param_values.append(param[pn])
             elif pn in self.__attributes__:
@@ -991,7 +1012,7 @@
         os.system('makeresults  %s %s %s %s;' % \
                             (self.results_filename,
                               preproc_optionnames,
-                              ' '.join(self.param_names),
+                              ' '.join(param_names),
                               costnames_string
                             ) \
                + 'appendresults %s.amat %s %s %s' % \
@@ -1059,6 +1080,8 @@
             if not self.best_param:
                 return self.train_and_tune(dataspec)
             param = self.best_param.copy()
+        elif 'kernel_type' not in param:
+            param['kernel_type'] = self.kernel_type
         
         trainset = self.train_inputspec(dataspec)
         if self.balanceC:
@@ -1067,7 +1090,7 @@
                           'nr_weight':len(self.weight),
                           'weight_label':self.labels})
         
-        train_samples, train_targets = self.get_datalist( trainset )
+        train_samples, train_targets = self.get_svminputlist( trainset, True )
         train_problem = svm_problem( train_targets ,
                                      train_samples )
         if self.verbosity > 1:
@@ -1085,7 +1108,7 @@
         decision than the standard ones, for instance when
         classifying bags of data (where each bag correspond to a target)
     """
-    def update_predictions_targets(predictions, targets, vmat):
+    def update_predictions_targets(self, predictions, targets, vmat):
         return predictions, targets
 
     """ Return the costs obtained by a libSVM model
@@ -1104,11 +1127,9 @@
             assert self.best_model <> None
             model = self.best_model
 
-        samples, targets = self.get_datalist( testset )
+        samples, targets = self.get_svminputlist( testset )
         nclasses = self.nclasses
         nsamples = len(samples)
-        if self.verbosity > 3:
-            print "SVM::test() called on ",nsamples," samples"
 
         costnames = self.costnames
         # Translation of the cost 'confusion_matrix'
@@ -1136,6 +1157,10 @@
         
         class_priors = self.get_class_priors( targets )
                                             # [int(t) for t in targets ]
+        if self.verbosity > 3:
+            print "SVM::test() called on ",len(targets)," samples"
+            print "class priors: ", class_priors
+
         cm_weights = [ 1./class_priors[t] for t in range(nclasses) ]
         if 'norm_ce' in self.costnames:
             if self.errorcosts == None:        
@@ -1170,8 +1195,6 @@
                     raise ValueError, "computation of cost %s not implemented in SVM::test()" % cn
             teststats.update(statVec,1.)
         
-        if self.verbosity > 3:
-            print "SVM::test() computed:", teststats
         return teststats #, outputs, costs
 
 
@@ -1339,7 +1362,7 @@
         return dataspec
 
 
-def normalize_data(data,mean,std):
+def normalize_data(data, mean=None, std=None):
     if mean == None:
         mean=[]
         for i in range(len(data[0])):
@@ -1348,8 +1371,7 @@
         std=[]
         for i in range(len(data[0])):
             std_tmp=get_std_cmp(data,i)
-            if std_tmp == 0. and self.verbosity > 0:
-                print "WARNING : standard deviation is 0 on component "+str(i)
+            if std_tmp == 0.:
                 std.append( 1. )
             else:
                 std.append( std_tmp )
@@ -1359,7 +1381,10 @@
     return mean, std
 
 def mean_std(data):
-    stds=array([get_std_cmp(data,i) for i in range(len(data[0]))])
+    stds=[get_std_cmp(data,i) for i in range(len(data[0]))]
+    while 0 in stds:
+        stds.remove(0)
+    stds=array(stds)
     return stds.mean(), stds.std()
 
 def get_std_cmp(data,i):
@@ -1372,11 +1397,6 @@
     values=[float(vec[i]) for vec in data]
     return  sum(values)/len(values)
 
-def arithm_mean(data):
-    if type(data[0]) == list:
-     return [sum( [data[i][coor] for i in range(len(data))] )*1.0/len(data) for coor in range(len(data[0]))]
-    else:
-     return sum(data)*1.0/len(data)
 def geom_mean(data):
     if type(data[0]) == list:
         res=[]
@@ -1389,10 +1409,100 @@
             prod *= value
         return prod**(1./len(data))
 
+
 if __name__ == '__main__':
 
-    # an EXAMPLE to use the class...
+    import os.path
 
 
+    """ =============================================================== """
+    """ 1. BUILD a SVM Hyper-Optimizer. """
+
     svm=SVM()
+    svm.costnames = ['class_error']
+    svm.verbosity = 1
 
+    """ ............................................................... """
+    """ Uncomment following lines to change default values (indicated). """
+
+    """ <bool> weight the coeff 'C' with the inverse prior proba of each class
+               (scaled so as to have average=1 on weights). Useful when classes are unbalanced. """
+    # svm.balanceC = 1
+
+    """ <list> of cost names to compute. """
+    # svm.costnames = ['class_error','confusion_matrix']
+
+    """ <string> name of the final cost to be minimized. """
+    # svm.maincost_name = svm.costnames[0]
+
+    """ <bool> let the algo automatically decide when to stop tuning hyperparameters. """
+    # svm.retrain_until_local_optimum_is_found = 1
+
+    """ <bool> after simple validation, retrain a model on {train + valid sets} before computing test error. """
+    # svm.retrain_on_valid = 1
+
+    """ <int> verbosity level. """
+    # svm.verbosity = 0
+
+    """ ............................................................... """
+    """ =============================================================== """
+
+
+    """ =============================================================== """
+    """ 2. PREPARE inputs and outputs. """
+
+    inputPath = '/u/larocheh/myUserExp/deep_non-local_benchmark/data/mnist-rotation'
+    outputPath = '.'
+
+    train_file, valid_file, test_file = [ os.path.join(inputPath,
+                                              'mnist_all_rotation_normalized_float_'+dataType+'.amat')
+                                          for dataType in ['train','valid','test']
+                                        ]
+
+    trainset, validset, testset = [ pl.AutoVMatrix(
+                                        specification = filename,
+                                        inputsize = 784,
+                                        targetsize = 1,
+                                    )
+                                    for filename in [ train_file, valid_file, test_file ]
+                                  ]
+
+    dataspec = {'trainset':trainset,
+                'validset':validset,
+                'testset': testset,
+               }
+    """ Note1: This dataspec corresponds to the standard setting for simple validation.
+               -> Do not specify any 'validset' to do cross-validation.
+               -> Do not specify any 'testset' if you do not want to check the test error
+                  during the model selection.
+        Note2: Keys of this dictionary can be changed using svm.trainset_key ['trainset'],
+               svm.validset_key ['validset'] and svm.testset_key ['testset'].
+    """
+
+    svm.results_filename = os.path.join( outputPath,
+                                         'results_%s_svm' % ( os.path.basename(train_file) )
+                           )
+
+    """ ................................................................. """
+    """ Uncomment following lines to change default values (empty lists). """
+
+    """ <list> of front-end hyperparameter names and values, to report in results_filename. """
+    # svm.preproc_optionnames = [ 'train_file', 'simple_valid' ]
+    # svm.preproc_optionvalues = [ train_file,   svm.validset_key in dataspec ]
+    """ ................................................................. """
+    """ =============================================================== """
+
+
+    """ =============================================================== """
+    """ 3. RUN EXPERIMENTS. """
+    
+    svm.kernel_type = 'rbf'
+    svm.train_and_tune(dataspec)
+
+    svm.kernel_type = 'poly'
+    svm.train_and_tune(dataspec)
+
+    svm.kernel_type = 'linear'
+    svm.train_and_tune(dataspec)
+
+    """ =============================================================== """



From nouiz at mail.berlios.de  Fri Mar 28 21:58:31 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 28 Mar 2008 21:58:31 +0100
Subject: [Plearn-commits] r8737 - trunk/plearn_learners/regressors
Message-ID: <200803282058.m2SKwVUw019263@sheep.berlios.de>

Author: nouiz
Date: 2008-03-28 21:58:30 +0100 (Fri, 28 Mar 2008)
New Revision: 8737

Modified:
   trunk/plearn_learners/regressors/RegressionTree.cc
Log:
small optimisation


Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2008-03-28 19:21:42 UTC (rev 8736)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2008-03-28 20:58:30 UTC (rev 8737)
@@ -220,7 +220,7 @@
     Vec sample_input(sorted_train_set->inputsize());
     Vec sample_output(outputsize());
     Vec sample_target(sorted_train_set->targetsize());
-    Vec sample_costs(getTestCostNames().size());
+    Vec sample_costs(nTestCosts());
 
     for (int train_sample_index = 0; train_sample_index < length;
          train_sample_index++)



From tihocan at mail.berlios.de  Sun Mar 30 05:29:30 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Sun, 30 Mar 2008 05:29:30 +0200
Subject: [Plearn-commits] r8738 - trunk/plearn/vmat
Message-ID: <200803300329.m2U3TUd3021871@sheep.berlios.de>

Author: tihocan
Date: 2008-03-30 05:29:29 +0200 (Sun, 30 Mar 2008)
New Revision: 8738

Modified:
   trunk/plearn/vmat/ReplicateSamplesVMatrix.cc
Log:
Made it work with targets with more elements

Modified: trunk/plearn/vmat/ReplicateSamplesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ReplicateSamplesVMatrix.cc	2008-03-28 20:58:30 UTC (rev 8737)
+++ trunk/plearn/vmat/ReplicateSamplesVMatrix.cc	2008-03-30 03:29:29 UTC (rev 8738)
@@ -50,6 +50,9 @@
     "the first 'n-n_j' samples of class j (having n_j samples) will be\n"
     "replicated so that each class also has n samples. If required, samples\n"
     "will be replicated more than once.\n"
+    "The class index is assumed to be the first element of the target. When\n"
+    "the 'operate_on_bags' option is set to true, the bag information must\n"
+    "be stored in the last element of the target.\n"
     "All samples are also shuffled so as to mix classes together."
 );
 
@@ -107,14 +110,15 @@
 {
     if (!source)
         return;
-    PLCHECK_MSG(operate_on_bags || source->targetsize() == 1,
+
+    PLCHECK_MSG(operate_on_bags || source->targetsize() >= 1,
             "In ReplicateSamplesVMatrix::build_ - The source VMat must have a "
-            "targetsize equal to 1 when not operating on bags, but its "
+            "targetsize at least 1 when not operating on bags, but its "
             "targetsize is " + tostring(source->targetsize()));
 
-    PLCHECK_MSG(!operate_on_bags || source->targetsize() == 2,
+    PLCHECK_MSG(!operate_on_bags || source->targetsize() >= 2,
             "In ReplicateSamplesVMatrix::build_ - The source VMat must have a "
-            "targetsize equal to 2 when operating on bags, but its targetsize "
+            "targetsize at least 2 when operating on bags, but its targetsize "
             "is " + tostring(source->targetsize()));
 
     updateMtime(indices_vmat);
@@ -135,7 +139,7 @@
             for (int j = 0; j < n_to_add; j++)
                 class_indices.append(TVec<int>());
         }
-        if (!operate_on_bags || int(round(target[1])) &
+        if (!operate_on_bags || int(round(target.lastElement())) &
                                 SumOverBagsVariable::TARGET_COLUMN_FIRST) {
             class_indices[c].append(i);
             indices.append(i);



From nouiz at mail.berlios.de  Mon Mar 31 18:48:31 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 31 Mar 2008 18:48:31 +0200
Subject: [Plearn-commits] r8739 - trunk/plearn/vmat
Message-ID: <200803311648.m2VGmVhr022493@sheep.berlios.de>

Author: nouiz
Date: 2008-03-31 18:48:30 +0200 (Mon, 31 Mar 2008)
New Revision: 8739

Modified:
   trunk/plearn/vmat/GaussianizeVMatrix.cc
Log:
optimisation to use less memory


Modified: trunk/plearn/vmat/GaussianizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-03-30 03:29:29 UTC (rev 8738)
+++ trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-03-31 16:48:30 UTC (rev 8739)
@@ -217,6 +217,8 @@
                 values_j.append(it->first);
             }
         }
+
+        stats[j].forget();//to keep the total memory used lower.
     }
     if(features_to_gaussianize.size()==0)
         PLWARNING("GaussianizeVMatrix::build_() 0 variable was gaussianized");



From nouiz at mail.berlios.de  Mon Mar 31 21:49:09 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 31 Mar 2008 21:49:09 +0200
Subject: [Plearn-commits] r8740 - trunk/plearn/vmat
Message-ID: <200803311949.m2VJn9Bw001735@sheep.berlios.de>

Author: nouiz
Date: 2008-03-31 21:49:09 +0200 (Mon, 31 Mar 2008)
New Revision: 8740

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
the function VMatrix::savePMAT(), now save in a temp file. After successfull save, we move it to the real name. This allow to don't reuse incompletly saved file in PrecomputedVMatrix and in other place too.


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-03-31 16:48:30 UTC (rev 8739)
+++ trunk/plearn/vmat/VMatrix.cc	2008-03-31 19:49:09 UTC (rev 8740)
@@ -1876,8 +1876,10 @@
         PLERROR("In VMat::save - Saving in a pmat file is only possible for constant width VMats (where width()!=-1)");
 
     int nsamples = length();
+    PPath pmatfiletmp=pmatfile+".tmp";
 
-    FileVMatrix m(pmatfile,nsamples,width());
+    {        
+    FileVMatrix m(pmatfiletmp,nsamples,width());
     m.setMetaInfoFrom(this);
     // m.setFieldInfos(getFieldInfos());
     // m.copySizesFrom(this);
@@ -1893,6 +1895,10 @@
     }
     m.saveFieldInfos();
     m.saveAllStringMappings();
+    }// to ensure that m is deleted?
+    
+    mv(pmatfiletmp,pmatfile);
+    mv(pmatfiletmp+".metadata",pmatfile+".metadata");
 }
 
 //////////////



From nouiz at mail.berlios.de  Mon Mar 31 22:30:01 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 31 Mar 2008 22:30:01 +0200
Subject: [Plearn-commits] r8742 - trunk/python_modules/plearn/parallel
Message-ID: <200803312030.m2VKU1A0006481@sheep.berlios.de>

Author: nouiz
Date: 2008-03-31 22:30:01 +0200 (Mon, 31 Mar 2008)
New Revision: 8742

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
in condor mode, the environnement variable will be transfered to the executing jobs. 


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-03-31 20:29:03 UTC (rev 8741)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-03-31 20:30:01 UTC (rev 8742)
@@ -797,6 +797,7 @@
                 output         = %s/condor.%s.$(Process).out
                 error          = %s/condor.%s.$(Process).error
                 log            = %s/condor.log
+                getenv         = True
                 ''' % (self.tmp_dir,req,
                        self.log_dir,self.unique_id,
                        self.log_dir,self.unique_id,
@@ -810,7 +811,7 @@
                 condor_dat.write("arguments      = %s \nqueue\n" %(' ; '.join(task.commands)))
         condor_dat.close()
 
-        launch_file = os.path.join(self.tmp_dir, 'launch.sh')
+        launch_file = os.path.join(self.log_dir, 'launch.sh')
         dbi_file=get_plearndir()+'/python_modules/plearn/parallel/dbi.py'
         overwrite_launch_file=False
         if not os.path.exists(dbi_file):
@@ -830,7 +831,7 @@
                 #!/bin/sh
                 PROGRAM=$1
                 shift\n'''))
-            if None != os.getenv("CONDOR_LOCAL_SOURCE"):
+            if os.getenv("CONDOR_LOCAL_SOURCE"):
                 launch_dat.write('source ' + os.getenv("CONDOR_LOCAL_SOURCE") + '\n')
             launch_dat.write(dedent('''\
                     echo "Executing on ${HOSTNAME}" 1>&2



From tihocan at mail.berlios.de  Mon Mar 31 22:29:03 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 31 Mar 2008 22:29:03 +0200
Subject: [Plearn-commits] r8741 - trunk/plearn_learners/classifiers
Message-ID: <200803312029.m2VKT3h8006394@sheep.berlios.de>

Author: tihocan
Date: 2008-03-31 22:29:03 +0200 (Mon, 31 Mar 2008)
New Revision: 8741

Modified:
   trunk/plearn_learners/classifiers/ToBagClassifier.cc
Log:
Fixed bugs when the underlying learner may output missing values

Modified: trunk/plearn_learners/classifiers/ToBagClassifier.cc
===================================================================
--- trunk/plearn_learners/classifiers/ToBagClassifier.cc	2008-03-31 19:49:09 UTC (rev 8740)
+++ trunk/plearn_learners/classifiers/ToBagClassifier.cc	2008-03-31 20:29:03 UTC (rev 8741)
@@ -199,19 +199,22 @@
 {
     costs.resize(nTestCosts());
     costs.fill(MISSING_VALUE);
+
+    int bag_info = int(round(target.lastElement()));
+    if (bag_info & SumOverBagsVariable::TARGET_COLUMN_FIRST)
+        bag_output.resize(0, 0);
+
+    // Ignore missing outputs from learner.
     if (is_missing(output[0])) {
-        // Ignore missing outputs from learner.
 #ifdef BOUNDCHECK
         for (int i = 1; i < output.length(); i++) {
             PLASSERT( is_missing(output[i]) );
         }
+#endif
         return;
-#endif
     }
+
     PLASSERT( sum(output) < 1.001 & sum(output) > 0.999);
-    int bag_info = int(round(target.lastElement()));
-    if (bag_info & SumOverBagsVariable::TARGET_COLUMN_FIRST)
-        bag_output.resize(0, 0);
     bag_output.appendRow(output);
     if (bag_info & SumOverBagsVariable::TARGET_COLUMN_LAST) {
         // Perform majority vote.



