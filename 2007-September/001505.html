<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r8057 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-September/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8057%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200709072208.l87M8hbl012274%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001504.html">
   <LINK REL="Next"  HREF="001506.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r8057 - trunk/plearn_learners/online</H1>
    <B>larocheh at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8057%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200709072208.l87M8hbl012274%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r8057 - trunk/plearn_learners/online">larocheh at mail.berlios.de
       </A><BR>
    <I>Sat Sep  8 00:08:43 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="001504.html">[Plearn-commits] r8056 - trunk/plearn_learners/online
</A></li>
        <LI>Next message: <A HREF="001506.html">[Plearn-commits] r8058 - trunk/plearn_learners/generic/EXPERIMENTAL
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1505">[ date ]</a>
              <a href="thread.html#1505">[ thread ]</a>
              <a href="subject.html#1505">[ subject ]</a>
              <a href="author.html#1505">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: larocheh
Date: 2007-09-08 00:08:43 +0200 (Sat, 08 Sep 2007)
New Revision: 8057

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
   trunk/plearn_learners/online/StackedAutoassociatorsNet.h
Log:
Corrected partial_costs option and added online version.


Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2007-09-07 22:07:33 UTC (rev 8056)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2007-09-07 22:08:43 UTC (rev 8057)
@@ -59,7 +59,9 @@
     fine_tuning_decrease_ct( 0. ),
     l1_neuron_decay( 0. ),
     l1_neuron_decay_center( 0 ),
+    online( false ),
     compute_all_test_costs( false ),
+    reconstruct_hidden( false ),
     n_layers( 0 ),
     currently_trained_layer( 0 )
 {
@@ -123,7 +125,7 @@
                   &quot;The layers of units in the network. The first element\n&quot;
                   &quot;of this vector should be the input layer and the\n&quot;
                   &quot;subsequent elements should be the hidden layers. The\n&quot;
-                  &quot;output should not be included in this layer.\n&quot;);
+                  &quot;output layer should not be included in layers.\n&quot;);
 
     declareOption(ol, &quot;connections&quot;, &amp;StackedAutoassociatorsNet::connections,
                   OptionBase::buildoption,
@@ -142,6 +144,13 @@
                   &quot;in the hidden layers. They must have the same input and\n&quot;
                   &quot;output sizes, compatible with their corresponding layers.&quot;);
 
+    declareOption(ol, &quot;direct_connections&quot;, 
+                  &amp;StackedAutoassociatorsNet::direct_connections,
+                  OptionBase::buildoption,
+                  &quot;Optional weights from each inputs to all other inputs'\n&quot;
+                  &quot;reconstruction, which can capture simple (linear or log-linear)\n&quot;
+                  &quot;correlations between inputs.&quot;);
+
     declareOption(ol, &quot;final_module&quot;, &amp;StackedAutoassociatorsNet::final_module,
                   OptionBase::buildoption,
                   &quot;Module that takes as input the output of the last layer\n&quot;
@@ -167,6 +176,11 @@
                   &quot;previous layers.\n&quot;
         );
 
+    declareOption(ol, &quot;online&quot;, &amp;StackedAutoassociatorsNet::online,
+                  OptionBase::buildoption,
+                  &quot;If true then all unsupervised training stages (as well as\n&quot;
+                  &quot;the fine-tuning stage) are done simultaneously.\n&quot;);
+
     declareOption(ol, &quot;partial_costs_weights&quot;, 
                   &amp;StackedAutoassociatorsNet::partial_costs_weights,
                   OptionBase::buildoption,
@@ -181,6 +195,13 @@
                   &quot;(up to the currently trained layer) should be computed.\n&quot;
         );
 
+    declareOption(ol, &quot;reconstruct_hidden&quot;, 
+                  &amp;StackedAutoassociatorsNet::reconstruct_hidden,
+                  OptionBase::buildoption,
+                  &quot;Indication that the autoassociators are also trained to\n&quot;
+                  &quot;reconstruct their hidden layers (inspired from CD1 in an RBM).\n&quot;
+        );
+
     declareOption(ol, &quot;greedy_stages&quot;, 
                   &amp;StackedAutoassociatorsNet::greedy_stages,
                   OptionBase::learntoption,
@@ -228,7 +249,7 @@
                     &quot;usage of weighted samples (weight size &gt; 0) is not\n&quot;
                     &quot;implemented yet.\n&quot;);
 
-        if( training_schedule.length() != n_layers-1 )        
+        if( !online &amp;&amp; training_schedule.length() != n_layers-1 )        
             PLERROR(&quot;StackedAutoassociatorsNet::build_() - \n&quot;
                     &quot;training_schedule should have %d elements.\n&quot;,
                     n_layers-1);
@@ -244,22 +265,34 @@
                     &quot;partial_costs_weights should have %d elements.\n&quot;,
                     n_layers-1);
 
-        if(greedy_stages.length() == 0)
+        if( online &amp;&amp; reconstruct_hidden )
+            PLERROR(&quot;StackedAutoassociatorsNet::build_()&quot;
+                    &quot; - \n&quot;
+                    &quot;cannot use online setting with reconstruct_hidden=true.\n&quot;);
+
+        if( !online )
         {
-            greedy_stages.resize(n_layers-1);
-            greedy_stages.clear();
+            if( greedy_stages.length() == 0)
+            {
+                greedy_stages.resize(n_layers-1);
+                greedy_stages.clear();
+            }
+            
+            if(stage &gt; 0)
+                currently_trained_layer = n_layers;
+            else
+            {            
+                currently_trained_layer = n_layers-1;
+                while(currently_trained_layer&gt;1
+                      &amp;&amp; greedy_stages[currently_trained_layer-1] &lt;= 0)
+                    currently_trained_layer--;
+            }
         }
-
-        if(stage &gt; 0)
+        else
+        {
             currently_trained_layer = n_layers;
-        else
-        {            
-            currently_trained_layer = n_layers-1;
-            while(currently_trained_layer&gt;1
-                  &amp;&amp; greedy_stages[currently_trained_layer-1] &lt;= 0)
-                currently_trained_layer--;
         }
-
+    
         build_layers_and_connections();
         build_costs();
     }
@@ -285,8 +318,24 @@
                 &quot;there should be either %d correlation connections or none.\n&quot;,
                 n_layers-1);
     
+    if( direct_connections.length() != 0 &amp;&amp;
+        direct_connections.length() != n_layers-1 )
+        PLERROR(&quot;StackedAutoassociatorsNet::build_layers_and_connections() - \n&quot;
+                &quot;there should be either %d direct connections or none.\n&quot;,
+                n_layers-1);
+
+    if(reconstruct_hidden &amp;&amp; compute_all_test_costs )
+        PLERROR(&quot;StackedAutoassociatorsNet::build_layers_and_connections() - \n&quot;
+                &quot;compute_all_test_costs option is not implemented for\n&quot;
+                &quot;reconstruct_hidden option.&quot;);
+
+    
     if(correlation_connections.length() != 0)
     {
+        if( compute_all_test_costs )
+            PLERROR(&quot;StackedAutoassociatorsNet::build_layers_and_connections() - \n&quot;
+                    &quot;compute_all_test_costs option is not implemented for\n&quot;
+                    &quot;correlation_connections.&quot;);
         correlation_layers.resize( layers.length()-1 );
         for( int i=0 ; i&lt;n_layers-1 ; i++ )
         {
@@ -344,6 +393,11 @@
 
         if(correlation_connections.length() != 0)
         {
+            if(reconstruct_hidden)
+                PLERROR(&quot;StackedAutoassociatorsNet::build_layers_and_connections()&quot;
+                        &quot; - \n&quot;
+                        &quot;cannot use correlation_connections with reconstruct_hidden=true.\n&quot;);
+
             if( correlation_connections[i]-&gt;up_size != layers[i+1]-&gt;size ||
                 correlation_connections[i]-&gt;down_size != layers[i+1]-&gt;size )
                 PLERROR(&quot;StackedAutoassociatorsNet::build_layers_and_connections()&quot;
@@ -368,6 +422,33 @@
             }
         }
 
+        if(direct_connections.length() != 0)
+        {
+            if( online )
+                PLERROR(&quot;StackedAutoassociatorsNet::build_layers_and_connections()&quot;
+                        &quot; - \n&quot;
+                        &quot;cannot use direct_connections in the online setting.\n&quot;);
+
+
+            if(reconstruct_hidden)
+                PLERROR(&quot;StackedAutoassociatorsNet::build_layers_and_connections()&quot;
+                        &quot; - \n&quot;
+                        &quot;cannot use direct_connections with reconstruct_hidden=true.\n&quot;);
+
+            if( direct_connections[i]-&gt;up_size != layers[i]-&gt;size ||
+                direct_connections[i]-&gt;down_size != layers[i]-&gt;size )
+                PLERROR(&quot;StackedAutoassociatorsNet::build_layers_and_connections()&quot;
+                        &quot; - \n&quot;
+                        &quot;direct_connections[%i] should have a up_size and &quot;
+                        &quot;down_size of %d.\n&quot;,
+                        i, layers[i]-&gt;size);
+            if( !(direct_connections[i]-&gt;random_gen) )
+            {
+                direct_connections[i]-&gt;random_gen = random_gen;
+                direct_connections[i]-&gt;forget();
+            }
+        }
+
         if( !(layers[i]-&gt;random_gen) )
         {
             layers[i]-&gt;random_gen = random_gen;
@@ -406,6 +487,10 @@
 {
     MODULE_LOG &lt;&lt; &quot;build_final_cost() called&quot; &lt;&lt; endl;
 
+    if( !final_cost )
+        PLERROR(&quot;StackedAutoassociatorsNet::build_costs() - \n&quot;
+                &quot;final_cost should be provided.\n&quot;);
+
     final_cost_gradient.resize( final_cost-&gt;input_size );
     final_cost-&gt;setLearningRate( fine_tuning_learning_rate );
 
@@ -445,6 +530,11 @@
     
     if(partial_costs)
     {
+
+        if( correlation_connections.length() != 0 )
+            PLERROR(&quot;StackedAutoassociatorsNet::build_costs() - \n&quot;
+                    &quot;correlation_connections cannot be used with partial costs.&quot;);
+            
         partial_costs_positions.resize(partial_costs.length());
         partial_costs_positions.clear();
         for(int i=0; i&lt;partial_costs.length(); i++)
@@ -485,28 +575,37 @@
 
     // deepCopyField(, copies);
 
+    // Public options
     deepCopyField(training_schedule, copies);
     deepCopyField(layers, copies);
     deepCopyField(connections, copies);
     deepCopyField(reconstruction_connections, copies);
+    deepCopyField(correlation_connections, copies);
+    deepCopyField(direct_connections, copies);
     deepCopyField(final_module, copies);
     deepCopyField(final_cost, copies);
     deepCopyField(partial_costs, copies);
     deepCopyField(partial_costs_weights, copies);
+
+    // Protected options
     deepCopyField(activations, copies);
     deepCopyField(expectations, copies);
     deepCopyField(activation_gradients, copies);
     deepCopyField(expectation_gradients, copies);
     deepCopyField(reconstruction_activations, copies);
-    deepCopyField(reconstruction_expectations, copies);
     deepCopyField(reconstruction_activation_gradients, copies);
-    deepCopyField(reconstruction_expectation_gradients, copies);
-    deepCopyField(correlation_connections, copies);
+    deepCopyField(reconstruction_activation_gradients_from_hid_rec, copies);
+    deepCopyField(reconstruction_expectation_gradients_from_hid_rec, copies);
+    deepCopyField(hidden_reconstruction_activations, copies);
+    deepCopyField(hidden_reconstruction_activation_gradients, copies);
     deepCopyField(correlation_activations, copies);
     deepCopyField(correlation_expectations, copies);
     deepCopyField(correlation_activation_gradients, copies);
     deepCopyField(correlation_expectation_gradients, copies);
     deepCopyField(correlation_layers, copies);
+    deepCopyField(direct_activations, copies);
+    deepCopyField(direct_and_reconstruction_activations, copies);
+    deepCopyField(direct_and_reconstruction_activation_gradients, copies);
     deepCopyField(partial_costs_positions, copies);
     deepCopyField(partial_cost_value, copies);
     deepCopyField(final_cost_input, copies);
@@ -587,115 +686,189 @@
     real lr = 0;
     int init_stage;
 
-    /***** initial greedy training *****/
-    for( int i=0 ; i&lt;n_layers-1 ; i++ )
+    if( !online )
     {
-        MODULE_LOG &lt;&lt; &quot;Training connection weights between layers &quot; &lt;&lt; i
-            &lt;&lt; &quot; and &quot; &lt;&lt; i+1 &lt;&lt; endl;
 
-        int end_stage = training_schedule[i];
-        int* this_stage = greedy_stages.subVec(i,1).data();
-        init_stage = *this_stage;
+        /***** initial greedy training *****/
+        for( int i=0 ; i&lt;n_layers-1 ; i++ )
+        {
+            MODULE_LOG &lt;&lt; &quot;Training connection weights between layers &quot; &lt;&lt; i
+                       &lt;&lt; &quot; and &quot; &lt;&lt; i+1 &lt;&lt; endl;
 
-        MODULE_LOG &lt;&lt; &quot;  stage = &quot; &lt;&lt; *this_stage &lt;&lt; endl;
-        MODULE_LOG &lt;&lt; &quot;  end_stage = &quot; &lt;&lt; end_stage &lt;&lt; endl;
-        MODULE_LOG &lt;&lt; &quot;  greedy_learning_rate = &quot; &lt;&lt; greedy_learning_rate &lt;&lt; endl;
+            int end_stage = training_schedule[i];
+            int* this_stage = greedy_stages.subVec(i,1).data();
+            init_stage = *this_stage;
 
-        if( report_progress &amp;&amp; *this_stage &lt; end_stage )
-            pb = new ProgressBar( &quot;Training layer &quot;+tostring(i)
-                                  +&quot; of &quot;+classname(),
-                                  end_stage - init_stage );
+            MODULE_LOG &lt;&lt; &quot;  stage = &quot; &lt;&lt; *this_stage &lt;&lt; endl;
+            MODULE_LOG &lt;&lt; &quot;  end_stage = &quot; &lt;&lt; end_stage &lt;&lt; endl;
+            MODULE_LOG &lt;&lt; &quot;  greedy_learning_rate = &quot; &lt;&lt; greedy_learning_rate &lt;&lt; endl;
 
-        train_costs.fill(MISSING_VALUE);
-        lr = greedy_learning_rate;
-        layers[i]-&gt;setLearningRate( lr );
-        connections[i]-&gt;setLearningRate( lr );
-        reconstruction_connections[i]-&gt;setLearningRate( lr );
-        if(correlation_connections.length() != 0)
-        {
-            correlation_connections[i]-&gt;setLearningRate( lr );
-            correlation_layers[i]-&gt;setLearningRate( lr );
-        }
-        layers[i+1]-&gt;setLearningRate( lr );
+            if( report_progress &amp;&amp; *this_stage &lt; end_stage )
+                pb = new ProgressBar( &quot;Training layer &quot;+tostring(i)
+                                      +&quot; of &quot;+classname(),
+                                      end_stage - init_stage );
 
-        reconstruction_activations.resize(layers[i]-&gt;size);
-        reconstruction_expectations.resize(layers[i]-&gt;size);
-        reconstruction_activation_gradients.resize(layers[i]-&gt;size);
-        reconstruction_expectation_gradients.resize(layers[i]-&gt;size);
+            train_costs.fill(MISSING_VALUE);
+            lr = greedy_learning_rate;
+            layers[i]-&gt;setLearningRate( lr );
+            connections[i]-&gt;setLearningRate( lr );
+            reconstruction_connections[i]-&gt;setLearningRate( lr );
+            if(correlation_connections.length() != 0)
+            {
+                correlation_connections[i]-&gt;setLearningRate( lr );
+                correlation_layers[i]-&gt;setLearningRate( lr );
+            }
+            if(direct_connections.length() != 0)
+            {
+                direct_connections[i]-&gt;setLearningRate( lr );
+            }
+            layers[i+1]-&gt;setLearningRate( lr );
+            if(partial_costs.length() != 0 &amp;&amp; partial_costs[i])
+                        partial_costs[i]-&gt;setLearningRate( lr );
 
-        for( ; *this_stage&lt;end_stage ; (*this_stage)++ )
-        {
-            if( !fast_exact_is_equal( greedy_decrease_ct , 0 ) )
+            // Make sure that storage not null, will be resized anyways by bprop calls
+            reconstruction_activations.resize(layers[i]-&gt;size);
+            reconstruction_activation_gradients.resize(layers[i]-&gt;size);
+            reconstruction_expectation_gradients.resize(layers[i]-&gt;size);
+
+            if(reconstruct_hidden)
             {
-                lr = greedy_learning_rate/(1 + greedy_decrease_ct 
-                                           * (*this_stage)); 
-                layers[i]-&gt;setLearningRate( lr );
-                connections[i]-&gt;setLearningRate( lr );
-                reconstruction_connections[i]-&gt;setLearningRate( lr );
-                layers[i+1]-&gt;setLearningRate( lr );
-                if(correlation_connections.length() != 0)
+                reconstruction_activation_gradients_from_hid_rec.resize(
+                    layers[i+1]-&gt;size);
+                reconstruction_expectation_gradients_from_hid_rec.resize(
+                    layers[i+1]-&gt;size);
+                hidden_reconstruction_activations.resize(layers[i+1]-&gt;size);
+                hidden_reconstruction_activation_gradients.resize(layers[i+1]-&gt;size);
+            }
+
+            if(direct_connections.length() != 0)
+            {
+                direct_activations.resize(layers[i]-&gt;size);
+                direct_and_reconstruction_activations.resize(layers[i]-&gt;size);
+                direct_and_reconstruction_activation_gradients.resize(layers[i]-&gt;size);
+            }
+            for( ; *this_stage&lt;end_stage ; (*this_stage)++ )
+            {
+                if( !fast_exact_is_equal( greedy_decrease_ct , 0 ) )
                 {
-                    correlation_connections[i]-&gt;setLearningRate( lr );
-                    correlation_layers[i]-&gt;setLearningRate( lr );
+                    lr = greedy_learning_rate/(1 + greedy_decrease_ct 
+                                               * (*this_stage)); 
+                    layers[i]-&gt;setLearningRate( lr );
+                    connections[i]-&gt;setLearningRate( lr );
+                    reconstruction_connections[i]-&gt;setLearningRate( lr );
+                    layers[i+1]-&gt;setLearningRate( lr );
+                    if(correlation_connections.length() != 0)
+                    {
+                        correlation_connections[i]-&gt;setLearningRate( lr );
+                        correlation_layers[i]-&gt;setLearningRate( lr );
+                    }
+                    if(direct_connections.length() != 0)
+                    {
+                        direct_connections[i]-&gt;setLearningRate( lr );
+                    }
+                    if(partial_costs.length() != 0 &amp;&amp; partial_costs[i])
+                        partial_costs[i]-&gt;setLearningRate( lr );
                 }
+
+                sample = *this_stage % nsamples;
+                train_set-&gt;getExample(sample, input, target, weight);
+                greedyStep( input, target, i, train_costs );
+                train_stats-&gt;update( train_costs );
+
+                if( pb )
+                    pb-&gt;update( *this_stage - init_stage + 1 );
             }
+        }
 
-            sample = *this_stage % nsamples;
-            train_set-&gt;getExample(sample, input, target, weight);
-            greedyStep( input, target, i, train_costs );
-            train_stats-&gt;update( train_costs );
+        /***** fine-tuning by gradient descent *****/
+        if( stage &lt; nstages )
+        {
 
-            if( pb )
-                pb-&gt;update( *this_stage - init_stage + 1 );
+            MODULE_LOG &lt;&lt; &quot;Fine-tuning all parameters, by gradient descent&quot; &lt;&lt; endl;
+            MODULE_LOG &lt;&lt; &quot;  stage = &quot; &lt;&lt; stage &lt;&lt; endl;
+            MODULE_LOG &lt;&lt; &quot;  nstages = &quot; &lt;&lt; nstages &lt;&lt; endl;
+            MODULE_LOG &lt;&lt; &quot;  fine_tuning_learning_rate = &quot; &lt;&lt; 
+                fine_tuning_learning_rate &lt;&lt; endl;
+
+            init_stage = stage;
+            if( report_progress &amp;&amp; stage &lt; nstages )
+                pb = new ProgressBar( &quot;Fine-tuning parameters of all layers of &quot;
+                                      + classname(),
+                                      nstages - init_stage );
+
+            setLearningRate( fine_tuning_learning_rate );
+            train_costs.fill(MISSING_VALUE);
+            for( ; stage&lt;nstages ; stage++ )
+            {
+                sample = stage % nsamples;
+                if( !fast_exact_is_equal( fine_tuning_decrease_ct, 0. ) )
+                    setLearningRate( fine_tuning_learning_rate
+                                     / (1. + fine_tuning_decrease_ct * stage ) );
+
+                train_set-&gt;getExample( sample, input, target, weight );
+                fineTuningStep( input, target, train_costs );
+                train_stats-&gt;update( train_costs );
+
+                if( pb )
+                    pb-&gt;update( stage - init_stage + 1 );
+            }
         }
+    
+        train_stats-&gt;finalize();
+        MODULE_LOG &lt;&lt; &quot;  train costs = &quot; &lt;&lt; train_stats-&gt;getMean() &lt;&lt; endl;
+
+        // Update currently_trained_layer
+        if(stage &gt; 0)
+            currently_trained_layer = n_layers;
+        else
+        {            
+            currently_trained_layer = n_layers-1;
+            while(currently_trained_layer&gt;1 
+                  &amp;&amp; greedy_stages[currently_trained_layer-1] &lt;= 0)
+                currently_trained_layer--;
+        }
     }
-
-    /***** fine-tuning by gradient descent *****/
-    if( stage &lt; nstages )
+    else
     {
+        // Train all layers simultaneously AND fine-tuning as well!
+        if( stage &lt; nstages )
+        {
 
-        MODULE_LOG &lt;&lt; &quot;Fine-tuning all parameters, by gradient descent&quot; &lt;&lt; endl;
-        MODULE_LOG &lt;&lt; &quot;  stage = &quot; &lt;&lt; stage &lt;&lt; endl;
-        MODULE_LOG &lt;&lt; &quot;  nstages = &quot; &lt;&lt; nstages &lt;&lt; endl;
-        MODULE_LOG &lt;&lt; &quot;  fine_tuning_learning_rate = &quot; &lt;&lt; 
-            fine_tuning_learning_rate &lt;&lt; endl;
+            MODULE_LOG &lt;&lt; &quot;Training all layers greedy layer-wise AND &quot;
+                       &lt;&lt; &quot;fine-tuning all parameters, by gradient descent&quot; 
+                       &lt;&lt; endl;
+            MODULE_LOG &lt;&lt; &quot;  stage = &quot; &lt;&lt; stage &lt;&lt; endl;
+            MODULE_LOG &lt;&lt; &quot;  nstages = &quot; &lt;&lt; nstages &lt;&lt; endl;
+            MODULE_LOG &lt;&lt; &quot;  fine_tuning_learning_rate = &quot; 
+                       &lt;&lt; fine_tuning_learning_rate &lt;&lt; endl;
+            MODULE_LOG &lt;&lt; &quot;  greedy_learning_rate = &quot; 
+                       &lt;&lt; greedy_learning_rate &lt;&lt; endl;
 
-        init_stage = stage;
-        if( report_progress &amp;&amp; stage &lt; nstages )
-            pb = new ProgressBar( &quot;Fine-tuning parameters of all layers of &quot;
-                                  + classname(),
-                                  nstages - init_stage );
+            init_stage = stage;
+            if( report_progress &amp;&amp; stage &lt; nstages )
+                pb = new ProgressBar( 
+                    &quot;Greedy layer-wise training AND fine-tuning parameters of &quot;
+                                      + classname(),
+                                      nstages - init_stage );
 
-        setLearningRate( fine_tuning_learning_rate );
-        train_costs.fill(MISSING_VALUE);
-        for( ; stage&lt;nstages ; stage++ )
-        {
-            sample = stage % nsamples;
-            if( !fast_exact_is_equal( fine_tuning_decrease_ct, 0. ) )
-                setLearningRate( fine_tuning_learning_rate
-                                 / (1. + fine_tuning_decrease_ct * stage ) );
+            setLearningRate( fine_tuning_learning_rate );
+            train_costs.fill(MISSING_VALUE);
+            for( ; stage&lt;nstages ; stage++ )
+            {
+                sample = stage % nsamples;
+                if( !fast_exact_is_equal( fine_tuning_decrease_ct, 0. ) )
+                    setLearningRate( fine_tuning_learning_rate
+                                     / (1. + fine_tuning_decrease_ct * stage ) );
 
-            train_set-&gt;getExample( sample, input, target, weight );
-            fineTuningStep( input, target, train_costs );
-            train_stats-&gt;update( train_costs );
+                train_set-&gt;getExample( sample, input, target, weight );
+                onlineStep( input, target, train_costs );
+                train_stats-&gt;update( train_costs );
 
-            if( pb )
-                pb-&gt;update( stage - init_stage + 1 );
+                if( pb )
+                    pb-&gt;update( stage - init_stage + 1 );
+            }
         }
-    }
-    
-    train_stats-&gt;finalize();
-    MODULE_LOG &lt;&lt; &quot;  train costs = &quot; &lt;&lt; train_stats-&gt;getMean() &lt;&lt; endl;
 
-    // Update currently_trained_layer
-    if(stage &gt; 0)
-        currently_trained_layer = n_layers;
-    else
-    {            
-        currently_trained_layer = n_layers-1;
-        while(currently_trained_layer&gt;1 
-              &amp;&amp; greedy_stages[currently_trained_layer-1] &lt;= 0)
-            currently_trained_layer--;
     }
 }
 
@@ -733,7 +906,7 @@
 
         // Update partial cost (might contain some weights for example)
         partial_costs[ index ]-&gt;bpropUpdate( expectations[ index + 1 ],
-                                             target, partial_cost_value,
+                                             target, partial_cost_value[0],
                                              expectation_gradients[ index + 1 ]
                                              );
 
@@ -757,30 +930,99 @@
 
     reconstruction_connections[ index ]-&gt;fprop( expectations[ index + 1],
                                                 reconstruction_activations);
-    layers[ index ]-&gt;fprop( reconstruction_activations,
-                            layers[ index ]-&gt;expectation);
+    if(direct_connections.length() != 0)
+    {
+        direct_connections[ index ]-&gt;fprop( expectations[ index ], 
+                                            direct_activations );
+        direct_and_reconstruction_activations.clear();
+        direct_and_reconstruction_activations += direct_activations;
+        direct_and_reconstruction_activations += reconstruction_activations;
 
-    layers[ index ]-&gt;activation &lt;&lt; reconstruction_activations;
-    layers[ index ]-&gt;expectation_is_up_to_date = true;
-    train_costs[index] = layers[ index ]-&gt;fpropNLL(expectations[index]);
-    
-    layers[ index ]-&gt;bpropNLL(expectations[index], train_costs[index],
-                              reconstruction_activation_gradients);
+        layers[ index ]-&gt;fprop( direct_and_reconstruction_activations,
+                                layers[ index ]-&gt;expectation);
+        
+        layers[ index ]-&gt;activation &lt;&lt; direct_and_reconstruction_activations;
+        layers[ index ]-&gt;expectation_is_up_to_date = true;
+        train_costs[index] = layers[ index ]-&gt;fpropNLL(expectations[index]);
+        
+        layers[ index ]-&gt;bpropNLL(expectations[index], train_costs[index],
+                                  direct_and_reconstruction_activation_gradients);
 
-    layers[ index ]-&gt;update(reconstruction_activation_gradients);
+        layers[ index ]-&gt;update(direct_and_reconstruction_activation_gradients);
 
-    // // This is a bad update! Propagates gradient through sigmoid again!
-    // layers[ index ]-&gt;bpropUpdate( reconstruction_activations, 
-    //                                   layers[ index ]-&gt;expectation,
-    //                                   reconstruction_activation_gradients,
-    //                                   reconstruction_expectation_gradients);
+        direct_connections[ index ]-&gt;bpropUpdate( 
+            expectations[ index ],
+            direct_activations,
+            reconstruction_expectation_gradients, // Will be overwritten later
+            direct_and_reconstruction_activation_gradients);
+        
+        reconstruction_connections[ index ]-&gt;bpropUpdate( 
+            expectations[ index + 1], 
+            reconstruction_activations, 
+            reconstruction_expectation_gradients, 
+            direct_and_reconstruction_activation_gradients);
+    }
+    else
+    {
+        layers[ index ]-&gt;fprop( reconstruction_activations,
+                                layers[ index ]-&gt;expectation);
+        
+        layers[ index ]-&gt;activation &lt;&lt; reconstruction_activations;
+        layers[ index ]-&gt;expectation_is_up_to_date = true;
+        real rec_err = layers[ index ]-&gt;fpropNLL(expectations[index]);
+        train_costs[index] = rec_err;
 
-    reconstruction_connections[ index ]-&gt;bpropUpdate( 
-        expectations[ index + 1], 
-        reconstruction_activations, 
-        reconstruction_expectation_gradients, 
-        reconstruction_activation_gradients);
+        layers[ index ]-&gt;bpropNLL(expectations[index], rec_err,
+                                  reconstruction_activation_gradients);
 
+        if(reconstruct_hidden)
+        {
+            connections[ index ]-&gt;fprop( layers[ index ]-&gt;expectation, 
+                                         hidden_reconstruction_activations );
+            layers[ index+1 ]-&gt;fprop( hidden_reconstruction_activations,
+                layers[ index+1 ]-&gt;expectation );
+            layers[ index+1 ]-&gt;activation &lt;&lt; hidden_reconstruction_activations;
+            layers[ index+1 ]-&gt;expectation_is_up_to_date = true;
+            real hid_rec_err = layers[ index+1 ]-&gt;fpropNLL(expectations[index+1]);
+            train_costs[index] += hid_rec_err;
+
+            layers[ index+1 ]-&gt;bpropNLL(expectations[index+1], hid_rec_err,
+                                        hidden_reconstruction_activation_gradients);
+            layers[ index+1 ]-&gt;update(hidden_reconstruction_activation_gradients);
+            
+            connections[ index ]-&gt;bpropUpdate( 
+                layers[ index ]-&gt;expectation, 
+                hidden_reconstruction_activations,
+                reconstruction_expectation_gradients_from_hid_rec,
+                hidden_reconstruction_activation_gradients);
+
+            layers[ index ]-&gt;bpropUpdate( 
+                reconstruction_activations,
+                layers[ index ]-&gt;expectation,
+                reconstruction_activation_gradients_from_hid_rec,
+                reconstruction_expectation_gradients_from_hid_rec);
+        }
+        
+        layers[ index ]-&gt;update(reconstruction_activation_gradients);
+
+        if(reconstruct_hidden)
+            reconstruction_activation_gradients +=
+                reconstruction_activation_gradients_from_hid_rec;
+
+        // // This is a bad update! Propagates gradient through sigmoid again!
+        // layers[ index ]-&gt;bpropUpdate( reconstruction_activations, 
+        //                                   layers[ index ]-&gt;expectation,
+        //                                   reconstruction_activation_gradients,
+        //                                   reconstruction_expectation_gradients);
+        
+        reconstruction_connections[ index ]-&gt;bpropUpdate( 
+            expectations[ index + 1], 
+            reconstruction_activations, 
+            reconstruction_expectation_gradients, 
+            reconstruction_activation_gradients);
+
+    }
+
     if(!fast_exact_is_equal(l1_neuron_decay,0))
     {
         // Compute L1 penalty gradient on neurons
@@ -932,6 +1174,276 @@
     }
 }
 
+void StackedAutoassociatorsNet::onlineStep( const Vec&amp; input, 
+                                            const Vec&amp; target,
+                                            Vec&amp; train_costs )
+{
+    real lr;
+    // fprop
+    expectations[0] &lt;&lt; input;
+
+    if(correlation_connections.length() != 0)
+    {
+        for( int i=0 ; i&lt;n_layers-1; i++ )
+        {
+            connections[i]-&gt;fprop( expectations[i], correlation_activations[i] );
+            layers[i+1]-&gt;fprop( correlation_activations[i],
+                                correlation_expectations[i] );
+            correlation_connections[i]-&gt;fprop( correlation_expectations[i], 
+                                               activations[i+1] );
+            correlation_layers[i]-&gt;fprop( activations[i+1], 
+                                          expectations[i+1] );
+        }
+    }
+    else
+    {
+        for( int i=0 ; i&lt;n_layers-1; i++ )
+        {
+            connections[i]-&gt;fprop( expectations[i], activations[i+1] );
+            layers[i+1]-&gt;fprop(activations[i+1],expectations[i+1]);
+            
+            if( partial_costs.length() != 0 &amp;&amp; partial_costs[ i ] )
+            {
+                // Set learning rates
+
+                if( !fast_exact_is_equal( greedy_decrease_ct , 0 ) )
+                    lr = greedy_learning_rate / 
+                        (1 + greedy_decrease_ct * stage);
+                else
+                    lr = greedy_learning_rate;
+
+                partial_costs[ i ]-&gt;setLearningRate( lr );
+                layers[ i+1 ]-&gt;setLearningRate( lr );
+                connections[ i ]-&gt;setLearningRate( lr );
+
+                partial_costs[ i ]-&gt;fprop( expectations[ i + 1],
+                                           target, partial_cost_value );
+                
+                // Update partial cost (might contain some weights for example)
+                partial_costs[ i ]-&gt;bpropUpdate( 
+                    expectations[ i + 1 ],
+                    target, partial_cost_value[0],
+                    expectation_gradients[ i + 1 ]
+                    );
+                
+                train_costs.subVec(partial_costs_positions[i],
+                                   partial_cost_value.length()) 
+                    &lt;&lt; partial_cost_value;
+                
+                if( !fast_exact_is_equal( partial_costs_weights.length(), 0 ) )
+                    expectation_gradients[ i + 1 ] *= partial_costs_weights[i];
+                
+                // Update hidden layer bias and weights
+                layers[ i+1 ]-&gt;bpropUpdate( activations[ i + 1 ],
+                                            expectations[ i + 1 ],
+                                            activation_gradients[ i + 1 ],
+                                            expectation_gradients[ i + 1 ] );
+                
+                connections[ i ]-&gt;bpropUpdate( expectations[ i ],
+                                               activations[ i + 1 ],
+                                               expectation_gradients[ i ],
+                                               activation_gradients[ i + 1 ] );
+
+                if( !fast_exact_is_equal( fine_tuning_decrease_ct , 0 ) )
+                    lr = fine_tuning_learning_rate / 
+                        (1 + fine_tuning_decrease_ct * stage);
+                else
+                    lr = fine_tuning_learning_rate;
+
+                layers[ i+1 ]-&gt;setLearningRate( lr );
+                connections[ i ]-&gt;setLearningRate( lr );
+            }
+        }
+    }
+
+    final_module-&gt;fprop( expectations[ n_layers-1 ],
+                         final_cost_input );
+    final_cost-&gt;fprop( final_cost_input, target, final_cost_value );
+
+    train_costs.subVec(train_costs.length()-final_cost_value.length(),
+                       final_cost_value.length()) &lt;&lt;
+        final_cost_value;
+
+    final_cost-&gt;bpropUpdate( final_cost_input, target,
+                             final_cost_value[0],
+                             final_cost_gradient );
+    final_module-&gt;bpropUpdate( expectations[ n_layers-1 ],
+                               final_cost_input,
+                               expectation_gradients[ n_layers-1 ],
+                               final_cost_gradient );
+
+    // Unsupervised greedy layer-wise cost
+
+    // Set learning rates
+    if( !fast_exact_is_equal( greedy_decrease_ct , 0 ) )
+        lr = greedy_learning_rate / (1 + greedy_decrease_ct * stage) ;
+    else
+        lr = greedy_learning_rate;
+
+    for( int i=0 ; i&lt;n_layers-1 ; i++ )
+    {
+        layers[i]-&gt;setLearningRate( lr );
+        connections[i]-&gt;setLearningRate( lr );
+        reconstruction_connections[i]-&gt;setLearningRate( lr );
+        if(correlation_layers.length() != 0)
+        {
+            correlation_layers[i]-&gt;setLearningRate( lr );
+            correlation_connections[i]-&gt;setLearningRate( lr );
+        }
+    }
+    layers[n_layers-1]-&gt;setLearningRate( lr );
+
+    // Backpropagate unsupervised gradient, layer-wise
+    for( int i=n_layers-1 ; i&gt;0 ; i-- )
+    {
+        reconstruction_connections[ i-1 ]-&gt;fprop( 
+            expectations[ i ],
+            reconstruction_activations);
+
+        layers[ i-1 ]-&gt;fprop( reconstruction_activations,
+                                layers[ i-1 ]-&gt;expectation);
+        
+        layers[ i-1 ]-&gt;activation &lt;&lt; reconstruction_activations;
+        layers[ i-1 ]-&gt;expectation_is_up_to_date = true;
+        real rec_err = layers[ i-1 ]-&gt;fpropNLL(expectations[i-1]);
+        train_costs[i-1] = rec_err;
+
+        layers[ i-1 ]-&gt;bpropNLL(expectations[i-1], rec_err,
+                                  reconstruction_activation_gradients);
+
+        layers[ i-1 ]-&gt;update(reconstruction_activation_gradients);
+
+        reconstruction_connections[ i-1 ]-&gt;bpropUpdate( 
+            expectations[ i ], 
+            reconstruction_activations, 
+            reconstruction_expectation_gradients, 
+            reconstruction_activation_gradients);
+
+        if(!fast_exact_is_equal(l1_neuron_decay,0))
+        {
+            // Compute L1 penalty gradient on neurons
+            real* hid = expectations[ i ].data();
+            real* grad = reconstruction_expectation_gradients.data();
+            int len = expectations[ i ].length();
+            for(int j=0; j&lt;len; j++)
+            {
+                if(*hid &gt; l1_neuron_decay_center)
+                    *grad -= l1_neuron_decay;
+                else if(*hid &lt; l1_neuron_decay_center)
+                    *grad += l1_neuron_decay;
+                hid++;
+                grad++;
+            }
+        }
+
+        if( correlation_connections.length() != 0 )
+        {
+            correlation_layers[i-1]-&gt;bpropUpdate( 
+                activations[i],
+                expectations[i],
+                reconstruction_activation_gradients,
+                reconstruction_expectation_gradients );
+            
+            correlation_connections[i-1]-&gt;bpropUpdate( 
+                correlation_expectations[i-1],
+                activations[i],
+                correlation_expectation_gradients[i-1],
+                reconstruction_activation_gradients);
+
+            layers[i]-&gt;bpropUpdate( correlation_activations[i-1],
+                                    correlation_expectations[i-1],
+                                    correlation_activation_gradients[i-1],
+                                    correlation_expectation_gradients[i-1] );
+            
+            connections[i-1]-&gt;bpropUpdate( expectations[i-1],
+                                           correlation_activations[i-1],
+                                           reconstruction_expectation_gradients,
+                                           correlation_activation_gradients[i-1] );
+        }
+        else
+        {
+            layers[i]-&gt;bpropUpdate( 
+                activations[i],
+                expectations[i],
+                reconstruction_activation_gradients,
+                reconstruction_expectation_gradients );
+            
+            connections[i-1]-&gt;bpropUpdate( 
+                expectations[i-1],
+                activations[i],
+                reconstruction_expectation_gradients,
+                reconstruction_activation_gradients);
+        }
+    }
+
+    // Put back fine-tuning learning rate
+    // Set learning rates
+    if( !fast_exact_is_equal( fine_tuning_decrease_ct , 0 ) )
+        lr = fine_tuning_learning_rate 
+            / (1 + fine_tuning_decrease_ct * stage) ;
+    else
+        lr = fine_tuning_learning_rate ;
+
+    // Set learning rate back for fine-tuning
+    for( int i=0 ; i&lt;n_layers-1 ; i++ )
+    {
+        layers[i]-&gt;setLearningRate( lr );
+        connections[i]-&gt;setLearningRate( lr );
+        //reconstruction_connections[i]-&gt;setLearningRate( lr );
+        if(correlation_layers.length() != 0)
+        {
+            correlation_layers[i]-&gt;setLearningRate( lr );
+            correlation_connections[i]-&gt;setLearningRate( lr );
+        }
+    }
+    layers[n_layers-1]-&gt;setLearningRate( lr );
+
+    // Fine-tuning backpropagation
+    if( correlation_connections.length() != 0 )
+    {
+        for( int i=n_layers-1 ; i&gt;0 ; i-- )
+        {
+            correlation_layers[i-1]-&gt;bpropUpdate( 
+                activations[i],
+                expectations[i],
+                activation_gradients[i],
+                expectation_gradients[i] );
+
+            correlation_connections[i-1]-&gt;bpropUpdate( 
+                correlation_expectations[i-1],
+                activations[i],
+                correlation_expectation_gradients[i-1],
+                activation_gradients[i] );
+
+            layers[i]-&gt;bpropUpdate( correlation_activations[i-1],
+                                    correlation_expectations[i-1],
+                                    correlation_activation_gradients[i-1],
+                                    correlation_expectation_gradients[i-1] );
+            
+            connections[i-1]-&gt;bpropUpdate( 
+                expectations[i-1],
+                correlation_activations[i-1],
+                expectation_gradients[i-1],
+                correlation_activation_gradients[i-1] );
+        }
+    }
+    else
+    {
+        for( int i=n_layers-1 ; i&gt;0 ; i-- )
+        {
+            layers[i]-&gt;bpropUpdate( activations[i],
+                                    expectations[i],
+                                    activation_gradients[i],
+                                    expectation_gradients[i] );
+            
+            connections[i-1]-&gt;bpropUpdate( expectations[i-1],
+                                           activations[i],
+                                           expectation_gradients[i-1],
+                                           activation_gradients[i] );
+        }        
+    }
+}
+
 void StackedAutoassociatorsNet::computeOutput(const Vec&amp; input, Vec&amp; output) const
 {
     // fprop
@@ -1009,11 +1521,20 @@
         {
             reconstruction_connections[ i ]-&gt;fprop( expectations[ i+1 ],
                                                     reconstruction_activations);
+            if( direct_connections.length() != 0 )
+            {
+                direct_connections[ i ]-&gt;fprop( 
+                    expectations[ i ], 
+                    direct_activations );
+                reconstruction_activations += direct_activations;
+            }
+
             layers[ i ]-&gt;fprop( reconstruction_activations,
-                                    layers[ i ]-&gt;expectation);
+                                layers[ i ]-&gt;expectation);
             
             layers[ i ]-&gt;activation &lt;&lt; reconstruction_activations;
             layers[ i ]-&gt;expectation_is_up_to_date = true;
+
             costs[i] = layers[ i ]-&gt;fpropNLL(expectations[ i ]);
 
             if( partial_costs &amp;&amp; partial_costs[i])
@@ -1032,6 +1553,13 @@
         reconstruction_connections[ currently_trained_layer-1 ]-&gt;fprop( 
             output,
             reconstruction_activations);
+        if( direct_connections.length() != 0 )
+        {
+            direct_connections[ currently_trained_layer-1 ]-&gt;fprop( 
+                expectations[ currently_trained_layer-1 ], 
+                direct_activations );
+            reconstruction_activations += direct_activations;
+        }
         layers[ currently_trained_layer-1 ]-&gt;fprop( 
             reconstruction_activations,
             layers[ currently_trained_layer-1 ]-&gt;expectation);
@@ -1043,6 +1571,22 @@
             layers[ currently_trained_layer-1 ]-&gt;fpropNLL(
                 expectations[ currently_trained_layer-1 ]);
 
+        if(reconstruct_hidden)
+        {
+            connections[ currently_trained_layer-1 ]-&gt;fprop( 
+                layers[ currently_trained_layer-1 ]-&gt;expectation, 
+                hidden_reconstruction_activations );
+            layers[ currently_trained_layer ]-&gt;fprop( 
+                hidden_reconstruction_activations,
+                layers[ currently_trained_layer ]-&gt;expectation );
+            layers[ currently_trained_layer ]-&gt;activation &lt;&lt; 
+                hidden_reconstruction_activations;
+            layers[ currently_trained_layer ]-&gt;expectation_is_up_to_date = true;
+            costs[ currently_trained_layer-1 ] += 
+                layers[ currently_trained_layer ]-&gt;fpropNLL(
+                    output);
+        }
+
         if( partial_costs &amp;&amp; partial_costs[ currently_trained_layer-1 ] )
         {
             partial_costs[ currently_trained_layer-1 ]-&gt;fprop( 
@@ -1074,10 +1618,10 @@
     
     for( int i=0 ; i&lt;partial_costs.size() ; i++ )
     {
-        TVec&lt;string&gt; cost_names = partial_costs[i]-&gt;name();
-        for(int j=0; j&lt;cost_names.length(); j++)
-            cost_names.push_back(&quot;partial_cost_&quot; + tostring(i+1) + &quot;_&quot; + 
-                cost_names[j]);
+        TVec&lt;string&gt; names = partial_costs[i]-&gt;name();
+        for(int j=0; j&lt;names.length(); j++)
+            cost_names.push_back(&quot;partial&quot; + tostring(i) + &quot;.&quot; + 
+                names[j]);
     }
 
     cost_names.append( final_cost-&gt;name() );
@@ -1104,6 +1648,10 @@
             correlation_layers[i]-&gt;setLearningRate( the_learning_rate );
             correlation_connections[i]-&gt;setLearningRate( the_learning_rate );
         }
+        if(direct_connections.length() != 0)
+        {
+            direct_connections[i]-&gt;setLearningRate( the_learning_rate );
+        }
     }
     layers[n_layers-1]-&gt;setLearningRate( the_learning_rate );
 

Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.h
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2007-09-07 22:07:33 UTC (rev 8056)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2007-09-07 22:08:43 UTC (rev 8057)
@@ -96,6 +96,9 @@
     //! The number of fine-tunig steps is defined by nstages.
     TVec&lt;int&gt; training_schedule;
 
+    //! Whether to do things by stages, including fine-tuning, or on-line
+    bool online;
+
     //! The layers of units in the network
     TVec&lt; PP&lt;RBMLayer&gt; &gt; layers;
 
@@ -110,6 +113,11 @@
     //! output sizes, compatible with their corresponding layers.
     TVec&lt; PP&lt;RBMConnection&gt; &gt; correlation_connections;
 
+    //! Optional weights from each inputs to all other inputs'
+    //! reconstruction, which can capture simple (linear or log-linear)
+    //! correlations between inputs.
+    mutable TVec&lt; PP&lt;RBMConnection&gt; &gt; direct_connections;
+
     //! Module that takes as input the output of the last layer
     //! (layers[n_layers-1), and feeds its output to final_cost
     //! which defines the fine-tuning criteria.
@@ -134,6 +142,10 @@
     //! layers (up to the currently trained layer) should be computed.
     bool compute_all_test_costs;
 
+    //! Indication that the autoassociators are also trained to
+    //! reconstruct their hidden layers (inspired from CD1 in an RBM)
+    bool reconstruct_hidden;
+
     //#####  Public Learnt Options  ###########################################
 
     //! Number of layers
@@ -184,6 +196,9 @@
     void fineTuningStep( const Vec&amp; input, const Vec&amp; target,
                          Vec&amp; train_costs );
 
+    void onlineStep( const Vec&amp; input, const Vec&amp; target,
+                         Vec&amp; train_costs );
+
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.
@@ -224,13 +239,25 @@
     
     //! Reconstruction expectations
     mutable Vec reconstruction_expectations;
-    
+        
     //! Reconstruction activation gradients
     mutable Vec reconstruction_activation_gradients;
-    
+
     //! Reconstruction expectation gradients
     mutable Vec reconstruction_expectation_gradients;
 
+    //! Reconstruction activation gradients coming from hidden reconstruction
+    mutable Vec reconstruction_activation_gradients_from_hid_rec;
+    
+    //! Reconstruction expectation gradients coming from hidden reconstruction
+    mutable Vec reconstruction_expectation_gradients_from_hid_rec;
+
+    //! Hidden reconstruction activations
+    mutable Vec hidden_reconstruction_activations;
+    
+    //! Hidden reconstruction activation gradients
+    mutable Vec hidden_reconstruction_activation_gradients;
+    
     //! Activations before the correlation layer
     mutable TVec&lt;Vec&gt; correlation_activations;
     
@@ -246,6 +273,15 @@
     //! Hidden layers for the correlation connections
     mutable TVec&lt; PP&lt;RBMLayer&gt; &gt; correlation_layers;
 
+    //! Activations from the direct connections
+    mutable Vec direct_activations;
+
+    //! Sum of activations from the direct and reconstruction connections
+    mutable Vec direct_and_reconstruction_activations;
+
+    //! Gradient of sum of activations from the direct and reconstruction connections
+    mutable Vec direct_and_reconstruction_activation_gradients;
+
     //! Position in the total cost vector of the different partial costs
     mutable TVec&lt;int&gt; partial_costs_positions;
     
@@ -273,7 +309,7 @@
     
     //! Indication whether final_cost has learning rate
     bool final_cost_has_learning_rate;
-    
+
 protected:
     //#####  Protected Member Functions  ######################################
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001504.html">[Plearn-commits] r8056 - trunk/plearn_learners/online
</A></li>
	<LI>Next message: <A HREF="001506.html">[Plearn-commits] r8058 - trunk/plearn_learners/generic/EXPERIMENTAL
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1505">[ date ]</a>
              <a href="thread.html#1505">[ thread ]</a>
              <a href="subject.html#1505">[ subject ]</a>
              <a href="author.html#1505">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
