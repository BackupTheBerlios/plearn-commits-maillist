<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r8090 - in trunk: plearn/base	plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-September/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8090%20-%20in%20trunk%3A%20plearn/base%0A%09plearn_learners/online&In-Reply-To=%3C200709212247.l8LMl1Ju005514%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001537.html">
   <LINK REL="Next"  HREF="001539.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r8090 - in trunk: plearn/base	plearn_learners/online</H1>
    <B>louradou at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8090%20-%20in%20trunk%3A%20plearn/base%0A%09plearn_learners/online&In-Reply-To=%3C200709212247.l8LMl1Ju005514%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r8090 - in trunk: plearn/base	plearn_learners/online">louradou at mail.berlios.de
       </A><BR>
    <I>Sat Sep 22 00:47:01 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="001537.html">[Plearn-commits] r8089 - trunk/plearn/io
</A></li>
        <LI>Next message: <A HREF="001539.html">[Plearn-commits] r8091 - trunk/python_modules/plearn/parallel
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1538">[ date ]</a>
              <a href="thread.html#1538">[ thread ]</a>
              <a href="subject.html#1538">[ subject ]</a>
              <a href="author.html#1538">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: louradou
Date: 2007-09-22 00:47:00 +0200 (Sat, 22 Sep 2007)
New Revision: 8090

Modified:
   trunk/plearn/base/stringutils.h
   trunk/plearn_learners/online/LayerCostModule.cc
   trunk/plearn_learners/online/LayerCostModule.h
Log:
LayerCostModule (a module to plug a cost function to a hidden layer):
The code was cleaned, and some functionalities were added.
NB: If someone knows how to compare properly 2 strings with PLearn, please let
me know!



Modified: trunk/plearn/base/stringutils.h
===================================================================
--- trunk/plearn/base/stringutils.h	2007-09-20 20:45:27 UTC (rev 8089)
+++ trunk/plearn/base/stringutils.h	2007-09-21 22:47:00 UTC (rev 8090)
@@ -140,9 +140,14 @@
 inline bool string_begins_with(const string&amp; s, const string&amp; beginning)
 {
     string::size_type n = beginning.size();
-    return (s.size() &gt;= n &amp;&amp; s.substr(0,n) == beginning);
+    return (s.size() &gt;= n  &amp;&amp;  beginning == s.substr(0,n-1) );
 }
-
+inline bool string_ends_with(const string&amp; s, const string&amp; end)
+{
+    string::size_type n = end.size();
+    string::size_type m = s.size();
+    return (m &gt;= n  &amp;&amp;  end == s.substr(m-n,m) );
+}
   
 //! replaces all occurences of searchstr in the text by replacestr
 //! returns the number of matches that got replaced

Modified: trunk/plearn_learners/online/LayerCostModule.cc
===================================================================
--- trunk/plearn_learners/online/LayerCostModule.cc	2007-09-20 20:45:27 UTC (rev 8089)
+++ trunk/plearn_learners/online/LayerCostModule.cc	2007-09-21 22:47:00 UTC (rev 8090)
@@ -45,25 +45,23 @@
 
 PLEARN_IMPLEMENT_OBJECT(
     LayerCostModule,
-    &quot;Computes a cost function on Layer, given:    \n&quot;,
-    &quot;* Expectations for a binomial RBM upper layer\n&quot;
-    &quot;* sigmoid(activation) for a Neural Network   \n&quot;
-    &quot;and Back-propagates the gradient.            \n&quot;
+    &quot;Computes a cost function on Layer, given:            \n&quot;,
+    &quot;* Expectations for a binomial RBM hidden layer, or   \n&quot;
+    &quot;* sigmoid(activation) for a layer of a Neural Net, or\n&quot;
+    &quot;* real outputs of any layer                          \n&quot;
+    &quot;and Back-propagates the gradient.                    \n&quot;
     &quot;\n&quot;
-    &quot;This function can be:                        \n&quot;
-    &quot;* The average Cross-Entropy                  \n&quot;
-    &quot;* The average Kullback-Leibler Divergence    \n&quot;
-    &quot;* Pascal's function...                       \n&quot;);
+    &quot;Several cost functions can be chosen.\n&quot;
+    &quot;Some only apply for binomial layers. \n&quot;);
 
 LayerCostModule::LayerCostModule():
     histo_size(10),
     alpha(0.0),
-    momentum(0.0)
+    momentum(0.0),
+    cost_function(&quot;&quot;),
+    cost_function_completename(&quot;&quot;)
 {
     output_size = 1;
-/*
-    ntest = 0;
-*/
 }
 
 void LayerCostModule::declareOptions(OptionList&amp; ol)
@@ -78,75 +76,124 @@
     declareOption(ol, &quot;cost_function&quot;, &amp;LayerCostModule::cost_function,
                   OptionBase::buildoption,
         &quot;The cost function applied to the layer:\n&quot;
-        &quot;- \&quot;stochastic_cross_entropy\&quot; [default]: average cross-entropy between pairs of binomial units\n&quot;
-        &quot;- \&quot;stochastic_kl_div\&quot;: average KL divergence between pairs of binomial units\n&quot;
-        &quot;- \&quot;kl_div\&quot;: KL divergence between distrubution of expectations (sampled with x)\n&quot;
-        &quot;- \&quot;kl_div_2\&quot;: good version of kl_div\n&quot;
-        &quot;- \&quot;kl_div_simple\&quot;: simple version of kl_div where we count at least one sample per histogram's bin\n&quot;);
+        &quot;- \&quot;pascal\&quot;:&quot;
+	                    &quot; Pascal Vincent's God given cost function.\n&quot;
+        &quot;- \&quot;correlation\&quot;:&quot;
+	                    &quot; average of a function applied to the correlations between outputs.\n&quot;
+        &quot;- \&quot;kl_div\&quot;:&quot;
+	                    &quot; KL divergence between distrubution of outputs (sampled with x)\n&quot;
+        &quot;- \&quot;kl_div_simple\&quot;:&quot;
+	                    &quot; simple version of kl_div where we count at least one sample per histogram's bin\n&quot;
+        &quot;- \&quot;stochastic_cross_entropy\&quot; [default]:&quot;
+	                    &quot; average cross-entropy between pairs of binomial units\n&quot;
+        &quot;- \&quot;stochastic_kl_div\&quot;:&quot;
+	                    &quot; average KL divergence between pairs of binomial units\n&quot;
+                 );
 
     declareOption(ol, &quot;histo_size&quot;, &amp;LayerCostModule::histo_size,
                   OptionBase::buildoption,
-        &quot;For \&quot;kl_div*\&quot; cost functions,\n&quot;
-        &quot;number of bins for the histograms (to estimate distributions of probabilities for expectations).\n&quot;
+        &quot;For \&quot;kl_div\&quot; cost functions,\n&quot;
+        &quot;number of bins for the histograms (to estimate distributions of outputs).\n&quot;
         &quot;The higher is histo_size, the more precise is the estimation.\n&quot;);
 
     declareOption(ol, &quot;alpha&quot;, &amp;LayerCostModule::alpha,
                   OptionBase::buildoption,
         &quot;(&gt;=0) For \&quot;pascal\&quot; cost function,\n&quot;
-        &quot;number of bins for the histograms (to estimate distributions of probabilities for expectations).\n&quot;
+        &quot;number of bins for the histograms (to estimate distributions of outputs).\n&quot;
         &quot;The higher is histo_size, the more precise is the estimation.\n&quot;);
 
     declareOption(ol, &quot;momentum&quot;, &amp;LayerCostModule::momentum,
                   OptionBase::buildoption,
-        &quot;(in [0,1[) For \&quot;pascal\&quot; cost function, momentum for the moving means\n&quot;);
+        &quot;(in [0,1[) For \&quot;pascal\&quot; cost function, momentum for the moving means.\n&quot;);
 
+
+
+    declareOption(ol, &quot;inputs_histo&quot;, &amp;LayerCostModule::inputs_histo,
+                  OptionBase::learntoption,
+                  &quot;Histograms (empirical ditribution) of the output, for all units.\n&quot;
+        );
+
+    declareOption(ol, &quot;inputs_expectation&quot;, &amp;LayerCostModule::inputs_expectation,
+                  OptionBase::learntoption,
+                  &quot;Expectation of the output (in [0,1[), for all units.\n&quot;
+        );
+
+    declareOption(ol, &quot;inputs_stds&quot;, &amp;LayerCostModule::inputs_stds,
+                  OptionBase::learntoption,
+                  &quot;Standard Deviation of the output, for all units.\n&quot;
+        );
+
+    declareOption(ol, &quot;inputs_correlations&quot;, &amp;LayerCostModule::inputs_correlations,
+                  OptionBase::learntoption,
+                  &quot;Correlation of the outputs, for all pairs of units.\n&quot;
+        );
+
+    declareOption(ol, &quot;inputs_cross_quadratic_mean&quot;, &amp;LayerCostModule::inputs_cross_quadratic_mean,
+                  OptionBase::learntoption,
+                  &quot;Expectation of the cross products between outputs, for all pairs of units.\n&quot;
+        );
+
+    declareOption(ol, &quot;cost_function_completename&quot;, &amp;LayerCostModule::cost_function_completename,
+                  OptionBase::learntoption,
+                  &quot;complete name of cost_function (take into account some internal settings).\n&quot;
+        );
 }
 
 void LayerCostModule::build_()
 {
-    PLASSERT( input_size &gt; 1 );
     PLASSERT( histo_size &gt; 1 );
     PLASSERT( momentum &gt;= 0.0);
     PLASSERT( momentum &lt; 1);
     
+    norm_factor = 1./(real)(input_size*(input_size-1));
+    
     string im = lowerstring( cost_function );
     // choose HERE the *default* cost function
     if( im == &quot;&quot; )
         cost_function = &quot;pascal&quot;;
     else
         cost_function = im;
+    if( ( cost_function_completename == &quot;&quot; ) || !string_ends_with(cost_function_completename, cost_function) )
+        cost_function_completename = string(cost_function);
 
      // list HERE all *stochastic* cost functions
     if( ( cost_function == &quot;stochastic_cross_entropy&quot;)
      || ( cost_function == &quot;stochastic_kl_div&quot;) )
         is_cost_function_stochastic = true;
-	
+        
     // list HERE all *non stochastic* cost functions
     // and the specific initialization
     else if( ( cost_function == &quot;kl_div&quot;)
-          || ( cost_function == &quot;kl_div_simple&quot;)
-	  || ( cost_function == &quot;kl_div_2&quot;) )
+          || ( cost_function == &quot;kl_div_simple&quot;) )
     {
         is_cost_function_stochastic = false;
-        expectations_histo.resize(input_size,histo_size);
-	LINHISTO_STEP = 1.0/(real)histo_size;
-        LOGHISTO_BASE = 10.0;
-        LOGHISTO_MIN = (real)pow(LOGHISTO_BASE,-(real)histo_size);
+        if( input_size &gt; 1 )
+            inputs_histo.resize(input_size,histo_size);
+        HISTO_STEP = 1.0/(real)histo_size;
     }
-    else if ( ( cost_function == &quot;pascal&quot;) )
+    else if( (cost_function == &quot;pascal&quot; )
+          || (cost_function == &quot;correlation&quot; ) )
     {
         is_cost_function_stochastic = false;
-	expectations_expectation.resize(input_size);
-	expectations_cross_quadratic_mean.resize(input_size,input_size);
-/*
-        expectations_expectation_testMemory.resize(input_size);
-        expectations_cross_quadratic_mean_testMemory.resize(input_size,input_size);
-*/
-	if( momentum &gt; 0.0)
-	{
-            expectations_expectation_trainMemory.resize(input_size);
-            expectations_cross_quadratic_mean_trainMemory.resize(input_size,input_size);
-	}
+        if( input_size &gt; 1 )
+        {
+            inputs_expectation.resize(input_size);
+            inputs_cross_quadratic_mean.resize(input_size,input_size);
+            if( cost_function == &quot;correlation&quot; )
+            {
+                inputs_stds.resize(input_size);
+                inputs_correlations.resize(input_size,input_size);
+            }
+            if( momentum &gt; 0.0)
+            {
+                inputs_expectation_trainMemory.resize(input_size);
+                inputs_cross_quadratic_mean_trainMemory.resize(input_size,input_size);
+            }
+            if( cost_function == &quot;pascal&quot; )
+                cost_function_completename = addprepostfix( func_pascal_prefix(), &quot;_&quot;, cost_function );
+            else if( cost_function == &quot;correlation&quot; )
+                cost_function_completename = addprepostfix( func_correlation_prefix(), &quot;_&quot;, cost_function );
+        }
     }
     else
         PLERROR(&quot;LayerCostModule::build_() does not recognize cost function %s&quot;,
@@ -155,14 +202,13 @@
     // The port story...
     ports.resize(0);
     portname_to_index.clear();
-    addPortName(&quot;expectations&quot;);
+    addPortName(&quot;input&quot;);
     addPortName(&quot;cost&quot;);
 
     port_sizes.resize(nPorts(), 2);
     port_sizes.fill(-1);
-    port_sizes(getPortIndex(&quot;expectations&quot;), 1) = input_size;
+    port_sizes(getPortIndex(&quot;input&quot;), 1) = input_size;
     port_sizes(getPortIndex(&quot;cost&quot;), 1) = 1;
-    
 }
 
 
@@ -178,15 +224,17 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
-    deepCopyField(expectations_histo, copies);
-    deepCopyField(expectations_expectation, copies);
-    deepCopyField(expectations_cross_quadratic_mean, copies);
-    deepCopyField(expectations_expectation_trainMemory, copies);
-    deepCopyField(expectations_cross_quadratic_mean_trainMemory, copies);
-/*
-    deepCopyField(expectations_expectation_testMemory, copies);
-    deepCopyField(expectations_cross_quadratic_mean_testMemory, copies);
-*/
+    deepCopyField(inputs_histo, copies);
+    
+    deepCopyField(inputs_expectation, copies);
+    deepCopyField(inputs_stds, copies);
+    
+    deepCopyField(inputs_correlations, copies);
+    deepCopyField(inputs_cross_quadratic_mean, copies);
+    
+    deepCopyField(inputs_expectation_trainMemory, copies);
+    deepCopyField(inputs_cross_quadratic_mean_trainMemory, copies);
+    
     deepCopyField(ports, copies);
 }
 
@@ -200,283 +248,211 @@
 
 void LayerCostModule::fprop(const TVec&lt;Mat*&gt;&amp; ports_value)
 {
+    PLASSERT( input_size &gt; 1 );
 
-    Mat* expectations = ports_value[getPortIndex(&quot;expectations&quot;)];
-    Mat* cost = ports_value[getPortIndex(&quot;cost&quot;)];
+    Mat* p_inputs = ports_value[getPortIndex(&quot;input&quot;)];
+    Mat* p_costs = ports_value[getPortIndex(&quot;cost&quot;)];
 
     PLASSERT( ports_value.length() == nPorts() );
 
-    if ( cost &amp;&amp; cost-&gt;isEmpty() )
+    if ( p_costs &amp;&amp; p_costs-&gt;isEmpty() )
     {
-        PLASSERT( expectations &amp;&amp; !expectations-&gt;isEmpty() );
-	cout &lt;&lt; &quot;1 regular fprop!!!&quot; &lt;&lt; endl;
-        fprop(*expectations, *cost);
+        PLASSERT( p_inputs &amp;&amp; !p_inputs-&gt;isEmpty() );
+        cout &lt;&lt; &quot;fprop&quot; &lt;&lt; endl;
+        fprop(*p_inputs, *p_costs);
     }
 }
 
-void LayerCostModule::fprop(const Mat&amp; expectations, Mat&amp; costs)
+void LayerCostModule::fprop(const Mat&amp; inputs, Mat&amp; costs)
 {
-    int batch_size = expectations.length();
-    costs.resize( batch_size, output_size );
+    int n_samples = inputs.length();
+    costs.resize( n_samples, output_size );
     
     if( !is_cost_function_stochastic )
     {
-        costs.clear();
+        costs.clear(); // costs(i,0) = 0
 
         if( cost_function == &quot;kl_div&quot; )
         {
         //! ************************************************************
         //! (non stochastic) SYMETRIC *** K-L DIVERGENCE ***
-        //! between probabilities of expectations vectors for all units
+        //! between probabilities of outputs vectors for all units
         //! ************************************************************
         //! 
-        //!      cost = - \sum_{i} \sum_{j#i} Div_{KL}[ Px(q{i}) | Px(q{j}) ]
+        //!      cost = - MEAN_{i,j#i} Div_{KL}[ Px(q{i}) | Px(q{j}) ]
         //!
-        //!           = - \sum_{i} \sum_{j#i} \sum_Q (Nx_j(Q) - Nx_j(Q)) log( Nx_i(Q) / Nx_j(Q) )
+        //!           = - MEAN_{i,j#i} SUM_Q (Nx_j(Q) - Nx_j(Q)) log( Nx_i(Q) / Nx_j(Q) )
         //!
-        //! where  q{i} = P(h{i}=1|x): expectation of the i^th units of the layer
-        //!        Px(.): empirical probability (given data x, we sample the q's)
-        //!        Q: interval in [0,1] = one bin of the histogram of the expectations q's
-        //!        Nx_i(Q): proportion of q{i} that belong to Q, with input data x
+        //! where  q{i} = P(h{i}=1|x): output of the i^th units of the layer.
+        //!        Px(.): empirical probability (given data x, we sample the q's).
+        //!        Q: interval in [0,1] = one bin of the histogram of the outputs q's.
+        //!           Q has size HISTO_STEP
+        //!        Nx_i(Q): proportion of q{i} that belong to Q, given data x.
         //!
-        //! Note: one q{i} *entirely* determines one binomial densities of probability
-        //!       ( Bijection {binomial Proba functions} &lt;-&gt; |R )
+        //! Note1: one q{i} *entirely* determines one binomial densities of probability.
+        //!        ( Bijection {binomial Proba functions} &lt;-&gt; |R )
         //!
+        //! Note2: there is a special processing for cases when
+        //!        NO outputs q{i} were observed for a given unit i
+        //!        at a given bin Q of the histograms whereas another q{j}
+        //!        has been observed in Q  (normally, KLdiv -&gt; infinity ).
+        //!        SEE function computeKLdiv().
         //! ************************************************************
 
-            // Filling the histogram (i.e. emperical distribution)
-            // of the expectations
-	    computeHisto(expectations);
-	    
+            computeHisto(inputs);
+            
+            costs(0,0) = computeKLdiv();
+        }
+        else if( cost_function == &quot;kl_div_simple&quot; )
+        {
+        //! ************************************************************
+        //! same as above with a very simple version of the KL-div:
+        //! when computing the histogram of the outputs for all units.
+        //! we add one count per histogram's bin so as to avoid
+        //! numerical problems with zeros.
+        //!
+        //! SEE function computeSafeHisto(real ).
+        //! ************************************************************
+
+            computeSafeHisto(inputs);
+            
             // Computing the KL divergence
             for (int i = 0; i &lt; input_size; i++)
                 for (int j = 0; j &lt; i; j++)
-		{
-		    // These variables are used in case one bin of 
-		    // the histogram is empty for one unit
-		    // and not for another one ( (Nj-Ni).log(Ni/Nj) = nan ).
-		    // In such case, we ''differ'' the count for the next bin and so on.
-                    real differ_count_i = 0.0;
-                    real differ_count_j = 0.0;
-		    for (int k = 0; k &lt; histo_size; k++)
-		    {
-                        real Ni_k = expectations_histo(i,k) + differ_count_i;
-			real Nj_k = expectations_histo(j,k) + differ_count_j;
-			if( fast_exact_is_equal(Ni_k, 0.0) )
-			{
-                         // differ_count_j += expectations_histo(j,k);
-                            differ_count_j = Nj_k;
-			    continue;
-			}
-                        else if( fast_exact_is_equal(Nj_k, 0.0) )
-			{
-                            differ_count_i = Ni_k;
-			    continue;
-			}
-                        else
-			{
-			    costs(0,0) += KLdivTerm(Ni_k,Nj_k);
-                            differ_count_i = 0.0;
-			    differ_count_j = 0.0;
-			}
-                    }
-		    if( differ_count_i &gt; 0.0 )
-		        &quot;cas ou on regroupe avec le dernier&quot;;
-		    else if ( differ_count_j &gt; 0.0 )
-		        &quot;cas ou on regroupe avec le dernier&quot;;		    
-                }
+                    for (int k = 0; k &lt; histo_size; k++)
+                        costs(0,0) += KLdivTerm( inputs_histo(i,k), inputs_histo(j,k));
+
             // Normalization w.r.t. number of units
-            costs(0,0) /= ((real)input_size *(real)(input_size-1)); // / (real)batch_size;
+            costs(0,0) *= norm_factor;
         }
-        else if( cost_function == &quot;kl_div_2&quot; )
+        else if( cost_function == &quot;pascal&quot; )
         {
         //! ************************************************************
-        //! (non stochastic) SYMETRIC *** K-L DIVERGENCE ***
-        //! between probabilities of expectations vectors for all units
+        //! a Pascal Vincent's god-given similarity measure
+        //! between outputs vectors for all units
         //! ************************************************************
         //! 
-        //!      cost = - \sum_{i} \sum_{j#i} Div_{KL}[ Px(q{i}) | Px(q{j}) ]
+        //!      cost = MEAN_{i,j#i} f( Ex[q{i}.q{j}] ) - alpha. MEAN_{i} f( Ex[q{i}] )
         //!
-        //!           = - \sum_{i} \sum_{j#i} \sum_Q (Nx_j(Q) - Nx_j(Q)) log( Nx_i(Q) / Nx_j(Q) )
+        //! where  q{i} = P(h{i}=1|x): output of the i^th units of the layer
+        //!        Ex(.): empirical expectation (given data x)
         //!
-        //! where  q{i} = P(h{i}=1|x): expectation of the i^th units of the layer
-        //!        Px(.): empirical probability (given data x, we sample the q's)
-        //!        Q: interval in [0,1] = one bin of the histogram of the expectations q's
-        //!        Nx_i(Q): proportion of q{i} that belong to Q, with input data x
-        //!
-        //! Note: one q{i} *entirely* determines one binomial densities of probability
-        //!       ( Bijection {binomial Proba functions} &lt;-&gt; |R )
-        //!
         //! ************************************************************
 
-            // Filling the histogram (i.e. emperical distribution)
-            // of the expectations
-	    computeHisto(expectations);
-	    
-            // Computing the KL divergence
+            computePascalStatistics(inputs);
+                                    
+            // Computing the cost
             for (int i = 0; i &lt; input_size; i++)
+            {
+                if (alpha &gt; 0.0 )
+                    costs(0,0) -= alpha * func_pascal(inputs_expectation[i]) *(real)(input_size-1);
                 for (int j = 0; j &lt; i; j++)
-		{
-		    // These variables are used in case one bin of 
-		    // the histogram is empty for one unit
-		    // and not for another one ( (Nj-Ni).log(Ni/Nj) = nan ).
-		    // In such case, we ''differ'' the count for the next bin and so on.
-                    real differ_count_i = 0.0;
-                    real differ_count_j = 0.0;
-		    int n_differ = 0;
-		    for (int k = 0; k &lt; histo_size; k++)
-		    {
-                        real Ni_k = expectations_histo(i,k) + differ_count_i;
-			real Nj_k = expectations_histo(j,k) + differ_count_j;
-			if( fast_exact_is_equal(Ni_k, 0.0) )
-			{
-                         // differ_count_j += expectations_histo(j,k);
-                            differ_count_j = Nj_k;
-			    n_differ += 1;
-			}
-                        else if( fast_exact_is_equal(Nj_k, 0.0) )
-			{
-                            differ_count_i = Ni_k;
-			    n_differ += 1;
-			}
-                        else
-			{
-			    costs(0,0) += KLdivTerm(Ni_k,Nj_k) *(real)(1+n_differ)/(real)histo_size;
-                            differ_count_i = 0.0;
-			    differ_count_j = 0.0;
-			}
-                    }
-		    if( differ_count_i &gt; 0.0 )
-		        &quot;cas ou on regroupe avec le dernier&quot;;
-		    else if ( differ_count_j &gt; 0.0 )
-		        &quot;cas ou on regroupe avec le dernier&quot;;		    
-                }
-            // Normalization w.r.t. number of units
-            costs(0,0) /= ((real)input_size *(real)(input_size-1)); // / (real)batch_size;
+                    costs(0,0) += func_pascal(inputs_cross_quadratic_mean(i,j));
+            }
+
+            costs(0,0) *= norm_factor;
         }
-        else if( cost_function == &quot;kl_div_simple&quot; )
+        else if( cost_function == &quot;correlation&quot; )
         {
-            // Filling the histogram (i.e. emperical distribution)
-            // of the expectations
-	    computeSafeHisto(expectations);
-	    
-            // Computing the KL divergence
-            for (int i = 0; i &lt; input_size; i++)
-                for (int j = 0; j &lt; i; j++)
-		    for (int k = 0; k &lt; histo_size; k++)
-		    {
-                        real Ni_k = expectations_histo(i,k);
-			real Nj_k = expectations_histo(j,k);
-			costs(0,0) += KLdivTerm(Ni_k,Nj_k);
-                    }
-            // Normalization w.r.t. number of units
-            costs(0,0) /= ((real)input_size *(real)(input_size-1)); // / (real)batch_size;
-        }
-        else if( cost_function == &quot;pascal&quot; )
-        {
         //! ************************************************************
-        //! a god-given similarity measure
-        //! between expectations vectors for all units
+        //! a correlation measure
+        //! between outputs for all units
         //! ************************************************************
-        //! 
-        //!      cost = \sum_{i} \sum_{j#i} exp( Ex[q{i}.q{j}] ) - alpha. \sum_{i} exp( Ex[q{i}] )
         //!
-        //! where  q{i} = P(h{i}=1|x): expectation of the i^th units of the layer
-        //!        Ex(.): empirical esperance (given data x, we sample the q's)
+        //!                            ( Ex[q{i}.q{j}] - Ex[q{i}]Ex[q{j}] )
+        //!      cost = MEAN_{i,j#i} f(  -------------------------------- )
+        //!                           (      StDx(q{i}) * StDx(q{j})     )
         //!
+        //! where  q{i} = P(h{i}=1|x): output of the i^th units of the layer
+        //!        Ex(.): empirical esperance (given data x)
+        //!        StDx(.): empirical standard deviation (given data x)
+        //!
         //! ************************************************************
 
-            // Computing statistics on expectations
-	    computePascalStatistics(expectations, false);
-	    
-	    cout &lt;&lt; &quot;1 fprop&quot; &lt;&lt; endl;
-	    	    
+            computeCorrelationStatistics(inputs);
+                        
             // Computing the cost
             for (int i = 0; i &lt; input_size; i++)
-	    {
-	        if (alpha &gt; 0.0 )
-		    costs(0,0) -= alpha*exp(expectations_expectation[i]);
                 for (int j = 0; j &lt; i; j++)
-                    costs(0,0) += exp(expectations_cross_quadratic_mean(i,j)) / (real)(input_size-1);
-            }
+                    costs(0,0) += func_correlation( inputs_correlations(i,j) );
 
-            // Normalization w.r.t. number of units
-            costs(0,0) /= (real)input_size;
+            costs(0,0) *= norm_factor;
         }
-	
+
+        
         return; // Do not fprop with the conventional stochastic fprop...
     }
     
-    for (int isample = 0; isample &lt; batch_size; isample++)
-        fprop(expectations(isample), costs(isample,0));
+    for (int isample = 0; isample &lt; n_samples; isample++)
+        fprop(inputs(isample), costs(isample,0));
 }
 
-void LayerCostModule::fprop(const Vec&amp; expectation, real&amp; cost) const
+void LayerCostModule::fprop(const Vec&amp; input, real&amp; cost) const
 {
-    PLASSERT( expectation.size() == input_size );
+    PLASSERT( input.size() == input_size );
     PLASSERT( is_cost_function_stochastic );
 
     cost = 0.0;
-    real  qi, qj, comp_qi, comp_qj; // The expectations qi=p(h_i=1)
-                                      //     and some basic operations on it (e.g.: 1-qi, qi/(1-qi)) 
-				      
+    real  qi, qj, comp_qi, comp_qj; // The outputs (units i,j)
+                                    // and some basic operations on it (e.g.: 1-qi, qi/(1-qi)) 
+                                      
     if( cost_function == &quot;stochastic_cross_entropy&quot; )
     {
     //! ************************************************************
     //! average *** CROSS ENTROPY ***
-    //! between pairs of units (given expectations = sigmoid(act) )
+    //! between pairs of units (given output = sigmoid(act) )
     //! ************************************************************
     //!
-    //!      cost = - \sum_{i} \sum_{j#i} CrossEntropy[( P(h_{i}|x) | P(h_{j}|x) )]
+    //!      cost = - MEAN_{i,j#i} CrossEntropy[( P(h_{i}|x) | P(h_{j}|x) )]
     //!
-    //!           = - \sum_{i} \sum_{j#i} [ q{i}.log(q{j}) + (1-q{i}).log(1-q{j}) ]
+    //!           = - MEAN_{i,j#i} [ q{i}.log(q{j}) + (1-q{i}).log(1-q{j}) ]
     //!
     //! where |  h_{i}: i^th units of the layer
     //!       \  P(.|x): output for input data x
-    //!        \ q{i}=P(h{i}=1|v): expectation of the i^th units of the layer
+    //!        \ q{i}=P(h{i}=1|v): output of the i^th units of the layer
     //!
     //! ************************************************************
 
         for( int i = 0; i &lt; input_size; i++ )
         {
-           qi = expectation[i];
+           qi = input[i];
            comp_qi = 1.0 - qi;
            for( int j = 0; j &lt; i; j++ )
            {
-               qj = expectation[j];
+               qj = input[j];
                comp_qj = 1.0 - qj;
-	       
+               
                // H(pi||pj) = H(pi) + D_{KL}(pi||pj)
                cost += qi*safeflog(qj) + comp_qi*safeflog(comp_qj);
-	       
+               
                // The symetric part (loop  j=i+1...size)
                cost += qj*safeflog(qi) + comp_qj*safeflog(comp_qi);
            }
         }
         // Normalization w.r.t. number of units
-        cost /= ((real)input_size *(real)(input_size-1));
+        cost *= norm_factor;
     }
     
     else if( cost_function == &quot;stochastic_kl_div&quot; )
     {
     //! ************************************************************
     //! average SYMETRIC *** K-L DIVERGENCE ***
-    //! between pairs of units (given expectations = sigmoid(act) )
+    //! between pairs of units (given outputs = sigmoid(act) )
     //! ************************************************************
     //!
-    //!      cost = - \sum_{i} \sum_{j#i} Div_{KL} [( P(h_{i}|v) | P(h_{j}|v) )]
+    //!      cost = - MEAN_{i,j#i} Div_{KL} [( P(h_{i}|v) | P(h_{j}|v) )]
     //!
-    //!           = - \sum_{i} \sum_{j#i} [ ( q{j} - q{i} ) log( q{i}/(1-q{i})*(1-q{j})/q{j} ) ]
+    //!           = - MEAN_{i,j#i} [ ( q{j} - q{i} ) log( q{i}/(1-q{i})*(1-q{j})/q{j} ) ]
     //!
     //! where |  h_{i}: i^th units of the layer
     //!       \  P(.|v):  output for input data x
-    //!        \ q{i}=P(h{i}=1|v): expectation of the i^th units of the layer
+    //!        \ q{i}=P(h{i}=1|v): output of the i^th units of the layer
     //!
     //! ************************************************************
 
         for( int i = 0; i &lt; input_size; i++ )
         {
-           qi = expectation[i];
+           qi = input[i];
            if(fast_exact_is_equal(qi, 1.0))
                comp_qi = REAL_MAX;
            else
@@ -484,25 +460,26 @@
        
            for( int j = 0; j &lt; i; j++ )
            {
-               qj = expectation[j];
+               qj = input[j];
                if(fast_exact_is_equal(qj, 1.0))
                    comp_qj = REAL_MAX;
                else
                    comp_qj = qj/(1.0 - qj);
-	       
+               
                //     - D_{KL}(pi||pj) - D_{KL}(pj||pi)
                cost += (qj-qi)*safeflog(comp_qi/comp_qj);
            }
         }
         // Normalization w.r.t. number of units
-        cost /= ((real)input_size *(real)(input_size-1));   
+        cost *= norm_factor;   
     }
 
     else
-        PLERROR(&quot;LayerCostModule::fprop() not implemented for cost function %s\n&quot;
-	        &quot;- It may be a printing error\n&quot;
-		&quot;- You can try to call LayerCostModule::fprop(const Mat&amp; expectations, Mat&amp; costs)\n&quot;
-		&quot;- Or else write the code corresponding to your cost function&quot;,
+        PLERROR(&quot;LayerCostModule::fprop() not implemented for cost_cfunction %s\n&quot;
+                &quot;- It may be a printing error.\n&quot;
+                &quot;- You can try to call LayerCostModule::fprop(const Mat&amp; inputs, Mat&amp; costs)&quot;
+                &quot;  if your cost function is non stochastic.\n&quot;
+                &quot;- Or else write the code corresponding to your cost function.\n&quot;,
                  cost_function.c_str());
 }
 
@@ -517,74 +494,75 @@
 void LayerCostModule::bpropAccUpdate(const TVec&lt;Mat*&gt;&amp; ports_value,
                                    const TVec&lt;Mat*&gt;&amp; ports_gradient)
 {
+    PLASSERT( input_size &gt; 1 );
     PLASSERT( ports_value.length() == nPorts() );
     PLASSERT( ports_gradient.length() == nPorts() );
 
-    const Mat* expectations = ports_value[getPortIndex(&quot;expectations&quot;)];
-    Mat* expectations_grad = ports_gradient[getPortIndex(&quot;expectations&quot;)];
-    Mat* cost = ports_value[getPortIndex(&quot;cost&quot;)];
-    Mat* cost_grad = ports_gradient[getPortIndex(&quot;cost&quot;)];
+    cout &lt;&lt; &quot;bpropAccUpdate&quot; &lt;&lt; endl;
 
-    if( expectations_grad &amp;&amp; expectations_grad-&gt;isEmpty()
-        &amp;&amp; cost_grad &amp;&amp; !cost_grad-&gt;isEmpty() )
+    const Mat* p_inputs = ports_value[getPortIndex(&quot;input&quot;)];
+    Mat* p_inputs_grad = ports_gradient[getPortIndex(&quot;input&quot;)];
+    Mat* p_cost_grad = ports_gradient[getPortIndex(&quot;cost&quot;)];
+
+    if( p_inputs_grad &amp;&amp; p_inputs_grad-&gt;isEmpty()
+        &amp;&amp; p_cost_grad &amp;&amp; !p_cost_grad-&gt;isEmpty() )
     {
-        int batch_size = expectations-&gt;length();
+        int n_samples = p_inputs-&gt;length();
 
-        PLASSERT( expectations &amp;&amp; !expectations-&gt;isEmpty());
-        PLASSERT( expectations-&gt;length() == batch_size );
-        PLASSERT( cost_grad-&gt;length() == batch_size );
+        PLASSERT( p_inputs &amp;&amp; !p_inputs-&gt;isEmpty());
+        PLASSERT( p_inputs-&gt;length() == n_samples );
+        PLASSERT( p_cost_grad-&gt;length() == n_samples );
 
-        expectations_grad-&gt;resize(batch_size, input_size);
-	expectations_grad-&gt;clear();
+        p_inputs_grad-&gt;resize(n_samples, input_size);
+        p_inputs_grad-&gt;clear();
 
         real qi, qj, comp_qi, comp_qj;
         Vec comp_q(input_size), log_term(input_size);
 
         if( cost_function == &quot;stochastic_cross_entropy&quot; )
         {
-            for (int isample = 0; isample &lt; batch_size; isample++)
+            for (int isample = 0; isample &lt; n_samples; isample++)
             {
-        	for (int i = 0 ; i &lt; input_size ; i++ )
+                for (int i = 0 ; i &lt; input_size ; i++ )
                 {
-                    qi = (*expectations)(isample,i);
-            	    comp_qi = 1.0 - qi;
+                    qi = (*p_inputs)(isample,i);
+                        comp_qi = 1.0 - qi;
                     comp_q[i] = comp_qi;
                     log_term[i] = safeflog(qi) - safeflog(comp_qi);
                 }
                 for (int i = 0; i &lt; input_size; i++ )
                 {
-                    qi = (*expectations)(isample,i);
+                    qi = (*p_inputs)(isample,i);
                     comp_qi = comp_q[i];
-                    (*expectations_grad)(isample,i) = 0.0;
+                    (*p_inputs_grad)(isample,i) = 0.0;
                     for (int j = 0; j &lt; i; j++ )
                     {
-                        qj = (*expectations)(isample,j);
+                        qj = (*p_inputs)(isample,j);
                         comp_qj=comp_q[j];
 
                         // log(pj) - log(1-pj) + pj/pi - (1-pj)/(1-pi)
-                        (*expectations_grad)(isample,i) += log_term[j] + qj/qi - comp_qi/comp_qj;
+                        (*p_inputs_grad)(isample,i) += log_term[j] + qj/qi - comp_qi/comp_qj;
 
                         // The symetric part (loop  j=i+1...input_size)
-                        (*expectations_grad)(isample,j) += log_term[i] + qi/qj - comp_qj/comp_qi;
+                        (*p_inputs_grad)(isample,j) += log_term[i] + qi/qj - comp_qj/comp_qi;
                     }
                 }
-                // Normalization
-                real norm_factor = 1.0 / ((real)input_size *(real)(input_size-1));
                 for (int i = 0; i &lt; input_size; i++ )
                 {
-                    (*expectations_grad)(isample, i) *= (*cost_grad)(isample,0) * norm_factor;
+                    (*p_inputs_grad)(isample, i) *= (*p_cost_grad)(isample,0)
+		                                    * norm_factor /(real)n_samples;
                 }
             }
         }
 
         else if( cost_function == &quot;stochastic_kl_div&quot; )
         {
-            for (int isample = 0; isample &lt; batch_size; isample++)
+            for (int isample = 0; isample &lt; n_samples; isample++)
             {
-        	for (int i = 0; i &lt; input_size; i++ )
+                for (int i = 0; i &lt; input_size; i++ )
                 {
-                    qi = (*expectations)(isample,i);
-            	    comp_qi = 1.0 - qi;
+                    qi = (*p_inputs)(isample,i);
+                        comp_qi = 1.0 - qi;
                     if(fast_exact_is_equal(qi, 1.0) || fast_exact_is_equal(qi, 0.0))
                         comp_q[i] = REAL_MAX;
                     else
@@ -593,263 +571,273 @@
                 }
                 for (int i = 0; i &lt; input_size; i++ )
                 {
-                    qi = (*expectations)(isample,i);
+                    qi = (*p_inputs)(isample,i);
                     comp_qi = comp_q[i];
 
-                    (*expectations_grad)(isample,i) = 0.0;
+                    (*p_inputs_grad)(isample,i) = 0.0;
                     for (int j = 0; j &lt; i ; j++ )
                     {
-                        qj = (*expectations)(isample,j);
+                        qj = (*p_inputs)(isample,j);
                         comp_qj=comp_q[j];
 
                         //   [qj - qi]/[qi (1-qi)] - log[ qi/(1-qi) * (1-qj)/qj]
-                        (*expectations_grad)(isample,i) += (qj - qi)*comp_qi - log_term[i] + log_term[j];
+                        (*p_inputs_grad)(isample,i) += (qj - qi)*comp_qi - log_term[i] + log_term[j];
 
                         // The symetric part (loop  j=i+1...input_size)
-                        (*expectations_grad)(isample,j) += (qi - qj)*comp_qj - log_term[j] + log_term[i];
+                        (*p_inputs_grad)(isample,j) += (qi - qj)*comp_qj - log_term[j] + log_term[i];
                     }
                 }
-                // Normalization
-                real norm_factor = 1.0 / ((real)input_size *(real)(input_size-1));
                 for (int i = 0; i &lt; input_size; i++ )
                 {
-                    (*expectations_grad)(isample, i) *= (*cost_grad)(isample,0) * norm_factor;
+                    (*p_inputs_grad)(isample, i) *= (*p_cost_grad)(isample,0)
+		                                     * norm_factor /(real)n_samples;
                 }
             }
         }
 
-
         else if( cost_function == &quot;kl_div&quot; )
         {
-	    computeHisto(*expectations);
-	    real one_count = 1. / (real)batch_size;
-	    
-            for (int isample = 0; isample &lt; batch_size; isample++)
+            computeHisto(*p_inputs);
+            
+            for (int isample = 0; isample &lt; n_samples; isample++)
             {
 
                 // Computing the difference of KL divergence
                 // for d_q
                 for (int i = 0; i &lt; input_size; i++)
-		{
-                    (*expectations_grad)(isample, i) = 0.0;
-		    
-		    qi = (*expectations)(isample,i);
-		    int index_i = histo_index(qi);
-		    if( ( index_i == histo_size-1 ) ) // we do not care about this...
-		        continue;
-		    real over_dqi=1.0/dq(qi);
-		    int shift_i;
-		    if( over_dqi &gt; 0.0)
-		        shift_i = 1;
-		    else
-		        shift_i = -1;
-		    // qi + dq(qi) ==&gt; | expectations_histo(i,index_i)   - one_count
-		    //                 \ expectations_histo(i,index_i+shift_i) + one_count
-		    
-		    for (int j = 0; j &lt; i; j++)
-		    {
-			(*expectations_grad)(isample, i) += delta_KLdivTerm(i, j, index_i, over_dqi, one_count);
-			
-                        qj = (*expectations)(isample,j);
-			int index_j = histo_index(qj);
-		        if( ( index_j == histo_size-1 ) || ( index_j == 0 ) )
-		            continue;
-			real over_dqj=1.0/dq(qj);
- 		        int shift_j;
-		        if( over_dqj &gt; 0.0)
-		            shift_j = 1;
-		        else
-		            shift_j = -1;
-            	        // qj + dq(qj) ==&gt; | expectations_histo(j,index_j)   - one_count
-  		        //                 \ expectations_histo(j,index_j+shift_j) + one_count
-			
-			(*expectations_grad)(isample, j) += delta_KLdivTerm(j, i, index_j, over_dqj, one_count);
+                {
+                    (*p_inputs_grad)(isample, i) = 0.0;
+                    
+                    qi = (*p_inputs)(isample,i);
+                    int index_i = histo_index(qi);
+                    if( ( index_i == histo_size-1 ) ) // we do not care about this...
+                        continue;
+                    real over_dqi=1.0/dq(qi);
+                    int shift_i;
+                    if( over_dqi &gt; 0.0)
+                        shift_i = 1;
+                    else
+                        shift_i = -1;
+                    // qi + dq(qi) ==&gt; | p_inputs_histo(i,index_i)   - one_count
+                    //                 \ p_inputs_histo(i,index_i+shift_i) + one_count
+                    
+                    for (int j = 0; j &lt; i; j++)
+                    {
+                        (*p_inputs_grad)(isample, i) += delta_KLdivTerm(i, j, index_i, over_dqi);
+                        
+                        qj = (*p_inputs)(isample,j);
+                        int index_j = histo_index(qj);
+                        if( ( index_j == histo_size-1 ) || ( index_j == 0 ) )
+                            continue;
+                        real over_dqj=1.0/dq(qj);
+                         int shift_j;
+                        if( over_dqj &gt; 0.0)
+                            shift_j = 1;
+                        else
+                            shift_j = -1;
+                            // qj + dq(qj) ==&gt; | p_inputs_histo(j,index_j)   - one_count
+                          //                 \ p_inputs_histo(j,index_j+shift_j) + one_count
+                        
+                        (*p_inputs_grad)(isample, j) += delta_KLdivTerm(j, i, index_j, over_dqj);
                     }
-		}
+                }
 
                 // Normalization
-                real norm_factor = 1.0 / ((real)input_size *(real)(input_size-1));
                 for (int i = 0; i &lt; input_size; i++ )
                 {
-                    (*expectations_grad)(isample, i) *= (*cost_grad)(isample,0) * norm_factor;
+                    (*p_inputs_grad)(isample, i) *= (*p_cost_grad)(isample,0) * norm_factor;
                 }
             }
-        }
-
-        else if( cost_function == &quot;kl_div_2&quot; )
-        {
-	    computeHisto(*expectations);
-	    real one_count = 1. / (real)batch_size;
-	    
-            for (int isample = 0; isample &lt; batch_size; isample++)
+            
+            
+            // debug Check
+            int i=0;
+            real cost_before = computeKLdiv();
+            for (int isample = 0; isample &lt; n_samples; isample++)
             {
-
-                // Computing the difference of KL divergence
-                // for d_q
-                for (int i = 0; i &lt; input_size; i++)
-		{
-                    (*expectations_grad)(isample, i) = 0.0;
-		    
-		    qi = (*expectations)(isample,i);
-		    int index_i = histo_index(qi);
-		    if( ( index_i == histo_size-1 ) ) // we do not care about this...
-		        continue;
-		    real over_dqi=1.0/dq(qi);
-		    int shift_i;
-		    if( over_dqi &gt; 0.0)
-		        shift_i = 1;
-		    else
-		        shift_i = -1;
-		    // qi + dq(qi) ==&gt; | expectations_histo(i,index_i)   - one_count
-		    //                 \ expectations_histo(i,index_i+shift_i) + one_count
-		    
-		    for (int j = 0; j &lt; i; j++)
-		    {
-			(*expectations_grad)(isample, i) += delta_KLdivTerm_2(i, j, index_i, over_dqi, one_count);
-			
-                        qj = (*expectations)(isample,j);
-			int index_j = histo_index(qj);
-		        if( ( index_j == histo_size-1 ) || ( index_j == 0 ) )
-		            continue;
-			real over_dqj=1.0/dq(qj);
- 		        int shift_j;
-		        if( over_dqj &gt; 0.0)
-		            shift_j = 1;
-		        else
-		            shift_j = -1;
-            	        // qj + dq(qj) ==&gt; | expectations_histo(j,index_j)   - one_count
-  		        //                 \ expectations_histo(j,index_j+shift_j) + one_count
-			
-			(*expectations_grad)(isample, j) += delta_KLdivTerm_2(j, i, index_j, over_dqj, one_count);
-                    }
-		}
-
-                // Normalization
-                real norm_factor = 1.0 / ((real)input_size *(real)(input_size-1));
-                for (int i = 0; i &lt; input_size; i++ )
+                real qi=(*p_inputs)(isample,i);
+                if( histo_index(qi) &lt; histo_size-1 )
                 {
-                    (*expectations_grad)(isample, i) *= (*cost_grad)(isample,0) * norm_factor;
+                  (*p_inputs)(isample,i) += dq(qi);
+                  computeHisto(*p_inputs);
+                  real cost_after = computeKLdiv();
+                  (*p_inputs)(isample,i) -= dq(qi);                  
+                  cout &lt;&lt; &quot;\tglobal cost comparison:&quot; &lt;&lt; cost_after - cost_before;
+                  cout &lt;&lt; &quot;  &lt;?&gt;  &quot; &lt;&lt; (*p_inputs_grad)(isample, i)*dq(qi) &lt;&lt; endl;
                 }
             }
+            
+            
         }
 
         else if( cost_function == &quot;kl_div_simple&quot; )
         {
-	    computeSafeHisto(*expectations);
-	    real one_count = 1. / (real)(batch_size+histo_size);
-	    
-            for (int isample = 0; isample &lt; batch_size; isample++)
+            computeSafeHisto(*p_inputs);
+            
+            for (int isample = 0; isample &lt; n_samples; isample++)
             {
 
                 // Computing the difference of KL divergence
                 // for d_q
                 for (int i = 0; i &lt; input_size; i++)
-		{
-                    (*expectations_grad)(isample, i) = 0.0;
-		    
-		    qi = (*expectations)(isample,i);
-		    int index_i = histo_index(qi);
-		    if( ( index_i == histo_size-1 ) ) // we do not care about this...
-		        continue;
-		    real over_dqi=1.0/dq(qi);
-		    int shift_i;
-		    if( over_dqi &gt; 0.0)
-		        shift_i = 1;
-		    else
-		        shift_i = -1;
-		    // qi + dq(qi) ==&gt; | expectations_histo(i,index_i)   - one_count
-		    //                 \ expectations_histo(i,index_i+shift_i) + one_count
-		    
-		    for (int j = 0; j &lt; i; j++)
-		    {
-			(*expectations_grad)(isample, i) += delta_SafeKLdivTerm(i, j, index_i, over_dqi, one_count);
-			
-                        qj = (*expectations)(isample,j);
-			int index_j = histo_index(qj);
-		        if( ( index_j == histo_size-1 ) || ( index_j == 0 ) )
-		            continue;
-			real over_dqj=1.0/dq(qj);
- 		        int shift_j;
-		        if( over_dqj &gt; 0.0)
-		            shift_j = 1;
-		        else
-		            shift_j = -1;
-            	        // qj + dq(qj) ==&gt; | expectations_histo(j,index_j)   - one_count
-  		        //                 \ expectations_histo(j,index_j+shift_j) + one_count
-			
-			(*expectations_grad)(isample, j) += delta_SafeKLdivTerm(j, i, index_j, over_dqj, one_count);
+                {
+                    (*p_inputs_grad)(isample, i) = 0.0;
+                    
+                    qi = (*p_inputs)(isample,i);
+                    int index_i = histo_index(qi);
+                    if( ( index_i == histo_size-1 ) ) // we do not care about this...
+                        continue;
+                    real over_dqi=1.0/dq(qi);
+                    int shift_i;
+                    if( over_dqi &gt; 0.0)
+                        shift_i = 1;
+                    else
+                        shift_i = -1;
+                    // qi + dq(qi) ==&gt; | p_inputs_histo(i,index_i)   - one_count
+                    //                 \ p_inputs_histo(i,index_i+shift_i) + one_count
+                    
+                    for (int j = 0; j &lt; i; j++)
+                    {
+                        (*p_inputs_grad)(isample, i) += delta_SafeKLdivTerm(i, j, index_i, over_dqi);
+                        
+                        qj = (*p_inputs)(isample,j);
+                        int index_j = histo_index(qj);
+                        if( ( index_j == histo_size-1 ) || ( index_j == 0 ) )
+                            continue;
+                        real over_dqj=1.0/dq(qj);
+                         int shift_j;
+                        if( over_dqj &gt; 0.0)
+                            shift_j = 1;
+                        else
+                            shift_j = -1;
+                            // qj + dq(qj) ==&gt; | p_inputs_histo(j,index_j)   - one_count
+                          //                 \ p_inputs_histo(j,index_j+shift_j) + one_count
+                        
+                        (*p_inputs_grad)(isample, j) += delta_SafeKLdivTerm(j, i, index_j, over_dqj);
                     }
-		}
+                }
 
                 // Normalization
-                real norm_factor = 1.0 / ((real)input_size *(real)(input_size-1));
                 for (int i = 0; i &lt; input_size; i++ )
                 {
-                    (*expectations_grad)(isample, i) *= (*cost_grad)(isample,0) * norm_factor;
+                    (*p_inputs_grad)(isample, i) *= (*p_cost_grad)(isample,0) * norm_factor;
                 }
             }
         }
 
         else if( cost_function == &quot;pascal&quot; )
         {
-	    computePascalStatistics(*expectations, true);
-	    
-	    cout &lt;&lt; &quot;1 BPropAccUpdate&quot; &lt;&lt; endl;
-	    
-	    real one_count = 1. / (real)batch_size;
-	    if( momentum &gt; 0.0 )
-	        for (int isample = 0; isample &lt; batch_size; isample++)
-		{
+            computePascalStatistics(*p_inputs);
+            
+            if( momentum &gt; 0.0 )
+                for (int isample = 0; isample &lt; n_samples; isample++)
+                {
                     for (int i = 0; i &lt; input_size; i++)
                     {
-                        qi = (*expectations)(isample, i);
-			if (alpha &gt; 0.0 )
-			    (*expectations_grad)(isample, i) -= alpha*exp(expectations_expectation[i]) *(1.0-momentum)*one_count;
+                        qi = (*p_inputs)(isample, i);
+                        if (alpha &gt; 0.0 )
+                            (*p_inputs_grad)(isample, i) -= alpha*deriv_func_pascal(inputs_expectation[i])
+			                                         *(1.0-momentum) *one_count
+                                                                 *(real)(input_size-1);
                         for (int j = 0; j &lt; i; j++)
                         {
-			    qj = (*expectations)(isample,j);
-                            (*expectations_grad)(isample, i) += exp(expectations_cross_quadratic_mean(i,j)) *qj*(1.0-momentum)*one_count / (real)(input_size-1);
-                            (*expectations_grad)(isample, j) += exp(expectations_cross_quadratic_mean(i,j)) *qi*(1.0-momentum)*one_count / (real)(input_size-1);
+                            real d_temp = deriv_func_pascal(inputs_cross_quadratic_mean(i,j));
+                            qj = (*p_inputs)(isample,j);
+                            (*p_inputs_grad)(isample, i) += d_temp *qj*(1.0-momentum)*one_count;
+                            (*p_inputs_grad)(isample, j) += d_temp *qi*(1.0-momentum)*one_count;
                         }
                     }
                     for (int i = 0; i &lt; input_size; i++)
                     {
-	                (*expectations_grad)(isample, i) /= (real)input_size;
-	            }
-		}
-	    else
-	        for (int isample = 0; isample &lt; batch_size; isample++)
-		{
+                        (*p_inputs_grad)(isample, i) *= norm_factor;
+                    }
+                }
+            else
+                for (int isample = 0; isample &lt; n_samples; isample++)
+                {
                     for (int i = 0; i &lt; input_size; i++)
                     {
-                        qi = (*expectations)(isample, i);
-			if (alpha &gt; 0.0 )
-			    (*expectations_grad)(isample, i) -= alpha*exp(expectations_expectation[i]) *one_count;
+                        qi = (*p_inputs)(isample, i);
+                        if (alpha &gt; 0.0 )
+                            (*p_inputs_grad)(isample, i) -= alpha*deriv_func_pascal(inputs_expectation[i])
+			                                         *one_count
+                                                                 *(real)(input_size-1);
                         for (int j = 0; j &lt; i; j++)
                         {
-			    qj = (*expectations)(isample,j);
-                            (*expectations_grad)(isample, i) += exp(expectations_cross_quadratic_mean(i,j)) *qj*one_count / (real)(input_size-1);
-                            (*expectations_grad)(isample, j) += exp(expectations_cross_quadratic_mean(i,j)) *qi*one_count / (real)(input_size-1);
+                            real d_temp = deriv_func_pascal(inputs_cross_quadratic_mean(i,j));
+                            qj = (*p_inputs)(isample,j);
+                            (*p_inputs_grad)(isample, i) += d_temp *qj *one_count;
+                            (*p_inputs_grad)(isample, j) += d_temp *qi *one_count;
                         }
                     }
                     for (int i = 0; i &lt; input_size; i++)
                     {
-	                (*expectations_grad)(isample, i) /= (real)input_size;
-	            }
-		}
+                        (*p_inputs_grad)(isample, i) *= norm_factor;
+                    }
+                }
         }
-	
+
+        else if( cost_function == &quot;correlation&quot;)
+        {
+            computeCorrelationStatistics(*p_inputs);
+            
+            if( momentum &gt; 0.0 )
+                PLERROR( &quot;not implemented yet&quot;);
+            else
+                for (int isample = 0; isample &lt; n_samples; isample++)
+                {
+                    Vec dSTDi_dqi, dCROSSij_dqj;
+                    dSTDi_dqi.resize( input_size );
+                    dCROSSij_dqj.resize( input_size );
+                    
+                    for (int i = 0; i &lt; input_size; i++)
+                    {
+                        qi = (*p_inputs)(isample, i);
+
+                        //!  dCROSSij_dqj[i] = d[ E(QiQj)-E(Qi)E(Qj) ]/d[qj(t)]
+                        //!                  = ( qi(t) - E(Qi) ) / n_samples 
+                        //!
+                        //!  dSTDi_dqi[i] = d[ STD(Qi) ]/d[qi(t)]
+                        //!               = d[ sqrt( E(Qi^2) -E(Qi)^2 ]/d[qi(t)]
+                        //!               = 1 / [ 2.STD(Qi) ] * d[ E(Qi^2) -E(Qi)^2 ]/d[qi(t)]
+                        //!               = 1 / [ 2.STD(Qi) ] * [ 2*qi(t) / n_samples - 2*E(Qi) / n_samples ]
+                        //!               = ( qi(t) - E(Qi) ) / ( n_samples * STD(Qi) )
+                        //!               = dCROSSij_dqj[i] / STD(Qi)
+                        //!
+                        dCROSSij_dqj[i] = ( qi - inputs_expectation[i] )*one_count;
+                        dSTDi_dqi[i] = dCROSSij_dqj[i] / inputs_stds[i];
+                        
+                        for (int j = 0; j &lt; i; j++)
+                        {
+                            qj = (*p_inputs)(isample,j);
+
+                            real correlation_denum = inputs_stds[i]*inputs_stds[j];
+                            real dfunc_dCorr = deriv_func_correlation( inputs_correlations(i,j) );
+                            real correlation_num = ( inputs_cross_quadratic_mean(i,j)
+                                                     - inputs_expectation[i]*inputs_expectation[j] );
+                                  
+                            (*p_inputs_grad)(isample, i) += dfunc_dCorr * ( 
+                                                                    correlation_denum * dCROSSij_dqj[j]
+                                                                  - correlation_num * dSTDi_dqi[i] * inputs_stds[j]
+                                                                    ) / (correlation_denum * correlation_denum);
+
+                            (*p_inputs_grad)(isample, j) += dfunc_dCorr * ( 
+                                                                    correlation_denum * dCROSSij_dqj[i]
+                                                                  - correlation_num * dSTDi_dqi[j] * inputs_stds[i]
+                                                                    ) / (correlation_denum * correlation_denum);
+                        }
+                    }
+                    for (int i = 0; i &lt; input_size; i++)
+                        (*p_inputs_grad)(isample, i) *= norm_factor;
+                }
+        }
         else
             PLERROR(&quot;LayerCostModule::bpropAccUpdate() not implemented for cost function %s&quot;,
                      cost_function.c_str());
 
-/*
-        ntest = 0;
-*/
-
         checkProp(ports_gradient);
     }
-    else if( !expectations_grad &amp;&amp; !cost_grad )
+    else if( !p_inputs_grad &amp;&amp; !p_cost_grad )
         return;
     else
         PLERROR(&quot;In LayerCostModule::bpropAccUpdate - Port configuration not implemented &quot;);
@@ -860,189 +848,150 @@
 ////////////////////////////////////////////////////
 // Auxiliary Functions for Pascal's cost function //
 ////////////////////////////////////////////////////
-void LayerCostModule::computePascalStatistics(const Mat&amp; expectations, bool duringTraining)
+void LayerCostModule::computePascalStatistics(const Mat&amp; inputs)
 {
-    int batch_size = expectations.length();
-    real one_count = 1. / (real)batch_size;
-    Vec expectation;
+    int n_samples = inputs.length();
+    one_count = 1. / (real)n_samples;
+    Vec input;
     
-    expectations_expectation.clear(); 
-    expectations_cross_quadratic_mean.clear(); 
+    inputs_expectation.clear(); 
+    inputs_cross_quadratic_mean.clear(); 
 
-    for (int isample = 0; isample &lt; batch_size; isample++)
+    for (int isample = 0; isample &lt; n_samples; isample++)
     {
-        expectation = expectations(isample);
+        input = inputs(isample);
         for (int i = 0; i &lt; input_size; i++)
-	{
-	    expectations_expectation[i] += expectation[i];
-	    for (int j = 0; j &lt; i; j++)
-                 expectations_cross_quadratic_mean(i,j) += expectation[i] * expectation[j];
+        {
+            inputs_expectation[i] += input[i];
+            for (int j = 0; j &lt; i; j++)
+                 inputs_cross_quadratic_mean(i,j) += input[i] * input[j];
         }
     }
-    expectations_cross_quadratic_mean *= one_count;
     
     for (int i = 0; i &lt; input_size; i++)
     {
-        expectations_expectation[i] *= one_count;
+        inputs_expectation[i] *= one_count;
         for (int j = 0; j &lt; i; j++)
         {
-             expectations_cross_quadratic_mean(i,j) *= one_count;
-//    	     expectations_cross_quadratic_mean(j,i) = expectations_cross_quadratic_mean(i,j);
+             inputs_cross_quadratic_mean(i,j) *= one_count;
         }
     }
-    if( ( momentum &gt; 0.0 ) &amp;&amp; duringTraining )
+    if( ( momentum &gt; 0.0 ) &amp;&amp; during_training )
     {
         for (int i = 0; i &lt; input_size; i++)
         {
-	    if(i == 0)
-	       cout &lt;&lt; &quot;.Check momentum....: expectations_expectation_trainMemory[0] = &quot; &lt;&lt; expectations_expectation_trainMemory[0] &lt;&lt; endl;
-
-            expectations_expectation[i] = momentum*expectations_expectation_trainMemory[i]
-	                                 +(1.0-momentum)*expectations_expectation[i];
-            expectations_expectation_trainMemory[i] = expectations_expectation[i];
+            inputs_expectation[i] = momentum*inputs_expectation_trainMemory[i]
+                                         +(1.0-momentum)*inputs_expectation[i];
+            inputs_expectation_trainMemory[i] = inputs_expectation[i];
             for (int j = 0; j &lt; i; j++)
             {
-                 expectations_cross_quadratic_mean(i,j) = momentum*expectations_cross_quadratic_mean_trainMemory(i,j)
-		                                       +(1.0-momentum)*expectations_cross_quadratic_mean(i,j);
-//        	 expectations_cross_quadratic_mean(j,i) = expectations_cross_quadratic_mean(i,j);
-        	 expectations_cross_quadratic_mean_trainMemory(i,j) = expectations_cross_quadratic_mean(i,j);
-//        	 expectations_cross_quadratic_mean_trainMemory(j,i) = expectations_cross_quadratic_mean(i,j);
+                 inputs_cross_quadratic_mean(i,j) = momentum*inputs_cross_quadratic_mean_trainMemory(i,j)
+                                                       +(1.0-momentum)*inputs_cross_quadratic_mean(i,j);
+                 inputs_cross_quadratic_mean_trainMemory(i,j) = inputs_cross_quadratic_mean(i,j);
             }
         }
     }
-/*    else if( !duringTraining )
-    {
-	PLASSERT( ntest+batch_size &gt; 0 );
-	for (int i = 0; i &lt; input_size; i++)
-        {
-            expectations_expectation[i] = ( (real)ntest*expectations_expectation_testMemory[i]
-	                                   +(real)batch_size*expectations_expectation[i] )/(real)(ntest+batch_size);
-            expectations_expectation_testMemory[i] = expectations_expectation[i];
-            for (int j = 0; j &lt; i; j++)
-            {
-                 expectations_cross_quadratic_mean(i,j) = ( (real)ntest*expectations_cross_quadratic_mean_testMemory(i,j)
-		                                           +(real)batch_size*expectations_cross_quadratic_mean(i,j) );
-        	 expectations_cross_quadratic_mean(j,i) = expectations_cross_quadratic_mean(i,j);
-        	 expectations_cross_quadratic_mean_testMemory(i,j) = expectations_cross_quadratic_mean(i,j);
-        	 expectations_cross_quadratic_mean_testMemory(j,i) = expectations_cross_quadratic_mean(i,j);
-            }
-        }
-	ntest += batch_size;
-    }
-*/
 }
-
-/////////////////////////
-// Auxiliary Functions //
-/////////////////////////
-real LayerCostModule::delta_KLdivTerm(int i, int j, int index_i, real over_dq, real one_count)
+string LayerCostModule::func_pascal_prefix()
 {
-    PLASSERT( over_dq &gt; 0.0 );
+    string prefix = &quot;exp&quot;;
+    return prefix;
+}
+real LayerCostModule::func_pascal(real value)
+{
+    return exp(value);
+}
+real LayerCostModule::deriv_func_pascal(real value)
+{
+    return exp(value);
+}
 
-    real grad_update = 0.0;
 
-    real Ni_ki = expectations_histo(i,index_i);
-    real Ni_ki_shift1 = expectations_histo(i,index_i+1);	    
-    real Nj_ki        = expectations_histo(j,index_i);
-    real Nj_ki_shift1 = expectations_histo(j,index_i+1);
+void LayerCostModule::computeCorrelationStatistics(const Mat&amp; inputs)
+{
+    int n_samples = inputs.length();
+    one_count = 1. / (real)n_samples;
+    Vec input;
+    
+    inputs_expectation.clear();
+    inputs_cross_quadratic_mean.clear(); 
+    inputs_correlations.clear();
 
-    PLASSERT( !fast_exact_is_equal(Ni_ki, 0.0) ); // Verification:
-                                                  // if expectations_histo is up to date,
-                                                  // the expectation(isample,i) has been counted
-    real differ_count_j_before = 0.0;
-    real differ_count_j_after = 0.0;
-    real differ_count_i_before = 0.0;
-    real differ_count_i_after = 0.0;
-
-    // What follows is only valuable when the qi's are increased (dq&gt;0).
-
-    if( !fast_exact_is_equal(Nj_ki, 0.0) )
-    // if it is zero, then INCREASING qi will not change anything
-    // (it was already counted in the next histograms's bin
+    for (int isample = 0; isample &lt; n_samples; isample++)
     {
-        // removing the term of the sum that will be modified
-        grad_update -= KLdivTerm( Ni_ki, Nj_ki );
-							       
-        if( fast_exact_is_equal(Ni_ki, one_count) )
-            differ_count_j_after = Nj_ki;
-        else
-            // adding the term of the sum with its modified value
-            grad_update += KLdivTerm( Ni_ki-one_count, Nj_ki );
-
-        if( !fast_exact_is_equal(Nj_ki_shift1,0.0) )
+        input = inputs(isample);
+        for (int i = 0; i &lt; input_size; i++)
         {
-            // adding the term of the sum with its modified value
-            grad_update += KLdivTerm( Ni_ki_shift1+one_count, Nj_ki_shift1+differ_count_j_after );
+            inputs_expectation[i] += input[i];
+            inputs_cross_quadratic_mean(i,i) += input[i] * input[i];
+            for (int j = 0; j &lt; i; j++)
+                 inputs_cross_quadratic_mean(i,j) += input[i] * input[j];
+        }
+    }
+    
+    for (int i = 0; i &lt; input_size; i++)
+    {
+        //! Normalization
+        inputs_expectation[i] *= one_count;
+        inputs_cross_quadratic_mean(i,i) *= one_count;
 
-            if( !fast_exact_is_equal(Ni_ki_shift1, 0.0) ) // &quot;cas o&#249; on regroupe avec le dernier&quot;;
-            {
-                // removing the term of the sum that will be modified
-                grad_update -= KLdivTerm( Ni_ki_shift1, Nj_ki_shift1 );		
-            }
-	    else
-	    {
-	        // We search   ki' &gt; k(i)+1   such that   n(i,ki') &gt; 0
-		differ_count_j_before = Nj_ki_shift1;
-		int ki;
-		for (ki = index_i+2; ki &lt; histo_size; ki++)
-		{
-		    differ_count_j_before += expectations_histo( j, ki );
-		    if( expectations_histo( i, ki )&gt;0 )
-		        break;
-		}
-		if( ki &lt; histo_size )
-		{
-                    grad_update -= KLdivTerm( expectations_histo( i, ki ), differ_count_j_before );
+            inputs_stds[i] = sqrt( inputs_cross_quadratic_mean(i,i)
+                              - inputs_expectation[i] * inputs_expectation[i] );
 
-		    if( differ_count_j_before &gt; Nj_ki_shift1 )		
-                        grad_update += KLdivTerm( expectations_histo( i, ki ), differ_count_j_before - Nj_ki_shift1 );
-		        // pb avec differ_count_j_after plus haut??? semble pas
-		}
-		else
-		    &quot;cas o&#249; on regroupe avec le dernier&quot;;
-	    }
-        }
-        else
+        for (int j = 0; j &lt; i; j++)
         {
-            differ_count_i_before = Ni_ki_shift1;
-            differ_count_i_after  = Ni_ki_shift1+one_count;
-	    int kj;
-	    for( kj = index_i+2; kj &lt; histo_size; kj++)
-	    {
-	        differ_count_i_after += expectations_histo( i, kj );
-		if( differ_count_i_before &gt; 0 )
-		    differ_count_i_before += expectations_histo( i, kj );
-		if( expectations_histo( j, kj ) &gt; 0 )
-		    break;
-	    }
-	    if( kj &lt; histo_size )
-            {
-                grad_update += KLdivTerm( differ_count_i_after, expectations_histo( j, kj ) );
+            //! Normalization
+            inputs_cross_quadratic_mean(i,j) *= one_count;
 
-		if( differ_count_i_before &gt; 0 )
-                    grad_update -= KLdivTerm( differ_count_i_before, expectations_histo( j, kj ) );
-	    }
-	    else
-		&quot;cas o&#249; on regroupe avec le dernier&quot;;   
+            //! Correlations
+            inputs_correlations(i,j) = (
+                                  inputs_cross_quadratic_mean(i,j)
+                                  - inputs_expectation[i]*inputs_expectation[j]
+                                  ) / ( inputs_stds[i] * inputs_stds[j] );
         }
     }
-    return grad_update *over_dq;
+    //! Be careful: 'inputs_correlations' matrix is only computed
+    //!  on the triangle subpart 'i' &gt; 'j' 
+    //!  ('i'/'j': first/second argument)
+
+    if(  during_training )
+    {
+        if( momentum &gt; 0.0 )
+            PLERROR(&quot;not implemented yet&quot;);
+    }
 }
+string LayerCostModule::func_correlation_prefix()
+{
+    string prefix = &quot;squared&quot;;
+    return &quot;square&quot;;
+}
+real LayerCostModule::func_correlation(real correlation)
+{
+    return correlation * correlation;
+}
+real LayerCostModule::deriv_func_correlation(real correlation)
+{
+    return 2 * correlation;
+}
+/////////////////////////
+// Auxiliary Functions //
+/////////////////////////
 
-real LayerCostModule::delta_KLdivTerm_2(int i, int j, int index_i, real over_dq, real one_count)
+
+real LayerCostModule::delta_KLdivTerm(int i, int j, int index_i, real over_dq)
 {
     PLASSERT( over_dq &gt; 0.0 );
 
     real grad_update = 0.0;
 
-    real Ni_ki = expectations_histo(i,index_i);
-    real Ni_ki_shift1 = expectations_histo(i,index_i+1);	    
-    real Nj_ki        = expectations_histo(j,index_i);
-    real Nj_ki_shift1 = expectations_histo(j,index_i+1);
+    real Ni_ki = inputs_histo(i,index_i);
+    real Ni_ki_shift1 = inputs_histo(i,index_i+1);            
+    real Nj_ki        = inputs_histo(j,index_i);
+    real Nj_ki_shift1 = inputs_histo(j,index_i+1);
 
     PLASSERT( !fast_exact_is_equal(Ni_ki, 0.0) ); // Verification:
-                                                  // if expectations_histo is up to date,
-                                                  // the expectation(isample,i) has been counted
+                                                  // if inputs_histo is up to date,
+                                                  // the input(isample,i) has been counted
     real differ_count_j_before = 0.0;
     real differ_count_j_after = 0.0;
     real differ_count_i_before = 0.0;
@@ -1060,108 +1009,115 @@
     {
         // removing the term of the sum that will be modified
         grad_update -= KLdivTerm( Ni_ki, Nj_ki ) *over_dq;
-							       
+                                                               
         if( fast_exact_is_equal(Ni_ki, one_count) )
-	{
+        {
             differ_count_j_after = Nj_ki;
-	    n_differ_j_after += 1;
-	}
+            n_differ_j_after += 1;
+        }
         else
             // adding the term of the sum with its modified value
-            grad_update += KLdivTerm( Ni_ki-one_count, Nj_ki ) *over_dq;
+            grad_update += KLdivTerm( Ni_ki-one_count, Nj_ki )
+	                   *over_dq;
 
         if( !fast_exact_is_equal(Nj_ki_shift1,0.0) )
         {
             // adding the term of the sum with its modified value
-            grad_update += KLdivTerm( Ni_ki_shift1+one_count, Nj_ki_shift1+differ_count_j_after ) *(real)(n_differ_j_after+1)*over_dq ;
+            grad_update += KLdivTerm( Ni_ki_shift1+one_count, Nj_ki_shift1+differ_count_j_after )
+	                  *(real)(1+n_differ_j_after)*over_dq ;
 
-            if( !fast_exact_is_equal(Ni_ki_shift1, 0.0) ) // &quot;cas o&#249; on regroupe avec le dernier&quot;;
+            if( !fast_exact_is_equal(Ni_ki_shift1, 0.0) ) // &quot;cas ou on regroupe avec le dernier&quot;;
             {
                 // removing the term of the sum that will be modified
-                grad_update -= KLdivTerm( Ni_ki_shift1, Nj_ki_shift1 )*over_dq;		
+                grad_update -= KLdivTerm( Ni_ki_shift1, Nj_ki_shift1 )
+		               *over_dq;                
             }
-	    else
-	    {
-	        // We search   ki' &gt; k(i)+1   such that   n(i,ki') &gt; 0
-		differ_count_j_before = Nj_ki_shift1;
+            else
+            {
+                // We search   ki' &gt; k(i)+1   such that   n(i,ki') &gt; 0
+                differ_count_j_before = Nj_ki_shift1;
                 n_differ_j_before += 1;
-		int ki;
-		for (ki = index_i+2; ki &lt; histo_size; ki++)
-		{
-		    differ_count_j_before += expectations_histo( j, ki );
-		    if( expectations_histo( i, ki )&gt;0 )
-		        break;
+                int ki;
+                for (ki = index_i+2; ki &lt; histo_size; ki++)
+                {
+                    differ_count_j_before += inputs_histo( j, ki );
+                    if( inputs_histo( i, ki )&gt;0 )
+                        break;
                     n_differ_j_before += 1;
-		}
-		if( ki &lt; histo_size )
-		{
-                    grad_update -= KLdivTerm( expectations_histo( i, ki ), differ_count_j_before )*(real)(1+n_differ_j_before)*over_dq;
+                }
+                if( ki &lt; histo_size )
+                {
+                    grad_update -= KLdivTerm( inputs_histo( i, ki ), differ_count_j_before )
+		                   *(real)(1+n_differ_j_before)*over_dq;
 
-		    if( differ_count_j_before &gt; Nj_ki_shift1 )		
-                        grad_update += KLdivTerm( expectations_histo( i, ki ), differ_count_j_before - Nj_ki_shift1 )*(real)(n_differ_j_before)*over_dq;
-		        // pb avec differ_count_j_after plus haut??? semble pas
-		}
-		else
-		    &quot;cas o&#249; on regroupe avec le dernier&quot;;
-	    }
+                    if( differ_count_j_before &gt; Nj_ki_shift1 )                
+                        grad_update += KLdivTerm( inputs_histo( i, ki ), differ_count_j_before - Nj_ki_shift1 )
+			               *(real)(n_differ_j_before)*over_dq;
+                        // pb avec differ_count_j_after plus haut??? semble pas
+                }
+                else
+                    &quot;cas ou on regroupe avec le dernier (easy)&quot;;
+            }
         }
         else
         {
             differ_count_i_before = Ni_ki_shift1;
-	    if( differ_count_i_before&gt;0.0 )
-	       n_differ_i_before += 1;
+            if( differ_count_i_before&gt;0.0 )
+               n_differ_i_before += 1;
             differ_count_i_after  = Ni_ki_shift1+one_count;
-	    n_differ_i_after += 1;
-	    int kj;
-	    for( kj = index_i+2; kj &lt; histo_size; kj++)
-	    {
-	        differ_count_i_after += expectations_histo( i, kj );
-		if( differ_count_i_before &gt; 0 )
-		    differ_count_i_before += expectations_histo( i, kj );
-		if( expectations_histo( j, kj ) &gt; 0 )
-		    break;
-		n_differ_i_after += 1;
-		if( differ_count_i_before &gt; 0 )
-		    n_differ_i_before += 1;
-	    }
-	    if( kj &lt; histo_size )
+            n_differ_i_after += 1;
+            int kj;
+            for( kj = index_i+2; kj &lt; histo_size; kj++)
             {
-                grad_update += KLdivTerm( differ_count_i_after, expectations_histo( j, kj ) ) *(real)(n_differ_i_after+1)*over_dq;
+                differ_count_i_after += inputs_histo( i, kj );
+                if( differ_count_i_before &gt; 0 )
+                    differ_count_i_before += inputs_histo( i, kj );
+                if( inputs_histo( j, kj ) &gt; 0 )
+                    break;
+                n_differ_i_after += 1;
+                if( differ_count_i_before &gt; 0 )
+                    n_differ_i_before += 1;
+            }
+            if( kj &lt; histo_size )
+            {
+                grad_update += KLdivTerm( differ_count_i_after, inputs_histo( j, kj ) )
+		               *(real)(1+n_differ_i_after)*over_dq;
 
-		if( differ_count_i_before &gt; 0 )
-                    grad_update -= KLdivTerm( differ_count_i_before, expectations_histo( j, kj ) ) *(real)(n_differ_i_before+1)*over_dq;
-	    }
-	    else
-		&quot;cas o&#249; on regroupe avec le dernier&quot;;   
+                if( differ_count_i_before &gt; 0 )
+                    grad_update -= KLdivTerm( differ_count_i_before, inputs_histo( j, kj ) )
+		                   *(real)(1+n_differ_i_before)*over_dq;
+            }
+            else
+                &quot;cas ou on regroupe avec le dernier&quot;;   
         }
     }
-    return grad_update;
+    return grad_update*over_dq;
 }
 
-real LayerCostModule::delta_SafeKLdivTerm(int i, int j, int index_i, real over_dq, real one_count)
+real LayerCostModule::delta_SafeKLdivTerm(int i, int j, int index_i, real over_dq)
 {
     //PLASSERT( over_dq &gt; 0.0 )
 
     real grad_update = 0.0;
-		    
-    real Ni_ki = expectations_histo(i,index_i);
+                    
+    real Ni_ki = inputs_histo(i,index_i);
     PLASSERT( !fast_exact_is_equal(Ni_ki, 0.0) ); // Verification:
-                                                  // if expectations_histo is up to date,
-                                                  // the expectation(isample,i) has been counted
-    real Ni_ki_shift1 = expectations_histo(i,index_i+1);
-		    
-    real Nj_ki        = expectations_histo(j,index_i);
-    real Nj_ki_shift1 = expectations_histo(j,index_i+1);
+                                                  // if inputs_histo is up to date,
+                                                  // the input(isample,i) has been counted
+    real Ni_ki_shift1 = inputs_histo(i,index_i+1);
+                    
+    real Nj_ki        = inputs_histo(j,index_i);
+    real Nj_ki_shift1 = inputs_histo(j,index_i+1);
 
 
         // removing the term of the sum that will be modified
         grad_update -= KLdivTerm( Ni_ki, Nj_ki );
-							       
+                                                               
         // adding the term of the sum with its modified value
         grad_update += KLdivTerm( Ni_ki-one_count, Nj_ki );
 
         grad_update += KLdivTerm( Ni_ki_shift1+one_count, Nj_ki_shift1 );
-	
+        
         grad_update -= KLdivTerm( Ni_ki_shift1, Nj_ki_shift1 );
 
     return grad_update *over_dq;
@@ -1173,46 +1129,99 @@
     return ( pj - pi ) * safeflog( pi/pj );
 }
 
-void LayerCostModule::computeHisto(const Mat&amp; expectations)
+real LayerCostModule::computeKLdiv()
 {
-    int index, batch_size = expectations.length();
-    real one_count = 1. / (real)batch_size;
-    Vec expectation;
+            real cost = 0;
+            for (int i = 0; i &lt; input_size; i++)
+                for (int j = 0; j &lt; i; j++)
+                {
+                    // These variables are used in case one bin of 
+                    // the histogram is empty for one unit
+                    // and not for another one ( (Nj-Ni).log(Ni/Nj) = nan ).
+                    // In such case, we ''differ'' the count for the next bin and so on.
+                    real differ_count_i = 0.0;
+                    real differ_count_j = 0.0;
+                    int n_differ = 0;
+                    real last_positive_Ni_k, last_positive_Nj_k;
+                    int last_n_differ;
+                    for (int k = 0; k &lt; histo_size; k++)
+                    {
+                        real Ni_k = inputs_histo(i,k) + differ_count_i;
+                        real Nj_k = inputs_histo(j,k) + differ_count_j;
+                        if( fast_exact_is_equal(Ni_k, 0.0) )
+                        {
+                         // differ_count_j += inputs_histo(j,k);
+                            differ_count_j = Nj_k;
+                            n_differ += 1;
+                        }
+                        else if( fast_exact_is_equal(Nj_k, 0.0) )
+                        {
+                            differ_count_i = Ni_k;
+                            n_differ += 1;
+                        }
+                        else
+                        {
+                            cost += KLdivTerm(Ni_k,Nj_k) *(real)(1+n_differ) *HISTO_STEP;
+                            differ_count_i = 0.0;
+                            differ_count_j = 0.0;
+                            n_differ = 0;
+                            last_positive_Ni_k = Ni_k;
+                            last_positive_Nj_k = Nj_k;
+                            last_n_differ = n_differ;
+                        }
+                    }
+                    if( differ_count_i &gt; 0.0 )
+                    {   
+                        &quot;cas ou on regroupe avec le dernier&quot;;   
+//                        cost -= KLdivTerm(last_positive_Ni_k,last_positive_Nj_k)
+//                                  *(real)(1+last_n_differ) *HISTO_STEP;
+//                        cost += KLdivTerm(last_positive_Ni_k+differ_count_i,last_positive_Nj_k)
+//                                 *(real)(1+last_n_differ+n_differ) *HISTO_STEP; 
+                    }
+                     
+                    else if ( differ_count_j &gt; 0.0 )
+                    {
+                        &quot;cas ou on regroupe avec le dernier&quot;;
+//                        cost -= KLdivTerm(last_positive_Ni_k,last_positive_Nj_k)
+//                                 *(real)(1+last_n_differ) *HISTO_STEP;
+//                        cost += KLdivTerm(last_positive_Ni_k,last_positive_Nj_k+differ_count_j)
+//                                 *(real)(1+last_n_differ+n_differ) *HISTO_STEP;
+                    }    
+                }
+            // Normalization w.r.t. number of units
+            return cost *norm_factor;
+}
+
+void LayerCostModule::computeHisto(const Mat&amp; inputs)
+{
+    int n_samples = inputs.length();
+    one_count = 1. / (real)n_samples;
+    Vec input;
     
-    expectations_histo.clear(); 
-    for (int isample = 0; isample &lt; batch_size; isample++)
+    inputs_histo.clear(); 
+    for (int isample = 0; isample &lt; n_samples; isample++)
     {
-        expectation = expectations(isample);
+        input = inputs(isample);
         for (int i = 0; i &lt; input_size; i++)
-        {
-	    index = histo_index(expectation[i]);
-            expectations_histo(i,index) += one_count;
-        }
+            inputs_histo(i, histo_index(input[i]) ) += one_count;
     }
 }
 
 
 
-void LayerCostModule::computeSafeHisto(const Mat&amp; expectations)
+void LayerCostModule::computeSafeHisto(const Mat&amp; inputs)
 {
-    int index, batch_size = expectations.length();
-    real one_count = 1. / (real)(batch_size+histo_size);
-    Vec expectation;
+    int n_samples = inputs.length();
+    one_count = 1. / (real)(n_samples+histo_size);
+    Vec input;
     
-    expectations_histo.fill(one_count);
-/*
-    for (int k = 0; k &lt; histo_size; k++)
-        for (int i = 0; i &lt; input_size; i++)
-            expectations_histo(i,k) = one_count;
-*/
-    for (int isample = 0; isample &lt; batch_size; isample++)
+    inputs_histo.fill(one_count);
+
+    for (int isample = 0; isample &lt; n_samples; isample++)
     {
-        expectation = expectations(isample);
+        input = inputs(isample);
         for (int i = 0; i &lt; input_size; i++)
-        {
-	    index = histo_index(expectation[i]);
-            expectations_histo(i,index) += one_count;
-        }
+            inputs_histo(i, histo_index(input[i])) += one_count;
     }
 }
 
@@ -1222,17 +1231,13 @@
 //
 int LayerCostModule::histo_index(real q)
 {
-    if( q &gt;=1.0 )
-       return histo_size-1;
+    PLASSERT( (q &gt;= 0.) &amp;&amp; (q &lt; 1.) );
 
-    if( !(q &gt;= 0.0) || !(q &lt; 1.0) )
-        PLERROR(&quot;LayerCostModule detected an anormal expectation value (%f)&quot;, q);
+    if( fast_exact_is_equal( q, 1. ) )
+       return histo_size - 1;
 
 // LINEAR SCALE
     return (int)floor(q*(real)histo_size);
-
-// LOG SCALE
-    return max(  (int)floor(log(LOGHISTO_BASE, q))+histo_size , 0);
 }
 
 // Returns the minimum amount dq which have to be added/removed to q
@@ -1244,54 +1249,38 @@
 //
 real LayerCostModule::dq(real q)
 {
-// LINEAR SCALE
     // ** Simple version **
-    return LINHISTO_STEP;
+    return HISTO_STEP;
 
     // ** Elaborated version **
-    if( fast_exact_is_equal( round(q*(real)histo_size) , ceil(q*(real)histo_size) ) )
-       return LINHISTO_STEP;
-    else
-       return -LINHISTO_STEP;
+    //if( fast_exact_is_equal( round(q*(real)histo_size) , ceil(q*(real)histo_size) ) )
+    //   return HISTO_STEP;
+    //else
+    //   return -HISTO_STEP;
 
     // ** BAD VERSION: too unstable **
     // return (real)histo_index(q+1.0/(real)histo_size)/(real)histo_size - q;
-
-// LOG SCALE
-    // ** Simple version **
-    //if( q &lt; LOGHISTO_MIN )
-    //  return LOGHISTO_BASE * LOGHISTO_MIN - q;
-    //return q*(LOGHISTO_BASE-1.0);
-    
-    // ** BAD VERSION: too unstable **
-    // real REF = LOGHISTO_BASE * LOGHISTO_MIN;
-    // while( true )
-    // {
-    //     if( q &lt; REF )
-    //         return REF - q;
-    //     REF *= LOGHISTO_BASE;
-    // }
 }
 
 
-
-
-
 ////////////
 // forget //
 ////////////
 void LayerCostModule::forget()
 {
+    inputs_histo.clear();
+
+    inputs_expectation.clear();
+    inputs_stds.clear();
+    
+    inputs_correlations.clear();
+    inputs_cross_quadratic_mean.clear();
     if( momentum &gt; 0.0)
     {
-        expectations_expectation_trainMemory.clear();
-        expectations_cross_quadratic_mean_trainMemory.clear();
+        inputs_expectation_trainMemory.clear();
+        inputs_cross_quadratic_mean_trainMemory.clear();
     }
-/*
-    expectations_expectation_testMemory.clear();
-    expectations_cross_quadratic_mean_testMemory.clear();
-    ntest = 0;
-*/
+    one_count = 0.;
 }
 
 

Modified: trunk/plearn_learners/online/LayerCostModule.h
===================================================================
--- trunk/plearn_learners/online/LayerCostModule.h	2007-09-20 20:45:27 UTC (rev 8089)
+++ trunk/plearn_learners/online/LayerCostModule.h	2007-09-21 22:47:00 UTC (rev 8090)
@@ -46,7 +46,7 @@
 namespace PLearn {
 
 /**
- * Computes a cost function for a (hidden) representation, given two &quot;expectation&quot; vectors. Backpropagates it.
+ * Computes a cost function for a (hidden) representation. Backpropagates it.
  */
 class LayerCostModule : public OnlineLearningModule
 {
@@ -62,17 +62,34 @@
     real alpha;
     
     real momentum;
+
+    //#####  Public Learnt Options  ###########################################
+
+    //! Histograms of inputs (estimated empiricially on some data)
+    //! Computed only when cost_function == 'kl_div' or 'kl_div_simpe'
+    Mat inputs_histo;
+
+    //! Statistics on inputs (estimated empiricially on some data)
+    //! Computed only when cost_function == 'correlation'
+    //! or (for some) 'pascal'
+    Vec inputs_expectation;
+    Vec inputs_stds;         //! only for 'correlation' cost function
     
+    Mat inputs_correlations; //! only for 'correlation' cost function
+    Mat inputs_cross_quadratic_mean;
+
+    //! The generic name of the cost function
+    string cost_function_completename;
+    
 public:
     //#####  Public Member Functions  #########################################
 
     //! Default constructor
     LayerCostModule();
 
-
     //! given the input and target, compute the cost
-    virtual void fprop(const Vec&amp; expectation, real&amp; cost) const;
-    virtual void fprop(const Mat&amp; expectations, Mat&amp; costs);
+    virtual void fprop(const Vec&amp; input, real&amp; cost) const;
+    virtual void fprop(const Mat&amp; inputs, Mat&amp; costs);
     //! Overridden.
     virtual void fprop(const TVec&lt;Mat*&gt;&amp; ports_value);
     
@@ -81,18 +98,27 @@
                                 const TVec&lt;Mat*&gt;&amp; ports_gradient);
 
     //! Some auxiliary function to deal with empirical histograms
-    virtual void computeHisto(const Mat&amp; expectations);
-    virtual void computeSafeHisto(const Mat&amp; expectations);
-    virtual real delta_KLdivTerm(int i, int j, int index_i, real over_dq, real one_count);
-    virtual real delta_KLdivTerm_2(int i, int j, int index_i, real over_dq, real one_count);
-    virtual real delta_SafeKLdivTerm(int i, int j, int index_i, real over_dq, real one_count);
+    virtual void computeHisto(const Mat&amp; inputs);
+    virtual void computeSafeHisto(const Mat&amp; inputs);
+    virtual real delta_KLdivTerm(int i, int j, int index_i, real over_dq);
+    virtual real delta_SafeKLdivTerm(int i, int j, int index_i, real over_dq);
     virtual real KLdivTerm(real pi, real pj);
+    virtual real computeKLdiv();
     virtual int histo_index(real q);
     virtual real dq(real q);
 
     //! Auxiliary function for the pascal's cost function
-    virtual void computePascalStatistics(const Mat&amp; expectations, bool duringTraining);
+    virtual void computePascalStatistics(const Mat&amp; inputs);
+    virtual string func_pascal_prefix();
+    virtual real   func_pascal(real correlation);
+    virtual real   deriv_func_pascal(real correlation);
 
+    //! Auxiliary function for the correlation's cost function
+    virtual void computeCorrelationStatistics(const Mat&amp; inputs);
+    virtual string func_correlation_prefix();
+    virtual real   func_correlation(real correlation);
+    virtual real   deriv_func_correlation(real correlation);
+
     //! Overridden to do nothing (in particular, no warning).
     virtual void setLearningRate(real dynamic_learning_rate) {}
 
@@ -127,23 +153,23 @@
     //! Does stochastic gradient makes sense with our cost function?
     bool is_cost_function_stochastic;
 
-    //! Histograms of expectations (estimated empiricially on the data)
-    Mat expectations_histo;
+    //! Normalizing factor applied to the cost function
+    //! to take into acount the number of weights
+    real norm_factor;
 
-    //! Some features of the histogram of expectations
-    real LINHISTO_STEP;
-    real LOGHISTO_BASE;
-    real LOGHISTO_MIN;
+    //! Variables for (non stochastic) KL Div cost function
+    //! ---------------------------------------------------
+    //! Range of a histogram's bin ( HISTO_STEP = 1/histo_size )
+    real HISTO_STEP;
+    //! the weight of a sample within a batch (usually, 1/n_samples)
+    real one_count; 
 
-    //! Statistics on matrix of expectations (estimated empiricially on the data)
-    Vec expectations_expectation;
-    Mat expectations_cross_quadratic_mean;
-    Vec expectations_expectation_trainMemory;
-    Mat expectations_cross_quadratic_mean_trainMemory;
-    Vec expectations_expectation_testMemory;
-    Mat expectations_cross_quadratic_mean_testMemory;
-    int ntest;
-
+    //! Variables for (non stochastic) Pascal's/correlation function
+    //! -------------------------------------------------------------
+    //! Statistics on outputs (estimated empiricially on the data)    
+    Vec inputs_expectation_trainMemory;
+    Mat inputs_cross_quadratic_mean_trainMemory;
+        
     //! Map from a port name to its index in the 'ports' vector.
     map&lt;string, int&gt; portname_to_index;
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001537.html">[Plearn-commits] r8089 - trunk/plearn/io
</A></li>
	<LI>Next message: <A HREF="001539.html">[Plearn-commits] r8091 - trunk/python_modules/plearn/parallel
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1538">[ date ]</a>
              <a href="thread.html#1538">[ thread ]</a>
              <a href="subject.html#1538">[ subject ]</a>
              <a href="author.html#1538">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
