<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r8056 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-September/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8056%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200709072207.l87M7Y2L012230%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001503.html">
   <LINK REL="Next"  HREF="001505.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r8056 - trunk/plearn_learners/online</H1>
    <B>larocheh at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8056%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200709072207.l87M7Y2L012230%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r8056 - trunk/plearn_learners/online">larocheh at mail.berlios.de
       </A><BR>
    <I>Sat Sep  8 00:07:34 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="001503.html">[Plearn-commits] r8055 - in trunk: commands commands/EXPERIMENTAL	doc scripts/EXPERIMENTAL
</A></li>
        <LI>Next message: <A HREF="001505.html">[Plearn-commits] r8057 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1504">[ date ]</a>
              <a href="thread.html#1504">[ thread ]</a>
              <a href="subject.html#1504">[ subject ]</a>
              <a href="author.html#1504">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: larocheh
Date: 2007-09-08 00:07:33 +0200 (Sat, 08 Sep 2007)
New Revision: 8056

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
Log:
Added decrease constant option and corrected partially supervised option.


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-09-07 21:14:30 UTC (rev 8055)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-09-07 22:07:33 UTC (rev 8056)
@@ -58,6 +58,7 @@
 ///////////////////
 DeepBeliefNet::DeepBeliefNet() :
     cd_learning_rate( 0. ),
+    cd_decrease_ct( 0. ),
     grad_learning_rate( 0. ),
     batch_size( 1 ),
     grad_decrease_ct( 0. ),
@@ -97,6 +98,11 @@
                   &quot;The learning rate used during contrastive divergence&quot;
                   &quot; learning&quot;);
 
+    declareOption(ol, &quot;cd_decrease_ct&quot;, &amp;DeepBeliefNet::cd_decrease_ct,
+                  OptionBase::buildoption,
+                  &quot;The decrease constant of the learning rate used during&quot;
+                  &quot; contrastive divergence&quot;);
+
     declareOption(ol, &quot;grad_learning_rate&quot;, &amp;DeepBeliefNet::grad_learning_rate,
                   OptionBase::buildoption,
                   &quot;The learning rate used during gradient descent&quot;);
@@ -104,7 +110,7 @@
     declareOption(ol, &quot;grad_decrease_ct&quot;, &amp;DeepBeliefNet::grad_decrease_ct,
                   OptionBase::buildoption,
                   &quot;The decrease constant of the learning rate used during&quot;
-                  &quot;gradient descent&quot;);
+                  &quot; gradient descent&quot;);
 
     declareOption(ol, &quot;batch_size&quot;, &amp;DeepBeliefNet::batch_size,
                   OptionBase::buildoption,
@@ -372,6 +378,10 @@
     if( partial_costs )
     {
         int n_partial_costs = partial_costs.length();
+        if( n_partial_costs != n_layers - 1)
+            PLERROR(&quot;DeepBeliefNet::build_costs() - \n&quot;
+                    &quot;partial_costs.length() (%d) != n_layers-1 (%d).\n&quot;,
+                    n_partial_costs, n_layers-1);
         partial_costs_indices.resize(n_partial_costs);
 
         for( int i=0; i&lt;n_partial_costs; i++ )
@@ -807,12 +817,16 @@
             pb = new ProgressBar( &quot;Training &quot;+classname(),
                                   nstages - stage );
 
+        setLearningRate( grad_learning_rate );
         for( ; stage&lt;nstages; stage++)
         {
             initialize_gibbs_chain=(stage%gibbs_chain_reinit_freq==0);
             // Do a step every 'minibatch_size' examples.
             if (stage % minibatch_size == 0) {
                 int sample_start = stage % nsamples;
+                if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
+                    setLearningRate( grad_learning_rate
+                                     / (1. + grad_decrease_ct * stage ));
                 if (batch_size &gt; 1 || minibatch_hack) {
                     train_set-&gt;getExamples(sample_start, minibatch_size,
                                            inputs, targets, weights, NULL, true);
@@ -858,6 +872,17 @@
 
             for( ; stage&lt;end_stage ; stage++ )
             {
+                 if( !fast_exact_is_equal( cd_decrease_ct, 0. ) )
+                 {
+                     real lr = cd_learning_rate 
+                         / (1. + cd_decrease_ct * 
+                            (stage - cumulative_schedule[i]));
+                     
+                     layers[i]-&gt;setLearningRate( lr );
+                     connections[i]-&gt;setLearningRate( lr );
+                     layers[i+1]-&gt;setLearningRate( lr );
+                 }
+                 
                 initialize_gibbs_chain=(stage%gibbs_chain_reinit_freq==0);
                 // Do a step every 'minibatch_size' examples.
                 if (stage % minibatch_size == 0) {
@@ -875,7 +900,6 @@
                         train_set-&gt;getExample(sample_start, input, target, weight);
                         greedyStep( input, target, i );
                     }
-
                 }
                 if( pb )
                     pb-&gt;update( stage - cumulative_schedule[i] + 1 );
@@ -907,6 +931,15 @@
             int previous_stage = cumulative_schedule[n_layers-2];
             for( ; stage&lt;end_stage ; stage++ )
             {
+                if( !fast_exact_is_equal( cd_decrease_ct, 0. ) )
+                {
+                    real lr = cd_learning_rate / 
+                        (1. + cd_decrease_ct * 
+                         (stage - cumulative_schedule[n_layers-2]));
+                    joint_layer-&gt;setLearningRate( lr );
+                    classification_module-&gt;joint_connection-&gt;setLearningRate( lr );
+                    layers[n_layers-1]-&gt;setLearningRate( lr );
+                }
                 initialize_gibbs_chain=(stage%gibbs_chain_reinit_freq==0);
                 int sample = stage % nsamples;
                 train_set-&gt;getExample( sample, input, target, weight );
@@ -949,7 +982,8 @@
 
                 if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
                     setLearningRate( grad_learning_rate
-                            / (1. + grad_decrease_ct * (stage - init_stage) ) );
+                            / (1. + grad_decrease_ct * 
+                               (stage - cumulative_schedule[n_layers-1])) );
 
                 if (minibatch_size &gt; 1 || minibatch_hack) {
                     train_set-&gt;getExamples(sample_start, minibatch_size, inputs,
@@ -999,6 +1033,7 @@
 void DeepBeliefNet::onlineStep( const Vec&amp; input, const Vec&amp; target,
                                 Vec&amp; train_costs)
 {
+    real lr;
     PLASSERT(batch_size == 1);
 
     TVec&lt;Vec&gt; cost;
@@ -1074,8 +1109,13 @@
 
     if (final_cost || (!partial_costs.isEmpty() &amp;&amp; partial_costs[n_layers-2]))
     {
-        layers[n_layers-1]-&gt;setLearningRate( grad_learning_rate );
-        connections[n_layers-2]-&gt;setLearningRate( grad_learning_rate );
+        if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
+            lr = grad_learning_rate / (1. + grad_decrease_ct * stage );
+        else
+            lr = grad_learning_rate;
+        
+        layers[n_layers-1]-&gt;setLearningRate( lr );
+        connections[n_layers-2]-&gt;setLearningRate( lr );
 
         layers[ n_layers-1 ]-&gt;bpropUpdate( layers[ n_layers-1 ]-&gt;activation,
                                            layers[ n_layers-1 ]-&gt;expectation,
@@ -1124,11 +1164,15 @@
             Vec target_exp = classification_module-&gt;target_layer-&gt;expectation;
             fill_one_hot( target_exp, (int) round(target[0]), real(0.), real(1.) );
 
-            joint_layer-&gt;setLearningRate( cd_learning_rate );
-            layers[ n_layers-1 ]-&gt;setLearningRate( cd_learning_rate );
-            classification_module-&gt;joint_connection-&gt;setLearningRate(
-                cd_learning_rate );
+            if( !fast_exact_is_equal( cd_decrease_ct, 0. ) )
+                lr = cd_learning_rate / (1. + cd_decrease_ct * stage );
+            else
+                lr = cd_learning_rate;
 
+            joint_layer-&gt;setLearningRate( lr );
+            layers[ n_layers-1 ]-&gt;setLearningRate( lr );
+            classification_module-&gt;joint_connection-&gt;setLearningRate( lr );
+
             save_layer_activation.resize(layers[ n_layers-2 ]-&gt;size);
             save_layer_activation &lt;&lt; layers[ n_layers-2 ]-&gt;activation;
             save_layer_expectation.resize(layers[ n_layers-2 ]-&gt;size);
@@ -1150,57 +1194,66 @@
     for( int i=n_layers-2 ; i&gt;=0 ; i-- )
     {
         if (i &lt;= n_layers - 3) {
-        connections[ i ]-&gt;setLearningRate( grad_learning_rate );
-        layers[ i+1 ]-&gt;setLearningRate( grad_learning_rate );
+            if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
+                lr = grad_learning_rate / (1. + grad_decrease_ct * stage );
+            else
+                lr = grad_learning_rate;
+            
+            connections[ i ]-&gt;setLearningRate( lr );
+            layers[ i+1 ]-&gt;setLearningRate( lr );
+            
 
-        layers[i+1]-&gt;bpropUpdate( layers[i+1]-&gt;activation,
-                                  layers[i+1]-&gt;expectation,
-                                  activation_gradients[i+1],
-                                  expectation_gradients[i+1] );
-
-        connections[i]-&gt;bpropUpdate( layers[i]-&gt;expectation,
-                                     layers[i+1]-&gt;activation,
-                                     expectation_gradients[i],
-                                     activation_gradients[i+1],
-                                     true);
+            layers[i+1]-&gt;bpropUpdate( layers[i+1]-&gt;activation,
+                                      layers[i+1]-&gt;expectation,
+                                      activation_gradients[i+1],
+                                      expectation_gradients[i+1] );
+            
+            connections[i]-&gt;bpropUpdate( layers[i]-&gt;expectation,
+                                         layers[i+1]-&gt;activation,
+                                         expectation_gradients[i],
+                                         activation_gradients[i+1],
+                                         true);
         }
 
         if (i &lt;= n_layers - 3 || !use_classification_cost ||
-                                 !top_layer_joint_cd) {
+            !top_layer_joint_cd) {
 
-        // N.B. the contrastiveDivergenceStep changes the activation and
-        // expectation fields of top layer of the RBM, so it must be
-        // done last
-        layers[i]-&gt;setLearningRate( cd_learning_rate );
-        layers[i+1]-&gt;setLearningRate( cd_learning_rate );
-        connections[i]-&gt;setLearningRate( cd_learning_rate );
-
-        if( i &gt; 0 )
-        {
-            save_layer_activation.resize(layers[i]-&gt;size);
-            save_layer_activation &lt;&lt; layers[i]-&gt;activation;
-            save_layer_expectation.resize(layers[i]-&gt;size);
-            save_layer_expectation &lt;&lt; layers[i]-&gt;expectation;
+            // N.B. the contrastiveDivergenceStep changes the activation and
+            // expectation fields of top layer of the RBM, so it must be
+            // done last
+            if( !fast_exact_is_equal( cd_decrease_ct, 0. ) )
+                lr = cd_learning_rate / (1. + cd_decrease_ct * stage );
+            else
+                lr = cd_learning_rate;
+            
+            layers[i]-&gt;setLearningRate( lr );
+            layers[i+1]-&gt;setLearningRate( lr );
+            connections[i]-&gt;setLearningRate( lr );
+            
+            if( i &gt; 0 )
+            {
+                save_layer_activation.resize(layers[i]-&gt;size);
+                save_layer_activation &lt;&lt; layers[i]-&gt;activation;
+                save_layer_expectation.resize(layers[i]-&gt;size);
+                save_layer_expectation &lt;&lt; layers[i]-&gt;expectation;
+            }
+            contrastiveDivergenceStep( layers[ i ],
+                                       connections[ i ],
+                                       layers[ i+1 ] ,
+                                       i, true);
+            if( i &gt; 0 )
+            {
+                layers[i]-&gt;activation &lt;&lt; save_layer_activation;
+                layers[i]-&gt;expectation &lt;&lt; save_layer_expectation;
+            }
         }
-        contrastiveDivergenceStep( layers[ i ],
-                                   connections[ i ],
-                                   layers[ i+1 ] ,
-                                   i, true);
-        if( i &gt; 0 )
-        {
-            layers[i]-&gt;activation &lt;&lt; save_layer_activation;
-            layers[i]-&gt;expectation &lt;&lt; save_layer_expectation;
-        }
-        }
     }
-
-
-
 }
 
 void DeepBeliefNet::onlineStep(const Mat&amp; inputs, const Mat&amp; targets,
                                Mat&amp; train_costs)
 {
+    real lr;
     // TODO Can we avoid this memory allocation?
     TVec&lt;Mat&gt; cost;
     Vec optimized_cost(inputs.length());
@@ -1280,9 +1333,14 @@
 
     if (final_cost || (!partial_costs.isEmpty() &amp;&amp; partial_costs[n_layers-2]))
     {
-        layers[n_layers-1]-&gt;setLearningRate( grad_learning_rate );
-        connections[n_layers-2]-&gt;setLearningRate( grad_learning_rate );
+        if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
+            lr = grad_learning_rate / (1. + grad_decrease_ct * stage );
+        else
+            lr = grad_learning_rate;
 
+        layers[n_layers-1]-&gt;setLearningRate( lr );
+        connections[n_layers-2]-&gt;setLearningRate( lr );
+
         layers[ n_layers-1 ]-&gt;bpropUpdate(
                 layers[ n_layers-1 ]-&gt;activations,
                 layers[ n_layers-1 ]-&gt;getExpectations(),
@@ -1335,11 +1393,15 @@
             Vec target_exp = classification_module-&gt;target_layer-&gt;expectation;
             fill_one_hot( target_exp, (int) round(target[0]), real(0.), real(1.) );
 
-            joint_layer-&gt;setLearningRate( cd_learning_rate );
-            layers[ n_layers-1 ]-&gt;setLearningRate( cd_learning_rate );
-            classification_module-&gt;joint_connection-&gt;setLearningRate(
-                cd_learning_rate );
+            if( !fast_exact_is_equal( cd_decrease_ct, 0. ) )
+               lr = cd_learning_rate / (1. + cd_decrease_ct * stage );
+            else
+               lr = cd_learning_rate;
 
+            joint_layer-&gt;setLearningRate( lr );
+            layers[ n_layers-1 ]-&gt;setLearningRate( lr );
+            classification_module-&gt;joint_connection-&gt;setLearningRate( lr );
+
             save_layer_activation.resize(layers[ n_layers-2 ]-&gt;size);
             save_layer_activation &lt;&lt; layers[ n_layers-2 ]-&gt;activation;
             save_layer_expectation.resize(layers[ n_layers-2 ]-&gt;size);
@@ -1370,8 +1432,13 @@
     for( int i=n_layers-2 ; i&gt;=0 ; i-- )
     {
         if (i &lt;= n_layers - 3) {
-            connections[ i ]-&gt;setLearningRate( grad_learning_rate );
-            layers[ i+1 ]-&gt;setLearningRate( grad_learning_rate );
+            if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
+                lr = grad_learning_rate / (1. + grad_decrease_ct * stage );
+            else
+                lr = grad_learning_rate;
+            
+            connections[ i ]-&gt;setLearningRate( lr );
+            layers[ i+1 ]-&gt;setLearningRate( lr );
 
             layers[i+1]-&gt;bpropUpdate( layers[i+1]-&gt;activations,
                                       layers[i+1]-&gt;getExpectations(),
@@ -1393,9 +1460,13 @@
             // N.B. the contrastiveDivergenceStep changes the activation and
             // expectation fields of top layer of the RBM, so it must be
             // done last
-            layers[i]-&gt;setLearningRate( cd_learning_rate );
-            layers[i+1]-&gt;setLearningRate( cd_learning_rate );
-            connections[i]-&gt;setLearningRate( cd_learning_rate );
+            if( !fast_exact_is_equal( cd_decrease_ct, 0. ) )
+                lr = cd_learning_rate / (1. + cd_decrease_ct * stage );
+            else
+                lr = cd_learning_rate;
+            layers[i]-&gt;setLearningRate( lr );
+            layers[i+1]-&gt;setLearningRate( lr );
+            connections[i]-&gt;setLearningRate( lr );
 
             if( i &gt; 0 )
             {
@@ -1438,6 +1509,7 @@
 ////////////////
 void DeepBeliefNet::greedyStep( const Vec&amp; input, const Vec&amp; target, int index )
 {
+    real lr;
     PLASSERT( index &lt; n_layers );
 
     layers[0]-&gt;expectation &lt;&lt; input;
@@ -1448,13 +1520,20 @@
         layers[i+1]-&gt;computeExpectation();
     }
 
-    // TODO: add another learning rate?
     if( !partial_costs.isEmpty() &amp;&amp; partial_costs[ index ] )
     {
         // put appropriate learning rate
-        connections[ index ]-&gt;setLearningRate( grad_learning_rate );
-        layers[ index+1 ]-&gt;setLearningRate( grad_learning_rate );
+        if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
+            lr = grad_learning_rate / 
+                (1. + grad_decrease_ct * 
+                 (stage - cumulative_schedule[index]));
+        else
+            lr = grad_learning_rate;
 
+        partial_costs[ index ]-&gt;setLearningRate( lr );
+        connections[ index ]-&gt;setLearningRate( lr );
+        layers[ index+1 ]-&gt;setLearningRate( lr );
+
         // Backward pass
         real cost;
         partial_costs[ index ]-&gt;fprop( layers[ index+1 ]-&gt;expectation,
@@ -1476,8 +1555,14 @@
                                            activation_gradients[ index+1 ] );
 
         // put back old learning rate
-        connections[ index ]-&gt;setLearningRate( cd_learning_rate );
-        layers[ index+1 ]-&gt;setLearningRate( cd_learning_rate );
+        if( !fast_exact_is_equal( cd_decrease_ct, 0. ) )
+            lr = cd_learning_rate / (1. + cd_decrease_ct * 
+                                     (stage - cumulative_schedule[index]));
+        else
+            lr = cd_learning_rate;
+
+        connections[ index ]-&gt;setLearningRate( lr );
+        layers[ index+1 ]-&gt;setLearningRate( lr );
     }
 
     contrastiveDivergenceStep( layers[ index ],
@@ -1491,6 +1576,7 @@
 /////////////////
 void DeepBeliefNet::greedyStep( const Mat&amp; inputs, const Mat&amp; targets, int index, Mat&amp; train_costs_m )
 {
+    real lr;
     PLASSERT( index &lt; n_layers );
 
     layers[0]-&gt;setExpectations(inputs);
@@ -1501,13 +1587,20 @@
         layers[i+1]-&gt;computeExpectations();
     }
 
-    // TODO: add another learning rate?
     if( !partial_costs.isEmpty() &amp;&amp; partial_costs[ index ] )
     {
         // put appropriate learning rate
-        connections[ index ]-&gt;setLearningRate( grad_learning_rate );
-        layers[ index+1 ]-&gt;setLearningRate( grad_learning_rate );
+        if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
+            lr = grad_learning_rate / 
+                (1. + grad_decrease_ct * 
+                 (stage - cumulative_schedule[index]));
+        else
+            lr = grad_learning_rate;
 
+        partial_costs[ index ]-&gt;setLearningRate( lr );
+        connections[ index ]-&gt;setLearningRate( lr );
+        layers[ index+1 ]-&gt;setLearningRate( lr );
+
         // Backward pass
         Vec costs;
         partial_costs[ index ]-&gt;fprop( layers[ index+1 ]-&gt;getExpectations(),
@@ -1529,8 +1622,13 @@
                                            activations_gradients[ index+1 ] );
 
         // put back old learning rate
-        connections[ index ]-&gt;setLearningRate( cd_learning_rate );
-        layers[ index+1 ]-&gt;setLearningRate( cd_learning_rate );
+        if( !fast_exact_is_equal( cd_decrease_ct, 0. ) )
+            lr = cd_learning_rate / (1. + cd_decrease_ct * 
+                                     (stage - cumulative_schedule[index]));
+        else
+            lr = cd_learning_rate;
+        connections[ index ]-&gt;setLearningRate( lr );
+        layers[ index+1 ]-&gt;setLearningRate( lr );
     }
 
     if (reconstruct_layerwise)
@@ -1557,6 +1655,7 @@
 /////////////////////
 void DeepBeliefNet::jointGreedyStep( const Vec&amp; input, const Vec&amp; target )
 {
+    real lr;
     PLASSERT( joint_layer );
     PLASSERT_MSG(batch_size == 1, &quot;Not implemented for mini-batches&quot;);
 
@@ -1577,8 +1676,17 @@
         layers[ n_layers-1 ]-&gt;computeExpectation();
 
         // put appropriate learning rate
-        connections[ n_layers-2 ]-&gt;setLearningRate( grad_learning_rate );
-        layers[ n_layers-1 ]-&gt;setLearningRate( grad_learning_rate );
+        if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
+            lr = grad_learning_rate 
+                / (1. + grad_decrease_ct * 
+                   (stage - cumulative_schedule[n_layers-2]));
+        else
+            lr = grad_learning_rate;
+        
+        partial_costs[ n_layers-2 ]-&gt;setLearningRate( lr );
+        connections[ n_layers-2 ]-&gt;setLearningRate( lr );
+        layers[ n_layers-1 ]-&gt;setLearningRate( lr );
+        
 
         // Backward pass
         real cost;
@@ -1602,8 +1710,15 @@
             activation_gradients[ n_layers-1 ] );
 
         // put back old learning rate
-        connections[ n_layers-2 ]-&gt;setLearningRate( cd_learning_rate );
-        layers[ n_layers-1 ]-&gt;setLearningRate( cd_learning_rate );
+        if( !fast_exact_is_equal( cd_decrease_ct, 0. ) )
+            lr = cd_learning_rate 
+                / (1. + cd_decrease_ct * 
+                   (stage - cumulative_schedule[n_layers-2]));
+        else
+            lr = cd_learning_rate;
+
+        connections[ n_layers-2 ]-&gt;setLearningRate( lr );
+        layers[ n_layers-1 ]-&gt;setLearningRate( lr );
     }
 
     Vec target_exp = classification_module-&gt;target_layer-&gt;expectation;
@@ -2227,6 +2342,8 @@
     {
         layers[i]-&gt;setLearningRate( the_learning_rate );
         connections[i]-&gt;setLearningRate( the_learning_rate );
+        if( partial_costs.length() != 0 &amp;&amp; partial_costs[i] )
+            partial_costs[i]-&gt;setLearningRate( the_learning_rate );
     }
     layers[n_layers-1]-&gt;setLearningRate( the_learning_rate );
 

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2007-09-07 21:14:30 UTC (rev 8055)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2007-09-07 22:07:33 UTC (rev 8056)
@@ -68,6 +68,10 @@
     //! The learning rate used during contrastive divergence learning
     real cd_learning_rate;
 
+    //! The decrease constant of the learning rate used during 
+    //! contrastive divergence learning
+    real cd_decrease_ct;
+
     //! The learning rate used during the gradient descent
     real grad_learning_rate;
 
@@ -184,7 +188,6 @@
     //! inside classification_module), if use_classification_cost
     PP&lt;RBMMixedLayer&gt; joint_layer;
 
-
 public:
     //#####  Public Member Functions  #########################################
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001503.html">[Plearn-commits] r8055 - in trunk: commands commands/EXPERIMENTAL	doc scripts/EXPERIMENTAL
</A></li>
	<LI>Next message: <A HREF="001505.html">[Plearn-commits] r8057 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1504">[ date ]</a>
              <a href="thread.html#1504">[ thread ]</a>
              <a href="subject.html#1504">[ subject ]</a>
              <a href="author.html#1504">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
