<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r8055 - in trunk: commands commands/EXPERIMENTAL	doc scripts/EXPERIMENTAL
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-September/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8055%20-%20in%20trunk%3A%20commands%20commands/EXPERIMENTAL%0A%09doc%20scripts/EXPERIMENTAL&In-Reply-To=%3C200709072114.l87LEY6K007407%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001502.html">
   <LINK REL="Next"  HREF="001504.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r8055 - in trunk: commands commands/EXPERIMENTAL	doc scripts/EXPERIMENTAL</H1>
    <B>plearner at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8055%20-%20in%20trunk%3A%20commands%20commands/EXPERIMENTAL%0A%09doc%20scripts/EXPERIMENTAL&In-Reply-To=%3C200709072114.l87LEY6K007407%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r8055 - in trunk: commands commands/EXPERIMENTAL	doc scripts/EXPERIMENTAL">plearner at mail.berlios.de
       </A><BR>
    <I>Fri Sep  7 23:14:34 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="001502.html">[Plearn-commits] r8054 - trunk/python_modules/plearn/parallel
</A></li>
        <LI>Next message: <A HREF="001504.html">[Plearn-commits] r8056 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1503">[ date ]</a>
              <a href="thread.html#1503">[ thread ]</a>
              <a href="subject.html#1503">[ subject ]</a>
              <a href="author.html#1503">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: plearner
Date: 2007-09-07 23:14:30 +0200 (Fri, 07 Sep 2007)
New Revision: 8055

Added:
   trunk/commands/EXPERIMENTAL/
   trunk/commands/EXPERIMENTAL/plearn_exp.cc
   trunk/scripts/EXPERIMENTAL/TLTester.py
   trunk/scripts/EXPERIMENTAL/generators.py
   trunk/scripts/EXPERIMENTAL/iTraining.py
   trunk/scripts/EXPERIMENTAL/itest.py
   trunk/scripts/EXPERIMENTAL/itest2.py
Modified:
   trunk/doc/installation_guide.tex
   trunk/scripts/EXPERIMENTAL/deepnetplot.py
Log:
Updated script and command for today's demo


Added: trunk/commands/EXPERIMENTAL/plearn_exp.cc
===================================================================
--- trunk/commands/EXPERIMENTAL/plearn_exp.cc	2007-09-06 20:50:10 UTC (rev 8054)
+++ trunk/commands/EXPERIMENTAL/plearn_exp.cc	2007-09-07 21:14:30 UTC (rev 8055)
@@ -0,0 +1,405 @@
+// -*- C++ -*-
+
+// plearn.cc
+// Copyright (C) 2002 Pascal Vincent, Julien Keable, Xavier Saint-Mleux, Rejean Ducharme
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* *******************************************************      
+ * $Id: plearn_light.cc 3995 2005-08-25 13:58:23Z chapados $
+ ******************************************************* */
+
+//! All includes should go into plearn_inc.h.
+#include &lt;commands/plearn_version.h&gt;
+#include &lt;commands/PLearnCommands/plearn_main.h&gt;
+
+
+/*****************
+ * Miscellaneous *
+ *****************/
+// #include &lt;plearn/db/UCISpecification.h&gt;
+// #include &lt;plearn/io/openUrl.h&gt;
+// #include &lt;plearn/math/ManualBinner.h&gt;
+// #include &lt;plearn/math/SoftHistogramBinner.h&gt;
+// #include &lt;plearn/misc/ShellScript.h&gt;
+// #include &lt;plearn/misc/RunObject.h&gt;
+// #include &lt;plearn_learners/misc/Grapher.h&gt;
+// #include &lt;plearn_learners/misc/VariableSelectionWithDirectedGradientDescent.h&gt;
+#include &lt;plearn_learners/testers/PTester.h&gt;
+
+/***********
+ * Command *
+ ***********/
+#include &lt;commands/PLearnCommands/VMatCommand.h&gt;
+#include &lt;commands/PLearnCommands/VMatViewCommand.h&gt;
+// #include &lt;commands/PLearnCommands/AutoRunCommand.h&gt;
+// #include &lt;commands/PLearnCommands/DiffCommand.h&gt;
+// #include &lt;commands/PLearnCommands/FieldConvertCommand.h&gt;
+#include &lt;commands/PLearnCommands/HelpCommand.h&gt;
+// #include &lt;commands/PLearnCommands/JulianDateCommand.h&gt;
+// #include &lt;commands/PLearnCommands/KolmogorovSmirnovCommand.h&gt;
+// #include &lt;commands/PLearnCommands/LearnerCommand.h&gt;
+// #include &lt;commands/PLearnCommands/PairwiseDiffsCommand.h&gt;
+#include &lt;commands/PLearnCommands/ReadAndWriteCommand.h&gt;
+#include &lt;commands/PLearnCommands/RunCommand.h&gt;
+#include &lt;commands/PLearnCommands/ServerCommand.h&gt;
+// #include &lt;commands/PLearnCommands/TestDependenciesCommand.h&gt;
+// #include &lt;commands/PLearnCommands/TestDependencyCommand.h&gt;
+
+// * extra stuff from Boost to generate help *
+// #include &lt;commands/PLearnCommands/HTMLHelpCommand.h&gt;
+
+// //#include &lt;commands/PLearnCommands/TxtmatCommand.h&gt;
+
+
+// /**************
+//  * Dictionary *
+//  **************/
+// #include &lt;plearn/dict/Dictionary.h&gt;
+// #include &lt;plearn/dict/FileDictionary.h&gt;
+// #include &lt;plearn/dict/VecDictionary.h&gt;
+// #include &lt;plearn/dict/ConditionalDictionary.h&gt;
+
+// /****************
+//  * HyperCommand *
+//  ****************/
+#include &lt;plearn_learners/hyper/HyperOptimize.h&gt;
+#include &lt;plearn_learners/hyper/HyperRetrain.h&gt;
+#include &lt;plearn_learners/hyper/HyperSetOption.h&gt;
+
+// /**********
+//  * Kernel *
+//  **********/
+// #include &lt;plearn/ker/AdditiveNormalizationKernel.h&gt;
+// #include &lt;plearn/ker/DistanceKernel.h&gt;
+// #include &lt;plearn/ker/DotProductKernel.h&gt;
+// #include &lt;plearn/ker/EpanechnikovKernel.h&gt;
+// #include &lt;plearn/ker/GaussianKernel.h&gt;
+// #include &lt;plearn/ker/GeodesicDistanceKernel.h&gt;
+// #include &lt;plearn/ker/IIDNoiseKernel.h&gt;
+// #include &lt;plearn/ker/NegOutputCostFunction.h&gt;
+// #include &lt;plearn/ker/NeuralNetworkARDKernel.h&gt;
+// #include &lt;plearn/ker/PolynomialKernel.h&gt;
+// #include &lt;plearn/ker/RationalQuadraticARDKernel.h&gt;
+// #include &lt;plearn/ker/SquaredExponentialARDKernel.h&gt;
+// #include &lt;plearn/ker/SummationKernel.h&gt;
+// #include &lt;plearn/ker/ThresholdedKernel.h&gt;
+// #include &lt;plearn/ker/VMatKernel.h&gt;
+
+// /*************
+//  * Optimizer *
+//  *************/
+// #include &lt;plearn/opt/AdaptGradientOptimizer.h&gt;
+// #include &lt;plearn/opt/ConjGradientOptimizer.h&gt;
+#include &lt;plearn/opt/GradientOptimizer.h&gt;
+
+// /****************
+//  * OptionOracle *
+//  ****************/
+// #include &lt;plearn_learners/hyper/CartesianProductOracle.h&gt;
+#include &lt;plearn_learners/hyper/EarlyStoppingOracle.h&gt;
+#include &lt;plearn_learners/hyper/ExplicitListOracle.h&gt;
+// #include &lt;plearn_learners/hyper/OptimizeOptionOracle.h&gt;
+
+// /************
+//  * PLearner *
+//  ************/
+
+// // Classifiers
+// #include &lt;plearn_learners/classifiers/BinaryStump.h&gt;
+// #include &lt;plearn_learners/classifiers/ClassifierFromConditionalPDistribution.h&gt;
+// #include &lt;plearn_learners/classifiers/ClassifierFromDensity.h&gt;
+// #include &lt;plearn_learners/classifiers/KNNClassifier.h&gt;
+// //#include &lt;plearn_learners/classifiers/SVMClassificationTorch.h&gt;
+// #include &lt;plearn_learners/classifiers/MultiInstanceNNet.h&gt;
+// //#include &lt;plearn_learners/classifiers/OverlappingAdaBoost.h&gt; // Does not currently compile.
+
+// // Generic
+// #include &lt;plearn_learners/generic/AddCostToLearner.h&gt;
+// #include &lt;plearn_learners/generic/AddLayersNNet.h&gt;
+// #include &lt;plearn_learners/generic/BestAveragingPLearner.h&gt;
+// //#include &lt;plearn_learners/generic/DistRepNNet.h&gt;
+// #include &lt;plearn_learners/generic/NNet.h&gt;
+// #include &lt;plearn_learners/generic/SelectInputSubsetLearner.h&gt;
+// #include &lt;plearn_learners/generic/ChainedLearners.h&gt;
+// #include &lt;plearn_learners/generic/StackedLearner.h&gt;
+// #include &lt;plearn_learners/generic/TestingLearner.h&gt;
+// #include &lt;plearn_learners/generic/VPLPreprocessedLearner.h&gt;
+// #include &lt;plearn_learners/generic/VPLPreprocessedLearner2.h&gt;
+// #include &lt;plearn_learners/generic/VPLCombinedLearner.h&gt;
+
+// // Hyper
+#include &lt;plearn_learners/hyper/HyperLearner.h&gt;
+
+// // Meta
+// #include &lt;plearn_learners/meta/AdaBoost.h&gt;
+// #include &lt;plearn_learners/meta/BaggingLearner.h&gt;
+
+// // Regressors
+// #include &lt;plearn_learners/regressors/ConstantRegressor.h&gt;
+// #include &lt;plearn_learners/regressors/GaussianProcessRegressor.h&gt;
+// #include &lt;plearn_learners/regressors/KernelRidgeRegressor.h&gt;
+// #include &lt;plearn_learners/regressors/KNNRegressor.h&gt;
+// #include &lt;plearn_learners/regressors/RankLearner.h&gt;
+// #include &lt;plearn_learners/regressors/RegressorFromDistribution.h&gt;
+// // Unsupervised
+// #include &lt;plearn_learners/unsupervised/UniformizeLearner.h&gt;
+
+// // PDistribution
+// #include &lt;plearn_learners/distributions/SpiralDistribution.h&gt;
+// #include &lt;plearn_learners/distributions/UniformDistribution.h&gt;
+
+// // Nearest-Neighbors
+// #include &lt;plearn_learners/nearest_neighbors/BallTreeNearestNeighbors.h&gt;
+// #include &lt;plearn_learners/nearest_neighbors/ExhaustiveNearestNeighbors.h&gt;
+// #include &lt;plearn_learners/nearest_neighbors/GenericNearestNeighbors.h&gt;
+
+// // Experimental
+// #include &lt;plearn_learners_experimental/DeepFeatureExtractorNNet.h&gt;
+
+// // Online
+// #include &lt;plearn_learners/online/BackConvolution2DModule.h&gt;
+// #include &lt;plearn_learners/online/ClassErrorCostModule.h&gt;
+// #include &lt;plearn_learners/online/CombiningCostsModule.h&gt;
+// #include &lt;plearn_learners/online/Convolution2DModule.h&gt;
+// #include &lt;plearn_learners/online/CostModule.h&gt;
+// #include &lt;plearn_learners/online/DeepBeliefNet.h&gt;
+// #include &lt;plearn_learners/online/GradNNetLayerModule.h&gt;
+// #include &lt;plearn_learners/online/ModulesLearner.h&gt;
+// #include &lt;plearn_learners/online/ModuleStackModule.h&gt;
+// #include &lt;plearn_learners/online/NLLCostModule.h&gt;
+// #include &lt;plearn_learners/online/OnlineLearningModule.h&gt;
+// #include &lt;plearn_learners/online/ProcessInputCostModule.h&gt;
+// #include &lt;plearn_learners/online/RBMBinomialLayer.h&gt;
+// #include &lt;plearn_learners/online/RBMClassificationModule.h&gt;
+// #include &lt;plearn_learners/online/RBMConnection.h&gt;
+// #include &lt;plearn_learners/online/RBMConv2DConnection.h&gt;
+// #include &lt;plearn_learners/online/RBMGaussianLayer.h&gt;
+// #include &lt;plearn_learners/online/RBMLayer.h&gt;
+// #include &lt;plearn_learners/online/RBMMatrixConnection.h&gt;
+// #include &lt;plearn_learners/online/RBMMatrixTransposeConnection.h&gt;
+// #include &lt;plearn_learners/online/RBMMixedConnection.h&gt;
+// #include &lt;plearn_learners/online/RBMMixedLayer.h&gt;
+// #include &lt;plearn_learners/online/RBMMultinomialLayer.h&gt;
+// #include &lt;plearn_learners/online/RBMTruncExpLayer.h&gt;
+// #include &lt;plearn_learners/online/SoftmaxModule.h&gt;
+// #include &lt;plearn_learners/online/SquaredErrorCostModule.h&gt;
+// #include &lt;plearn_learners/online/StackedAutoassociatorsNet.h&gt;
+// #include &lt;plearn_learners/online/Subsampling2DModule.h&gt;
+// #include &lt;plearn_learners/online/Supersampling2DModule.h&gt;
+// #include &lt;plearn_learners/online/TanhModule.h&gt;
+
+// /************
+//  * Splitter *
+//  ************/
+// #include &lt;plearn/vmat/BinSplitter.h&gt;
+// #include &lt;plearn/vmat/BootstrapSplitter.h&gt;
+// #include &lt;plearn/vmat/ClassSeparationSplitter.h&gt;
+// #include &lt;plearn/vmat/ConcatSetsSplitter.h&gt;
+// #include &lt;plearn/vmat/DBSplitter.h&gt;
+#include &lt;plearn/vmat/ExplicitSplitter.h&gt;
+// #include &lt;plearn/vmat/FilterSplitter.h&gt;
+#include &lt;plearn/vmat/FractionSplitter.h&gt;
+// #include &lt;plearn/vmat/KFoldSplitter.h&gt;
+#include &lt;plearn/vmat/NoSplitSplitter.h&gt;
+// #include &lt;plearn/vmat/MultiTaskSeparationSplitter.h&gt;
+// #include &lt;plearn/vmat/RepeatSplitter.h&gt;
+// #include &lt;plearn/vmat/SourceVMatrixSplitter.h&gt;
+// #include &lt;plearn/vmat/StackedSplitter.h&gt;
+// #include &lt;plearn/vmat/TestInTrainSplitter.h&gt;
+// #include &lt;plearn/vmat/ToBagSplitter.h&gt;
+#include &lt;plearn/vmat/TrainTestSplitter.h&gt;
+// #include &lt;plearn/vmat/TrainValidTestSplitter.h&gt;
+
+// /************
+//  * Variable *
+//  ************/
+// #include &lt;plearn/var/MatrixElementsVariable.h&gt;
+
+// /*********************
+//  * VecStatsCollector *
+//  *********************/
+// #include &lt;plearn/math/LiftStatsCollector.h&gt;
+
+// /***********
+//  * VMatrix *
+//  ***********/
+// #include &lt;plearn/vmat/AddMissingVMatrix.h&gt;
+// #include &lt;plearn/vmat/AppendNeighborsVMatrix.h&gt;
+// #include &lt;plearn/vmat/AsciiVMatrix.h&gt;
+#include &lt;plearn/vmat/AutoVMatrix.h&gt;
+// #include &lt;plearn/vmat/BootstrapVMatrix.h&gt;
+// #include &lt;plearn/vmat/CenteredVMatrix.h&gt;
+// #include &lt;plearn/vmat/ClassSubsetVMatrix.h&gt;
+// #include &lt;plearn/vmat/CompactVMatrix.h&gt;
+// #include &lt;plearn/vmat/CompressedVMatrix.h&gt;
+// #include &lt;plearn/vmat/CumVMatrix.h&gt;
+// #include &lt;plearn/vmat/DatedJoinVMatrix.h&gt;
+// // #include &lt;plearn/vmat/DictionaryVMatrix.h&gt;
+// #include &lt;plearn/vmat/DisregardRowsVMatrix.h&gt;
+// #include &lt;plearn/vmat/ExtractNNetParamsVMatrix.h&gt;
+#include &lt;plearn/vmat/FilteredVMatrix.h&gt;
+// #include &lt;plearn/vmat/FinancePreprocVMatrix.h&gt;
+// #include &lt;plearn/vmat/GaussianizeVMatrix.h&gt;
+// #include &lt;plearn/vmat/GeneralizedOneHotVMatrix.h&gt;
+// #include &lt;plearn/vmat/GetInputVMatrix.h&gt;
+// #include &lt;plearn/vmat/GramVMatrix.h&gt;
+// #include &lt;plearn/vmat/IndexedVMatrix.h&gt;
+// #include &lt;plearn/vmat/JulianizeVMatrix.h&gt;
+// #include &lt;plearn/vmat/KNNVMatrix.h&gt;
+// #include &lt;plearn/vmat/KNNImputationVMatrix.h&gt;
+// // Commented out because triggers WordNet, which does not work really fine yet.
+// //#include &lt;plearn/vmat/LemmatizeVMatrix.h&gt;
+// #include &lt;plearn/vmat/LocalNeighborsDifferencesVMatrix.h&gt;
+// #include &lt;plearn/vmat/LocallyPrecomputedVMatrix.h&gt;
+// #include &lt;plearn/vmat/MeanImputationVMatrix.h&gt;
+// //#include &lt;plearn/vmat/MixUnlabeledNeighbourVMatrix.h&gt;
+// #include &lt;plearn/vmat/MultiInstanceVMatrix.h&gt;
+// #include &lt;plearn/vmat/MultiTargetOneHotVMatrix.h&gt;
+// #include &lt;plearn/vmat/MultiToUniInstanceSelectRandomVMatrix.h&gt;
+// #include &lt;plearn/vmat/OneHotVMatrix.h&gt;
+// #include &lt;plearn/vmat/PLearnerOutputVMatrix.h&gt;
+// #include &lt;plearn/vmat/PairsVMatrix.h&gt;
+// #include &lt;plearn/vmat/PrecomputedVMatrix.h&gt;
+// #include &lt;plearn/vmat/ProcessDatasetVMatrix.h&gt;
+#include &lt;plearn/vmat/ProcessingVMatrix.h&gt;
+// #include &lt;plearn/vmat/ProcessSymbolicSequenceVMatrix.h&gt;
+// #include &lt;plearn/vmat/RandomSamplesVMatrix.h&gt;
+// #include &lt;plearn/vmat/RandomSamplesFromVMatrix.h&gt;
+// #include &lt;plearn/vmat/RankedVMatrix.h&gt;
+// #include &lt;plearn/vmat/RegularGridVMatrix.h&gt;
+// #include &lt;plearn/vmat/RemoveDuplicateVMatrix.h&gt;
+// #include &lt;plearn/vmat/ReorderByMissingVMatrix.h&gt;
+// //#include &lt;plearn/vmat/SelectAttributsSequenceVMatrix.h&gt;
+// #include &lt;plearn/vmat/SelectRowsMultiInstanceVMatrix.h&gt;
+// #include &lt;plearn/vmat/ShuffleColumnsVMatrix.h&gt;
+// #include &lt;plearn/vmat/SortRowsVMatrix.h&gt;
+// #include &lt;plearn/vmat/SparseVMatrix.h&gt;
+// #include &lt;plearn/vmat/SplitWiseValidationVMatrix.h&gt;
+// #include &lt;plearn/vmat/SubInputVMatrix.h&gt;
+// #include &lt;plearn/vmat/TemporaryDiskVMatrix.h&gt;
+// #include &lt;plearn/vmat/TemporaryFileVMatrix.h&gt;
+// #include &lt;plearn/vmat/TextFilesVMatrix.h&gt;
+// #include &lt;plearn/vmat/ThresholdVMatrix.h&gt;
+// #include &lt;plearn/vmat/TransposeVMatrix.h&gt;
+// #include &lt;plearn/vmat/UCIDataVMatrix.h&gt;
+// #include &lt;plearn/vmat/UniformizeVMatrix.h&gt;
+// #include &lt;plearn/vmat/VariableDeletionVMatrix.h&gt;
+// #include &lt;plearn/vmat/ViewSplitterVMatrix.h&gt;
+// #include &lt;plearn/vmat/VMatrixFromDistribution.h&gt;
+
+
+
+// **** Require LAPACK and BLAS
+
+// Unsupervised/KernelProjection
+// #include &lt;plearn_learners/unsupervised/Isomap.h&gt;
+// #include &lt;plearn_learners/unsupervised/KernelPCA.h&gt;
+// #include &lt;plearn_learners/unsupervised/LLE.h&gt;
+// #include &lt;plearn_learners/unsupervised/PCA.h&gt;
+// #include &lt;plearn_learners/unsupervised/SpectralClustering.h&gt;
+
+// Kernels
+// #include &lt;plearn/ker/LLEKernel.h&gt;
+// #include &lt;plearn/ker/ReconstructionWeightsKernel.h&gt;
+
+// Regressors
+// #include &lt;plearn_learners/regressors/LinearRegressor.h&gt;
+// #include &lt;plearn_learners/regressors/PLS.h&gt;
+
+// PDistribution
+// #include &lt;plearn_learners/distributions/GaussianDistribution.h&gt;
+// #include &lt;plearn_learners/distributions/GaussMix.h&gt;
+// #include &lt;plearn_learners/distributions/RandomGaussMix.h&gt;
+// #include &lt;plearn_learners/distributions/ParzenWindow.h&gt;
+// #include &lt;plearn_learners/distributions/ManifoldParzen2.h&gt;
+
+// Experimental
+// #include &lt;plearn_learners_experimental/LinearInductiveTransferClassifier.h&gt;
+
+// SurfaceTemplate
+// #include &lt;plearn_learners_experimental/SurfaceTemplate/SurfaceTemplateLearner.h&gt;
+
+// ***************************************************
+// ***   New EXPERIMENTAL stuff
+
+// includes Pascal's gradient hack
+#include &lt;plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h&gt;
+
+
+// Stuff used for DeepReconstrctorNet experiments
+#include &lt;plearn/var/Variable.h&gt;
+#include &lt;plearn/var/SquareVariable.h&gt;
+#include &lt;plearn/math/TVec_impl.h&gt;
+#include &lt;plearn/var/EXPERIMENTAL/MultiMaxVariable.h&gt;
+#include &lt;plearn/var/SoftmaxVariable.h&gt;
+#include &lt;plearn/var/SumSquareVariable.h&gt;
+#include &lt;plearn/var/Func.h&gt;
+#include &lt;plearn/var/EXPERIMENTAL/DoubleProductVariable.h&gt;
+#include &lt;plearn/var/EXPERIMENTAL/TransposedDoubleProductVariable.h&gt;
+#include &lt;plearn/var/EXPERIMENTAL/ProbabilityPairsVariable.h&gt;
+#include &lt;plearn/var/EXPERIMENTAL/ProbabilityPairsInverseVariable.h&gt;
+#include &lt;plearn/var/EXPERIMENTAL/SoftSoftMaxVariable.h&gt;
+#include &lt;plearn/var/EXPERIMENTAL/LogSoftSoftMaxVariable.h&gt;
+#include &lt;plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h&gt;
+#include &lt;plearn/var/SourceVariable.h&gt;
+#include &lt;plearn/var/ExpVariable.h&gt;
+#include &lt;plearn/var/SigmoidVariable.h&gt;
+#include &lt;plearn/var/ProductTransposeVariable.h&gt;
+#include &lt;plearn/var/NegCrossEntropySigmoidVariable.h&gt;
+#include &lt;plearn/var/LogSoftmaxVariable.h&gt;
+#include &lt;plearn/var/ClassificationLossVariable.h&gt;
+// #include &lt;plearn/var/EXPERIMENTAL/MultiSampleVariable.h&gt;
+
+// Stuff used for transformationLearner experiments
+#include &lt;plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h&gt;
+
+using namespace PLearn;
+
+int main(int argc, char** argv)
+{
+    return plearn_main( argc, argv, 
+                        PLEARN_MAJOR_VERSION, 
+                        PLEARN_MINOR_VERSION, 
+                        PLEARN_FIXLEVEL       );
+}
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Modified: trunk/doc/installation_guide.tex
===================================================================
--- trunk/doc/installation_guide.tex	2007-09-06 20:50:10 UTC (rev 8054)
+++ trunk/doc/installation_guide.tex	2007-09-07 21:14:30 UTC (rev 8055)
@@ -177,7 +177,7 @@
 python (which calls upon plearn) using essentially the following software
 packages: \\
 \begin{itemize}
-\item {\bf numarray} for efficient numeric array operations in python.
+\item {\bf numpy} (part of scipy) for efficient numeric array operations in python.
 \item {\bf matplotlib} for 2D plots.
 \item {\bf mayavi} for 3D interactive plots.
 \item {\bf pygtk} with {\bf gtk+2} for sophisticated GUIs.
@@ -505,6 +505,14 @@
 % which I couldn't get to compile and link from the source
 % <A HREF="http://ftp.gnome.org/pub/GNOME/sources/gnome-python-extras/">http://ftp.gnome.org/pub/GNOME/sources/gnome-python-extras/</A>
 
+% For python 2.5:
+% \item {\tt python25}
+% \item {\tt boost-jam}
+% \item {\tt boost1.34.nopython}
+% \item {\tt scipy-py25} for efficient matrix/vector manipulations in python
+% \item {\tt matplotlib-py25} for 2D graphics
+
+
 \subsection{Environment setup}
 
 You should make sure the following variables and paths are correctly defined in your

Added: trunk/scripts/EXPERIMENTAL/TLTester.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/TLTester.py	2007-09-06 20:50:10 UTC (rev 8054)
+++ trunk/scripts/EXPERIMENTAL/TLTester.py	2007-09-07 21:14:30 UTC (rev 8055)
@@ -0,0 +1,165 @@
+import os ,sys, time, matplotlib, math, copy
+from numpy import *
+from matplotlib.pylab import *
+from matplotlib.colors import *
+
+from plearn.io.server import *
+from plearn.pyplearn import *
+from plearn.plotting import *
+from copy import *
+
+from generators import *
+
+UNDEFINED = -1
+EMPTY_MAT = TMat()
+
+class TLTester(object):
+    
+    #iLearner = UNDEFINED
+    #generator = UNDEFINED
+    #dim = 2
+    
+    def __init__(self,
+                 iLearner,
+                 generator):
+        assert(2*generator.nbTransforms == iLearner.nbTransforms)
+        assert(generator.dim == iLearner.dim)
+        self.dim = generator.dim
+        self.iLearner = iLearner
+        self.generator = generator
+
+
+    def biasAreNull(self,
+                    biasSet):
+        if(type(biasSet)==type(EMPTY_MAT)):
+            if(biasSet.nrows == 0 and biasSet.ncols == 0):
+                return True
+            else:
+                return False
+        elif(len(biasSet)==0):
+            return True
+        else:
+            for i in range(biasSet.shape[0]):
+                for j in range(biasSet.shape[1]):
+                    if(biasSet[i][j] != 0):
+                        return false
+            return True
+
+
+            
+        
+    def linearToLinearIncrement(self,transform,bias):
+        newTransform = copy(transform)
+        newBias = copy(bias)
+        for i in range(newTransform.shape[0]):
+            newTransform[i][i] = newTransform[i][i] - 1;
+        return (newTransform, newBias)
+
+    def linearIncrementToLinear(self,transform,bias):
+        newTransform = copy(transform)
+        newBias = copy(bias)
+        for i in range(newTransform.shape[0]):
+            newTransform[i][i] = newTransform[i][i] + 1;
+        return (newTransform,newBias)
+    
+    
+    def inverseTransformation(self,transform,bias,transformFamily):
+        if(transformFamily == LINEAR):
+            invTransform = self.zeroMatrix(self.dim,self.dim)
+            inv = inverse(transform)
+            for i in range(self.dim):
+                for j in range(self.dim):
+                    invTransform[i,j]=inv[i,j]
+            if(len(bias) &gt;0):
+                invBias = -1*dot(invTransform ,bias)
+            else:
+                invBias = []
+            return (invTransform,invBias)
+        else:
+            id = identity(len(transform))
+            invTransform = zeros((self.dim,self.dim))
+            inv = inverse(transform + id)
+            for i in range(self.dim):
+                for j in range(self.dim):
+                    invTransform[i][j]=inv[i][j]
+            if(len(bias) &gt; 0):
+                invBias = -1*dot(invTransform,bias)
+            else:
+                invBias = []
+            invTransform = invTransform - id
+            return (invTransform,invBias)
+
+
+    def conversionGeneratorToLearner(self,transform,bias):
+        if(self.generator.transformFamily == self.iLearner.transformFamily):
+            newTransform = copy(transform)
+            newBias = copy(bias)
+            return(newTransform,newBias)
+        elif(self.generator.transformFamily == LINEAR):
+            return self.linearToLinearIncrement(transform,bias)
+        else:
+            return self.linearIncrementToLinear(transform,bias)
+
+
+    def prepareILearner(self):
+        self.transmitParametersToLearn()
+        self.transmitDataSet()
+
+        
+    def transmitDataSet(self):
+        self.iLearner.setTrainingSet(self.generator.newDataSet())
+
+
+         
+    
+
+    def transmitParametersToLearn(self):
+        #transformations: 
+        transformsToLearn = []
+        temp = []
+        biasToLearn = array(zeros(self.iLearner.nbTransforms*self.dim), 'd')
+        biasToLearn.resize(self.iLearner.nbTransforms,self.dim)
+        
+        if(not self.iLearner.withBias and self.generator.withBias):
+            assert(self.biasAreNull(self.generator.biasSet))
+        K = self.generator.nbTransforms
+        for i in range(K):
+            if(self.generator.withBias):
+                biasToLearn[i] = copy(self.generator.biasSet[i,:])
+            (t,b)=self.conversionGeneratorToLearner(self.generator.transforms[i],biasToLearn[i])
+            transformsToLearn.append(t.copy())
+            biasToLearn[i]=copy(b)
+            (t,b)= self.inverseTransformation(t,                                       
+                                              b,
+                                              self.iLearner.transformFamily)
+            temp.append(t.copy())
+            biasToLearn[i + K]=copy(b)
+        for i in range(K):
+            transformsToLearn.append(temp[i])
+            
+    
+        
+        #noise variance
+        noiseVariance = UNDEFINED
+        if(self.iLearner.learnNoiseVariance):
+            noiseVariance=self.generator.noiseVariance
+        #transform distribution
+        transformDistribution = []
+        if(self.iLearner.learnTransformDistribution):
+            transformDistribution = [0.0,0.0]
+            for i in range(self.generator.nbTransforms):
+                p = 0.5*exp(self.generator.transformDistribution[i])
+                transformDistribution[i]=log(p)
+                transformDistribution[i + self.generator.nbTransforms] = log(p)
+        #transmission
+        self.iLearner.setParametersToLearn(transformsToLearn,
+                                      biasToLearn,
+                                      noiseVariance,
+                                      transformDistribution)
+            
+    def run(self):
+        self.prepareILearner()
+        self.iLearner.run()
+        
+            
+            

Modified: trunk/scripts/EXPERIMENTAL/deepnetplot.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/deepnetplot.py	2007-09-06 20:50:10 UTC (rev 8054)
+++ trunk/scripts/EXPERIMENTAL/deepnetplot.py	2007-09-07 21:14:30 UTC (rev 8055)
@@ -50,6 +50,8 @@
     print 'w : plot the weight matrices associated with the current pixel'
     print 'W : same as w but for all a group'
     print 'C : same as w but for the hidden unit that has the highest value in each group'
+    print 'Z : set all the pixels of the current layer to zero'
+    print 'B : set all the pixels of the current layer to 1'
     print 'o : set the current hidden layer to its original state'
     print 'O : same as o but for every layer'
     print 't : now we have the same scale for  W, C'
@@ -107,6 +109,9 @@
         n = self.matrixToLayer(x,y)
         return self.hidden_layer[n]
 
+    def fill(self,value):
+        self.hidden_layer.fill(value)
+
     def setElement(self, x,y, value):
         n = self.matrixToLayer(x,y)
         self.hidden_layer[n] = value
@@ -379,6 +384,16 @@
                 
             # set pixel -- z,x,c,v,b
 
+            elif char=='Z':
+                hl.fill(0.)
+                self.rep_axes[i].imshow(hl.getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
+                draw()
+
+            elif char=='B':
+                hl.fill(1.)
+                self.rep_axes[i].imshow(hl.getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
+                draw()
+
             elif char in ['z', 'x', 'c', 'v', 'b']:
                 
                 x,y = event.xdata, event.ydata
@@ -922,7 +937,7 @@
 ### main ###
 ############
 
-server_command = &quot;slearn server&quot;
+server_command = &quot;plearn_exp server&quot;
 serv = launch_plearn_server(command = server_command)
 
 #print &quot;Press Enter to continue&quot;

Added: trunk/scripts/EXPERIMENTAL/generators.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/generators.py	2007-09-06 20:50:10 UTC (rev 8054)
+++ trunk/scripts/EXPERIMENTAL/generators.py	2007-09-07 21:14:30 UTC (rev 8055)
@@ -0,0 +1,625 @@
+#generators2.py : re-implantation of generators1.py with
+#                 numpy array module instead numarray
+
+#generation procedures with 'TransformationLearner' distribution
+import os, sys, time, matplotlib, math, numpy ,copy
+
+from numpy import *
+from math import *
+from random import *
+
+from plearn.io.server import *
+from plearn.pyplearn import *
+from copy import *
+
+UNDEFINED = -1
+BEHAVIOR_GENERATOR = 1
+LINEAR = 0
+LINEAR_INCREMENT = 1
+EMPTY_BIAS = TMat()
+
+#------------------- TREE GENERATOR --------------------------------------
+
+class TreeGenerator(object):
+
+    #DESCRIPTION:
+
+    #generates a data set :
+    #     equivalent in building a tree 
+    #
+    #            0      1        2     ...         
+    #  
+    #            r -&gt; child1  -&gt; child1  ...       
+    #                         -&gt; child2  ...
+    #                             ...    ...
+    #                         -&gt; childn  ...
+    #
+    #              -&gt; child2  -&gt; child1  ...
+    #                         -&gt; child2  ...
+    #                              ...   ...
+    #                         -&gt; childn  ...
+    #                      ...
+    #              -&gt; childn  -&gt; child1  ...
+    #                         -&gt; child2  ...
+    #                              ...   ...
+    #                         -&gt; childn  ... 
+    #
+    # The child are generated by the same following process:
+    #  1) choose a transformation  
+    #  2) apply the transformation to the parent
+    #  3) add noise to the result 
+    
+
+
+    #PARAMETERS OF THE  GENERATOR
+
+    #parameters of the transformation learner used in the generation process:
+    tl=UNDEFINED                   #c++ transformation learner object
+    dim=2                          #dimension of input space
+    nbTransforms = UNDEFINED       #number of transformations
+    transformFamily=UNDEFINED      #transformation function family
+    transforms = []                #transformation matrices
+    withBias = False               #add a bias to the transformation function ?
+    biasSet = UNDEFINED            #transformation bias (if any)
+    noiseVariance = UNDEFINED      #noise variance 
+    transformDistribution = []     #transformation distribution (in log form) 
+
+    #shape of the tree data set to generate
+    root = []
+    deepness = 1
+    branchingFactor = 1
+
+
+
+    #CONSTRUCTOR
+    
+    def __init__(self,
+                 tl,
+                 dim=2,
+                 builded=False,
+                 deepness=1,
+                 branchingFactor=1,
+                 root=[]):
+        #c++ transformation learner object
+        self.tl = tl
+        #number of transformations
+        self.nbTransforms = self.tl.getOption(&quot;nbTransforms&quot;)
+        #transformation function family
+        self.transformFamily = self.tl.getOption(&quot;transformFamily&quot;)
+        #add a bias to the transformation function ? 
+        self.withBias = self.tl.getOption(&quot;withBias&quot;)
+        #if the generator is not builded, some parameters might be undefined.
+        #We call the default generator building procedure
+        #(use default initialization procedures when initial values are needed)
+        if(not builded):
+            self.dim = dim
+            self.tl.generatorBuild(self.dim,
+                                   [],
+                                   EMPTY_BIAS,
+                                   UNDEFINED,
+                                   [])
+        else:
+            self.dim = self.tl.getOption(&quot;trainingSetLength&quot;)
+            assert(self.dim == dim)
+        #transformation matrices 
+        self.transforms=self.tl.getOption(&quot;transforms&quot;)
+        #transformation bias (if any)
+        if(self.withBias):
+            self.biasSet= self.tl.getOption(&quot;biasSet&quot;)
+        else:
+            self.biasSet = EMPTY_BIAS
+        #noise variance
+        self.noiseVariance = self.tl.getOption(&quot;noiseVariance&quot;)
+        #transformation distribution (in log form)
+        self.transformDistribution  = self.tl.getOption(&quot;transformDistribution&quot;)
+        #generation process default parameters
+        if(len(root) == 0):
+            self.root = array(zeros(self.dim), 'd')
+            for i in range(self.dim):
+                self.root[i]=uniform(0,10)
+        else:
+            self.setRoot(root)
+        assert(deepness &gt;=1)
+        self.deepness = deepness
+        assert(branchingFactor &gt;= 1)
+        self.branchingFactor = branchingFactor
+
+
+    #changes the default root of the tree     
+    def setRoot(self,root):
+        assert(len(root) == self.dim)
+        self.root = copy(root)
+            
+
+    #builds the TransformationLearner object (our generator) 
+    #that is, call method 'TransformationLearner::generatorBuild'      
+    def build(self,
+              transforms=[],
+              biasSet=EMPTY_BIAS,
+              noiseVariance=UNDEFINED,
+              transformDistribution=[]):
+        #verifications fot the transformation matrices and bias
+        if(len(transforms)&gt;0):
+            assert(self.nbTransforms == len(transforms))
+            for i in range(self.nbTransforms):
+                assert(transforms[i].shape[0] == self.dim)
+                assert(transforms[i].shape[1] == self.dim)
+            if(self.withBias):
+                assert(biasSet.shape[0]==self.nbTransforms)
+                assert(baisSet.shape[1]==self.dim)
+        else:
+            biasSet=EMPTY_BIAS
+        #verifications for the transformation distribution
+        if(len(transformDistribution)&gt;0):
+            assert(len(transformDistribution)==self.nbTransforms)
+            sum=0
+            w=0
+            for i in range(self.nbTransforms):
+                w = exp(transformDistribution[i])
+                assert(0&lt;=w&lt;=1)
+                sum += w
+            assert(sum == 1)
+        #we are ready to call the building method:
+        self.tl.generatorBuild(self.dim,
+                               transforms,
+                               biasSet,
+                               noiseVariance,
+                               transformDistribution)
+        #capture new parameters of the Transformation Learner
+        self.transforms = self.tl.getOption(&quot;transforms&quot;)
+        if(self.withBias):
+            self.biasSet = self.tl.getOption(&quot;biasSet&quot;)
+        else:
+            self.biasSet= EMPTY_BIAS
+        self.noiseVariance = self.tl.getOption(&quot;noiseVariance&quot;)
+        self.transformDistribution = self.tl.getOption(&quot;transformDistribution&quot;)
+
+
+
+    #sets the transformations matrices (and bias if any) of the TransformationLearner
+    def setTransforms(self,
+                      transforms,
+                      biasSet=EMPTY_BIAS):
+        assert(self.nbTransforms==len(transforms))
+        for i in range(self.nbTransforms):
+            assert(self.dim == transforms[i].shape[0])
+            assert(self.dim == transforms[i].shape[1])
+        self.transforms = copy(transforms)
+        if(self.withBias):
+            assert(biasSet.shape[0]==self.nbTransforms)
+            assert(biasSet.shape[1]==self.dim)
+            self.biasSet = biasSet.copy()
+        else:
+            biasSet = EMPTY_BIAS
+        self.tl.setTransformsParameters(transforms,biasSet)
+             
+
+    #sets the noise variance of the TransformationLearner
+    def setNoiseVariance(self,
+                         noiseVariance):
+        assert(noiseVariance &gt; 0)
+        self.noiseVariance = noiseVariance
+        self.tl.setNoiseVariance(noiseVariance)
+        
+
+    #sets the transformation distribution of the TransformationLearner
+    def setTransformDistribution(self,
+                                 transformDistribution):
+        assert(self.nbTransforms == len(transformDistribution))
+        sum = 0
+        for i in range(self.nbTransforms):
+            p = exp(transformDistribution[i])
+            assert(0&lt;=p and p&lt;=1)
+            sum += p
+        assert(sum == 1)
+        self.transformDistribution  = copy(transformDistribution)
+        self.tl.setTransformDistribution(transformDistribution)
+
+
+    #computes the length of a generation tree 
+    def computeTreeLength(self,
+                          deepness,
+                          branchingFactor):
+        if(branchingFactor == 1):
+            return deepness + 1
+        else:
+            return ((1.0 - pow(branchingFactor,deepness + 1))/(1.0 - b))
+
+
+    #changes the default generation process parameters
+    def setDefaultTreeParameters(self,
+                                 deepness,
+                                 branchingFactor,
+                                 root):
+        self.setRoot(root)
+        assert(deepness &gt;=1)
+        self.deepness = deepness
+        assert(branchingFactor &gt;= 1)
+        self.branchingFactor = branchingFactor
+    
+    #returns a rotation matrix 
+    #(theta here must be given in Rad units)
+    def get2DRotationMatrix(self,theta):
+        M = array([cos(theta) ,-sin(theta),sin(theta) ,cos(theta)], 'd')
+        M.resize(2,2)
+        if(self.transformFamily == LINEAR_INCREMENT):
+                M[0][0] -= 1;
+                M[1][1] -= 1;
+        return M
+
+    
+    #generates a data set and stores it in a file 
+    def generateAndStoreDataSet(self,
+                                filename,
+                                deepness=UNDEFINED,
+                                branchingFactor=UNDEFINED,
+                                root =[]):
+        dataSet = self.newDataSet( False, deepness,branchingFactor,root)
+        self.writeDataSet(dataSet, filename)
+    
+
+
+    
+    #stores a data set in a file 
+    def writeDataSet(self,dataSet,filename):
+        out = open(filename, 'w')
+        out.write(str(dataSet.shape[1]) + &quot; 0 0\n&quot;)
+        out.write(str(dataSet))
+        out.close()
+
+
+    #creates a new data set
+    #(use default generation parameters if the given generation parameters are not well defined
+    # or unspecified)
+    def newDataSet(self,
+                   returnTransforms=False,
+                   deepness=UNDEFINED,
+                   branchingFactor=UNDEFINED,
+                   root=[],
+                   transformIndex = UNDEFINED):
+        if(len(root)==0):
+            root= copy(self.root)
+        else:
+            assert(len(root)==self.dim)
+        if(deepness&lt;1):
+            deepness=self.deepness
+        if(branchingFactor&lt;1):
+            branchingFactor=self.branchingFactor
+        dataSet = self.tl.returnTreeDataSet(root,
+                                            deepness,
+                                            branchingFactor,
+                                            transformIndex)
+        if(returnTransforms):
+            return (dataSet, self.transforms, self.biasSet)
+        else:
+            return dataSet 
+
+
+# ----------------- SEQUENTIAL GENERATOR -----------------------------------------
+
+class SequentialGenerator(TreeGenerator):
+
+    nbDataPoints = 40
+
+
+    def __init__(self,
+                 tl,
+                 dim=2,
+                 builded=False,
+                 nbDataPoints=40,
+                 root=[]):
+        assert(nbDataPoints &gt;=2)
+        self.nbDataPoints = nbDataPoints
+        TreeGenerator.__init__(self,
+                               tl,
+                               dim,
+                               builded,
+                               nbDataPoints - 1,
+                               1,
+                               root)
+
+
+    def setTreeParameters(self,
+                          deepness,
+                          branchingFactor,
+                          root):
+        assert(branchingFactor == 1)
+        TreeGenerator.setTreeParameters(self,
+                                        deepness,
+                                        branchingFactor,
+                                        root)
+        self.nbDataPoints=self.computeTreeLength(deepness,
+                                                 branchingFactor)
+
+
+    def setSequenceParameters(self,
+                              nbDataPoints,
+                              root):
+        self.setTreeParameters(nbDataPoints - 1,1,root)
+
+
+    def newDataSet(self,
+                   returnTransforms=False,
+                   nbDataPoints=UNDEFINED,
+                   root=[],
+                   transformIndex =UNDEFINED):
+        if(nbDataPoints&lt;2):
+            nbDataPoints = self.nbDataPoints
+        return TreeGenerator.newDataSet(self,
+                                        returnTransforms,
+                                        nbDataPoints - 1,
+                                        1,
+                                        root,
+                                        transformIndex)
+
+
+    
+#-------------CIRCLE GENERATOR ----------------------------------
+class CircleGenerator(SequentialGenerator):
+
+    ray = 10
+    center =array([0,0],'d')
+
+    def __init__(self,
+                 tl,
+                 builded=False,
+                 nbDataPoints=40,
+                 center=array([0,0],'d'),
+                 ray=10):
+        assert(tl.getOption(&quot;nbTransforms&quot;) == 1)
+        assert(ray&gt;0)
+        self.ray = ray
+        assert(len(center)==2)
+        root = copy(center)
+        root[1] += ray
+        SequentialGenerator.__init__(self,
+                                     tl,
+                                     2,
+                                     builded,
+                                     nbDataPoints,
+                                     root)
+        self.setTransforms(nbDataPoints)
+
+
+    def setTreeParameters(self,
+                          deepness,
+                          branchingFactor,
+                          root):
+        SequentialGenerator.settreeParameters(self,
+                                              deepness,
+                                              branchingFactor,
+                                              root)
+        self.setTransforms(self.nbDataPoints) 
+
+
+
+    def build(self,
+              nbDataPoints,
+              noiseVariance=UNDEFINED,
+              transformDistribution=[]):
+        assert(nbDataPoints&gt;=2)
+        self.nbDataPoints = nbDataPoints
+        theta=2*pi/nbDataPoints
+        M = self.get2DRotationMatrix(theta)
+        transforms = [M]
+        biasSet = EMPTY_BIAS
+        if(self.withBias):
+            biasSet = array([0,0],'d')
+            biasSet.resize(1,2)
+        SequentialGenerator.build(self,
+                                  transforms,
+                                  biasSet,
+                                  noiseVariance,
+                                  transformDistribution)
+
+    def setRoot(self,
+                root):
+        SequentialGenerator.setRoot(self,
+                                    root)
+        self.center = copy(root)
+        self.center[1] -= self.ray
+
+
+    def setCircleParameters(self,
+                            center,
+                            ray,
+                            nbDataPoints):
+        assert(len(center)==2)
+        assert(ray&gt;0)
+        assert(nbDataPoints &gt;=2)
+        self.center = copy(center)        #center
+        self.ray = ray                    #ray
+        self.root = copy(center)          #root
+        self.root[1] += ray
+        self.setTransforms(nbDataPoints)
+    
+
+    def setTransforms(self,
+                      nbDataPoints):
+        assert(nbDataPoints &gt;=2)
+        self.nbDataPoints = nbDataPoints
+        self.defaultDeepness = nbDataPoints - 1
+        theta = 2*pi/nbDataPoints
+        transforms = [self.get2DRotationMatrix(theta)]
+        biasSet = EMPTY_BIAS
+        if(self.withBias):
+            biasSet = array([0,0],'d')
+            biasSet.resize(1,2)
+        SequentialGenerator.setTransforms(self,
+                                          transforms,
+                                          biasSet)
+        
+
+    def newDataSet(self,
+                   returnTransforms=False,
+                   center=[],
+                   ray=UNDEFINED,
+                   nbDataPoints=UNDEFINED):
+        if(len(center) != 2):
+            center = copy(self.center)
+        if(ray&lt;=0):
+            ray = self.ray
+        if(nbDataPoints &lt; 2):
+            nbDataPoints = self.nbDataPoints
+        self.setCircleParameters(center,
+                                 ray,
+                                 nbDataPoints)
+        return SequentialGenerator.newDataSet(self,
+                                              returnTransforms)
+
+    
+# ------------- Spiral Generator ---------------------------------------
+
+class SpiralGenerator(SequentialGenerator):
+
+    alpha = 1.01
+    theta = 0.1
+
+
+    def __init__(self,
+                 tl,
+                 builded=False,
+                 nbDataPoints=40,
+                 root=[],
+                 alpha = 1.01,
+                 theta = 0.1
+                 ):
+
+        assert(tl.getOption(&quot;nbTransforms&quot;) ==1)
+        assert(alpha &gt; 0)
+        assert(theta &gt; 0)
+        self.alpha = alpha
+        self.theta = theta
+        SequentialGenerator.__init__(self,
+                                     tl,
+                                     2,
+                                     builded,
+                                     nbDataPoints,
+                                     root)
+        self.setTransforms(alpha,theta)
+
+
+    def setTransforms(self,
+                      alpha,
+                      theta):
+        assert(alpha &gt; 0)
+        assert(theta &gt; 0)
+        self.alpha = alpha
+        self.theta = theta
+        transforms = self.get2DRotationMatrix(theta)
+        transforms = alpha*transforms
+        biasSet=EMPTY_BIAS
+        if(self.withBias):
+            biasSet = array([0,0], 'd')
+            biasSet.resize(1,2)
+        SequentialGenerator.setTransforms(self,[transforms],biasSet) 
+
+
+    def setSpiralParameters(self,
+                            nbDataPoints,
+                            root,
+                            alpha,
+                            theta):
+        SequentialGenerator.setDefaultSequenceParameters(self,
+                                                         nbDataPoints,
+                                                         root)
+        self.setTransforms(alpha,theta)
+
+
+    def build(self,
+              alpha,
+              theta,
+              noiseVariance=UNDEFINED,
+              transformDistribution=[]):
+        self.setTransforms(alpha,theta)
+        SequentialGenerator.build(self,
+                                  self.transforms,
+                                  self.biasSet,
+                                  noiseVariance,
+                                  transformDistribution)
+
+
+    def newDataSet(self,
+                   returnTransforms=False,
+                   nbDataPoints=UNDEFINED,
+                   root=[],
+                   alpha=UNDEFINED,
+                   theta=UNDEFINED):
+        if(alpha &lt;= 0):
+            alpha = self.alpha
+        if(theta &lt;= 0):
+            theta = self.theta
+        if(nbDataPoints &lt; 2):
+            nbDataPoints = self.nbDataPoints
+        if(len(root) != self.dim):
+            root = self.root
+        self.setSpiralParameters(self,nbDataPoints,root,alpha,theta)        
+        return SequentialGenerator.newDataSet(self,
+                                              returnTransforms,
+                                              nbDataPoints,
+                                              root) 
+
+#--------------- LINE GENERATOR ------------------------------------
+class LineGenerator(SequentialGenerator):
+    
+
+    
+    def __init__(self,
+                 tl,
+                 dim=2,
+                 builded=False,
+                 nbDataPoints=40,
+                 increment=[]
+                 ):
+        assert(tl.getOption(&quot;nbTransforms&quot;) == 1)
+        assert(tl.getOption(&quot;withBias&quot;))
+        t = array(zeros(dim*dim),'d')
+        t.resize(dim,dim)
+        transforms = [t]
+        self.transforms = transforms
+        biasSet = array(increment, 'd')
+        biasSet.resize(1,dim)
+        SequentialGenerator__init__(self,
+                                    tl,
+                                    dim,
+                                    builded,
+                                    transforms,
+                                    biasSet)
+
+    def setTransforms(self,
+                      increment):
+        t = array(zeros(self.dim*self.dim),d)
+        t.resize(self.dim,self.dim)
+        transforms = [t]
+        biasSet = array(increment, 'd')
+        biasSet.resize(1,self.dim)
+        SequentialGenerator.setTransforms(self,
+                                          transforms,
+                                          biasSet)
+
+    def build(self,
+              increment,
+              noiseVariance=UNDEFINED,
+              transformDistribution=[]):
+        assert(len(increment)==self.dim)
+        biasSet = array(increment, 'd')
+        biasSet.resize(1,self.dim)
+        SequentialGenerator.build(self,
+                                  [zeros(self.dim,self.dim)],
+                                  biasSet,
+                                  noiseVariance,
+                                  transformDistribution)
+        
+        
+    def newDataSet(self,
+                   returnDataSet,
+                   root=[],
+                   nbDataPoints=UNDEFINED,
+                   increment=array([])):
+        if(len(increment)!= self.dim):
+            increment = copy(self.biasSet[0,:])
+        self.setTransforms(increment)
+        return SequentialGenerator.newDataSet(self,
+                                              returnDataSet,
+                                              root,
+                                              nbDataPoints) 

Added: trunk/scripts/EXPERIMENTAL/iTraining.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/iTraining.py	2007-09-06 20:50:10 UTC (rev 8054)
+++ trunk/scripts/EXPERIMENTAL/iTraining.py	2007-09-07 21:14:30 UTC (rev 8055)
@@ -0,0 +1,602 @@
+#iTraining2.py
+#to control interactively the training process
+
+import os, sys, time, matplotlib, math, numpy ,copy
+
+from matplotlib.pylab import plot,show,draw,close,text,scatter,colorbar
+from matplotlib.colors import *
+
+from plearn.io.server import *
+from plearn.pyplearn import *
+from plearn.plotting import *
+from numpy import *
+from pickle import *
+from copy import *
+
+
+
+
+#RECONSTRUCTION CANDIDATE
+
+#consists in a 4-element tuple:
+#   (target, target&lt;s neighbor, transformation, weight)
+TARGET_IDX = 0
+NEIGHBOR_IDX = 1
+TRANSFORM_IDX = 2
+WEIGHT = 3
+
+
+#OTHER CONSTANTS
+DEFAULT = 0
+UNDEFINED = -1
+EMPTY_BIAS = TMat()
+
+#ITERATIVE LEARNER ----------------------------------------------------------
+class IterativeLearner(object):
+
+
+    #GRAPHICAL OUTPUT FORMAT (constants)
+
+    #a data point is represented graphically by a dot
+    #    his size/shape/color might change according to his nature:
+    #          -target ?
+    #          -neighbor ?
+    #          -ordinary training point ?
+    #          -reconstruction ?
+    #we also need some formats to draw the transformations
+
+    #sizes
+    MIN_SIZE = 10.0
+    MAX_SIZE = 100.0
+    TARGET_SIZE = MAX_SIZE
+    NEIGHBOR_SIZE = MIN_SIZE
+    DEFAULT_SIZE = MIN_SIZE
+    RECONSTRUCTION_SIZE_FACTOR = 1
+
+    #colors
+    TARGET_COLOR = 'b'
+    NEIGHBOR_COLOR = 'k'
+    DEFAULT_COLOR = 'w'
+    RECONSTRUCTION_COLOR = 'y'
+    TRANSFORMATION_COLOR  = 'k'
+
+    #shapes
+    TARGET_SHAPE = 'o'
+    NEIGHBOR_SHAPE = 'o'
+    DEFAULT_SHAPE = 'o'
+    RECONSTRUCTION_SHAPE = 'd'
+
+    #index of a transformation : size of the police
+    TRANSFORMATION_DIGIT_SIZE = 12
+
+    #LEARNER
+    
+    learner = UNDEFINED                    #TransformationLearner object
+    dim = 2                                #dimension of input space
+    nbTransforms = UNDEFINED               #number  of transforms to learn
+    withBias = False                       #includes a bias addition in the transformation function?
+    transformsToLearn = []                 #the transformations matrices to learn
+    biasToLearn = EMPTY_BIAS                #the transformations bias to learn, if any
+    learnedTransforms = []                 #learned transformations matrices
+    learnedBias = EMPTY_BIAS                #learned transformations bias, if any
+    learnNoiseVariance = False             #noise variance = learned parameter ?
+    noiseVariance = UNDEFINED              #noise variance (fixed or learned)
+    noiseVarianceToLearn=UNDEFINED         #noise variance to learn (if any)
+    learnTransformDistribution = False     #transformation distribution = learned parameter ?
+    transformDistribution = []             #transformation distribution (fixed or learned)
+    transformDistributionToLearn = []      #transformation distribution to learn (if any)
+    data = array([])                       #training data points
+    transformFamily=0                      #type of transformation functions used
+    
+
+    #TARGET AND CORRESPONDING RECONSTRUCTION CANDIDATES
+    testTargetIdx = 0
+    target = []
+    neighbors = array([])
+    reconstructions= array([])
+    weights = array([])
+    recSizes = array([])
+    recColors = array([])
+    choosenTransforms = array([])
+
+
+    #INITIALIZATION PROCEDURES
+
+    #constructor
+    def __init__(self,
+                 learner,
+                 dim=2,
+                 testTargetIdx=0,
+                 transformsToLearn=[],
+                 bias=array([]),
+                 noiseVarianceToLearn=UNDEFINED,
+                 transformDistributionToLearn=[],
+                 data=array([])):
+        self.learner = learner
+        self.dim = dim
+        self.nbTransforms = self.learner.getOption(&quot;nbTransforms&quot;)
+        self.withBias = self.learner.getOption(&quot;withBias&quot;)
+        self.transformFamily = self.learner.getOption(&quot;transformFamily&quot;)
+        self.testTargetIdx = testTargetIdx
+        self.learnNoiseVariance = self.learner.getOption(&quot;learnNoiseVariance&quot;)
+        if(self.learnNoiseVariance and noiseVarianceToLearn&gt;0):
+            self.setNoiseVarianceToLearn(noiseVarianceToLearn)
+        else:
+            self.updateNoiseVariance()
+        self.learnTransformDistribution = self.learner.getOption(&quot;learnTransformDistribution&quot;)
+        if(self.learnTransformDistribution and len(transformDistributionToLearn) &gt;0):
+            self.setTransformDistributionToLearn(transformDistributionToLearn)        
+        if(len(transformsToLearn) != 0):
+            self.setTransformsToLearn(transformsToLearn, bias)
+        if(len(data)!=0):
+            self.setTrainingSet(data)
+
+
+    def setNoiseVarianceToLearn(self,
+                                noiseVarianceToLearn):
+        assert(noiseVarianceToLearn&gt;0)
+        self.noiseVarianceToLearn = noiseVarianceToLearn
+
+    def setTransformDistributionToLearn(self,
+                                        transformDistributionToLearn):
+        assert(len(transformDistributionToLearn) == self.nbTransforms)
+        sum = 0
+        for i in range(self.nbTransforms):
+            p = exp(transformDistributionToLearn[i])
+            assert( 0&lt;= p &lt;=1)
+            sum = sum + p
+        assert(sum == 1)
+        self.transformDistributionToLearn = copy(transformDistributionToLearn)
+        
+        
+
+    #specifies the set of transformation functions that might be learned        
+    def setTransformsToLearn(self,
+                             transformsToLearn,
+                             bias=EMPTY_BIAS):
+        assert(len(transformsToLearn)==self.nbTransforms)
+        if(self.withBias):
+            assert(bias.shape[0] == self.nbTransforms)
+            assert(bias.shape[1] == self.dim)
+            self.biasToLearn= bias.copy()
+        else:
+            self.biasToLearn = EMPTY_BIAS
+        for i in range(self.nbTransforms):
+            assert(self.dim == transformsToLearn[i].shape[0])
+            assert(self.dim == transformsToLearn[i].shape[1])
+        self.transformsToLearn = copy(transformsToLearn)
+
+
+    #defines the training set of the learner with the given datas
+    def setTrainingSet(self,
+                       datas):
+        assert(self.dim == datas.shape[1])
+        self.data = copy(datas)
+        trainset = pl.MemoryVMatrix(data=self.data,
+                                    inputsize = self.dim,
+                                    targetsize = 0,
+                                    weightsize = 0,
+                                    length = datas.shape[0],
+                                    width = datas.shape[1])
+        self.learner.setTrainingSet(trainset,True)
+        self.updateLearnedParameters()
+        if(not self.learnTransformDistribution):
+            self.updateTransformDistribution()
+        
+
+
+    #gets the current value of the learner's transformations , noise variance(optional)
+    #and transformation distribution(optional)
+    def updateLearnedParameters(self):
+        self.updateTransforms()
+        if(self.learnNoiseVariance):
+            self.updateNoiseVariance()
+        if(self.learnTransformDistribution):
+            self.updateTransformDistribution()
+
+    
+    #registers the &quot;real values&quot; of the parameters to learn
+    def setParametersToLearn(self,
+                             transformsToLearn,
+                             biasToLearn=array([]),
+                             noiseVarianceToLearn = -1,
+                             transformDistributionToLearn = []):
+        self.setTransformsToLearn(transformsToLearn, biasToLearn)
+        if(self.learnNoiseVariance):
+            self.setNoiseVarianceToLearn(noiseVarianceToLearn)
+        if(self.learnTransformDistribution):
+            self.setTransformDistributionToLearn(transformDistributionToLearn)
+        
+        
+
+    #updates the variables 'learnedTransforms' and 'learnedBias' to ensure that
+    #they correspond to the learner 's transformation matrices and bias 
+    def updateTransforms(self):
+        self.learnedTransforms = self.learner.getOption(&quot;transforms&quot;)
+        if(self.withBias):
+            self.learnedBias = self.learner.getOption(&quot;biasSet&quot;)
+
+
+    #updates the variable 'noiseVariance' to ensure it correspond to the learner's noise variance
+    def updateNoiseVariance(self):
+        self.noiseVariance = self.learner.getOption(&quot;noiseVariance&quot;)
+
+
+    #updates the variable 'transformDistribution' to ensure it correspond to the learner's transformation distribution
+    def updateTransformDistribution(self):
+        self.transformDistribution = self.learner.getOption(&quot;transformDistribution&quot;)
+
+
+    #updates reconstruction datas 
+    def updateReconstructionDatas(self):
+        self.target = self.learner.returnTrainingPoint(self.testTargetIdx)
+        temp = self.learner.returnReconstructionCandidates(self.testTargetIdx)
+        reconstructionCandidates = array(temp, 'd')
+        reconstructionCandidates.resize(len(temp),4)
+        self.weights = copy(reconstructionCandidates[:,WEIGHT])
+        for i in range(self.weights.shape[0]):
+            self.weights[i]=exp(self.weights[i])
+            self.recSizes = (multiply(self.weights,self.MAX_SIZE - self.MIN_SIZE)
+                             +
+                             self.MIN_SIZE)
+        self.choosenTransforms = copy(reconstructionCandidates[:,TRANSFORM_IDX])
+        self.neighbors = self.learner.returnNeighbors(self.testTargetIdx)
+        self.reconstructions = self.learner.returnReconstructions(self.testTargetIdx)
+        
+
+    #extracts the data points from a file and returns them
+    #(in a matricial form)
+    def load_data(self,
+                  filename):
+        data_in = open(filename)
+        (inputsize, targetsize, weightsize) = [int(x) for x in data_in.readline().split()]
+        assert(targetsize == 0)
+        assert(weightsize == 0)
+        data = []
+        n_samples = 0
+        for line in data_in.readlines():
+            data +=[float(x) for x in line.split()]
+            n_samples = n_samples + 1
+        data_in.close()
+        data = array(data, 'd')
+        data.resize(n_samples,inputsize)
+        return data
+
+
+    #sets the training set (extracts first the data points from a file)
+    def setTrainingSetFromFile(self,
+                               filename):
+        self.setTrainingSet(self.load_data(filename))
+
+
+    #re-initializes the present object    
+    def reset(self,
+              dim = 2,
+              testTargetIdx = 0,
+              transformsToLearn=[],
+              biasToLearn=array([]),
+              noiseVarianceToLearn=UNDEFINED,
+              transformDistributionToLearn =[],
+              data=array([])):
+        self.__init__(self.learner,
+                      dim,
+                      testTargetIdx,
+                      transformsToLearn,
+                      biasToLearn,
+                      noiseVarianceToLearn,
+                      transformDistributionToLearn,
+                      data)
+
+
+    #GRAPHICAL PROCEDURES
+    
+    #draws the target test point
+    #(big blue dot)
+    def drawTarget(self):
+        scatter([self.target[0]],
+                [self.target[1]],
+                [self.TARGET_SIZE],
+                c=self.TARGET_COLOR,
+                marker = self.TARGET_SHAPE)
+
+
+    #draws the neighbors associated to the reconstruction candidates
+    #(small red dots)
+    def drawNeighbors(self):
+        scatter(self.neighbors[:,0].tolist(),
+                self.neighbors[:,1].tolist(),
+                self.NEIGHBOR_SIZE,
+                c =  self.NEIGHBOR_COLOR,
+                marker=self.NEIGHBOR_SHAPE)
+
+    #draws all the training data points
+    #(small black circles)
+    def drawTrainingSet(self):
+        scatter(self.data[:,0].tolist(),
+                self.data[:,1].tolist(),
+                self.DEFAULT_SIZE,
+                c = self.DEFAULT_COLOR,
+                marker = self.DEFAULT_SHAPE)
+
+    #draws the reconstructions of the test target point
+    #(yellow diamonds, the most probable the reconstruction, the bigger the diamond shape)
+    def drawReconstructions(self):
+        scatter(self.reconstructions[:,0].tolist(),
+                self.reconstructions[:,1].tolist(),
+                self.recSizes.tolist(),
+                c=self.RECONSTRUCTION_COLOR,
+                marker=self.RECONSTRUCTION_SHAPE)
+
+        
+    #draws the transformations associated to the reconstruction candidates
+    #   -each transformation is represented as an arrow, and an integer
+    #    (the transformation index)
+    def drawChoosenTransforms(self):
+        for i in range(self.reconstructions.shape[0]):
+            mid_X = 0.5*(self.neighbors[i][0] + self.reconstructions[i][0])
+            mid_Y = 0.5*(self.neighbors[i][1] + self.reconstructions[i][1])
+            label = str(int(self.choosenTransforms[i]))
+            text(mid_X,
+                 mid_Y,
+                 label,
+                 fontsize=self.TRANSFORMATION_DIGIT_SIZE)
+            plot([self.neighbors[i][0],self.reconstructions[i][0]],
+                 [self.neighbors[i][1],self.reconstructions[i][1]],
+                 c=self.TRANSFORMATION_COLOR)
+
+
+
+    # draws the graph representing the different reconstruction candidates
+    # of the test  target point
+    def drawLearningGraph(self):
+        clf()
+        self.updateReconstructionDatas()
+        self.drawTrainingSet()
+        self.drawTarget()
+        self.drawNeighbors()
+        self.drawChoosenTransforms()
+        self.drawReconstructions()
+        draw()
+      
+
+    #CONTROL PROCEDURES
+
+
+    #prints the list of control keys
+    def help(self):
+        print &quot;CONTROL PROCEDURES AND CORRESPONDING KEYS:\n&quot;
+        
+        print &quot;printLearnState() ........... 'p'&quot;
+        print &quot;initEStep() ................. 'i'&quot;
+        print &quot;smallEStep() ................ 's'&quot;
+        print &quot;largeEStepA() ............... 'a'&quot;
+        print &quot;largeEStepB() ............... 'b'&quot;
+        print &quot;MStep() ..................... 'm'&quot;
+        print &quot;MStepTransformations() ...... 't'&quot;
+        print &quot;MStepTransformationsDiv() ... 'y'&quot;
+        print &quot;MStepNoiseVariance() ........ 'n'&quot;
+        print &quot;MStepTransformDistribution()  'd'&quot;
+        print &quot;printReconstructionsProbas()  'r'&quot;
+        print &quot;printTransforms() ........... 'z'&quot;
+        print &quot;printNoiseVariance() ........ 'x'&quot;
+        print &quot;printDistribution() ......... 'c'&quot;
+        print &quot;nextStage() ................. ' '&quot;
+        print &quot;routine1() .................. '1'&quot;
+        print &quot;help() ...................... 'h'&quot;
+        
+
+
+
+    r1_NB_ITERATIONS = 1
+    def routine1(self):
+        for i in range(self.r1_NB_ITERATIONS):
+            self.MStep()
+            self.smallEStep()
+            self.nextStage()
+        self.printLearnState()
+    
+
+    #prints the learned parameters and the values of the parameters to learn
+    #(control key == 'p')
+    def printLearnState(self): 
+        self.updateLearnedParameters()
+        n = len(self.transformsToLearn)
+        print &quot;transformations to learn:&quot;
+        for i in range(n):
+            print &quot;\n&quot;
+            print self.transformsToLearn[i]
+            if(self.withBias):
+                print &quot;bias: &quot;, self.biasToLearn[i,:]
+        print &quot;\n&quot;
+        print &quot;learned transformations:&quot;
+        for i in range(n):
+            print &quot;\n&quot;
+            print self.learnedTransforms[i]
+            if(self.withBias):
+                print &quot;bias: &quot;, self.learnedBias[i,:]
+        print &quot;\n&quot;
+        if(self.learnNoiseVariance):
+            print &quot;noise variance to learn:  &quot;, self.noiseVarianceToLearn
+            print &quot;learned noise variance: &quot; , self.noiseVariance
+            print &quot;\n&quot;
+        if(self.learnTransformDistribution):
+            print &quot;transformation distribution to learn, format =(log,proba):&quot;
+            print [[&quot;(&quot;,x,&quot; &quot;, exp(x),&quot;)&quot;] for x in self.transformDistributionToLearn]
+            print &quot;learned transformation distribution (log/proba) :&quot;
+            print [(x, exp(x)) for x in self.transformDistribution]
+            print &quot;\n&quot;
+            
+
+
+    #initEStep  (control key == 'i')
+    def initEStep(self):
+        assert(len(self.data!= 0))
+        #print &quot;** initEStep **&quot;
+        self.learner.initEStep()
+        self.updateTransforms()
+    
+        
+    #smallEStep   (control key == 's')
+    def smallEStep(self):   
+        #print &quot;** smallEStep **&quot;
+        self.learner.smallEStep()
+        self.printReconstructionsProbas()
+
+
+    #largeEStepA   (control key == 'a')
+    def largeEStepA(self):
+        #print &quot;** largeEStepA **&quot;
+        self.learner.largeEStepA()
+        self.printReconstructionsProbas()
+     
+
+    #largeEStepB   (control key == 'b')
+    def largeEStepB(self):
+        #print &quot;** largeEStepB **&quot;
+        self.learner.largeEStepB()
+        self.printReconstructionsProbas()
+
+
+    #MStep (control key == 'm')
+    def MStep(self):
+        #print &quot;** MStep **&quot;
+        self.learner.MStep()
+        #self.printLearnState()
+
+    #MStepTransformations (control key == 'T')
+    def MStepTransformations(self):
+        #print &quot;** MStepTransformations **&quot;
+        self.learner.MStepTransformations()
+        self.printTransforms()
+
+    mstd_t = 0
+    #MStepTransformationDiv (control key == 'H')
+    def MStepTransformationDiv(self):
+        #print &quot;** MStepTransformations**&quot;
+        #print &quot;transform: &quot;, mstd_t
+        self.learner.MStepTransformationDiv(self.mstd_t)
+        self.mstd_t = (self.mstd_t + 1) % self.nbTransforms
+
+    #MStepNoiseVariance (control key == 'N')
+    def MStepNoiseVariance(self):
+        #print &quot;** MStepNoiseVariance **&quot;
+        self.learner.MStepNoiseVariance()
+        self.printNoiseVariance()
+
+    #MStepDistribution (control key == 'D')
+    def MStepTransformDistribution(self):
+        #print &quot;** MStepDistribution **&quot;
+        self.learner.MStepTransformDistribution()
+        self.printLearnState()
+
+    #prints the  probabilities of the reconstructions associated to the present test target point
+    #(control key == r)
+    def printReconstructionsProbas(self):
+        self.updateReconstructionDatas()
+        print &quot;reconstructions and their weights (weight format : (log,proba))&quot;
+        for i in range(self.reconstructions.shape[0]):
+            print  self.reconstructions[i], &quot; (&quot; , log(self.weights[i]),&quot;, &quot;, self.weights[i], &quot;)&quot;
+        print &quot;\n&quot;
+    
+    #prints the current learned transformations (control key == 't')
+    def printTransforms(self):
+        self.updateLearnedParameters()
+        print &quot;current transformations:\n&quot;
+        for i in range(self.nbTransforms):
+            print self.learnedTransforms[i]
+            if(self.withBias):
+                print self.learnedBias[i,:]
+            print &quot;\n&quot;
+        
+    #prints the value of the learner's noise variance (control key == 'n')
+    def printNoiseVariance(self):
+        self.updateLearnedParameters()
+        print &quot;noise variance: &quot; , self.noiseVariance , &quot;\n&quot;
+        
+
+    #prints the value of the learner's transformation distribution
+    #(control key == 'd')
+    def printDistribution(self):
+        self.updateLearnedParameters()
+        print &quot;transformation distribution (log, proba): \n&quot;
+        print [(x, exp(x)) for x in self.transformDistribution ]
+        print &quot;\n&quot;
+
+
+    #increment the learner variable 'stages' of 1 (control key == 'n')
+    def nextStage(self):
+        self.learner.nextStage()
+
+
+    #GENERAL USE PROCEDURES
+
+
+    #returns the square euclidean distance between data points x and y 
+    def squareEuclideanDistance(self,x,y):
+        return pow(x[0] - y[0],2) + pow(x[1] - y[1],2)   
+    
+
+    #RUN PROCEDURE (main)
+
+    i=1
+    def run(self):
+        self.learner.buildLearnedParameters()
+        self.initEStep()
+        self.drawLearningGraph()
+        self.i = 1
+        def mouse_press(event):
+            if(event.button == 2):
+                p = [event.xdata,event.ydata]
+                min_idx = 0
+                min_d = self.squareEuclideanDistance(self.data[0],p)
+                for i in range(1,len(self.data)):
+                    d = self.squareEuclideanDistance(self.data[i],p)
+                    if(d&lt; min_d):
+                        min_d = d
+                        min_idx = i
+                self.testTargetIdx = min_idx
+                self.target = self.data[min_idx]
+                print &quot;new target: &quot;, min_idx, &quot;\n&quot;
+                self.drawLearningGraph()
+        
+        def key_press(event):
+            if(event.key == '1'):
+                self.routine1()
+            if(event.key == 'p' ):
+                self.printLearnState()
+            if(event.key == 'i'):
+                self.initEStep()
+            if(event.key == 's'):
+                self.smallEStep()
+            if(event.key == 'a'):
+                self.largeEStepA()
+            if(event.key == 'b'):
+
+                self.largeEStepB()
+            if(event.key == 'm'):
+                self.MStep()
+            if(event.key == 't' ):
+                self.MStepTransformations()
+            if(event.key == 'n'):
+                self.MStepNoiseVariance()
+            if(event.key == 'd'):
+                self.MStepTransformDistribution()
+            if(event.key == 'y'):
+                self.MStepTransformationDiv()
+            if(event.key == 'r'):
+                self.printReconstructionsProbas()
+            if(event.key == 'z'):
+                self.printTransforms()
+            if(event.key == 'x'):
+                self.printNoiseVariance()
+            if(event.key == 'c' ):
+                self.printDistribution()
+            if(event.key == 'h'):
+                self.help()
+            if(event.key == ' '):
+                self.nextStage()
+            self.drawLearningGraph()
+            
+        connect('button_press_event',mouse_press)
+        connect('key_press_event', key_press)
+        show()   

Added: trunk/scripts/EXPERIMENTAL/itest.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/itest.py	2007-09-06 20:50:10 UTC (rev 8054)
+++ trunk/scripts/EXPERIMENTAL/itest.py	2007-09-07 21:14:30 UTC (rev 8055)
@@ -0,0 +1,770 @@
+import os ,sys, time, matplotlib, math, copy
+
+from matplotlib.pylab import *
+from matplotlib.colors import *
+from numpy.numarray import *
+# from numarray import *
+# from numarray.linear_algebra import *
+
+from plearn.io.server import *
+from plearn.pyplearn import *
+from plearn.plotting import *
+from copy import *
+
+from iTraining import *
+from generators import *
+from TLTester import *
+#exec open(&quot;iTraining.py&quot;).read()
+#exec open(&quot;generators.py&quot;).read()
+#exec open(&quot;TLTester.py&quot;).read()
+
+server_command = 'plearn_exp server'
+serv = launch_plearn_server(command = server_command)
+
+#iTraining2.py
+#to control interactively the training process
+
+import os, sys, time, matplotlib, math, numpy ,copy
+
+from matplotlib.pylab import plot,show,draw,close,text,scatter,colorbar
+from matplotlib.colors import *
+
+from plearn.io.server import *
+from plearn.pyplearn import *
+from plearn.plotting import *
+from numpy import *
+from pickle import *
+from copy import *
+
+
+
+
+#RECONSTRUCTION CANDIDATE
+
+#consists in a 4-element tuple:
+#   (target, target&lt;s neighbor, transformation, weight)
+TARGET_IDX = 0
+NEIGHBOR_IDX = 1
+TRANSFORM_IDX = 2
+WEIGHT = 3
+
+
+#OTHER CONSTANTS
+DEFAULT = 0
+UNDEFINED = -1
+EMPTY_BIAS = TMat()
+
+#ITERATIVE LEARNER ----------------------------------------------------------
+class IterativeLearner(object):
+
+
+    #GRAPHICAL OUTPUT FORMAT (constants)
+
+    #a data point is represented graphically by a dot
+    #    his size/shape/color might change according to his nature:
+    #          -target ?
+    #          -neighbor ?
+    #          -ordinary training point ?
+    #          -reconstruction ?
+    #we also need some formats to draw the transformations
+
+    #sizes
+    MIN_SIZE = 10.0
+    MAX_SIZE = 100.0
+    TARGET_SIZE = MAX_SIZE
+    NEIGHBOR_SIZE = MIN_SIZE
+    DEFAULT_SIZE = MIN_SIZE
+    RECONSTRUCTION_SIZE_FACTOR = 1
+
+    #colors
+    TARGET_COLOR = 'b'
+    NEIGHBOR_COLOR = 'k'
+    DEFAULT_COLOR = 'w'
+    RECONSTRUCTION_COLOR = 'y'
+    TRANSFORMATION_COLOR  = 'k'
+
+    #shapes
+    TARGET_SHAPE = 'o'
+    NEIGHBOR_SHAPE = 'o'
+    DEFAULT_SHAPE = 'o'
+    RECONSTRUCTION_SHAPE = 'd'
+
+    #index of a transformation : size of the police
+    TRANSFORMATION_DIGIT_SIZE = 12
+
+    #LEARNER
+    
+    learner = UNDEFINED                    #TransformationLearner object
+    dim = 2                                #dimension of input space
+    nbTransforms = UNDEFINED               #number  of transforms to learn
+    withBias = False                       #includes a bias addition in the transformation function?
+    transformsToLearn = []                 #the transformations matrices to learn
+    biasToLearn = EMPTY_BIAS                #the transformations bias to learn, if any
+    learnedTransforms = []                 #learned transformations matrices
+    learnedBias = EMPTY_BIAS                #learned transformations bias, if any
+    learnNoiseVariance = False             #noise variance = learned parameter ?
+    noiseVariance = UNDEFINED              #noise variance (fixed or learned)
+    noiseVarianceToLearn=UNDEFINED         #noise variance to learn (if any)
+    learnTransformDistribution = False     #transformation distribution = learned parameter ?
+    transformDistribution = []             #transformation distribution (fixed or learned)
+    transformDistributionToLearn = []      #transformation distribution to learn (if any)
+    data = array([])                       #training data points
+    transformFamily=0                      #type of transformation functions used
+    
+
+    #TARGET AND CORRESPONDING RECONSTRUCTION CANDIDATES
+    testTargetIdx = 0
+    target = []
+    neighbors = array([])
+    reconstructions= array([])
+    weights = array([])
+    recSizes = array([])
+    recColors = array([])
+    choosenTransforms = array([])
+
+
+    #INITIALIZATION PROCEDURES
+
+    #constructor
+    def __init__(self,
+                 learner,
+                 dim=2,
+                 testTargetIdx=0,
+                 transformsToLearn=[],
+                 bias=array([]),
+                 noiseVarianceToLearn=UNDEFINED,
+                 transformDistributionToLearn=[],
+                 data=array([])):
+        self.learner = learner
+        self.dim = dim
+        self.nbTransforms = self.learner.getOption(&quot;nbTransforms&quot;)
+        self.withBias = self.learner.getOption(&quot;withBias&quot;)
+        self.transformFamily = self.learner.getOption(&quot;transformFamily&quot;)
+        self.testTargetIdx = testTargetIdx
+        self.learnNoiseVariance = self.learner.getOption(&quot;learnNoiseVariance&quot;)
+        if(self.learnNoiseVariance and noiseVarianceToLearn&gt;0):
+            self.setNoiseVarianceToLearn(noiseVarianceToLearn)
+        else:
+            self.updateNoiseVariance()
+        self.learnTransformDistribution = self.learner.getOption(&quot;learnTransformDistribution&quot;)
+        if(self.learnTransformDistribution and len(transformDistributionToLearn) &gt;0):
+            self.setTransformDistributionToLearn(transformDistributionToLearn)        
+        if(len(transformsToLearn) != 0):
+            self.setTransformsToLearn(transformsToLearn, bias)
+        if(len(data)!=0):
+            self.setTrainingSet(data)
+
+
+    def setNoiseVarianceToLearn(self,
+                                noiseVarianceToLearn):
+        assert(noiseVarianceToLearn&gt;0)
+        self.noiseVarianceToLearn = noiseVarianceToLearn
+
+    def setTransformDistributionToLearn(self,
+                                        transformDistributionToLearn):
+        assert(len(transformDistributionToLearn) == self.nbTransforms)
+        sum = 0
+        for i in range(self.nbTransforms):
+            p = exp(transformDistributionToLearn[i])
+            assert( 0&lt;= p &lt;=1)
+            sum = sum + p
+        assert(sum == 1)
+        self.transformDistributionToLearn = copy(transformDistributionToLearn)
+        
+        
+
+    #specifies the set of transformation functions that might be learned        
+    def setTransformsToLearn(self,
+                             transformsToLearn,
+                             bias=EMPTY_BIAS):
+        assert(len(transformsToLearn)==self.nbTransforms)
+        if(self.withBias):
+            assert(bias.shape[0] == self.nbTransforms)
+            assert(bias.shape[1] == self.dim)
+            self.biasToLearn= bias.copy()
+        else:
+            self.biasToLearn = EMPTY_BIAS
+        for i in range(self.nbTransforms):
+            assert(self.dim == transformsToLearn[i].shape[0])
+            assert(self.dim == transformsToLearn[i].shape[1])
+        self.transformsToLearn = copy(transformsToLearn)
+
+
+    #defines the training set of the learner with the given datas
+    def setTrainingSet(self,
+                       datas):
+        assert(self.dim == datas.shape[1])
+        self.data = copy(datas)
+        trainset = pl.MemoryVMatrix(data=self.data,
+                                    inputsize = self.dim,
+                                    targetsize = 0,
+                                    weightsize = 0,
+                                    length = datas.shape[0],
+                                    width = datas.shape[1])
+        self.learner.setTrainingSet(trainset,True)
+        self.updateLearnedParameters()
+        if(not self.learnTransformDistribution):
+            self.updateTransformDistribution()
+        
+
+
+    #gets the current value of the learner's transformations , noise variance(optional)
+    #and transformation distribution(optional)
+    def updateLearnedParameters(self):
+        self.updateTransforms()
+        if(self.learnNoiseVariance):
+            self.updateNoiseVariance()
+        if(self.learnTransformDistribution):
+            self.updateTransformDistribution()
+
+    
+    #registers the &quot;real values&quot; of the parameters to learn
+    def setParametersToLearn(self,
+                             transformsToLearn,
+                             biasToLearn=array([]),
+                             noiseVarianceToLearn = -1,
+                             transformDistributionToLearn = []):
+        self.setTransformsToLearn(transformsToLearn, biasToLearn)
+        if(self.learnNoiseVariance):
+            self.setNoiseVarianceToLearn(noiseVarianceToLearn)
+        if(self.learnTransformDistribution):
+            self.setTransformDistributionToLearn(transformDistributionToLearn)
+        
+        
+
+    #updates the variables 'learnedTransforms' and 'learnedBias' to ensure that
+    #they correspond to the learner 's transformation matrices and bias 
+    def updateTransforms(self):
+        self.learnedTransforms = self.learner.getOption(&quot;transforms&quot;)
+        if(self.withBias):
+            self.learnedBias = self.learner.getOption(&quot;biasSet&quot;)
+
+
+    #updates the variable 'noiseVariance' to ensure it correspond to the learner's noise variance
+    def updateNoiseVariance(self):
+        self.noiseVariance = self.learner.getOption(&quot;noiseVariance&quot;)
+
+
+    #updates the variable 'transformDistribution' to ensure it correspond to the learner's transformation distribution
+    def updateTransformDistribution(self):
+        self.transformDistribution = self.learner.getOption(&quot;transformDistribution&quot;)
+
+
+    #updates reconstruction datas 
+    def updateReconstructionDatas(self):
+        self.target = self.learner.returnTrainingPoint(self.testTargetIdx)
+        temp = self.learner.returnReconstructionCandidates(self.testTargetIdx)
+        reconstructionCandidates = array(temp, 'd')
+        reconstructionCandidates.resize(len(temp),4)
+        self.weights = copy(reconstructionCandidates[:,WEIGHT])
+        for i in range(self.weights.shape[0]):
+            self.weights[i]=exp(self.weights[i])
+            self.recSizes = (multiply(self.weights,self.MAX_SIZE - self.MIN_SIZE)
+                             +
+                             self.MIN_SIZE)
+        self.choosenTransforms = copy(reconstructionCandidates[:,TRANSFORM_IDX])
+        self.neighbors = self.learner.returnNeighbors(self.testTargetIdx)
+        self.reconstructions = self.learner.returnReconstructions(self.testTargetIdx)
+        
+
+    #extracts the data points from a file and returns them
+    #(in a matricial form)
+    def load_data(self,
+                  filename):
+        data_in = open(filename)
+        (inputsize, targetsize, weightsize) = [int(x) for x in data_in.readline().split()]
+        assert(targetsize == 0)
+        assert(weightsize == 0)
+        data = []
+        n_samples = 0
+        for line in data_in.readlines():
+            data +=[float(x) for x in line.split()]
+            n_samples = n_samples + 1
+        data_in.close()
+        data = array(data, 'd')
+        data.resize(n_samples,inputsize)
+        return data
+
+
+    #sets the training set (extracts first the data points from a file)
+    def setTrainingSetFromFile(self,
+                               filename):
+        self.setTrainingSet(self.load_data(filename))
+
+
+    #re-initializes the present object    
+    def reset(self,
+              dim = 2,
+              testTargetIdx = 0,
+              transformsToLearn=[],
+              biasToLearn=array([]),
+              noiseVarianceToLearn=UNDEFINED,
+              transformDistributionToLearn =[],
+              data=array([])):
+        self.__init__(self.learner,
+                      dim,
+                      testTargetIdx,
+                      transformsToLearn,
+                      biasToLearn,
+                      noiseVarianceToLearn,
+                      transformDistributionToLearn,
+                      data)
+
+
+    #GRAPHICAL PROCEDURES
+    
+    #draws the target test point
+    #(big blue dot)
+    def drawTarget(self):
+        scatter([self.target[0]],
+                [self.target[1]],
+                [self.TARGET_SIZE],
+                c=self.TARGET_COLOR,
+                marker = self.TARGET_SHAPE)
+
+
+    #draws the neighbors associated to the reconstruction candidates
+    #(small red dots)
+    def drawNeighbors(self):
+        scatter(self.neighbors[:,0].tolist(),
+                self.neighbors[:,1].tolist(),
+                self.NEIGHBOR_SIZE,
+                c =  self.NEIGHBOR_COLOR,
+                marker=self.NEIGHBOR_SHAPE)
+
+    #draws all the training data points
+    #(small black circles)
+    def drawTrainingSet(self):
+        scatter(self.data[:,0].tolist(),
+                self.data[:,1].tolist(),
+                self.DEFAULT_SIZE,
+                c = self.DEFAULT_COLOR,
+                marker = self.DEFAULT_SHAPE)
+
+    #draws the reconstructions of the test target point
+    #(yellow diamonds, the most probable the reconstruction, the bigger the diamond shape)
+    def drawReconstructions(self):
+        scatter(self.reconstructions[:,0].tolist(),
+                self.reconstructions[:,1].tolist(),
+                self.recSizes.tolist(),
+                c=self.RECONSTRUCTION_COLOR,
+                marker=self.RECONSTRUCTION_SHAPE)
+
+        
+    #draws the transformations associated to the reconstruction candidates
+    #   -each transformation is represented as an arrow, and an integer
+    #    (the transformation index)
+    def drawChoosenTransforms(self):
+        for i in range(self.reconstructions.shape[0]):
+            mid_X = 0.5*(self.neighbors[i][0] + self.reconstructions[i][0])
+            mid_Y = 0.5*(self.neighbors[i][1] + self.reconstructions[i][1])
+            label = str(int(self.choosenTransforms[i]))
+            text(mid_X,
+                 mid_Y,
+                 label,
+                 fontsize=self.TRANSFORMATION_DIGIT_SIZE)
+            plot([self.neighbors[i][0],self.reconstructions[i][0]],
+                 [self.neighbors[i][1],self.reconstructions[i][1]],
+                 c=self.TRANSFORMATION_COLOR)
+
+
+
+    # draws the graph representing the different reconstruction candidates
+    # of the test  target point
+    def drawLearningGraph(self):
+        clf()
+        self.updateReconstructionDatas()
+        self.drawTrainingSet()
+        self.drawTarget()
+        self.drawNeighbors()
+        self.drawChoosenTransforms()
+        self.drawReconstructions()
+        draw()
+      
+
+    #CONTROL PROCEDURES
+
+
+    #prints the list of control keys
+    def help(self):
+        print &quot;CONTROL PROCEDURES AND CORRESPONDING KEYS:\n&quot;
+        
+        print &quot;printLearnState() ........... 'p'&quot;
+        print &quot;initEStep() ................. 'i'&quot;
+        print &quot;smallEStep() ................ 's'&quot;
+        print &quot;largeEStepA() ............... 'a'&quot;
+        print &quot;largeEStepB() ............... 'b'&quot;
+        print &quot;MStep() ..................... 'm'&quot;
+        print &quot;MStepTransformations() ...... 't'&quot;
+        print &quot;MStepTransformationsDiv() ... 'y'&quot;
+        print &quot;MStepNoiseVariance() ........ 'n'&quot;
+        print &quot;MStepTransformDistribution()  'd'&quot;
+        print &quot;printReconstructionsProbas()  'r'&quot;
+        print &quot;printTransforms() ........... 'z'&quot;
+        print &quot;printNoiseVariance() ........ 'x'&quot;
+        print &quot;printDistribution() ......... 'c'&quot;
+        print &quot;nextStage() ................. ' '&quot;
+        print &quot;routine1() .................. '1'&quot;
+        print &quot;help() ...................... 'h'&quot;
+        
+
+
+
+    r1_NB_ITERATIONS = 1
+    def routine1(self):
+        for i in range(self.r1_NB_ITERATIONS):
+            self.MStep()
+            self.smallEStep()
+            self.nextStage()
+        self.printLearnState()
+    
+
+    #prints the learned parameters and the values of the parameters to learn
+    #(control key == 'p')
+    def printLearnState(self): 
+        self.updateLearnedParameters()
+        n = len(self.transformsToLearn)
+        print &quot;transformations to learn:&quot;
+        for i in range(n):
+            print &quot;\n&quot;
+            print self.transformsToLearn[i]
+            if(self.withBias):
+                print &quot;bias: &quot;, self.biasToLearn[i,:]
+        print &quot;\n&quot;
+        print &quot;learned transformations:&quot;
+        for i in range(n):
+            print &quot;\n&quot;
+            print self.learnedTransforms[i]
+            if(self.withBias):
+                print &quot;bias: &quot;, self.learnedBias[i,:]
+        print &quot;\n&quot;
+        if(self.learnNoiseVariance):
+            print &quot;noise variance to learn:  &quot;, self.noiseVarianceToLearn
+            print &quot;learned noise variance: &quot; , self.noiseVariance
+            print &quot;\n&quot;
+        if(self.learnTransformDistribution):
+            print &quot;transformation distribution to learn, format =(log,proba):&quot;
+            print [[&quot;(&quot;,x,&quot; &quot;, exp(x),&quot;)&quot;] for x in self.transformDistributionToLearn]
+            print &quot;learned transformation distribution (log/proba) :&quot;
+            print [(x, exp(x)) for x in self.transformDistribution]
+            print &quot;\n&quot;
+            
+
+
+    #initEStep  (control key == 'i')
+    def initEStep(self):
+        assert(len(self.data!= 0))
+        #print &quot;** initEStep **&quot;
+        self.learner.initEStep()
+        self.updateTransforms()
+    
+        
+    #smallEStep   (control key == 's')
+    def smallEStep(self):   
+        #print &quot;** smallEStep **&quot;
+        self.learner.smallEStep()
+        self.printReconstructionsProbas()
+
+
+    #largeEStepA   (control key == 'a')
+    def largeEStepA(self):
+        #print &quot;** largeEStepA **&quot;
+        self.learner.largeEStepA()
+        self.printReconstructionsProbas()
+     
+
+    #largeEStepB   (control key == 'b')
+    def largeEStepB(self):
+        #print &quot;** largeEStepB **&quot;
+        self.learner.largeEStepB()
+        self.printReconstructionsProbas()
+
+
+    #MStep (control key == 'm')
+    def MStep(self):
+        #print &quot;** MStep **&quot;
+        self.learner.MStep()
+        #self.printLearnState()
+
+    #MStepTransformations (control key == 'T')
+    def MStepTransformations(self):
+        #print &quot;** MStepTransformations **&quot;
+        self.learner.MStepTransformations()
+        self.printTransforms()
+
+    mstd_t = 0
+    #MStepTransformationDiv (control key == 'H')
+    def MStepTransformationDiv(self):
+        #print &quot;** MStepTransformations**&quot;
+        #print &quot;transform: &quot;, mstd_t
+        self.learner.MStepTransformationDiv(self.mstd_t)
+        self.mstd_t = (self.mstd_t + 1) % self.nbTransforms
+
+    #MStepNoiseVariance (control key == 'N')
+    def MStepNoiseVariance(self):
+        #print &quot;** MStepNoiseVariance **&quot;
+        self.learner.MStepNoiseVariance()
+        self.printNoiseVariance()
+
+    #MStepDistribution (control key == 'D')
+    def MStepTransformDistribution(self):
+        #print &quot;** MStepDistribution **&quot;
+        self.learner.MStepTransformDistribution()
+        self.printLearnState()
+
+    #prints the  probabilities of the reconstructions associated to the present test target point
+    #(control key == r)
+    def printReconstructionsProbas(self):
+        self.updateReconstructionDatas()
+        print &quot;reconstructions and their weights (weight format : (log,proba))&quot;
+        for i in range(self.reconstructions.shape[0]):
+            print  self.reconstructions[i], &quot; (&quot; , log(self.weights[i]),&quot;, &quot;, self.weights[i], &quot;)&quot;
+        print &quot;\n&quot;
+    
+    #prints the current learned transformations (control key == 't')
+    def printTransforms(self):
+        self.updateLearnedParameters()
+        print &quot;current transformations:\n&quot;
+        for i in range(self.nbTransforms):
+            print self.learnedTransforms[i]
+            if(self.withBias):
+                print self.learnedBias[i,:]
+            print &quot;\n&quot;
+        
+    #prints the value of the learner's noise variance (control key == 'n')
+    def printNoiseVariance(self):
+        self.updateLearnedParameters()
+        print &quot;noise variance: &quot; , self.noiseVariance , &quot;\n&quot;
+        
+
+    #prints the value of the learner's transformation distribution
+    #(control key == 'd')
+    def printDistribution(self):
+        self.updateLearnedParameters()
+        print &quot;transformation distribution (log, proba): \n&quot;
+        print [(x, exp(x)) for x in self.transformDistribution ]
+        print &quot;\n&quot;
+
+
+    #increment the learner variable 'stages' of 1 (control key == 'n')
+    def nextStage(self):
+        self.learner.nextStage()
+
+
+    #GENERAL USE PROCEDURES
+
+
+    #returns the square euclidean distance between data points x and y 
+    def squareEuclideanDistance(self,x,y):
+        return pow(x[0] - y[0],2) + pow(x[1] - y[1],2)   
+    
+
+    #RUN PROCEDURE (main)
+
+    i=1
+    def run(self):
+        self.learner.buildLearnedParameters()
+        self.initEStep()
+        self.drawLearningGraph()
+        self.i = 1
+        def mouse_press(event):
+            if(event.button == 2):
+                p = [event.xdata,event.ydata]
+                min_idx = 0
+                min_d = self.squareEuclideanDistance(self.data[0],p)
+                for i in range(1,len(self.data)):
+                    d = self.squareEuclideanDistance(self.data[i],p)
+                    if(d&lt; min_d):
+                        min_d = d
+                        min_idx = i
+                self.testTargetIdx = min_idx
+                self.target = self.data[min_idx]
+                print &quot;new target: &quot;, min_idx, &quot;\n&quot;
+                self.drawLearningGraph()
+        
+        def key_press(event):
+            if(event.key == '1'):
+                self.routine1()
+            if(event.key == 'p' ):
+                self.printLearnState()
+            if(event.key == 'i'):
+                self.initEStep()
+            if(event.key == 's'):
+                self.smallEStep()
+            if(event.key == 'a'):
+                self.largeEStepA()
+            if(event.key == 'b'):
+
+                self.largeEStepB()
+            if(event.key == 'm'):
+                self.MStep()
+            if(event.key == 't' ):
+                self.MStepTransformations()
+            if(event.key == 'n'):
+                self.MStepNoiseVariance()
+            if(event.key == 'd'):
+                self.MStepTransformDistribution()
+            if(event.key == 'y'):
+                self.MStepTransformationDiv()
+            if(event.key == 'r'):
+                self.printReconstructionsProbas()
+            if(event.key == 'z'):
+                self.printTransforms()
+            if(event.key == 'x'):
+                self.printNoiseVariance()
+            if(event.key == 'c' ):
+                self.printDistribution()
+            if(event.key == 'h'):
+                self.help()
+            if(event.key == ' '):
+                self.nextStage()
+            self.drawLearningGraph()
+            
+        connect('button_press_event',mouse_press)
+        connect('key_press_event', key_press)
+        show()   
+DEFAULT = 0
+BEHAVIOR_LEARNER = 0
+BEHAVIOR_GENERATOR = 1
+
+DEFAULT_DIM = 2
+
+
+generator_TRANSFORMS = []
+generator_BIAS_SET = array([])
+generator_NOISE_VARIANCE = 0.0001
+generator_TRANSFORM_DISTRIBUTION = []
+
+#parameters of a tree generator
+generator_MODE_TREE = 0
+generator_ROOT = [] #[1,1]
+generator_DEEPNESS = 3
+generator_BRANCHING_FACTOR = 3
+generator_TREE_TRANSFORMS = []
+generator_TREE_BIAS_SET = array([])
+generator_TREE_NOISE_VARIANCE = 0.00001
+generator_TREE_TRANSFORM_DISTRIBUTION = []
+
+#parameters of a sequential generator
+generator_MODE_SEQUENTIAL = 1
+generator_START = [1,1]
+generator_SEQUENCE_LENGTH = 40
+
+
+#parameters of a circle generator
+generator_MODE_CIRCLE = 2
+generator_CENTER = [0,0]
+generator_NB_CIRCLE_POINTS = 40
+generator_RAY = 10
+
+#parameters of a spiral generator
+generator_MODE_SPIRAL = 3
+generator_SPIRAL_ROOT = [1,1]
+generator_ALPHA = 1.01
+generator_THETA = 0.1
+generator_NB_SPIRAL_POINTS = 40
+
+
+
+#We suppose that the noisePrecision follows a gamma distribution
+#with parameters alpha, and beta
+#      (reminds that noisePrecision = 1/noiseVariance)
+#
+#Accorging to that distribution:
+#          E(noisePrecision) = alpha/beta
+#          Var(noisePrecision)=alpha/(beta^2)
+#
+#Given those 2 last values, we can deduce the value of alpha and beta:
+#
+#          beta = mean/var
+#          alpha = (mean^2)/var
+#
+#It is what the following procedure is doing:
+# -find alpha and beta,
+# and then returns them in a pair
+def computeNoiseVarianceParameters(mean,var):
+    beta = (1.0*mean)/var
+    alpha = mean*beta
+    return [alpha,beta]
+
+
+
+
+ALPHA = 0
+BETA = 1
+#learner_NOISE_PRECISION_MEAN = 1.0/0.0001
+#learner_NOISE_PRECISION_VAR = 1.0
+#learner_NOISE_DISTRIBUTION_PARAMETERS = (UNDEFINED,UNDEFINED)
+#if(learner_NOISE_PRECISION_MEAN &gt; 0 and learner_NOISE_PRECISION_VAR &gt; 0):
+#   learner_NOISE_DISTRIBUTION_PARAMETERS = computeNoiseVarianceParameters(learner_NOISE_PRECISION_MEAN,
+#                                                                           learner_NOISE_PRECISION_VAR)
+#print &quot;noise distribution parameters: &quot;
+#print learner_NOISE_DISTRIBUTION_PARAMETERS
+learnerSpec = pl.TransformationLearner(
+    behavior = BEHAVIOR_LEARNER,
+    transformFamily = LINEAR_INCREMENT,
+    withBias = False,
+    learnNoiseVariance = True,
+    regOnNoiseVariance = False,
+    emphasisOnDiversity=True,
+    diversityFactor=0.25,
+    noiseAlpha =  1,
+    noiseBeta = 1,
+    learnTransformDistribution = False,
+    regOnTransformDistribution = False,
+    transformDistributionAlpha = 2,
+    noiseVariance =3.,
+    transformsVariance =4.0 ,
+    nbTransforms = 2,
+    nbNeighbors = 2,
+    initializationMode = DEFAULT)
+learner = serv.new(learnerSpec)
+iLearner = IterativeLearner(learner)
+
+
+generatorSpec = pl.TransformationLearner(
+    behavior = BEHAVIOR_GENERATOR,
+    transformFamily = LINEAR_INCREMENT,
+    withBias = False,
+    noiseAlpha = UNDEFINED,
+    noiseBeta = UNDEFINED,
+    transformDistributionAlpha = UNDEFINED,
+    noiseVariance = generator_NOISE_VARIANCE,
+    transformsVariance = 1,
+    nbTransforms = 1,
+    nbNeighbors = 1)
+
+gen = serv.new(generatorSpec)
+
+
+generatorMode = generator_MODE_CIRCLE
+
+#generator = TreeGenerator(gen,
+ #                         DEFAULT_DIM,
+  #                        False,
+   #                       generator_DEEPNESS,
+    #                      generator_BRANCHING_FACTOR,
+     #                     generator_ROOT)
+#generator= SequentialGenerator(gen,
+ #                              DEFAULT_DIM,
+  #                             False,
+   #                            generator_SEQUENCE_LENGTH,
+    #                           generator_START
+     #                          )
+
+generator = CircleGenerator(gen,
+                            False,
+                            generator_NB_CIRCLE_POINTS,
+                            generator_CENTER,
+                            generator_RAY)
+#generator = SpiralGenerator(gen,
+ #                           False,
+  #                          generator_NB_SPIRAL_POINTS,
+   #                         generator_SPIRAL_ROOT,
+    #                        generator_ALPHA,
+     #                       generator_THETA)
+
+tester = TLTester(iLearner,generator)
+if(generator_NOISE_VARIANCE &gt; 0):
+    generator.setNoiseVariance(generator_NOISE_VARIANCE)
+tester.run()

Added: trunk/scripts/EXPERIMENTAL/itest2.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/itest2.py	2007-09-06 20:50:10 UTC (rev 8054)
+++ trunk/scripts/EXPERIMENTAL/itest2.py	2007-09-07 21:14:30 UTC (rev 8055)
@@ -0,0 +1,169 @@
+import os ,sys, time, matplotlib, math, copy
+
+from matplotlib.pylab import *
+from matplotlib.colors import *
+from numpy.numarray import *
+# from numarray import *
+#from numarray.linear_algebra import *
+
+from plearn.io.server import *
+from plearn.pyplearn import *
+from plearn.plotting import *
+from copy import *
+
+from iTraining import *
+from generators import *
+from TLTester import *
+#exec open(&quot;iTraining.py&quot;).read()
+#exec open(&quot;generators.py&quot;).read()
+#exec open(&quot;TLTester.py&quot;).read()
+
+server_command = 'plearn_exp server'
+serv = launch_plearn_server(command = server_command)
+
+LINEAR = 0
+LINEAR_INCREMENT = 1
+UNDEFINED = -1
+DEFAULT = 0
+BEHAVIOR_LEARNER = 0
+BEHAVIOR_GENERATOR = 1
+
+DEFAULT_DIM = 2
+
+
+generator_TRANSFORMS = []
+generator_BIAS_SET = array([])
+generator_NOISE_VARIANCE = 0.0001
+generator_TRANSFORM_DISTRIBUTION = []
+
+#parameters of a tree generator
+generator_MODE_TREE = 0
+generator_ROOT = [] #[1,1]
+generator_DEEPNESS = 3
+generator_BRANCHING_FACTOR = 3
+generator_TREE_TRANSFORMS = []
+generator_TREE_BIAS_SET = array([])
+generator_TREE_NOISE_VARIANCE = 0.00001
+generator_TREE_TRANSFORM_DISTRIBUTION = []
+
+#parameters of a sequential generator
+generator_MODE_SEQUENTIAL = 1
+generator_START = [1,1]
+generator_SEQUENCE_LENGTH = 40
+
+
+#parameters of a circle generator
+generator_MODE_CIRCLE = 2
+generator_CENTER = [0,0]
+generator_NB_CIRCLE_POINTS = 40
+generator_RAY = 10
+
+#parameters of a spiral generator
+generator_MODE_SPIRAL = 3
+generator_SPIRAL_ROOT = [1,1]
+generator_ALPHA = 1.01
+generator_THETA = 0.1
+generator_NB_SPIRAL_POINTS = 40
+
+
+
+#We suppose that the noisePrecision follows a gamma distribution
+#with parameters alpha, and beta
+#      (reminds that noisePrecision = 1/noiseVariance)
+#
+#Accorging to that distribution:
+#          E(noisePrecision) = alpha/beta
+#          Var(noisePrecision)=alpha/(beta^2)
+#
+#Given those 2 last values, we can deduce the value of alpha and beta:
+#
+#          beta = mean/var
+#          alpha = (mean^2)/var
+#
+#It is what the following procedure is doing:
+# -find alpha and beta,
+# and then returns them in a pair
+def computeNoiseVarianceParameters(mean,var):
+    beta = (1.0*mean)/var
+    alpha = mean*beta
+    return [alpha,beta]
+
+
+
+
+ALPHA = 0
+BETA = 1
+#learner_NOISE_PRECISION_MEAN = 1.0/0.0001
+#learner_NOISE_PRECISION_VAR = 1.0
+#learner_NOISE_DISTRIBUTION_PARAMETERS = (UNDEFINED,UNDEFINED)
+#if(learner_NOISE_PRECISION_MEAN &gt; 0 and learner_NOISE_PRECISION_VAR &gt; 0):
+#   learner_NOISE_DISTRIBUTION_PARAMETERS = computeNoiseVarianceParameters(learner_NOISE_PRECISION_MEAN,
+#                                                                           learner_NOISE_PRECISION_VAR)
+#print &quot;noise distribution parameters: &quot;
+#print learner_NOISE_DISTRIBUTION_PARAMETERS
+learnerSpec = pl.TransformationLearner(
+    behavior = BEHAVIOR_LEARNER,
+    transformFamily = LINEAR_INCREMENT,
+    withBias = False,
+    learnNoiseVariance = True,
+    regOnNoiseVariance = False,
+    noiseAlpha =  1,
+    noiseBeta = 1,
+    learnTransformDistribution = False,
+    regOnTransformDistribution = False,
+    transformDistributionAlpha = 2,
+    noiseVariance =3.,
+    transformsVariance =4.0 ,
+    nbTransforms = 2,
+    nbNeighbors = 2,
+    initializationMode = DEFAULT)
+learner = serv.new(learnerSpec)
+iLearner = IterativeLearner(learner)
+
+
+generatorSpec = pl.TransformationLearner(
+    behavior = BEHAVIOR_GENERATOR,
+    transformFamily = LINEAR_INCREMENT,
+    withBias = False,
+    noiseAlpha = UNDEFINED,
+    noiseBeta = UNDEFINED,
+    transformDistributionAlpha = UNDEFINED,
+    noiseVariance = generator_NOISE_VARIANCE,
+    transformsVariance = 1,
+    nbTransforms = 1,
+    nbNeighbors = 1)
+
+gen = serv.new(generatorSpec)
+
+
+generatorMode = generator_MODE_CIRCLE
+
+#generator = TreeGenerator(gen,
+ #                         DEFAULT_DIM,
+  #                        False,
+   #                       generator_DEEPNESS,
+    #                      generator_BRANCHING_FACTOR,
+     #                     generator_ROOT)
+#generator= SequentialGenerator(gen,
+ #                              DEFAULT_DIM,
+  #                             False,
+   #                            generator_SEQUENCE_LENGTH,
+    #                           generator_START
+     #                          )
+
+generator = CircleGenerator(gen,
+                            False,
+                            generator_NB_CIRCLE_POINTS,
+                            generator_CENTER,
+                            generator_RAY)
+#generator = SpiralGenerator(gen,
+ #                           False,
+  #                          generator_NB_SPIRAL_POINTS,
+   #                         generator_SPIRAL_ROOT,
+    #                        generator_ALPHA,
+     #                       generator_THETA)
+
+tester = TLTester(iLearner,generator)
+if(generator_NOISE_VARIANCE &gt; 0):
+    generator.setNoiseVariance(generator_NOISE_VARIANCE)
+tester.run()


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001502.html">[Plearn-commits] r8054 - trunk/python_modules/plearn/parallel
</A></li>
	<LI>Next message: <A HREF="001504.html">[Plearn-commits] r8056 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1503">[ date ]</a>
              <a href="thread.html#1503">[ thread ]</a>
              <a href="subject.html#1503">[ subject ]</a>
              <a href="author.html#1503">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
