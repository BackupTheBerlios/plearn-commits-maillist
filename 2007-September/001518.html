<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r8070 - in trunk: commands plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-September/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8070%20-%20in%20trunk%3A%20commands%20plearn_learners/online&In-Reply-To=%3C200709121724.l8CHOcCV012160%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001517.html">
   <LINK REL="Next"  HREF="001519.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r8070 - in trunk: commands plearn_learners/online</H1>
    <B>louradou at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8070%20-%20in%20trunk%3A%20commands%20plearn_learners/online&In-Reply-To=%3C200709121724.l8CHOcCV012160%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r8070 - in trunk: commands plearn_learners/online">louradou at mail.berlios.de
       </A><BR>
    <I>Wed Sep 12 19:24:38 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="001517.html">[Plearn-commits] r8069 - trunk/python_modules/plearn/pymake
</A></li>
        <LI>Next message: <A HREF="001519.html">[Plearn-commits] r8071 - trunk/plearn/python
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1518">[ date ]</a>
              <a href="thread.html#1518">[ thread ]</a>
              <a href="subject.html#1518">[ subject ]</a>
              <a href="author.html#1518">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: louradou
Date: 2007-09-12 19:24:37 +0200 (Wed, 12 Sep 2007)
New Revision: 8070

Added:
   trunk/plearn_learners/online/LayerCostModule.cc
   trunk/plearn_learners/online/LayerCostModule.h
Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
Added a new module to compute (and propagate the gradient of) a new cost
during the training of a RBM with Binomial hidden units.
This additionial cost encourages to have more diversity in the filters.



Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2007-09-11 19:41:53 UTC (rev 8069)
+++ trunk/commands/plearn_noblas_inc.h	2007-09-12 17:24:37 UTC (rev 8070)
@@ -205,6 +205,7 @@
 #include &lt;plearn_learners/online/ForwardModule.h&gt;
 #include &lt;plearn_learners/online/GradNNetLayerModule.h&gt;
 #include &lt;plearn_learners/online/IdentityModule.h&gt;
+#include &lt;plearn_learners/online/LayerCostModule.h&gt;
 #include &lt;plearn_learners/online/LinearCombinationModule.h&gt;
 #include &lt;plearn_learners/online/MatrixModule.h&gt;
 #include &lt;plearn_learners/online/MaxSubsampling2DModule.h&gt;

Added: trunk/plearn_learners/online/LayerCostModule.cc
===================================================================
--- trunk/plearn_learners/online/LayerCostModule.cc	2007-09-11 19:41:53 UTC (rev 8069)
+++ trunk/plearn_learners/online/LayerCostModule.cc	2007-09-12 17:24:37 UTC (rev 8070)
@@ -0,0 +1,1350 @@
+// -*- C++ -*-
+
+// LayerCostModule.cc
+//
+// Copyright (C) 2007 Jerome Louradour
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Author: Jerome Louradour
+
+/*! \file LayerCostModule.cc */
+
+
+
+#include &quot;LayerCostModule.h&quot;
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    LayerCostModule,
+    &quot;Computes a cost function on Layer, given:    \n&quot;,
+    &quot;* Expectations for a binomial RBM upper layer\n&quot;
+    &quot;* sigmoid(activation) for a Neural Network   \n&quot;
+    &quot;and Back-propagates the gradient.            \n&quot;
+    &quot;\n&quot;
+    &quot;This function can be:                        \n&quot;
+    &quot;* The average Cross-Entropy                  \n&quot;
+    &quot;* The average Kullback-Leibler Divergence    \n&quot;
+    &quot;* Pascal's function...                       \n&quot;);
+
+LayerCostModule::LayerCostModule():
+    histo_size(10),
+    alpha(0.0),
+    momentum(0.0)
+{
+    output_size = 1;
+/*
+    ntest = 0;
+*/
+}
+
+void LayerCostModule::declareOptions(OptionList&amp; ol)
+{
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+
+    redeclareOption(ol, &quot;input_size&quot;, &amp;LayerCostModule::input_size,
+                     OptionBase::nosave,
+        &quot;Size of the layer.&quot;);
+
+    declareOption(ol, &quot;cost_function&quot;, &amp;LayerCostModule::cost_function,
+                  OptionBase::buildoption,
+        &quot;The cost function applied to the layer:\n&quot;
+        &quot;- \&quot;stochastic_cross_entropy\&quot; [default]: average cross-entropy between pairs of binomial units\n&quot;
+        &quot;- \&quot;stochastic_kl_div\&quot;: average KL divergence between pairs of binomial units\n&quot;
+        &quot;- \&quot;kl_div\&quot;: KL divergence between distrubution of expectations (sampled with x)\n&quot;
+        &quot;- \&quot;kl_div_2\&quot;: good version of kl_div\n&quot;
+        &quot;- \&quot;kl_div_simple\&quot;: simple version of kl_div where we count at least one sample per histogram's bin\n&quot;);
+
+    declareOption(ol, &quot;histo_size&quot;, &amp;LayerCostModule::histo_size,
+                  OptionBase::buildoption,
+        &quot;For \&quot;kl_div*\&quot; cost functions,\n&quot;
+        &quot;number of bins for the histograms (to estimate distributions of probabilities for expectations).\n&quot;
+        &quot;The higher is histo_size, the more precise is the estimation.\n&quot;);
+
+    declareOption(ol, &quot;alpha&quot;, &amp;LayerCostModule::alpha,
+                  OptionBase::buildoption,
+        &quot;(&gt;=0) For \&quot;pascal\&quot; cost function,\n&quot;
+        &quot;number of bins for the histograms (to estimate distributions of probabilities for expectations).\n&quot;
+        &quot;The higher is histo_size, the more precise is the estimation.\n&quot;);
+
+    declareOption(ol, &quot;momentum&quot;, &amp;LayerCostModule::momentum,
+                  OptionBase::buildoption,
+        &quot;(in [0,1[) For \&quot;pascal\&quot; cost function, momentum for the moving means\n&quot;);
+
+}
+
+void LayerCostModule::build_()
+{
+    PLASSERT( input_size &gt; 1 );
+    PLASSERT( histo_size &gt; 1 );
+    PLASSERT( momentum &gt;= 0.0);
+    PLASSERT( momentum &lt; 1);
+    
+    string im = lowerstring( cost_function );
+    // choose HERE the *default* cost function
+    if( im == &quot;&quot; )
+        cost_function = &quot;pascal&quot;;
+    else
+        cost_function = im;
+
+     // list HERE all *stochastic* cost functions
+    if( ( cost_function == &quot;stochastic_cross_entropy&quot;)
+     || ( cost_function == &quot;stochastic_kl_div&quot;) )
+        is_cost_function_stochastic = true;
+	
+    // list HERE all *non stochastic* cost functions
+    // and the specific initialization
+    else if( ( cost_function == &quot;kl_div&quot;)
+          || ( cost_function == &quot;kl_div_simple&quot;)
+	  || ( cost_function == &quot;kl_div_2&quot;) )
+    {
+        is_cost_function_stochastic = false;
+        expectations_histo.resize(input_size,histo_size);
+	LINHISTO_STEP = 1.0/(real)histo_size;
+        LOGHISTO_BASE = 10.0;
+        LOGHISTO_MIN = (real)pow(LOGHISTO_BASE,-(real)histo_size);
+    }
+    else if ( ( cost_function == &quot;pascal&quot;) )
+    {
+        is_cost_function_stochastic = false;
+	expectations_expectation.resize(input_size);
+	expectations_cross_quadratic_mean.resize(input_size,input_size);
+/*
+        expectations_expectation_testMemory.resize(input_size);
+        expectations_cross_quadratic_mean_testMemory.resize(input_size,input_size);
+*/
+	if( momentum &gt; 0.0)
+	{
+            expectations_expectation_trainMemory.resize(input_size);
+            expectations_cross_quadratic_mean_trainMemory.resize(input_size,input_size);
+	}
+    }
+    else
+        PLERROR(&quot;LayerCostModule::build_() does not recognize cost function %s&quot;,
+                 cost_function.c_str());
+
+    // The port story...
+    ports.resize(0);
+    portname_to_index.clear();
+    addPortName(&quot;expectations&quot;);
+    addPortName(&quot;cost&quot;);
+
+    port_sizes.resize(nPorts(), 2);
+    port_sizes.fill(-1);
+    port_sizes(getPortIndex(&quot;expectations&quot;), 1) = input_size;
+    port_sizes(getPortIndex(&quot;cost&quot;), 1) = 1;
+    
+}
+
+
+// ### Nothing to add here, simply calls build_
+void LayerCostModule::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void LayerCostModule::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(expectations_histo, copies);
+    deepCopyField(expectations_expectation, copies);
+    deepCopyField(expectations_cross_quadratic_mean, copies);
+    deepCopyField(expectations_expectation_trainMemory, copies);
+    deepCopyField(expectations_cross_quadratic_mean_trainMemory, copies);
+/*
+    deepCopyField(expectations_expectation_testMemory, copies);
+    deepCopyField(expectations_cross_quadratic_mean_testMemory, copies);
+*/
+    deepCopyField(ports, copies);
+}
+
+
+
+
+///////////
+// fprop //
+///////////
+
+
+void LayerCostModule::fprop(const TVec&lt;Mat*&gt;&amp; ports_value)
+{
+
+    Mat* expectations = ports_value[getPortIndex(&quot;expectations&quot;)];
+    Mat* cost = ports_value[getPortIndex(&quot;cost&quot;)];
+
+    PLASSERT( ports_value.length() == nPorts() );
+
+    if ( cost &amp;&amp; cost-&gt;isEmpty() )
+    {
+        PLASSERT( expectations &amp;&amp; !expectations-&gt;isEmpty() );
+	cout &lt;&lt; &quot;1 regular fprop!!!&quot; &lt;&lt; endl;
+        fprop(*expectations, *cost);
+    }
+}
+
+void LayerCostModule::fprop(const Mat&amp; expectations, Mat&amp; costs)
+{
+    int batch_size = expectations.length();
+    costs.resize( batch_size, output_size );
+    
+    if( !is_cost_function_stochastic )
+    {
+        costs.clear();
+
+        if( cost_function == &quot;kl_div&quot; )
+        {
+        //! ************************************************************
+        //! (non stochastic) SYMETRIC *** K-L DIVERGENCE ***
+        //! between probabilities of expectations vectors for all units
+        //! ************************************************************
+        //! 
+        //!      cost = - \sum_{i} \sum_{j#i} Div_{KL}[ Px(q{i}) | Px(q{j}) ]
+        //!
+        //!           = - \sum_{i} \sum_{j#i} \sum_Q (Nx_j(Q) - Nx_j(Q)) log( Nx_i(Q) / Nx_j(Q) )
+        //!
+        //! where  q{i} = P(h{i}=1|x): expectation of the i^th units of the layer
+        //!        Px(.): empirical probability (given data x, we sample the q's)
+        //!        Q: interval in [0,1] = one bin of the histogram of the expectations q's
+        //!        Nx_i(Q): proportion of q{i} that belong to Q, with input data x
+        //!
+        //! Note: one q{i} *entirely* determines one binomial densities of probability
+        //!       ( Bijection {binomial Proba functions} &lt;-&gt; |R )
+        //!
+        //! ************************************************************
+
+            // Filling the histogram (i.e. emperical distribution)
+            // of the expectations
+	    computeHisto(expectations);
+	    
+            // Computing the KL divergence
+            for (int i = 0; i &lt; input_size; i++)
+                for (int j = 0; j &lt; i; j++)
+		{
+		    // These variables are used in case one bin of 
+		    // the histogram is empty for one unit
+		    // and not for another one ( (Nj-Ni).log(Ni/Nj) = nan ).
+		    // In such case, we ''differ'' the count for the next bin and so on.
+                    real differ_count_i = 0.0;
+                    real differ_count_j = 0.0;
+		    for (int k = 0; k &lt; histo_size; k++)
+		    {
+                        real Ni_k = expectations_histo(i,k) + differ_count_i;
+			real Nj_k = expectations_histo(j,k) + differ_count_j;
+			if( fast_exact_is_equal(Ni_k, 0.0) )
+			{
+                         // differ_count_j += expectations_histo(j,k);
+                            differ_count_j = Nj_k;
+			    continue;
+			}
+                        else if( fast_exact_is_equal(Nj_k, 0.0) )
+			{
+                            differ_count_i = Ni_k;
+			    continue;
+			}
+                        else
+			{
+			    costs(0,0) += KLdivTerm(Ni_k,Nj_k);
+                            differ_count_i = 0.0;
+			    differ_count_j = 0.0;
+			}
+                    }
+		    if( differ_count_i &gt; 0.0 )
+		        &quot;cas ou on regroupe avec le dernier&quot;;
+		    else if ( differ_count_j &gt; 0.0 )
+		        &quot;cas ou on regroupe avec le dernier&quot;;		    
+                }
+            // Normalization w.r.t. number of units
+            costs(0,0) /= ((real)input_size *(real)(input_size-1)); // / (real)batch_size;
+        }
+        else if( cost_function == &quot;kl_div_2&quot; )
+        {
+        //! ************************************************************
+        //! (non stochastic) SYMETRIC *** K-L DIVERGENCE ***
+        //! between probabilities of expectations vectors for all units
+        //! ************************************************************
+        //! 
+        //!      cost = - \sum_{i} \sum_{j#i} Div_{KL}[ Px(q{i}) | Px(q{j}) ]
+        //!
+        //!           = - \sum_{i} \sum_{j#i} \sum_Q (Nx_j(Q) - Nx_j(Q)) log( Nx_i(Q) / Nx_j(Q) )
+        //!
+        //! where  q{i} = P(h{i}=1|x): expectation of the i^th units of the layer
+        //!        Px(.): empirical probability (given data x, we sample the q's)
+        //!        Q: interval in [0,1] = one bin of the histogram of the expectations q's
+        //!        Nx_i(Q): proportion of q{i} that belong to Q, with input data x
+        //!
+        //! Note: one q{i} *entirely* determines one binomial densities of probability
+        //!       ( Bijection {binomial Proba functions} &lt;-&gt; |R )
+        //!
+        //! ************************************************************
+
+            // Filling the histogram (i.e. emperical distribution)
+            // of the expectations
+	    computeHisto(expectations);
+	    
+            // Computing the KL divergence
+            for (int i = 0; i &lt; input_size; i++)
+                for (int j = 0; j &lt; i; j++)
+		{
+		    // These variables are used in case one bin of 
+		    // the histogram is empty for one unit
+		    // and not for another one ( (Nj-Ni).log(Ni/Nj) = nan ).
+		    // In such case, we ''differ'' the count for the next bin and so on.
+                    real differ_count_i = 0.0;
+                    real differ_count_j = 0.0;
+		    int n_differ = 0;
+		    for (int k = 0; k &lt; histo_size; k++)
+		    {
+                        real Ni_k = expectations_histo(i,k) + differ_count_i;
+			real Nj_k = expectations_histo(j,k) + differ_count_j;
+			if( fast_exact_is_equal(Ni_k, 0.0) )
+			{
+                         // differ_count_j += expectations_histo(j,k);
+                            differ_count_j = Nj_k;
+			    n_differ += 1;
+			}
+                        else if( fast_exact_is_equal(Nj_k, 0.0) )
+			{
+                            differ_count_i = Ni_k;
+			    n_differ += 1;
+			}
+                        else
+			{
+			    costs(0,0) += KLdivTerm(Ni_k,Nj_k) *(real)(1+n_differ)/(real)histo_size;
+                            differ_count_i = 0.0;
+			    differ_count_j = 0.0;
+			}
+                    }
+		    if( differ_count_i &gt; 0.0 )
+		        &quot;cas ou on regroupe avec le dernier&quot;;
+		    else if ( differ_count_j &gt; 0.0 )
+		        &quot;cas ou on regroupe avec le dernier&quot;;		    
+                }
+            // Normalization w.r.t. number of units
+            costs(0,0) /= ((real)input_size *(real)(input_size-1)); // / (real)batch_size;
+        }
+        else if( cost_function == &quot;kl_div_simple&quot; )
+        {
+            // Filling the histogram (i.e. emperical distribution)
+            // of the expectations
+	    computeSafeHisto(expectations);
+	    
+            // Computing the KL divergence
+            for (int i = 0; i &lt; input_size; i++)
+                for (int j = 0; j &lt; i; j++)
+		    for (int k = 0; k &lt; histo_size; k++)
+		    {
+                        real Ni_k = expectations_histo(i,k);
+			real Nj_k = expectations_histo(j,k);
+			costs(0,0) += KLdivTerm(Ni_k,Nj_k);
+                    }
+            // Normalization w.r.t. number of units
+            costs(0,0) /= ((real)input_size *(real)(input_size-1)); // / (real)batch_size;
+        }
+        else if( cost_function == &quot;pascal&quot; )
+        {
+        //! ************************************************************
+        //! a god-given similarity measure
+        //! between expectations vectors for all units
+        //! ************************************************************
+        //! 
+        //!      cost = \sum_{i} \sum_{j#i} exp( Ex[q{i}.q{j}] ) - alpha. \sum_{i} exp( Ex[q{i}] )
+        //!
+        //! where  q{i} = P(h{i}=1|x): expectation of the i^th units of the layer
+        //!        Ex(.): empirical esperance (given data x, we sample the q's)
+        //!
+        //! ************************************************************
+
+            // Computing statistics on expectations
+	    computePascalStatistics(expectations, false);
+	    
+	    cout &lt;&lt; &quot;1 fprop&quot; &lt;&lt; endl;
+	    	    
+            // Computing the cost
+            for (int i = 0; i &lt; input_size; i++)
+	    {
+	        if (alpha &gt; 0.0 )
+		    costs(0,0) -= alpha*exp(expectations_expectation[i]);
+                for (int j = 0; j &lt; i; j++)
+                    costs(0,0) += exp(expectations_cross_quadratic_mean(i,j)) / (real)(input_size-1);
+            }
+
+            // Normalization w.r.t. number of units
+            costs(0,0) /= (real)input_size;
+        }
+	
+        return; // Do not fprop with the conventional stochastic fprop...
+    }
+    
+    for (int isample = 0; isample &lt; batch_size; isample++)
+        fprop(expectations(isample), costs(isample,0));
+}
+
+void LayerCostModule::fprop(const Vec&amp; expectation, real&amp; cost) const
+{
+    PLASSERT( expectation.size() == input_size );
+    PLASSERT( is_cost_function_stochastic );
+
+    cost = 0.0;
+    real  qi, qj, comp_qi, comp_qj; // The expectations qi=p(h_i=1)
+                                      //     and some basic operations on it (e.g.: 1-qi, qi/(1-qi)) 
+				      
+    if( cost_function == &quot;stochastic_cross_entropy&quot; )
+    {
+    //! ************************************************************
+    //! average *** CROSS ENTROPY ***
+    //! between pairs of units (given expectations = sigmoid(act) )
+    //! ************************************************************
+    //!
+    //!      cost = - \sum_{i} \sum_{j#i} CrossEntropy[( P(h_{i}|x) | P(h_{j}|x) )]
+    //!
+    //!           = - \sum_{i} \sum_{j#i} [ q{i}.log(q{j}) + (1-q{i}).log(1-q{j}) ]
+    //!
+    //! where |  h_{i}: i^th units of the layer
+    //!       \  P(.|x): output for input data x
+    //!        \ q{i}=P(h{i}=1|v): expectation of the i^th units of the layer
+    //!
+    //! ************************************************************
+
+        for( int i = 0; i &lt; input_size; i++ )
+        {
+           qi = expectation[i];
+           comp_qi = 1.0 - qi;
+           for( int j = 0; j &lt; i; j++ )
+           {
+               qj = expectation[j];
+               comp_qj = 1.0 - qj;
+	       
+               // H(pi||pj) = H(pi) + D_{KL}(pi||pj)
+               cost += qi*safeflog(qj) + comp_qi*safeflog(comp_qj);
+	       
+               // The symetric part (loop  j=i+1...size)
+               cost += qj*safeflog(qi) + comp_qj*safeflog(comp_qi);
+           }
+        }
+        // Normalization w.r.t. number of units
+        cost /= ((real)input_size *(real)(input_size-1));
+    }
+    
+    else if( cost_function == &quot;stochastic_kl_div&quot; )
+    {
+    //! ************************************************************
+    //! average SYMETRIC *** K-L DIVERGENCE ***
+    //! between pairs of units (given expectations = sigmoid(act) )
+    //! ************************************************************
+    //!
+    //!      cost = - \sum_{i} \sum_{j#i} Div_{KL} [( P(h_{i}|v) | P(h_{j}|v) )]
+    //!
+    //!           = - \sum_{i} \sum_{j#i} [ ( q{j} - q{i} ) log( q{i}/(1-q{i})*(1-q{j})/q{j} ) ]
+    //!
+    //! where |  h_{i}: i^th units of the layer
+    //!       \  P(.|v):  output for input data x
+    //!        \ q{i}=P(h{i}=1|v): expectation of the i^th units of the layer
+    //!
+    //! ************************************************************
+
+        for( int i = 0; i &lt; input_size; i++ )
+        {
+           qi = expectation[i];
+           if(fast_exact_is_equal(qi, 1.0))
+               comp_qi = REAL_MAX;
+           else
+               comp_qi = qi/(1.0 - qi);
+       
+           for( int j = 0; j &lt; i; j++ )
+           {
+               qj = expectation[j];
+               if(fast_exact_is_equal(qj, 1.0))
+                   comp_qj = REAL_MAX;
+               else
+                   comp_qj = qj/(1.0 - qj);
+	       
+               //     - D_{KL}(pi||pj) - D_{KL}(pj||pi)
+               cost += (qj-qi)*safeflog(comp_qi/comp_qj);
+           }
+        }
+        // Normalization w.r.t. number of units
+        cost /= ((real)input_size *(real)(input_size-1));   
+    }
+
+    else
+        PLERROR(&quot;LayerCostModule::fprop() not implemented for cost function %s\n&quot;
+	        &quot;- It may be a printing error\n&quot;
+		&quot;- You can try to call LayerCostModule::fprop(const Mat&amp; expectations, Mat&amp; costs)\n&quot;
+		&quot;- Or else write the code corresponding to your cost function&quot;,
+                 cost_function.c_str());
+}
+
+
+
+
+////////////////////
+// bpropAccUpdate //
+////////////////////
+
+
+void LayerCostModule::bpropAccUpdate(const TVec&lt;Mat*&gt;&amp; ports_value,
+                                   const TVec&lt;Mat*&gt;&amp; ports_gradient)
+{
+    PLASSERT( ports_value.length() == nPorts() );
+    PLASSERT( ports_gradient.length() == nPorts() );
+
+    const Mat* expectations = ports_value[getPortIndex(&quot;expectations&quot;)];
+    Mat* expectations_grad = ports_gradient[getPortIndex(&quot;expectations&quot;)];
+    Mat* cost = ports_value[getPortIndex(&quot;cost&quot;)];
+    Mat* cost_grad = ports_gradient[getPortIndex(&quot;cost&quot;)];
+
+    if( expectations_grad &amp;&amp; expectations_grad-&gt;isEmpty()
+        &amp;&amp; cost_grad &amp;&amp; !cost_grad-&gt;isEmpty() )
+    {
+        int batch_size = expectations-&gt;length();
+
+        PLASSERT( expectations &amp;&amp; !expectations-&gt;isEmpty());
+        PLASSERT( expectations-&gt;length() == batch_size );
+        PLASSERT( cost_grad-&gt;length() == batch_size );
+
+        expectations_grad-&gt;resize(batch_size, input_size);
+	expectations_grad-&gt;clear();
+
+        real qi, qj, comp_qi, comp_qj;
+        Vec comp_q(input_size), log_term(input_size);
+
+        if( cost_function == &quot;stochastic_cross_entropy&quot; )
+        {
+            for (int isample = 0; isample &lt; batch_size; isample++)
+            {
+        	for (int i = 0 ; i &lt; input_size ; i++ )
+                {
+                    qi = (*expectations)(isample,i);
+            	    comp_qi = 1.0 - qi;
+                    comp_q[i] = comp_qi;
+                    log_term[i] = safeflog(qi) - safeflog(comp_qi);
+                }
+                for (int i = 0; i &lt; input_size; i++ )
+                {
+                    qi = (*expectations)(isample,i);
+                    comp_qi = comp_q[i];
+                    (*expectations_grad)(isample,i) = 0.0;
+                    for (int j = 0; j &lt; i; j++ )
+                    {
+                        qj = (*expectations)(isample,j);
+                        comp_qj=comp_q[j];
+
+                        // log(pj) - log(1-pj) + pj/pi - (1-pj)/(1-pi)
+                        (*expectations_grad)(isample,i) += log_term[j] + qj/qi - comp_qi/comp_qj;
+
+                        // The symetric part (loop  j=i+1...input_size)
+                        (*expectations_grad)(isample,j) += log_term[i] + qi/qj - comp_qj/comp_qi;
+                    }
+                }
+                // Normalization
+                real norm_factor = 1.0 / ((real)input_size *(real)(input_size-1));
+                for (int i = 0; i &lt; input_size; i++ )
+                {
+                    (*expectations_grad)(isample, i) *= (*cost_grad)(isample,0) * norm_factor;
+                }
+            }
+        }
+
+        else if( cost_function == &quot;stochastic_kl_div&quot; )
+        {
+            for (int isample = 0; isample &lt; batch_size; isample++)
+            {
+        	for (int i = 0; i &lt; input_size; i++ )
+                {
+                    qi = (*expectations)(isample,i);
+            	    comp_qi = 1.0 - qi;
+                    if(fast_exact_is_equal(qi, 1.0) || fast_exact_is_equal(qi, 0.0))
+                        comp_q[i] = REAL_MAX;
+                    else
+                        comp_q[i] = 1.0/(qi*comp_qi);
+                    log_term[i] = safeflog(qi) - safeflog(comp_qi);
+                }
+                for (int i = 0; i &lt; input_size; i++ )
+                {
+                    qi = (*expectations)(isample,i);
+                    comp_qi = comp_q[i];
+
+                    (*expectations_grad)(isample,i) = 0.0;
+                    for (int j = 0; j &lt; i ; j++ )
+                    {
+                        qj = (*expectations)(isample,j);
+                        comp_qj=comp_q[j];
+
+                        //   [qj - qi]/[qi (1-qi)] - log[ qi/(1-qi) * (1-qj)/qj]
+                        (*expectations_grad)(isample,i) += (qj - qi)*comp_qi - log_term[i] + log_term[j];
+
+                        // The symetric part (loop  j=i+1...input_size)
+                        (*expectations_grad)(isample,j) += (qi - qj)*comp_qj - log_term[j] + log_term[i];
+                    }
+                }
+                // Normalization
+                real norm_factor = 1.0 / ((real)input_size *(real)(input_size-1));
+                for (int i = 0; i &lt; input_size; i++ )
+                {
+                    (*expectations_grad)(isample, i) *= (*cost_grad)(isample,0) * norm_factor;
+                }
+            }
+        }
+
+
+        else if( cost_function == &quot;kl_div&quot; )
+        {
+	    computeHisto(*expectations);
+	    real one_count = 1. / (real)batch_size;
+	    
+            for (int isample = 0; isample &lt; batch_size; isample++)
+            {
+
+                // Computing the difference of KL divergence
+                // for d_q
+                for (int i = 0; i &lt; input_size; i++)
+		{
+                    (*expectations_grad)(isample, i) = 0.0;
+		    
+		    qi = (*expectations)(isample,i);
+		    int index_i = histo_index(qi);
+		    if( ( index_i == histo_size-1 ) ) // we do not care about this...
+		        continue;
+		    real over_dqi=1.0/dq(qi);
+		    int shift_i;
+		    if( over_dqi &gt; 0.0)
+		        shift_i = 1;
+		    else
+		        shift_i = -1;
+		    // qi + dq(qi) ==&gt; | expectations_histo(i,index_i)   - one_count
+		    //                 \ expectations_histo(i,index_i+shift_i) + one_count
+		    
+		    for (int j = 0; j &lt; i; j++)
+		    {
+			(*expectations_grad)(isample, i) += delta_KLdivTerm(i, j, index_i, over_dqi, one_count);
+			
+                        qj = (*expectations)(isample,j);
+			int index_j = histo_index(qj);
+		        if( ( index_j == histo_size-1 ) || ( index_j == 0 ) )
+		            continue;
+			real over_dqj=1.0/dq(qj);
+ 		        int shift_j;
+		        if( over_dqj &gt; 0.0)
+		            shift_j = 1;
+		        else
+		            shift_j = -1;
+            	        // qj + dq(qj) ==&gt; | expectations_histo(j,index_j)   - one_count
+  		        //                 \ expectations_histo(j,index_j+shift_j) + one_count
+			
+			(*expectations_grad)(isample, j) += delta_KLdivTerm(j, i, index_j, over_dqj, one_count);
+                    }
+		}
+
+                // Normalization
+                real norm_factor = 1.0 / ((real)input_size *(real)(input_size-1));
+                for (int i = 0; i &lt; input_size; i++ )
+                {
+                    (*expectations_grad)(isample, i) *= (*cost_grad)(isample,0) * norm_factor;
+                }
+            }
+        }
+
+        else if( cost_function == &quot;kl_div_2&quot; )
+        {
+	    computeHisto(*expectations);
+	    real one_count = 1. / (real)batch_size;
+	    
+            for (int isample = 0; isample &lt; batch_size; isample++)
+            {
+
+                // Computing the difference of KL divergence
+                // for d_q
+                for (int i = 0; i &lt; input_size; i++)
+		{
+                    (*expectations_grad)(isample, i) = 0.0;
+		    
+		    qi = (*expectations)(isample,i);
+		    int index_i = histo_index(qi);
+		    if( ( index_i == histo_size-1 ) ) // we do not care about this...
+		        continue;
+		    real over_dqi=1.0/dq(qi);
+		    int shift_i;
+		    if( over_dqi &gt; 0.0)
+		        shift_i = 1;
+		    else
+		        shift_i = -1;
+		    // qi + dq(qi) ==&gt; | expectations_histo(i,index_i)   - one_count
+		    //                 \ expectations_histo(i,index_i+shift_i) + one_count
+		    
+		    for (int j = 0; j &lt; i; j++)
+		    {
+			(*expectations_grad)(isample, i) += delta_KLdivTerm_2(i, j, index_i, over_dqi, one_count);
+			
+                        qj = (*expectations)(isample,j);
+			int index_j = histo_index(qj);
+		        if( ( index_j == histo_size-1 ) || ( index_j == 0 ) )
+		            continue;
+			real over_dqj=1.0/dq(qj);
+ 		        int shift_j;
+		        if( over_dqj &gt; 0.0)
+		            shift_j = 1;
+		        else
+		            shift_j = -1;
+            	        // qj + dq(qj) ==&gt; | expectations_histo(j,index_j)   - one_count
+  		        //                 \ expectations_histo(j,index_j+shift_j) + one_count
+			
+			(*expectations_grad)(isample, j) += delta_KLdivTerm_2(j, i, index_j, over_dqj, one_count);
+                    }
+		}
+
+                // Normalization
+                real norm_factor = 1.0 / ((real)input_size *(real)(input_size-1));
+                for (int i = 0; i &lt; input_size; i++ )
+                {
+                    (*expectations_grad)(isample, i) *= (*cost_grad)(isample,0) * norm_factor;
+                }
+            }
+        }
+
+        else if( cost_function == &quot;kl_div_simple&quot; )
+        {
+	    computeSafeHisto(*expectations);
+	    real one_count = 1. / (real)(batch_size+histo_size);
+	    
+            for (int isample = 0; isample &lt; batch_size; isample++)
+            {
+
+                // Computing the difference of KL divergence
+                // for d_q
+                for (int i = 0; i &lt; input_size; i++)
+		{
+                    (*expectations_grad)(isample, i) = 0.0;
+		    
+		    qi = (*expectations)(isample,i);
+		    int index_i = histo_index(qi);
+		    if( ( index_i == histo_size-1 ) ) // we do not care about this...
+		        continue;
+		    real over_dqi=1.0/dq(qi);
+		    int shift_i;
+		    if( over_dqi &gt; 0.0)
+		        shift_i = 1;
+		    else
+		        shift_i = -1;
+		    // qi + dq(qi) ==&gt; | expectations_histo(i,index_i)   - one_count
+		    //                 \ expectations_histo(i,index_i+shift_i) + one_count
+		    
+		    for (int j = 0; j &lt; i; j++)
+		    {
+			(*expectations_grad)(isample, i) += delta_SafeKLdivTerm(i, j, index_i, over_dqi, one_count);
+			
+                        qj = (*expectations)(isample,j);
+			int index_j = histo_index(qj);
+		        if( ( index_j == histo_size-1 ) || ( index_j == 0 ) )
+		            continue;
+			real over_dqj=1.0/dq(qj);
+ 		        int shift_j;
+		        if( over_dqj &gt; 0.0)
+		            shift_j = 1;
+		        else
+		            shift_j = -1;
+            	        // qj + dq(qj) ==&gt; | expectations_histo(j,index_j)   - one_count
+  		        //                 \ expectations_histo(j,index_j+shift_j) + one_count
+			
+			(*expectations_grad)(isample, j) += delta_SafeKLdivTerm(j, i, index_j, over_dqj, one_count);
+                    }
+		}
+
+                // Normalization
+                real norm_factor = 1.0 / ((real)input_size *(real)(input_size-1));
+                for (int i = 0; i &lt; input_size; i++ )
+                {
+                    (*expectations_grad)(isample, i) *= (*cost_grad)(isample,0) * norm_factor;
+                }
+            }
+        }
+
+        else if( cost_function == &quot;pascal&quot; )
+        {
+	    computePascalStatistics(*expectations, true);
+	    
+	    cout &lt;&lt; &quot;1 BPropAccUpdate&quot; &lt;&lt; endl;
+	    
+	    real one_count = 1. / (real)batch_size;
+	    if( momentum &gt; 0.0 )
+	        for (int isample = 0; isample &lt; batch_size; isample++)
+		{
+                    for (int i = 0; i &lt; input_size; i++)
+                    {
+                        qi = (*expectations)(isample, i);
+			if (alpha &gt; 0.0 )
+			    (*expectations_grad)(isample, i) -= alpha*exp(expectations_expectation[i]) *(1.0-momentum)*one_count;
+                        for (int j = 0; j &lt; i; j++)
+                        {
+			    qj = (*expectations)(isample,j);
+                            (*expectations_grad)(isample, i) += exp(expectations_cross_quadratic_mean(i,j)) *qj*(1.0-momentum)*one_count / (real)(input_size-1);
+                            (*expectations_grad)(isample, j) += exp(expectations_cross_quadratic_mean(i,j)) *qi*(1.0-momentum)*one_count / (real)(input_size-1);
+                        }
+                    }
+                    for (int i = 0; i &lt; input_size; i++)
+                    {
+	                (*expectations_grad)(isample, i) /= (real)input_size;
+	            }
+		}
+	    else
+	        for (int isample = 0; isample &lt; batch_size; isample++)
+		{
+                    for (int i = 0; i &lt; input_size; i++)
+                    {
+                        qi = (*expectations)(isample, i);
+			if (alpha &gt; 0.0 )
+			    (*expectations_grad)(isample, i) -= alpha*exp(expectations_expectation[i]) *one_count;
+                        for (int j = 0; j &lt; i; j++)
+                        {
+			    qj = (*expectations)(isample,j);
+                            (*expectations_grad)(isample, i) += exp(expectations_cross_quadratic_mean(i,j)) *qj*one_count / (real)(input_size-1);
+                            (*expectations_grad)(isample, j) += exp(expectations_cross_quadratic_mean(i,j)) *qi*one_count / (real)(input_size-1);
+                        }
+                    }
+                    for (int i = 0; i &lt; input_size; i++)
+                    {
+	                (*expectations_grad)(isample, i) /= (real)input_size;
+	            }
+		}
+        }
+	
+        else
+            PLERROR(&quot;LayerCostModule::bpropAccUpdate() not implemented for cost function %s&quot;,
+                     cost_function.c_str());
+
+/*
+        ntest = 0;
+*/
+
+        checkProp(ports_gradient);
+    }
+    else if( !expectations_grad &amp;&amp; !cost_grad )
+        return;
+    else
+        PLERROR(&quot;In LayerCostModule::bpropAccUpdate - Port configuration not implemented &quot;);
+
+}
+
+
+////////////////////////////////////////////////////
+// Auxiliary Functions for Pascal's cost function //
+////////////////////////////////////////////////////
+void LayerCostModule::computePascalStatistics(const Mat&amp; expectations, bool duringTraining)
+{
+    int batch_size = expectations.length();
+    real one_count = 1. / (real)batch_size;
+    Vec expectation;
+    
+    expectations_expectation.clear(); 
+    expectations_cross_quadratic_mean.clear(); 
+
+    for (int isample = 0; isample &lt; batch_size; isample++)
+    {
+        expectation = expectations(isample);
+        for (int i = 0; i &lt; input_size; i++)
+	{
+	    expectations_expectation[i] += expectation[i];
+	    for (int j = 0; j &lt; i; j++)
+                 expectations_cross_quadratic_mean(i,j) += expectation[i] * expectation[j];
+        }
+    }
+    expectations_cross_quadratic_mean *= one_count;
+    
+    for (int i = 0; i &lt; input_size; i++)
+    {
+        expectations_expectation[i] *= one_count;
+        for (int j = 0; j &lt; i; j++)
+        {
+             expectations_cross_quadratic_mean(i,j) *= one_count;
+//    	     expectations_cross_quadratic_mean(j,i) = expectations_cross_quadratic_mean(i,j);
+        }
+    }
+    if( ( momentum &gt; 0.0 ) &amp;&amp; duringTraining )
+    {
+        for (int i = 0; i &lt; input_size; i++)
+        {
+	    if(i == 0)
+	       cout &lt;&lt; &quot;.Check momentum....: expectations_expectation_trainMemory[0] = &quot; &lt;&lt; expectations_expectation_trainMemory[0] &lt;&lt; endl;
+
+            expectations_expectation[i] = momentum*expectations_expectation_trainMemory[i]
+	                                 +(1.0-momentum)*expectations_expectation[i];
+            expectations_expectation_trainMemory[i] = expectations_expectation[i];
+            for (int j = 0; j &lt; i; j++)
+            {
+                 expectations_cross_quadratic_mean(i,j) = momentum*expectations_cross_quadratic_mean_trainMemory(i,j)
+		                                       +(1.0-momentum)*expectations_cross_quadratic_mean(i,j);
+//        	 expectations_cross_quadratic_mean(j,i) = expectations_cross_quadratic_mean(i,j);
+        	 expectations_cross_quadratic_mean_trainMemory(i,j) = expectations_cross_quadratic_mean(i,j);
+//        	 expectations_cross_quadratic_mean_trainMemory(j,i) = expectations_cross_quadratic_mean(i,j);
+            }
+        }
+    }
+/*    else if( !duringTraining )
+    {
+	PLASSERT( ntest+batch_size &gt; 0 );
+	for (int i = 0; i &lt; input_size; i++)
+        {
+            expectations_expectation[i] = ( (real)ntest*expectations_expectation_testMemory[i]
+	                                   +(real)batch_size*expectations_expectation[i] )/(real)(ntest+batch_size);
+            expectations_expectation_testMemory[i] = expectations_expectation[i];
+            for (int j = 0; j &lt; i; j++)
+            {
+                 expectations_cross_quadratic_mean(i,j) = ( (real)ntest*expectations_cross_quadratic_mean_testMemory(i,j)
+		                                           +(real)batch_size*expectations_cross_quadratic_mean(i,j) );
+        	 expectations_cross_quadratic_mean(j,i) = expectations_cross_quadratic_mean(i,j);
+        	 expectations_cross_quadratic_mean_testMemory(i,j) = expectations_cross_quadratic_mean(i,j);
+        	 expectations_cross_quadratic_mean_testMemory(j,i) = expectations_cross_quadratic_mean(i,j);
+            }
+        }
+	ntest += batch_size;
+    }
+*/
+}
+
+/////////////////////////
+// Auxiliary Functions //
+/////////////////////////
+real LayerCostModule::delta_KLdivTerm(int i, int j, int index_i, real over_dq, real one_count)
+{
+    PLASSERT( over_dq &gt; 0.0 );
+
+    real grad_update = 0.0;
+
+    real Ni_ki = expectations_histo(i,index_i);
+    real Ni_ki_shift1 = expectations_histo(i,index_i+1);	    
+    real Nj_ki        = expectations_histo(j,index_i);
+    real Nj_ki_shift1 = expectations_histo(j,index_i+1);
+
+    PLASSERT( !fast_exact_is_equal(Ni_ki, 0.0) ); // Verification:
+                                                  // if expectations_histo is up to date,
+                                                  // the expectation(isample,i) has been counted
+    real differ_count_j_before = 0.0;
+    real differ_count_j_after = 0.0;
+    real differ_count_i_before = 0.0;
+    real differ_count_i_after = 0.0;
+
+    // What follows is only valuable when the qi's are increased (dq&gt;0).
+
+    if( !fast_exact_is_equal(Nj_ki, 0.0) )
+    // if it is zero, then INCREASING qi will not change anything
+    // (it was already counted in the next histograms's bin
+    {
+        // removing the term of the sum that will be modified
+        grad_update -= KLdivTerm( Ni_ki, Nj_ki );
+							       
+        if( fast_exact_is_equal(Ni_ki, one_count) )
+            differ_count_j_after = Nj_ki;
+        else
+            // adding the term of the sum with its modified value
+            grad_update += KLdivTerm( Ni_ki-one_count, Nj_ki );
+
+        if( !fast_exact_is_equal(Nj_ki_shift1,0.0) )
+        {
+            // adding the term of the sum with its modified value
+            grad_update += KLdivTerm( Ni_ki_shift1+one_count, Nj_ki_shift1+differ_count_j_after );
+
+            if( !fast_exact_is_equal(Ni_ki_shift1, 0.0) ) // &quot;cas o&#249; on regroupe avec le dernier&quot;;
+            {
+                // removing the term of the sum that will be modified
+                grad_update -= KLdivTerm( Ni_ki_shift1, Nj_ki_shift1 );		
+            }
+	    else
+	    {
+	        // We search   ki' &gt; k(i)+1   such that   n(i,ki') &gt; 0
+		differ_count_j_before = Nj_ki_shift1;
+		int ki;
+		for (ki = index_i+2; ki &lt; histo_size; ki++)
+		{
+		    differ_count_j_before += expectations_histo( j, ki );
+		    if( expectations_histo( i, ki )&gt;0 )
+		        break;
+		}
+		if( ki &lt; histo_size )
+		{
+                    grad_update -= KLdivTerm( expectations_histo( i, ki ), differ_count_j_before );
+
+		    if( differ_count_j_before &gt; Nj_ki_shift1 )		
+                        grad_update += KLdivTerm( expectations_histo( i, ki ), differ_count_j_before - Nj_ki_shift1 );
+		        // pb avec differ_count_j_after plus haut??? semble pas
+		}
+		else
+		    &quot;cas o&#249; on regroupe avec le dernier&quot;;
+	    }
+        }
+        else
+        {
+            differ_count_i_before = Ni_ki_shift1;
+            differ_count_i_after  = Ni_ki_shift1+one_count;
+	    int kj;
+	    for( kj = index_i+2; kj &lt; histo_size; kj++)
+	    {
+	        differ_count_i_after += expectations_histo( i, kj );
+		if( differ_count_i_before &gt; 0 )
+		    differ_count_i_before += expectations_histo( i, kj );
+		if( expectations_histo( j, kj ) &gt; 0 )
+		    break;
+	    }
+	    if( kj &lt; histo_size )
+            {
+                grad_update += KLdivTerm( differ_count_i_after, expectations_histo( j, kj ) );
+
+		if( differ_count_i_before &gt; 0 )
+                    grad_update -= KLdivTerm( differ_count_i_before, expectations_histo( j, kj ) );
+	    }
+	    else
+		&quot;cas o&#249; on regroupe avec le dernier&quot;;   
+        }
+    }
+    return grad_update *over_dq;
+}
+
+real LayerCostModule::delta_KLdivTerm_2(int i, int j, int index_i, real over_dq, real one_count)
+{
+    PLASSERT( over_dq &gt; 0.0 );
+
+    real grad_update = 0.0;
+
+    real Ni_ki = expectations_histo(i,index_i);
+    real Ni_ki_shift1 = expectations_histo(i,index_i+1);	    
+    real Nj_ki        = expectations_histo(j,index_i);
+    real Nj_ki_shift1 = expectations_histo(j,index_i+1);
+
+    PLASSERT( !fast_exact_is_equal(Ni_ki, 0.0) ); // Verification:
+                                                  // if expectations_histo is up to date,
+                                                  // the expectation(isample,i) has been counted
+    real differ_count_j_before = 0.0;
+    real differ_count_j_after = 0.0;
+    real differ_count_i_before = 0.0;
+    real differ_count_i_after = 0.0;
+    int n_differ_j_before = 0;
+    int n_differ_j_after = 0;
+    int n_differ_i_before = 0;
+    int n_differ_i_after = 0;
+
+    // What follows is only valuable when the qi's are increased (dq&gt;0).
+
+    if( !fast_exact_is_equal(Nj_ki, 0.0) )
+    // if it is zero, then INCREASING qi will not change anything
+    // (it was already counted in the next histograms's bin
+    {
+        // removing the term of the sum that will be modified
+        grad_update -= KLdivTerm( Ni_ki, Nj_ki ) *over_dq;
+							       
+        if( fast_exact_is_equal(Ni_ki, one_count) )
+	{
+            differ_count_j_after = Nj_ki;
+	    n_differ_j_after += 1;
+	}
+        else
+            // adding the term of the sum with its modified value
+            grad_update += KLdivTerm( Ni_ki-one_count, Nj_ki ) *over_dq;
+
+        if( !fast_exact_is_equal(Nj_ki_shift1,0.0) )
+        {
+            // adding the term of the sum with its modified value
+            grad_update += KLdivTerm( Ni_ki_shift1+one_count, Nj_ki_shift1+differ_count_j_after ) *(real)(n_differ_j_after+1)*over_dq ;
+
+            if( !fast_exact_is_equal(Ni_ki_shift1, 0.0) ) // &quot;cas o&#249; on regroupe avec le dernier&quot;;
+            {
+                // removing the term of the sum that will be modified
+                grad_update -= KLdivTerm( Ni_ki_shift1, Nj_ki_shift1 )*over_dq;		
+            }
+	    else
+	    {
+	        // We search   ki' &gt; k(i)+1   such that   n(i,ki') &gt; 0
+		differ_count_j_before = Nj_ki_shift1;
+                n_differ_j_before += 1;
+		int ki;
+		for (ki = index_i+2; ki &lt; histo_size; ki++)
+		{
+		    differ_count_j_before += expectations_histo( j, ki );
+		    if( expectations_histo( i, ki )&gt;0 )
+		        break;
+                    n_differ_j_before += 1;
+		}
+		if( ki &lt; histo_size )
+		{
+                    grad_update -= KLdivTerm( expectations_histo( i, ki ), differ_count_j_before )*(real)(1+n_differ_j_before)*over_dq;
+
+		    if( differ_count_j_before &gt; Nj_ki_shift1 )		
+                        grad_update += KLdivTerm( expectations_histo( i, ki ), differ_count_j_before - Nj_ki_shift1 )*(real)(n_differ_j_before)*over_dq;
+		        // pb avec differ_count_j_after plus haut??? semble pas
+		}
+		else
+		    &quot;cas o&#249; on regroupe avec le dernier&quot;;
+	    }
+        }
+        else
+        {
+            differ_count_i_before = Ni_ki_shift1;
+	    if( differ_count_i_before&gt;0.0 )
+	       n_differ_i_before += 1;
+            differ_count_i_after  = Ni_ki_shift1+one_count;
+	    n_differ_i_after += 1;
+	    int kj;
+	    for( kj = index_i+2; kj &lt; histo_size; kj++)
+	    {
+	        differ_count_i_after += expectations_histo( i, kj );
+		if( differ_count_i_before &gt; 0 )
+		    differ_count_i_before += expectations_histo( i, kj );
+		if( expectations_histo( j, kj ) &gt; 0 )
+		    break;
+		n_differ_i_after += 1;
+		if( differ_count_i_before &gt; 0 )
+		    n_differ_i_before += 1;
+	    }
+	    if( kj &lt; histo_size )
+            {
+                grad_update += KLdivTerm( differ_count_i_after, expectations_histo( j, kj ) ) *(real)(n_differ_i_after+1)*over_dq;
+
+		if( differ_count_i_before &gt; 0 )
+                    grad_update -= KLdivTerm( differ_count_i_before, expectations_histo( j, kj ) ) *(real)(n_differ_i_before+1)*over_dq;
+	    }
+	    else
+		&quot;cas o&#249; on regroupe avec le dernier&quot;;   
+        }
+    }
+    return grad_update;
+}
+
+real LayerCostModule::delta_SafeKLdivTerm(int i, int j, int index_i, real over_dq, real one_count)
+{
+    //PLASSERT( over_dq &gt; 0.0 )
+
+    real grad_update = 0.0;
+		    
+    real Ni_ki = expectations_histo(i,index_i);
+    PLASSERT( !fast_exact_is_equal(Ni_ki, 0.0) ); // Verification:
+                                                  // if expectations_histo is up to date,
+                                                  // the expectation(isample,i) has been counted
+    real Ni_ki_shift1 = expectations_histo(i,index_i+1);
+		    
+    real Nj_ki        = expectations_histo(j,index_i);
+    real Nj_ki_shift1 = expectations_histo(j,index_i+1);
+
+
+        // removing the term of the sum that will be modified
+        grad_update -= KLdivTerm( Ni_ki, Nj_ki );
+							       
+        // adding the term of the sum with its modified value
+        grad_update += KLdivTerm( Ni_ki-one_count, Nj_ki );
+
+        grad_update += KLdivTerm( Ni_ki_shift1+one_count, Nj_ki_shift1 );
+	
+        grad_update -= KLdivTerm( Ni_ki_shift1, Nj_ki_shift1 );
+
+    return grad_update *over_dq;
+}
+
+
+real LayerCostModule::KLdivTerm(real pi, real pj)
+{
+    return ( pj - pi ) * safeflog( pi/pj );
+}
+
+void LayerCostModule::computeHisto(const Mat&amp; expectations)
+{
+    int index, batch_size = expectations.length();
+    real one_count = 1. / (real)batch_size;
+    Vec expectation;
+    
+    expectations_histo.clear(); 
+    for (int isample = 0; isample &lt; batch_size; isample++)
+    {
+        expectation = expectations(isample);
+        for (int i = 0; i &lt; input_size; i++)
+        {
+	    index = histo_index(expectation[i]);
+            expectations_histo(i,index) += one_count;
+        }
+    }
+}
+
+
+
+void LayerCostModule::computeSafeHisto(const Mat&amp; expectations)
+{
+    int index, batch_size = expectations.length();
+    real one_count = 1. / (real)(batch_size+histo_size);
+    Vec expectation;
+    
+    expectations_histo.fill(one_count);
+/*
+    for (int k = 0; k &lt; histo_size; k++)
+        for (int i = 0; i &lt; input_size; i++)
+            expectations_histo(i,k) = one_count;
+*/
+    for (int isample = 0; isample &lt; batch_size; isample++)
+    {
+        expectation = expectations(isample);
+        for (int i = 0; i &lt; input_size; i++)
+        {
+	    index = histo_index(expectation[i]);
+            expectations_histo(i,index) += one_count;
+        }
+    }
+}
+
+
+// Return the index of the (1D) histogram
+// corresponding to the real input value q in [0,1]
+//
+int LayerCostModule::histo_index(real q)
+{
+    if( q &gt;=1.0 )
+       return histo_size-1;
+
+    if( !(q &gt;= 0.0) || !(q &lt; 1.0) )
+        PLERROR(&quot;LayerCostModule detected an anormal expectation value (%f)&quot;, q);
+
+// LINEAR SCALE
+    return (int)floor(q*(real)histo_size);
+
+// LOG SCALE
+    return max(  (int)floor(log(LOGHISTO_BASE, q))+histo_size , 0);
+}
+
+// Returns the minimum amount dq which have to be added/removed to q
+// so that q+dq will be counted in the next/previous bin of the histogram
+//   (cf. LayerCostModule::histo_index)
+//
+// Note: we do not care about cases where histo_index(q)=histo_size
+//      (this is done in the bpropAccUpdate code)
+//
+real LayerCostModule::dq(real q)
+{
+// LINEAR SCALE
+    // ** Simple version **
+    return LINHISTO_STEP;
+
+    // ** Elaborated version **
+    if( fast_exact_is_equal( round(q*(real)histo_size) , ceil(q*(real)histo_size) ) )
+       return LINHISTO_STEP;
+    else
+       return -LINHISTO_STEP;
+
+    // ** BAD VERSION: too unstable **
+    // return (real)histo_index(q+1.0/(real)histo_size)/(real)histo_size - q;
+
+// LOG SCALE
+    // ** Simple version **
+    //if( q &lt; LOGHISTO_MIN )
+    //  return LOGHISTO_BASE * LOGHISTO_MIN - q;
+    //return q*(LOGHISTO_BASE-1.0);
+    
+    // ** BAD VERSION: too unstable **
+    // real REF = LOGHISTO_BASE * LOGHISTO_MIN;
+    // while( true )
+    // {
+    //     if( q &lt; REF )
+    //         return REF - q;
+    //     REF *= LOGHISTO_BASE;
+    // }
+}
+
+
+
+
+
+////////////
+// forget //
+////////////
+void LayerCostModule::forget()
+{
+    if( momentum &gt; 0.0)
+    {
+        expectations_expectation_trainMemory.clear();
+        expectations_cross_quadratic_mean_trainMemory.clear();
+    }
+/*
+    expectations_expectation_testMemory.clear();
+    expectations_cross_quadratic_mean_testMemory.clear();
+    ntest = 0;
+*/
+}
+
+
+/////////////////
+// addPortName //
+/////////////////
+void LayerCostModule::addPortName(const string&amp; name)
+{
+    PLASSERT( portname_to_index.find(name) == portname_to_index.end() );
+    portname_to_index[name] = ports.length();
+    ports.append(name);
+}
+
+//////////////
+// getPorts //
+//////////////
+const TVec&lt;string&gt;&amp; LayerCostModule::getPorts()
+{
+    return ports;
+}
+
+///////////////////
+// getPortsSizes //
+///////////////////
+const TMat&lt;int&gt;&amp; LayerCostModule::getPortSizes()
+{
+    return port_sizes;
+}
+
+//////////////////
+// getPortIndex //
+//////////////////
+int LayerCostModule::getPortIndex(const string&amp; port)
+{
+    map&lt;string, int&gt;::const_iterator it = portname_to_index.find(port);
+    if (it == portname_to_index.end())
+        return -1;
+    else
+        return it-&gt;second;
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/LayerCostModule.h
===================================================================
--- trunk/plearn_learners/online/LayerCostModule.h	2007-09-11 19:41:53 UTC (rev 8069)
+++ trunk/plearn_learners/online/LayerCostModule.h	2007-09-12 17:24:37 UTC (rev 8070)
@@ -0,0 +1,191 @@
+// -*- C++ -*-
+
+// LayerCostModule.h
+//
+// Copyright (C) 2007 Jerome Louradour
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Jerome Louradour
+
+/*! \file LayerCostModule.h */
+
+#ifndef LayerCostModule_INC
+#define LayerCostModule_INC
+
+#include &lt;map&gt;
+#include &lt;plearn_learners/online/OnlineLearningModule.h&gt;
+#include &lt;plearn/vmat/VMat.h&gt;
+
+namespace PLearn {
+
+/**
+ * Computes a cost function for a (hidden) representation, given two &quot;expectation&quot; vectors. Backpropagates it.
+ */
+class LayerCostModule : public OnlineLearningModule
+{
+    typedef OnlineLearningModule inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    string cost_function;
+    
+    int histo_size;
+    
+    real alpha;
+    
+    real momentum;
+    
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    LayerCostModule();
+
+
+    //! given the input and target, compute the cost
+    virtual void fprop(const Vec&amp; expectation, real&amp; cost) const;
+    virtual void fprop(const Mat&amp; expectations, Mat&amp; costs);
+    //! Overridden.
+    virtual void fprop(const TVec&lt;Mat*&gt;&amp; ports_value);
+    
+    //! backpropagate the derivative w.r.t. activation
+    virtual void bpropAccUpdate(const TVec&lt;Mat*&gt;&amp; ports_value,
+                                const TVec&lt;Mat*&gt;&amp; ports_gradient);
+
+    //! Some auxiliary function to deal with empirical histograms
+    virtual void computeHisto(const Mat&amp; expectations);
+    virtual void computeSafeHisto(const Mat&amp; expectations);
+    virtual real delta_KLdivTerm(int i, int j, int index_i, real over_dq, real one_count);
+    virtual real delta_KLdivTerm_2(int i, int j, int index_i, real over_dq, real one_count);
+    virtual real delta_SafeKLdivTerm(int i, int j, int index_i, real over_dq, real one_count);
+    virtual real KLdivTerm(real pi, real pj);
+    virtual int histo_index(real q);
+    virtual real dq(real q);
+
+    //! Auxiliary function for the pascal's cost function
+    virtual void computePascalStatistics(const Mat&amp; expectations, bool duringTraining);
+
+    //! Overridden to do nothing (in particular, no warning).
+    virtual void setLearningRate(real dynamic_learning_rate) {}
+
+    //! Returns all ports in a RBMModule.
+    virtual const TVec&lt;string&gt;&amp; getPorts();
+
+    //! The ports' sizes are given by the corresponding RBM layers.
+    virtual const TMat&lt;int&gt;&amp; getPortSizes();
+
+    //! Return the index (as in the list of ports returned by getPorts()) of
+    //! a given port.
+    //! If 'port' does not exist, -1 is returned.
+    virtual int getPortIndex(const string&amp; port);
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(LayerCostModule);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
+
+    virtual void forget();
+
+protected:
+
+    //! Does stochastic gradient makes sense with our cost function?
+    bool is_cost_function_stochastic;
+
+    //! Histograms of expectations (estimated empiricially on the data)
+    Mat expectations_histo;
+
+    //! Some features of the histogram of expectations
+    real LINHISTO_STEP;
+    real LOGHISTO_BASE;
+    real LOGHISTO_MIN;
+
+    //! Statistics on matrix of expectations (estimated empiricially on the data)
+    Vec expectations_expectation;
+    Mat expectations_cross_quadratic_mean;
+    Vec expectations_expectation_trainMemory;
+    Mat expectations_cross_quadratic_mean_trainMemory;
+    Vec expectations_expectation_testMemory;
+    Mat expectations_cross_quadratic_mean_testMemory;
+    int ntest;
+
+    //! Map from a port name to its index in the 'ports' vector.
+    map&lt;string, int&gt; portname_to_index;
+
+    //! List of port names.
+    TVec&lt;string&gt; ports;
+
+    //#####  Protected Member Functions  ######################################
+
+    //! Add a new port to the 'portname_to_index' map and 'ports' vector.
+    void addPortName(const string&amp; name);
+
+    //! Declares the class options.
+    static void declareOptions(OptionList&amp; ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(LayerCostModule);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001517.html">[Plearn-commits] r8069 - trunk/python_modules/plearn/pymake
</A></li>
	<LI>Next message: <A HREF="001519.html">[Plearn-commits] r8071 - trunk/plearn/python
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1518">[ date ]</a>
              <a href="thread.html#1518">[ thread ]</a>
              <a href="subject.html#1518">[ subject ]</a>
              <a href="author.html#1518">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
