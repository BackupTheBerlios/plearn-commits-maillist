From chapados at mail.berlios.de  Mon Jul  2 22:42:07 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Mon, 2 Jul 2007 22:42:07 +0200
Subject: [Plearn-commits] r7685 - trunk/plearn/vmat
Message-ID: <200707022042.l62Kg7Es011118@sheep.berlios.de>

Author: chapados
Date: 2007-07-02 22:42:06 +0200 (Mon, 02 Jul 2007)
New Revision: 7685

Modified:
   trunk/plearn/vmat/BootstrapVMatrix.cc
Log:
Bugfix when number of samples is larger than source matrix and no repetitions is wanted

Modified: trunk/plearn/vmat/BootstrapVMatrix.cc
===================================================================
--- trunk/plearn/vmat/BootstrapVMatrix.cc	2007-06-30 02:32:23 UTC (rev 7684)
+++ trunk/plearn/vmat/BootstrapVMatrix.cc	2007-07-02 20:42:06 UTC (rev 7685)
@@ -174,7 +174,7 @@
         {
             indices = TVec<int>(0, l-1, 1); // Range-vector
             rgen->shuffleElements(indices);
-            indices = indices.subVec(0, nsamp);
+            indices = indices.subVec(0, min(indices.size(), nsamp));
         }
         if (!shuffle)
             sortElements(indices);



From nouiz at mail.berlios.de  Tue Jul  3 17:19:57 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 3 Jul 2007 17:19:57 +0200
Subject: [Plearn-commits] r7686 - trunk/plearn_learners/generic
Message-ID: <200707031519.l63FJvqB007268@sheep.berlios.de>

Author: nouiz
Date: 2007-07-03 17:19:57 +0200 (Tue, 03 Jul 2007)
New Revision: 7686

Modified:
   trunk/plearn_learners/generic/AddCostToLearner.cc
   trunk/plearn_learners/generic/AddCostToLearner.h
Log:
Added linear_class_error, square_class_error and confusion_matrix as cost


Modified: trunk/plearn_learners/generic/AddCostToLearner.cc
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.cc	2007-07-02 20:42:06 UTC (rev 7685)
+++ trunk/plearn_learners/generic/AddCostToLearner.cc	2007-07-03 15:19:57 UTC (rev 7686)
@@ -92,7 +92,8 @@
       rescale_output(0),
       rescale_target(0),
       to_max(1),
-      to_min(0)
+      to_min(0),
+      nb_class(-1)
 {}
 
 ////////////////////
@@ -127,6 +128,12 @@
         "   target that must be either 0 or 1. The output must also be one-\n"
         "   dimensional, and is interpreted as the predicted probability for\n"
         "   class 1 (thus class 1 is chosen when the output is > 0.5)\n"
+        " - 'linear_class_error': as class_error execpt that the output is the\n"
+        "   difference between the class values\n"
+        " - 'square_class_error': as class_error execpt that the output is the\n"
+        "   square of the difference between the class values\n"
+        " - 'confusion_matrix': give the confusion matrix,\n"
+        "   the row is the predicted class, the column is the targeted class\n"
         " - 'lift_output': to compute the lift cost (for the positive class)\n"
         " - 'opposite_lift_output': to compute the lift cost (for the negative) class\n"
         " - 'cross_entropy': -t*log(o) - (1-t)*log(1-o)\n"
@@ -159,6 +166,9 @@
 
     declareOption(ol, "to_min", &AddCostToLearner::to_min, OptionBase::buildoption,
                   "Lower bound of the destination interval [to_min, to_max] (used in rescaling).");
+    
+    declareOption(ol, "nb_class", &AddCostToLearner::nb_class, OptionBase::buildoption,
+                  "The number of class. Used by the confusion_matrix cost");
 
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
@@ -228,6 +238,11 @@
         } else if (c == "squared_norm_reconstruction_error") {
         } else if (c == "class_error") {
         } else if (c == "binary_class_error") {
+        } else if (c == "linear_class_error") {
+        } else if (c == "square_class_error") {
+        } else if (c == "confusion_matrix") {
+            if(nb_class<=0)
+                PLERROR("In AddCostToLearner::build_ there must be a positive number of class. nb_class ="+nb_class);
         } else if (c == "NLL") {
             // Output should be in [0,1].
             output_min = max(output_min, real(0));
@@ -374,10 +389,10 @@
             }
         }
     }
-
+    int ind_cost = n_original_costs - 1;
     for (int i = 0; i < this->costs.length(); i++) {
         string c = this->costs[i];
-        int ind_cost = i + n_original_costs;
+        ind_cost++;
         if (c == "lift_output" || c == "opposite_lift_output") {
 #ifdef BOUNDCHECK
             if (desired_target.length() != 1 && (sub_learner_output.length() != 1 || sub_learner_output.length() != 2)) {
@@ -444,7 +459,7 @@
                     "The sub learner output must have a size equal to the "
                     "number of classes");
             costs[ind_cost] = - pl_log(sub_learner_output[class_target]);
-        } else if (c == "class_error") {
+       } else if (c == "class_error") {
             int output_length = sub_learner_output.length();
             bool good = true;
             if (output_length == target_length) {
@@ -470,6 +485,46 @@
             PLASSERT( sub_learner_output.length() == 1 );
             real predict = sub_learner_output[0] > 0.5 ? 1 : 0;
             costs[ind_cost] = is_equal(t, predict) ? 0 : 1;
+         } else if (c == "linear_class_error") {
+            int output_length = sub_learner_output.length();
+            int diff = 0;
+            if (output_length == target_length) {
+                for (int k = 0; k < desired_target.length(); k++)
+                    diff += abs(int(round(desired_target[k])) - int(round(sub_learner_output[k])));
+            } else if (target_length == 1) {
+                // We assume the target is a number between 0 and c-1, and the output
+                // is a vector of length c giving the weight for each class.
+                diff = abs(argmax(sub_learner_output) - int(round(desired_target[0])));
+            } else {
+                PLERROR("In AddCostToLearner::computeCostsFromOutputs - Wrong "
+                        "output and/or target for the 'linear_class_error' cost");
+            }
+            costs[ind_cost] = diff;
+         } else if (c == "square_class_error") {
+            int output_length = sub_learner_output.length();
+            int diff = 0;
+            if (output_length == target_length) {
+                for (int k = 0; k < desired_target.length(); k++) {
+                    int d = int(round(desired_target[k])) - int(round(sub_learner_output[k]));
+                    diff += d*d;
+                }
+            } else if (target_length == 1) {
+                // We assume the target is a number between 0 and c-1, and the output
+                // is a vector of length c giving the weight for each class.
+                diff = argmax(sub_learner_output) - int(round(desired_target[0]));
+                diff *= diff;
+            } else {
+                PLERROR("In AddCostToLearner::computeCostsFromOutputs - Wrong "
+                        "output and/or target for the 'square_class_error' cost");
+            }
+            costs[ind_cost] = diff;
+        } else if (c == "confusion_matrix") {
+            for(int local_ind = ind_cost ; local_ind < (nb_class*nb_class+ind_cost); local_ind++){
+                costs[local_ind] = 0;
+            }
+            int local_ind = ind_cost + argmax(sub_learner_output) + int(round(desired_target[0]))*nb_class;
+            costs[local_ind] = 1;
+            ind_cost += nb_class*nb_class - 1;//less one as the loop add one
         } else if (c == "mse") {
             costs[ind_cost] = powdistance(desired_target, sub_learner_output);
         } else if (c == "squared_norm_reconstruction_error") {
@@ -507,7 +562,14 @@
 {
     TVec<string> sub_costs = learner_->getTestCostNames();
     for (int i = 0; i < this->costs.length(); i++) {
-        sub_costs.append(costs[i]);
+        if(costs[i] == "confusion_matrix")
+            for(int conf_i=0; conf_i< nb_class;conf_i++)
+                for(int conf_j=0; conf_j<nb_class;conf_j++){
+                    string s = "confusion_matrix_i"+tostring(conf_i)+"_j"+tostring(conf_j);
+                    sub_costs.append(s);
+                }
+        else
+            sub_costs.append(costs[i]);
     }
     return sub_costs;
 }

Modified: trunk/plearn_learners/generic/AddCostToLearner.h
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.h	2007-07-02 20:42:06 UTC (rev 7685)
+++ trunk/plearn_learners/generic/AddCostToLearner.h	2007-07-03 15:19:57 UTC (rev 7686)
@@ -121,6 +121,7 @@
     bool rescale_target;
     real to_max;
     real to_min;
+    int nb_class;
 
     // ****************
     // * Constructors *



From tihocan at mail.berlios.de  Tue Jul  3 19:49:37 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 3 Jul 2007 19:49:37 +0200
Subject: [Plearn-commits] r7687 - trunk/plearn_learners/generic
Message-ID: <200707031749.l63Hnbu4031788@sheep.berlios.de>

Author: tihocan
Date: 2007-07-03 19:49:37 +0200 (Tue, 03 Jul 2007)
New Revision: 7687

Modified:
   trunk/plearn_learners/generic/AddCostToLearner.cc
   trunk/plearn_learners/generic/AddCostToLearner.h
Log:
Renamed 'nb_class' into 'n_classes' (for coherency with other variable names in PLearn)

Modified: trunk/plearn_learners/generic/AddCostToLearner.cc
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.cc	2007-07-03 15:19:57 UTC (rev 7686)
+++ trunk/plearn_learners/generic/AddCostToLearner.cc	2007-07-03 17:49:37 UTC (rev 7687)
@@ -93,7 +93,7 @@
       rescale_target(0),
       to_max(1),
       to_min(0),
-      nb_class(-1)
+      n_classes(-1)
 {}
 
 ////////////////////
@@ -167,8 +167,8 @@
     declareOption(ol, "to_min", &AddCostToLearner::to_min, OptionBase::buildoption,
                   "Lower bound of the destination interval [to_min, to_max] (used in rescaling).");
     
-    declareOption(ol, "nb_class", &AddCostToLearner::nb_class, OptionBase::buildoption,
-                  "The number of class. Used by the confusion_matrix cost");
+    declareOption(ol, "n_classes", &AddCostToLearner::n_classes, OptionBase::buildoption,
+        "The number of classes. Only needed for the 'confusion_matrix' cost.");
 
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
@@ -241,8 +241,8 @@
         } else if (c == "linear_class_error") {
         } else if (c == "square_class_error") {
         } else if (c == "confusion_matrix") {
-            if(nb_class<=0)
-                PLERROR("In AddCostToLearner::build_ there must be a positive number of class. nb_class ="+nb_class);
+            if(n_classes<=0)
+                PLERROR("In AddCostToLearner::build_ there must be a positive number of class. n_classes ="+n_classes);
         } else if (c == "NLL") {
             // Output should be in [0,1].
             output_min = max(output_min, real(0));
@@ -519,12 +519,12 @@
             }
             costs[ind_cost] = diff;
         } else if (c == "confusion_matrix") {
-            for(int local_ind = ind_cost ; local_ind < (nb_class*nb_class+ind_cost); local_ind++){
+            for(int local_ind = ind_cost ; local_ind < (n_classes*n_classes+ind_cost); local_ind++){
                 costs[local_ind] = 0;
             }
-            int local_ind = ind_cost + argmax(sub_learner_output) + int(round(desired_target[0]))*nb_class;
+            int local_ind = ind_cost + argmax(sub_learner_output) + int(round(desired_target[0]))*n_classes;
             costs[local_ind] = 1;
-            ind_cost += nb_class*nb_class - 1;//less one as the loop add one
+            ind_cost += n_classes*n_classes - 1;//less one as the loop add one
         } else if (c == "mse") {
             costs[ind_cost] = powdistance(desired_target, sub_learner_output);
         } else if (c == "squared_norm_reconstruction_error") {
@@ -563,8 +563,8 @@
     TVec<string> sub_costs = learner_->getTestCostNames();
     for (int i = 0; i < this->costs.length(); i++) {
         if(costs[i] == "confusion_matrix")
-            for(int conf_i=0; conf_i< nb_class;conf_i++)
-                for(int conf_j=0; conf_j<nb_class;conf_j++){
+            for(int conf_i=0; conf_i< n_classes;conf_i++)
+                for(int conf_j=0; conf_j<n_classes;conf_j++){
                     string s = "confusion_matrix_i"+tostring(conf_i)+"_j"+tostring(conf_j);
                     sub_costs.append(s);
                 }

Modified: trunk/plearn_learners/generic/AddCostToLearner.h
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.h	2007-07-03 15:19:57 UTC (rev 7686)
+++ trunk/plearn_learners/generic/AddCostToLearner.h	2007-07-03 17:49:37 UTC (rev 7687)
@@ -121,7 +121,7 @@
     bool rescale_target;
     real to_max;
     real to_min;
-    int nb_class;
+    int n_classes;
 
     // ****************
     // * Constructors *



From lamblin at mail.berlios.de  Wed Jul  4 02:41:35 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 4 Jul 2007 02:41:35 +0200
Subject: [Plearn-commits] r7688 - trunk/python_modules/plearn/pymake
Message-ID: <200707040041.l640fZQ5012078@sheep.berlios.de>

Author: lamblin
Date: 2007-07-04 02:41:33 +0200 (Wed, 04 Jul 2007)
New Revision: 7688

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
Add support for 'hostname/nprocesses' syntax in .hosts files


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2007-07-03 17:49:37 UTC (rev 7687)
+++ trunk/python_modules/plearn/pymake/pymake.py	2007-07-04 00:41:33 UTC (rev 7688)
@@ -628,7 +628,7 @@
             i = token.find('/')
             if i == -1:
                 num = 1
-                host = host[1:]
+                host = token[1:]
             else:
                 num = int(token[i+1:])
                 host = token[1:i]
@@ -640,7 +640,7 @@
     # For distcc, localhost is usually not added when the list of hosts is
     # long because localhost is busy doing the preprocessing. This does not
     # hold for pymake, so add localhost if it is not present.
-    if 'localhost' not in hosts:
+    if 'localhost' not in hosts and myhostname not in hosts:
         hosts.extend(['localhost'] * smallest_num)
 
     return hosts
@@ -692,9 +692,39 @@
     else:
         return _process_distcc_hosts(contents)
 
+def process_hostspath_list(hostspath_list):
+    list_of_hosts = []
+    for hostspath in hostspath_list:
+        f = open(hostspath,'r')
+        for line in f:
+            line = line.strip()
+            # Remove comments
+            i = line.find('#')
+            if i != -1:
+                line = line[:i]
 
+            # Get number of processors to use (syntax: host/n_proc)
+            i = line.find('/')
+            if i == -1:
+                host = line
+                n_proc = 1
+            else:
+                host = line[:i]
+                n_proc = int(line[i+1:])
+
+            # Remove ''
+            if host:
+                list_of_hosts.extend([host] * n_proc)
+
+        f.close()
+
+    if 'localhost' not in list_of_hosts and myhostname not in list_of_hosts:
+        list_of_hosts.extend(['localhost'] * nprocesses_on_localhost)
+
+    return list_of_hosts
+
+
 def get_list_of_hosts():
-    list_of_hosts = []
     distcc_list_of_hosts = get_distcc_hosts()
 
     if force_32bits:
@@ -711,16 +741,12 @@
         if distcc_list_of_hosts is not None:
             print '*** Overriding distcc settings. (Remove pymake *.hosts file to use distcc settings.)'
         print '*** Parallel compilation using list of hosts from file(s): ' + string.join( hostspath_list, ', ' )
-        for hostspath in hostspath_list:
-            f = open(hostspath,'r')
-            list_of_hosts = list_of_hosts + filter(lambda s: s and s[0]!='#', map(string.strip,f.readlines()))
-            f.close()
-        list_of_hosts = nprocesses_per_processor * list_of_hosts
+        list_of_hosts = process_hostspath_list(hostspath_list)
     elif distcc_list_of_hosts is not None:
         print '*** Parallel compilation using distcc list of hosts (%d)' % len(distcc_list_of_hosts)
         list_of_hosts = distcc_list_of_hosts
     else:
-        list_of_hosts = nprocesses_per_processor * ['localhost']
+        list_of_hosts = nprocesses_on_localhost * ['localhost']
         print '*** No hosts file found: no parallel compilation, using localhost'
         print '    (create a '+platform + '.hosts file in your .pymake directory to list hosts for parallel compilation.)'
 
@@ -1990,12 +2016,22 @@
         # message to distinguish that case from the case of (say) a
         # single error message for a syntax error, etc.
         if not hasattr(self,"compilation_status"):
-            if warningmsgs and warningmsgs[0].startswith('ssh: '):
-                # The hostname has a problem, so we remove it from the list
-                # and retry on another machine
-                list_of_hosts.remove(self.hostname)
-                self.hostname = ''
-                self.retry_compilation = True
+            if warningmsgs:
+                if warningmsgs[0].startswith('ssh: '):
+                    # The hostname has a problem, so we remove it from the list
+                    # and retry on another machine
+                    list_of_hosts.remove(self.hostname)
+                    self.hostname = ''
+                    self.retry_compilation = True
+                elif warningmsgs[0].startswith('ssh_exchange_identification :'):
+                    # It happens sometimes when logging multiple times onto the
+                    # same machine. No need to remove it from the list, we will
+                    # try again.
+                    self.retry_compilation = True
+                else:
+                    # Warning messages were uninformative, abort
+                    self.compilation_status = -2
+                    self.retry_compilation = False
             else:
                 # There was an undefined problem,
                 # we consider the compilation has failed
@@ -2324,7 +2360,7 @@
             useqt, qtdir, \
             default_compiler, default_linker, \
             sourcedirs, mandatory_includedirs, linkeroptions_tail, \
-            options_choices, nprocesses_per_processor, rshcommand, \
+            options_choices, nprocesses_on_localhost, rshcommand, \
             compileflags, cpp_variables, cpp_definitions, \
             objspolicy, objsdir, \
             default_wrapper, dllwrap_basic_options, \
@@ -2369,7 +2405,7 @@
     mandatory_includedirs = []
     linkeroptions_tail = '-lstdc++ -lm'
     options_choices = []
-    nprocesses_per_processor = 1
+    nprocesses_on_localhost = 1
     rshcommand = 'rsh '
     compileflags = ''
     # List of all C preprocessor variables possibly initialized at compilation
@@ -2527,9 +2563,10 @@
                 if (option[5] != '='):
                     print 'Syntax for \'-local\' option is \'-local=<nb_proc>\', but' \
                           ' read \'' + option + '\': one processor will be used'
-                    nprocesses_per_processor=1
+                    # Keep default value (defined in config file or above
+                    # nprocesses_on_localhost=1
                 else:
-                    nprocesses_per_processor=int(option[6:])
+                    nprocesses_on_localhost=int(option[6:])
 
     local_ofiles_base_path= '/tmp/.pymake/local_ofiles/' # could add an option for that...
 
@@ -2654,7 +2691,7 @@
     ####  Get the list of hosts on which to compile
     # (Will be obsoleted when the batch job interface will come out)
     if local_compilation:
-        list_of_hosts = nprocesses_per_processor * ['localhost']
+        list_of_hosts = nprocesses_on_localhost * ['localhost']
     else:
         list_of_hosts = get_list_of_hosts()
 



From lamblin at mail.berlios.de  Wed Jul  4 04:34:06 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 4 Jul 2007 04:34:06 +0200
Subject: [Plearn-commits] r7689 - in trunk: . python_modules/plearn/pymake
Message-ID: <200707040234.l642Y66i018369@sheep.berlios.de>

Author: lamblin
Date: 2007-07-04 04:34:04 +0200 (Wed, 04 Jul 2007)
New Revision: 7689

Modified:
   trunk/pymake.config.model
   trunk/python_modules/plearn/pymake/pymake.py
Log:
Support for 'host/nprocesses nice_value' format in pymake *.hosts files


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2007-07-04 00:41:33 UTC (rev 7688)
+++ trunk/pymake.config.model	2007-07-04 02:34:04 UTC (rev 7689)
@@ -65,7 +65,7 @@
 if domain_name.endswith('apstat.com') or domain_name.endswith('iro.umontreal.ca') or domain_name.endswith('public'):
     rshcommand= 'ssh -x'
 
-nice_value = '1'
+default_nice_value = 1
 
 # List of directories in which to look for .h includes and corresponding
 # .cc files to compile and link with your program (no need to include
@@ -753,7 +753,7 @@
 
 #####  Network Setup  #######################################################
 
-# nprocesses_per_processor = 1
+# nprocesses_on_localhost = 1
 
 # ###
 # # Declaration of other options

Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2007-07-04 00:41:33 UTC (rev 7688)
+++ trunk/python_modules/plearn/pymake/pymake.py	2007-07-04 02:34:04 UTC (rev 7689)
@@ -694,13 +694,22 @@
 
 def process_hostspath_list(hostspath_list):
     list_of_hosts = []
+    nice_values = {} # dictionary containing hostname:nice_value
     for hostspath in hostspath_list:
         f = open(hostspath,'r')
         for line in f:
+            nice_value = default_nice_value
             line = line.strip()
+
             # Remove comments
             i = line.find('#')
             if i != -1:
+                line = line[:i].strip()
+
+            # Get nice value to use (a number, preceded by a space)
+            i = line.find(' ')
+            if i != -1:
+                nice_value = int(line[i+1:])
                 line = line[:i]
 
             # Get number of processors to use (syntax: host/n_proc)
@@ -712,19 +721,21 @@
                 host = line[:i]
                 n_proc = int(line[i+1:])
 
-            # Remove ''
+            # Remove empty strings
             if host:
                 list_of_hosts.extend([host] * n_proc)
+                nice_values[host] = nice_value
 
         f.close()
 
     if 'localhost' not in list_of_hosts and myhostname not in list_of_hosts:
         list_of_hosts.extend(['localhost'] * nprocesses_on_localhost)
 
-    return list_of_hosts
+    return (list_of_hosts, nice_values)
 
 
 def get_list_of_hosts():
+    nice_values = {}
     distcc_list_of_hosts = get_distcc_hosts()
 
     if force_32bits:
@@ -741,7 +752,7 @@
         if distcc_list_of_hosts is not None:
             print '*** Overriding distcc settings. (Remove pymake *.hosts file to use distcc settings.)'
         print '*** Parallel compilation using list of hosts from file(s): ' + string.join( hostspath_list, ', ' )
-        list_of_hosts = process_hostspath_list(hostspath_list)
+        (list_of_hosts, nice_values) = process_hostspath_list(hostspath_list)
     elif distcc_list_of_hosts is not None:
         print '*** Parallel compilation using distcc list of hosts (%d)' % len(distcc_list_of_hosts)
         list_of_hosts = distcc_list_of_hosts
@@ -752,7 +763,7 @@
 
     # randomize order of list_of_hosts
     shuffle(list_of_hosts)
-    return list_of_hosts
+    return (list_of_hosts, nice_values)
 
 
 ###  For text processing of the source files
@@ -911,7 +922,8 @@
 
 ###  File compilation and linking
 
-def parallel_compile(files_to_compile, num_retries=3, ofiles_to_copy=[]):
+def parallel_compile(files_to_compile, list_of_hosts, nice_values={},
+        num_retries=3, ofiles_to_copy=[]):
     """files_to_compile is a list of FileInfo of .cc files"""
 
     def wait_for_some_completion(outs, errs, ofiles_to_copy, files_to_check):
@@ -931,7 +943,13 @@
             info = outs[f]
         del errs[info.launched.childerr]
         del outs[info.launched.fromchild]
-        info.finished_compilation() # print error messages, warnings, and get failure/success status
+
+        # print error messages, warnings, and get failure/success status
+        info.finished_compilation()
+        if info.remove_hostname:
+            list_of_hosts.remove(info.hostname)
+            info.hostname = ''
+
         # check for new ofiles to copy
         if local_ofiles:
             for f in files_to_check:
@@ -963,19 +981,21 @@
                 hostname = 'localhost'
             else:
                 hostname = list_of_hosts[hostnum]
-            ccfile.launch_compilation(hostname)
+            nice_value = nice_values.get(hostname, default_nice_value)
+            ccfile.launch_compilation(hostname, nice_value)
             outs[ccfile.launched.fromchild] = ccfile
             errs[ccfile.launched.childerr] = ccfile
             hostnum = hostnum + 1
         else: # all processes are busy, wait for something to finish...
             hostname = ''
             while hostname == '':
-                hostname= wait_for_some_completion(outs, errs, ofiles_to_copy, files_to_check)
+                hostname = wait_for_some_completion(outs, errs, ofiles_to_copy, files_to_check)
                 # This should not happen unless localhost is unable to compile
                 if not list_of_hosts:
                     raise "Couldn't access ANY of the listed hosts for compilation"
             else:
-                ccfile.launch_compilation(hostname)
+                nice_value = nice_values.get(hostname, default_nice_value)
+                ccfile.launch_compilation(hostname, nice_value)
                 outs[ccfile.launched.fromchild] = ccfile
                 errs[ccfile.launched.childerr] = ccfile
 
@@ -992,7 +1012,8 @@
         if num_retries > 0:
             if verbose >= 2:
                 print '[ RETRYING COMPILATION FOR %d FILES ]' % len(ccfiles_to_retry)
-            parallel_compile(ccfiles_to_retry, num_retries-1, ofiles_to_copy)
+            parallel_compile(ccfiles_to_retry, list_of_hosts, nice_values,
+                    num_retries-1, ofiles_to_copy)
         else:
             if verbose >= 2:
                 print '[ %d FILES TO RETRY, BUT NO MORE TRIES AVAILABLE ]' % len(ccfiles_to_retry)
@@ -1002,7 +1023,7 @@
 
     for ccfile in files_to_compile:
         ccfile.make_objs_dir() # make sure the OBJS dir exists, otherwise the command will fail
-        ccfile.launch_compilation('localhost')
+        ccfile.launch_compilation('localhost', default_nice_value)
         ccfile.finished_compilation() # print error messages, warnings, and get failure/success status
 
 def sequential_link(executables_to_link, linkname):
@@ -1492,6 +1513,7 @@
     - errormsgs: a list of error messages taken from the launched process' stdout
     - warningmsgs: a list of warning messages taken from the launched process' stderr
     - retry_compilation: whether to retry the compilation because the current failure was caused by an out of memory error, etc.
+    - remove_hostname: whether to remove hostname from list_of_hosts, because the current failure was caused by this machine
     """
 
     # static patterns useful for parsing the file
@@ -1823,7 +1845,7 @@
         the symbolic link, is specified then the symbolic link will have
         this path and name, and the object files will be placed in the OBJS
         subdirectory of the directory where the source file is.
-        
+
         If the third argument is given, it overrides the target of the link,
         i.e. the location of the executable object."""
 
@@ -1896,7 +1918,7 @@
 
         return compileroptions
 
-    def compile_command(self):
+    def compile_command(self, nice_value):
         """returns the command line necessary to compile this .cc file"""
 
         compiler = default_compiler
@@ -1907,10 +1929,10 @@
                 compiler = optdef.compiler
 
 
-        if platform == 'win32' or nice_value == '0':
-          nice = ''
+        if platform == 'win32' or nice_value == 0:
+            nice = ''
         else:
-          nice = nice_command + nice_value + ' '
+            nice = nice_command + str(nice_value) + ' '
 
         command = nice + compiler + ' ' + self.compiler_options() + \
                   ' -o ' + self.corresponding_ofile + ' -c ' + self.filepath
@@ -1930,9 +1952,9 @@
 
 
 
-    def launch_compilation(self,hostname):
+    def launch_compilation(self, hostname, nice_value):
         """Launches the compilation of this .cc file on given host using rsh
-        (or ssh if the -ssh option is given).
+        (or ssh if the -ssh option is given), with given nice value.
         The call returns the pid of the launched process.
         This will create a field 'launched', which is a Popen3 object,
         and 'hostname' to remember on which host it has been launched.
@@ -1945,7 +1967,7 @@
             pass
 
         # We do an 'echo $?'  at the end because rsh doesn't transmit the status byte correctly.
-        quotedcommand = "'cd " + self.filedir + "; " + self.compile_command() + "; echo $?'"
+        quotedcommand = "'cd " + self.filedir + "; " + self.compile_command(nice_value) + "; echo $?'"
         self.hostname = hostname
         if hostname=='localhost' or hostname==myhostname:
             if platform=='win32':
@@ -1997,6 +2019,7 @@
             del errormsgs[-1]
 
         msg = warningmsgs + errormsgs
+        self.remove_hostname = False
 
         # Somewhat dumb heuristic to figure out why compilation failed.
         #
@@ -2020,10 +2043,9 @@
                 if warningmsgs[0].startswith('ssh: '):
                     # The hostname has a problem, so we remove it from the list
                     # and retry on another machine
-                    list_of_hosts.remove(self.hostname)
-                    self.hostname = ''
+                    self.remove_hostname = True
                     self.retry_compilation = True
-                elif warningmsgs[0].startswith('ssh_exchange_identification :'):
+                elif warningmsgs[0].startswith('ssh_exchange_identification: '):
                     # It happens sometimes when logging multiple times onto the
                     # same machine. No need to remove it from the list, we will
                     # try again.
@@ -2245,12 +2267,12 @@
                 os.remove(corresponding_output)
             except OSError:
                 pass
-            
+
             if local_ofiles:
                 copyfile_verbose(self.corresponding_output, new_corresponding_output)
-                    
+
             os.rename(new_corresponding_output, corresponding_output)
-                   
+
             if verbose>=2:
                 print "Successfully created " + new_corresponding_output + " and renamed it to " + corresponding_output
 
@@ -2364,7 +2386,7 @@
             compileflags, cpp_variables, cpp_definitions, \
             objspolicy, objsdir, \
             default_wrapper, dllwrap_basic_options, \
-            nice_command, nice_value, verbose
+            nice_command, default_nice_value, verbose
 
     # Variables that can be useful to have read access to in the config file
     global optionargs, otherargs, linkname, link_target_override, \
@@ -2377,7 +2399,7 @@
     # TODO: fix it
     global cpp_exts, h_exts, \
             undefined_cpp_vars, defined_cpp_vars, cpp_vars_values, \
-            list_of_hosts, options
+            options
 
     # initialize a few variables from the environment
     platform = get_platform()
@@ -2430,7 +2452,7 @@
 
     # nice default command and value
     nice_command = 'env nice -n'
-    nice_value = '10'
+    default_nice_value = 10
     verbose=3
 
     ######## Regular pymake processing
@@ -2664,7 +2686,7 @@
     if 'help' in optionargs:
         printusage()
         sys.exit()
-        
+
     if 'getoptions' in optionargs:
         if len(otherargs)==0:
             print 'Usage of "-getoptions" is: pymake -getoptions <list of targets, files or directories>'
@@ -2674,7 +2696,7 @@
             configpath = get_config_path(target)
             execfile( configpath, globals() )
             printusage()
-            
+
         sys.exit()
 
     if 'vcproj' in optionargs:
@@ -2688,12 +2710,13 @@
     else:
         vcproj = 0
 
-    ####  Get the list of hosts on which to compile
+    ####  Get the list of hosts on which to compile, and nice values
     # (Will be obsoleted when the batch job interface will come out)
+    nice_values = {}
     if local_compilation:
         list_of_hosts = nprocesses_on_localhost * ['localhost']
     else:
-        list_of_hosts = get_list_of_hosts()
+        (list_of_hosts, nice_values) = get_list_of_hosts()
 
 
     ######  The compilation and linking
@@ -2738,9 +2761,10 @@
             if platform=='win32':
                 win32_parallel_compile(ccfiles_to_compile.keys())
             else:
-                ofiles_to_copy= get_ofiles_to_copy(executables_to_link.keys())
-                ofiles_to_copy= [x for x in ofiles_to_copy if x not in [y.corresponding_ofile for y in ccfiles_to_compile.keys()]]
-                parallel_compile(ccfiles_to_compile.keys(), ofiles_to_copy= ofiles_to_copy)
+                ofiles_to_copy = get_ofiles_to_copy(executables_to_link.keys())
+                ofiles_to_copy = [x for x in ofiles_to_copy if x not in [y.corresponding_ofile for y in ccfiles_to_compile.keys()]]
+                parallel_compile(ccfiles_to_compile.keys(), list_of_hosts, nice_values,
+                        ofiles_to_copy = ofiles_to_copy)
 
             if force_link or (executables_to_link.keys() and not create_dll):
                 print '++++ Linking', string.join(map(lambda x: x.filebase, executables_to_link.keys()))



From lamblin at mail.berlios.de  Wed Jul  4 04:55:39 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 4 Jul 2007 04:55:39 +0200
Subject: [Plearn-commits] r7690 - in trunk: plearn_learners/hyper
	plearn_learners_experimental/SurfaceTemplate
Message-ID: <200707040255.l642tdZj019259@sheep.berlios.de>

Author: lamblin
Date: 2007-07-04 04:55:38 +0200 (Wed, 04 Jul 2007)
New Revision: 7690

Modified:
   trunk/plearn_learners/hyper/OptimizeOptionOracle.cc
   trunk/plearn_learners_experimental/SurfaceTemplate/geometry.cc
Log:
Fix compilation warnings


Modified: trunk/plearn_learners/hyper/OptimizeOptionOracle.cc
===================================================================
--- trunk/plearn_learners/hyper/OptimizeOptionOracle.cc	2007-07-04 02:34:04 UTC (rev 7689)
+++ trunk/plearn_learners/hyper/OptimizeOptionOracle.cc	2007-07-04 02:55:38 UTC (rev 7690)
@@ -241,7 +241,7 @@
         best = last;
     }
 
-    real next; // The next value we are going to try.
+    real next = 0; // The next value we are going to try.
     if (current_direction == "up") {
         if (best * factor < upper_bound*0.99) {
             // We can try much higher (the 0.99 is there to ensure we do not indefinitely

Modified: trunk/plearn_learners_experimental/SurfaceTemplate/geometry.cc
===================================================================
--- trunk/plearn_learners_experimental/SurfaceTemplate/geometry.cc	2007-07-04 02:34:04 UTC (rev 7689)
+++ trunk/plearn_learners_experimental/SurfaceTemplate/geometry.cc	2007-07-04 02:55:38 UTC (rev 7690)
@@ -200,7 +200,7 @@
     if( theta !=0 )
     {
         Vec axis = eigen_vecs.subMat( 3, 0, 1, 3 ).toVecCopy();
-        axis /= sin( theta/2.0 );
+        axis /= real(sin( theta/2.0 ));
         rot << rotationFromAxisAngle( axis, theta );
     }
     else



From lamblin at mail.berlios.de  Wed Jul  4 05:31:18 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 4 Jul 2007 05:31:18 +0200
Subject: [Plearn-commits] r7691 - trunk/plearn_learners/online
Message-ID: <200707040331.l643VIcj020731@sheep.berlios.de>

Author: lamblin
Date: 2007-07-04 05:31:17 +0200 (Wed, 04 Jul 2007)
New Revision: 7691

Modified:
   trunk/plearn_learners/online/RBMModule.cc
   trunk/plearn_learners/online/RBMModule.h
Log:
Cosmetic changes


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-07-04 02:55:38 UTC (rev 7690)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-07-04 03:31:17 UTC (rev 7691)
@@ -130,7 +130,7 @@
                   OptionBase::buildoption,
         "Connection between the visible and hidden layers.");
 
-    declareOption(ol, "reconstruction_connection", 
+    declareOption(ol, "reconstruction_connection",
                   &RBMModule::reconstruction_connection,
                   OptionBase::buildoption,
         "Reconstuction connection between the hidden and visible layers.");
@@ -176,7 +176,7 @@
         "- by the usual formula if 'standard_cd_weights_grad' is true\n"
         "- by the true gradient if 'standard_cd_weights_grad' is false.");
 
-    declareOption(ol, "n_Gibbs_steps_CD", 
+    declareOption(ol, "n_Gibbs_steps_CD",
                   &RBMModule::n_Gibbs_steps_CD,
                   OptionBase::buildoption,
                   "Number of Gibbs sampling steps in negative phase of "
@@ -189,7 +189,7 @@
                   "and thus a Gibbs chain has to be run. This option gives the minimum number\n"
                   "of Gibbs steps to perform in the chain before outputting a sample.\n");
 
-    declareOption(ol, "n_Gibbs_steps_per_generated_sample", 
+    declareOption(ol, "n_Gibbs_steps_per_generated_sample",
                   &RBMModule::n_Gibbs_steps_per_generated_sample,
                   OptionBase::buildoption,
                   "Used in generative mode (when visible_sample or hidden_sample is requested)\n"
@@ -204,7 +204,7 @@
                   "Whether to compute the exact RBM generative model's log-likelihood\n"
                   "(on the neg_log_likelihood port). If false then the neg_log_likelihood\n"
                   "port just computes the input visible's free energy.\n");
-    
+
     declareOption(ol, "minimize_log_likelihood",
                   &RBMModule::minimize_log_likelihood,
                   OptionBase::buildoption,
@@ -212,7 +212,7 @@
                   "i.e. take stochastic gradient steps w.r.t. the log-likelihood instead\n"
                   "of w.r.t. the contrastive divergence.\n");
 
-    declareOption(ol, "Gibbs_step", 
+    declareOption(ol, "Gibbs_step",
                   &RBMModule::Gibbs_step,
                   OptionBase::learntoption,
                   "Used in generative mode (when visible_sample or hidden_sample is requested)\n"
@@ -220,14 +220,14 @@
                   "Keeps track of the number of steps that have been run since the beginning\n"
                   "of the chain.\n");
 
-    declareOption(ol, "log_partition_function", 
+    declareOption(ol, "log_partition_function",
                   &RBMModule::log_partition_function,
                   OptionBase::learntoption,
                   "log(Z) = log(sum_{h,x} exp(-energy(h,x))\n"
                   "only computed if compute_log_likelihood is true and\n"
                   "the neg_log_likelihood port is requested.\n");
 
-    declareOption(ol, "partition_function_is_stale", 
+    declareOption(ol, "partition_function_is_stale",
                   &RBMModule::partition_function_is_stale,
                   OptionBase::learntoption,
                   "Whether parameters have changed since the last computation\n"
@@ -283,8 +283,8 @@
     addPortName("visible_expectation");
     addPortName("hidden_sample");
     addPortName("energy");
-    addPortName("hidden_bias"); 
-    addPortName("weights"); 
+    addPortName("hidden_bias");
+    addPortName("weights");
     addPortName("neg_log_likelihood");
     if(reconstruction_connection)
     {
@@ -309,8 +309,8 @@
     }
     if (hidden_layer) {
         port_sizes(getPortIndex("hidden.state"), 1) = hidden_layer->size;
-        port_sizes(getPortIndex("hidden_activations.state"), 1) = hidden_layer->size; 
-        port_sizes(getPortIndex("hidden_sample"), 1) = hidden_layer->size; 
+        port_sizes(getPortIndex("hidden_activations.state"), 1) = hidden_layer->size;
+        port_sizes(getPortIndex("hidden_sample"), 1) = hidden_layer->size;
         port_sizes(getPortIndex("hidden_bias"),1) = hidden_layer->size;
         if(visible_layer)
             port_sizes(getPortIndex("weights"),1) = hidden_layer->size * visible_layer->size;
@@ -320,20 +320,20 @@
     if(reconstruction_connection)
     {
         if (visible_layer) {
-            port_sizes(getPortIndex("visible_reconstruction.state"),1) = 
-                visible_layer->size; 
-            port_sizes(getPortIndex("visible_reconstruction_activations.state"),1) = 
-                       visible_layer->size; 
+            port_sizes(getPortIndex("visible_reconstruction.state"),1) =
+                visible_layer->size;
+            port_sizes(getPortIndex("visible_reconstruction_activations.state"),1) =
+                       visible_layer->size;
         }
-        port_sizes(getPortIndex("reconstruction_error.state"),1) = 1; 
+        port_sizes(getPortIndex("reconstruction_error.state"),1) = 1;
     }
     if (compute_contrastive_divergence)
     {
-        port_sizes(getPortIndex("contrastive_divergence"),1) = 1; 
-        if (visible_layer) 
-            port_sizes(getPortIndex("negative_phase_visible_samples.state"),1) = visible_layer->size; 
+        port_sizes(getPortIndex("contrastive_divergence"),1) = 1;
+        if (visible_layer)
+            port_sizes(getPortIndex("negative_phase_visible_samples.state"),1) = visible_layer->size;
         if (hidden_layer)
-            port_sizes(getPortIndex("negative_phase_hidden_expectations.state"),1) = hidden_layer->size; 
+            port_sizes(getPortIndex("negative_phase_hidden_expectations.state"),1) = hidden_layer->size;
         if (fast_exact_is_equal(cd_learning_rate, 0))
             PLWARNING("In RBMModule::build_ - Contrastive divergence is "
                     "computed but 'cd_learning_rate' is set to 0: no internal "
@@ -606,7 +606,7 @@
     PLASSERT( hidden_layer );
     PLASSERT( connection );
 
-    Mat* visible = ports_value[getPortIndex("visible")]; 
+    Mat* visible = ports_value[getPortIndex("visible")];
     Mat* hidden = ports_value[getPortIndex("hidden.state")];
     hidden_act = ports_value[getPortIndex("hidden_activations.state")];
     Mat* visible_sample = ports_value[getPortIndex("visible_sample")];
@@ -621,11 +621,11 @@
     Mat* reconstruction_error = 0;
     if(reconstruction_connection)
     {
-        visible_reconstruction = 
-            ports_value[getPortIndex("visible_reconstruction.state")]; 
-        visible_reconstruction_activations = 
+        visible_reconstruction =
+            ports_value[getPortIndex("visible_reconstruction.state")];
+        visible_reconstruction_activations =
             ports_value[getPortIndex("visible_reconstruction_activations.state")];
-        reconstruction_error = 
+        reconstruction_error =
             ports_value[getPortIndex("reconstruction_error.state")];
     }
     Mat* contrastive_divergence = 0;
@@ -634,15 +634,15 @@
     Mat* negative_phase_hidden_activations = NULL;
     if (compute_contrastive_divergence)
     {
-        contrastive_divergence = ports_value[getPortIndex("contrastive_divergence")]; 
+        contrastive_divergence = ports_value[getPortIndex("contrastive_divergence")];
         if (!contrastive_divergence || !contrastive_divergence->isEmpty())
             PLERROR("In RBMModule::fprop - When option "
                     "'compute_contrastive_divergence' is 'true', the "
                     "'contrastive_divergence' port should be provided, as an "
                     "output.");
-        negative_phase_visible_samples = 
+        negative_phase_visible_samples =
             ports_value[getPortIndex("negative_phase_visible_samples.state")];
-        negative_phase_hidden_expectations = 
+        negative_phase_hidden_expectations =
             ports_value[getPortIndex("negative_phase_hidden_expectations.state")];
         negative_phase_hidden_activations =
             ports_value[getPortIndex("negative_phase_hidden_activations.state")];
@@ -656,14 +656,14 @@
     {
         // When an input is provided, that would restart the chain for
         // unconditional sampling, from that example.
-        Gibbs_step = 0; 
+        Gibbs_step = 0;
         visible_layer->setExpectations(*visible);
     }
 
     // COMPUTE ENERGY
-    if (energy) 
+    if (energy)
     {
-        PLASSERT_MSG( energy->isEmpty(), 
+        PLASSERT_MSG( energy->isEmpty(),
                       "RBMModule: the energy port can only be an output port\n" );
         if (visible && !visible->isEmpty()
             && hidden && !hidden->isEmpty())
@@ -685,6 +685,7 @@
         }
         found_a_valid_configuration = true;
     }
+    // COMPUTE NLL
     if (neg_log_likelihood && neg_log_likelihood->isEmpty() && compute_log_likelihood)
     {
         if (partition_function_is_stale && !during_training)
@@ -758,7 +759,7 @@
             computeEnergy(*visible,*hidden,*neg_log_likelihood);
             *neg_log_likelihood += log_partition_function;
         }
-        else if (visible && !visible->isEmpty()) 
+        else if (visible && !visible->isEmpty())
         {
             // neg-log-likelihood(visible) = free_energy(visible) + log(partition_function)
             computeFreeEnergyOfVisible(*visible,*neg_log_likelihood);
@@ -772,7 +773,7 @@
         }
         else PLERROR("RBMModule: neg_log_likelihood currently computable only of the visible as inputs");
     }
-    
+
     // REGULAR FPROP
     // we are given the visible units and we want to compute the hidden
     // activation and/or the hidden expectation
@@ -794,8 +795,8 @@
         // Since we return below, the other ports must be unused.
         //PLASSERT( !visible_sample && !hidden_sample );
         found_a_valid_configuration = true;
-    } 
-    
+    }
+
     // SAMPLING
     if ((visible_sample && visible_sample->isEmpty())               // is asked to sample visible units (discrete)
         || (visible_expectation && visible_expectation->isEmpty())  //              "                   (continous)
@@ -820,7 +821,7 @@
             Gibbs_step=0;
             //cout << "sampling hidden from visible expectation" << endl;
         }
-        else if (visible_expectation && !visible_expectation->isEmpty()) 
+        else if (visible_expectation && !visible_expectation->isEmpty())
         {
              PLERROR("In RBMModule::fprop visible_expectation can only be an output port (use visible as input port");
         }
@@ -866,11 +867,11 @@
         }
         found_a_valid_configuration = true;
     }// END SAMPLING
-    
+
     // COMPUTE CONTRASTIVE DIVERGENCE CRITERION
     if (contrastive_divergence)
     {
-        PLASSERT_MSG( contrastive_divergence->isEmpty(), 
+        PLASSERT_MSG( contrastive_divergence->isEmpty(),
                       "RBMModule: the contrastive_divergence port can only be an output port\n" );
         if (visible && !visible->isEmpty())
         {
@@ -882,15 +883,16 @@
             {
                 PLASSERT(!hidden_act);
                 computePositivePhaseHiddenActivations(*visible);
-                
+
                 // we need to save the hidden activations somewhere
                 hidden_act_store.resize(mbs,hidden_layer->size);
                 hidden_act_store << hidden_layer->activations;
                 h_act = &hidden_act_store;
-            } else 
+            }
+            else
             {
                 // hidden_act must have been computed above if they were requested on port
-                PLASSERT(hidden_act && !hidden_act->isEmpty()); 
+                PLASSERT(hidden_act && !hidden_act->isEmpty());
                 h_act = hidden_act;
             }
             if (!hidden_expectations_are_computed) // it must be because hidden outputs were not asked
@@ -902,7 +904,8 @@
                 hidden_exp_store.resize(mbs,hidden_layer->size);
                 hidden_exp_store << hidden_expectations;
                 h = &hidden_exp_store;
-            } else
+            }
+            else
             {
                 // hidden exp. must have been computed above if they were requested on port
                 PLASSERT(hidden && !hidden->isEmpty());
@@ -937,19 +940,19 @@
             PLASSERT(hidden_layer->classname()=="RBMBinomialLayer");
 
             // note that h_act and h may point to hidden_act_store and hidden_exp_store
-            PLASSERT(h_act && !h_act->isEmpty()); 
+            PLASSERT(h_act && !h_act->isEmpty());
             PLASSERT(h && !h->isEmpty());
 
             contrastive_divergence->resize(hidden_expectations.length(),1);
             // compute contrastive divergence itself
             for (int i=0;i<mbs;i++)
             {
-                (*contrastive_divergence)(i,0) = 
+                (*contrastive_divergence)(i,0) =
                     // positive phase energy
                     visible_layer->energy((*visible)(i))
                     - dot((*h)(i),(*h_act)(i))
                     // minus
-                    - 
+                    -
                     // negative phase energy
                     (visible_layer->energy(visible_layer->samples(i))
                      - dot(hidden_expectations(i),hidden_layer->activations(i)));
@@ -960,20 +963,18 @@
                     "only possible if only visible are provided in input).\n");
         found_a_valid_configuration = true;
     }
-    
 
-    
-    
+
     // COMPUTE AUTOASSOCIATOR RECONSTRUCTION ERROR
-    if ( visible && !visible->isEmpty() && 
-         ( ( visible_reconstruction && visible_reconstruction->isEmpty() ) || 
-           ( visible_reconstruction_activations && 
+    if ( visible && !visible->isEmpty() &&
+         ( ( visible_reconstruction && visible_reconstruction->isEmpty() ) ||
+           ( visible_reconstruction_activations &&
              visible_reconstruction_activations->isEmpty() ) ||
-           ( reconstruction_error && reconstruction_error->isEmpty() ) ) ) 
-    {        
+           ( reconstruction_error && reconstruction_error->isEmpty() ) ) )
+    {
         // Autoassociator reconstruction cost
         PLASSERT( ports_value.length() == nPorts() );
-        computePositivePhaseHiddenActivations(*visible); 
+        computePositivePhaseHiddenActivations(*visible);
         if(!hidden_expectations_are_computed)
         {
             hidden_layer->computeExpectations();
@@ -981,24 +982,24 @@
         }
 
         // Don't need to verify if they are asked in a port, this was done previously
-        
+
         computeVisibleActivations(hidden_layer->getExpectations(),true);
-        if(visible_reconstruction_activations) 
+        if(visible_reconstruction_activations)
         {
             PLASSERT( visible_reconstruction_activations->isEmpty() );
             const Mat& to_store = visible_layer->activations;
-            visible_reconstruction_activations->resize(to_store.length(), 
+            visible_reconstruction_activations->resize(to_store.length(),
                                                        to_store.width());
             *visible_reconstruction_activations << to_store;
         }
         if (visible_reconstruction || reconstruction_error)
-        {        
+        {
             visible_layer->computeExpectations();
             if(visible_reconstruction)
             {
                 PLASSERT( visible_reconstruction->isEmpty() );
                 const Mat& to_store = visible_layer->getExpectations();
-                visible_reconstruction->resize(to_store.length(), 
+                visible_reconstruction->resize(to_store.length(),
                                                            to_store.width());
                 *visible_reconstruction << to_store;
             }
@@ -1013,25 +1014,23 @@
         found_a_valid_configuration = true;
     }
     // COMPUTE VISIBLE GIVEN HIDDEN
-    else if ( visible_reconstruction && visible_reconstruction->isEmpty() 
+    else if ( visible_reconstruction && visible_reconstruction->isEmpty()
          && hidden && !hidden->isEmpty())
-           
-    {        
+    {
         // Don't need to verify if they are asked in a port, this was done previously
-        
-	computeVisibleActivations(*hidden,true);
+        computeVisibleActivations(*hidden,true);
         if(visible_reconstruction_activations)
         {
             PLASSERT( visible_reconstruction_activations->isEmpty() );
             const Mat& to_store = visible_layer->activations;
-            visible_reconstruction_activations->resize(to_store.length(), 
+            visible_reconstruction_activations->resize(to_store.length(),
                                                        to_store.width());
             *visible_reconstruction_activations << to_store;
-        }      
+        }
         visible_layer->computeExpectations();
         PLASSERT( visible_reconstruction->isEmpty() );
         const Mat& to_store = visible_layer->getExpectations();
-        visible_reconstruction->resize(to_store.length(), 
+        visible_reconstruction->resize(to_store.length(),
                                        to_store.width());
         *visible_reconstruction << to_store;
         found_a_valid_configuration = true;
@@ -1082,8 +1081,8 @@
     hidden_act = ports_value[getPortIndex("hidden_activations.state")];
     Mat* reconstruction_error_grad = 0;
     Mat* hidden_bias_grad = ports_gradient[getPortIndex("hidden_bias")];
-    weights = ports_value[getPortIndex("weights")]; 
-    Mat* weights_grad = ports_gradient[getPortIndex("weights")];    
+    weights = ports_value[getPortIndex("weights")];
+    Mat* weights_grad = ports_gradient[getPortIndex("weights")];
     hidden_bias = ports_value[getPortIndex("hidden_bias")];
     Mat* contrastive_divergence_grad = NULL;
 
@@ -1099,7 +1098,7 @@
     }
 
     if(reconstruction_connection)
-        reconstruction_error_grad = 
+        reconstruction_error_grad =
             ports_gradient[getPortIndex("reconstruction_error.state")];
 
     // Ensure the visible gradient is not provided as input. This is because we
@@ -1109,7 +1108,7 @@
 
     bool compute_visible_grad = visible_grad && visible_grad->isEmpty();
     bool compute_weights_grad = weights_grad && weights_grad->isEmpty();
-    
+
     int mbs = (visible && !visible->isEmpty()) ? visible->length() : -1;
 
     if (hidden_grad && !hidden_grad->isEmpty())
@@ -1147,7 +1146,7 @@
                 store_visible_grad = &visible_exp_grad;
             }
             store_visible_grad->resize(mbs,visible_layer->size);
-            
+
             if (weights)
             {
                 int up = connection->up_size;
@@ -1261,7 +1260,7 @@
         // Perform a step of contrastive divergence.
         PLASSERT( visible && !visible->isEmpty() );
         setAllLearningRates(cd_learning_rate);
-        Mat* negative_phase_visible_samples = 
+        Mat* negative_phase_visible_samples =
             compute_contrastive_divergence?ports_value[getPortIndex("negative_phase_visible_samples.state")]:0;
         const Mat* negative_phase_hidden_expectations =
             compute_contrastive_divergence ?
@@ -1271,7 +1270,7 @@
             compute_contrastive_divergence ?
                 ports_value[getPortIndex("negative_phase_hidden_activations.state")]
                 : NULL;
-        
+
         PLASSERT( visible && hidden );
         PLASSERT( !negative_phase_visible_samples ||
                   !negative_phase_visible_samples->isEmpty() );
@@ -1487,7 +1486,7 @@
         reconstruction_connection->bpropUpdate(
             *hidden, *visible_reconstruction_activations,
             hidden_exp_grad, visible_act_grad, false);
-        
+
         // Hidden layer bias update
         hidden_layer->bpropUpdate(*hidden_act,
                                   *hidden, hidden_act_grad,
@@ -1515,7 +1514,7 @@
         }
         else
         {
-            visible_exp_grad.resize(mbs,visible_layer->size);        
+            visible_exp_grad.resize(mbs,visible_layer->size);
             connection->bpropUpdate(
                 *visible, *hidden_act,
                 visible_exp_grad, hidden_act_grad, true);

Modified: trunk/plearn_learners/online/RBMModule.h
===================================================================
--- trunk/plearn_learners/online/RBMModule.h	2007-07-04 02:55:38 UTC (rev 7690)
+++ trunk/plearn_learners/online/RBMModule.h	2007-07-04 03:31:17 UTC (rev 7691)
@@ -74,12 +74,12 @@
 
     bool compute_contrastive_divergence;
 
-    //! Number of Gibbs sampling steps in negative phase 
+    //! Number of Gibbs sampling steps in negative phase
     //! of contrastive divergence.
     int n_Gibbs_steps_CD;
 
     //! used to generate samples from the RBM
-    int min_n_Gibbs_steps; 
+    int min_n_Gibbs_steps;
     int n_Gibbs_steps_per_generated_sample;
 
     bool compute_log_likelihood;
@@ -94,8 +94,8 @@
     bool standard_cd_grad;
     bool standard_cd_bias_grad;
     bool standard_cd_weights_grad;
-    
 
+
 public:
     //#####  Public Member Functions  #########################################
 
@@ -242,7 +242,7 @@
     Mat hidden_exp_store;
     Mat hidden_act_store;
     Mat* hidden_act;
-    bool hidden_activations_are_computed;    
+    bool hidden_activations_are_computed;
 
     //! Used to store the contrastive divergence gradient w.r.t. weights.
     Mat store_weights_grad;
@@ -312,7 +312,7 @@
     //! it in the 'energy' matrix.
     //! The 'positive_phase' boolean is used to save computations when we know
     //! we are in the positive phase of fprop.
-    void computeEnergy(const Mat& visible, const Mat& hidden, Mat& energy, 
+    void computeEnergy(const Mat& visible, const Mat& hidden, Mat& energy,
                        bool positive_phase = true);
 
 private:



From lamblin at mail.berlios.de  Wed Jul  4 10:34:50 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 4 Jul 2007 10:34:50 +0200
Subject: [Plearn-commits] r7692 - trunk/plearn_learners/online
Message-ID: <200707040834.l648Yo5P018865@sheep.berlios.de>

Author: lamblin
Date: 2007-07-04 10:34:49 +0200 (Wed, 04 Jul 2007)
New Revision: 7692

Modified:
   trunk/plearn_learners/online/MaxSubsampling2DModule.cc
   trunk/plearn_learners/online/RBMConnection.cc
   trunk/plearn_learners/online/RBMConnection.h
   trunk/plearn_learners/online/RBMConv2DConnection.cc
   trunk/plearn_learners/online/RBMConv2DConnection.h
   trunk/plearn_learners/online/RBMMatrixConnection.cc
   trunk/plearn_learners/online/RBMMatrixConnection.h
   trunk/plearn_learners/online/RBMMixedConnection.cc
   trunk/plearn_learners/online/RBMMixedConnection.h
   trunk/plearn_learners/online/RBMModule.cc
Log:
  - RBMModule does not use RBMMatrixTransposeConnection as
reconstruction connection, but any other RBMConnection
  - Implementation of bpropAccUpdate in RBMConnection and sub-classes


Modified: trunk/plearn_learners/online/MaxSubsampling2DModule.cc
===================================================================
--- trunk/plearn_learners/online/MaxSubsampling2DModule.cc	2007-07-04 03:31:17 UTC (rev 7691)
+++ trunk/plearn_learners/online/MaxSubsampling2DModule.cc	2007-07-04 08:34:49 UTC (rev 7692)
@@ -230,16 +230,14 @@
         PLCHECK_MSG( false, "Unknown port configuration" );
 }
 
-/////////////////
-// bpropUpdate //
-/////////////////
-
-
+////////////////////
+// bpropAccUpdate //
+////////////////////
 void MaxSubsampling2DModule::bpropAccUpdate(const TVec<Mat*>& ports_value,
                                             const TVec<Mat*>& ports_gradient)
 {
     PLASSERT( ports_value.length() == nPorts()
-              && ports_gradient.length() == nPorts());
+              && ports_gradient.length() == nPorts() );
     // check which ports are input
     // (ports_value[i] && !ports_value[i]->isEmpty())
     // which ports are output (ports_value[i] && ports_value[i]->isEmpty())

Modified: trunk/plearn_learners/online/RBMConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMConnection.cc	2007-07-04 03:31:17 UTC (rev 7691)
+++ trunk/plearn_learners/online/RBMConnection.cc	2007-07-04 08:34:49 UTC (rev 7692)
@@ -136,6 +136,17 @@
         up_size = output_size;
     else
         output_size = up_size;
+
+    ports.resize(2);
+    ports[0] = "down";
+    ports[1] = "up";
+    // NOT weights here, because it only makes sense with an
+    // RBMMatrixConnection
+
+    port_sizes.resize(nPorts(), 2);
+    port_sizes.column(0).fill(-1);
+    port_sizes(0, 1) = down_size;
+    port_sizes(1, 1) = up_size;
 }
 
 ///////////
@@ -286,7 +297,7 @@
     PLERROR("In RBMConnection::getAllWeights(): not implemented");
 }
 
-void RBMConnection::setAllWeights(const Mat& rbm_weights) 
+void RBMConnection::setAllWeights(const Mat& rbm_weights)
 {
     PLERROR("In RBMConnection::setAllWeights(): not implemented");
 }
@@ -294,7 +305,7 @@
 void RBMConnection::petiteCulotteOlivierUpdate(
     const Vec& input, const Mat& rbm_weights,
     const Vec& output,
-    Vec& input_gradient, 
+    Vec& input_gradient,
     Mat& rbm_weights_gradient,
     const Vec& output_gradient,
     bool accumulate)
@@ -302,7 +313,7 @@
     PLERROR("In RBMConnection::bpropUpdate(): not implemented");
 }
 
- 
+
 /////////////
 // bpropCD //
 /////////////
@@ -312,17 +323,32 @@
     PLERROR("In RBMConnection::petiteCulotteOlivierCD(): not implemented");
 }
 
-void RBMConnection::petiteCulotteOlivierCD( const Vec& pos_down_values, 
-                                            const Vec& pos_up_values,   
-                                            const Vec& neg_down_values, 
-                                            const Vec& neg_up_values, 
+void RBMConnection::petiteCulotteOlivierCD( const Vec& pos_down_values,
+                                            const Vec& pos_up_values,
+                                            const Vec& neg_down_values,
+                                            const Vec& neg_up_values,
                                             Mat& weights_gradient,
                                             bool accumulate)
 {
     PLERROR("In RBMConnection::petiteCulotteOlivierCD(): not implemented");
 }
 
+//////////////
+// getPorts //
+//////////////
+const TVec<string>& RBMConnection::getPorts()
+{
+    return ports;
+}
 
+//////////////////
+// getPortSizes //
+//////////////////
+const TMat<int>& RBMConnection::getPortSizes()
+{
+    return port_sizes;
+}
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/online/RBMConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMConnection.h	2007-07-04 03:31:17 UTC (rev 7691)
+++ trunk/plearn_learners/online/RBMConnection.h	2007-07-04 08:34:49 UTC (rev 7692)
@@ -202,12 +202,12 @@
         Vec& input_gradient, Mat& rbm_weights_gradient,
         const Vec& output_gradient,
         bool accumulate = false);
-    
+
     //! Computes the contrastive divergence gradient with respect to the weights
     //! It should be noted that bpropCD does not call clearstats().
-    virtual void petiteCulotteOlivierCD(Mat& weights_gradient, 
+    virtual void petiteCulotteOlivierCD(Mat& weights_gradient,
                                         bool accumulate = false);
-    
+
     //! Computes the contrastive divergence gradient with respect to the weights
     //! given the positive and negative phase values.
     virtual void petiteCulotteOlivierCD(const Vec& pos_down_values,
@@ -216,7 +216,13 @@
                                         const Vec& neg_up_values,
                                         Mat& weights_gradient,
                                         bool accumulate = false);
-    
+
+    //! Return the list of ports in the module.
+    virtual const TVec<string>& getPorts();
+
+    //! Return the size of all ports
+    virtual const TMat<int>& getPortSizes();
+
     //! return the number of parameters
     virtual int nParameters() const = 0;
 
@@ -256,6 +262,9 @@
     //! Number of examples accumulated in *_neg_stats
     int neg_count;
 
+    //! Port names
+    TVec<string> ports;
+
 protected:
     //#####  Protected Member Functions  ######################################
 

Modified: trunk/plearn_learners/online/RBMConv2DConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMConv2DConnection.cc	2007-07-04 03:31:17 UTC (rev 7691)
+++ trunk/plearn_learners/online/RBMConv2DConnection.cc	2007-07-04 08:34:49 UTC (rev 7692)
@@ -632,6 +632,113 @@
     // kernel -= learning_rate/n * kernel_gradient
     multiplyAcc( kernel, kernel_gradient, -learning_rate/batch_size );
 }
+
+////////////////////
+// bpropAccUpdate //
+////////////////////
+void RBMConv2DConnection::bpropAccUpdate(const TVec<Mat*>& ports_value,
+                                         const TVec<Mat*>& ports_gradient)
+{
+    PLASSERT( ports_value.length() == nPorts()
+              && ports_gradient.length() == nPorts() );
+
+    Mat* down = ports_value[0];
+    Mat* up = ports_value[1];
+    Mat* down_grad = ports_gradient[0];
+    Mat* up_grad = ports_gradient[1];
+
+    PLASSERT( down && !down->isEmpty() );
+    PLASSERT( up && !up->isEmpty() );
+
+    int batch_size = down->length();
+    PLASSERT( up->length() == batch_size );
+
+    // If we have up_grad
+    if( up_grad && !up_grad->isEmpty() )
+    {
+        // down_grad should not be provided
+        PLASSERT( !down_grad || down_grad->isEmpty() );
+        PLASSERT( up_grad->length() == batch_size );
+        PLASSERT( up_grad->width() == up_size );
+
+        // If we want down_grad
+        bool compute_down_grad = false;
+        if( down_grad && down_grad->isEmpty() )
+        {
+            PLASSERT( down_grad->width() == down_size );
+            down_grad->resize(batch_size, down_size);
+            compute_down_grad = true;
+        }
+
+        kernel_gradient.clear();
+        for (int k=0; k<batch_size; k++)
+        {
+            down_image = (*down)(k).toMat(down_image_length, down_image_width);
+            up_image = (*up)(k).toMat(up_image_length, up_image_width);
+            up_image_gradient = (*up_grad)(k)
+                .toMat(up_image_length, up_image_width);
+
+            if( compute_down_grad )
+            {
+                down_image_gradient = (*down_grad)(k)
+                    .toMat(down_image_length, down_image_width);
+                convolve2Dbackprop(down_image, kernel,
+                                   up_image_gradient, down_image_gradient,
+                                   kernel_gradient,
+                                   kernel_step1, kernel_step2, true);
+            }
+            else
+                convolve2Dbackprop(down_image, up_image_gradient,
+                                   kernel_gradient,
+                                   kernel_step1, kernel_step2, true);
+        }
+        // kernel -= learning_rate/n * kernel_gradient
+        multiplyAcc(kernel, kernel_gradient, -learning_rate/batch_size);
+    }
+    else if( down_grad && !down_grad->isEmpty() )
+    {
+        PLASSERT( down_grad->length() == batch_size );
+        PLASSERT( down_grad->width() == down_size );
+
+        // If we want up_grad
+        bool compute_up_grad = false;
+        if( up_grad && up_grad->isEmpty() )
+        {
+            PLASSERT( up_grad->width() == up_size );
+            up_grad->resize(batch_size, up_size);
+            compute_up_grad = true;
+        }
+
+        kernel_gradient.clear();
+        for (int k=0; k<batch_size; k++)
+        {
+            down_image = (*down)(k).toMat(down_image_length, down_image_width);
+            up_image = (*up)(k).toMat(up_image_length, up_image_width);
+            down_image_gradient = (*down_grad)(k)
+                .toMat(down_image_length, down_image_width);
+
+            if( compute_up_grad )
+            {
+                up_image_gradient = (*up_grad)(k)
+                    .toMat(up_image_length, up_image_width);
+                backConvolve2Dbackprop(kernel, up_image, up_image_gradient,
+                                       down_image_gradient, kernel_gradient,
+                                       kernel_step1, kernel_step2, true);
+            }
+            else
+                backConvolve2Dbackprop(up_image, down_image_gradient,
+                                       kernel_gradient,
+                                       kernel_step1, kernel_step2, true);
+        }
+        // kernel -= learning_rate/n * kernel_gradient
+        multiplyAcc(kernel, kernel_gradient, -learning_rate/batch_size);
+    }
+    else
+        PLCHECK_MSG( false,
+                     "Unknown port configuration" );
+}
+
+
 //! reset the parameters to the state they would be BEFORE starting training.
 //! Note that this method is necessarily called from build().
 void RBMConv2DConnection::forget()

Modified: trunk/plearn_learners/online/RBMConv2DConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMConv2DConnection.h	2007-07-04 03:31:17 UTC (rev 7691)
+++ trunk/plearn_learners/online/RBMConv2DConnection.h	2007-07-04 08:34:49 UTC (rev 7692)
@@ -180,6 +180,9 @@
                              const Mat& output_gradients,
                              bool accumulate=false);
 
+    virtual void bpropAccUpdate(const TVec<Mat*>& ports_value,
+                                const TVec<Mat*>& ports_gradient);
+
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from
     //! build().

Modified: trunk/plearn_learners/online/RBMMatrixConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.cc	2007-07-04 03:31:17 UTC (rev 7691)
+++ trunk/plearn_learners/online/RBMMatrixConnection.cc	2007-07-04 08:34:49 UTC (rev 7692)
@@ -497,7 +497,7 @@
 ///////////
 // fprop //
 ///////////
-void RBMMatrixConnection::fprop(const Vec& input, const Mat& rbm_weights, 
+void RBMMatrixConnection::fprop(const Vec& input, const Mat& rbm_weights,
                           Vec& output) const
 {
     product( output, rbm_weights, input );
@@ -584,7 +584,7 @@
 void RBMMatrixConnection::petiteCulotteOlivierUpdate(
     const Vec& input, const Mat& rbm_weights,
     const Vec& output,
-    Vec& input_gradient, 
+    Vec& input_gradient,
     Mat& rbm_weights_gradient,
     const Vec& output_gradient,
     bool accumulate)
@@ -602,7 +602,7 @@
         transposeProductAcc( input_gradient, rbm_weights, output_gradient );
 
         // rbm_weights_gradient += output_gradient' * input
-        externalProductAcc( rbm_weights_gradient, output_gradient, 
+        externalProductAcc( rbm_weights_gradient, output_gradient,
                             input);
 
     }
@@ -614,13 +614,81 @@
         transposeProduct( input_gradient, rbm_weights, output_gradient );
 
         // rbm_weights_gradient = output_gradient' * input
-        externalProduct( rbm_weights_gradient, output_gradient, 
+        externalProduct( rbm_weights_gradient, output_gradient,
                          input);
     }
 
 }
 
- 
+
+////////////////////
+// bpropAccUpdate //
+////////////////////
+void RBMMatrixConnection::bpropAccUpdate(const TVec<Mat*>& ports_value,
+                                         const TVec<Mat*>& ports_gradient)
+{
+    //TODO: add weights as port?
+    PLASSERT( ports_value.length() == nPorts()
+              && ports_gradient.length() == nPorts() );
+
+    Mat* down = ports_value[0];
+    Mat* up = ports_value[1];
+    Mat* down_grad = ports_gradient[0];
+    Mat* up_grad = ports_gradient[1];
+
+    PLASSERT( down && !down->isEmpty() );
+    PLASSERT( up && !up->isEmpty() );
+
+    int batch_size = down->length();
+    PLASSERT( up->length() == batch_size );
+
+    // If we have up_grad
+    if( up_grad && !up_grad->isEmpty() )
+    {
+        // down_grad should not be provided
+        PLASSERT( !down_grad || down_grad->isEmpty() );
+        PLASSERT( up_grad->length() == batch_size );
+        PLASSERT( up_grad->width() == up_size );
+
+        // If we want down_grad
+        if( down_grad && down_grad->isEmpty() )
+        {
+            PLASSERT( down_grad->width() == down_size );
+            down_grad->resize(batch_size, down_size);
+
+            // down_grad = up_grad * weights
+            product(*down_grad, *up_grad, weights);
+        }
+
+        // weights -= learning_rate/n * up_grad' * down
+        transposeProductScaleAcc(weights, *up_grad, *down,
+                                 -learning_rate/batch_size, real(1));
+    }
+    else if( down_grad && !down_grad->isEmpty() )
+    {
+        PLASSERT( down_grad->length() == batch_size );
+        PLASSERT( down_grad->width() == down_size );
+
+        // If we wand up_grad
+        if( up_grad && up_grad->isEmpty() )
+        {
+            PLASSERT( up_grad->width() == up_size );
+            up_grad->resize(batch_size, up_size);
+
+            // up_grad = down_grad * weights'
+            productTranspose(*up_grad, *down_grad, weights);
+        }
+
+        // weights = -learning_rate/n * up' * down_grad
+        transposeProductScaleAcc(weights, *up, *down_grad,
+                                 -learning_rate/batch_size, real(1));
+    }
+    else
+        PLCHECK_MSG( false,
+                     "Unknown port configuration" );
+}
+
+
 /////////////
 // bpropCD //
 /////////////
@@ -636,7 +704,7 @@
     int w_mod = weights_gradient.mod();
     int wps_mod = weights_pos_stats.mod();
     int wns_mod = weights_neg_stats.mod();
-    
+
     if(accumulate)
     {
         for( int i=0 ; i<l ; i++, w_i+=w_mod, wps_i+=wps_mod, wns_i+=wns_mod )
@@ -653,13 +721,13 @@
 
 // Instead of using the statistics, we assume we have only one markov chain
 // runned and we update the parameters from the first 4 values of the chain
-void RBMMatrixConnection::petiteCulotteOlivierCD( 
+void RBMMatrixConnection::petiteCulotteOlivierCD(
     const Vec& pos_down_values, // v_0
     const Vec& pos_up_values,   // h_0
     const Vec& neg_down_values, // v_1
     const Vec& neg_up_values, // h_1
     Mat& weights_gradient,
-    bool accumulate) 
+    bool accumulate)
 {
     int l = weights.length();
     int w = weights.width();

Modified: trunk/plearn_learners/online/RBMMatrixConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.h	2007-07-04 03:31:17 UTC (rev 7691)
+++ trunk/plearn_learners/online/RBMMatrixConnection.h	2007-07-04 08:34:49 UTC (rev 7692)
@@ -68,11 +68,11 @@
 
     //#####  Learned Options  #################################################
 
-    //! Matrix containing unit-to-unit weights (output_size ?? input_size)
+    //! Matrix containing unit-to-unit weights (output_size ? input_size)
     Mat weights;
 
     //! used for Gibbs chain methods only
-    real gibbs_ma_coefficient; 
+    real gibbs_ma_coefficient;
 
 
     //#####  Not Options  #####################################################
@@ -177,7 +177,7 @@
     // virtual void bpropUpdate(const Vec& input, const Vec& output,
     //                          const Vec& output_gradient);
 
-    //! given the input and the conneciton weights, 
+    //! given the input and the connection weights,
     //! compute the output (possibly resize it  appropriately)
     virtual void fprop(const Vec& input, const Mat& rbm_weights,
                        Vec& output) const;
@@ -200,6 +200,9 @@
                              const Mat& output_gradients,
                              bool accumulate = false);
 
+    virtual void bpropAccUpdate(const TVec<Mat*>& ports_value,
+                                const TVec<Mat*>& ports_gradient);
+
     //! back-propagates the output gradient to the input and the weights
     //! (the weights are not updated)
     virtual void petiteCulotteOlivierUpdate(
@@ -208,12 +211,12 @@
         Vec& input_gradient, Mat& rbm_weights_gradient,
         const Vec& output_gradient,
         bool accumulate = false);
-    
+
     //! Computes the contrastive divergence gradient with respect to the weights
     //! It should be noted that bpropCD does not call clearstats().
     virtual void petiteCulotteOlivierCD(Mat& weights_gradient,
                                         bool accumulate = false);
-    
+
     //! Computes the contrastive divergence gradient with respect to the weights
     //! given the positive and negative phase values.
     virtual void petiteCulotteOlivierCD(const Vec& pos_down_values,

Modified: trunk/plearn_learners/online/RBMMixedConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMixedConnection.cc	2007-07-04 03:31:17 UTC (rev 7691)
+++ trunk/plearn_learners/online/RBMMixedConnection.cc	2007-07-04 08:34:49 UTC (rev 7692)
@@ -863,6 +863,141 @@
     }
 }
 
+////////////////////
+// bpropAccUpdate //
+////////////////////
+void RBMMixedConnection::bpropAccUpdate(const TVec<Mat*>& ports_value,
+                                        const TVec<Mat*>& ports_gradient)
+{
+    PLASSERT( ports_value.length() == nPorts()
+              && ports_gradient.length() == nPorts() );
+
+    Mat* down = ports_value[0];
+    Mat* up = ports_value[1];
+    Mat* down_grad = ports_gradient[0];
+    Mat* up_grad = ports_gradient[1];
+
+    PLASSERT( down && !down->isEmpty() );
+    PLASSERT( up && !up->isEmpty() );
+
+    int batch_size = down->length();
+    PLASSERT( up->length() == batch_size );
+
+    // If we have up_grad
+    if( up_grad && !up_grad->isEmpty() )
+    {
+        // down_grad should not be provided
+        PLASSERT( !down_grad || down_grad->isEmpty() );
+        PLASSERT( up_grad->length() == batch_size );
+        PLASSERT( up_grad->width() == up_size );
+
+        // If we want down_grad
+        bool compute_down_grad = false;
+        if( down_grad && down_grad->isEmpty() )
+        {
+            PLASSERT( down_grad->width() == down_size );
+            down_grad->resize(batch_size, down_size);
+            compute_down_grad = true;
+        }
+
+        for (int j=0; j<n_down_blocks; j++)
+        {
+            int init_j = down_init_positions[j];
+            int down_size_j = down_block_sizes[j];
+            Mat sub_down = down->subMatColumns(init_j, down_size_j);
+            Mat sub_down_grad;
+            Mat* p_sub_down_grad = NULL;
+            if( compute_down_grad )
+            {
+                sub_down_grad = down_grad->subMatColumns(init_j, down_size_j);
+                p_sub_down_grad = &sub_down_grad;
+            }
+
+            for (int i=0; i<n_up_blocks; i++)
+            {
+                if(sub_connections(i,j))
+                {
+                    int init_i = up_init_positions[i];
+                    int up_size_i = up_block_sizes[i];
+                    Mat sub_up = up->subMatColumns(init_i, up_size_i);
+                    Mat sub_up_grad =
+                        up_grad->subMatColumns(init_i, up_size_i);
+
+                    TVec<Mat*> sub_ports_value(2);
+                    sub_ports_value[0] = &sub_down;
+                    sub_ports_value[1] = &sub_up;
+                    TVec<Mat*> sub_ports_gradient(2);
+                    // NOT &sub_down_grad because we may want a NULL pointer
+                    sub_ports_gradient[0] = p_sub_down_grad;
+                    sub_ports_gradient[1] = &sub_up_grad;
+
+                    if( compute_down_grad )
+                        sub_down_grad.resize(0, down_size_j);
+
+                    sub_connections(i,j)->bpropAccUpdate( sub_ports_value,
+                                                          sub_ports_gradient );
+                }
+            }
+        }
+    }
+    else if( down_grad && !down_grad->isEmpty() )
+    {
+        PLASSERT( down_grad->length() == batch_size );
+        PLASSERT( down_grad->width() == down_size );
+
+        // If we wand up_grad
+        bool compute_up_grad = false;
+        if( up_grad && up_grad->isEmpty() )
+        {
+            PLASSERT( up_grad->width() == up_size );
+            up_grad->resize(batch_size, up_size);
+            compute_up_grad = true;
+        }
+
+        for (int i=0; i<n_up_blocks; i++)
+        {
+            int init_i = up_init_positions[i];
+            int up_size_i = up_block_sizes[i];
+            Mat sub_up = up->subMatColumns(init_i, up_size_i);
+            Mat sub_up_grad;
+            Mat* p_sub_up_grad = NULL;
+            if( compute_up_grad )
+            {
+                sub_up_grad = up_grad->subMatColumns(init_i, up_size_i);
+                p_sub_up_grad = &sub_up_grad;
+            }
+
+            for (int j=0; j<n_down_blocks; j++)
+            {
+                int init_j = down_init_positions[j];
+                int down_size_j = down_block_sizes[j];
+                Mat sub_down = down->subMatColumns(init_j, down_size_j);
+                Mat sub_down_grad =
+                    down_grad->subMatColumns(init_j, down_size_j);
+
+                TVec<Mat*> sub_ports_value(2);
+                sub_ports_value[0] = &sub_down;
+                sub_ports_value[1] = &sub_up;
+                TVec<Mat*> sub_ports_gradient(2);
+                sub_ports_gradient[0] = &sub_down_grad;
+                // NOT &sub_up_grad because we may want a NULL pointer
+                sub_ports_gradient[1] = p_sub_up_grad;
+
+                if( compute_up_grad )
+                    sub_up_grad.resize(0, up_size_i);
+
+                sub_connections(i,j)->bpropAccUpdate( sub_ports_value,
+                                                      sub_ports_gradient );
+            }
+        }
+    }
+    else
+        PLCHECK_MSG( false,
+                     "Unknown port configuration" );
+
+}
+
+
 //! reset the parameters to the state they would be BEFORE starting training.
 //! Note that this method is necessarily called from build().
 void RBMMixedConnection::forget()

Modified: trunk/plearn_learners/online/RBMMixedConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMMixedConnection.h	2007-07-04 03:31:17 UTC (rev 7691)
+++ trunk/plearn_learners/online/RBMMixedConnection.h	2007-07-04 08:34:49 UTC (rev 7692)
@@ -163,6 +163,9 @@
                              const Mat& output_gradients,
                              bool accumulate=false);
 
+    virtual void bpropAccUpdate(const TVec<Mat*>& ports_value,
+                                const TVec<Mat*>& ports_gradient);
+
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from
     //! build().

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-07-04 03:31:17 UTC (rev 7691)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-07-04 08:34:49 UTC (rev 7692)
@@ -133,7 +133,7 @@
     declareOption(ol, "reconstruction_connection",
                   &RBMModule::reconstruction_connection,
                   OptionBase::buildoption,
-        "Reconstuction connection between the hidden and visible layers.");
+        "Reconstruction connection between the hidden and visible layers.");
 
     declareOption(ol, "grad_learning_rate", &RBMModule::grad_learning_rate,
                   OptionBase::buildoption,
@@ -531,13 +531,14 @@
     if (using_reconstruction_connection)
     {
         PLASSERT( reconstruction_connection );
-        reconstruction_connection->setAsDownInputs(hidden);
+        reconstruction_connection->setAsUpInputs(hidden);
         visible_layer->getAllActivations(reconstruction_connection, 0, true);
     }
     else
     {
         if(weights && !weights->isEmpty())
         {
+            PLASSERT( connection->classname() == "RBMMatrixConnection" );
             Mat old_weights;
             Vec old_activation;
             connection->getAllWeights(old_weights);
@@ -983,7 +984,7 @@
 
         // Don't need to verify if they are asked in a port, this was done previously
 
-        computeVisibleActivations(hidden_layer->getExpectations(),true);
+        computeVisibleActivations(hidden_layer->getExpectations(), true);
         if(visible_reconstruction_activations)
         {
             PLASSERT( visible_reconstruction_activations->isEmpty() );
@@ -1000,7 +1001,7 @@
                 PLASSERT( visible_reconstruction->isEmpty() );
                 const Mat& to_store = visible_layer->getExpectations();
                 visible_reconstruction->resize(to_store.length(),
-                                                           to_store.width());
+                                               to_store.width());
                 *visible_reconstruction << to_store;
             }
             if(reconstruction_error)
@@ -1120,67 +1121,67 @@
         // Note: we need to perform the following steps even if the gradient
         // learning rate is equal to 0. This is because we must propagate the
         // gradient to the visible layer, even though no update is required.
-            setAllLearningRates(grad_learning_rate);
-            PLASSERT( hidden && hidden_act );
-            // Compute gradient w.r.t. activations of the hidden layer.
-            hidden_layer->bpropUpdate(
-                    *hidden_act, *hidden, hidden_act_grad, *hidden_grad,
-                    false);
-            if (hidden_bias_grad)
-            {
-                PLASSERT( hidden_bias_grad->isEmpty() &&
-                          hidden_bias_grad->width() == hidden_layer->size );
-                hidden_bias_grad->resize(mbs,hidden_layer->size);
-                *hidden_bias_grad += hidden_act_grad;
-            }
-            // Compute gradient w.r.t. expectations of the visible layer (=
-            // inputs).
-            Mat* store_visible_grad = NULL;
-            if (compute_visible_grad) {
-                PLASSERT( visible_grad->width() == visible_layer->size );
-                store_visible_grad = visible_grad;
-            } else {
-                // We do not actually need to store the gradient, but since it
-                // is required in bpropUpdate, we provide a dummy matrix to
-                // store it.
-                store_visible_grad = &visible_exp_grad;
-            }
-            store_visible_grad->resize(mbs,visible_layer->size);
+        setAllLearningRates(grad_learning_rate);
+        PLASSERT( hidden && hidden_act );
+        // Compute gradient w.r.t. activations of the hidden layer.
+        hidden_layer->bpropUpdate(
+                *hidden_act, *hidden, hidden_act_grad, *hidden_grad,
+                false);
+        if (hidden_bias_grad)
+        {
+            PLASSERT( hidden_bias_grad->isEmpty() &&
+                      hidden_bias_grad->width() == hidden_layer->size );
+            hidden_bias_grad->resize(mbs,hidden_layer->size);
+            *hidden_bias_grad += hidden_act_grad;
+        }
+        // Compute gradient w.r.t. expectations of the visible layer (=
+        // inputs).
+        Mat* store_visible_grad = NULL;
+        if (compute_visible_grad) {
+            PLASSERT( visible_grad->width() == visible_layer->size );
+            store_visible_grad = visible_grad;
+        } else {
+            // We do not actually need to store the gradient, but since it
+            // is required in bpropUpdate, we provide a dummy matrix to
+            // store it.
+            store_visible_grad = &visible_exp_grad;
+        }
+        store_visible_grad->resize(mbs,visible_layer->size);
 
-            if (weights)
+        if (weights)
+        {
+            int up = connection->up_size;
+            int down = connection->down_size;
+            PLASSERT( !weights->isEmpty() &&
+                      weights_grad && weights_grad->isEmpty() &&
+                      weights_grad->width() == up * down );
+            weights_grad->resize(mbs, up * down);
+            Mat w, wg;
+            Vec v,h,vg,hg;
+            for(int i=0; i<mbs; i++)
             {
-                int up = connection->up_size;
-                int down = connection->down_size;
-                PLASSERT( !weights->isEmpty() &&
-                          weights_grad && weights_grad->isEmpty() &&
-                          weights_grad->width() == up * down );
-                weights_grad->resize(mbs, up * down);
-                Mat w, wg;
-                Vec v,h,vg,hg;
-                for(int i=0; i<mbs; i++)
-                {
-                    w = Mat(up, down,(*weights)(i));
-                    wg = Mat(up, down,(*weights_grad)(i));
-                    v = (*visible)(i);
-                    h = (*hidden_act)(i);
-                    vg = (*store_visible_grad)(i);
-                    hg = hidden_act_grad(i);
-                    connection->petiteCulotteOlivierUpdate(
-                        v,
-                        w,
-                        h,
-                        vg,
-                        wg,
-                        hg,true);
-                }
+                w = Mat(up, down,(*weights)(i));
+                wg = Mat(up, down,(*weights_grad)(i));
+                v = (*visible)(i);
+                h = (*hidden_act)(i);
+                vg = (*store_visible_grad)(i);
+                hg = hidden_act_grad(i);
+                connection->petiteCulotteOlivierUpdate(
+                    v,
+                    w,
+                    h,
+                    vg,
+                    wg,
+                    hg,true);
             }
-            else
-            {
-                connection->bpropUpdate(
-                    *visible, *hidden_act, *store_visible_grad,
-                    hidden_act_grad, true);
-            }
-            partition_function_is_stale = true;
+        }
+        else
+        {
+            connection->bpropUpdate(
+                *visible, *hidden_act, *store_visible_grad,
+                hidden_act_grad, true);
+        }
+        partition_function_is_stale = true;
     }
 
     if (cd_learning_rate > 0 && minimize_log_likelihood) {
@@ -1479,14 +1480,24 @@
             visible_act_grad(t) *= (*reconstruction_error_grad)(t,0);
 
         // Visible bias update
-        columnSum(visible_act_grad,visible_bias_grad);
+        columnMean(visible_act_grad, visible_bias_grad);
         visible_layer->update(visible_bias_grad);
 
         // Reconstruction connection update
-        reconstruction_connection->bpropUpdate(
-            *hidden, *visible_reconstruction_activations,
-            hidden_exp_grad, visible_act_grad, false);
+        hidden_exp_grad.resize(mbs, hidden_layer->size);
+        hidden_exp_grad.clear();
+        hidden_exp_grad.resize(0, hidden_layer->size);
 
+        TVec<Mat*> rec_ports_value(2);
+        rec_ports_value[0] = visible_reconstruction_activations;
+        rec_ports_value[1] = hidden;
+        TVec<Mat*> rec_ports_gradient(2);
+        rec_ports_gradient[0] = &visible_act_grad;
+        rec_ports_gradient[1] = &hidden_exp_grad;
+
+        reconstruction_connection->bpropAccUpdate( rec_ports_value,
+                                                   rec_ports_gradient );
+
         // Hidden layer bias update
         hidden_layer->bpropUpdate(*hidden_act,
                                   *hidden, hidden_act_grad,



From dorionc at mail.berlios.de  Wed Jul  4 16:15:37 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Wed, 4 Jul 2007 16:15:37 +0200
Subject: [Plearn-commits] r7693 - trunk/python_modules/plearn/analysis
Message-ID: <200707041415.l64EFbTm028006@sheep.berlios.de>

Author: dorionc
Date: 2007-07-04 16:15:37 +0200 (Wed, 04 Jul 2007)
New Revision: 7693

Modified:
   trunk/python_modules/plearn/analysis/latex.py
Log:
Minor modifications


Modified: trunk/python_modules/plearn/analysis/latex.py
===================================================================
--- trunk/python_modules/plearn/analysis/latex.py	2007-07-04 08:34:49 UTC (rev 7692)
+++ trunk/python_modules/plearn/analysis/latex.py	2007-07-04 14:15:37 UTC (rev 7693)
@@ -205,6 +205,7 @@
 
 \documentclass[11pt]{article}
 \usepackage{apstat_article_style}
+\usepackage{apstat_cover_classical}
 \usepackage{lscape}
 
 %%%%%  My caption settings  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@@ -293,9 +294,13 @@
     tex_file.write(TEX_END)
     tex_file.close()
 
-    #TBA: pdf_name = file_name.replace(".tex", '.pdf')
-    #TBA: os.system("rm -f %s"%pdf_name)
-    #TBA: os.system("pdflatex -interaction=nonstopmode %s >& /dev/null"%file_name)
-    #TBA: assert os.path.exists(pdf_name), "PDF could not be created!"
-    #TBA: os.system("pdflatex %s >& /dev/null"%file_name)
-    #TBA: os.system("pdflatex %s >& /dev/null"%file_name)
+    pdf_name = file_name.replace(".tex", '.pdf')
+    os.system("rm -f %s"%pdf_name)
+    assert not os.path.exists(pdf_name)
+    print "Creating %s from %s..."%(pdf_name, file_name)
+    
+    os.system("pdflatex -interaction=nonstopmode %s >& /dev/null"%file_name)
+    assert os.path.exists(pdf_name), "PDF could not be created!"
+    os.system("pdflatex %s -interaction=nonstopmode >& /dev/null"%file_name)
+    os.system("pdflatex %s -interaction=nonstopmode >& /dev/null"%file_name)
+    print "%s created."%pdf_name



From nouiz at mail.berlios.de  Wed Jul  4 16:25:22 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 4 Jul 2007 16:25:22 +0200
Subject: [Plearn-commits] r7694 - trunk/plearn_learners/generic
Message-ID: <200707041425.l64EPM0J028648@sheep.berlios.de>

Author: nouiz
Date: 2007-07-04 16:25:22 +0200 (Wed, 04 Jul 2007)
New Revision: 7694

Modified:
   trunk/plearn_learners/generic/AddCostToLearner.cc
   trunk/plearn_learners/generic/AddCostToLearner.h
Log:
Added a build option that let us tell for witch target we want the confusion matrix


Modified: trunk/plearn_learners/generic/AddCostToLearner.cc
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.cc	2007-07-04 14:15:37 UTC (rev 7693)
+++ trunk/plearn_learners/generic/AddCostToLearner.cc	2007-07-04 14:25:22 UTC (rev 7694)
@@ -93,7 +93,8 @@
       rescale_target(0),
       to_max(1),
       to_min(0),
-      n_classes(-1)
+      n_classes(-1),
+      confusion_matrix_target(0)
 {}
 
 ////////////////////
@@ -132,7 +133,7 @@
         "   difference between the class values\n"
         " - 'square_class_error': as class_error execpt that the output is the\n"
         "   square of the difference between the class values\n"
-        " - 'confusion_matrix': give the confusion matrix,\n"
+        " - 'confusion_matrix': give the confusion matrix of the target confusion_matrix_target,\n"
         "   the row is the predicted class, the column is the targeted class\n"
         " - 'lift_output': to compute the lift cost (for the positive class)\n"
         " - 'opposite_lift_output': to compute the lift cost (for the negative) class\n"
@@ -170,6 +171,9 @@
     declareOption(ol, "n_classes", &AddCostToLearner::n_classes, OptionBase::buildoption,
         "The number of classes. Only needed for the 'confusion_matrix' cost.");
 
+    declareOption(ol, "confusion_matrix_target", &AddCostToLearner::confusion_matrix_target, OptionBase::buildoption,
+                  "The target number for witch we calculate the confusion matrix. Default to 0.");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -519,10 +523,31 @@
             }
             costs[ind_cost] = diff;
         } else if (c == "confusion_matrix") {
+
+#ifdef BOUNDCHECK
+            if (confusion_matrix_target >= target_length)
+                PLERROR("In AddCostToLearner::computeCostsFromOutputs - confusion_matrix_target(%d) "
+                        "not in the range of target_length(%d)", confusion_matrix_target, target_length);            
+            if (sub_learner_output[confusion_matrix_target] >= n_classes
+                || desired_target[confusion_matrix_target] >= n_classes)
+                PLERROR("In AddCostToLearner::computeCostsFromOutputs - confusion_matrix sub_learner_output[i](%f) "
+                        "or desired_target[i](%d) higher or egual to n_classes (%d)", sub_learner_output[confusion_matrix_target],
+                        desired_target[confusion_matrix_target], n_classes);
+#endif
             for(int local_ind = ind_cost ; local_ind < (n_classes*n_classes+ind_cost); local_ind++){
                 costs[local_ind] = 0;
             }
-            int local_ind = ind_cost + argmax(sub_learner_output) + int(round(desired_target[0]))*n_classes;
+            int output_length = sub_learner_output.length();
+            int local_ind = 0;
+            if (output_length == target_length) {
+                local_ind = ind_cost + int(round(sub_learner_output[confusion_matrix_target]))
+                    + int(round(desired_target[confusion_matrix_target]))*n_classes;
+            } else if (target_length == 1){
+                local_ind = ind_cost + argmax(sub_learner_output) + int(round(desired_target[confusion_matrix_target]))*n_classes;
+            } else {
+                PLERROR("In AddCostToLearner::computeCostsFromOutputs - Wrong "
+                        "output and/or target for the 'confusion_matrix' cost");
+            }
             costs[local_ind] = 1;
             ind_cost += n_classes*n_classes - 1;//less one as the loop add one
         } else if (c == "mse") {

Modified: trunk/plearn_learners/generic/AddCostToLearner.h
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.h	2007-07-04 14:15:37 UTC (rev 7693)
+++ trunk/plearn_learners/generic/AddCostToLearner.h	2007-07-04 14:25:22 UTC (rev 7694)
@@ -122,6 +122,7 @@
     real to_max;
     real to_min;
     int n_classes;
+    int confusion_matrix_target;
 
     // ****************
     // * Constructors *



From nouiz at mail.berlios.de  Wed Jul  4 16:35:22 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 4 Jul 2007 16:35:22 +0200
Subject: [Plearn-commits] r7695 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200707041435.l64EZMWo029251@sheep.berlios.de>

Author: nouiz
Date: 2007-07-04 16:35:21 +0200 (Wed, 04 Jul 2007)
New Revision: 7695

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
Log:
-Removed the verbosity from NatGradNNet as it is in PLearner.
-Added a BOUNDCHECK


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-07-04 14:25:22 UTC (rev 7694)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-07-04 14:35:21 UTC (rev 7695)
@@ -72,7 +72,6 @@
       self_adjusted_scaling_and_bias(false),
       target_mean_activation(-4), // 
       target_stdev_activation(3), // 2.5% of the time we are above 1
-      verbosity(0),
       //corr_profiling_start(0), 
       //corr_profiling_end(0),
       use_pvgrad(false),
@@ -94,10 +93,6 @@
                   OptionBase::buildoption,
                   "Number of outputs of the neural network, which can be derived from  output_type and targetsize_\n");
 
-    declareOption(ol, "verbosity", &NatGradNNet::verbosity,
-                  OptionBase::buildoption,
-                  "Verbosity level\n");
-
     declareOption(ol, "n_layers", &NatGradNNet::n_layers,
                   OptionBase::learntoption,
                   "Number of layers of weights (ie. 2 for a neural net with one hidden layer).\n"
@@ -1015,6 +1010,11 @@
         for (int i=0;i<n_examples;i++)
         {
             int target_class = int(round(target(i,0)));
+#ifdef BOUNDCHECK
+            if(target_class>=noutputs)
+                PLERROR("In NatGradNNet::fbpropLoss one target value %d is higher then allowed by nout %d",
+                        target_class, noutputs);
+#endif          
             Vec outp = output(i);
             Vec grad = out_grad(i);
             exp(outp,grad); // map log-prob to prob

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-07-04 14:25:22 UTC (rev 7694)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-07-04 14:35:21 UTC (rev 7695)
@@ -140,8 +140,6 @@
     // average with this coefficient (near 0 for very slow averaging)
     real activation_statistics_moving_average_coefficient;
 
-    int verbosity;
-
     //! Stages for profiling the correlation between the gradients' elements
     //int corr_profiling_start, corr_profiling_end;
 



From tihocan at mail.berlios.de  Wed Jul  4 17:29:48 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 4 Jul 2007 17:29:48 +0200
Subject: [Plearn-commits] r7696 - trunk/plearn_learners/generic
Message-ID: <200707041529.l64FTm24000916@sheep.berlios.de>

Author: tihocan
Date: 2007-07-04 17:29:47 +0200 (Wed, 04 Jul 2007)
New Revision: 7696

Modified:
   trunk/plearn_learners/generic/AddCostToLearner.cc
Log:
Typo fixes in help

Modified: trunk/plearn_learners/generic/AddCostToLearner.cc
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.cc	2007-07-04 14:35:21 UTC (rev 7695)
+++ trunk/plearn_learners/generic/AddCostToLearner.cc	2007-07-04 15:29:47 UTC (rev 7696)
@@ -133,8 +133,9 @@
         "   difference between the class values\n"
         " - 'square_class_error': as class_error execpt that the output is the\n"
         "   square of the difference between the class values\n"
-        " - 'confusion_matrix': give the confusion matrix of the target confusion_matrix_target,\n"
-        "   the row is the predicted class, the column is the targeted class\n"
+        " - 'confusion_matrix': give the confusion matrix for the target\n"
+        "   'confusion_matrix_target', where the row is the predicted class\n"
+        "    and the column is the target class\n"
         " - 'lift_output': to compute the lift cost (for the positive class)\n"
         " - 'opposite_lift_output': to compute the lift cost (for the negative) class\n"
         " - 'cross_entropy': -t*log(o) - (1-t)*log(1-o)\n"
@@ -171,8 +172,10 @@
     declareOption(ol, "n_classes", &AddCostToLearner::n_classes, OptionBase::buildoption,
         "The number of classes. Only needed for the 'confusion_matrix' cost.");
 
-    declareOption(ol, "confusion_matrix_target", &AddCostToLearner::confusion_matrix_target, OptionBase::buildoption,
-                  "The target number for witch we calculate the confusion matrix. Default to 0.");
+    declareOption(ol, "confusion_matrix_target",
+                  &AddCostToLearner::confusion_matrix_target,
+                  OptionBase::buildoption,
+        "Index of the target for which the confusion matrix is computed.");
 
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);



From nouiz at mail.berlios.de  Thu Jul  5 22:13:34 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 5 Jul 2007 22:13:34 +0200
Subject: [Plearn-commits] r7697 - trunk/scripts
Message-ID: <200707052013.l65KDYRn025522@sheep.berlios.de>

Author: nouiz
Date: 2007-07-05 22:13:33 +0200 (Thu, 05 Jul 2007)
New Revision: 7697

Modified:
   trunk/scripts/cdispatch
Log:
Print short help if it receive bad parameter


Modified: trunk/scripts/cdispatch
===================================================================
--- trunk/scripts/cdispatch	2007-07-04 15:29:47 UTC (rev 7696)
+++ trunk/scripts/cdispatch	2007-07-05 20:13:33 UTC (rev 7697)
@@ -140,7 +140,8 @@
 	$REQ = substr($ARGV[0],6);
 	shift;
     } elsif (substr($ARGV[0],0,1) eq '-'){
-	die "Unknow parameter ($ARGV[0]) or wrong parameter order\n";
+	print "Unknow parameter ($ARGV[0]) or wrong parameter order\n";
+	die $ShortHelp
     } else {
 	$REQ = "";
     }



From nouiz at mail.berlios.de  Thu Jul  5 22:54:29 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 5 Jul 2007 22:54:29 +0200
Subject: [Plearn-commits] r7698 -
	branches/cgi-desjardin/plearn_learners/second_iteration
Message-ID: <200707052054.l65KsTfE028916@sheep.berlios.de>

Author: nouiz
Date: 2007-07-05 22:54:28 +0200 (Thu, 05 Jul 2007)
New Revision: 7698

Modified:
   branches/cgi-desjardin/plearn_learners/second_iteration/ComputePurenneError.cc
Log:
Modified cost name and added cost class_error


Modified: branches/cgi-desjardin/plearn_learners/second_iteration/ComputePurenneError.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/ComputePurenneError.cc	2007-07-05 20:13:33 UTC (rev 7697)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/ComputePurenneError.cc	2007-07-05 20:54:28 UTC (rev 7698)
@@ -84,7 +84,7 @@
     Vec sample_target(train_set->targetsize());
     real sample_weight;
     Vec sample_output(2);
-    Vec sample_costs(3);
+    Vec sample_costs(4);
     ProgressBar* pb = NULL;
     if (report_progress)
     {
@@ -114,10 +114,11 @@
 
 TVec<string> ComputePurenneError::getTrainCostNames() const
 {
-    TVec<string> return_msg(3);
+    TVec<string> return_msg(4);
     return_msg[0] = "mse";
-    return_msg[1] = "cse";
-    return_msg[2] = "cle";
+    return_msg[1] = "class_error";
+    return_msg[2] = "linear_class_error";
+    return_msg[3] = "square_class_error";
     return return_msg;
 }
 
@@ -141,9 +142,9 @@
 void ComputePurenneError::computeCostsFromOutputs(const Vec& inputv, const Vec& outputv, const Vec& targetv, Vec& costsv) const
 {
     costsv[0] = pow((outputv[0] - targetv[0]), 2.0);
-    costsv[1] = pow((outputv[1] - targetv[1]), 2.0);
-    if (outputv[1] == targetv[1]) costsv[2] = 0.0;
-    else costsv[2] = 1.0;
+    costsv[1] = outputv[1] == targetv[1] ? 0 : 1;
+    costsv[2] = int(round(fabs(outputv[1] - targetv[1])));
+    costsv[3] = pow((outputv[1] - targetv[1]), 2.0);
 }
 
 } // end of namespace PLearn



From nouiz at mail.berlios.de  Thu Jul  5 22:56:07 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 5 Jul 2007 22:56:07 +0200
Subject: [Plearn-commits] r7699 -
	branches/cgi-desjardin/plearn_learners/second_iteration
Message-ID: <200707052056.l65Ku70v029045@sheep.berlios.de>

Author: nouiz
Date: 2007-07-05 22:56:06 +0200 (Thu, 05 Jul 2007)
New Revision: 7699

Modified:
   branches/cgi-desjardin/plearn_learners/second_iteration/SecondIterationWrapper.cc
Log:
-changed cost name and added a cost
-BUGFIX: if base_regressor is not present, outputsize() return -1


Modified: branches/cgi-desjardin/plearn_learners/second_iteration/SecondIterationWrapper.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/SecondIterationWrapper.cc	2007-07-05 20:54:28 UTC (rev 7698)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/SecondIterationWrapper.cc	2007-07-05 20:56:06 UTC (rev 7699)
@@ -141,7 +141,7 @@
     Vec sample_target(train_set->targetsize());
     real sample_weight;
     Vec sample_output(base_regressor->outputsize());
-    Vec sample_costs(3);
+    Vec sample_costs(4);
     ProgressBar* pb = NULL;
     if (report_progress)
     {
@@ -168,7 +168,7 @@
     Vec reference_vector(ref_train->width());
     real sample_weight;
     Vec sample_output(base_regressor->outputsize());
-    Vec sample_costs(3);
+    Vec sample_costs(4);
     Vec train_mean(3);
     Vec train_std_error(3);
     Vec valid_mean(3);
@@ -319,15 +319,16 @@
 
 int SecondIterationWrapper::outputsize() const
 {
-    return base_regressor->outputsize();
+    return base_regressor?base_regressor->outputsize():-1;
 }
 
 TVec<string> SecondIterationWrapper::getTrainCostNames() const
 {
-    TVec<string> return_msg(3);
+    TVec<string> return_msg(4);
     return_msg[0] = "mse";
-    return_msg[1] = "cse";
-    return_msg[2] = "cle";
+    return_msg[1] = "square_class_error";
+    return_msg[2] = "linear_class_error";
+    return_msg[3] = "class_error";
     return return_msg;
 }
 
@@ -353,16 +354,18 @@
     if (class_prediction == 1)
     {
         real class_pred;
-        if (outputv[0] <= 1.5) class_pred = 1.0;
+        if (outputv[0] <= 0.5) class_pred = 0.;
+        else if (outputv[0] <= 1.5) class_pred = 1.0;
         else if (outputv[0] <= 2.5) class_pred = 2.0;
         else class_pred = 3.0;
         costsv[1] = pow((class_pred - targetv[0]), 2.0);
-        if (class_pred == targetv[0]) costsv[2] = 0.0;
-        else costsv[2] = 1.0;
+        costsv[2] = fabs(class_pred - targetv[0]);
+        costsv[3] = class_pred == targetv[0]?0:1;
         return;
     }
     costsv[1] = 0.0;
     costsv[2] = 0.0;
+    costsv[3] = 0.0;
 }
 
 } // end of namespace PLearn



From yoshua at mail.berlios.de  Thu Jul  5 23:18:20 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Thu, 5 Jul 2007 23:18:20 +0200
Subject: [Plearn-commits] r7700 - trunk/plearn_learners/online/EXPERIMENTAL
Message-ID: <200707052118.l65LIKSI030972@sheep.berlios.de>

Author: yoshua
Date: 2007-07-05 23:18:19 +0200 (Thu, 05 Jul 2007)
New Revision: 7700

Added:
   trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
   trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.h
Log:
Clone of RBMModule which also computes and optimizes the KL(p0||p1) criterion


Added: trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-05 20:56:06 UTC (rev 7699)
+++ trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-05 21:18:19 UTC (rev 7700)
@@ -0,0 +1,1780 @@
+// -*- C++ -*-
+
+// KLp0p1RBMModule.cc
+//
+// Copyright (C) 2007 Olivier Delalleau, Yoshua Bengio
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau, Yoshua Bengio
+
+/*! \file KLp0p1RBMModule.cc */
+
+
+
+#include "KLp0p1RBMModule.h"
+#include <plearn/vmat/VMat.h>
+#include <plearn_learners/online/RBMMatrixConnection.h>
+
+#define PL_LOG_MODULE_NAME "KLp0p1RBMModule"
+#include <plearn/io/pl_log.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    KLp0p1RBMModule,
+    "Implement KL(p0||p1) criterion for RBMs",
+    "This criterion is described and justified in the paper by Le Roux and Bengio entitled"
+    "'Representational Power of Restricted Boltzmann Machines and Deep Belief Networks'."
+    "The exact and very inefficient implementation of this criterion is done here."
+    "For an example x the cost is:"
+    "  C(x) = - log P1(x) = - log (1/n)sum_{k=1}^n sum_h P(x|h) P(h|x^k)"
+    "where {x^1, ... x^n} is the training set of examples x^k, h is a hidden layer bit vector,"
+    "P(x|h) is the hidden-to-visible conditional distribution and P(h|x) is the"
+    "input-to-hidden conditional distribution. Both are the usual found in Binomial"
+    "layer RBMs here."
+    "The gradient on the weight Wij is"
+    "  dC(x)/dWij = (1/(n P1(x))) "
+    "       sum_{k=1}^n sum_h P(x|h) P(h|x^k) (h_i(x_j - P(x_j|h)) + x_j^k(h_i - P(h_i|x^k)))"
+    "Apart from the KLp0p1 output port, and the fact that CD learning is replaced by minimization"
+    "of KLp0p1, this module acts like a regular RBMModule."
+);
+
+///////////////
+// KLp0p1RBMModule //
+///////////////
+KLp0p1RBMModule::KLp0p1RBMModule():
+    cd_learning_rate(0),
+    grad_learning_rate(0),
+    klp0p1_learning_rate(0),
+    compute_contrastive_divergence(false),
+    n_Gibbs_steps_CD(1),
+    min_n_Gibbs_steps(1),
+    n_Gibbs_steps_per_generated_sample(-1),
+    compute_log_likelihood(false),
+    minimize_log_likelihood(false),
+    Gibbs_step(0),
+    log_partition_function(0),
+    partition_function_is_stale(true),
+    standard_cd_grad(true),
+    standard_cd_bias_grad(true),
+    standard_cd_weights_grad(true),
+    hidden_bias(NULL),
+    weights(NULL),
+    hidden_act(NULL),
+    hidden_activations_are_computed(false)
+{
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void KLp0p1RBMModule::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "training_set", &KLp0p1RBMModule::training_set,
+                  OptionBase::buildoption,
+                  "VMatrix with one input example per row, the training set.");
+
+    declareOption(ol, "visible_layer", &KLp0p1RBMModule::visible_layer,
+                  OptionBase::buildoption,
+        "Visible layer of the RBM.");
+
+    declareOption(ol, "hidden_layer", &KLp0p1RBMModule::hidden_layer,
+                  OptionBase::buildoption,
+        "Hidden layer of the RBM.");
+
+    declareOption(ol, "connection", &KLp0p1RBMModule::connection,
+                  OptionBase::buildoption,
+        "Connection between the visible and hidden layers.");
+
+    declareOption(ol, "reconstruction_connection", 
+                  &KLp0p1RBMModule::reconstruction_connection,
+                  OptionBase::buildoption,
+        "Reconstuction connection between the hidden and visible layers.");
+
+    declareOption(ol, "grad_learning_rate", &KLp0p1RBMModule::grad_learning_rate,
+                  OptionBase::buildoption,
+        "Learning rate for the gradient descent step.");
+
+    declareOption(ol, "cd_learning_rate", &KLp0p1RBMModule::cd_learning_rate,
+                  OptionBase::buildoption,
+        "Learning rate for the constrastive divergence step. Note that when\n"
+        "set to 0, the gradient of the contrastive divergence will not be\n"
+        "computed at all.");
+
+    declareOption(ol, "klp0p1_learning_rate", &KLp0p1RBMModule::klp0p1_learning_rate,
+                  OptionBase::buildoption,
+        "Learning rate for the KLp0p1 criterion update. If\n"
+        "set to 0, the gradient of KLp0p1 (and corresponding update) will not be\n"
+        "computed at all.");
+
+    declareOption(ol, "compute_contrastive_divergence", &KLp0p1RBMModule::compute_contrastive_divergence,
+                  OptionBase::buildoption,
+        "Compute the constrastive divergence in an output port.");
+
+    declareOption(ol, "standard_cd_grad",
+                  &KLp0p1RBMModule::standard_cd_grad,
+                  OptionBase::buildoption,
+        "Whether to use the standard contrastive divergence gradient for\n"
+        "updates, or the true gradient of the contrastive divergence. This\n"
+        "affects only the gradient w.r.t. internal parameters of the layers\n"
+        "and connections. Currently, this option works only with layers of\n"
+        "the type 'RBMBinomialLayer', connected by a 'RBMMatrixConnection'.");
+
+    declareOption(ol, "standard_cd_bias_grad",
+                  &KLp0p1RBMModule::standard_cd_bias_grad,
+                  OptionBase::buildoption,
+        "This option is only used when biases of the hidden layer are given\n"
+        "through the 'hidden_bias' port. When this is the case, the gradient\n"
+        "of contrastive divergence w.r.t. these biases is either computed:\n"
+        "- by the usual formula if 'standard_cd_bias_grad' is true\n"
+        "- by the true gradient if 'standard_cd_bias_grad' is false.");
+
+    declareOption(ol, "standard_cd_weights_grad",
+                  &KLp0p1RBMModule::standard_cd_weights_grad,
+                  OptionBase::buildoption,
+        "This option is only used when weights of the connection are given\n"
+        "through the 'weights' port. When this is the case, the gradient of\n"
+        "contrastive divergence w.r.t. weights is either computed:\n"
+        "- by the usual formula if 'standard_cd_weights_grad' is true\n"
+        "- by the true gradient if 'standard_cd_weights_grad' is false.");
+
+    declareOption(ol, "n_Gibbs_steps_CD", 
+                  &KLp0p1RBMModule::n_Gibbs_steps_CD,
+                  OptionBase::buildoption,
+                  "Number of Gibbs sampling steps in negative phase of "
+                  "contrastive divergence.");
+
+    declareOption(ol, "min_n_Gibbs_steps", &KLp0p1RBMModule::min_n_Gibbs_steps,
+                  OptionBase::buildoption,
+                  "Used in generative mode (when visible_sample or hidden_sample is requested)\n"
+                  "when one has to sample from the joint or a marginal of visible and hidden,\n"
+                  "and thus a Gibbs chain has to be run. This option gives the minimum number\n"
+                  "of Gibbs steps to perform in the chain before outputting a sample.\n");
+
+    declareOption(ol, "n_Gibbs_steps_per_generated_sample", 
+                  &KLp0p1RBMModule::n_Gibbs_steps_per_generated_sample,
+                  OptionBase::buildoption,
+                  "Used in generative mode (when visible_sample or hidden_sample is requested)\n"
+                  "when one has to sample from the joint or a marginal of visible and hidden,\n"
+                  "This option gives the number of steps to run in the Gibbs chain between\n"
+                  "consecutive generated samples that are produced in output of the fprop method.\n"
+                  "By default this is equal to min_n_Gibbs_steps.\n");
+
+    declareOption(ol, "compute_log_likelihood",
+                  &KLp0p1RBMModule::compute_log_likelihood,
+                  OptionBase::buildoption,
+                  "Whether to compute the exact RBM generative model's log-likelihood\n"
+                  "(on the neg_log_likelihood port). If false then the neg_log_likelihood\n"
+                  "port just computes the input visible's free energy.\n");
+    
+    declareOption(ol, "minimize_log_likelihood",
+                  &KLp0p1RBMModule::minimize_log_likelihood,
+                  OptionBase::buildoption,
+                  "Whether to minimize the exact RBM generative model's log-likelihood\n"
+                  "i.e. take stochastic gradient steps w.r.t. the log-likelihood instead\n"
+                  "of w.r.t. the contrastive divergence.\n");
+
+    declareOption(ol, "Gibbs_step", 
+                  &KLp0p1RBMModule::Gibbs_step,
+                  OptionBase::learntoption,
+                  "Used in generative mode (when visible_sample or hidden_sample is requested)\n"
+                  "when one has to sample from the joint or a marginal of visible and hidden,\n"
+                  "Keeps track of the number of steps that have been run since the beginning\n"
+                  "of the chain.\n");
+
+    declareOption(ol, "log_partition_function", 
+                  &KLp0p1RBMModule::log_partition_function,
+                  OptionBase::learntoption,
+                  "log(Z) = log(sum_{h,x} exp(-energy(h,x))\n"
+                  "only computed if compute_log_likelihood is true and\n"
+                  "the neg_log_likelihood port is requested.\n");
+
+    declareOption(ol, "partition_function_is_stale", 
+                  &KLp0p1RBMModule::partition_function_is_stale,
+                  OptionBase::learntoption,
+                  "Whether parameters have changed since the last computation\n"
+                  "of the log_partition_function (to know if it should be recomputed\n"
+                  "when the neg_log_likelihood port is requested.\n");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+////////////
+// build_ //
+////////////
+void KLp0p1RBMModule::build_()
+{
+    PLASSERT( cd_learning_rate >= 0 && grad_learning_rate >= 0 );
+    if(visible_layer)
+        visible_bias_grad.resize(visible_layer->size);
+
+    conf_visible_layer = PLearn::deepCopy(visible_layer);
+    conf_hidden_layer = PLearn::deepCopy(hidden_layer);
+
+    // Forward random generator to underlying modules.
+    if (random_gen) {
+        if (hidden_layer && !hidden_layer->random_gen) {
+            hidden_layer->random_gen = random_gen;
+            hidden_layer->build();
+            hidden_layer->forget();
+        }
+        if (visible_layer && !visible_layer->random_gen) {
+            visible_layer->random_gen = random_gen;
+            visible_layer->build();
+            visible_layer->forget();
+        }
+        if (connection && !connection->random_gen) {
+            connection->random_gen = random_gen;
+            connection->build();
+            connection->forget();
+        }
+        if (reconstruction_connection &&
+                !reconstruction_connection->random_gen) {
+            reconstruction_connection->random_gen = random_gen;
+            reconstruction_connection->build();
+            reconstruction_connection->forget();
+        }
+    }
+
+    // buid ports and port_sizes
+
+    ports.resize(0);
+    portname_to_index.clear();
+    addPortName("visible");
+    addPortName("hidden.state");
+    addPortName("hidden_activations.state");
+    addPortName("visible_sample");
+    addPortName("visible_expectation");
+    addPortName("hidden_sample");
+    addPortName("energy");
+    addPortName("hidden_bias"); 
+    addPortName("weights"); 
+    addPortName("neg_log_likelihood");
+    addPortName("KLp0p1"); 
+    if(reconstruction_connection)
+    {
+        addPortName("visible_reconstruction.state");
+        addPortName("visible_reconstruction_activations.state");
+        addPortName("reconstruction_error.state");
+    }
+    if (compute_contrastive_divergence)
+    {
+        addPortName("contrastive_divergence");
+        addPortName("negative_phase_visible_samples.state");
+        addPortName("negative_phase_hidden_expectations.state");
+        addPortName("negative_phase_hidden_activations.state");
+    }
+
+    port_sizes.resize(nPorts(), 2);
+    port_sizes.fill(-1);
+    if (visible_layer) {
+        port_sizes(getPortIndex("visible"), 1) = visible_layer->size;
+        port_sizes(getPortIndex("visible_sample"), 1) = visible_layer->size;
+        port_sizes(getPortIndex("visible_expectation"), 1) = visible_layer->size;
+    }
+    if (hidden_layer) {
+        port_sizes(getPortIndex("hidden.state"), 1) = hidden_layer->size;
+        port_sizes(getPortIndex("hidden_activations.state"), 1) = hidden_layer->size; 
+        port_sizes(getPortIndex("hidden_sample"), 1) = hidden_layer->size; 
+        port_sizes(getPortIndex("hidden_bias"),1) = hidden_layer->size;
+        if(visible_layer)
+            port_sizes(getPortIndex("weights"),1) = hidden_layer->size * visible_layer->size;
+    }
+    port_sizes(getPortIndex("energy"),1) = 1;
+    port_sizes(getPortIndex("neg_log_likelihood"),1) = 1;
+    port_sizes(getPortIndex("KLp0p1"),1) = 1;
+    if(reconstruction_connection)
+    {
+        if (visible_layer) {
+            port_sizes(getPortIndex("visible_reconstruction.state"),1) = 
+                visible_layer->size; 
+            port_sizes(getPortIndex("visible_reconstruction_activations.state"),1) = 
+                       visible_layer->size; 
+        }
+        port_sizes(getPortIndex("reconstruction_error.state"),1) = 1; 
+    }
+    if (compute_contrastive_divergence)
+    {
+        port_sizes(getPortIndex("contrastive_divergence"),1) = 1; 
+        if (visible_layer) 
+            port_sizes(getPortIndex("negative_phase_visible_samples.state"),1) = visible_layer->size; 
+        if (hidden_layer)
+            port_sizes(getPortIndex("negative_phase_hidden_expectations.state"),1) = hidden_layer->size; 
+        if (fast_exact_is_equal(cd_learning_rate, 0))
+            PLWARNING("In KLp0p1RBMModule::build_ - Contrastive divergence is "
+                    "computed but 'cd_learning_rate' is set to 0: no internal "
+                    "update will be performed AND no contrastive divergence "
+                    "gradient will be propagated.");
+    }
+
+    PLCHECK_MSG(!(!standard_cd_grad && standard_cd_bias_grad), "You cannot "
+            "compute the standard CD gradient w.r.t. external hidden bias and "
+            "use the 'true' CD gradient w.r.t. internal hidden bias");
+
+    if (n_Gibbs_steps_per_generated_sample<0)
+        n_Gibbs_steps_per_generated_sample = min_n_Gibbs_steps;
+
+}
+
+///////////
+// build //
+///////////
+void KLp0p1RBMModule::build()
+{
+    inherited::build();
+    build_();
+}
+
+/////////////////
+// addPortName //
+/////////////////
+void KLp0p1RBMModule::addPortName(const string& name)
+{
+    PLASSERT( portname_to_index.find(name) == portname_to_index.end() );
+    portname_to_index[name] = ports.length();
+    ports.append(name);
+}
+
+///////////////////
+// computeEnergy //
+///////////////////
+// FULLY OBSERVED CASE
+// we know x and h:
+// energy(h,x) = -b'x - c'h - h'Wx
+//  = visible_layer->energy(x) + hidden_layer->energy(h)
+//      - dot(h, hidden_layer->activation-c)
+//  = visible_layer->energy(x) - dot(h, hidden_layer->activation)
+void KLp0p1RBMModule::computeEnergy(const Mat& visible, const Mat& hidden,
+                              Mat& energy, bool positive_phase)
+{
+    int mbs=hidden.length();
+    energy.resize(mbs, 1);
+    Mat* hidden_activations = NULL;
+    if (positive_phase) {
+        computePositivePhaseHiddenActivations(visible);
+        hidden_activations = hidden_act;
+    } else {
+        computeHiddenActivations(visible);
+        hidden_activations = & hidden_layer->activations;
+    }
+    PLASSERT( hidden_activations );
+    for (int i=0;i<mbs;i++)
+        energy(i,0) = visible_layer->energy(visible(i))
+            - dot(hidden(i), (*hidden_activations)(i));
+            // Why not: + hidden_layer->energy(hidden(i)) ?
+}
+
+///////////////////////////////
+// computeFreeEnergyOfHidden //
+///////////////////////////////
+// FREE-ENERGY(hidden) CASE
+// we know h:
+// free energy = -log sum_x e^{-energy(h,x)}
+//  = -c'h - sum_i log sigmoid(b_i + W_{.i}'h) .... FOR BINOMIAL INPUT LAYER
+// or more robustly,
+//  = hidden_layer->energy(h) - sum_i softplus(visible_layer->activation[i])
+void KLp0p1RBMModule::computeFreeEnergyOfHidden(const Mat& hidden, Mat& energy)
+{
+    int mbs=hidden.length();
+    if (energy.isEmpty())
+        energy.resize(mbs,1);
+    else {
+        PLASSERT( energy.length() == mbs && energy.width() == 1 );
+    }
+    PLASSERT(visible_layer->classname()=="RBMBinomialLayer");
+    computeVisibleActivations(hidden, false);
+    for (int i=0;i<mbs;i++)
+    {
+        energy(i,0) = hidden_layer->energy(hidden(i));
+        if (use_fast_approximations)
+            for (int j=0;j<visible_layer->size;j++)
+                energy(i,0) -= tabulated_softplus(visible_layer->activations(i,j));
+        else
+            for (int j=0;j<visible_layer->size;j++)
+                energy(i,0) -= softplus(visible_layer->activations(i,j));
+    }
+}
+
+////////////////////////////////
+// computeFreeEnergyOfVisible //
+////////////////////////////////
+// FREE-ENERGY(visible) CASE
+// we know x:
+// free energy = -log sum_h e^{-energy(h,x)}
+//  = -b'x - sum_i log sigmoid(c_i + W_i'x) .... FOR BINOMIAL HIDDEN LAYER
+// or more robustly,
+//  = visible_layer->energy(x) - sum_i softplus(hidden_layer->activation[i])
+void KLp0p1RBMModule::computeFreeEnergyOfVisible(const Mat& visible, Mat& energy,
+                                           bool positive_phase)
+{
+    int mbs=visible.length();
+    if (energy.isEmpty())
+        energy.resize(mbs,1);
+    else {
+        PLASSERT( energy.length() == mbs && energy.width() == 1 );
+    }
+    PLASSERT(hidden_layer->classname()=="RBMBinomialLayer");
+    Mat* hidden_activations = NULL;
+    if (positive_phase) {
+        computePositivePhaseHiddenActivations(visible);
+        hidden_activations = hidden_act;
+    }
+    else {
+        computeHiddenActivations(visible);
+        hidden_activations = & hidden_layer->activations;
+    }
+    PLASSERT( hidden_activations && hidden_activations->length() == mbs
+            && hidden_activations->width() == hidden_layer->size );
+    for (int i=0;i<mbs;i++)
+    {
+        energy(i,0) = visible_layer->energy(visible(i));
+        if (use_fast_approximations)
+            for (int j=0;j<hidden_layer->size;j++)
+                energy(i,0) -= tabulated_softplus((*hidden_activations)(i,j));
+        else
+            for (int j=0;j<hidden_layer->size;j++)
+                energy(i,0) -= softplus((*hidden_activations)(i,j));
+    }
+}
+
+//////////////////////////////
+// computeHiddenActivations //
+//////////////////////////////
+void KLp0p1RBMModule::computeHiddenActivations(const Mat& visible)
+{
+    if(weights && !weights->isEmpty())
+    {
+        Mat old_weights;
+        Vec old_activation;
+        connection->getAllWeights(old_weights);
+        old_activation = hidden_layer->activation;
+        int up = connection->up_size;
+        int down = connection->down_size;
+        PLASSERT( weights->width() == up * down  );
+        hidden_layer->setBatchSize( visible.length() );
+        for(int i=0; i<visible.length(); i++)
+        {
+            connection->setAllWeights(Mat(up, down, (*weights)(i)));
+            connection->setAsDownInput(visible(i));
+            hidden_layer->activation = hidden_layer->activations(i);
+            hidden_layer->getAllActivations(connection, 0, false);
+            if (hidden_bias && !hidden_bias->isEmpty())
+                hidden_layer->activation += (*hidden_bias)(i);
+        }
+        connection->setAllWeights(old_weights);
+        hidden_layer->activation = old_activation;
+    }
+    else
+    {
+        connection->setAsDownInputs(visible);
+        hidden_layer->getAllActivations(connection, 0, true);
+        if (hidden_bias && !hidden_bias->isEmpty())
+            hidden_layer->activations += *hidden_bias;
+    }
+}
+
+///////////////////////////////////////////
+// computePositivePhaseHiddenActivations //
+///////////////////////////////////////////
+void KLp0p1RBMModule::computePositivePhaseHiddenActivations(const Mat& visible)
+{
+    if (hidden_activations_are_computed) {
+        // Nothing to do.
+        PLASSERT( !hidden_act || !hidden_act->isEmpty() );
+        return;
+    }
+    computeHiddenActivations(visible);
+    if (hidden_act && hidden_act->isEmpty())
+    {
+        hidden_act->resize(visible.length(),hidden_layer->size);
+        *hidden_act << hidden_layer->activations;
+    }
+    hidden_activations_are_computed = true;
+}
+
+///////////////////////////////
+// computeVisibleActivations //
+///////////////////////////////
+void KLp0p1RBMModule::computeVisibleActivations(const Mat& hidden,
+                                          bool using_reconstruction_connection)
+{
+    if (using_reconstruction_connection)
+    {
+        PLASSERT( reconstruction_connection );
+        reconstruction_connection->setAsDownInputs(hidden);
+        visible_layer->getAllActivations(reconstruction_connection, 0, true);
+    }
+    else
+    {
+        if(weights && !weights->isEmpty())
+        {
+            Mat old_weights;
+            Vec old_activation;
+            connection->getAllWeights(old_weights);
+            old_activation = visible_layer->activation;
+            int up = connection->up_size;
+            int down = connection->down_size;
+            PLASSERT( weights->width() == up * down  );
+            visible_layer->setBatchSize( hidden.length() );
+            for(int i=0; i<hidden.length(); i++)
+            {
+                connection->setAllWeights(Mat(up,down,(*weights)(i)));
+                connection->setAsUpInput(hidden(i));
+                visible_layer->activation = visible_layer->activations(i);
+                visible_layer->getAllActivations(connection, 0, false);
+            }
+            connection->setAllWeights(old_weights);
+            visible_layer->activation = old_activation;
+        }
+        else
+        {
+            connection->setAsUpInputs(hidden);
+            visible_layer->getAllActivations(connection, 0, true);
+        }
+    }
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void KLp0p1RBMModule::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(hidden_layer,     copies);
+    deepCopyField(visible_layer,    copies);
+    deepCopyField(connection,       copies);
+    deepCopyField(reconstruction_connection, copies);
+
+    deepCopyField(hidden_exp_grad, copies);
+    deepCopyField(hidden_act_grad, copies);
+    deepCopyField(store_weights_grad, copies);
+    deepCopyField(store_hidden_bias_grad, copies);
+    deepCopyField(visible_exp_grad, copies);
+    deepCopyField(visible_act_grad, copies);
+    deepCopyField(visible_bias_grad, copies);
+    deepCopyField(hidden_exp_store, copies);
+    deepCopyField(hidden_act_store, copies);
+
+    deepCopyField(ports, copies);
+    deepCopyField(energy_inputs, copies);
+}
+
+///////////
+// fprop //
+///////////
+void KLp0p1RBMModule::fprop(const Vec& input, Vec& output) const
+{
+    PLERROR("In KLp0p1RBMModule::fprop - Not implemented");
+}
+
+void KLp0p1RBMModule::fprop(const TVec<Mat*>& ports_value)
+{
+
+    PLASSERT( ports_value.length() == nPorts() );
+    PLASSERT( visible_layer );
+    PLASSERT( hidden_layer );
+    PLASSERT( connection );
+
+    Mat* visible = ports_value[getPortIndex("visible")]; 
+    Mat* hidden = ports_value[getPortIndex("hidden.state")];
+    hidden_act = ports_value[getPortIndex("hidden_activations.state")];
+    Mat* visible_sample = ports_value[getPortIndex("visible_sample")];
+    Mat* visible_expectation = ports_value[getPortIndex("visible_expectation")];
+    Mat* hidden_sample = ports_value[getPortIndex("hidden_sample")];
+    Mat* energy = ports_value[getPortIndex("energy")];
+    Mat* neg_log_likelihood = ports_value[getPortIndex("neg_log_likelihood")];
+    Mat* KLp0p1 = ports_value[getPortIndex("KLp0p1")];
+    hidden_bias = ports_value[getPortIndex("hidden_bias")];
+    weights = ports_value[getPortIndex("weights")];
+    Mat* visible_reconstruction = 0;
+    Mat* visible_reconstruction_activations = 0;
+    Mat* reconstruction_error = 0;
+    if(reconstruction_connection)
+    {
+        visible_reconstruction = 
+            ports_value[getPortIndex("visible_reconstruction.state")]; 
+        visible_reconstruction_activations = 
+            ports_value[getPortIndex("visible_reconstruction_activations.state")];
+        reconstruction_error = 
+            ports_value[getPortIndex("reconstruction_error.state")];
+    }
+    Mat* contrastive_divergence = 0;
+    Mat* negative_phase_visible_samples = 0;
+    Mat* negative_phase_hidden_expectations = 0;
+    Mat* negative_phase_hidden_activations = NULL;
+    if (compute_contrastive_divergence)
+    {
+        contrastive_divergence = ports_value[getPortIndex("contrastive_divergence")]; 
+        if (!contrastive_divergence || !contrastive_divergence->isEmpty())
+            PLERROR("In KLp0p1RBMModule::fprop - When option "
+                    "'compute_contrastive_divergence' is 'true', the "
+                    "'contrastive_divergence' port should be provided, as an "
+                    "output.");
+        negative_phase_visible_samples = 
+            ports_value[getPortIndex("negative_phase_visible_samples.state")];
+        negative_phase_hidden_expectations = 
+            ports_value[getPortIndex("negative_phase_hidden_expectations.state")];
+        negative_phase_hidden_activations =
+            ports_value[getPortIndex("negative_phase_hidden_activations.state")];
+    }
+
+    bool hidden_expectations_are_computed = false;
+    hidden_activations_are_computed = false;
+    bool found_a_valid_configuration = false;
+
+    if (visible && !visible->isEmpty())
+    {
+        // When an input is provided, that would restart the chain for
+        // unconditional sampling, from that example.
+        Gibbs_step = 0; 
+        visible_layer->setExpectations(*visible);
+    }
+
+    // COMPUTE ENERGY
+    if (energy) 
+    {
+        PLASSERT_MSG( energy->isEmpty(), 
+                      "KLp0p1RBMModule: the energy port can only be an output port\n" );
+        if (visible && !visible->isEmpty()
+            && hidden && !hidden->isEmpty())
+        {
+            computeEnergy(*visible, *hidden, *energy);
+        }
+        else if (visible && !visible->isEmpty())
+        {
+            computeFreeEnergyOfVisible(*visible,*energy);
+        }
+        else if (hidden && !hidden->isEmpty())
+        {
+            computeFreeEnergyOfHidden(*hidden,*energy);
+        }
+        else
+        {
+            PLERROR("KLp0p1RBMModule: unknown configuration to compute energy (currently\n"
+                    "only possible if at least visible or hidden are provided).\n");
+        }
+        found_a_valid_configuration = true;
+    }
+    if (neg_log_likelihood && neg_log_likelihood->isEmpty() && compute_log_likelihood)
+    {
+        if (partition_function_is_stale && !during_training)
+        {
+            PLASSERT_MSG(hidden_layer->size<32 || visible_layer->size<32,
+                         "To compute exact log-likelihood of an RBM, hidden_layer->size "
+                         "or visible_layer->size must be <32");
+            // recompute partition function
+            if (hidden_layer->size > visible_layer->size)
+                // do it by log-summing minus-free-energy of visible configurations
+            {
+                PLASSERT(visible_layer->classname()=="RBMBinomialLayer");
+                // assuming a binary input we sum over all bit configurations
+                int n_configurations = 1 << visible_layer->size; // = 2^{visible_layer->size}
+                energy_inputs.resize(1, visible_layer->size);
+                Vec input = energy_inputs(0);
+                // COULD BE DONE MORE EFFICIENTLY BY DOING MANY CONFIGURATIONS
+                // AT ONCE IN A 'MINIBATCH'
+                Mat free_energy(1, 1);
+                log_partition_function = 0;
+                for (int c=0;c<n_configurations;c++)
+                {
+                    // convert integer c into a bit-wise visible representation
+                    int x=c;
+                    for (int i=0;i<visible_layer->size;i++)
+                    {
+                        input[i]= x & 1; // take least significant bit
+                        x >>= 1; // and shift right (divide by 2)
+                    }
+                    computeFreeEnergyOfVisible(energy_inputs,free_energy,false);
+                    if (c==0)
+                        log_partition_function = -free_energy(0,0);
+                    else
+                        log_partition_function = logadd(log_partition_function,-free_energy(0,0));
+                }
+            }
+            else
+                // do it by summing free-energy of hidden configurations
+            {
+                PLASSERT(hidden_layer->classname()=="RBMBinomialLayer");
+                // assuming a binary hidden we sum over all bit configurations
+                int n_configurations = 1 << hidden_layer->size; // = 2^{hidden_layer->size}
+                energy_inputs.resize(1, hidden_layer->size);
+                Vec input = energy_inputs(0);
+                // COULD BE DONE MORE EFFICIENTLY BY DOING MANY CONFIGURATIONS
+                // AT ONCE IN A 'MINIBATCH'
+                Mat free_energy(1,1);
+                log_partition_function = 0;
+                for (int c=0;c<n_configurations;c++)
+                {
+                    // convert integer c into a bit-wise hidden representation
+                    int x=c;
+                    for (int i=0;i<hidden_layer->size;i++)
+                    {
+                        input[i]= x & 1; // take least significant bit
+                        x >>= 1; // and shift right (divide by 2)
+                    }
+                    computeFreeEnergyOfHidden(energy_inputs, free_energy);
+                    if (c==0)
+                        log_partition_function = -free_energy(0,0);
+                    else
+                        log_partition_function = logadd(log_partition_function,-free_energy(0,0));
+                }
+            }
+            partition_function_is_stale=false;
+        }
+        if (visible && !visible->isEmpty()
+            && hidden && !hidden->isEmpty())
+        {
+            // neg-log-likelihood(visible,hidden) = energy(visible,visible) + log(partition_function)
+            computeEnergy(*visible,*hidden,*neg_log_likelihood);
+            *neg_log_likelihood += log_partition_function;
+        }
+        else if (visible && !visible->isEmpty()) 
+        {
+            // neg-log-likelihood(visible) = free_energy(visible) + log(partition_function)
+            computeFreeEnergyOfVisible(*visible,*neg_log_likelihood);
+            *neg_log_likelihood += log_partition_function;
+        }
+        else if (hidden && !hidden->isEmpty())
+        {
+            // neg-log-likelihood(hidden) = free_energy(hidden) + log(partition_function)
+            computeFreeEnergyOfHidden(*hidden,*neg_log_likelihood);
+            *neg_log_likelihood += log_partition_function;
+        }
+        else PLERROR("KLp0p1RBMModule: neg_log_likelihood currently computable only of the visible as inputs");
+    }
+    
+    // REGULAR FPROP
+    // we are given the visible units and we want to compute the hidden
+    // activation and/or the hidden expectation
+    if ( visible && !visible->isEmpty() &&
+         ((hidden && hidden->isEmpty() ) ||
+          (hidden_act && hidden_act->isEmpty())) )
+    {
+        computePositivePhaseHiddenActivations(*visible);
+        if (hidden) {
+            PLASSERT( hidden->isEmpty() );
+            PLCHECK_MSG( !hidden_layer->expectations_are_up_to_date, "Safety "
+                    "check: how were expectations computed previously?" );
+            hidden_layer->computeExpectations();
+            hidden_expectations_are_computed=true;
+            const Mat& hidden_out = hidden_layer->getExpectations();
+            hidden->resize(hidden_out.length(), hidden_out.width());
+            *hidden << hidden_out;
+        }
+        // Since we return below, the other ports must be unused.
+        //PLASSERT( !visible_sample && !hidden_sample );
+        found_a_valid_configuration = true;
+    } 
+
+    // compute KLp0p1 cost, given visible input
+    if (KLp0p1 && KLp0p1->isEmpty() && visible && !visible->isEmpty())
+    {
+        int mbs=visible->length();
+        KLp0p1->resize(mbs,1);
+        KLp0p1->clear();
+        PLASSERT_MSG(training_set,"KLp0p1RBMModule: training_set must be provided");
+        int n=training_set.length();
+        PLASSERT_MSG(n>0,"KLp0p1RBMModule: training_set must have n>0 rows");
+
+        // compute all P(hidden_i=1|x^k) for all x^k in training set
+        const Mat& ph=hidden_layer->getExpectations();
+        training_set->getMat(0,0,visible_layer->getExpectations());
+        hidden_layer->getAllActivations(connection,0,true);
+        hidden_layer->computeExpectations();
+
+        PLASSERT_MSG(hidden_layer->size<32,"To compute KLp0p1 of an RBM, hidden_layer->size must be <32");
+        PLASSERT(hidden_layer->classname()=="RBMBinomialLayer");
+        real logn=safelog(n);
+        // assuming a binary hidden we sum over all bit configurations
+        int n_configurations = 1 << hidden_layer->size; // = 2^{hidden_layer->size}
+        // put all h configurations in the hidden_layer->samples
+        conf_hidden_layer->setBatchSize(n_configurations);
+        conf_visible_layer->setBatchSize(n_configurations);
+        for (int c=0;c<n_configurations;c++)
+        {
+            // convert integer c into a bit-wise hidden representation
+            int N=c;
+            for (int i=0;i<hidden_layer->size;i++)
+            {
+                conf_hidden_layer->samples(c,i)= N & 1; // take least significant bit
+                N >>= 1; // and shift right (divide by 2)
+            }
+        }
+        // compute all P(visible_i=1|h) for each h configuration
+        visible_layer->getAllActivations(connection,0,true);
+        visible_layer->computeExpectations();
+
+        for (int c=0;c<n_configurations;c++)
+        {
+            //  C(x) = - log P1(x) = - log (1/n)sum_{k=1}^n sum_h P(x|h) P(h|x^k)
+            //                     = - log sum_h P(x|h) (sum_k P(h|x^k))/n
+            real log_sum_ph_given_xk = 0;
+            Vec h = conf_hidden_layer->samples(c);
+            for (int k=0;k<n;k++)
+            {
+                real lp=h[0]==1?safelog(ph(k,0)):safelog(1-ph(k,0));
+                for (int i=1;i<hidden_layer->size;i++)
+                {
+                    real p_hi_given_xk = h[i]==1?safelog(ph(k,i)):safelog(1-ph(k,i)); 
+                    lp = logadd(lp,p_hi_given_xk);
+                }
+                // now lp = log P(h|x^k)
+                if (k==0)
+                    log_sum_ph_given_xk = lp;
+                else
+                    log_sum_ph_given_xk = logadd(log_sum_ph_given_xk,lp);
+            }
+            log_sum_ph_given_xk -= logn;
+            // now log_sum_ph_given_xk = log (1/n) sum_k P(h|x^k)
+            for (int t=0;t<mbs;t++)
+                if (c==0)
+                    (*KLp0p1)(t,0) = conf_visible_layer->fpropNLL((*visible)(t)) + log_sum_ph_given_xk;
+                else
+                    (*KLp0p1)(t,0) = logadd((*KLp0p1)(t,0), conf_visible_layer->fpropNLL((*visible)(t)) + log_sum_ph_given_xk);
+        }
+        *KLp0p1 *= -1;
+    }
+
+    // SAMPLING
+    if ((visible_sample && visible_sample->isEmpty())               // is asked to sample visible units (discrete)
+        || (visible_expectation && visible_expectation->isEmpty())  //              "                   (continous)
+        || (hidden_sample && hidden_sample->isEmpty()))             // or to sample hidden units
+    {
+        if (hidden_sample && !hidden_sample->isEmpty()) // sample visible conditionally on hidden
+        {
+            sampleVisibleGivenHidden(*hidden_sample);
+            Gibbs_step=0;
+            //cout << "sampling visible from hidden" << endl;
+        }
+        else if (visible_sample && !visible_sample->isEmpty()) // if an input is provided, sample hidden conditionally
+        {
+            sampleHiddenGivenVisible(*visible_sample);
+            Gibbs_step=0;
+            //cout << "sampling hidden from (discrete) visible" << endl;
+        }
+        else if (visible && !visible->isEmpty()) // if an input is provided, sample hidden conditionally
+        {
+            visible_layer->generateSamples(); // WHY THIS LINE????
+            sampleHiddenGivenVisible(visible_layer->samples);
+            Gibbs_step=0;
+            //cout << "sampling hidden from visible expectation" << endl;
+        }
+        else if (visible_expectation && !visible_expectation->isEmpty()) 
+        {
+             PLERROR("In KLp0p1RBMModule::fprop visible_expectation can only be an output port (use visible as input port");
+        }
+        else // sample unconditionally: Gibbs sample after k steps
+        {
+            // the visible_layer->expectations contain the "state" from which we
+            // start or continue the chain
+            int min_n = max(Gibbs_step+n_Gibbs_steps_per_generated_sample,
+                            min_n_Gibbs_steps);
+            //cout << "Gibbs sampling " << Gibbs_step+1;
+            for (;Gibbs_step<min_n;Gibbs_step++)
+            {
+                sampleHiddenGivenVisible(visible_layer->samples);
+                sampleVisibleGivenHidden(hidden_layer->samples);
+            }
+              //cout << " -> " << Gibbs_step << endl;
+        }
+
+        if ( hidden && hidden->isEmpty())   // fill hidden.state with expectations
+        {
+              const Mat& hidden_expect = hidden_layer->getExpectations();
+              hidden->resize(hidden_expect.length(), hidden_expect.width());
+              *hidden << hidden_expect;
+        }
+        if (visible_sample && visible_sample->isEmpty()) // provide sample of the visible units
+        {
+            visible_sample->resize(visible_layer->samples.length(),
+                                   visible_layer->samples.width());
+            *visible_sample << visible_layer->samples;
+        }
+        if (hidden_sample && hidden_sample->isEmpty()) // provide sample of the hidden units
+        {
+            hidden_sample->resize(hidden_layer->samples.length(),
+                                  hidden_layer->samples.width());
+            *hidden_sample << hidden_layer->samples;
+        }
+        if (visible_expectation && visible_expectation->isEmpty()) // provide expectation of the visible units
+        {
+            const Mat& to_store = visible_layer->getExpectations();
+            visible_expectation->resize(to_store.length(),
+                                        to_store.width());
+            *visible_expectation << to_store;
+        }
+        found_a_valid_configuration = true;
+    }// END SAMPLING
+    
+    // COMPUTE CONTRASTIVE DIVERGENCE CRITERION
+    if (contrastive_divergence)
+    {
+        PLASSERT_MSG( contrastive_divergence->isEmpty(), 
+                      "KLp0p1RBMModule: the contrastive_divergence port can only be an output port\n" );
+        if (visible && !visible->isEmpty())
+        {
+            int mbs = visible->length();
+            const Mat& hidden_expectations = hidden_layer->getExpectations();
+            Mat* h=0;
+            Mat* h_act=0;
+            if (!hidden_activations_are_computed) // it must be because neither hidden nor hidden_act were asked
+            {
+                PLASSERT(!hidden_act);
+                computePositivePhaseHiddenActivations(*visible);
+                
+                // we need to save the hidden activations somewhere
+                hidden_act_store.resize(mbs,hidden_layer->size);
+                hidden_act_store << hidden_layer->activations;
+                h_act = &hidden_act_store;
+            } else 
+            {
+                // hidden_act must have been computed above if they were requested on port
+                PLASSERT(hidden_act && !hidden_act->isEmpty()); 
+                h_act = hidden_act;
+            }
+            if (!hidden_expectations_are_computed) // it must be because hidden outputs were not asked
+            {
+                PLASSERT(!hidden);
+                hidden_layer->computeExpectations();
+                hidden_expectations_are_computed=true;
+                // we need to save the hidden expectations somewhere
+                hidden_exp_store.resize(mbs,hidden_layer->size);
+                hidden_exp_store << hidden_expectations;
+                h = &hidden_exp_store;
+            } else
+            {
+                // hidden exp. must have been computed above if they were requested on port
+                PLASSERT(hidden && !hidden->isEmpty());
+                h = hidden;
+            }
+            // perform negative phase
+            for( int i=0; i<n_Gibbs_steps_CD; i++)
+            {
+                hidden_layer->generateSamples();
+                // (Negative phase) Generate visible samples.
+                sampleVisibleGivenHidden(hidden_layer->samples);
+                // compute corresponding hidden expectations.
+                computeHiddenActivations(visible_layer->samples);
+                hidden_layer->computeExpectations();
+            }
+            PLASSERT(negative_phase_visible_samples);
+            PLASSERT(negative_phase_hidden_expectations &&
+                     negative_phase_hidden_expectations->isEmpty());
+            PLASSERT(negative_phase_hidden_activations &&
+                     negative_phase_hidden_activations->isEmpty());
+            negative_phase_visible_samples->resize(mbs,visible_layer->size);
+            *negative_phase_visible_samples << visible_layer->samples;
+            negative_phase_hidden_expectations->resize(hidden_expectations.length(),
+                                                       hidden_expectations.width());
+            *negative_phase_hidden_expectations << hidden_expectations;
+            const Mat& neg_hidden_act = hidden_layer->activations;
+            negative_phase_hidden_activations->resize(neg_hidden_act.length(),
+                                                      neg_hidden_act.width());
+            *negative_phase_hidden_activations << neg_hidden_act;
+
+            // compute the energy (again for now only in the binomial case)
+            PLASSERT(hidden_layer->classname()=="RBMBinomialLayer");
+
+            // note that h_act and h may point to hidden_act_store and hidden_exp_store
+            PLASSERT(h_act && !h_act->isEmpty()); 
+            PLASSERT(h && !h->isEmpty());
+
+            contrastive_divergence->resize(hidden_expectations.length(),1);
+            // compute contrastive divergence itself
+            for (int i=0;i<mbs;i++)
+            {
+                (*contrastive_divergence)(i,0) = 
+                    // positive phase energy
+                    visible_layer->energy((*visible)(i))
+                    - dot((*h)(i),(*h_act)(i))
+                    // minus
+                    - 
+                    // negative phase energy
+                    (visible_layer->energy(visible_layer->samples(i))
+                     - dot(hidden_expectations(i),hidden_layer->activations(i)));
+            }
+        }
+        else
+            PLERROR("KLp0p1RBMModule: unknown configuration to compute contrastive_divergence (currently\n"
+                    "only possible if only visible are provided in input).\n");
+        found_a_valid_configuration = true;
+    }
+    
+
+    
+    
+    // COMPUTE AUTOASSOCIATOR RECONSTRUCTION ERROR
+    if ( visible && !visible->isEmpty() && 
+         ( ( visible_reconstruction && visible_reconstruction->isEmpty() ) || 
+           ( visible_reconstruction_activations && 
+             visible_reconstruction_activations->isEmpty() ) ||
+           ( reconstruction_error && reconstruction_error->isEmpty() ) ) ) 
+    {        
+        // Autoassociator reconstruction cost
+        PLASSERT( ports_value.length() == nPorts() );
+        computePositivePhaseHiddenActivations(*visible); 
+        if(!hidden_expectations_are_computed)
+        {
+            hidden_layer->computeExpectations();
+            hidden_expectations_are_computed=true;
+        }
+
+        // Don't need to verify if they are asked in a port, this was done previously
+        
+        computeVisibleActivations(hidden_layer->getExpectations(),true);
+        if(visible_reconstruction_activations) 
+        {
+            PLASSERT( visible_reconstruction_activations->isEmpty() );
+            const Mat& to_store = visible_layer->activations;
+            visible_reconstruction_activations->resize(to_store.length(), 
+                                                       to_store.width());
+            *visible_reconstruction_activations << to_store;
+        }
+        if (visible_reconstruction || reconstruction_error)
+        {        
+            visible_layer->computeExpectations();
+            if(visible_reconstruction)
+            {
+                PLASSERT( visible_reconstruction->isEmpty() );
+                const Mat& to_store = visible_layer->getExpectations();
+                visible_reconstruction->resize(to_store.length(), 
+                                                           to_store.width());
+                *visible_reconstruction << to_store;
+            }
+            if(reconstruction_error)
+            {
+                PLASSERT( reconstruction_error->isEmpty() );
+                reconstruction_error->resize(visible->length(),1);
+                visible_layer->fpropNLL(*visible,
+                                        *reconstruction_error);
+            }
+        }
+        found_a_valid_configuration = true;
+    }
+    // COMPUTE VISIBLE GIVEN HIDDEN
+    else if ( visible_reconstruction && visible_reconstruction->isEmpty() 
+         && hidden && !hidden->isEmpty())
+           
+    {        
+        // Don't need to verify if they are asked in a port, this was done previously
+        
+	computeVisibleActivations(*hidden,true);
+        if(visible_reconstruction_activations)
+        {
+            PLASSERT( visible_reconstruction_activations->isEmpty() );
+            const Mat& to_store = visible_layer->activations;
+            visible_reconstruction_activations->resize(to_store.length(), 
+                                                       to_store.width());
+            *visible_reconstruction_activations << to_store;
+        }      
+        visible_layer->computeExpectations();
+        PLASSERT( visible_reconstruction->isEmpty() );
+        const Mat& to_store = visible_layer->getExpectations();
+        visible_reconstruction->resize(to_store.length(), 
+                                       to_store.width());
+        *visible_reconstruction << to_store;
+        found_a_valid_configuration = true;
+    }
+
+    // Reset some class fields to ensure they are not reused by mistake.
+    hidden_act = NULL;
+    hidden_bias = NULL;
+    weights = NULL;
+    hidden_activations_are_computed = false;
+
+
+
+    if (!found_a_valid_configuration)
+    {
+        /*
+        if (visible)
+            cout << "visible_empty : "<< (bool) visible->isEmpty() << endl;
+        if (hidden)
+            cout << "hidden_empty : "<< (bool) hidden->isEmpty() << endl;
+        if (visible_sample)
+            cout << "visible_sample_empty : "<< (bool) visible_sample->isEmpty() << endl;
+        if (hidden_sample)
+            cout << "hidden_sample_empty : "<< (bool) hidden_sample->isEmpty() << endl;
+        if (visible_expectation)
+            cout << "visible_expectation_empty : "<< (bool) visible_expectation->isEmpty() << endl;
+
+        */
+        PLERROR("In KLp0p1RBMModule::fprop - Unknown port configuration for module %s", name.c_str());
+    }
+
+    checkProp(ports_value);
+
+}
+
+////////////////////
+// bpropAccUpdate //
+////////////////////
+void KLp0p1RBMModule::bpropAccUpdate(const TVec<Mat*>& ports_value,
+                               const TVec<Mat*>& ports_gradient)
+{
+    PLASSERT( ports_value.length() == nPorts() );
+    PLASSERT( ports_gradient.length() == nPorts() );
+    Mat* visible_grad = ports_gradient[getPortIndex("visible")];
+    Mat* hidden_grad = ports_gradient[getPortIndex("hidden.state")];
+    Mat* visible = ports_value[getPortIndex("visible")];
+    Mat* hidden = ports_value[getPortIndex("hidden.state")];
+    hidden_act = ports_value[getPortIndex("hidden_activations.state")];
+    Mat* reconstruction_error_grad = 0;
+    Mat* hidden_bias_grad = ports_gradient[getPortIndex("hidden_bias")];
+    weights = ports_value[getPortIndex("weights")]; 
+    Mat* weights_grad = ports_gradient[getPortIndex("weights")];    
+    hidden_bias = ports_value[getPortIndex("hidden_bias")];
+    Mat* contrastive_divergence_grad = NULL;
+    Mat* KLp0p1 = ports_value[getPortIndex("KLp0p1")];
+
+    // Ensure the gradient w.r.t. contrastive divergence is 1 (if provided).
+    if (compute_contrastive_divergence) {
+        contrastive_divergence_grad =
+            ports_gradient[getPortIndex("contrastive_divergence")];
+        if (contrastive_divergence_grad) {
+            PLASSERT( !contrastive_divergence_grad->isEmpty() );
+            PLASSERT( min(*contrastive_divergence_grad) >= 1 );
+            PLASSERT( max(*contrastive_divergence_grad) <= 1 );
+        }
+    }
+
+    if(reconstruction_connection)
+        reconstruction_error_grad = 
+            ports_gradient[getPortIndex("reconstruction_error.state")];
+
+    // Ensure the visible gradient is not provided as input. This is because we
+    // accumulate more than once in 'visible_grad'.
+    PLASSERT_MSG( !visible_grad || visible_grad->isEmpty(), "Cannot provide "
+            "an input gradient w.r.t. visible units" );
+
+    bool compute_visible_grad = visible_grad && visible_grad->isEmpty();
+    bool compute_weights_grad = weights_grad && weights_grad->isEmpty();
+    
+    int mbs = (visible && !visible->isEmpty()) ? visible->length() : -1;
+
+    if (hidden_grad && !hidden_grad->isEmpty())
+    {
+        // Note: the assert below is for behavior compatibility with previous
+        // code. It might not be necessary, or might need to be modified.
+        PLASSERT( visible && !visible->isEmpty() );
+
+        // Note: we need to perform the following steps even if the gradient
+        // learning rate is equal to 0. This is because we must propagate the
+        // gradient to the visible layer, even though no update is required.
+            setAllLearningRates(grad_learning_rate);
+            PLASSERT( hidden && hidden_act );
+            // Compute gradient w.r.t. activations of the hidden layer.
+            hidden_layer->bpropUpdate(
+                    *hidden_act, *hidden, hidden_act_grad, *hidden_grad,
+                    false);
+            if (hidden_bias_grad)
+            {
+                PLASSERT( hidden_bias_grad->isEmpty() &&
+                          hidden_bias_grad->width() == hidden_layer->size );
+                hidden_bias_grad->resize(mbs,hidden_layer->size);
+                *hidden_bias_grad += hidden_act_grad;
+            }
+            // Compute gradient w.r.t. expectations of the visible layer (=
+            // inputs).
+            Mat* store_visible_grad = NULL;
+            if (compute_visible_grad) {
+                PLASSERT( visible_grad->width() == visible_layer->size );
+                store_visible_grad = visible_grad;
+            } else {
+                // We do not actually need to store the gradient, but since it
+                // is required in bpropUpdate, we provide a dummy matrix to
+                // store it.
+                store_visible_grad = &visible_exp_grad;
+            }
+            store_visible_grad->resize(mbs,visible_layer->size);
+            
+            if (weights)
+            {
+                int up = connection->up_size;
+                int down = connection->down_size;
+                PLASSERT( !weights->isEmpty() &&
+                          weights_grad && weights_grad->isEmpty() &&
+                          weights_grad->width() == up * down );
+                weights_grad->resize(mbs, up * down);
+                Mat w, wg;
+                Vec v,h,vg,hg;
+                for(int i=0; i<mbs; i++)
+                {
+                    w = Mat(up, down,(*weights)(i));
+                    wg = Mat(up, down,(*weights_grad)(i));
+                    v = (*visible)(i);
+                    h = (*hidden_act)(i);
+                    vg = (*store_visible_grad)(i);
+                    hg = hidden_act_grad(i);
+                    connection->petiteCulotteOlivierUpdate(
+                        v,
+                        w,
+                        h,
+                        vg,
+                        wg,
+                        hg,true);
+                }
+            }
+            else
+            {
+                connection->bpropUpdate(
+                    *visible, *hidden_act, *store_visible_grad,
+                    hidden_act_grad, true);
+            }
+            partition_function_is_stale = true;
+    }
+
+    if (cd_learning_rate > 0 && minimize_log_likelihood) {
+        PLASSERT( visible && !visible->isEmpty() );
+        PLASSERT( hidden && !hidden->isEmpty() );
+        setAllLearningRates(cd_learning_rate);
+
+        // positive phase
+        visible_layer->accumulatePosStats(*visible);
+        hidden_layer->accumulatePosStats(*hidden);
+        connection->accumulatePosStats(*visible,*hidden);
+
+        // negative phase
+        PLASSERT_MSG(hidden_layer->size<32 || visible_layer->size<32,
+                     "To minimize exact log-likelihood of an RBM, hidden_layer->size "
+                     "or visible_layer->size must be <32");
+        // gradient of partition function
+        if (hidden_layer->size > visible_layer->size)
+            // do it by summing over visible configurations
+        {
+            PLASSERT(visible_layer->classname()=="RBMBinomialLayer");
+            // assuming a binary input we sum over all bit configurations
+            int n_configurations = 1 << visible_layer->size; // = 2^{visible_layer->size}
+            energy_inputs.resize(1, visible_layer->size);
+            Vec input = energy_inputs(0);
+            // COULD BE DONE MORE EFFICIENTLY BY DOING MANY CONFIGURATIONS
+            // AT ONCE IN A 'MINIBATCH'
+            for (int c=0;c<n_configurations;c++)
+            {
+                // convert integer c into a bit-wise visible representation
+                int x=c;
+                for (int i=0;i<visible_layer->size;i++)
+                {
+                    input[i]= x & 1; // take least significant bit
+                    x >>= 1; // and shift right (divide by 2)
+                }
+                connection->setAsDownInput(input);
+                hidden_layer->getAllActivations(connection,0,false);
+                hidden_layer->computeExpectation();
+                visible_layer->accumulateNegStats(input);
+                hidden_layer->accumulateNegStats(hidden_layer->expectation);
+                connection->accumulateNegStats(input,hidden_layer->expectation);
+            }
+        }
+        else
+        {
+            PLASSERT(hidden_layer->classname()=="RBMBinomialLayer");
+            // assuming a binary hidden we sum over all bit configurations
+            int n_configurations = 1 << hidden_layer->size; // = 2^{hidden_layer->size}
+            energy_inputs.resize(1, hidden_layer->size);
+            Vec h = energy_inputs(0);
+            for (int c=0;c<n_configurations;c++)
+            {
+                // convert integer c into a bit-wise hidden representation
+                int x=c;
+                for (int i=0;i<hidden_layer->size;i++)
+                {
+                    h[i]= x & 1; // take least significant bit
+                    x >>= 1; // and shift right (divide by 2)
+                }
+                connection->setAsUpInput(h);
+                visible_layer->getAllActivations(connection,0,false);
+                visible_layer->computeExpectation();
+                visible_layer->accumulateNegStats(visible_layer->expectation);
+                hidden_layer->accumulateNegStats(h);
+                connection->accumulateNegStats(visible_layer->expectation,h);
+            }
+        }
+        // update
+        visible_layer->update();
+        hidden_layer->update();
+        connection->update();
+    }
+    if (cd_learning_rate > 0 && !minimize_log_likelihood) {
+        EXTREME_MODULE_LOG << "Performing contrastive divergence step in RBM '"
+                           << name << "'" << endl;
+        // Perform a step of contrastive divergence.
+        PLASSERT( visible && !visible->isEmpty() );
+        setAllLearningRates(cd_learning_rate);
+        Mat* negative_phase_visible_samples = 
+            compute_contrastive_divergence?ports_value[getPortIndex("negative_phase_visible_samples.state")]:0;
+        const Mat* negative_phase_hidden_expectations =
+            compute_contrastive_divergence ?
+                ports_value[getPortIndex("negative_phase_hidden_expectations.state")]
+                : NULL;
+        Mat* negative_phase_hidden_activations =
+            compute_contrastive_divergence ?
+                ports_value[getPortIndex("negative_phase_hidden_activations.state")]
+                : NULL;
+        
+        PLASSERT( visible && hidden );
+        PLASSERT( !negative_phase_visible_samples ||
+                  !negative_phase_visible_samples->isEmpty() );
+        if (!negative_phase_visible_samples)
+        {
+            // Generate hidden samples.
+            hidden_layer->setExpectations(*hidden);
+            for( int i=0; i<n_Gibbs_steps_CD; i++)
+            {
+                hidden_layer->generateSamples();
+                // (Negative phase) Generate visible samples.
+                sampleVisibleGivenHidden(hidden_layer->samples);
+                // compute corresponding hidden expectations.
+                computeHiddenActivations(visible_layer->samples);
+                hidden_layer->computeExpectations();
+            }
+            PLASSERT( !compute_contrastive_divergence );
+            PLASSERT( !negative_phase_hidden_expectations );
+            PLASSERT( !negative_phase_hidden_activations );
+            negative_phase_hidden_expectations = &(hidden_layer->getExpectations());
+            negative_phase_visible_samples = &(visible_layer->samples);
+            negative_phase_hidden_activations = &(hidden_layer->activations);
+        }
+        PLASSERT( negative_phase_hidden_expectations &&
+                  !negative_phase_hidden_expectations->isEmpty() );
+        PLASSERT( negative_phase_hidden_activations &&
+                  !negative_phase_hidden_activations->isEmpty() );
+
+        // Perform update.
+        visible_layer->update(*visible, *negative_phase_visible_samples);
+
+        bool connection_update_is_done = false;
+        if (compute_weights_grad) {
+            // First resize the 'weights_grad' matrix.
+            int up = connection->up_size;
+            int down = connection->down_size;
+            PLASSERT( weights && !weights->isEmpty() &&
+                      weights_grad->width() == up * down );
+            weights_grad->resize(mbs, up * down);
+
+            if (standard_cd_weights_grad)
+            {
+                // Perform both computation of weights gradient and do update
+                // at the same time.
+                Mat wg;
+                Vec vp, hp, vn, hn;
+                for(int i=0; i<mbs; i++)
+                {
+                    vp = (*visible)(i);
+                    hp = (*hidden)(i);
+                    vn = (*negative_phase_visible_samples)(i);
+                    hn = (*negative_phase_hidden_expectations)(i);
+                    wg = Mat(up, down,(*weights_grad)(i));
+                    connection->petiteCulotteOlivierCD(
+                            vp, hp,
+                            vn,
+                            hn,
+                            wg,
+                            true);
+                    connection_update_is_done = true;
+                }
+            }
+        }
+        if (!standard_cd_weights_grad || !standard_cd_grad) {
+            // Compute 'true' gradient of contrastive divergence w.r.t.
+            // the weights matrix.
+            int up = connection->up_size;
+            int down = connection->down_size;
+            Mat* weights_g = weights_grad;
+            if (!weights_g) {
+                // We need to store the gradient in another matrix.
+                store_weights_grad.resize(mbs, up * down);
+                store_weights_grad.clear();
+                weights_g = & store_weights_grad;
+            }
+            PLASSERT( connection->classname() == "RBMMatrixConnection" &&
+                      visible_layer->classname() == "RBMBinomialLayer" &&
+                      hidden_layer->classname() == "RBMBinomialLayer" );
+
+            for (int k = 0; k < mbs; k++) {
+                int idx = 0;
+                for (int i = 0; i < up; i++) {
+                    real p_i_p = (*hidden)(k, i);
+                    real a_i_p = (*hidden_act)(k, i);
+                    real p_i_n =
+                        (*negative_phase_hidden_expectations)(k, i);
+                    real a_i_n =
+                        (*negative_phase_hidden_activations)(k, i);
+
+                    real scale_p = 1 + (1 - p_i_p) * a_i_p;
+                    real scale_n = 1 + (1 - p_i_n) * a_i_n;
+                    for (int j = 0; j < down; j++, idx++) {
+                        // Weight 'idx' is the (i,j)-th element in the
+                        // 'weights' matrix.
+                        real v_j_p = (*visible)(k, j);
+                        real v_j_n =
+                            (*negative_phase_visible_samples)(k, j);
+                        (*weights_g)(k, idx) +=
+                            p_i_n * v_j_n * scale_n     // Negative phase.
+                            -(p_i_p * v_j_p * scale_p); // Positive phase.
+                    }
+                }
+            }
+            if (!standard_cd_grad) {
+                // Update connection manually.
+                Mat& weights = ((RBMMatrixConnection*)
+                                get_pointer(connection))->weights;
+                real lr = cd_learning_rate / mbs;
+                for (int k = 0; k < mbs; k++) {
+                    int idx = 0;
+                    for (int i = 0; i < up; i++)
+                        for (int j = 0; j < down; j++, idx++)
+                            weights(i, j) -= lr * (*weights_g)(k, idx);
+                }
+                connection_update_is_done = true;
+            }
+        }
+        if (!connection_update_is_done)
+            // Perform standard update of the connection.
+            connection->update(*visible, *hidden,
+                    *negative_phase_visible_samples,
+                    *negative_phase_hidden_expectations);
+
+        Mat* hidden_bias_g = hidden_bias_grad;
+        if (!standard_cd_grad && !hidden_bias_grad) {
+            // We need to compute the CD gradient w.r.t. bias of hidden layer,
+            // but there is no bias coming from the outside. Thus we need
+            // another matrix to store this gradient.
+            store_hidden_bias_grad.resize(mbs, hidden_layer->size);
+            store_hidden_bias_grad.clear();
+            hidden_bias_g = & store_hidden_bias_grad;
+        }
+
+        if (hidden_bias_g)
+        {
+            if (hidden_bias_g->isEmpty()) {
+                PLASSERT(hidden_bias_g->width() == hidden_layer->size);
+                hidden_bias_g->resize(mbs,hidden_layer->size);
+            }
+            PLASSERT_MSG( hidden_layer->classname() == "RBMBinomialLayer" &&
+                          visible_layer->classname() == "RBMBinomialLayer",
+                          "Only implemented for binomial layers" );
+            // d(contrastive_divergence)/dhidden_bias
+            for (int k = 0; k < hidden_bias_g->length(); k++) {
+                for (int i = 0; i < hidden_bias_g->width(); i++) {
+                    real p_i_p = (*hidden)(k, i);
+                    real a_i_p = (*hidden_act)(k, i);
+                    real p_i_n = (*negative_phase_hidden_expectations)(k, i);
+                    real a_i_n = (*negative_phase_hidden_activations)(k, i);
+                    (*hidden_bias_g)(k, i) +=
+                        standard_cd_bias_grad ? p_i_n - p_i_p :
+                        p_i_n * (1 - p_i_n) * a_i_n + p_i_n     // Neg. phase
+                     -( p_i_p * (1 - p_i_p) * a_i_p + p_i_p );  // Pos. phase
+
+                }
+            }
+        }
+
+        if (standard_cd_grad) {
+            hidden_layer->update(*hidden, *negative_phase_hidden_expectations);
+        } else {
+            PLASSERT( hidden_layer->classname() == "RBMBinomialLayer" );
+            // Update hidden layer by hand.
+            Vec& bias = hidden_layer->bias;
+            real lr = cd_learning_rate / mbs;
+            for (int i = 0; i < mbs; i++)
+                bias -= lr * (*hidden_bias_g)(i);
+        }
+
+
+        partition_function_is_stale = true;
+    } else {
+        PLCHECK_MSG( !contrastive_divergence_grad ||
+                     (!hidden_bias_grad && !weights_grad),
+                "You currently cannot compute the "
+                "gradient of contrastive divergence w.r.t. external ports "
+                "when 'cd_learning_rate' is set to 0" );
+    }
+
+    if (reconstruction_error_grad && !reconstruction_error_grad->isEmpty()) {
+        setAllLearningRates(grad_learning_rate);
+        PLASSERT( reconstruction_connection != 0 );
+        // Perform gradient descent on Autoassociator reconstruction cost
+        Mat* visible_reconstruction = ports_value[getPortIndex("visible_reconstruction.state")];
+        Mat* visible_reconstruction_activations = ports_value[getPortIndex("visible_reconstruction_activations.state")];
+        Mat* reconstruction_error = ports_value[getPortIndex("reconstruction_error.state")];
+        PLASSERT( hidden != 0 );
+        PLASSERT( visible  && hidden_act &&
+                  visible_reconstruction && visible_reconstruction_activations &&
+                  reconstruction_error);
+        //int mbs = reconstruction_error_grad->length();
+
+        PLCHECK_MSG( !weights, "In KLp0p1RBMModule::bpropAccUpdate(): reconstruction cost "
+                     "for conditional weights is not implemented");
+
+        // Backprop reconstruction gradient
+
+        // Must change visible_layer's expectation
+        visible_layer->getExpectations() << *visible_reconstruction;
+        visible_layer->bpropNLL(*visible,*reconstruction_error,
+                                visible_act_grad);
+
+        // Combine with incoming gradient
+        PLASSERT( (*reconstruction_error_grad).width() == 1 );
+        for (int t=0;t<mbs;t++)
+            visible_act_grad(t) *= (*reconstruction_error_grad)(t,0);
+
+        // Visible bias update
+        columnSum(visible_act_grad,visible_bias_grad);
+        visible_layer->update(visible_bias_grad);
+
+        // Reconstruction connection update
+        reconstruction_connection->bpropUpdate(
+            *hidden, *visible_reconstruction_activations,
+            hidden_exp_grad, visible_act_grad, false);
+        
+        // Hidden layer bias update
+        hidden_layer->bpropUpdate(*hidden_act,
+                                  *hidden, hidden_act_grad,
+                                  hidden_exp_grad, false);
+        if (hidden_bias_grad)
+        {
+            if (hidden_bias_grad->isEmpty()) {
+                PLASSERT( hidden_bias_grad->width() == hidden_layer->size );
+                hidden_bias_grad->resize(mbs,hidden_layer->size);
+            }
+            *hidden_bias_grad += hidden_act_grad;
+        }
+        // Connection update
+        if(compute_visible_grad)
+        {
+            // The length of 'visible_grad' must be either 0 (if not computed
+            // previously) or the size of the mini-batches (otherwise).
+            PLASSERT( visible_grad->width() == visible_layer->size &&
+                      visible_grad->length() == 0 ||
+                      visible_grad->length() == mbs );
+            visible_grad->resize(mbs, visible_grad->width());
+            connection->bpropUpdate(
+                *visible, *hidden_act,
+                *visible_grad, hidden_act_grad, true);
+        }
+        else
+        {
+            visible_exp_grad.resize(mbs,visible_layer->size);        
+            connection->bpropUpdate(
+                *visible, *hidden_act,
+                visible_exp_grad, hidden_act_grad, true);
+        }
+        partition_function_is_stale = true;
+    }
+
+    // compute KLp0p1 cost, given visible input
+    if (klp0p1_learning_rate>0 && visible && !visible->isEmpty())
+    {
+        // WE ASSUME THAT THIS BPROP IS CALLED JUST AFTER THE CORRESPONDING FPROP!!!
+        // consequentely, we have
+        //   * P(h_i=1|x^k) for each x^k in the training set, in hidden_layer->expectations
+        //   * every h configuration in conf_hidden_layer->samples
+        //   * P(visible_j=1|h) for each h configuration, in conf_visible_layer->expectations
+        //   * x^t for every t in the input visible, in *visible
+        //   * -log P1(x^t) for each input visible(t) in KLp0p1(t,0)
+        //
+        // We want to compute
+        //   dC(x)/dWij = (1/(n P1(x))) 
+        //       sum_{k=1}^n sum_h P(x|h) P(h|x^k) (h_i(x_j - P(x_j|h)) + x_j^k(h_i - P(h_i|x^k)))
+        //
+        PLASSERT_MSG(KLp0p1 && !KLp0p1->isEmpty(), "Must compute KLp0p1 in order to compute its gradient, connect that port!");
+        int mbs=visible->length();
+        int n=training_set.length();
+        PLASSERT(connection->classname()=="RBMMatrixConnection");
+        PP<RBMMatrixConnection> matrix_connection = PP<RBMMatrixConnection>(connection);
+        Mat& W = matrix_connection->weights;
+        Vec& hidden_bias = hidden_layer->bias;
+        Vec& visible_bias = visible_layer->bias;
+        Vec pxtj_given_h(visible_layer->size);
+        Vec phi_given_xk(hidden_layer->size);
+        const Mat& ph_given_Xk=hidden_layer->getExpectations();
+        const Mat& pvisible_given_H=conf_visible_layer->getExpectations();
+        int n_configurations = 1 << hidden_layer->size; // = 2^{hidden_layer->size}
+        real logn=safelog(n);
+        for (int t=0;t<mbs;t++)
+        {
+            Vec xt = (*visible)(t);
+            for (int k=0;k<n;k++)
+            {
+                Vec ph_given_xk = ph_given_Xk(k);
+                Vec xk = (*visible)(k);
+                for (int c=0;c<n_configurations;c++)
+                {
+                    Vec h = conf_hidden_layer->samples(c);
+                    Vec pvisible_given_h=pvisible_given_H(c);
+                    real lp = (*KLp0p1)(t,0) - logn; // lp = log (1/(n P1(x^t)))
+                    // compute and multiply by P(h|x^k)
+                    for (int i=0;i<hidden_layer->size;i++)
+                        lp = logadd(lp,phi_given_xk[i]=(h[i]==1?safelog(ph_given_xk[i]):safelog(1-ph_given_xk[i]))); 
+                    // now lp = log ( (1/(n P1(x^t))) P(h|x^k) )
+
+                    // compute and multiply by P(x^t|h)
+                    for (int j=0;j<visible_layer->size;j++)
+                        lp = logadd(lp,pxtj_given_h[j]=(xt[j]*safelog(pvisible_given_h[j])+(1-xt[j])*safelog(1-pvisible_given_h[j])));
+                    // now lp = log ( (1/(n P1(x^t))) P(h|x^k)  P(x^t|h) )
+                    real coeff = exp(lp);
+                    for (int j=0;j<visible_layer->size;j++)
+                        visible_bias[j] -= klp0p1_learning_rate*coeff*(xt[j]-pxtj_given_h[j]);
+                    for (int i=0;i<hidden_layer->size;i++)
+                    {
+                        hidden_bias[i] -= klp0p1_learning_rate*coeff*(h[i]-phi_given_xk[i]);
+                        for (int j=0;j<visible_layer->size;j++)
+                            W(i,j) -= klp0p1_learning_rate*coeff*(h[i]*(xt[j]-pxtj_given_h[j])-xk[j]*(h[i]-phi_given_xk[i]));
+                    }
+                }
+            }
+        }
+    }
+
+    // Explicit error message in the case of the 'visible' port.
+    if (compute_visible_grad && visible_grad->isEmpty())
+        PLERROR("In KLp0p1RBMModule::bpropAccUpdate - The gradient with respect "
+                "to the 'visible' port was asked, but not computed");
+
+    checkProp(ports_gradient);
+
+    // Reset pointers to ensure we do not reuse them by mistake.
+    hidden_act = NULL;
+    weights = NULL;
+    hidden_bias = NULL;
+}
+
+////////////
+// forget //
+////////////
+void KLp0p1RBMModule::forget()
+{
+    DBG_MODULE_LOG << "Forgetting KLp0p1RBMModule '" << name << "'" << endl;
+    PLASSERT( hidden_layer && visible_layer && connection );
+    hidden_layer->forget();
+    visible_layer->forget();
+    connection->forget();
+    if (reconstruction_connection)
+        reconstruction_connection->forget();
+}
+
+//////////////////
+// getPortIndex //
+//////////////////
+int KLp0p1RBMModule::getPortIndex(const string& port)
+{
+    map<string, int>::const_iterator it = portname_to_index.find(port);
+    if (it == portname_to_index.end())
+        return -1;
+    else
+        return it->second;
+}
+
+//////////////
+// getPorts //
+//////////////
+const TVec<string>& KLp0p1RBMModule::getPorts()
+{
+    return ports;
+}
+
+///////////////////
+// getPortsSizes //
+///////////////////
+const TMat<int>& KLp0p1RBMModule::getPortSizes()
+{
+    return port_sizes;
+}
+
+//////////////////////
+// bpropDoesNothing //
+//////////////////////
+/* THIS METHOD IS OPTIONAL
+bool KLp0p1RBMModule::bpropDoesNothing()
+{
+}
+*/
+
+/////////////////////////
+// setAllLearningRates //
+/////////////////////////
+void KLp0p1RBMModule::setAllLearningRates(real lr)
+{
+    hidden_layer->setLearningRate(lr);
+    visible_layer->setLearningRate(lr);
+    connection->setLearningRate(lr);
+    if(reconstruction_connection)
+        reconstruction_connection->setLearningRate(lr);
+}
+
+//////////////////////////////
+// sampleHiddenGivenVisible //
+//////////////////////////////
+void KLp0p1RBMModule::sampleHiddenGivenVisible(const Mat& visible)
+{
+    computeHiddenActivations(visible);
+    hidden_layer->computeExpectations();
+    hidden_layer->generateSamples();
+}
+
+//////////////////////////////
+// sampleVisibleGivenHidden //
+//////////////////////////////
+void KLp0p1RBMModule::sampleVisibleGivenHidden(const Mat& hidden)
+{
+    computeVisibleActivations(hidden);
+    visible_layer->computeExpectations();
+    visible_layer->generateSamples();
+}
+
+/////////////////////
+// setLearningRate //
+/////////////////////
+void KLp0p1RBMModule::setLearningRate(real dynamic_learning_rate)
+{
+    // Out of safety, force the user to go through the two different learning
+    // rate. May need to be removed if it causes unwanted crashes.
+    PLERROR("In KLp0p1RBMModule::setLearningRate - Do not use this method, instead "
+            "explicitely use 'cd_learning_rate' and 'grad_learning_rate'");
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.h
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.h	2007-07-05 20:56:06 UTC (rev 7699)
+++ trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.h	2007-07-05 21:18:19 UTC (rev 7700)
@@ -0,0 +1,355 @@
+// -*- C++ -*-
+
+// KLp0p1RBMModule.h
+//
+// Copyright (C) 2007 Olivier Delalleau, Yoshua Bengio
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file KLp0p1RBMModule.h */
+
+
+#ifndef KLp0p1RBMModule_INC
+#define KLp0p1RBMModule_INC
+
+#include <map>
+#include <plearn_learners/online/OnlineLearningModule.h>
+#include <plearn_learners/online/RBMConnection.h>
+#include <plearn_learners/online/RBMLayer.h>
+#include <plearn/vmat/VMat.h>
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class KLp0p1RBMModule : public OnlineLearningModule
+{
+    typedef OnlineLearningModule inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    VMat training_set;
+
+    PP<RBMLayer> hidden_layer;
+    PP<RBMLayer> visible_layer;
+    PP<RBMConnection> connection;
+    PP<RBMConnection> reconstruction_connection;
+
+    real cd_learning_rate;
+    real grad_learning_rate;
+    real klp0p1_learning_rate;
+
+    bool compute_contrastive_divergence;
+
+    //! Number of Gibbs sampling steps in negative phase 
+    //! of contrastive divergence.
+    int n_Gibbs_steps_CD;
+
+    //! used to generate samples from the RBM
+    int min_n_Gibbs_steps; 
+    int n_Gibbs_steps_per_generated_sample;
+
+    bool compute_log_likelihood;
+    bool minimize_log_likelihood;
+
+    //#####  Public Learnt Options  ############################################
+    //! used to generate samples from the RBM
+    int Gibbs_step;
+    real log_partition_function;
+    bool partition_function_is_stale;
+
+    bool standard_cd_grad;
+    bool standard_cd_bias_grad;
+    bool standard_cd_weights_grad;
+    
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    KLp0p1RBMModule();
+
+    // Your other public member functions go here
+
+    //! given the input, compute the output (possibly resize it appropriately)
+    virtual void fprop(const Vec& input, Vec& output) const;
+
+    /* Optional
+       THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
+    //! Adapt based on the output gradient, and obtain the input gradient.
+    //! The flag indicates wether the input_gradient is accumulated or set.
+    //! This method should only be called just after a corresponding
+    //! fprop; it should be called with the same arguments as fprop
+    //! for the first two arguments (and output should not have been
+    //! modified since then).
+    //! Since sub-classes are supposed to learn ONLINE, the object
+    //! is 'ready-to-be-used' just after any bpropUpdate.
+    virtual void bpropUpdate(const Vec& input, const Vec& output,
+                             Vec& input_gradient,
+                             const Vec& output_gradient,
+                             bool accumulate=false);
+    */
+
+    /* Optional
+       A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+       JUST CALLS
+            bpropUpdate(input, output, input_gradient, output_gradient)
+       AND IGNORES INPUT GRADIENT.
+    //! This version does not obtain the input gradient.
+    virtual void bpropUpdate(const Vec& input, const Vec& output,
+                             const Vec& output_gradient);
+    */
+
+    /* Optional
+       N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+       RAISES A PLERROR.
+    //! Similar to bpropUpdate, but adapt based also on the estimation
+    //! of the diagonal of the Hessian matrix, and propagates this
+    //! back. If these methods are defined, you can use them INSTEAD of
+    //! bpropUpdate(...)
+    virtual void bbpropUpdate(const Vec& input, const Vec& output,
+                              Vec& input_gradient,
+                              const Vec& output_gradient,
+                              Vec& input_diag_hessian,
+                              const Vec& output_diag_hessian,
+                              bool accumulate=false);
+    */
+
+    /* Optional
+       N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS,
+       WHICH JUST CALLS
+            bbpropUpdate(input, output, input_gradient, output_gradient,
+                         out_hess, in_hess)
+       AND IGNORES INPUT HESSIAN AND INPUT GRADIENT.
+    //! This version does not obtain the input gradient and diag_hessian.
+    virtual void bbpropUpdate(const Vec& input, const Vec& output,
+                              const Vec& output_gradient,
+                              const Vec& output_diag_hessian);
+    */
+
+
+    //! Reset the parameters to the state they would be BEFORE starting
+    //! training.  Note that this method is necessarily called from
+    //! build().
+    virtual void forget();
+
+
+    /* Optional
+       THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS DOES NOT
+       DO ANYTHING.
+    //! Perform some processing after training, or after a series of
+    //! fprop/bpropUpdate calls to prepare the model for truly out-of-sample
+    //! operation.
+    virtual void finalize();
+    */
+
+    /* Optional
+       THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS RETURNS false
+    //! In case bpropUpdate does not do anything, make it known
+    virtual bool bpropDoesNothing();
+    */
+
+    //! Throws an error (please use explicitely the two different kinds of
+    //! learning rates available here).
+    virtual void setLearningRate(real dynamic_learning_rate);
+
+    //! Overridden.
+    virtual void fprop(const TVec<Mat*>& ports_value);
+
+    //! Overridden.
+    virtual void bpropAccUpdate(const TVec<Mat*>& ports_value,
+                                const TVec<Mat*>& ports_gradient);
+
+    //! Returns all ports in a KLp0p1RBMModule.
+    virtual const TVec<string>& getPorts();
+
+    //! The ports' sizes are given by the corresponding RBM layers.
+    virtual const TMat<int>& getPortSizes();
+
+    //! Return the index (as in the list of ports returned by getPorts()) of
+    //! a given port.
+    //! If 'port' does not exist, -1 is returned.
+    virtual int getPortIndex(const string& port);
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(KLp0p1RBMModule);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+
+protected:
+
+    PP<RBMLayer> conf_hidden_layer;
+    PP<RBMLayer> conf_visible_layer;
+
+    Mat* hidden_bias;
+    Mat* weights;
+
+    //! Used to store gradient w.r.t. expectations of the hidden layer.
+    Mat hidden_exp_grad;
+
+    //! Used to store gradient w.r.t. activations of the hidden layer.
+    Mat hidden_act_grad;
+
+    //! Used to store gradient w.r.t. expectations of the visible layer.
+    Mat visible_exp_grad;
+
+    //! Used to store gradient w.r.t. activations of the visible layer.
+    Mat visible_act_grad;
+
+    //! Used to store gradient w.r.t. bias of visible layer
+    Vec visible_bias_grad;
+
+    //! Used to cache the hidden layer expectations and activations
+    Mat hidden_exp_store;
+    Mat hidden_act_store;
+    Mat* hidden_act;
+    bool hidden_activations_are_computed;    
+
+    //! Used to store the contrastive divergence gradient w.r.t. weights.
+    Mat store_weights_grad;
+
+    //! Used to store the contrastive divergence gradient w.r.t. hidden bias.
+    Mat store_hidden_bias_grad;
+
+    //! List of port names.
+    TVec<string> ports;
+
+    //! Map from a port name to its index in the 'ports' vector.
+    map<string, int> portname_to_index;
+
+    //! Used to store inputs generated to compute the free energy.
+    Mat energy_inputs;
+
+    //#####  Protected Member Functions  ######################################
+
+    //! Add a new port to the 'portname_to_index' map and 'ports' vector.
+    void addPortName(const string& name);
+
+    //! Forward the given learning rate to all elements of this module.
+    void setAllLearningRates(real lr);
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+    //! Compute activations on the hidden layer based on the provided
+    //! visible input.
+    //! If 'hidden_bias' is not null nor empty, then it is used as an
+    //! additional bias for hidden activations.
+    void computeHiddenActivations(const Mat& visible);
+
+    //! Compute activations on the visible layer.
+    //! If 'using_reconstruction_connection' is true, then we use the
+    //! reconstruction connection to compute these activations. Otherwise, we
+    //! use the normal connection, in a 'top->down' fashion.
+    void computeVisibleActivations(const Mat& hidden,
+                                   bool using_reconstruction_connection=false);
+
+    //! Compute activations on the hidden layer based on the provided visible
+    //! input during positive phase. This method is called to ensure hidden
+    //! hidden activations are computed only once, and during a fprop it should
+    //! always be called with the same 'visible' input.
+    //! If 'hidden_act' is not null, it is filled with the computed hidden
+    //! activations.
+    void computePositivePhaseHiddenActivations(const Mat& visible);
+
+    //! Sample hidden layer data based on the provided 'visible' inputs.
+    void sampleHiddenGivenVisible(const Mat& visible);
+
+    //! Sample visible layer data based on the provided 'hidden' inputs.
+    void sampleVisibleGivenHidden(const Mat& hidden);
+
+    //! Compute free energy on the visible layer and store it in the 'energy'
+    //! matrix.
+    //! The 'positive_phase' boolean is used to save computations when we know
+    //! we are in the positive phase of fprop.
+    void computeFreeEnergyOfVisible(const Mat& visible, Mat& energy,
+                                    bool positive_phase = true);
+
+    //! Compute free energy on the hidden layer and store it in the 'energy'
+    //! matrix.
+    void computeFreeEnergyOfHidden(const Mat& hidden, Mat& energy);
+
+    //! Compute energy of the joint (visible, hidden) configuration and store
+    //! it in the 'energy' matrix.
+    //! The 'positive_phase' boolean is used to save computations when we know
+    //! we are in the positive phase of fprop.
+    void computeEnergy(const Mat& visible, const Mat& hidden, Mat& energy, 
+                       bool positive_phase = true);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(KLp0p1RBMModule);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From lamblin at mail.berlios.de  Fri Jul  6 02:23:21 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 6 Jul 2007 02:23:21 +0200
Subject: [Plearn-commits] r7701 -
	trunk/python_modules/plearn/learners/modulelearners/sampler/example/data
Message-ID: <200707060023.l660NLPQ029056@sheep.berlios.de>

Author: lamblin
Date: 2007-07-06 02:21:21 +0200 (Fri, 06 Jul 2007)
New Revision: 7701

Modified:
   trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/DBN-3RBM.babyAI-1obj.psave
Log:
Changed saved model so that it can be used with newer version of RBMs


Modified: trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/DBN-3RBM.babyAI-1obj.psave
===================================================================
(Binary files differ)



From lamblin at mail.berlios.de  Fri Jul  6 03:43:51 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 6 Jul 2007 03:43:51 +0200
Subject: [Plearn-commits] r7702 - trunk/plearn/base
Message-ID: <200707060143.l661hpfB000843@sheep.berlios.de>

Author: lamblin
Date: 2007-07-06 03:43:50 +0200 (Fri, 06 Jul 2007)
New Revision: 7702

Modified:
   trunk/plearn/base/Object.cc
Log:
Declare "classname()" method, so we can dynamically test object types


Modified: trunk/plearn/base/Object.cc
===================================================================
--- trunk/plearn/base/Object.cc	2007-07-06 00:21:21 UTC (rev 7701)
+++ trunk/plearn/base/Object.cc	2007-07-06 01:43:50 UTC (rev 7702)
@@ -740,7 +740,11 @@
     declareMethod(rmm, "build", &Object::build,
                   (BodyDoc("Build newly created object (after setting options).\n")));
 
+    declareMethod(rmm, "classname", &Object::classname,
+                  (BodyDoc("Returns the name of the class"),
+                   RetDoc ("Class name as string")));
 
+
 #ifdef PL_PYTHON_VERSION 
     declareMethod(rmm, "setOptionFromPython", &Object::setOptionFromPython,
                   (BodyDoc("Change an option within the object from a PythonObjectWrapper"),



From yoshua at mail.berlios.de  Fri Jul  6 21:34:11 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Fri, 6 Jul 2007 21:34:11 +0200
Subject: [Plearn-commits] r7703 - trunk/plearn_learners/online
Message-ID: <200707061934.l66JYBjc024259@sheep.berlios.de>

Author: yoshua
Date: 2007-07-06 21:34:10 +0200 (Fri, 06 Jul 2007)
New Revision: 7703

Modified:
   trunk/plearn_learners/online/RBMLayer.h
   trunk/plearn_learners/online/RBMModule.cc
Log:
Make the behavior of n_Gibbs_steps_per_generated sample
as claimed in the documentation!


Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2007-07-06 01:43:50 UTC (rev 7702)
+++ trunk/plearn_learners/online/RBMLayer.h	2007-07-06 19:34:10 UTC (rev 7703)
@@ -276,14 +276,14 @@
     //! Transforms a shallow copy into a deep copy
     virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
-protected:
 
     //#####  Not Options  #####################################################
-
+public:
     //! Accumulates positive contribution to the gradient of bias
     Vec bias_pos_stats;
     //! Accumulates negative contribution to the gradient of bias
     Vec bias_neg_stats;
+protected:
     //! Stores the momentum of the gradient
     Vec bias_inc;
 

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-07-06 01:43:50 UTC (rev 7702)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-07-06 19:34:10 UTC (rev 7703)
@@ -97,7 +97,7 @@
     compute_contrastive_divergence(false),
     n_Gibbs_steps_CD(1),
     min_n_Gibbs_steps(1),
-    n_Gibbs_steps_per_generated_sample(1),
+    n_Gibbs_steps_per_generated_sample(-1),
     compute_log_likelihood(false),
     minimize_log_likelihood(false),
     Gibbs_step(0),
@@ -344,6 +344,10 @@
     PLCHECK_MSG(!(!standard_cd_grad && standard_cd_bias_grad), "You cannot "
             "compute the standard CD gradient w.r.t. external hidden bias and "
             "use the 'true' CD gradient w.r.t. internal hidden bias");
+
+    if (n_Gibbs_steps_per_generated_sample<0)
+        n_Gibbs_steps_per_generated_sample = min_n_Gibbs_steps;
+
 }
 
 ///////////
@@ -817,7 +821,6 @@
         }
         else if (visible && !visible->isEmpty()) // if an input is provided, sample hidden conditionally
         {
-            visible_layer->generateSamples();
             sampleHiddenGivenVisible(visible_layer->samples);
             Gibbs_step=0;
             //cout << "sampling hidden from visible expectation" << endl;



From yoshua at mail.berlios.de  Fri Jul  6 21:35:26 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Fri, 6 Jul 2007 21:35:26 +0200
Subject: [Plearn-commits] r7704 - in trunk/plearn_learners: classifiers
	generic
Message-ID: <200707061935.l66JZQJS024425@sheep.berlios.de>

Author: yoshua
Date: 2007-07-06 21:35:25 +0200 (Fri, 06 Jul 2007)
New Revision: 7704

Modified:
   trunk/plearn_learners/classifiers/KNNClassifier.cc
   trunk/plearn_learners/generic/PLearner.cc
   trunk/plearn_learners/generic/PLearner.h
Log:
Added random_gen as an option of PLearner


Modified: trunk/plearn_learners/classifiers/KNNClassifier.cc
===================================================================
--- trunk/plearn_learners/classifiers/KNNClassifier.cc	2007-07-06 19:34:10 UTC (rev 7703)
+++ trunk/plearn_learners/classifiers/KNNClassifier.cc	2007-07-06 19:35:25 UTC (rev 7704)
@@ -95,7 +95,7 @@
     );
 
 KNNClassifier::KNNClassifier()
-    : knn(new ExhaustiveNearestNeighbors(new GaussianKernel(), false)),
+    : 
       nclasses(-1),
       kmin(5),
       kmult(0.0),
@@ -149,7 +149,7 @@
 void KNNClassifier::build_()
 {
     if (!knn)
-        PLERROR("KNNClassifier::build_: the 'knn' option must be specified");
+        knn=new ExhaustiveNearestNeighbors(new GaussianKernel(), false);
 
     if (nclasses <= 1)
         PLERROR("KNNClassifier::build_: the 'nclasses' option must be specified and >= 2");

Modified: trunk/plearn_learners/generic/PLearner.cc
===================================================================
--- trunk/plearn_learners/generic/PLearner.cc	2007-07-06 19:34:10 UTC (rev 7703)
+++ trunk/plearn_learners/generic/PLearner.cc	2007-07-06 19:35:25 UTC (rev 7704)
@@ -139,6 +139,10 @@
         "at the learner's. \n");
 
     declareOption(
+        ol, "random_gen", &PLearner::random_gen, OptionBase::learntoption, 
+        "The random number generator used in this learner. Constructed from the seed.\n");
+
+    declareOption(
         ol, "seed", &PLearner::seed_, OptionBase::buildoption, 
         "The initial seed for the random number generator used in this\n"
         "learner, for instance for parameter initialization.\n"

Modified: trunk/plearn_learners/generic/PLearner.h
===================================================================
--- trunk/plearn_learners/generic/PLearner.h	2007-07-06 19:34:10 UTC (rev 7703)
+++ trunk/plearn_learners/generic/PLearner.h	2007-07-06 19:35:25 UTC (rev 7704)
@@ -114,7 +114,7 @@
      *  learner (see forget() method).  Default value=1827 for experiment
      *  reproducibility.
      */
-    long seed_;
+    int seed_;
 
     /**
      *  The current training stage, since last fresh initialization (forget()):



From yoshua at mail.berlios.de  Fri Jul  6 21:36:31 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Fri, 6 Jul 2007 21:36:31 +0200
Subject: [Plearn-commits] r7705 - in trunk/python_modules/plearn/learners: .
	autolr
Message-ID: <200707061936.l66JaVdh024570@sheep.berlios.de>

Author: yoshua
Date: 2007-07-06 21:36:31 +0200 (Fri, 06 Jul 2007)
New Revision: 7705

Modified:
   trunk/python_modules/plearn/learners/__init__.py
   trunk/python_modules/plearn/learners/autolr/__init__.py
Log:
Added function test(learner,testset) in learners


Modified: trunk/python_modules/plearn/learners/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/__init__.py	2007-07-06 19:35:25 UTC (rev 7704)
+++ trunk/python_modules/plearn/learners/__init__.py	2007-07-06 19:36:31 UTC (rev 7705)
@@ -0,0 +1,8 @@
+
+import plearn.bridgemode
+from plearn.bridge import *
+
+def test(learner,testset):
+    ts = pl.VecStatsCollector()
+    learner.test(testset,ts,0,0)
+    return [ts.getStat("E["+str(i)+"]") for i in range(0,ts.length())]

Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-07-06 19:35:25 UTC (rev 7704)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-07-06 19:36:31 UTC (rev 7705)
@@ -10,7 +10,7 @@
 if plearn.bridgemode.useserver:
     servers_lock = Lock() # Lock on the servers list so we don't have race conditions
     servers = [[serv, 0, Lock()]] # List of [server, amount_of_jobs_server_is_running, lock_on_the_server] lists
-    servers_max = 1e10 # Maximal amount of servers we are willing to run
+    servers_max = 1e10 # Default maximal number of servers we are willing to run
 
 def execute(object, tasks, use_threads = False):
     def job(object):



From yoshua at mail.berlios.de  Fri Jul  6 21:40:13 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Fri, 6 Jul 2007 21:40:13 +0200
Subject: [Plearn-commits] r7706 - trunk/plearn/math
Message-ID: <200707061940.l66JeDl5024936@sheep.berlios.de>

Author: yoshua
Date: 2007-07-06 21:40:12 +0200 (Fri, 06 Jul 2007)
New Revision: 7706

Modified:
   trunk/plearn/math/VecStatsCollector.cc
Log:
Added declared method length to VecStatsCollector


Modified: trunk/plearn/math/VecStatsCollector.cc
===================================================================
--- trunk/plearn/math/VecStatsCollector.cc	2007-07-06 19:36:31 UTC (rev 7705)
+++ trunk/plearn/math/VecStatsCollector.cc	2007-07-06 19:40:12 UTC (rev 7706)
@@ -213,8 +213,13 @@
                  "A vector of strings corresponding to the names of each field"
                  " in the VecStatsCollector.\n")));
 
-}
+   declareMethod(
+        rmm, "length", &VecStatsCollector::length,
+        (BodyDoc("Returns the number of statistics collected.\n"),
+         RetDoc ("=stats.length()")));
 
+ }
+
 int VecStatsCollector::length() const
 {
     return stats.length();



From yoshua at mail.berlios.de  Fri Jul  6 22:07:16 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Fri, 6 Jul 2007 22:07:16 +0200
Subject: [Plearn-commits] r7707 - trunk/python_modules/plearn/learners/online
Message-ID: <200707062007.l66K7GsS027147@sheep.berlios.de>

Author: yoshua
Date: 2007-07-06 22:07:16 +0200 (Fri, 06 Jul 2007)
New Revision: 7707

Modified:
   trunk/python_modules/plearn/learners/online/__init__.py
Log:
Update to be compatible with P.L.'s recent changes in RBMs


Modified: trunk/python_modules/plearn/learners/online/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/online/__init__.py	2007-07-06 19:40:12 UTC (rev 7706)
+++ trunk/python_modules/plearn/learners/online/__init__.py	2007-07-06 20:07:16 UTC (rev 7707)
@@ -82,7 +82,7 @@
             hidden_layer = pl.RBMBinomialLayer(size = hidden_size),
             connection = conx,
             reconstruction_connection = ifthenelse(have_reconstruction,
-                                                   pl.RBMMatrixTransposeConnection(rbm_matrix_connection = conx),
+                                                   conx,
                                                    None))
 
 



From louradou at mail.berlios.de  Fri Jul  6 22:25:53 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Fri, 6 Jul 2007 22:25:53 +0200
Subject: [Plearn-commits] r7708 - trunk/plearn_learners/online
Message-ID: <200707062025.l66KPrA4028504@sheep.berlios.de>

Author: louradou
Date: 2007-07-06 22:25:53 +0200 (Fri, 06 Jul 2007)
New Revision: 7708

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
only a detail concerning the sampling from visible units with RBM



Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-07-06 20:07:16 UTC (rev 7707)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-07-06 20:25:53 UTC (rev 7708)
@@ -817,13 +817,13 @@
         {
             sampleHiddenGivenVisible(*visible_sample);
             Gibbs_step=0;
-            //cout << "sampling hidden from (discrete) visible" << endl;
+            //cout << "sampling hidden from visible" << endl;
         }
         else if (visible && !visible->isEmpty()) // if an input is provided, sample hidden conditionally
         {
-            sampleHiddenGivenVisible(visible_layer->samples);
+	    sampleHiddenGivenVisible(*visible);
             Gibbs_step=0;
-            //cout << "sampling hidden from visible expectation" << endl;
+            //cout << "sampling hidden from visible" << endl;
         }
         else if (visible_expectation && !visible_expectation->isEmpty())
         {



From saintmlx at mail.berlios.de  Fri Jul  6 22:29:15 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Fri, 6 Jul 2007 22:29:15 +0200
Subject: [Plearn-commits] r7709 - trunk/plearn/python
Message-ID: <200707062029.l66KTFVg028977@sheep.berlios.de>

Author: saintmlx
Date: 2007-07-06 22:29:15 +0200 (Fri, 06 Jul 2007)
New Revision: 7709

Modified:
   trunk/plearn/python/PythonCodeSnippet.cc
Log:
- fixed mem. leak (objects returned to c++ from python)


Modified: trunk/plearn/python/PythonCodeSnippet.cc
===================================================================
--- trunk/plearn/python/PythonCodeSnippet.cc	2007-07-06 20:25:53 UTC (rev 7708)
+++ trunk/plearn/python/PythonCodeSnippet.cc	2007-07-06 20:29:15 UTC (rev 7709)
@@ -322,7 +322,10 @@
     }
 
     if(instance_method) Py_DECREF(pFunc);
-    return PythonObjectWrapper(return_value);
+    //return PythonObjectWrapper(return_value);
+    PythonObjectWrapper r(return_value);
+    Py_DECREF(return_value);
+    return r;
 }
 
 
@@ -388,7 +391,10 @@
 
     if(instance_method) 
         Py_DECREF(pFunc);
-    return PythonObjectWrapper(return_value);
+    //return PythonObjectWrapper(return_value);
+    PythonObjectWrapper r(return_value);
+    Py_DECREF(return_value);
+    return r;
 }
 
 



From yoshua at mail.berlios.de  Fri Jul  6 23:02:16 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Fri, 6 Jul 2007 23:02:16 +0200
Subject: [Plearn-commits] r7710 -
	trunk/plearn_learners/online/test/DeepBeliefNet
Message-ID: <200707062102.l66L2GNL031491@sheep.berlios.de>

Author: yoshua
Date: 2007-07-06 23:02:15 +0200 (Fri, 06 Jul 2007)
New Revision: 7710

Modified:
   trunk/plearn_learners/online/test/DeepBeliefNet/pytest.config
Log:
Disable DBN_SimpleRBM until we  understand why it now
yields different weights and the same NLL.


Modified: trunk/plearn_learners/online/test/DeepBeliefNet/pytest.config
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/pytest.config	2007-07-06 20:29:15 UTC (rev 7709)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/pytest.config	2007-07-06 21:02:15 UTC (rev 7710)
@@ -119,5 +119,5 @@
     resources = [ "SimpleRBM_test.pyplearn" ],
     precision = 1e-06,
     pfileprg = "__program__",
-    disabled = False
+    disabled = True
     )



From lamblin at mail.berlios.de  Fri Jul  6 23:56:01 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 6 Jul 2007 23:56:01 +0200
Subject: [Plearn-commits] r7711 - trunk/plearn/base
Message-ID: <200707062156.l66Lu1mP001554@sheep.berlios.de>

Author: lamblin
Date: 2007-07-06 23:56:01 +0200 (Fri, 06 Jul 2007)
New Revision: 7711

Modified:
   trunk/plearn/base/Object.cc
   trunk/plearn/base/Object.h
Log:
New exported global method: Object* deepCopy(Object*)


Modified: trunk/plearn/base/Object.cc
===================================================================
--- trunk/plearn/base/Object.cc	2007-07-06 21:02:15 UTC (rev 7710)
+++ trunk/plearn/base/Object.cc	2007-07-06 21:56:01 UTC (rev 7711)
@@ -1000,6 +1000,12 @@
     return TypeFactory::instance().newObject(classname);
 }
 
+Object* remote_deepCopy(Object* source)
+{
+    CopiesMap copies;
+    return source->deepCopy(copies);
+}
+
 BEGIN_DECLARE_REMOTE_FUNCTIONS
 
     declareFunction("newObject", &newObject,
@@ -1020,6 +1026,12 @@
                             "file containing the object to load"),
                      RetDoc ("newly created object")));
 
+    declareFunction("deepCopy", &remote_deepCopy,
+                    (BodyDoc("Returns deep copy of a PLearn object.\n"),
+                     ArgDoc ("source", "object to be deep-copied"),
+                     RetDoc ("deep copy of the object")));
+
+
 END_DECLARE_REMOTE_FUNCTIONS
 
 

Modified: trunk/plearn/base/Object.h
===================================================================
--- trunk/plearn/base/Object.h	2007-07-06 21:02:15 UTC (rev 7710)
+++ trunk/plearn/base/Object.h	2007-07-06 21:56:01 UTC (rev 7711)
@@ -1095,6 +1095,7 @@
 
 Object* newObjectFromClassname(const string& classname);
 
+Object* remote_deepCopy(Object* source);
 
 inline PStream &operator>>(PStream &in, Object &o)
 {



From lamblin at mail.berlios.de  Sat Jul  7 00:11:18 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Sat, 7 Jul 2007 00:11:18 +0200
Subject: [Plearn-commits] r7712 - trunk/plearn/io
Message-ID: <200707062211.l66MBIXm002688@sheep.berlios.de>

Author: lamblin
Date: 2007-07-07 00:11:17 +0200 (Sat, 07 Jul 2007)
New Revision: 7712

Modified:
   trunk/plearn/io/PStream.cc
   trunk/plearn/io/PStream.h
Log:
Do not use "long" with binary read and write anymore, since its size
depends on the platform. Replace "long long" by int64_t.


Modified: trunk/plearn/io/PStream.cc
===================================================================
--- trunk/plearn/io/PStream.cc	2007-07-06 21:56:01 UTC (rev 7711)
+++ trunk/plearn/io/PStream.cc	2007-07-06 22:11:17 UTC (rev 7712)
@@ -1160,7 +1160,8 @@
     }
     return *this;
 }
-  
+
+/* Commented out because "long" has not the same size on every platform
 PStream& PStream::operator>>(long &x)
 {
     switch(inmode)
@@ -1205,7 +1206,56 @@
     }
     return *this;
 }
-  
+*/
+
+PStream& PStream::operator>>(int64_t &x)
+{
+    switch(inmode)
+    {
+    case raw_ascii:
+    case pretty_ascii:
+        skipBlanks();
+        readAsciiNum(x);
+        break;
+    case raw_binary:
+        read(reinterpret_cast<char *>(&x), sizeof(int64_t));
+        break;
+    case plearn_ascii:
+    case plearn_binary:
+    {
+        skipBlanksAndCommentsAndSeparators();
+        int c = get();
+        if(c==0x07 || c==0x08)  // plearn_binary 32 bits integer
+        {
+            int32_t y;
+            read(reinterpret_cast<char*>(&y),sizeof(int32_t));
+            if( (c==0x07 && byte_order()==BIG_ENDIAN_ORDER) 
+                || (c==0x08 && byte_order()==LITTLE_ENDIAN_ORDER) )
+                endianswap(&y);
+            x = y;
+        }
+        else if(c==0x16 || c==0x17)  // plearn_binary 64 bits integer
+        {
+            read(reinterpret_cast<char*>(&x),sizeof(int64_t));
+            if( (c==0x16 && byte_order()==BIG_ENDIAN_ORDER) 
+                || (c==0x17 && byte_order()==LITTLE_ENDIAN_ORDER) )
+                endianswap(&x);
+        }
+        else  // plearn_ascii
+        {
+            unget();
+            readAsciiNum(x);
+        }
+        break;
+    }
+    default:
+        PLERROR("In PStream::operator>>  unknown inmode!!!!!!!!!");
+        break;
+    }
+    return *this;
+}
+
+/*
 PStream& PStream::operator>>(unsigned long &x)
 {
     switch(inmode)
@@ -1243,8 +1293,56 @@
     }
     return *this;
 }
+*/
 
+PStream& PStream::operator>>(uint64_t &x)
+{
+    switch(inmode)
+    {
+    case raw_ascii:
+    case pretty_ascii:
+        skipBlanks();
+        readAsciiNum(x);
+        break;
+    case raw_binary:
+        read(reinterpret_cast<char *>(&x), sizeof(uint64_t));
+        break;
+    case plearn_ascii:
+    case plearn_binary:
+    {
+        skipBlanksAndCommentsAndSeparators();
+        int c = get();
+        if(c==0x0B || c==0x0C)  // plearn_binary 32 bits unsigned integer
+        {
+            uint32_t y;
+            read(reinterpret_cast<char*>(&y),sizeof(uint32_t));
+            if( (c==0x0B && byte_order()==BIG_ENDIAN_ORDER) 
+                || (c==0x0C && byte_order()==LITTLE_ENDIAN_ORDER) )
+                endianswap(&y);
+            x = y;
+        }
+        else if(c==0x18 || c==0x19) // plearn_binary 64 bits unsigned integer
+        {
+            read(reinterpret_cast<char*>(&x), sizeof(uint64_t));
+            if( (c==0x18 && byte_order()==BIG_ENDIAN_ORDER)
+                || (c==0x19 && byte_order()==LITTLE_ENDIAN_ORDER) )
+                endianswap(&x);
+        }
+        else  // plearn_ascii
+        {
+            unget();
+            readAsciiNum(x);
+        }
+        break;
+    }
+    default:
+        PLERROR("In PStream::operator>>  unknown inmode!!!!!!!!!");
+        break;
+    }
+    return *this;
+}
 
+/*
 PStream& PStream::operator>>(long long &x)
 {
     switch(inmode)
@@ -1320,8 +1418,8 @@
     }
     return *this;
 }
+*/
 
-  
 PStream& PStream::operator>>(short &x)
 {
     switch(inmode)
@@ -1744,6 +1842,7 @@
     return *this;
 }
 
+/* Commented out because "long" has not the same size on every platform
 PStream& PStream::operator<<(long x) 
 { 
     switch(outmode)
@@ -1784,7 +1883,39 @@
     }
     return *this;
 }
+*/
 
+PStream& PStream::operator<<(int64_t x)
+{
+    switch(outmode)
+    {
+    case raw_binary:
+        write(reinterpret_cast<char *>(&x), sizeof(int64_t));
+        break;
+    case raw_ascii:
+    case pretty_ascii:
+        writeAsciiNum(x);
+        break;
+    case plearn_ascii:
+        writeAsciiNum(x);
+        put(' ');
+        break;
+    case plearn_binary:
+#ifdef BIGENDIAN
+        put((char)0x17);
+#else
+        put((char)0x16);
+#endif
+        write((char*)&x, sizeof(int64_t));
+        break;
+    default:
+        PLERROR("In PStream::operator<<  unknown outmode!!!!!!!!!");
+        break;
+    }
+    return *this;
+}
+
+/*
 PStream& PStream::operator<<(unsigned long x) 
 { 
     switch(outmode)
@@ -1814,7 +1945,38 @@
     }
     return *this;
 }
+*/
 
+PStream& PStream::operator<<(uint64_t x)
+{
+    switch(outmode)
+    {
+    case raw_binary:
+        write(reinterpret_cast<char *>(&x), sizeof(uint64_t));
+        break;
+    case raw_ascii:
+    case pretty_ascii:
+        writeAsciiNum(x);
+        break;
+    case plearn_ascii:
+        writeAsciiNum(x);
+        put(' ');
+        break;
+    case plearn_binary:
+#ifdef BIGENDIAN
+        put((char)0x19);
+#else
+        put((char)0x18);
+#endif
+        write((char*)&x, sizeof(uint64_t));
+        break;
+    default:
+        PLERROR("In PStream::operator<<  unknown outmode!!!!!!!!!");
+        break;
+    }
+    return *this;
+}
+
 PStream& PStream::operator<<(long long x) 
 { 
     switch(outmode)
@@ -1845,6 +2007,7 @@
     return *this;
 }
 
+/*
 PStream& PStream::operator<<(unsigned long long x) 
 { 
     switch(outmode)
@@ -1874,8 +2037,8 @@
     }
     return *this;
 }
+*/
 
-
 PStream& PStream::operator<<(short x) 
 { 
     switch(outmode)
@@ -2043,8 +2206,10 @@
     IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(unsigned short)
     IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(int)
     IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(unsigned int)
-    IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(long)
-    IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(unsigned long)
+    //IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(long)
+    //IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(unsigned long)
+    IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(int64_t)
+    IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(uint64_t)
 
 //! The binread_ for float and double are special
 

Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2007-07-06 21:56:01 UTC (rev 7711)
+++ trunk/plearn/io/PStream.h	2007-07-06 22:11:17 UTC (rev 7712)
@@ -460,13 +460,15 @@
     PStream& operator>>(signed char &x);
     PStream& operator>>(unsigned char &x);
     PStream& operator>>(int &x);
-    PStream& operator>>(unsigned int &x);  
-    PStream& operator>>(long &x);  
-    PStream& operator>>(unsigned long &x);
+    PStream& operator>>(unsigned int &x);
+    //PStream& operator>>(long &x);
+    //PStream& operator>>(unsigned long &x);
+    PStream& operator>>(int64_t &x);
+    PStream& operator>>(uint64_t &x);
     PStream& operator>>(short &x);
     PStream& operator>>(unsigned short &x);
-    PStream& operator>>(long long &x);
-    PStream& operator>>(unsigned long long &x);
+    //PStream& operator>>(long long &x);
+    //PStream& operator>>(unsigned long long &x);
     PStream& operator>>(pl_pstream_manip func) { return (*func)(*this); }
 
     // operator<<'s for base types
@@ -494,10 +496,12 @@
     PStream& operator<<(bool x);  
     PStream& operator<<(int x);
     PStream& operator<<(unsigned int x);
-    PStream& operator<<(long x);
-    PStream& operator<<(unsigned long x);
-    PStream& operator<<(long long x);
-    PStream& operator<<(unsigned long long x);
+    //PStream& operator<<(long x);
+    //PStream& operator<<(unsigned long x);
+    PStream& operator<<(int64_t x);
+    PStream& operator<<(uint64_t x);
+    //PStream& operator<<(long long x);
+    //PStream& operator<<(unsigned long long x);
     PStream& operator<<(short x);
     PStream& operator<<(unsigned short x);
     PStream& operator<<(pl_pstream_manip func) { return (*func)(*this); }
@@ -876,6 +880,7 @@
 inline void binwrite_(PStream& out, unsigned int* x, unsigned int n) 
 { out.write((char*)x, streamsize(n*sizeof(unsigned int))); }
 
+/*
 inline void binwrite_(PStream& out, const long* x, unsigned int n) 
 { out.write((char*)x, streamsize(n*sizeof(long))); }
 inline void binwrite_(PStream& out, long* x, unsigned int n) 
@@ -885,7 +890,18 @@
 { out.write((char*)x, streamsize(n*sizeof(unsigned long))); }
 inline void binwrite_(PStream& out, unsigned long* x, unsigned int n) 
 { out.write((char*)x, streamsize(n*sizeof(unsigned long))); }
+*/
 
+inline void binwrite_(PStream& out, const int64_t* x, unsigned int n) 
+{ out.write((char*)x, streamsize(n*sizeof(int64_t))); }
+inline void binwrite_(PStream& out, int64_t* x, unsigned int n) 
+{ out.write((char*)x, streamsize(n*sizeof(int64_t))); }
+
+inline void binwrite_(PStream& out, const uint64_t* x, unsigned int n) 
+{ out.write((char*)x, streamsize(n*sizeof(uint64_t))); }
+inline void binwrite_(PStream& out, uint64_t* x, unsigned int n) 
+{ out.write((char*)x, streamsize(n*sizeof(uint64_t))); }
+
 inline void binwrite_(PStream& out, const float* x, unsigned int n) 
 { out.write((char*)x, streamsize(n*sizeof(float))); }
 inline void binwrite_(PStream& out, float* x, unsigned int n) 
@@ -936,8 +952,10 @@
 void binread_(PStream& in, unsigned short* x, unsigned int n, unsigned char typecode);
 void binread_(PStream& in, int* x, unsigned int n, unsigned char typecode);
 void binread_(PStream& in, unsigned int* x, unsigned int n, unsigned char typecode);
-void binread_(PStream& in, long* x, unsigned int n, unsigned char typecode);
-void binread_(PStream& in, unsigned long* x, unsigned int n, unsigned char typecode);
+//void binread_(PStream& in, long* x, unsigned int n, unsigned char typecode);
+//void binread_(PStream& in, unsigned long* x, unsigned int n, unsigned char typecode);
+void binread_(PStream& in, int64_t* x, unsigned int n, unsigned char typecode);
+void binread_(PStream& in, uint64_t* x, unsigned int n, unsigned char typecode);
 void binread_(PStream& in, float* x, unsigned int n, unsigned char typecode);
 void binread_(PStream& in, double* x, unsigned int n, unsigned char typecode);
 



From lamblin at mail.berlios.de  Sat Jul  7 00:22:49 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Sat, 7 Jul 2007 00:22:49 +0200
Subject: [Plearn-commits] r7713 - trunk/plearn/io
Message-ID: <200707062222.l66MMnfO003401@sheep.berlios.de>

Author: lamblin
Date: 2007-07-07 00:22:48 +0200 (Sat, 07 Jul 2007)
New Revision: 7713

Modified:
   trunk/plearn/io/PStream.cc
Log:
I forgot to comment out one function


Modified: trunk/plearn/io/PStream.cc
===================================================================
--- trunk/plearn/io/PStream.cc	2007-07-06 22:11:17 UTC (rev 7712)
+++ trunk/plearn/io/PStream.cc	2007-07-06 22:22:48 UTC (rev 7713)
@@ -1977,6 +1977,7 @@
     return *this;
 }
 
+/*
 PStream& PStream::operator<<(long long x) 
 { 
     switch(outmode)
@@ -2007,7 +2008,6 @@
     return *this;
 }
 
-/*
 PStream& PStream::operator<<(unsigned long long x) 
 { 
     switch(outmode)



From lamblin at mail.berlios.de  Sat Jul  7 00:35:45 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Sat, 7 Jul 2007 00:35:45 +0200
Subject: [Plearn-commits] r7714 - trunk/plearn/io
Message-ID: <200707062235.l66MZjwo014713@sheep.berlios.de>

Author: lamblin
Date: 2007-07-07 00:35:37 +0200 (Sat, 07 Jul 2007)
New Revision: 7714

Modified:
   trunk/plearn/io/PStream.cc
   trunk/plearn/io/PStream.h
Log:
ApSTAT's compiler does not seem to like "uint64_t", rolling back to
previous version. Let's see that next week.


Modified: trunk/plearn/io/PStream.cc
===================================================================
--- trunk/plearn/io/PStream.cc	2007-07-06 22:22:48 UTC (rev 7713)
+++ trunk/plearn/io/PStream.cc	2007-07-06 22:35:37 UTC (rev 7714)
@@ -1160,8 +1160,7 @@
     }
     return *this;
 }
-
-/* Commented out because "long" has not the same size on every platform
+  
 PStream& PStream::operator>>(long &x)
 {
     switch(inmode)
@@ -1206,56 +1205,7 @@
     }
     return *this;
 }
-*/
-
-PStream& PStream::operator>>(int64_t &x)
-{
-    switch(inmode)
-    {
-    case raw_ascii:
-    case pretty_ascii:
-        skipBlanks();
-        readAsciiNum(x);
-        break;
-    case raw_binary:
-        read(reinterpret_cast<char *>(&x), sizeof(int64_t));
-        break;
-    case plearn_ascii:
-    case plearn_binary:
-    {
-        skipBlanksAndCommentsAndSeparators();
-        int c = get();
-        if(c==0x07 || c==0x08)  // plearn_binary 32 bits integer
-        {
-            int32_t y;
-            read(reinterpret_cast<char*>(&y),sizeof(int32_t));
-            if( (c==0x07 && byte_order()==BIG_ENDIAN_ORDER) 
-                || (c==0x08 && byte_order()==LITTLE_ENDIAN_ORDER) )
-                endianswap(&y);
-            x = y;
-        }
-        else if(c==0x16 || c==0x17)  // plearn_binary 64 bits integer
-        {
-            read(reinterpret_cast<char*>(&x),sizeof(int64_t));
-            if( (c==0x16 && byte_order()==BIG_ENDIAN_ORDER) 
-                || (c==0x17 && byte_order()==LITTLE_ENDIAN_ORDER) )
-                endianswap(&x);
-        }
-        else  // plearn_ascii
-        {
-            unget();
-            readAsciiNum(x);
-        }
-        break;
-    }
-    default:
-        PLERROR("In PStream::operator>>  unknown inmode!!!!!!!!!");
-        break;
-    }
-    return *this;
-}
-
-/*
+  
 PStream& PStream::operator>>(unsigned long &x)
 {
     switch(inmode)
@@ -1293,56 +1243,8 @@
     }
     return *this;
 }
-*/
 
-PStream& PStream::operator>>(uint64_t &x)
-{
-    switch(inmode)
-    {
-    case raw_ascii:
-    case pretty_ascii:
-        skipBlanks();
-        readAsciiNum(x);
-        break;
-    case raw_binary:
-        read(reinterpret_cast<char *>(&x), sizeof(uint64_t));
-        break;
-    case plearn_ascii:
-    case plearn_binary:
-    {
-        skipBlanksAndCommentsAndSeparators();
-        int c = get();
-        if(c==0x0B || c==0x0C)  // plearn_binary 32 bits unsigned integer
-        {
-            uint32_t y;
-            read(reinterpret_cast<char*>(&y),sizeof(uint32_t));
-            if( (c==0x0B && byte_order()==BIG_ENDIAN_ORDER) 
-                || (c==0x0C && byte_order()==LITTLE_ENDIAN_ORDER) )
-                endianswap(&y);
-            x = y;
-        }
-        else if(c==0x18 || c==0x19) // plearn_binary 64 bits unsigned integer
-        {
-            read(reinterpret_cast<char*>(&x), sizeof(uint64_t));
-            if( (c==0x18 && byte_order()==BIG_ENDIAN_ORDER)
-                || (c==0x19 && byte_order()==LITTLE_ENDIAN_ORDER) )
-                endianswap(&x);
-        }
-        else  // plearn_ascii
-        {
-            unget();
-            readAsciiNum(x);
-        }
-        break;
-    }
-    default:
-        PLERROR("In PStream::operator>>  unknown inmode!!!!!!!!!");
-        break;
-    }
-    return *this;
-}
 
-/*
 PStream& PStream::operator>>(long long &x)
 {
     switch(inmode)
@@ -1418,8 +1320,8 @@
     }
     return *this;
 }
-*/
 
+  
 PStream& PStream::operator>>(short &x)
 {
     switch(inmode)
@@ -1842,7 +1744,6 @@
     return *this;
 }
 
-/* Commented out because "long" has not the same size on every platform
 PStream& PStream::operator<<(long x) 
 { 
     switch(outmode)
@@ -1883,39 +1784,7 @@
     }
     return *this;
 }
-*/
 
-PStream& PStream::operator<<(int64_t x)
-{
-    switch(outmode)
-    {
-    case raw_binary:
-        write(reinterpret_cast<char *>(&x), sizeof(int64_t));
-        break;
-    case raw_ascii:
-    case pretty_ascii:
-        writeAsciiNum(x);
-        break;
-    case plearn_ascii:
-        writeAsciiNum(x);
-        put(' ');
-        break;
-    case plearn_binary:
-#ifdef BIGENDIAN
-        put((char)0x17);
-#else
-        put((char)0x16);
-#endif
-        write((char*)&x, sizeof(int64_t));
-        break;
-    default:
-        PLERROR("In PStream::operator<<  unknown outmode!!!!!!!!!");
-        break;
-    }
-    return *this;
-}
-
-/*
 PStream& PStream::operator<<(unsigned long x) 
 { 
     switch(outmode)
@@ -1945,39 +1814,7 @@
     }
     return *this;
 }
-*/
 
-PStream& PStream::operator<<(uint64_t x)
-{
-    switch(outmode)
-    {
-    case raw_binary:
-        write(reinterpret_cast<char *>(&x), sizeof(uint64_t));
-        break;
-    case raw_ascii:
-    case pretty_ascii:
-        writeAsciiNum(x);
-        break;
-    case plearn_ascii:
-        writeAsciiNum(x);
-        put(' ');
-        break;
-    case plearn_binary:
-#ifdef BIGENDIAN
-        put((char)0x19);
-#else
-        put((char)0x18);
-#endif
-        write((char*)&x, sizeof(uint64_t));
-        break;
-    default:
-        PLERROR("In PStream::operator<<  unknown outmode!!!!!!!!!");
-        break;
-    }
-    return *this;
-}
-
-/*
 PStream& PStream::operator<<(long long x) 
 { 
     switch(outmode)
@@ -2037,8 +1874,8 @@
     }
     return *this;
 }
-*/
 
+
 PStream& PStream::operator<<(short x) 
 { 
     switch(outmode)
@@ -2206,10 +2043,8 @@
     IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(unsigned short)
     IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(int)
     IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(unsigned int)
-    //IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(long)
-    //IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(unsigned long)
-    IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(int64_t)
-    IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(uint64_t)
+    IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(long)
+    IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(unsigned long)
 
 //! The binread_ for float and double are special
 

Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2007-07-06 22:22:48 UTC (rev 7713)
+++ trunk/plearn/io/PStream.h	2007-07-06 22:35:37 UTC (rev 7714)
@@ -460,15 +460,13 @@
     PStream& operator>>(signed char &x);
     PStream& operator>>(unsigned char &x);
     PStream& operator>>(int &x);
-    PStream& operator>>(unsigned int &x);
-    //PStream& operator>>(long &x);
-    //PStream& operator>>(unsigned long &x);
-    PStream& operator>>(int64_t &x);
-    PStream& operator>>(uint64_t &x);
+    PStream& operator>>(unsigned int &x);  
+    PStream& operator>>(long &x);  
+    PStream& operator>>(unsigned long &x);
     PStream& operator>>(short &x);
     PStream& operator>>(unsigned short &x);
-    //PStream& operator>>(long long &x);
-    //PStream& operator>>(unsigned long long &x);
+    PStream& operator>>(long long &x);
+    PStream& operator>>(unsigned long long &x);
     PStream& operator>>(pl_pstream_manip func) { return (*func)(*this); }
 
     // operator<<'s for base types
@@ -496,12 +494,10 @@
     PStream& operator<<(bool x);  
     PStream& operator<<(int x);
     PStream& operator<<(unsigned int x);
-    //PStream& operator<<(long x);
-    //PStream& operator<<(unsigned long x);
-    PStream& operator<<(int64_t x);
-    PStream& operator<<(uint64_t x);
-    //PStream& operator<<(long long x);
-    //PStream& operator<<(unsigned long long x);
+    PStream& operator<<(long x);
+    PStream& operator<<(unsigned long x);
+    PStream& operator<<(long long x);
+    PStream& operator<<(unsigned long long x);
     PStream& operator<<(short x);
     PStream& operator<<(unsigned short x);
     PStream& operator<<(pl_pstream_manip func) { return (*func)(*this); }
@@ -880,7 +876,6 @@
 inline void binwrite_(PStream& out, unsigned int* x, unsigned int n) 
 { out.write((char*)x, streamsize(n*sizeof(unsigned int))); }
 
-/*
 inline void binwrite_(PStream& out, const long* x, unsigned int n) 
 { out.write((char*)x, streamsize(n*sizeof(long))); }
 inline void binwrite_(PStream& out, long* x, unsigned int n) 
@@ -890,18 +885,7 @@
 { out.write((char*)x, streamsize(n*sizeof(unsigned long))); }
 inline void binwrite_(PStream& out, unsigned long* x, unsigned int n) 
 { out.write((char*)x, streamsize(n*sizeof(unsigned long))); }
-*/
 
-inline void binwrite_(PStream& out, const int64_t* x, unsigned int n) 
-{ out.write((char*)x, streamsize(n*sizeof(int64_t))); }
-inline void binwrite_(PStream& out, int64_t* x, unsigned int n) 
-{ out.write((char*)x, streamsize(n*sizeof(int64_t))); }
-
-inline void binwrite_(PStream& out, const uint64_t* x, unsigned int n) 
-{ out.write((char*)x, streamsize(n*sizeof(uint64_t))); }
-inline void binwrite_(PStream& out, uint64_t* x, unsigned int n) 
-{ out.write((char*)x, streamsize(n*sizeof(uint64_t))); }
-
 inline void binwrite_(PStream& out, const float* x, unsigned int n) 
 { out.write((char*)x, streamsize(n*sizeof(float))); }
 inline void binwrite_(PStream& out, float* x, unsigned int n) 
@@ -952,10 +936,8 @@
 void binread_(PStream& in, unsigned short* x, unsigned int n, unsigned char typecode);
 void binread_(PStream& in, int* x, unsigned int n, unsigned char typecode);
 void binread_(PStream& in, unsigned int* x, unsigned int n, unsigned char typecode);
-//void binread_(PStream& in, long* x, unsigned int n, unsigned char typecode);
-//void binread_(PStream& in, unsigned long* x, unsigned int n, unsigned char typecode);
-void binread_(PStream& in, int64_t* x, unsigned int n, unsigned char typecode);
-void binread_(PStream& in, uint64_t* x, unsigned int n, unsigned char typecode);
+void binread_(PStream& in, long* x, unsigned int n, unsigned char typecode);
+void binread_(PStream& in, unsigned long* x, unsigned int n, unsigned char typecode);
 void binread_(PStream& in, float* x, unsigned int n, unsigned char typecode);
 void binread_(PStream& in, double* x, unsigned int n, unsigned char typecode);
 



From chapados at mail.berlios.de  Sat Jul  7 17:54:00 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Sat, 7 Jul 2007 17:54:00 +0200
Subject: [Plearn-commits] r7715 -
	trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results
Message-ID: <200707071554.l67Fs0fm014885@sheep.berlios.de>

Author: chapados
Date: 2007-07-07 17:54:00 +0200 (Sat, 07 Jul 2007)
New Revision: 7715

Modified:
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
Log:
New test results following new option in PLearner

Modified: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2007-07-06 22:35:37 UTC (rev 7714)
+++ trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2007-07-07 15:54:00 UTC (rev 7715)
@@ -5,10 +5,10 @@
 !R 1 GaussianProcessRegressor(
 kernel = *1 ->SummationKernel(
 terms = 2 [ *2 ->RationalQuadraticARDKernel(
-isp_alpha = 14.8530952695504261 ;
-isp_signal_sigma = 29.6219285856411894 ;
+isp_alpha = 14.8530952695687528 ;
+isp_signal_sigma = 29.6219285855842109 ;
 isp_global_sigma = 0 ;
-isp_input_sigma = 1 [ 22.2544311482102088 ] ;
+isp_input_sigma = 1 [ 22.2544311479547581 ] ;
 kronecker_indexes = []
 ;
 cache_threshold = 1000000 ;
@@ -19,7 +19,7 @@
 data_inputsize = 1 ;
 n_examples = 5  )
 *3 ->IIDNoiseKernel(
-isp_noise_sigma = -1.86446658047284264 ;
+isp_noise_sigma = -1.86446658049196934 ;
 isp_kronecker_sigma = -100 ;
 kronecker_indexes = []
 ;
@@ -62,19 +62,19 @@
 ;
 save_gram_matrix = 0 ;
 alpha = 5  1  [ 
--1.11770047926741944 	
--1.44398600683733602 	
-2.34477475115816247 	
-0.359163702825210596 	
--0.273169390815579172 	
+-1.11770047929104055 	
+-1.44398600685698719 	
+2.34477475120374246 	
+0.35916370282251725 	
+-0.273169390814979818 	
 ]
 ;
 gram_inverse = 5  5  [ 
-2.1738430348361808 	-0.0428570226562168741 	-2.24278913022280779 	0.21118794494333884 	-0.016000849842130186 	
--0.0428570226562171239 	2.63229682417954214 	-2.36514023152167674 	-0.310986674739582525 	0.0223156036853270311 	
--2.24278913022280779 	-2.3651402315216763 	4.57667695961490306 	0.0265284039064695096 	-8.51433617983321869e-05 	
-0.211187944943338812 	-0.310986674739582469 	0.0265284039064695096 	0.113894354038512086 	-0.0105928862093947491 	
--0.0160008498421301826 	0.0223156036853270276 	-8.51433617983319294e-05 	-0.0105928862093947491 	0.0346336314417334534 	
+2.17384303484850605 	-0.0428570226279049171 	-2.24278913026098703 	0.211187944941121059 	-0.0160008498417494489 	
+-0.0428570226279050698 	2.63229682419555111 	-2.36514023156742859 	-0.310986674738180424 	0.0223156036848867825 	
+-2.24278913026098703 	-2.36514023156742859 	4.57667695969712351 	0.0265284039081269407 	-8.51433619161135006e-05 	
+0.211187944941121086 	-0.31098667473818048 	0.0265284039081269442 	0.113894354037727866 	-0.0105928862091330695 	
+-0.0160008498417494489 	0.0223156036848867825 	-8.51433619161134599e-05 	-0.0105928862091330678 	0.0346336314417550195 	
 ]
 ;
 target_mean = 1 [ 10 ] ;
@@ -86,6 +86,7 @@
 20 	
 ]
 ;
+random_gen = *0 ;
 seed = 1827 ;
 stage = 10 ;
 n_examples = 5 ;
@@ -101,18 +102,18 @@
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827  )
 
-!R 1 1 [ 14.9482509617851225 ] 
-!R 1 1 [ 14.4446958356332527 ] 
+!R 1 1 [ 14.9482509617864245 ] 
+!R 1 1 [ 14.4446958356311814 ] 
 !R 2 4  1  [ 
-13.4992090127014244 	
-14.4141233331841647 	
-14.9482509617851225 	
-14.4446958356332527 	
+13.4992090127012432 	
+14.4141233331850245 	
+14.9482509617864245 	
+14.4446958356311814 	
 ]
 1 [ 4  4  [ 
-0.510839430940516981 	0.283483493893925242 	0.0642765858976659388 	-0.555619592261329842 	
-0.283483493893680105 	0.391896855155454082 	0.103882973848694604 	-0.369897017886295032 	
-0.0642765858979466032 	0.10388297384912093 	0.285799680582960625 	0.185897571271347317 	
--0.555619592261404449 	-0.369897017886252399 	0.185897571271155471 	2.23845483384018262 	
+0.510839430938520356 	0.283483493894024718 	0.0642765858965077541 	-0.555619592268588036 	
+0.283483493894031824 	0.391896855152693624 	0.103882973846950222 	-0.369897017895095104 	
+0.064276585896521965 	0.103882973846953774 	0.285799680578093407 	0.185897571267990003 	
+-0.555619592268573825 	-0.369897017895084446 	0.185897571267993555 	2.23845483386523281 	
 ]
 ] 



From yoshua at mail.berlios.de  Sun Jul  8 01:00:49 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Sun, 8 Jul 2007 01:00:49 +0200
Subject: [Plearn-commits] r7716 - trunk/plearn_learners/online
Message-ID: <200707072300.l67N0nFa024084@sheep.berlios.de>

Author: yoshua
Date: 2007-07-08 01:00:47 +0200 (Sun, 08 Jul 2007)
New Revision: 7716

Modified:
   trunk/plearn_learners/online/OnlineLearningModule.cc
Log:
More explicit error message when unknown port name used in a module.


Modified: trunk/plearn_learners/online/OnlineLearningModule.cc
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.cc	2007-07-07 15:54:00 UTC (rev 7715)
+++ trunk/plearn_learners/online/OnlineLearningModule.cc	2007-07-07 23:00:47 UTC (rev 7716)
@@ -398,6 +398,9 @@
 ///////////////////
 int OnlineLearningModule::getPortLength(const string& port)
 {
+    if (getPortIndex(port)<0)
+        PLERROR("Port named %s not known by module %s of class %s\n",
+                port.c_str(),name.c_str(),classname().c_str());
     PLASSERT( getPortIndex(port) >= 0 );
     return getPortSizes()(getPortIndex(port), 0);
 }



From tihocan at mail.berlios.de  Mon Jul  9 16:18:56 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 9 Jul 2007 16:18:56 +0200
Subject: [Plearn-commits] r7717 - trunk/plearn/io
Message-ID: <200707091418.l69EIuju010984@sheep.berlios.de>

Author: tihocan
Date: 2007-07-09 16:18:56 +0200 (Mon, 09 Jul 2007)
New Revision: 7717

Modified:
   trunk/plearn/io/PPath.cc
   trunk/plearn/io/PPath.h
Log:
Modified the way the static map metaprotocol_to_metapath map is initialized to fix a bug with gcc 3.4.4 under Windows

Modified: trunk/plearn/io/PPath.cc
===================================================================
--- trunk/plearn/io/PPath.cc	2007-07-07 23:00:47 UTC (rev 7716)
+++ trunk/plearn/io/PPath.cc	2007-07-09 14:18:56 UTC (rev 7717)
@@ -232,12 +232,18 @@
     return default_;
 }
 
-
 ////////////////////////////
 // metaprotocolToMetapath //
 ////////////////////////////
-map<string, PPath> PPath::metaprotocol_to_metapath;
 
+// Static map that stores the binding metaprotocol <-> metapath.
+// It is embedded within a function to prevent potential compiler issues during
+// static initialization (e.g. under Windows with gcc 3.4.4).
+map<string, PPath>& metaprotocol_to_metapath() {
+    static map<string, PPath> metaprotocol_to_metapath;
+    return metaprotocol_to_metapath;
+}
+
 const map<string, PPath>& PPath::metaprotocolToMetapath()
 {
     static  bool                mappings;
@@ -292,14 +298,14 @@
             {
                 // Default PPath settings. Defined only if the HOME environment
                 // variable exists.
-                metaprotocol_to_metapath["HOME"] = "${HOME}";
-                metaprotocol_to_metapath["PLEARNDIR"] = "HOME:PLearn";
-                metaprotocol_to_metapath["PLEARN_LIBDIR"] = "PLEARNDIR:external_libs";
+                metaprotocol_to_metapath()["HOME"] = "${HOME}";
+                metaprotocol_to_metapath()["PLEARNDIR"] = "HOME:PLearn";
+                metaprotocol_to_metapath()["PLEARN_LIBDIR"] = "PLEARNDIR:external_libs";
             }
         }
     }
 
-    return metaprotocol_to_metapath;
+    return metaprotocol_to_metapath();
 }
 
 ////////////////////////////
@@ -312,7 +318,7 @@
     const map<string, PPath>& bindings = metaprotocolToMetapath();
     bool already_here = bindings.find(metaprotocol) != bindings.end();
     if (!already_here || force)
-        metaprotocol_to_metapath[metaprotocol] = metapath;
+        metaprotocol_to_metapath()[metaprotocol] = metapath;
     return !already_here;
 }
 
@@ -404,19 +410,22 @@
         new_path = string(buf);
 #elif defined(_MINGW_)
         // We need to convert the path by ourselves.
-        if (!startsWith(*the_path, "/cygdrive/"))
-            pout << "SHIT: " << *the_path << endl;
-        PLASSERT( startsWith(*the_path, "/cygdrive/") );
-        // Remove '/cygdrive'.
-        new_path = the_path->substr(9);
-        // Copy drive letter from second to first position.
-        new_path[0] = new_path[1];
-        // Add ':' after drive letter.
-        new_path[1] = ':';
-        // Replace '/' by '\'.
-        for (string::size_type i = 0; i < new_path.size(); i++)
-            if (new_path[i] == '/')
-                new_path[i] = '\\';
+        if (!startsWith(*the_path, "/cygdrive/")) {
+            PLWARNING("Path '%s' is expected to start with '/cygdrive/'",
+                    the_path->c_str());
+            new_path = *the_path;
+        } else {
+            // Remove '/cygdrive'.
+            new_path = the_path->substr(9);
+            // Copy drive letter from second to first position.
+            new_path[0] = new_path[1];
+            // Add ':' after drive letter.
+            new_path[1] = ':';
+            // Replace '/' by '\'.
+            for (string::size_type i = 0; i < new_path.size(); i++)
+                if (new_path[i] == '/')
+                    new_path[i] = '\\';
+        }
 #endif
         the_path = &new_path;
     }
@@ -487,8 +496,8 @@
         if ( it != metaprotocolToMetapath().end() )
             metapath = it->second;
         else
-            metapath = getenv(meta);            
-        
+            metapath = getenv(meta);
+
         if ( !metapath.isEmpty() )
         {      
             string after_colon = endmeta == length()-1 ? "" : substr(endmeta+1);

Modified: trunk/plearn/io/PPath.h
===================================================================
--- trunk/plearn/io/PPath.h	2007-07-07 23:00:47 UTC (rev 7716)
+++ trunk/plearn/io/PPath.h	2007-07-09 14:18:56 UTC (rev 7717)
@@ -505,12 +505,6 @@
 
              private:
 
-                 //! The map storing the bindings metaprotocol <-> metapath.
-                 //! It should never be manipulated directly: instead, use the
-                 //! metaprotocolToMetapath() and addMetaprotocolToMetapath()
-                 //! functions.
-                 static map<string, PPath> metaprotocol_to_metapath;
-
                  //! Whether or not to display the canonical path in
                  //! errorDisplay(). Default value is 'false', and it can be
                  //! modified through the setCanonicalInErrors(..) function.



From dorionc at mail.berlios.de  Mon Jul  9 17:09:11 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Mon, 9 Jul 2007 17:09:11 +0200
Subject: [Plearn-commits] r7718 - trunk/python_modules/plearn/analysis
Message-ID: <200707091509.l69F9BeQ017133@sheep.berlios.de>

Author: dorionc
Date: 2007-07-09 17:09:10 +0200 (Mon, 09 Jul 2007)
New Revision: 7718

Modified:
   trunk/python_modules/plearn/analysis/experiment_results.py
   trunk/python_modules/plearn/analysis/xp.py
Log:
Fixed bugged introduced by the use of absolute expdir paths

Modified: trunk/python_modules/plearn/analysis/experiment_results.py
===================================================================
--- trunk/python_modules/plearn/analysis/experiment_results.py	2007-07-09 14:18:56 UTC (rev 7717)
+++ trunk/python_modules/plearn/analysis/experiment_results.py	2007-07-09 15:09:10 UTC (rev 7718)
@@ -208,7 +208,7 @@
         return len(self.metainfos) == 0
 
     def toString(self, expkey=None, short=False):
-        s = '%s\n' % relative_path(self.path)
+        s = '%s\n' % relative_path(self.expdir)
         if short and expkey is None:
             return s
         
@@ -308,7 +308,7 @@
 
     def keycmp(exp, other, expkey):
         """Compare two experiments along a given key"""
-        if exp.path == other.path:
+        if exp.expdir == other.expdir:
             return 0
         
         exp_subkey   = exp.getSubKey(expkey)

Modified: trunk/python_modules/plearn/analysis/xp.py
===================================================================
--- trunk/python_modules/plearn/analysis/xp.py	2007-07-09 14:18:56 UTC (rev 7717)
+++ trunk/python_modules/plearn/analysis/xp.py	2007-07-09 15:09:10 UTC (rev 7718)
@@ -146,7 +146,7 @@
             duplicates = []
             for x in experiments:
                 if ExpKey.keycmp(x, exp, expkey) == 0:
-                    duplicates.append( x.path )
+                    duplicates.append( x.expdir )
             if duplicates:
                 if options.sh:
                     shfile.write( 'rm -rf ' + "\nrm -rf ".join(duplicates) + '\n' )
@@ -205,30 +205,29 @@
     option_groups = classmethod( option_groups )
     
     def routine( self, expkey, options, experiments ):    
-        reffunc = os.symlink
+        reffunc = relative_link #os.symlink
         if options.move:
             reffunc = lambda src,dest: os.system("mv %s %s"%(src,dest))        
 
+        print >>sys.stderr, "N:", len(experiments)
         for exp in experiments:
             subkey = exp.getKey( expkey )
+            print >>sys.stderr, exp.expdir
             if options.name is None:
                 dirname = "_".join([ "%s=%s" % (lhs, str(rhs))
                                      for (lhs, rhs) in subkey.iteritems() ])
             else:
                 dirname = options.name
 
-            if not exp.path.startswith("expdir"):
-                dirname = os.path.join( os.path.dirname(exp.path), dirname )
-
             if not os.path.exists( dirname ):
                 os.mkdir( dirname )
 
             pushd( dirname )
-            if not os.path.exists( exp.path ):
-                expdir = os.path.basename( exp.path )
+            expdir = os.path.basename( exp.expdir )
+            if not os.path.exists( expdir ):
                 assert expdir.startswith("expdir"), expdir
                 try:
-                    reffunc( os.path.join('..',expdir), expdir )
+                    reffunc( exp.expdir, expdir )
                 except OSError, err:
                     raise OSError( '%s\n in %s\n with expdir=%s'
                                    % (str(err), os.getcwd(), expdir)
@@ -267,7 +266,7 @@
 def xpathfunction( func ):
     def function( obj, *args, **kwargs ):
         if isinstance( obj, Experiment ):
-            func( obj.path, *args, **kwargs )
+            func( obj.expdir, *args, **kwargs )
         elif len(args) and isinstance( obj, str ):
             func( obj, *args, **kwargs )
         else:



From dorionc at mail.berlios.de  Mon Jul  9 17:10:37 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Mon, 9 Jul 2007 17:10:37 +0200
Subject: [Plearn-commits] r7719 - trunk/python_modules/plearn/analysis
Message-ID: <200707091510.l69FAbhl017377@sheep.berlios.de>

Author: dorionc
Date: 2007-07-09 17:10:37 +0200 (Mon, 09 Jul 2007)
New Revision: 7719

Modified:
   trunk/python_modules/plearn/analysis/xp.py
Log:
Cleaned some debug prints

Modified: trunk/python_modules/plearn/analysis/xp.py
===================================================================
--- trunk/python_modules/plearn/analysis/xp.py	2007-07-09 15:09:10 UTC (rev 7718)
+++ trunk/python_modules/plearn/analysis/xp.py	2007-07-09 15:10:37 UTC (rev 7719)
@@ -209,10 +209,8 @@
         if options.move:
             reffunc = lambda src,dest: os.system("mv %s %s"%(src,dest))        
 
-        print >>sys.stderr, "N:", len(experiments)
         for exp in experiments:
             subkey = exp.getKey( expkey )
-            print >>sys.stderr, exp.expdir
             if options.name is None:
                 dirname = "_".join([ "%s=%s" % (lhs, str(rhs))
                                      for (lhs, rhs) in subkey.iteritems() ])



From lamblin at mail.berlios.de  Tue Jul 10 00:15:27 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 10 Jul 2007 00:15:27 +0200
Subject: [Plearn-commits] r7720 - in trunk/python_modules/plearn: .
	learners/autolr learners/modulelearners
Message-ID: <200707092215.l69MFR8W009656@sheep.berlios.de>

Author: lamblin
Date: 2007-07-10 00:15:27 +0200 (Tue, 10 Jul 2007)
New Revision: 7720

Modified:
   trunk/python_modules/plearn/bridge.py
   trunk/python_modules/plearn/learners/autolr/__init__.py
   trunk/python_modules/plearn/learners/modulelearners/__init__.py
Log:
Better implementation of deepcopy, moved to plearn/bridge.py


Modified: trunk/python_modules/plearn/bridge.py
===================================================================
--- trunk/python_modules/plearn/bridge.py	2007-07-09 15:10:37 UTC (rev 7719)
+++ trunk/python_modules/plearn/bridge.py	2007-07-09 22:15:27 UTC (rev 7720)
@@ -19,4 +19,16 @@
 
 if plearn.bridgemode.interactive:
     from pylab import *
-    
+
+
+def deepcopy(plearnobject, use_threads = False):
+    # actually not a deep-copy, only copy options
+    if plearn.bridgemode.useserver:
+        o = assign(plearnobject.getObject(), use_threads)
+    else:
+        o = deepCopy(plearnobject)
+    if o==None:
+        print "deepcopy failed"
+        raise NotImplementedError
+    return o
+

Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-07-09 15:10:37 UTC (rev 7719)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-07-09 22:15:27 UTC (rev 7720)
@@ -79,19 +79,6 @@
     return o
 
 
-# ugly way to copy until done properly with PLearn's deepcopy
-def deepcopy(plearnobject, use_threads = False):
-    # actually not a deep-copy, only copy options
-    if plearn.bridgemode.useserver:
-        o = assign(plearnobject.getObject(), use_threads)
-    else:
-        o = newObject(str(plearnobject))
-    if o==None:
-        print "deepcopy failed"
-        raise NotImplementedError
-    return o
-
-
 def merge_schedules(schedules):
     """Merge several learning rate schedules into a kind of multi-schedule
     with one column per schedule but a unified sequence of stages.

Modified: trunk/python_modules/plearn/learners/modulelearners/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/__init__.py	2007-07-09 15:10:37 UTC (rev 7719)
+++ trunk/python_modules/plearn/learners/modulelearners/__init__.py	2007-07-09 22:15:27 UTC (rev 7720)
@@ -10,10 +10,6 @@
 
 tmp_file='/tmp/modulelearner.py'
 
-def dirtydeepcopy( myObject ):
-    myObject.save(tmp_file,'plearn_ascii')
-    return loadObject(tmp_file)
-
 if plearn.bridgemode.useserver:
     learner  = serv.new(learner)
     trainset = serv.new(trainset)
@@ -408,7 +404,7 @@
 #    new_network.setOptionFromPython('connections',new_connections_list)
 #    new_network.setOptionFromPython('ports',new_ports_list)
 #    new_network.setOptionFromPython('modules',new_modules_list)
-    return dirtydeepcopy( new_network )
+    return deepcopy( new_network )
     return new_network
     
 
@@ -430,7 +426,7 @@
         new_learner.setOptionFromPython(port_option_name, new_output_ports_list)
     new_learner.setOptionFromPython('module',new_network)
 
-    return dirtydeepcopy( new_learner )
+    return deepcopy( new_learner )
     return new_learner
 
 
@@ -447,7 +443,7 @@
     if len(portslist) == 1:
        new_ports_list.append((output_port_tuple[0],portslist[0]))
        setPorts(mynewObject, new_ports_list)
-       return dirtydeepcopy( mynewObject )
+       return deepcopy( mynewObject )
        return mynewObject
     new_modules_list = getModules(myObject)
     new_connections_list = getConnections(myObject)
@@ -482,7 +478,7 @@
     setConnections(mynewObject, new_connections_list)
     setPorts(mynewObject, new_ports_list)
     setModules(mynewObject, new_modules_list)
-    return dirtydeepcopy( mynewObject )
+    return deepcopy( mynewObject )
     return mynewObject
 
 



From lamblin at mail.berlios.de  Tue Jul 10 00:38:35 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 10 Jul 2007 00:38:35 +0200
Subject: [Plearn-commits] r7721 - trunk/plearn/io
Message-ID: <200707092238.l69McZXH028674@sheep.berlios.de>

Author: lamblin
Date: 2007-07-10 00:38:28 +0200 (Tue, 10 Jul 2007)
New Revision: 7721

Modified:
   trunk/plearn/io/PStream.cc
   trunk/plearn/io/PStream.h
Log:
New try... will this work at ApSTAT?


Modified: trunk/plearn/io/PStream.cc
===================================================================
--- trunk/plearn/io/PStream.cc	2007-07-09 22:15:27 UTC (rev 7720)
+++ trunk/plearn/io/PStream.cc	2007-07-09 22:38:28 UTC (rev 7721)
@@ -1160,7 +1160,8 @@
     }
     return *this;
 }
-  
+
+/* Commented out because "long" has not the same size on every platform
 PStream& PStream::operator>>(long &x)
 {
     switch(inmode)
@@ -1205,7 +1206,56 @@
     }
     return *this;
 }
-  
+*/
+
+PStream& PStream::operator>>(int64_t &x)
+{
+    switch(inmode)
+    {
+    case raw_ascii:
+    case pretty_ascii:
+        skipBlanks();
+        readAsciiNum(x);
+        break;
+    case raw_binary:
+        read(reinterpret_cast<char *>(&x), sizeof(int64_t));
+        break;
+    case plearn_ascii:
+    case plearn_binary:
+    {
+        skipBlanksAndCommentsAndSeparators();
+        int c = get();
+        if(c==0x07 || c==0x08)  // plearn_binary 32 bits integer
+        {
+            int32_t y;
+            read(reinterpret_cast<char*>(&y),sizeof(int32_t));
+            if( (c==0x07 && byte_order()==BIG_ENDIAN_ORDER) 
+                || (c==0x08 && byte_order()==LITTLE_ENDIAN_ORDER) )
+                endianswap(&y);
+            x = y;
+        }
+        else if(c==0x16 || c==0x17)  // plearn_binary 64 bits integer
+        {
+            read(reinterpret_cast<char*>(&x),sizeof(int64_t));
+            if( (c==0x16 && byte_order()==BIG_ENDIAN_ORDER) 
+                || (c==0x17 && byte_order()==LITTLE_ENDIAN_ORDER) )
+                endianswap(&x);
+        }
+        else  // plearn_ascii
+        {
+            unget();
+            readAsciiNum(x);
+        }
+        break;
+    }
+    default:
+        PLERROR("In PStream::operator>>  unknown inmode!!!!!!!!!");
+        break;
+    }
+    return *this;
+}
+
+/*
 PStream& PStream::operator>>(unsigned long &x)
 {
     switch(inmode)
@@ -1243,8 +1293,56 @@
     }
     return *this;
 }
+*/
 
+PStream& PStream::operator>>(uint64_t &x)
+{
+    switch(inmode)
+    {
+    case raw_ascii:
+    case pretty_ascii:
+        skipBlanks();
+        readAsciiNum(x);
+        break;
+    case raw_binary:
+        read(reinterpret_cast<char *>(&x), sizeof(uint64_t));
+        break;
+    case plearn_ascii:
+    case plearn_binary:
+    {
+        skipBlanksAndCommentsAndSeparators();
+        int c = get();
+        if(c==0x0B || c==0x0C)  // plearn_binary 32 bits unsigned integer
+        {
+            uint32_t y;
+            read(reinterpret_cast<char*>(&y),sizeof(uint32_t));
+            if( (c==0x0B && byte_order()==BIG_ENDIAN_ORDER) 
+                || (c==0x0C && byte_order()==LITTLE_ENDIAN_ORDER) )
+                endianswap(&y);
+            x = y;
+        }
+        else if(c==0x18 || c==0x19) // plearn_binary 64 bits unsigned integer
+        {
+            read(reinterpret_cast<char*>(&x), sizeof(uint64_t));
+            if( (c==0x18 && byte_order()==BIG_ENDIAN_ORDER)
+                || (c==0x19 && byte_order()==LITTLE_ENDIAN_ORDER) )
+                endianswap(&x);
+        }
+        else  // plearn_ascii
+        {
+            unget();
+            readAsciiNum(x);
+        }
+        break;
+    }
+    default:
+        PLERROR("In PStream::operator>>  unknown inmode!!!!!!!!!");
+        break;
+    }
+    return *this;
+}
 
+/*
 PStream& PStream::operator>>(long long &x)
 {
     switch(inmode)
@@ -1320,8 +1418,8 @@
     }
     return *this;
 }
+*/
 
-  
 PStream& PStream::operator>>(short &x)
 {
     switch(inmode)
@@ -1744,6 +1842,7 @@
     return *this;
 }
 
+/* Commented out because "long" has not the same size on every platform
 PStream& PStream::operator<<(long x) 
 { 
     switch(outmode)
@@ -1784,7 +1883,39 @@
     }
     return *this;
 }
+*/
 
+PStream& PStream::operator<<(int64_t x)
+{
+    switch(outmode)
+    {
+    case raw_binary:
+        write(reinterpret_cast<char *>(&x), sizeof(int64_t));
+        break;
+    case raw_ascii:
+    case pretty_ascii:
+        writeAsciiNum(x);
+        break;
+    case plearn_ascii:
+        writeAsciiNum(x);
+        put(' ');
+        break;
+    case plearn_binary:
+#ifdef BIGENDIAN
+        put((char)0x17);
+#else
+        put((char)0x16);
+#endif
+        write((char*)&x, sizeof(int64_t));
+        break;
+    default:
+        PLERROR("In PStream::operator<<  unknown outmode!!!!!!!!!");
+        break;
+    }
+    return *this;
+}
+
+/*
 PStream& PStream::operator<<(unsigned long x) 
 { 
     switch(outmode)
@@ -1814,7 +1945,39 @@
     }
     return *this;
 }
+*/
 
+PStream& PStream::operator<<(uint64_t x)
+{
+    switch(outmode)
+    {
+    case raw_binary:
+        write(reinterpret_cast<char *>(&x), sizeof(uint64_t));
+        break;
+    case raw_ascii:
+    case pretty_ascii:
+        writeAsciiNum(x);
+        break;
+    case plearn_ascii:
+        writeAsciiNum(x);
+        put(' ');
+        break;
+    case plearn_binary:
+#ifdef BIGENDIAN
+        put((char)0x19);
+#else
+        put((char)0x18);
+#endif
+        write((char*)&x, sizeof(uint64_t));
+        break;
+    default:
+        PLERROR("In PStream::operator<<  unknown outmode!!!!!!!!!");
+        break;
+    }
+    return *this;
+}
+
+/*
 PStream& PStream::operator<<(long long x) 
 { 
     switch(outmode)
@@ -1874,8 +2037,8 @@
     }
     return *this;
 }
+*/
 
-
 PStream& PStream::operator<<(short x) 
 { 
     switch(outmode)
@@ -2043,8 +2206,10 @@
     IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(unsigned short)
     IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(int)
     IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(unsigned int)
-    IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(long)
-    IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(unsigned long)
+    //IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(long)
+    //IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(unsigned long)
+    IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(int64_t)
+    IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(uint64_t)
 
 //! The binread_ for float and double are special
 

Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2007-07-09 22:15:27 UTC (rev 7720)
+++ trunk/plearn/io/PStream.h	2007-07-09 22:38:28 UTC (rev 7721)
@@ -41,10 +41,7 @@
 #include <set>
 #include <sstream>
 #include <fstream>
-/*
-// we use PRInt64 and PRUint64 for types from NSPR for 64 bit integers
-#include <nspr/prlong.h>
-*/
+#include <plearn/base/pstdint.h>
 #include <plearn/base/byte_order.h>
 #include <plearn/base/pl_hash_fun.h>
 #include <plearn/base/plerror.h>
@@ -460,13 +457,15 @@
     PStream& operator>>(signed char &x);
     PStream& operator>>(unsigned char &x);
     PStream& operator>>(int &x);
-    PStream& operator>>(unsigned int &x);  
-    PStream& operator>>(long &x);  
-    PStream& operator>>(unsigned long &x);
+    PStream& operator>>(unsigned int &x);
+    //PStream& operator>>(long &x);
+    //PStream& operator>>(unsigned long &x);
+    PStream& operator>>(int64_t &x);
+    PStream& operator>>(uint64_t &x);
     PStream& operator>>(short &x);
     PStream& operator>>(unsigned short &x);
-    PStream& operator>>(long long &x);
-    PStream& operator>>(unsigned long long &x);
+    //PStream& operator>>(long long &x);
+    //PStream& operator>>(unsigned long long &x);
     PStream& operator>>(pl_pstream_manip func) { return (*func)(*this); }
 
     // operator<<'s for base types
@@ -494,10 +493,12 @@
     PStream& operator<<(bool x);  
     PStream& operator<<(int x);
     PStream& operator<<(unsigned int x);
-    PStream& operator<<(long x);
-    PStream& operator<<(unsigned long x);
-    PStream& operator<<(long long x);
-    PStream& operator<<(unsigned long long x);
+    //PStream& operator<<(long x);
+    //PStream& operator<<(unsigned long x);
+    PStream& operator<<(int64_t x);
+    PStream& operator<<(uint64_t x);
+    //PStream& operator<<(long long x);
+    //PStream& operator<<(unsigned long long x);
     PStream& operator<<(short x);
     PStream& operator<<(unsigned short x);
     PStream& operator<<(pl_pstream_manip func) { return (*func)(*this); }
@@ -876,6 +877,7 @@
 inline void binwrite_(PStream& out, unsigned int* x, unsigned int n) 
 { out.write((char*)x, streamsize(n*sizeof(unsigned int))); }
 
+/*
 inline void binwrite_(PStream& out, const long* x, unsigned int n) 
 { out.write((char*)x, streamsize(n*sizeof(long))); }
 inline void binwrite_(PStream& out, long* x, unsigned int n) 
@@ -885,7 +887,18 @@
 { out.write((char*)x, streamsize(n*sizeof(unsigned long))); }
 inline void binwrite_(PStream& out, unsigned long* x, unsigned int n) 
 { out.write((char*)x, streamsize(n*sizeof(unsigned long))); }
+*/
 
+inline void binwrite_(PStream& out, const int64_t* x, unsigned int n) 
+{ out.write((char*)x, streamsize(n*sizeof(int64_t))); }
+inline void binwrite_(PStream& out, int64_t* x, unsigned int n) 
+{ out.write((char*)x, streamsize(n*sizeof(int64_t))); }
+
+inline void binwrite_(PStream& out, const uint64_t* x, unsigned int n) 
+{ out.write((char*)x, streamsize(n*sizeof(uint64_t))); }
+inline void binwrite_(PStream& out, uint64_t* x, unsigned int n) 
+{ out.write((char*)x, streamsize(n*sizeof(uint64_t))); }
+
 inline void binwrite_(PStream& out, const float* x, unsigned int n) 
 { out.write((char*)x, streamsize(n*sizeof(float))); }
 inline void binwrite_(PStream& out, float* x, unsigned int n) 
@@ -936,8 +949,10 @@
 void binread_(PStream& in, unsigned short* x, unsigned int n, unsigned char typecode);
 void binread_(PStream& in, int* x, unsigned int n, unsigned char typecode);
 void binread_(PStream& in, unsigned int* x, unsigned int n, unsigned char typecode);
-void binread_(PStream& in, long* x, unsigned int n, unsigned char typecode);
-void binread_(PStream& in, unsigned long* x, unsigned int n, unsigned char typecode);
+//void binread_(PStream& in, long* x, unsigned int n, unsigned char typecode);
+//void binread_(PStream& in, unsigned long* x, unsigned int n, unsigned char typecode);
+void binread_(PStream& in, int64_t* x, unsigned int n, unsigned char typecode);
+void binread_(PStream& in, uint64_t* x, unsigned int n, unsigned char typecode);
 void binread_(PStream& in, float* x, unsigned int n, unsigned char typecode);
 void binread_(PStream& in, double* x, unsigned int n, unsigned char typecode);
 



From lamblin at mail.berlios.de  Tue Jul 10 00:48:14 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 10 Jul 2007 00:48:14 +0200
Subject: [Plearn-commits] r7722 - trunk/plearn/base
Message-ID: <200707092248.l69MmE7k009283@sheep.berlios.de>

Author: lamblin
Date: 2007-07-10 00:48:13 +0200 (Tue, 10 Jul 2007)
New Revision: 7722

Added:
   trunk/plearn/base/pstdint.h
Log:
It might work better with this file...


Added: trunk/plearn/base/pstdint.h
===================================================================
--- trunk/plearn/base/pstdint.h	2007-07-09 22:38:28 UTC (rev 7721)
+++ trunk/plearn/base/pstdint.h	2007-07-09 22:48:13 UTC (rev 7722)
@@ -0,0 +1,679 @@
+/*  A portable stdint.h
+ *
+ *  Copyright (c) 2005-2007 Paul Hsieh
+ *
+ *  Redistribution and use in source and binary forms, with or without
+ *  modification, are permitted provided that the following conditions
+ *  are met:
+ *
+ *      Redistributions of source code must retain the above copyright
+ *      notice, this list of conditions and the following disclaimer.
+ *
+ *      Redistributions in binary form must not misrepresent the orignal
+ *      source in the documentation and/or other materials provided
+ *      with the distribution.
+ *
+ *      The names of the authors nor its contributors may be used to
+ *      endorse or promote products derived from this software without
+ *      specific prior written permission.
+ *
+ *  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+ *  FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+ *  COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+ *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ *  (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ *  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ *  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
+ *  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ *  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
+ *  OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ ****************************************************************************
+ *
+ *  Version 0.1.8
+ *
+ *  The ANSI C standard committee, for the C99 standard, specified the
+ *  inclusion of a new standard include file called stdint.h.  This is
+ *  a very useful and long desired include file which contains several
+ *  very precise definitions for integer scalar types that is
+ *  critically important for making portable several classes of
+ *  applications including cryptography, hashing, variable length
+ *  integer libraries and so on.  But for most developers its likely
+ *  useful just for programming sanity.
+ *
+ *  The problem is that most compiler vendors have decided not to
+ *  implement the C99 standard, and the next C++ language standard
+ *  (which has a lot more mindshare these days) will be a long time in
+ *  coming and its unknown whether or not it will include stdint.h or
+ *  how much adoption it will have.  Either way, it will be a long time
+ *  before all compilers come with a stdint.h and it also does nothing
+ *  for the extremely large number of compilers available today which
+ *  do not include this file, or anything comparable to it.
+ *
+ *  So that's what this file is all about.  Its an attempt to build a
+ *  single universal include file that works on as many platforms as
+ *  possible to deliver what stdint.h is supposed to.  A few things
+ *  that should be noted about this file:
+ *
+ *    1) It is not guaranteed to be portable and/or present an identical
+ *       interface on all platforms.  The extreme variability of the
+ *       ANSI C standard makes this an impossibility right from the
+ *       very get go. Its really only meant to be useful for the vast
+ *       majority of platforms that possess the capability of
+ *       implementing usefully and precisely defined, standard sized
+ *       integer scalars.  Systems which are not intrinsically 2s
+ *       complement may produce invalid constants.
+ *
+ *    2) There is an unavoidable use of non-reserved symbols.
+ *
+ *    3) Other standard include files are invoked.
+ *
+ *    4) This file may come in conflict with future platforms that do
+ *       include stdint.h.  The hope is that one or the other can be
+ *       used with no real difference.
+ *
+ *    5) In the current verison, if your platform can't represent
+ *       int32_t, int16_t and int8_t, it just dumps out with a compiler
+ *       error.
+ *
+ *    6) 64 bit integers may or may not be defined.  Test for their
+ *       presence with the test: #ifdef INT64_MAX or #ifdef UINT64_MAX.
+ *       Note that this is different from the C99 specification which
+ *       requires the existence of 64 bit support in the compiler.  If
+ *       this is not defined for your platform, yet it is capable of
+ *       dealing with 64 bits then it is because this file has not yet
+ *       been extended to cover all of your system's capabilities.
+ *
+ *    7) (u)intptr_t may or may not be defined.  Test for its presence
+ *       with the test: #ifdef PTRDIFF_MAX.  If this is not defined
+ *       for your platform, then it is because this file has not yet
+ *       been extended to cover all of your system's capabilities, not
+ *       because its optional.
+ *
+ *    8) The following might not been defined even if your platform is
+ *       capable of defining it:
+ *
+ *       WCHAR_MIN
+ *       WCHAR_MAX
+ *       (u)int64_t
+ *       PTRDIFF_MIN
+ *       PTRDIFF_MAX
+ *       (u)intptr_t
+ *
+ *    9) The following have not been defined:
+ *
+ *       WINT_MIN
+ *       WINT_MAX
+ *
+ *   10) The criteria for defining (u)int_least(*)_t isn't clear,
+ *       except for systems which don't have a type that precisely
+ *       defined 8, 16, or 32 bit types (which this include file does
+ *       not support anyways). Default definitions have been given.
+ *
+ *   11) The criteria for defining (u)int_fast(*)_t isn't something I
+ *       would trust to any particular compiler vendor or the ANSI C
+ *       committee.  It is well known that "compatible systems" are
+ *       commonly created that have very different performance
+ *       characteristics from the systems they are compatible with,
+ *       especially those whose vendors make both the compiler and the
+ *       system.  Default definitions have been given, but its strongly
+ *       recommended that users never use these definitions for any
+ *       reason (they do *NOT* deliver any serious guarantee of
+ *       improved performance -- not in this file, nor any vendor's
+ *       stdint.h).
+ *
+ *   12) The following macros:
+ *
+ *       PRINTF_INTMAX_MODIFIER
+ *       PRINTF_INT64_MODIFIER
+ *       PRINTF_INT32_MODIFIER
+ *       PRINTF_INT16_MODIFIER
+ *       PRINTF_LEAST64_MODIFIER
+ *       PRINTF_LEAST32_MODIFIER
+ *       PRINTF_LEAST16_MODIFIER
+ *       PRINTF_INTPTR_MODIFIER
+ *
+ *       are strings which have been defined as the modifiers required
+ *       for the "d", "u" and "x" printf formats to correctly output
+ *       (u)intmax_t, (u)int64_t, (u)int32_t, (u)int16_t, (u)least64_t,
+ *       (u)least32_t, (u)least16_t and (u)intptr_t types respectively.
+ *       PRINTF_INTPTR_MODIFIER is not defined for some systems which
+ *       provide their own stdint.h.  PRINTF_INT64_MODIFIER is not
+ *       defined if INT64_MAX is not defined.  These are an extension
+ *       beyond what C99 specifies must be in stdint.h.
+ *
+ *       In addition, the following macros are defined:
+ *
+ *       PRINTF_INTMAX_HEX_WIDTH
+ *       PRINTF_INT64_HEX_WIDTH
+ *       PRINTF_INT32_HEX_WIDTH
+ *       PRINTF_INT16_HEX_WIDTH
+ *       PRINTF_INT8_HEX_WIDTH
+ *       PRINTF_INTMAX_DEC_WIDTH
+ *       PRINTF_INT64_DEC_WIDTH
+ *       PRINTF_INT32_DEC_WIDTH
+ *       PRINTF_INT16_DEC_WIDTH
+ *       PRINTF_INT8_DEC_WIDTH
+ *
+ *       Which specifies the maximum number of characters required to
+ *       print the number of that type in either hexadecimal or decimal.
+ *       These are an extension beyond what C99 specifies must be in
+ *       stdint.h.
+ *
+ *  Compilers tested (all with 0 warnings at their highest respective
+ *  settings): Borland Turbo C 2.0, WATCOM C/C++ 11.0 (16 bits and 32
+ *  bits), Microsoft Visual C++ 6.0 (32 bit), Microsoft Visual Studio
+ *  .net (VC7), Intel C++ 4.0, GNU gcc v3.3.3
+ *
+ *  This file should be considered a work in progress.  Suggestions for
+ *  improvements, especially those which increase coverage are strongly
+ *  encouraged.
+ *
+ *  Acknowledgements
+ *
+ *  The following people have made significant contributions to the
+ *  development and testing of this file:
+ *
+ *  Chris Howie
+ *  John Steele Scott
+ *  Dave Thorup
+ *
+ */
+
+#include <stddef.h>
+#include <limits.h>
+#include <signal.h>
+
+/*
+ *  For gcc with _STDINT_H, fill in the PRINTF_INT*_MODIFIER macros, and
+ *  do nothing else.  On the Mac OS X version of gcc this is _STDINT_H_.
+ */
+
+#if ((defined(__STDC__) && __STDC__ && __STDC_VERSION__ >= 199901L) || (defined (__WATCOMC__) && (defined (_STDINT_H_INCLUDED) || __WATCOMC__ >= 1250)) || (defined(__GNUC__) && (defined(_STDINT_H) || defined(_STDINT_H_)) )) && !defined (_PSTDINT_H_INCLUDED)
+#include <stdint.h>
+#define _PSTDINT_H_INCLUDED
+# ifndef PRINTF_INT64_MODIFIER
+#  define PRINTF_INT64_MODIFIER "ll"
+# endif
+# ifndef PRINTF_INT32_MODIFIER
+#  define PRINTF_INT32_MODIFIER "l"
+# endif
+# ifndef PRINTF_INT16_MODIFIER
+#  define PRINTF_INT16_MODIFIER "h"
+# endif
+# ifndef PRINTF_INTMAX_MODIFIER
+#  define PRINTF_INTMAX_MODIFIER PRINTF_INT64_MODIFIER
+# endif
+# ifndef PRINTF_INT64_HEX_WIDTH
+#  define PRINTF_INT64_HEX_WIDTH "16"
+# endif
+# ifndef PRINTF_INT32_HEX_WIDTH
+#  define PRINTF_INT32_HEX_WIDTH "8"
+# endif
+# ifndef PRINTF_INT16_HEX_WIDTH
+#  define PRINTF_INT16_HEX_WIDTH "4"
+# endif
+# ifndef PRINTF_INT8_HEX_WIDTH
+#  define PRINTF_INT8_HEX_WIDTH "2"
+# endif
+# ifndef PRINTF_INT64_DEC_WIDTH
+#  define PRINTF_INT64_DEC_WIDTH "20"
+# endif
+# ifndef PRINTF_INT32_DEC_WIDTH
+#  define PRINTF_INT32_DEC_WIDTH "10"
+# endif
+# ifndef PRINTF_INT16_DEC_WIDTH
+#  define PRINTF_INT16_DEC_WIDTH "5"
+# endif
+# ifndef PRINTF_INT8_DEC_WIDTH
+#  define PRINTF_INT8_DEC_WIDTH "3"
+# endif
+# ifndef PRINTF_INTMAX_HEX_WIDTH
+#  define PRINTF_INTMAX_HEX_WIDTH PRINTF_INT64_HEX_WIDTH
+# endif
+# ifndef PRINTF_INTMAX_DEC_WIDTH
+#  define PRINTF_INTMAX_DEC_WIDTH PRINTF_INT64_DEC_WIDTH
+# endif
+#endif
+
+#ifndef _PSTDINT_H_INCLUDED
+#define _PSTDINT_H_INCLUDED
+
+#ifndef SIZE_MAX
+# define SIZE_MAX (~(size_t)0)
+#endif
+
+/*
+ *  Deduce the type assignments from limits.h under the assumption that
+ *  integer sizes in bits are powers of 2, and follow the ANSI
+ *  definitions.
+ */
+
+#ifndef UINT8_MAX
+# define UINT8_MAX 0xff
+#endif
+#ifndef uint8_t
+# if (UCHAR_MAX == UINT8_MAX) || defined (S_SPLINT_S)
+    typedef unsigned char uint8_t;
+#   define UINT8_C(v) ((uint8_t) v)
+# else
+#   error "Platform not supported"
+# endif
+#endif
+
+#ifndef INT8_MAX
+# define INT8_MAX 0x7f
+#endif
+#ifndef INT8_MIN
+# define INT8_MIN INT8_C(0x80)
+#endif
+#ifndef int8_t
+# if (SCHAR_MAX == INT8_MAX) || defined (S_SPLINT_S)
+    typedef signed char int8_t;
+#   define INT8_C(v) ((int8_t) v)
+# else
+#   error "Platform not supported"
+# endif
+#endif
+
+#ifndef UINT16_MAX
+# define UINT16_MAX 0xffff
+#endif
+#ifndef uint16_t
+#if (UINT_MAX == UINT16_MAX) || defined (S_SPLINT_S)
+  typedef unsigned int uint16_t;
+# ifndef PRINTF_INT16_MODIFIER
+#  define PRINTF_INT16_MODIFIER ""
+# endif
+# define UINT16_C(v) ((uint16_t) (v))
+#elif (USHRT_MAX == UINT16_MAX)
+  typedef unsigned short uint16_t;
+# define UINT16_C(v) ((uint16_t) (v))
+# ifndef PRINTF_INT16_MODIFIER
+#  define PRINTF_INT16_MODIFIER "h"
+# endif
+#else
+#error "Platform not supported"
+#endif
+#endif
+
+#ifndef INT16_MAX
+# define INT16_MAX 0x7fff
+#endif
+#ifndef INT16_MIN
+# define INT16_MIN INT16_C(0x8000)
+#endif
+#ifndef int16_t
+#if (INT_MAX == INT16_MAX) || defined (S_SPLINT_S)
+  typedef signed int int16_t;
+# define INT16_C(v) ((int16_t) (v))
+# ifndef PRINTF_INT16_MODIFIER
+#  define PRINTF_INT16_MODIFIER ""
+# endif
+#elif (SHRT_MAX == INT16_MAX)
+  typedef signed short int16_t;
+# define INT16_C(v) ((int16_t) (v))
+# ifndef PRINTF_INT16_MODIFIER
+#  define PRINTF_INT16_MODIFIER "h"
+# endif
+#else
+#error "Platform not supported"
+#endif
+#endif
+
+#ifndef UINT32_MAX
+# define UINT32_MAX (0xffffffffUL)
+#endif
+#ifndef uint32_t
+#if (ULONG_MAX == UINT32_MAX) || defined (S_SPLINT_S)
+  typedef unsigned long uint32_t;
+# define UINT32_C(v) v ## UL
+# ifndef PRINTF_INT32_MODIFIER
+#  define PRINTF_INT32_MODIFIER "l"
+# endif
+#elif (UINT_MAX == UINT32_MAX)
+  typedef unsigned int uint32_t;
+# ifndef PRINTF_INT32_MODIFIER
+#  define PRINTF_INT32_MODIFIER ""
+# endif
+# define UINT32_C(v) v ## U
+#elif (USHRT_MAX == UINT32_MAX)
+  typedef unsigned short uint32_t;
+# define UINT32_C(v) ((unsigned short) (v))
+# ifndef PRINTF_INT32_MODIFIER
+#  define PRINTF_INT32_MODIFIER ""
+# endif
+#else
+#error "Platform not supported"
+#endif
+#endif
+
+#ifndef INT32_MAX
+# define INT32_MAX (0x7fffffffL)
+#endif
+#ifndef INT32_MIN
+# define INT32_MIN INT32_C(0x80000000)
+#endif
+#ifndef int32_t
+#if (LONG_MAX == INT32_MAX) || defined (S_SPLINT_S)
+  typedef signed long int32_t;
+# define INT32_C(v) v ## L
+# ifndef PRINTF_INT32_MODIFIER
+#  define PRINTF_INT32_MODIFIER "l"
+# endif
+#elif (INT_MAX == INT32_MAX)
+  typedef signed int int32_t;
+# define INT32_C(v) v
+# ifndef PRINTF_INT32_MODIFIER
+#  define PRINTF_INT32_MODIFIER ""
+# endif
+#elif (SHRT_MAX == INT32_MAX)
+  typedef signed short int32_t;
+# define INT32_C(v) ((short) (v))
+# ifndef PRINTF_INT32_MODIFIER
+#  define PRINTF_INT32_MODIFIER ""
+# endif
+#else
+#error "Platform not supported"
+#endif
+#endif
+
+/*
+ *  The macro stdint_int64_defined is temporarily used to record
+ *  whether or not 64 integer support is available.  It must be
+ *  defined for any 64 integer extensions for new platforms that are
+ *  added.
+ */
+
+#undef stdint_int64_defined
+#if (defined(__STDC__) && defined(__STDC_VERSION__)) || defined (S_SPLINT_S)
+# if (__STDC__ && __STDC_VERSION >= 199901L) || defined (S_SPLINT_S)
+#  define stdint_int64_defined
+   typedef long long int64_t;
+   typedef unsigned long long uint64_t;
+#  define UINT64_C(v) v ## ULL
+#  define  INT64_C(v) v ## LL
+#  ifndef PRINTF_INT64_MODIFIER
+#   define PRINTF_INT64_MODIFIER "ll"
+#  endif
+# endif
+#endif
+
+#if !defined (stdint_int64_defined)
+# if defined(__GNUC__)
+#  define stdint_int64_defined
+   __extension__ typedef long long int64_t;
+   __extension__ typedef unsigned long long uint64_t;
+#  define UINT64_C(v) v ## ULL
+#  define  INT64_C(v) v ## LL
+#  ifndef PRINTF_INT64_MODIFIER
+#   define PRINTF_INT64_MODIFIER "ll"
+#  endif
+# elif defined(__MWERKS__) || defined (__SUNPRO_C) || defined (__SUNPRO_CC) || defined (__APPLE_CC__) || defined (_LONG_LONG) || defined (_CRAYC) || defined (S_SPLINT_S)
+#  define stdint_int64_defined
+   typedef long long int64_t;
+   typedef unsigned long long uint64_t;
+#  define UINT64_C(v) v ## ULL
+#  define  INT64_C(v) v ## LL
+#  ifndef PRINTF_INT64_MODIFIER
+#   define PRINTF_INT64_MODIFIER "ll"
+#  endif
+# elif (defined(__WATCOMC__) && defined(__WATCOM_INT64__)) || (defined(_MSC_VER) && _INTEGRAL_MAX_BITS >= 64) || (defined (__BORLANDC__) && __BORLANDC__ > 0x460) || defined (__alpha) || defined (__DECC)
+#  define stdint_int64_defined
+   typedef __int64 int64_t;
+   typedef unsigned __int64 uint64_t;
+#  define UINT64_C(v) v ## UI64
+#  define  INT64_C(v) v ## I64
+#  ifndef PRINTF_INT64_MODIFIER
+#   define PRINTF_INT64_MODIFIER "I64"
+#  endif
+# endif
+#endif
+
+#if !defined (LONG_LONG_MAX) && defined (INT64_C)
+# define LONG_LONG_MAX INT64_C (9223372036854775807)
+#endif
+#ifndef ULONG_LONG_MAX
+# define ULONG_LONG_MAX UINT64_C (18446744073709551615)
+#endif
+
+#if !defined (INT64_MAX) && defined (INT64_C)
+# define INT64_MAX INT64_C (9223372036854775807)
+#endif
+#if !defined (INT64_MIN) && defined (INT64_C)
+# define INT64_MIN INT64_C (-9223372036854775808)
+#endif
+#if !defined (UINT64_MAX) && defined (INT64_C)
+# define UINT64_MAX UINT64_C (18446744073709551615)
+#endif
+
+/*
+ *  Width of hexadecimal for number field.
+ */
+
+#ifndef PRINTF_INT64_HEX_WIDTH
+# define PRINTF_INT64_HEX_WIDTH "16"
+#endif
+#ifndef PRINTF_INT32_HEX_WIDTH
+# define PRINTF_INT32_HEX_WIDTH "8"
+#endif
+#ifndef PRINTF_INT16_HEX_WIDTH
+# define PRINTF_INT16_HEX_WIDTH "4"
+#endif
+#ifndef PRINTF_INT8_HEX_WIDTH
+# define PRINTF_INT8_HEX_WIDTH "2"
+#endif
+
+#ifndef PRINTF_INT64_DEC_WIDTH
+# define PRINTF_INT64_DEC_WIDTH "20"
+#endif
+#ifndef PRINTF_INT32_DEC_WIDTH
+# define PRINTF_INT32_DEC_WIDTH "10"
+#endif
+#ifndef PRINTF_INT16_DEC_WIDTH
+# define PRINTF_INT16_DEC_WIDTH "5"
+#endif
+#ifndef PRINTF_INT8_DEC_WIDTH
+# define PRINTF_INT8_DEC_WIDTH "3"
+#endif
+
+/*
+ *  Ok, lets not worry about 128 bit integers for now.  Moore's law says
+ *  we don't need to worry about that until about 2040 at which point
+ *  we'll have bigger things to worry about.
+ */
+
+#ifdef stdint_int64_defined
+  typedef int64_t intmax_t;
+  typedef uint64_t uintmax_t;
+# define  INTMAX_MAX   INT64_MAX
+# define  INTMAX_MIN   INT64_MIN
+# define UINTMAX_MAX  UINT64_MAX
+# define UINTMAX_C(v) UINT64_C(v)
+# define  INTMAX_C(v)  INT64_C(v)
+# ifndef PRINTF_INTMAX_MODIFIER
+#   define PRINTF_INTMAX_MODIFIER PRINTF_INT64_MODIFIER
+# endif
+# ifndef PRINTF_INTMAX_HEX_WIDTH
+#  define PRINTF_INTMAX_HEX_WIDTH PRINTF_INT64_HEX_WIDTH
+# endif
+# ifndef PRINTF_INTMAX_DEC_WIDTH
+#  define PRINTF_INTMAX_DEC_WIDTH PRINTF_INT64_DEC_WIDTH
+# endif
+#else
+  typedef int32_t intmax_t;
+  typedef uint32_t uintmax_t;
+# define  INTMAX_MAX   INT32_MAX
+# define UINTMAX_MAX  UINT32_MAX
+# define UINTMAX_C(v) UINT32_C(v)
+# define  INTMAX_C(v)  INT32_C(v)
+# ifndef PRINTF_INTMAX_MODIFIER
+#   define PRINTF_INTMAX_MODIFIER PRINTF_INT32_MODIFIER
+# endif
+# ifndef PRINTF_INTMAX_HEX_WIDTH
+#  define PRINTF_INTMAX_HEX_WIDTH PRINTF_INT32_HEX_WIDTH
+# endif
+# ifndef PRINTF_INTMAX_DEC_WIDTH
+#  define PRINTF_INTMAX_DEC_WIDTH PRINTF_INT32_DEC_WIDTH
+# endif
+#endif
+
+/*
+ *  Because this file currently only supports platforms which have
+ *  precise powers of 2 as bit sizes for the default integers, the
+ *  least definitions are all trivial.  Its possible that a future
+ *  version of this file could have different definitions.
+ */
+
+#ifndef stdint_least_defined
+  typedef   int8_t   int_least8_t;
+  typedef  uint8_t  uint_least8_t;
+  typedef  int16_t  int_least16_t;
+  typedef uint16_t uint_least16_t;
+  typedef  int32_t  int_least32_t;
+  typedef uint32_t uint_least32_t;
+# define PRINTF_LEAST32_MODIFIER PRINTF_INT32_MODIFIER
+# define PRINTF_LEAST16_MODIFIER PRINTF_INT16_MODIFIER
+# define  UINT_LEAST8_MAX  UINT8_MAX
+# define   INT_LEAST8_MAX   INT8_MAX
+# define UINT_LEAST16_MAX UINT16_MAX
+# define  INT_LEAST16_MAX  INT16_MAX
+# define UINT_LEAST32_MAX UINT32_MAX
+# define  INT_LEAST32_MAX  INT32_MAX
+# define   INT_LEAST8_MIN   INT8_MIN
+# define  INT_LEAST16_MIN  INT16_MIN
+# define  INT_LEAST32_MIN  INT32_MIN
+# ifdef stdint_int64_defined
+    typedef  int64_t  int_least64_t;
+    typedef uint64_t uint_least64_t;
+#   define PRINTF_LEAST64_MODIFIER PRINTF_INT64_MODIFIER
+#   define UINT_LEAST64_MAX UINT64_MAX
+#   define  INT_LEAST64_MAX  INT64_MAX
+#   define  INT_LEAST64_MIN  INT64_MIN
+# endif
+#endif
+#undef stdint_least_defined
+
+/*
+ *  The ANSI C committee pretending to know or specify anything about
+ *  performance is the epitome of misguided arrogance.  The mandate of
+ *  this file is to *ONLY* ever support that absolute minimum
+ *  definition of the fast integer types, for compatibility purposes.
+ *  No extensions, and no attempt to suggest what may or may not be a
+ *  faster integer type will ever be made in this file.  Developers are
+ *  warned to stay away from these types when using this or any other
+ *  stdint.h.
+ */
+
+typedef   int_least8_t   int_fast8_t;
+typedef  uint_least8_t  uint_fast8_t;
+typedef  int_least16_t  int_fast16_t;
+typedef uint_least16_t uint_fast16_t;
+typedef  int_least32_t  int_fast32_t;
+typedef uint_least32_t uint_fast32_t;
+#define  UINT_FAST8_MAX  UINT_LEAST8_MAX
+#define   INT_FAST8_MAX   INT_LEAST8_MAX
+#define UINT_FAST16_MAX UINT_LEAST16_MAX
+#define  INT_FAST16_MAX  INT_LEAST16_MAX
+#define UINT_FAST32_MAX UINT_LEAST32_MAX
+#define  INT_FAST32_MAX  INT_LEAST32_MAX
+#define   INT_FAST8_MIN   IN_LEASTT8_MIN
+#define  INT_FAST16_MIN  INT_LEAST16_MIN
+#define  INT_FAST32_MIN  INT_LEAST32_MIN
+#ifdef stdint_int64_defined
+  typedef  int_least64_t  int_fast64_t;
+  typedef uint_least64_t uint_fast64_t;
+# define UINT_FAST64_MAX UINT_LEAST64_MAX
+# define  INT_FAST64_MAX  INT_LEAST64_MAX
+# define  INT_FAST64_MIN  INT_LEAST64_MIN
+#endif
+
+#undef stdint_int64_defined
+
+/*
+ *  Whatever piecemeal, per compiler thing we can do about the wchar_t
+ *  type limits.
+ */
+
+#if defined(__WATCOMC__) || defined(_MSC_VER) || defined (__GNUC__)
+# include <wchar.h>
+# ifndef WCHAR_MIN
+#  define WCHAR_MIN 0
+# endif
+# ifndef WCHAR_MAX
+#  define WCHAR_MAX ((wchar_t)-1)
+# endif
+#endif
+
+/*
+ *  Whatever piecemeal, per compiler/platform thing we can do about the
+ *  (u)intptr_t types and limits.
+ */
+
+#if defined (_MSC_VER) && defined (_UINTPTR_T_DEFINED)
+# define STDINT_H_UINTPTR_T_DEFINED
+#endif
+
+#ifndef STDINT_H_UINTPTR_T_DEFINED
+# if defined (__alpha__) || defined (__ia64__) || defined (__x86_64__) || defined (_WIN64)
+#  define stdint_intptr_bits 64
+# elif defined (__WATCOMC__) || defined (__TURBOC__)
+#  if defined(__TINY__) || defined(__SMALL__) || defined(__MEDIUM__)
+#    define stdint_intptr_bits 16
+#  else
+#    define stdint_intptr_bits 32
+#  endif
+# elif defined (__i386__) || defined (_WIN32) || defined (WIN32)
+#  define stdint_intptr_bits 32
+# elif defined (__INTEL_COMPILER)
+/* TODO -- what will Intel do about x86-64? */
+# endif
+
+# ifdef stdint_intptr_bits
+#  define stdint_intptr_glue3_i(a,b,c)  a##b##c
+#  define stdint_intptr_glue3(a,b,c)    stdint_intptr_glue3_i(a,b,c)
+#  ifndef PRINTF_INTPTR_MODIFIER
+#    define PRINTF_INTPTR_MODIFIER      stdint_intptr_glue3(PRINTF_INT,stdint_intptr_bits,_MODIFIER)
+#  endif
+#  ifndef PTRDIFF_MAX
+#    define PTRDIFF_MAX                 stdint_intptr_glue3(INT,stdint_intptr_bits,_MAX)
+#  endif
+#  ifndef PTRDIFF_MIN
+#    define PTRDIFF_MIN                 stdint_intptr_glue3(INT,stdint_intptr_bits,_MIN)
+#  endif
+#  ifndef UINTPTR_MAX
+#    define UINTPTR_MAX                 stdint_intptr_glue3(UINT,stdint_intptr_bits,_MAX)
+#  endif
+#  ifndef INTPTR_MAX
+#    define INTPTR_MAX                  stdint_intptr_glue3(INT,stdint_intptr_bits,_MAX)
+#  endif
+#  ifndef INTPTR_MIN
+#    define INTPTR_MIN                  stdint_intptr_glue3(INT,stdint_intptr_bits,_MIN)
+#  endif
+#  ifndef INTPTR_C
+#    define INTPTR_C(x)                 stdint_intptr_glue3(INT,stdint_intptr_bits,_C)(x)
+#  endif
+#  ifndef UINTPTR_C
+#    define UINTPTR_C(x)                stdint_intptr_glue3(UINT,stdint_intptr_bits,_C)(x)
+#  endif
+  typedef stdint_intptr_glue3(uint,stdint_intptr_bits,_t) uintptr_t;
+  typedef stdint_intptr_glue3( int,stdint_intptr_bits,_t)  intptr_t;
+# else
+/* TODO -- This following is likely wrong for some platforms, and does
+   nothing for the definition of uintptr_t. */
+  typedef ptrdiff_t intptr_t;
+# endif
+# define STDINT_H_UINTPTR_T_DEFINED
+#endif
+
+/*
+ *  Assumes sig_atomic_t is signed and we have a 2s complement machine.
+ */
+
+#ifndef SIG_ATOMIC_MAX
+# define SIG_ATOMIC_MAX ((((sig_atomic_t) 1) << (sizeof (sig_atomic_t)*CHAR_BIT-1)) - 1)
+#endif
+
+#endif



From lamblin at mail.berlios.de  Tue Jul 10 00:55:24 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 10 Jul 2007 00:55:24 +0200
Subject: [Plearn-commits] r7723 - trunk/plearn/base
Message-ID: <200707092255.l69MtORi014116@sheep.berlios.de>

Author: lamblin
Date: 2007-07-10 00:55:23 +0200 (Tue, 10 Jul 2007)
New Revision: 7723

Modified:
   trunk/plearn/base/pstdint.h
Log:
dos2unix


Modified: trunk/plearn/base/pstdint.h
===================================================================
--- trunk/plearn/base/pstdint.h	2007-07-09 22:48:13 UTC (rev 7722)
+++ trunk/plearn/base/pstdint.h	2007-07-09 22:55:23 UTC (rev 7723)
@@ -1,679 +1,679 @@
-/*  A portable stdint.h
- *
- *  Copyright (c) 2005-2007 Paul Hsieh
- *
- *  Redistribution and use in source and binary forms, with or without
- *  modification, are permitted provided that the following conditions
- *  are met:
- *
- *      Redistributions of source code must retain the above copyright
- *      notice, this list of conditions and the following disclaimer.
- *
- *      Redistributions in binary form must not misrepresent the orignal
- *      source in the documentation and/or other materials provided
- *      with the distribution.
- *
- *      The names of the authors nor its contributors may be used to
- *      endorse or promote products derived from this software without
- *      specific prior written permission.
- *
- *  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- *  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- *  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
- *  FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
- *  COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
- *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
- *  (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
- *  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
- *  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
- *  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- *  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
- *  OF THE POSSIBILITY OF SUCH DAMAGE.
- *
- ****************************************************************************
- *
- *  Version 0.1.8
- *
- *  The ANSI C standard committee, for the C99 standard, specified the
- *  inclusion of a new standard include file called stdint.h.  This is
- *  a very useful and long desired include file which contains several
- *  very precise definitions for integer scalar types that is
- *  critically important for making portable several classes of
- *  applications including cryptography, hashing, variable length
- *  integer libraries and so on.  But for most developers its likely
- *  useful just for programming sanity.
- *
- *  The problem is that most compiler vendors have decided not to
- *  implement the C99 standard, and the next C++ language standard
- *  (which has a lot more mindshare these days) will be a long time in
- *  coming and its unknown whether or not it will include stdint.h or
- *  how much adoption it will have.  Either way, it will be a long time
- *  before all compilers come with a stdint.h and it also does nothing
- *  for the extremely large number of compilers available today which
- *  do not include this file, or anything comparable to it.
- *
- *  So that's what this file is all about.  Its an attempt to build a
- *  single universal include file that works on as many platforms as
- *  possible to deliver what stdint.h is supposed to.  A few things
- *  that should be noted about this file:
- *
- *    1) It is not guaranteed to be portable and/or present an identical
- *       interface on all platforms.  The extreme variability of the
- *       ANSI C standard makes this an impossibility right from the
- *       very get go. Its really only meant to be useful for the vast
- *       majority of platforms that possess the capability of
- *       implementing usefully and precisely defined, standard sized
- *       integer scalars.  Systems which are not intrinsically 2s
- *       complement may produce invalid constants.
- *
- *    2) There is an unavoidable use of non-reserved symbols.
- *
- *    3) Other standard include files are invoked.
- *
- *    4) This file may come in conflict with future platforms that do
- *       include stdint.h.  The hope is that one or the other can be
- *       used with no real difference.
- *
- *    5) In the current verison, if your platform can't represent
- *       int32_t, int16_t and int8_t, it just dumps out with a compiler
- *       error.
- *
- *    6) 64 bit integers may or may not be defined.  Test for their
- *       presence with the test: #ifdef INT64_MAX or #ifdef UINT64_MAX.
- *       Note that this is different from the C99 specification which
- *       requires the existence of 64 bit support in the compiler.  If
- *       this is not defined for your platform, yet it is capable of
- *       dealing with 64 bits then it is because this file has not yet
- *       been extended to cover all of your system's capabilities.
- *
- *    7) (u)intptr_t may or may not be defined.  Test for its presence
- *       with the test: #ifdef PTRDIFF_MAX.  If this is not defined
- *       for your platform, then it is because this file has not yet
- *       been extended to cover all of your system's capabilities, not
- *       because its optional.
- *
- *    8) The following might not been defined even if your platform is
- *       capable of defining it:
- *
- *       WCHAR_MIN
- *       WCHAR_MAX
- *       (u)int64_t
- *       PTRDIFF_MIN
- *       PTRDIFF_MAX
- *       (u)intptr_t
- *
- *    9) The following have not been defined:
- *
- *       WINT_MIN
- *       WINT_MAX
- *
- *   10) The criteria for defining (u)int_least(*)_t isn't clear,
- *       except for systems which don't have a type that precisely
- *       defined 8, 16, or 32 bit types (which this include file does
- *       not support anyways). Default definitions have been given.
- *
- *   11) The criteria for defining (u)int_fast(*)_t isn't something I
- *       would trust to any particular compiler vendor or the ANSI C
- *       committee.  It is well known that "compatible systems" are
- *       commonly created that have very different performance
- *       characteristics from the systems they are compatible with,
- *       especially those whose vendors make both the compiler and the
- *       system.  Default definitions have been given, but its strongly
- *       recommended that users never use these definitions for any
- *       reason (they do *NOT* deliver any serious guarantee of
- *       improved performance -- not in this file, nor any vendor's
- *       stdint.h).
- *
- *   12) The following macros:
- *
- *       PRINTF_INTMAX_MODIFIER
- *       PRINTF_INT64_MODIFIER
- *       PRINTF_INT32_MODIFIER
- *       PRINTF_INT16_MODIFIER
- *       PRINTF_LEAST64_MODIFIER
- *       PRINTF_LEAST32_MODIFIER
- *       PRINTF_LEAST16_MODIFIER
- *       PRINTF_INTPTR_MODIFIER
- *
- *       are strings which have been defined as the modifiers required
- *       for the "d", "u" and "x" printf formats to correctly output
- *       (u)intmax_t, (u)int64_t, (u)int32_t, (u)int16_t, (u)least64_t,
- *       (u)least32_t, (u)least16_t and (u)intptr_t types respectively.
- *       PRINTF_INTPTR_MODIFIER is not defined for some systems which
- *       provide their own stdint.h.  PRINTF_INT64_MODIFIER is not
- *       defined if INT64_MAX is not defined.  These are an extension
- *       beyond what C99 specifies must be in stdint.h.
- *
- *       In addition, the following macros are defined:
- *
- *       PRINTF_INTMAX_HEX_WIDTH
- *       PRINTF_INT64_HEX_WIDTH
- *       PRINTF_INT32_HEX_WIDTH
- *       PRINTF_INT16_HEX_WIDTH
- *       PRINTF_INT8_HEX_WIDTH
- *       PRINTF_INTMAX_DEC_WIDTH
- *       PRINTF_INT64_DEC_WIDTH
- *       PRINTF_INT32_DEC_WIDTH
- *       PRINTF_INT16_DEC_WIDTH
- *       PRINTF_INT8_DEC_WIDTH
- *
- *       Which specifies the maximum number of characters required to
- *       print the number of that type in either hexadecimal or decimal.
- *       These are an extension beyond what C99 specifies must be in
- *       stdint.h.
- *
- *  Compilers tested (all with 0 warnings at their highest respective
- *  settings): Borland Turbo C 2.0, WATCOM C/C++ 11.0 (16 bits and 32
- *  bits), Microsoft Visual C++ 6.0 (32 bit), Microsoft Visual Studio
- *  .net (VC7), Intel C++ 4.0, GNU gcc v3.3.3
- *
- *  This file should be considered a work in progress.  Suggestions for
- *  improvements, especially those which increase coverage are strongly
- *  encouraged.
- *
- *  Acknowledgements
- *
- *  The following people have made significant contributions to the
- *  development and testing of this file:
- *
- *  Chris Howie
- *  John Steele Scott
- *  Dave Thorup
- *
- */
-
-#include <stddef.h>
-#include <limits.h>
-#include <signal.h>
-
-/*
- *  For gcc with _STDINT_H, fill in the PRINTF_INT*_MODIFIER macros, and
- *  do nothing else.  On the Mac OS X version of gcc this is _STDINT_H_.
- */
-
-#if ((defined(__STDC__) && __STDC__ && __STDC_VERSION__ >= 199901L) || (defined (__WATCOMC__) && (defined (_STDINT_H_INCLUDED) || __WATCOMC__ >= 1250)) || (defined(__GNUC__) && (defined(_STDINT_H) || defined(_STDINT_H_)) )) && !defined (_PSTDINT_H_INCLUDED)
-#include <stdint.h>
-#define _PSTDINT_H_INCLUDED
-# ifndef PRINTF_INT64_MODIFIER
-#  define PRINTF_INT64_MODIFIER "ll"
-# endif
-# ifndef PRINTF_INT32_MODIFIER
-#  define PRINTF_INT32_MODIFIER "l"
-# endif
-# ifndef PRINTF_INT16_MODIFIER
-#  define PRINTF_INT16_MODIFIER "h"
-# endif
-# ifndef PRINTF_INTMAX_MODIFIER
-#  define PRINTF_INTMAX_MODIFIER PRINTF_INT64_MODIFIER
-# endif
-# ifndef PRINTF_INT64_HEX_WIDTH
-#  define PRINTF_INT64_HEX_WIDTH "16"
-# endif
-# ifndef PRINTF_INT32_HEX_WIDTH
-#  define PRINTF_INT32_HEX_WIDTH "8"
-# endif
-# ifndef PRINTF_INT16_HEX_WIDTH
-#  define PRINTF_INT16_HEX_WIDTH "4"
-# endif
-# ifndef PRINTF_INT8_HEX_WIDTH
-#  define PRINTF_INT8_HEX_WIDTH "2"
-# endif
-# ifndef PRINTF_INT64_DEC_WIDTH
-#  define PRINTF_INT64_DEC_WIDTH "20"
-# endif
-# ifndef PRINTF_INT32_DEC_WIDTH
-#  define PRINTF_INT32_DEC_WIDTH "10"
-# endif
-# ifndef PRINTF_INT16_DEC_WIDTH
-#  define PRINTF_INT16_DEC_WIDTH "5"
-# endif
-# ifndef PRINTF_INT8_DEC_WIDTH
-#  define PRINTF_INT8_DEC_WIDTH "3"
-# endif
-# ifndef PRINTF_INTMAX_HEX_WIDTH
-#  define PRINTF_INTMAX_HEX_WIDTH PRINTF_INT64_HEX_WIDTH
-# endif
-# ifndef PRINTF_INTMAX_DEC_WIDTH
-#  define PRINTF_INTMAX_DEC_WIDTH PRINTF_INT64_DEC_WIDTH
-# endif
-#endif
-
-#ifndef _PSTDINT_H_INCLUDED
-#define _PSTDINT_H_INCLUDED
-
-#ifndef SIZE_MAX
-# define SIZE_MAX (~(size_t)0)
-#endif
-
-/*
- *  Deduce the type assignments from limits.h under the assumption that
- *  integer sizes in bits are powers of 2, and follow the ANSI
- *  definitions.
- */
-
-#ifndef UINT8_MAX
-# define UINT8_MAX 0xff
-#endif
-#ifndef uint8_t
-# if (UCHAR_MAX == UINT8_MAX) || defined (S_SPLINT_S)
-    typedef unsigned char uint8_t;
-#   define UINT8_C(v) ((uint8_t) v)
-# else
-#   error "Platform not supported"
-# endif
-#endif
-
-#ifndef INT8_MAX
-# define INT8_MAX 0x7f
-#endif
-#ifndef INT8_MIN
-# define INT8_MIN INT8_C(0x80)
-#endif
-#ifndef int8_t
-# if (SCHAR_MAX == INT8_MAX) || defined (S_SPLINT_S)
-    typedef signed char int8_t;
-#   define INT8_C(v) ((int8_t) v)
-# else
-#   error "Platform not supported"
-# endif
-#endif
-
-#ifndef UINT16_MAX
-# define UINT16_MAX 0xffff
-#endif
-#ifndef uint16_t
-#if (UINT_MAX == UINT16_MAX) || defined (S_SPLINT_S)
-  typedef unsigned int uint16_t;
-# ifndef PRINTF_INT16_MODIFIER
-#  define PRINTF_INT16_MODIFIER ""
-# endif
-# define UINT16_C(v) ((uint16_t) (v))
-#elif (USHRT_MAX == UINT16_MAX)
-  typedef unsigned short uint16_t;
-# define UINT16_C(v) ((uint16_t) (v))
-# ifndef PRINTF_INT16_MODIFIER
-#  define PRINTF_INT16_MODIFIER "h"
-# endif
-#else
-#error "Platform not supported"
-#endif
-#endif
-
-#ifndef INT16_MAX
-# define INT16_MAX 0x7fff
-#endif
-#ifndef INT16_MIN
-# define INT16_MIN INT16_C(0x8000)
-#endif
-#ifndef int16_t
-#if (INT_MAX == INT16_MAX) || defined (S_SPLINT_S)
-  typedef signed int int16_t;
-# define INT16_C(v) ((int16_t) (v))
-# ifndef PRINTF_INT16_MODIFIER
-#  define PRINTF_INT16_MODIFIER ""
-# endif
-#elif (SHRT_MAX == INT16_MAX)
-  typedef signed short int16_t;
-# define INT16_C(v) ((int16_t) (v))
-# ifndef PRINTF_INT16_MODIFIER
-#  define PRINTF_INT16_MODIFIER "h"
-# endif
-#else
-#error "Platform not supported"
-#endif
-#endif
-
-#ifndef UINT32_MAX
-# define UINT32_MAX (0xffffffffUL)
-#endif
-#ifndef uint32_t
-#if (ULONG_MAX == UINT32_MAX) || defined (S_SPLINT_S)
-  typedef unsigned long uint32_t;
-# define UINT32_C(v) v ## UL
-# ifndef PRINTF_INT32_MODIFIER
-#  define PRINTF_INT32_MODIFIER "l"
-# endif
-#elif (UINT_MAX == UINT32_MAX)
-  typedef unsigned int uint32_t;
-# ifndef PRINTF_INT32_MODIFIER
-#  define PRINTF_INT32_MODIFIER ""
-# endif
-# define UINT32_C(v) v ## U
-#elif (USHRT_MAX == UINT32_MAX)
-  typedef unsigned short uint32_t;
-# define UINT32_C(v) ((unsigned short) (v))
-# ifndef PRINTF_INT32_MODIFIER
-#  define PRINTF_INT32_MODIFIER ""
-# endif
-#else
-#error "Platform not supported"
-#endif
-#endif
-
-#ifndef INT32_MAX
-# define INT32_MAX (0x7fffffffL)
-#endif
-#ifndef INT32_MIN
-# define INT32_MIN INT32_C(0x80000000)
-#endif
-#ifndef int32_t
-#if (LONG_MAX == INT32_MAX) || defined (S_SPLINT_S)
-  typedef signed long int32_t;
-# define INT32_C(v) v ## L
-# ifndef PRINTF_INT32_MODIFIER
-#  define PRINTF_INT32_MODIFIER "l"
-# endif
-#elif (INT_MAX == INT32_MAX)
-  typedef signed int int32_t;
-# define INT32_C(v) v
-# ifndef PRINTF_INT32_MODIFIER
-#  define PRINTF_INT32_MODIFIER ""
-# endif
-#elif (SHRT_MAX == INT32_MAX)
-  typedef signed short int32_t;
-# define INT32_C(v) ((short) (v))
-# ifndef PRINTF_INT32_MODIFIER
-#  define PRINTF_INT32_MODIFIER ""
-# endif
-#else
-#error "Platform not supported"
-#endif
-#endif
-
-/*
- *  The macro stdint_int64_defined is temporarily used to record
- *  whether or not 64 integer support is available.  It must be
- *  defined for any 64 integer extensions for new platforms that are
- *  added.
- */
-
-#undef stdint_int64_defined
-#if (defined(__STDC__) && defined(__STDC_VERSION__)) || defined (S_SPLINT_S)
-# if (__STDC__ && __STDC_VERSION >= 199901L) || defined (S_SPLINT_S)
-#  define stdint_int64_defined
-   typedef long long int64_t;
-   typedef unsigned long long uint64_t;
-#  define UINT64_C(v) v ## ULL
-#  define  INT64_C(v) v ## LL
-#  ifndef PRINTF_INT64_MODIFIER
-#   define PRINTF_INT64_MODIFIER "ll"
-#  endif
-# endif
-#endif
-
-#if !defined (stdint_int64_defined)
-# if defined(__GNUC__)
-#  define stdint_int64_defined
-   __extension__ typedef long long int64_t;
-   __extension__ typedef unsigned long long uint64_t;
-#  define UINT64_C(v) v ## ULL
-#  define  INT64_C(v) v ## LL
-#  ifndef PRINTF_INT64_MODIFIER
-#   define PRINTF_INT64_MODIFIER "ll"
-#  endif
-# elif defined(__MWERKS__) || defined (__SUNPRO_C) || defined (__SUNPRO_CC) || defined (__APPLE_CC__) || defined (_LONG_LONG) || defined (_CRAYC) || defined (S_SPLINT_S)
-#  define stdint_int64_defined
-   typedef long long int64_t;
-   typedef unsigned long long uint64_t;
-#  define UINT64_C(v) v ## ULL
-#  define  INT64_C(v) v ## LL
-#  ifndef PRINTF_INT64_MODIFIER
-#   define PRINTF_INT64_MODIFIER "ll"
-#  endif
-# elif (defined(__WATCOMC__) && defined(__WATCOM_INT64__)) || (defined(_MSC_VER) && _INTEGRAL_MAX_BITS >= 64) || (defined (__BORLANDC__) && __BORLANDC__ > 0x460) || defined (__alpha) || defined (__DECC)
-#  define stdint_int64_defined
-   typedef __int64 int64_t;
-   typedef unsigned __int64 uint64_t;
-#  define UINT64_C(v) v ## UI64
-#  define  INT64_C(v) v ## I64
-#  ifndef PRINTF_INT64_MODIFIER
-#   define PRINTF_INT64_MODIFIER "I64"
-#  endif
-# endif
-#endif
-
-#if !defined (LONG_LONG_MAX) && defined (INT64_C)
-# define LONG_LONG_MAX INT64_C (9223372036854775807)
-#endif
-#ifndef ULONG_LONG_MAX
-# define ULONG_LONG_MAX UINT64_C (18446744073709551615)
-#endif
-
-#if !defined (INT64_MAX) && defined (INT64_C)
-# define INT64_MAX INT64_C (9223372036854775807)
-#endif
-#if !defined (INT64_MIN) && defined (INT64_C)
-# define INT64_MIN INT64_C (-9223372036854775808)
-#endif
-#if !defined (UINT64_MAX) && defined (INT64_C)
-# define UINT64_MAX UINT64_C (18446744073709551615)
-#endif
-
-/*
- *  Width of hexadecimal for number field.
- */
-
-#ifndef PRINTF_INT64_HEX_WIDTH
-# define PRINTF_INT64_HEX_WIDTH "16"
-#endif
-#ifndef PRINTF_INT32_HEX_WIDTH
-# define PRINTF_INT32_HEX_WIDTH "8"
-#endif
-#ifndef PRINTF_INT16_HEX_WIDTH
-# define PRINTF_INT16_HEX_WIDTH "4"
-#endif
-#ifndef PRINTF_INT8_HEX_WIDTH
-# define PRINTF_INT8_HEX_WIDTH "2"
-#endif
-
-#ifndef PRINTF_INT64_DEC_WIDTH
-# define PRINTF_INT64_DEC_WIDTH "20"
-#endif
-#ifndef PRINTF_INT32_DEC_WIDTH
-# define PRINTF_INT32_DEC_WIDTH "10"
-#endif
-#ifndef PRINTF_INT16_DEC_WIDTH
-# define PRINTF_INT16_DEC_WIDTH "5"
-#endif
-#ifndef PRINTF_INT8_DEC_WIDTH
-# define PRINTF_INT8_DEC_WIDTH "3"
-#endif
-
-/*
- *  Ok, lets not worry about 128 bit integers for now.  Moore's law says
- *  we don't need to worry about that until about 2040 at which point
- *  we'll have bigger things to worry about.
- */
-
-#ifdef stdint_int64_defined
-  typedef int64_t intmax_t;
-  typedef uint64_t uintmax_t;
-# define  INTMAX_MAX   INT64_MAX
-# define  INTMAX_MIN   INT64_MIN
-# define UINTMAX_MAX  UINT64_MAX
-# define UINTMAX_C(v) UINT64_C(v)
-# define  INTMAX_C(v)  INT64_C(v)
-# ifndef PRINTF_INTMAX_MODIFIER
-#   define PRINTF_INTMAX_MODIFIER PRINTF_INT64_MODIFIER
-# endif
-# ifndef PRINTF_INTMAX_HEX_WIDTH
-#  define PRINTF_INTMAX_HEX_WIDTH PRINTF_INT64_HEX_WIDTH
-# endif
-# ifndef PRINTF_INTMAX_DEC_WIDTH
-#  define PRINTF_INTMAX_DEC_WIDTH PRINTF_INT64_DEC_WIDTH
-# endif
-#else
-  typedef int32_t intmax_t;
-  typedef uint32_t uintmax_t;
-# define  INTMAX_MAX   INT32_MAX
-# define UINTMAX_MAX  UINT32_MAX
-# define UINTMAX_C(v) UINT32_C(v)
-# define  INTMAX_C(v)  INT32_C(v)
-# ifndef PRINTF_INTMAX_MODIFIER
-#   define PRINTF_INTMAX_MODIFIER PRINTF_INT32_MODIFIER
-# endif
-# ifndef PRINTF_INTMAX_HEX_WIDTH
-#  define PRINTF_INTMAX_HEX_WIDTH PRINTF_INT32_HEX_WIDTH
-# endif
-# ifndef PRINTF_INTMAX_DEC_WIDTH
-#  define PRINTF_INTMAX_DEC_WIDTH PRINTF_INT32_DEC_WIDTH
-# endif
-#endif
-
-/*
- *  Because this file currently only supports platforms which have
- *  precise powers of 2 as bit sizes for the default integers, the
- *  least definitions are all trivial.  Its possible that a future
- *  version of this file could have different definitions.
- */
-
-#ifndef stdint_least_defined
-  typedef   int8_t   int_least8_t;
-  typedef  uint8_t  uint_least8_t;
-  typedef  int16_t  int_least16_t;
-  typedef uint16_t uint_least16_t;
-  typedef  int32_t  int_least32_t;
-  typedef uint32_t uint_least32_t;
-# define PRINTF_LEAST32_MODIFIER PRINTF_INT32_MODIFIER
-# define PRINTF_LEAST16_MODIFIER PRINTF_INT16_MODIFIER
-# define  UINT_LEAST8_MAX  UINT8_MAX
-# define   INT_LEAST8_MAX   INT8_MAX
-# define UINT_LEAST16_MAX UINT16_MAX
-# define  INT_LEAST16_MAX  INT16_MAX
-# define UINT_LEAST32_MAX UINT32_MAX
-# define  INT_LEAST32_MAX  INT32_MAX
-# define   INT_LEAST8_MIN   INT8_MIN
-# define  INT_LEAST16_MIN  INT16_MIN
-# define  INT_LEAST32_MIN  INT32_MIN
-# ifdef stdint_int64_defined
-    typedef  int64_t  int_least64_t;
-    typedef uint64_t uint_least64_t;
-#   define PRINTF_LEAST64_MODIFIER PRINTF_INT64_MODIFIER
-#   define UINT_LEAST64_MAX UINT64_MAX
-#   define  INT_LEAST64_MAX  INT64_MAX
-#   define  INT_LEAST64_MIN  INT64_MIN
-# endif
-#endif
-#undef stdint_least_defined
-
-/*
- *  The ANSI C committee pretending to know or specify anything about
- *  performance is the epitome of misguided arrogance.  The mandate of
- *  this file is to *ONLY* ever support that absolute minimum
- *  definition of the fast integer types, for compatibility purposes.
- *  No extensions, and no attempt to suggest what may or may not be a
- *  faster integer type will ever be made in this file.  Developers are
- *  warned to stay away from these types when using this or any other
- *  stdint.h.
- */
-
-typedef   int_least8_t   int_fast8_t;
-typedef  uint_least8_t  uint_fast8_t;
-typedef  int_least16_t  int_fast16_t;
-typedef uint_least16_t uint_fast16_t;
-typedef  int_least32_t  int_fast32_t;
-typedef uint_least32_t uint_fast32_t;
-#define  UINT_FAST8_MAX  UINT_LEAST8_MAX
-#define   INT_FAST8_MAX   INT_LEAST8_MAX
-#define UINT_FAST16_MAX UINT_LEAST16_MAX
-#define  INT_FAST16_MAX  INT_LEAST16_MAX
-#define UINT_FAST32_MAX UINT_LEAST32_MAX
-#define  INT_FAST32_MAX  INT_LEAST32_MAX
-#define   INT_FAST8_MIN   IN_LEASTT8_MIN
-#define  INT_FAST16_MIN  INT_LEAST16_MIN
-#define  INT_FAST32_MIN  INT_LEAST32_MIN
-#ifdef stdint_int64_defined
-  typedef  int_least64_t  int_fast64_t;
-  typedef uint_least64_t uint_fast64_t;
-# define UINT_FAST64_MAX UINT_LEAST64_MAX
-# define  INT_FAST64_MAX  INT_LEAST64_MAX
-# define  INT_FAST64_MIN  INT_LEAST64_MIN
-#endif
-
-#undef stdint_int64_defined
-
-/*
- *  Whatever piecemeal, per compiler thing we can do about the wchar_t
- *  type limits.
- */
-
-#if defined(__WATCOMC__) || defined(_MSC_VER) || defined (__GNUC__)
-# include <wchar.h>
-# ifndef WCHAR_MIN
-#  define WCHAR_MIN 0
-# endif
-# ifndef WCHAR_MAX
-#  define WCHAR_MAX ((wchar_t)-1)
-# endif
-#endif
-
-/*
- *  Whatever piecemeal, per compiler/platform thing we can do about the
- *  (u)intptr_t types and limits.
- */
-
-#if defined (_MSC_VER) && defined (_UINTPTR_T_DEFINED)
-# define STDINT_H_UINTPTR_T_DEFINED
-#endif
-
-#ifndef STDINT_H_UINTPTR_T_DEFINED
-# if defined (__alpha__) || defined (__ia64__) || defined (__x86_64__) || defined (_WIN64)
-#  define stdint_intptr_bits 64
-# elif defined (__WATCOMC__) || defined (__TURBOC__)
-#  if defined(__TINY__) || defined(__SMALL__) || defined(__MEDIUM__)
-#    define stdint_intptr_bits 16
-#  else
-#    define stdint_intptr_bits 32
-#  endif
-# elif defined (__i386__) || defined (_WIN32) || defined (WIN32)
-#  define stdint_intptr_bits 32
-# elif defined (__INTEL_COMPILER)
-/* TODO -- what will Intel do about x86-64? */
-# endif
-
-# ifdef stdint_intptr_bits
-#  define stdint_intptr_glue3_i(a,b,c)  a##b##c
-#  define stdint_intptr_glue3(a,b,c)    stdint_intptr_glue3_i(a,b,c)
-#  ifndef PRINTF_INTPTR_MODIFIER
-#    define PRINTF_INTPTR_MODIFIER      stdint_intptr_glue3(PRINTF_INT,stdint_intptr_bits,_MODIFIER)
-#  endif
-#  ifndef PTRDIFF_MAX
-#    define PTRDIFF_MAX                 stdint_intptr_glue3(INT,stdint_intptr_bits,_MAX)
-#  endif
-#  ifndef PTRDIFF_MIN
-#    define PTRDIFF_MIN                 stdint_intptr_glue3(INT,stdint_intptr_bits,_MIN)
-#  endif
-#  ifndef UINTPTR_MAX
-#    define UINTPTR_MAX                 stdint_intptr_glue3(UINT,stdint_intptr_bits,_MAX)
-#  endif
-#  ifndef INTPTR_MAX
-#    define INTPTR_MAX                  stdint_intptr_glue3(INT,stdint_intptr_bits,_MAX)
-#  endif
-#  ifndef INTPTR_MIN
-#    define INTPTR_MIN                  stdint_intptr_glue3(INT,stdint_intptr_bits,_MIN)
-#  endif
-#  ifndef INTPTR_C
-#    define INTPTR_C(x)                 stdint_intptr_glue3(INT,stdint_intptr_bits,_C)(x)
-#  endif
-#  ifndef UINTPTR_C
-#    define UINTPTR_C(x)                stdint_intptr_glue3(UINT,stdint_intptr_bits,_C)(x)
-#  endif
-  typedef stdint_intptr_glue3(uint,stdint_intptr_bits,_t) uintptr_t;
-  typedef stdint_intptr_glue3( int,stdint_intptr_bits,_t)  intptr_t;
-# else
-/* TODO -- This following is likely wrong for some platforms, and does
-   nothing for the definition of uintptr_t. */
-  typedef ptrdiff_t intptr_t;
-# endif
-# define STDINT_H_UINTPTR_T_DEFINED
-#endif
-
-/*
- *  Assumes sig_atomic_t is signed and we have a 2s complement machine.
- */
-
-#ifndef SIG_ATOMIC_MAX
-# define SIG_ATOMIC_MAX ((((sig_atomic_t) 1) << (sizeof (sig_atomic_t)*CHAR_BIT-1)) - 1)
-#endif
-
-#endif
+/*  A portable stdint.h
+ *
+ *  Copyright (c) 2005-2007 Paul Hsieh
+ *
+ *  Redistribution and use in source and binary forms, with or without
+ *  modification, are permitted provided that the following conditions
+ *  are met:
+ *
+ *      Redistributions of source code must retain the above copyright
+ *      notice, this list of conditions and the following disclaimer.
+ *
+ *      Redistributions in binary form must not misrepresent the orignal
+ *      source in the documentation and/or other materials provided
+ *      with the distribution.
+ *
+ *      The names of the authors nor its contributors may be used to
+ *      endorse or promote products derived from this software without
+ *      specific prior written permission.
+ *
+ *  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+ *  FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+ *  COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+ *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ *  (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ *  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ *  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
+ *  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ *  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
+ *  OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ ****************************************************************************
+ *
+ *  Version 0.1.8
+ *
+ *  The ANSI C standard committee, for the C99 standard, specified the
+ *  inclusion of a new standard include file called stdint.h.  This is
+ *  a very useful and long desired include file which contains several
+ *  very precise definitions for integer scalar types that is
+ *  critically important for making portable several classes of
+ *  applications including cryptography, hashing, variable length
+ *  integer libraries and so on.  But for most developers its likely
+ *  useful just for programming sanity.
+ *
+ *  The problem is that most compiler vendors have decided not to
+ *  implement the C99 standard, and the next C++ language standard
+ *  (which has a lot more mindshare these days) will be a long time in
+ *  coming and its unknown whether or not it will include stdint.h or
+ *  how much adoption it will have.  Either way, it will be a long time
+ *  before all compilers come with a stdint.h and it also does nothing
+ *  for the extremely large number of compilers available today which
+ *  do not include this file, or anything comparable to it.
+ *
+ *  So that's what this file is all about.  Its an attempt to build a
+ *  single universal include file that works on as many platforms as
+ *  possible to deliver what stdint.h is supposed to.  A few things
+ *  that should be noted about this file:
+ *
+ *    1) It is not guaranteed to be portable and/or present an identical
+ *       interface on all platforms.  The extreme variability of the
+ *       ANSI C standard makes this an impossibility right from the
+ *       very get go. Its really only meant to be useful for the vast
+ *       majority of platforms that possess the capability of
+ *       implementing usefully and precisely defined, standard sized
+ *       integer scalars.  Systems which are not intrinsically 2s
+ *       complement may produce invalid constants.
+ *
+ *    2) There is an unavoidable use of non-reserved symbols.
+ *
+ *    3) Other standard include files are invoked.
+ *
+ *    4) This file may come in conflict with future platforms that do
+ *       include stdint.h.  The hope is that one or the other can be
+ *       used with no real difference.
+ *
+ *    5) In the current verison, if your platform can't represent
+ *       int32_t, int16_t and int8_t, it just dumps out with a compiler
+ *       error.
+ *
+ *    6) 64 bit integers may or may not be defined.  Test for their
+ *       presence with the test: #ifdef INT64_MAX or #ifdef UINT64_MAX.
+ *       Note that this is different from the C99 specification which
+ *       requires the existence of 64 bit support in the compiler.  If
+ *       this is not defined for your platform, yet it is capable of
+ *       dealing with 64 bits then it is because this file has not yet
+ *       been extended to cover all of your system's capabilities.
+ *
+ *    7) (u)intptr_t may or may not be defined.  Test for its presence
+ *       with the test: #ifdef PTRDIFF_MAX.  If this is not defined
+ *       for your platform, then it is because this file has not yet
+ *       been extended to cover all of your system's capabilities, not
+ *       because its optional.
+ *
+ *    8) The following might not been defined even if your platform is
+ *       capable of defining it:
+ *
+ *       WCHAR_MIN
+ *       WCHAR_MAX
+ *       (u)int64_t
+ *       PTRDIFF_MIN
+ *       PTRDIFF_MAX
+ *       (u)intptr_t
+ *
+ *    9) The following have not been defined:
+ *
+ *       WINT_MIN
+ *       WINT_MAX
+ *
+ *   10) The criteria for defining (u)int_least(*)_t isn't clear,
+ *       except for systems which don't have a type that precisely
+ *       defined 8, 16, or 32 bit types (which this include file does
+ *       not support anyways). Default definitions have been given.
+ *
+ *   11) The criteria for defining (u)int_fast(*)_t isn't something I
+ *       would trust to any particular compiler vendor or the ANSI C
+ *       committee.  It is well known that "compatible systems" are
+ *       commonly created that have very different performance
+ *       characteristics from the systems they are compatible with,
+ *       especially those whose vendors make both the compiler and the
+ *       system.  Default definitions have been given, but its strongly
+ *       recommended that users never use these definitions for any
+ *       reason (they do *NOT* deliver any serious guarantee of
+ *       improved performance -- not in this file, nor any vendor's
+ *       stdint.h).
+ *
+ *   12) The following macros:
+ *
+ *       PRINTF_INTMAX_MODIFIER
+ *       PRINTF_INT64_MODIFIER
+ *       PRINTF_INT32_MODIFIER
+ *       PRINTF_INT16_MODIFIER
+ *       PRINTF_LEAST64_MODIFIER
+ *       PRINTF_LEAST32_MODIFIER
+ *       PRINTF_LEAST16_MODIFIER
+ *       PRINTF_INTPTR_MODIFIER
+ *
+ *       are strings which have been defined as the modifiers required
+ *       for the "d", "u" and "x" printf formats to correctly output
+ *       (u)intmax_t, (u)int64_t, (u)int32_t, (u)int16_t, (u)least64_t,
+ *       (u)least32_t, (u)least16_t and (u)intptr_t types respectively.
+ *       PRINTF_INTPTR_MODIFIER is not defined for some systems which
+ *       provide their own stdint.h.  PRINTF_INT64_MODIFIER is not
+ *       defined if INT64_MAX is not defined.  These are an extension
+ *       beyond what C99 specifies must be in stdint.h.
+ *
+ *       In addition, the following macros are defined:
+ *
+ *       PRINTF_INTMAX_HEX_WIDTH
+ *       PRINTF_INT64_HEX_WIDTH
+ *       PRINTF_INT32_HEX_WIDTH
+ *       PRINTF_INT16_HEX_WIDTH
+ *       PRINTF_INT8_HEX_WIDTH
+ *       PRINTF_INTMAX_DEC_WIDTH
+ *       PRINTF_INT64_DEC_WIDTH
+ *       PRINTF_INT32_DEC_WIDTH
+ *       PRINTF_INT16_DEC_WIDTH
+ *       PRINTF_INT8_DEC_WIDTH
+ *
+ *       Which specifies the maximum number of characters required to
+ *       print the number of that type in either hexadecimal or decimal.
+ *       These are an extension beyond what C99 specifies must be in
+ *       stdint.h.
+ *
+ *  Compilers tested (all with 0 warnings at their highest respective
+ *  settings): Borland Turbo C 2.0, WATCOM C/C++ 11.0 (16 bits and 32
+ *  bits), Microsoft Visual C++ 6.0 (32 bit), Microsoft Visual Studio
+ *  .net (VC7), Intel C++ 4.0, GNU gcc v3.3.3
+ *
+ *  This file should be considered a work in progress.  Suggestions for
+ *  improvements, especially those which increase coverage are strongly
+ *  encouraged.
+ *
+ *  Acknowledgements
+ *
+ *  The following people have made significant contributions to the
+ *  development and testing of this file:
+ *
+ *  Chris Howie
+ *  John Steele Scott
+ *  Dave Thorup
+ *
+ */
+
+#include <stddef.h>
+#include <limits.h>
+#include <signal.h>
+
+/*
+ *  For gcc with _STDINT_H, fill in the PRINTF_INT*_MODIFIER macros, and
+ *  do nothing else.  On the Mac OS X version of gcc this is _STDINT_H_.
+ */
+
+#if ((defined(__STDC__) && __STDC__ && __STDC_VERSION__ >= 199901L) || (defined (__WATCOMC__) && (defined (_STDINT_H_INCLUDED) || __WATCOMC__ >= 1250)) || (defined(__GNUC__) && (defined(_STDINT_H) || defined(_STDINT_H_)) )) && !defined (_PSTDINT_H_INCLUDED)
+#include <stdint.h>
+#define _PSTDINT_H_INCLUDED
+# ifndef PRINTF_INT64_MODIFIER
+#  define PRINTF_INT64_MODIFIER "ll"
+# endif
+# ifndef PRINTF_INT32_MODIFIER
+#  define PRINTF_INT32_MODIFIER "l"
+# endif
+# ifndef PRINTF_INT16_MODIFIER
+#  define PRINTF_INT16_MODIFIER "h"
+# endif
+# ifndef PRINTF_INTMAX_MODIFIER
+#  define PRINTF_INTMAX_MODIFIER PRINTF_INT64_MODIFIER
+# endif
+# ifndef PRINTF_INT64_HEX_WIDTH
+#  define PRINTF_INT64_HEX_WIDTH "16"
+# endif
+# ifndef PRINTF_INT32_HEX_WIDTH
+#  define PRINTF_INT32_HEX_WIDTH "8"
+# endif
+# ifndef PRINTF_INT16_HEX_WIDTH
+#  define PRINTF_INT16_HEX_WIDTH "4"
+# endif
+# ifndef PRINTF_INT8_HEX_WIDTH
+#  define PRINTF_INT8_HEX_WIDTH "2"
+# endif
+# ifndef PRINTF_INT64_DEC_WIDTH
+#  define PRINTF_INT64_DEC_WIDTH "20"
+# endif
+# ifndef PRINTF_INT32_DEC_WIDTH
+#  define PRINTF_INT32_DEC_WIDTH "10"
+# endif
+# ifndef PRINTF_INT16_DEC_WIDTH
+#  define PRINTF_INT16_DEC_WIDTH "5"
+# endif
+# ifndef PRINTF_INT8_DEC_WIDTH
+#  define PRINTF_INT8_DEC_WIDTH "3"
+# endif
+# ifndef PRINTF_INTMAX_HEX_WIDTH
+#  define PRINTF_INTMAX_HEX_WIDTH PRINTF_INT64_HEX_WIDTH
+# endif
+# ifndef PRINTF_INTMAX_DEC_WIDTH
+#  define PRINTF_INTMAX_DEC_WIDTH PRINTF_INT64_DEC_WIDTH
+# endif
+#endif
+
+#ifndef _PSTDINT_H_INCLUDED
+#define _PSTDINT_H_INCLUDED
+
+#ifndef SIZE_MAX
+# define SIZE_MAX (~(size_t)0)
+#endif
+
+/*
+ *  Deduce the type assignments from limits.h under the assumption that
+ *  integer sizes in bits are powers of 2, and follow the ANSI
+ *  definitions.
+ */
+
+#ifndef UINT8_MAX
+# define UINT8_MAX 0xff
+#endif
+#ifndef uint8_t
+# if (UCHAR_MAX == UINT8_MAX) || defined (S_SPLINT_S)
+    typedef unsigned char uint8_t;
+#   define UINT8_C(v) ((uint8_t) v)
+# else
+#   error "Platform not supported"
+# endif
+#endif
+
+#ifndef INT8_MAX
+# define INT8_MAX 0x7f
+#endif
+#ifndef INT8_MIN
+# define INT8_MIN INT8_C(0x80)
+#endif
+#ifndef int8_t
+# if (SCHAR_MAX == INT8_MAX) || defined (S_SPLINT_S)
+    typedef signed char int8_t;
+#   define INT8_C(v) ((int8_t) v)
+# else
+#   error "Platform not supported"
+# endif
+#endif
+
+#ifndef UINT16_MAX
+# define UINT16_MAX 0xffff
+#endif
+#ifndef uint16_t
+#if (UINT_MAX == UINT16_MAX) || defined (S_SPLINT_S)
+  typedef unsigned int uint16_t;
+# ifndef PRINTF_INT16_MODIFIER
+#  define PRINTF_INT16_MODIFIER ""
+# endif
+# define UINT16_C(v) ((uint16_t) (v))
+#elif (USHRT_MAX == UINT16_MAX)
+  typedef unsigned short uint16_t;
+# define UINT16_C(v) ((uint16_t) (v))
+# ifndef PRINTF_INT16_MODIFIER
+#  define PRINTF_INT16_MODIFIER "h"
+# endif
+#else
+#error "Platform not supported"
+#endif
+#endif
+
+#ifndef INT16_MAX
+# define INT16_MAX 0x7fff
+#endif
+#ifndef INT16_MIN
+# define INT16_MIN INT16_C(0x8000)
+#endif
+#ifndef int16_t
+#if (INT_MAX == INT16_MAX) || defined (S_SPLINT_S)
+  typedef signed int int16_t;
+# define INT16_C(v) ((int16_t) (v))
+# ifndef PRINTF_INT16_MODIFIER
+#  define PRINTF_INT16_MODIFIER ""
+# endif
+#elif (SHRT_MAX == INT16_MAX)
+  typedef signed short int16_t;
+# define INT16_C(v) ((int16_t) (v))
+# ifndef PRINTF_INT16_MODIFIER
+#  define PRINTF_INT16_MODIFIER "h"
+# endif
+#else
+#error "Platform not supported"
+#endif
+#endif
+
+#ifndef UINT32_MAX
+# define UINT32_MAX (0xffffffffUL)
+#endif
+#ifndef uint32_t
+#if (ULONG_MAX == UINT32_MAX) || defined (S_SPLINT_S)
+  typedef unsigned long uint32_t;
+# define UINT32_C(v) v ## UL
+# ifndef PRINTF_INT32_MODIFIER
+#  define PRINTF_INT32_MODIFIER "l"
+# endif
+#elif (UINT_MAX == UINT32_MAX)
+  typedef unsigned int uint32_t;
+# ifndef PRINTF_INT32_MODIFIER
+#  define PRINTF_INT32_MODIFIER ""
+# endif
+# define UINT32_C(v) v ## U
+#elif (USHRT_MAX == UINT32_MAX)
+  typedef unsigned short uint32_t;
+# define UINT32_C(v) ((unsigned short) (v))
+# ifndef PRINTF_INT32_MODIFIER
+#  define PRINTF_INT32_MODIFIER ""
+# endif
+#else
+#error "Platform not supported"
+#endif
+#endif
+
+#ifndef INT32_MAX
+# define INT32_MAX (0x7fffffffL)
+#endif
+#ifndef INT32_MIN
+# define INT32_MIN INT32_C(0x80000000)
+#endif
+#ifndef int32_t
+#if (LONG_MAX == INT32_MAX) || defined (S_SPLINT_S)
+  typedef signed long int32_t;
+# define INT32_C(v) v ## L
+# ifndef PRINTF_INT32_MODIFIER
+#  define PRINTF_INT32_MODIFIER "l"
+# endif
+#elif (INT_MAX == INT32_MAX)
+  typedef signed int int32_t;
+# define INT32_C(v) v
+# ifndef PRINTF_INT32_MODIFIER
+#  define PRINTF_INT32_MODIFIER ""
+# endif
+#elif (SHRT_MAX == INT32_MAX)
+  typedef signed short int32_t;
+# define INT32_C(v) ((short) (v))
+# ifndef PRINTF_INT32_MODIFIER
+#  define PRINTF_INT32_MODIFIER ""
+# endif
+#else
+#error "Platform not supported"
+#endif
+#endif
+
+/*
+ *  The macro stdint_int64_defined is temporarily used to record
+ *  whether or not 64 integer support is available.  It must be
+ *  defined for any 64 integer extensions for new platforms that are
+ *  added.
+ */
+
+#undef stdint_int64_defined
+#if (defined(__STDC__) && defined(__STDC_VERSION__)) || defined (S_SPLINT_S)
+# if (__STDC__ && __STDC_VERSION >= 199901L) || defined (S_SPLINT_S)
+#  define stdint_int64_defined
+   typedef long long int64_t;
+   typedef unsigned long long uint64_t;
+#  define UINT64_C(v) v ## ULL
+#  define  INT64_C(v) v ## LL
+#  ifndef PRINTF_INT64_MODIFIER
+#   define PRINTF_INT64_MODIFIER "ll"
+#  endif
+# endif
+#endif
+
+#if !defined (stdint_int64_defined)
+# if defined(__GNUC__)
+#  define stdint_int64_defined
+   __extension__ typedef long long int64_t;
+   __extension__ typedef unsigned long long uint64_t;
+#  define UINT64_C(v) v ## ULL
+#  define  INT64_C(v) v ## LL
+#  ifndef PRINTF_INT64_MODIFIER
+#   define PRINTF_INT64_MODIFIER "ll"
+#  endif
+# elif defined(__MWERKS__) || defined (__SUNPRO_C) || defined (__SUNPRO_CC) || defined (__APPLE_CC__) || defined (_LONG_LONG) || defined (_CRAYC) || defined (S_SPLINT_S)
+#  define stdint_int64_defined
+   typedef long long int64_t;
+   typedef unsigned long long uint64_t;
+#  define UINT64_C(v) v ## ULL
+#  define  INT64_C(v) v ## LL
+#  ifndef PRINTF_INT64_MODIFIER
+#   define PRINTF_INT64_MODIFIER "ll"
+#  endif
+# elif (defined(__WATCOMC__) && defined(__WATCOM_INT64__)) || (defined(_MSC_VER) && _INTEGRAL_MAX_BITS >= 64) || (defined (__BORLANDC__) && __BORLANDC__ > 0x460) || defined (__alpha) || defined (__DECC)
+#  define stdint_int64_defined
+   typedef __int64 int64_t;
+   typedef unsigned __int64 uint64_t;
+#  define UINT64_C(v) v ## UI64
+#  define  INT64_C(v) v ## I64
+#  ifndef PRINTF_INT64_MODIFIER
+#   define PRINTF_INT64_MODIFIER "I64"
+#  endif
+# endif
+#endif
+
+#if !defined (LONG_LONG_MAX) && defined (INT64_C)
+# define LONG_LONG_MAX INT64_C (9223372036854775807)
+#endif
+#ifndef ULONG_LONG_MAX
+# define ULONG_LONG_MAX UINT64_C (18446744073709551615)
+#endif
+
+#if !defined (INT64_MAX) && defined (INT64_C)
+# define INT64_MAX INT64_C (9223372036854775807)
+#endif
+#if !defined (INT64_MIN) && defined (INT64_C)
+# define INT64_MIN INT64_C (-9223372036854775808)
+#endif
+#if !defined (UINT64_MAX) && defined (INT64_C)
+# define UINT64_MAX UINT64_C (18446744073709551615)
+#endif
+
+/*
+ *  Width of hexadecimal for number field.
+ */
+
+#ifndef PRINTF_INT64_HEX_WIDTH
+# define PRINTF_INT64_HEX_WIDTH "16"
+#endif
+#ifndef PRINTF_INT32_HEX_WIDTH
+# define PRINTF_INT32_HEX_WIDTH "8"
+#endif
+#ifndef PRINTF_INT16_HEX_WIDTH
+# define PRINTF_INT16_HEX_WIDTH "4"
+#endif
+#ifndef PRINTF_INT8_HEX_WIDTH
+# define PRINTF_INT8_HEX_WIDTH "2"
+#endif
+
+#ifndef PRINTF_INT64_DEC_WIDTH
+# define PRINTF_INT64_DEC_WIDTH "20"
+#endif
+#ifndef PRINTF_INT32_DEC_WIDTH
+# define PRINTF_INT32_DEC_WIDTH "10"
+#endif
+#ifndef PRINTF_INT16_DEC_WIDTH
+# define PRINTF_INT16_DEC_WIDTH "5"
+#endif
+#ifndef PRINTF_INT8_DEC_WIDTH
+# define PRINTF_INT8_DEC_WIDTH "3"
+#endif
+
+/*
+ *  Ok, lets not worry about 128 bit integers for now.  Moore's law says
+ *  we don't need to worry about that until about 2040 at which point
+ *  we'll have bigger things to worry about.
+ */
+
+#ifdef stdint_int64_defined
+  typedef int64_t intmax_t;
+  typedef uint64_t uintmax_t;
+# define  INTMAX_MAX   INT64_MAX
+# define  INTMAX_MIN   INT64_MIN
+# define UINTMAX_MAX  UINT64_MAX
+# define UINTMAX_C(v) UINT64_C(v)
+# define  INTMAX_C(v)  INT64_C(v)
+# ifndef PRINTF_INTMAX_MODIFIER
+#   define PRINTF_INTMAX_MODIFIER PRINTF_INT64_MODIFIER
+# endif
+# ifndef PRINTF_INTMAX_HEX_WIDTH
+#  define PRINTF_INTMAX_HEX_WIDTH PRINTF_INT64_HEX_WIDTH
+# endif
+# ifndef PRINTF_INTMAX_DEC_WIDTH
+#  define PRINTF_INTMAX_DEC_WIDTH PRINTF_INT64_DEC_WIDTH
+# endif
+#else
+  typedef int32_t intmax_t;
+  typedef uint32_t uintmax_t;
+# define  INTMAX_MAX   INT32_MAX
+# define UINTMAX_MAX  UINT32_MAX
+# define UINTMAX_C(v) UINT32_C(v)
+# define  INTMAX_C(v)  INT32_C(v)
+# ifndef PRINTF_INTMAX_MODIFIER
+#   define PRINTF_INTMAX_MODIFIER PRINTF_INT32_MODIFIER
+# endif
+# ifndef PRINTF_INTMAX_HEX_WIDTH
+#  define PRINTF_INTMAX_HEX_WIDTH PRINTF_INT32_HEX_WIDTH
+# endif
+# ifndef PRINTF_INTMAX_DEC_WIDTH
+#  define PRINTF_INTMAX_DEC_WIDTH PRINTF_INT32_DEC_WIDTH
+# endif
+#endif
+
+/*
+ *  Because this file currently only supports platforms which have
+ *  precise powers of 2 as bit sizes for the default integers, the
+ *  least definitions are all trivial.  Its possible that a future
+ *  version of this file could have different definitions.
+ */
+
+#ifndef stdint_least_defined
+  typedef   int8_t   int_least8_t;
+  typedef  uint8_t  uint_least8_t;
+  typedef  int16_t  int_least16_t;
+  typedef uint16_t uint_least16_t;
+  typedef  int32_t  int_least32_t;
+  typedef uint32_t uint_least32_t;
+# define PRINTF_LEAST32_MODIFIER PRINTF_INT32_MODIFIER
+# define PRINTF_LEAST16_MODIFIER PRINTF_INT16_MODIFIER
+# define  UINT_LEAST8_MAX  UINT8_MAX
+# define   INT_LEAST8_MAX   INT8_MAX
+# define UINT_LEAST16_MAX UINT16_MAX
+# define  INT_LEAST16_MAX  INT16_MAX
+# define UINT_LEAST32_MAX UINT32_MAX
+# define  INT_LEAST32_MAX  INT32_MAX
+# define   INT_LEAST8_MIN   INT8_MIN
+# define  INT_LEAST16_MIN  INT16_MIN
+# define  INT_LEAST32_MIN  INT32_MIN
+# ifdef stdint_int64_defined
+    typedef  int64_t  int_least64_t;
+    typedef uint64_t uint_least64_t;
+#   define PRINTF_LEAST64_MODIFIER PRINTF_INT64_MODIFIER
+#   define UINT_LEAST64_MAX UINT64_MAX
+#   define  INT_LEAST64_MAX  INT64_MAX
+#   define  INT_LEAST64_MIN  INT64_MIN
+# endif
+#endif
+#undef stdint_least_defined
+
+/*
+ *  The ANSI C committee pretending to know or specify anything about
+ *  performance is the epitome of misguided arrogance.  The mandate of
+ *  this file is to *ONLY* ever support that absolute minimum
+ *  definition of the fast integer types, for compatibility purposes.
+ *  No extensions, and no attempt to suggest what may or may not be a
+ *  faster integer type will ever be made in this file.  Developers are
+ *  warned to stay away from these types when using this or any other
+ *  stdint.h.
+ */
+
+typedef   int_least8_t   int_fast8_t;
+typedef  uint_least8_t  uint_fast8_t;
+typedef  int_least16_t  int_fast16_t;
+typedef uint_least16_t uint_fast16_t;
+typedef  int_least32_t  int_fast32_t;
+typedef uint_least32_t uint_fast32_t;
+#define  UINT_FAST8_MAX  UINT_LEAST8_MAX
+#define   INT_FAST8_MAX   INT_LEAST8_MAX
+#define UINT_FAST16_MAX UINT_LEAST16_MAX
+#define  INT_FAST16_MAX  INT_LEAST16_MAX
+#define UINT_FAST32_MAX UINT_LEAST32_MAX
+#define  INT_FAST32_MAX  INT_LEAST32_MAX
+#define   INT_FAST8_MIN   IN_LEASTT8_MIN
+#define  INT_FAST16_MIN  INT_LEAST16_MIN
+#define  INT_FAST32_MIN  INT_LEAST32_MIN
+#ifdef stdint_int64_defined
+  typedef  int_least64_t  int_fast64_t;
+  typedef uint_least64_t uint_fast64_t;
+# define UINT_FAST64_MAX UINT_LEAST64_MAX
+# define  INT_FAST64_MAX  INT_LEAST64_MAX
+# define  INT_FAST64_MIN  INT_LEAST64_MIN
+#endif
+
+#undef stdint_int64_defined
+
+/*
+ *  Whatever piecemeal, per compiler thing we can do about the wchar_t
+ *  type limits.
+ */
+
+#if defined(__WATCOMC__) || defined(_MSC_VER) || defined (__GNUC__)
+# include <wchar.h>
+# ifndef WCHAR_MIN
+#  define WCHAR_MIN 0
+# endif
+# ifndef WCHAR_MAX
+#  define WCHAR_MAX ((wchar_t)-1)
+# endif
+#endif
+
+/*
+ *  Whatever piecemeal, per compiler/platform thing we can do about the
+ *  (u)intptr_t types and limits.
+ */
+
+#if defined (_MSC_VER) && defined (_UINTPTR_T_DEFINED)
+# define STDINT_H_UINTPTR_T_DEFINED
+#endif
+
+#ifndef STDINT_H_UINTPTR_T_DEFINED
+# if defined (__alpha__) || defined (__ia64__) || defined (__x86_64__) || defined (_WIN64)
+#  define stdint_intptr_bits 64
+# elif defined (__WATCOMC__) || defined (__TURBOC__)
+#  if defined(__TINY__) || defined(__SMALL__) || defined(__MEDIUM__)
+#    define stdint_intptr_bits 16
+#  else
+#    define stdint_intptr_bits 32
+#  endif
+# elif defined (__i386__) || defined (_WIN32) || defined (WIN32)
+#  define stdint_intptr_bits 32
+# elif defined (__INTEL_COMPILER)
+/* TODO -- what will Intel do about x86-64? */
+# endif
+
+# ifdef stdint_intptr_bits
+#  define stdint_intptr_glue3_i(a,b,c)  a##b##c
+#  define stdint_intptr_glue3(a,b,c)    stdint_intptr_glue3_i(a,b,c)
+#  ifndef PRINTF_INTPTR_MODIFIER
+#    define PRINTF_INTPTR_MODIFIER      stdint_intptr_glue3(PRINTF_INT,stdint_intptr_bits,_MODIFIER)
+#  endif
+#  ifndef PTRDIFF_MAX
+#    define PTRDIFF_MAX                 stdint_intptr_glue3(INT,stdint_intptr_bits,_MAX)
+#  endif
+#  ifndef PTRDIFF_MIN
+#    define PTRDIFF_MIN                 stdint_intptr_glue3(INT,stdint_intptr_bits,_MIN)
+#  endif
+#  ifndef UINTPTR_MAX
+#    define UINTPTR_MAX                 stdint_intptr_glue3(UINT,stdint_intptr_bits,_MAX)
+#  endif
+#  ifndef INTPTR_MAX
+#    define INTPTR_MAX                  stdint_intptr_glue3(INT,stdint_intptr_bits,_MAX)
+#  endif
+#  ifndef INTPTR_MIN
+#    define INTPTR_MIN                  stdint_intptr_glue3(INT,stdint_intptr_bits,_MIN)
+#  endif
+#  ifndef INTPTR_C
+#    define INTPTR_C(x)                 stdint_intptr_glue3(INT,stdint_intptr_bits,_C)(x)
+#  endif
+#  ifndef UINTPTR_C
+#    define UINTPTR_C(x)                stdint_intptr_glue3(UINT,stdint_intptr_bits,_C)(x)
+#  endif
+  typedef stdint_intptr_glue3(uint,stdint_intptr_bits,_t) uintptr_t;
+  typedef stdint_intptr_glue3( int,stdint_intptr_bits,_t)  intptr_t;
+# else
+/* TODO -- This following is likely wrong for some platforms, and does
+   nothing for the definition of uintptr_t. */
+  typedef ptrdiff_t intptr_t;
+# endif
+# define STDINT_H_UINTPTR_T_DEFINED
+#endif
+
+/*
+ *  Assumes sig_atomic_t is signed and we have a 2s complement machine.
+ */
+
+#ifndef SIG_ATOMIC_MAX
+# define SIG_ATOMIC_MAX ((((sig_atomic_t) 1) << (sizeof (sig_atomic_t)*CHAR_BIT-1)) - 1)
+#endif
+
+#endif



From saintmlx at mail.berlios.de  Tue Jul 10 01:08:32 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 10 Jul 2007 01:08:32 +0200
Subject: [Plearn-commits] r7724 - trunk/python_modules/plearn/pybridge
Message-ID: <200707092308.l69N8Wlx031819@sheep.berlios.de>

Author: saintmlx
Date: 2007-07-10 01:08:31 +0200 (Tue, 10 Jul 2007)
New Revision: 7724

Added:
   trunk/python_modules/plearn/pybridge/pl_global_funcs.py
Log:
- enable calling global PLearn funcs. from embedded python



Added: trunk/python_modules/plearn/pybridge/pl_global_funcs.py
===================================================================
--- trunk/python_modules/plearn/pybridge/pl_global_funcs.py	2007-07-09 22:55:23 UTC (rev 7723)
+++ trunk/python_modules/plearn/pybridge/pl_global_funcs.py	2007-07-09 23:08:31 UTC (rev 7724)
@@ -0,0 +1,47 @@
+#
+# pl_global_funcs
+# Copyright (C) 2007 Xavier Saint-Mleux, ApSTAT Technologies inc.
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+
+class pl:
+    class __metaclass__(type):
+        def __getattr__(cls, name):
+            def newObj(**kwargs):
+                call_build= True
+                obj= newObjectFromClassname(name)
+                for k in kwargs.keys():
+                    if k=='__call_build':
+                        call_build= kwargs[k]
+                    else:
+                        obj.__setattr__(k, kwargs[k])
+                if call_build:
+                    obj.build()
+                return obj
+            return newObj



From saintmlx at mail.berlios.de  Tue Jul 10 01:09:31 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 10 Jul 2007 01:09:31 +0200
Subject: [Plearn-commits] r7725 - trunk/plearn/python
Message-ID: <200707092309.l69N9VDW000675@sheep.berlios.de>

Author: saintmlx
Date: 2007-07-10 01:09:30 +0200 (Tue, 10 Jul 2007)
New Revision: 7725

Modified:
   trunk/plearn/python/PythonCodeSnippet.cc
Log:
- enable calling PLearn global funcs. from embedded python



Modified: trunk/plearn/python/PythonCodeSnippet.cc
===================================================================
--- trunk/plearn/python/PythonCodeSnippet.cc	2007-07-09 23:08:31 UTC (rev 7724)
+++ trunk/plearn/python/PythonCodeSnippet.cc	2007-07-09 23:09:30 UTC (rev 7725)
@@ -43,6 +43,7 @@
 // Python stuff must be included first
 #include "PythonCodeSnippet.h"
 #include "PythonEmbedder.h"
+#include "PythonExtension.h"
 
 // From PLearn
 #include <plearn/io/fileutils.h>
@@ -517,7 +518,8 @@
 
     //always include EmbeddedCodeSnippet to check for an object to instantiate
     string extracode= "\nfrom plearn.pybridge.embedded_code_snippet "
-        "import EmbeddedCodeSnippet\n";
+        "import EmbeddedCodeSnippet\n"
+        "from plearn.pybridge import pl_global_funcs\n";
 
     if (code != "") {
 #ifdef WIN32
@@ -541,12 +543,21 @@
         }
     }
 
-    //try to find an EmbeddedCodeSnippet to instantiate
+    //get the global env. as an stl map
     PythonObjectWrapper wrapped_globals(globals);
     Py_XDECREF(globals);
     map<string, PyObject*> global_map= 
         wrapped_globals.as<map<string, PyObject*> >();
 
+    //inject global funcs
+    map<string, PyObject*>::iterator it_gf= 
+        global_map.find("pl_global_funcs");
+    if(it_gf == global_map.end())
+        PLERROR("in PythonCodeSnippet::compileGlobalCode : "
+                "plearn.pybridge.pl_global_funcs not present in global env!");
+    injectPLearnGlobalFunctions(it_gf->second);//inject in global_funcs module
+
+    //try to find an EmbeddedCodeSnippet to instantiate
     PyObject* snippet_found= 0;
     map<string, PyObject*>::iterator it_id= 
         global_map.find("pl_embedded_code_snippet_type");



From yoshua at mail.berlios.de  Tue Jul 10 01:18:06 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Tue, 10 Jul 2007 01:18:06 +0200
Subject: [Plearn-commits] r7726 - trunk/python_modules/plearn/learners/autolr
Message-ID: <200707092318.l69NI6fq009691@sheep.berlios.de>

Author: yoshua
Date: 2007-07-10 01:18:05 +0200 (Tue, 10 Jul 2007)
New Revision: 7726

Modified:
   trunk/python_modules/plearn/learners/autolr/__init__.py
Log:
Fixed uncorrect handling of selected_costs (cost_indices) in autolr


Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-07-09 23:09:30 UTC (rev 7725)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-07-09 23:18:05 UTC (rev 7726)
@@ -300,6 +300,10 @@
 
     def error_curve(active,start_t,current_t):
         delta_t = current_t+1-start_t
+        # in all_results, column 0 is stage, column 1 is option value (learning rate)
+        # and column 2+test*n_costs+cost is the cost value for cost number 'cost'
+        # (index in the selected_costnames list), in testset 'test'.
+        # And testset 0 is the one used for selection.
         return all_results[active][start_t:current_t+1,2+cost_to_select_best]
         
     def error_curve_dominates(c1,c2,t):
@@ -429,7 +433,7 @@
                 if logfile:
                     print >>logfile, " test" + str(j+1),": ",
                 for k in range(0,n_costs):
-                    err = ts.getStat("E["+str(k)+"]")
+                    err = ts.getStat("E["+str(cost_indices[k])+"]")
                     results[t,j*n_costs+k+2]=err
                     costname = costnames[cost_indices[k]]
                     if logfile:



From yoshua at mail.berlios.de  Tue Jul 10 01:18:33 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Tue, 10 Jul 2007 01:18:33 +0200
Subject: [Plearn-commits] r7727 - trunk/plearn_learners/online
Message-ID: <200707092318.l69NIXin010495@sheep.berlios.de>

Author: yoshua
Date: 2007-07-10 01:18:32 +0200 (Tue, 10 Jul 2007)
New Revision: 7727

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
Fixes to the sampling code in fprop of RBMModule


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-07-09 23:18:05 UTC (rev 7726)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-07-09 23:18:32 UTC (rev 7727)
@@ -662,7 +662,8 @@
         // When an input is provided, that would restart the chain for
         // unconditional sampling, from that example.
         Gibbs_step = 0;
-        visible_layer->setExpectations(*visible);
+        visible_layer->samples.resize(visible->length(),visible->width());
+        visible_layer->samples << *visible;
     }
 
     // COMPUTE ENERGY
@@ -802,6 +803,77 @@
         found_a_valid_configuration = true;
     }
 
+    // COMPUTE AUTOASSOCIATOR RECONSTRUCTION ERROR
+    if ( visible && !visible->isEmpty() &&
+         ( ( visible_reconstruction && visible_reconstruction->isEmpty() ) ||
+           ( visible_reconstruction_activations &&
+             visible_reconstruction_activations->isEmpty() ) ||
+           ( reconstruction_error && reconstruction_error->isEmpty() ) ) )
+    {
+        // Autoassociator reconstruction cost
+        PLASSERT( ports_value.length() == nPorts() );
+        if(!hidden_expectations_are_computed)
+        {
+            computePositivePhaseHiddenActivations(*visible);
+            hidden_layer->computeExpectations();
+            hidden_expectations_are_computed=true;
+        }
+
+        // Don't need to verify if they are asked in a port, this was done previously
+
+        computeVisibleActivations(hidden_layer->getExpectations(), true);
+        if(visible_reconstruction_activations)
+        {
+            PLASSERT( visible_reconstruction_activations->isEmpty() );
+            const Mat& to_store = visible_layer->activations;
+            visible_reconstruction_activations->resize(to_store.length(),
+                                                       to_store.width());
+            *visible_reconstruction_activations << to_store;
+        }
+        if (visible_reconstruction || reconstruction_error)
+        {
+            visible_layer->computeExpectations();
+            if(visible_reconstruction)
+            {
+                PLASSERT( visible_reconstruction->isEmpty() );
+                const Mat& to_store = visible_layer->getExpectations();
+                visible_reconstruction->resize(to_store.length(),
+                                               to_store.width());
+                *visible_reconstruction << to_store;
+            }
+            if(reconstruction_error)
+            {
+                PLASSERT( reconstruction_error->isEmpty() );
+                reconstruction_error->resize(visible->length(),1);
+                visible_layer->fpropNLL(*visible,
+                                        *reconstruction_error);
+            }
+        }
+        found_a_valid_configuration = true;
+    }
+    // COMPUTE VISIBLE GIVEN HIDDEN
+    else if ( visible_reconstruction && visible_reconstruction->isEmpty()
+         && hidden && !hidden->isEmpty())
+    {
+        // Don't need to verify if they are asked in a port, this was done previously
+        computeVisibleActivations(*hidden,true);
+        if(visible_reconstruction_activations)
+        {
+            PLASSERT( visible_reconstruction_activations->isEmpty() );
+            const Mat& to_store = visible_layer->activations;
+            visible_reconstruction_activations->resize(to_store.length(),
+                                                       to_store.width());
+            *visible_reconstruction_activations << to_store;
+        }
+        visible_layer->computeExpectations();
+        PLASSERT( visible_reconstruction->isEmpty() );
+        const Mat& to_store = visible_layer->getExpectations();
+        visible_reconstruction->resize(to_store.length(),
+                                       to_store.width());
+        *visible_reconstruction << to_store;
+        found_a_valid_configuration = true;
+    }
+
     // SAMPLING
     if ((visible_sample && visible_sample->isEmpty())               // is asked to sample visible units (discrete)
         || (visible_expectation && visible_expectation->isEmpty())  //              "                   (continous)
@@ -816,15 +888,10 @@
         else if (visible_sample && !visible_sample->isEmpty()) // if an input is provided, sample hidden conditionally
         {
             sampleHiddenGivenVisible(*visible_sample);
+            hidden_activations_are_computed = false;
             Gibbs_step=0;
             //cout << "sampling hidden from visible" << endl;
         }
-        else if (visible && !visible->isEmpty()) // if an input is provided, sample hidden conditionally
-        {
-	    sampleHiddenGivenVisible(*visible);
-            Gibbs_step=0;
-            //cout << "sampling hidden from visible" << endl;
-        }
         else if (visible_expectation && !visible_expectation->isEmpty())
         {
              PLERROR("In RBMModule::fprop visible_expectation can only be an output port (use visible as input port");
@@ -833,6 +900,17 @@
         {
             // the visible_layer->expectations contain the "state" from which we
             // start or continue the chain
+            if (visible_layer->samples.isEmpty())
+            {
+                if (visible && !visible->isEmpty())
+                    visible_layer->samples << *visible;
+                else if (!visible_layer->getExpectations().isEmpty())
+                    visible_layer->samples << visible_layer->getExpectations();
+                else if (!hidden_layer->samples.isEmpty())
+                    sampleVisibleGivenHidden(hidden_layer->samples);    
+                else if (!hidden_layer->getExpectations().isEmpty())
+                    sampleVisibleGivenHidden(hidden_layer->getExpectations());    
+            }
             int min_n = max(Gibbs_step+n_Gibbs_steps_per_generated_sample,
                             min_n_Gibbs_steps);
             //cout << "Gibbs sampling " << Gibbs_step+1;
@@ -841,7 +919,8 @@
                 sampleHiddenGivenVisible(visible_layer->samples);
                 sampleVisibleGivenHidden(hidden_layer->samples);
             }
-              //cout << " -> " << Gibbs_step << endl;
+            hidden_activations_are_computed = false;
+            //cout << " -> " << Gibbs_step << endl;
         }
 
         if ( hidden && hidden->isEmpty())   // fill hidden.state with expectations
@@ -869,6 +948,18 @@
                                         to_store.width());
             *visible_expectation << to_store;
         }
+        if (hidden && hidden->isEmpty())
+        {
+            hidden->resize(hidden_layer->samples.length(),
+                           hidden_layer->samples.width());
+            *hidden << hidden_layer->samples;
+        }
+        if (hidden_act && hidden_act->isEmpty())
+        {
+            hidden_act->resize(hidden_layer->samples.length(),
+                               hidden_layer->samples.width());
+            *hidden_act << hidden_layer->getExpectations();
+        }
         found_a_valid_configuration = true;
     }// END SAMPLING
 
@@ -923,6 +1014,7 @@
                 sampleVisibleGivenHidden(hidden_layer->samples);
                 // compute corresponding hidden expectations.
                 computeHiddenActivations(visible_layer->samples);
+                hidden_activations_are_computed = false;
                 hidden_layer->computeExpectations();
             }
             PLASSERT(negative_phase_visible_samples);
@@ -969,77 +1061,6 @@
     }
 
 
-    // COMPUTE AUTOASSOCIATOR RECONSTRUCTION ERROR
-    if ( visible && !visible->isEmpty() &&
-         ( ( visible_reconstruction && visible_reconstruction->isEmpty() ) ||
-           ( visible_reconstruction_activations &&
-             visible_reconstruction_activations->isEmpty() ) ||
-           ( reconstruction_error && reconstruction_error->isEmpty() ) ) )
-    {
-        // Autoassociator reconstruction cost
-        PLASSERT( ports_value.length() == nPorts() );
-        computePositivePhaseHiddenActivations(*visible);
-        if(!hidden_expectations_are_computed)
-        {
-            hidden_layer->computeExpectations();
-            hidden_expectations_are_computed=true;
-        }
-
-        // Don't need to verify if they are asked in a port, this was done previously
-
-        computeVisibleActivations(hidden_layer->getExpectations(), true);
-        if(visible_reconstruction_activations)
-        {
-            PLASSERT( visible_reconstruction_activations->isEmpty() );
-            const Mat& to_store = visible_layer->activations;
-            visible_reconstruction_activations->resize(to_store.length(),
-                                                       to_store.width());
-            *visible_reconstruction_activations << to_store;
-        }
-        if (visible_reconstruction || reconstruction_error)
-        {
-            visible_layer->computeExpectations();
-            if(visible_reconstruction)
-            {
-                PLASSERT( visible_reconstruction->isEmpty() );
-                const Mat& to_store = visible_layer->getExpectations();
-                visible_reconstruction->resize(to_store.length(),
-                                               to_store.width());
-                *visible_reconstruction << to_store;
-            }
-            if(reconstruction_error)
-            {
-                PLASSERT( reconstruction_error->isEmpty() );
-                reconstruction_error->resize(visible->length(),1);
-                visible_layer->fpropNLL(*visible,
-                                        *reconstruction_error);
-            }
-        }
-        found_a_valid_configuration = true;
-    }
-    // COMPUTE VISIBLE GIVEN HIDDEN
-    else if ( visible_reconstruction && visible_reconstruction->isEmpty()
-         && hidden && !hidden->isEmpty())
-    {
-        // Don't need to verify if they are asked in a port, this was done previously
-        computeVisibleActivations(*hidden,true);
-        if(visible_reconstruction_activations)
-        {
-            PLASSERT( visible_reconstruction_activations->isEmpty() );
-            const Mat& to_store = visible_layer->activations;
-            visible_reconstruction_activations->resize(to_store.length(),
-                                                       to_store.width());
-            *visible_reconstruction_activations << to_store;
-        }
-        visible_layer->computeExpectations();
-        PLASSERT( visible_reconstruction->isEmpty() );
-        const Mat& to_store = visible_layer->getExpectations();
-        visible_reconstruction->resize(to_store.length(),
-                                       to_store.width());
-        *visible_reconstruction << to_store;
-        found_a_valid_configuration = true;
-    }
-
     // Reset some class fields to ensure they are not reused by mistake.
     hidden_act = NULL;
     hidden_bias = NULL;



From lamblin at mail.berlios.de  Tue Jul 10 01:36:06 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 10 Jul 2007 01:36:06 +0200
Subject: [Plearn-commits] r7728 - trunk/plearn/io
Message-ID: <200707092336.l69Na6t1029978@sheep.berlios.de>

Author: lamblin
Date: 2007-07-10 01:36:05 +0200 (Tue, 10 Jul 2007)
New Revision: 7728

Modified:
   trunk/plearn/io/PStream.h
Log:
I love trial and error


Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2007-07-09 23:18:32 UTC (rev 7727)
+++ trunk/plearn/io/PStream.h	2007-07-09 23:36:05 UTC (rev 7728)
@@ -41,7 +41,7 @@
 #include <set>
 #include <sstream>
 #include <fstream>
-#include <plearn/base/pstdint.h>
+#include <stdint.h>
 #include <plearn/base/byte_order.h>
 #include <plearn/base/pl_hash_fun.h>
 #include <plearn/base/plerror.h>



From saintmlx at mail.berlios.de  Tue Jul 10 01:48:06 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 10 Jul 2007 01:48:06 +0200
Subject: [Plearn-commits] r7729 - trunk/plearn/python
Message-ID: <200707092348.l69Nm6fv030626@sheep.berlios.de>

Author: saintmlx
Date: 2007-07-10 01:48:05 +0200 (Tue, 10 Jul 2007)
New Revision: 7729

Modified:
   trunk/plearn/python/PythonExtension.cc
   trunk/plearn/python/PythonExtension.h
Log:
- Trial and error rules! ;)



Modified: trunk/plearn/python/PythonExtension.cc
===================================================================
--- trunk/plearn/python/PythonExtension.cc	2007-07-09 23:36:05 UTC (rev 7728)
+++ trunk/plearn/python/PythonExtension.cc	2007-07-09 23:48:05 UTC (rev 7729)
@@ -75,13 +75,9 @@
 static PObjectPool<PyMethodDef> pyfuncs(50);
 static TVec<string> funcs_help;
 
-// Init func for python module.
-// init module, then inject global funcs
-void initPythonExtensionModule(char* module_name)
+
+void injectPLearnGlobalFunctions(PyObject* env)
 {
-    PythonObjectWrapper::initializePython();
-    PyObject* plext= Py_InitModule(module_name, NULL);
-
     const RemoteMethodMap::MethodMap& global_funcs= 
         getGlobalFunctionMap().getMap();
 
@@ -102,11 +98,18 @@
                                        it->first.second));
         py_method->ml_doc= const_cast<char*>(funcs_help.last().c_str());
     
+        /* module= env if env is a module; NULL otherwise */
+        PyObject* module= 0;
+        if(PyModule_Check(env))
+            module= env;
+
+        // N.B.: module == NULL works on python2.3, 2.4 and 2.5, but is not
+        // documented
         PyObject* pyfunc= 
-            PyCFunction_NewEx(py_method, self, plext);
+            PyCFunction_NewEx(py_method, self, module);
 	    
         if(pyfunc) 
-            PyObject_SetAttrString(plext, 
+            PyObject_SetAttrString(env, 
                                    py_method->ml_name, 
                                    pyfunc);
         else
@@ -116,6 +119,16 @@
     }
 }
 
+
+// Init func for python module.
+// init module, then inject global funcs
+void initPythonExtensionModule(char* module_name)
+{
+    PythonObjectWrapper::initializePython();
+    PyObject* plext= Py_InitModule(module_name, NULL);
+    injectPLearnGlobalFunctions(plext);
+}
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/python/PythonExtension.h
===================================================================
--- trunk/plearn/python/PythonExtension.h	2007-07-09 23:36:05 UTC (rev 7728)
+++ trunk/plearn/python/PythonExtension.h	2007-07-09 23:48:05 UTC (rev 7729)
@@ -41,6 +41,9 @@
 // Trampoline for global PLearn 'remote' functions
   PyObject* pythonGlobalFuncTramp(PyObject* self, PyObject* args);
 
+// inject all PLearn global functions into a python env.
+void injectPLearnGlobalFunctions(PyObject* env);
+
 // Init func for python module.
   void initPythonExtensionModule(char* module_name);
 



From lamblin at mail.berlios.de  Tue Jul 10 02:08:54 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 10 Jul 2007 02:08:54 +0200
Subject: [Plearn-commits] r7730 - trunk/plearn/io
Message-ID: <200707100008.l6A08s5n031843@sheep.berlios.de>

Author: lamblin
Date: 2007-07-10 02:08:53 +0200 (Tue, 10 Jul 2007)
New Revision: 7730

Modified:
   trunk/plearn/io/PStream.h
Log:
Trial++


Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2007-07-09 23:48:05 UTC (rev 7729)
+++ trunk/plearn/io/PStream.h	2007-07-10 00:08:53 UTC (rev 7730)
@@ -478,7 +478,7 @@
 
     //! Overload to print out raw pointers in the form 0x????????
     PStream& operator<<(const void *x);
-    
+
     //! Warning: string output will be formatted according to outmode
     //! (if you want to output a raw string use the write method instead)
     //! (unless you're in raw_ascii or raw_binary mode!)
@@ -493,8 +493,8 @@
     PStream& operator<<(bool x);  
     PStream& operator<<(int x);
     PStream& operator<<(unsigned int x);
-    //PStream& operator<<(long x);
-    //PStream& operator<<(unsigned long x);
+    PStream& operator<<(long x);
+    PStream& operator<<(unsigned long x);
     PStream& operator<<(int64_t x);
     PStream& operator<<(uint64_t x);
     //PStream& operator<<(long long x);



From lamblin at mail.berlios.de  Tue Jul 10 02:10:09 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 10 Jul 2007 02:10:09 +0200
Subject: [Plearn-commits] r7731 - trunk/plearn/io
Message-ID: <200707100010.l6A0A9gV031949@sheep.berlios.de>

Author: lamblin
Date: 2007-07-10 02:10:08 +0200 (Tue, 10 Jul 2007)
New Revision: 7731

Modified:
   trunk/plearn/io/PStream.cc
Log:
Oops


Modified: trunk/plearn/io/PStream.cc
===================================================================
--- trunk/plearn/io/PStream.cc	2007-07-10 00:08:53 UTC (rev 7730)
+++ trunk/plearn/io/PStream.cc	2007-07-10 00:10:08 UTC (rev 7731)
@@ -1842,7 +1842,7 @@
     return *this;
 }
 
-/* Commented out because "long" has not the same size on every platform
+//* Commented out because "long" has not the same size on every platform
 PStream& PStream::operator<<(long x) 
 { 
     switch(outmode)
@@ -1883,7 +1883,7 @@
     }
     return *this;
 }
-*/
+//*/
 
 PStream& PStream::operator<<(int64_t x)
 {



From lamblin at mail.berlios.de  Tue Jul 10 02:23:31 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 10 Jul 2007 02:23:31 +0200
Subject: [Plearn-commits] r7732 - trunk/plearn/io
Message-ID: <200707100023.l6A0NVSo001314@sheep.berlios.de>

Author: lamblin
Date: 2007-07-10 02:23:26 +0200 (Tue, 10 Jul 2007)
New Revision: 7732

Modified:
   trunk/plearn/io/PStream.cc
Log:
Trial++


Modified: trunk/plearn/io/PStream.cc
===================================================================
--- trunk/plearn/io/PStream.cc	2007-07-10 00:10:08 UTC (rev 7731)
+++ trunk/plearn/io/PStream.cc	2007-07-10 00:23:26 UTC (rev 7732)
@@ -1915,7 +1915,7 @@
     return *this;
 }
 
-/*
+//*
 PStream& PStream::operator<<(unsigned long x) 
 { 
     switch(outmode)
@@ -1945,7 +1945,7 @@
     }
     return *this;
 }
-*/
+//*/
 
 PStream& PStream::operator<<(uint64_t x)
 {



From lamblin at mail.berlios.de  Tue Jul 10 03:19:08 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 10 Jul 2007 03:19:08 +0200
Subject: [Plearn-commits] r7733 - in trunk: plearn/base plearn/io
	plearn/math plearn/misc plearn/vmat plearn_learners/generic
	plearn_learners/online plearn_learners_experimental/SurfaceTemplate
Message-ID: <200707100119.l6A1J89S005548@sheep.berlios.de>

Author: lamblin
Date: 2007-07-10 03:19:04 +0200 (Tue, 10 Jul 2007)
New Revision: 7733

Modified:
   trunk/plearn/base/ProgressBar.cc
   trunk/plearn/base/ProgressBar.h
   trunk/plearn/io/PStream.cc
   trunk/plearn/io/PStream.h
   trunk/plearn/math/PRandom.cc
   trunk/plearn/math/PRandom.h
   trunk/plearn/math/random.cc
   trunk/plearn/math/random.h
   trunk/plearn/misc/vmatmain.cc
   trunk/plearn/vmat/AddMissingVMatrix.h
   trunk/plearn/vmat/BootstrapVMatrix.cc
   trunk/plearn/vmat/BootstrapVMatrix.h
   trunk/plearn/vmat/ClassSeparationSplitter.h
   trunk/plearn/vmat/MultiToUniInstanceSelectRandomVMatrix.h
   trunk/plearn/vmat/RandomSamplesFromVMatrix.h
   trunk/plearn/vmat/RandomSamplesVMatrix.h
   trunk/plearn/vmat/RepeatSplitter.h
   trunk/plearn/vmat/SelectRowsMultiInstanceVMatrix.h
   trunk/plearn/vmat/ShuffleColumnsVMatrix.h
   trunk/plearn/vmat/TrainTestSplitter.h
   trunk/plearn/vmat/VMatrixFromDistribution.h
   trunk/plearn_learners/generic/BestAveragingPLearner.cc
   trunk/plearn_learners/generic/BestAveragingPLearner.h
   trunk/plearn_learners/online/ModuleTester.cc
   trunk/plearn_learners/online/ModuleTester.h
   trunk/plearn_learners_experimental/SurfaceTemplate/ScoreLayerVariable.h
Log:
Replace long by int32_t and unsigned long by uint32_t.
Trial++


Modified: trunk/plearn/base/ProgressBar.cc
===================================================================
--- trunk/plearn/base/ProgressBar.cc	2007-07-10 00:23:26 UTC (rev 7732)
+++ trunk/plearn/base/ProgressBar.cc	2007-07-10 01:19:04 UTC (rev 7733)
@@ -67,7 +67,7 @@
     return plugin;
 }
 
-ProgressBar::ProgressBar(string _title, unsigned long the_maxpos)
+ProgressBar::ProgressBar(string _title, uint32_t the_maxpos)
     :title(_title),currentpos(0), maxpos(the_maxpos),closed(false)
 {
     if (plugin == NULL)
@@ -76,7 +76,7 @@
     plugin->addProgressBar(this);
 }
 
-ProgressBar::ProgressBar(ostream& _out, string _title, unsigned long the_maxpos)
+ProgressBar::ProgressBar(ostream& _out, string _title, uint32_t the_maxpos)
     :title(_title),currentpos(0), maxpos(the_maxpos),closed(false)
 {
     if (plugin == NULL)
@@ -84,7 +84,7 @@
 
     plugin->addProgressBar(this);
 }
-ProgressBar::ProgressBar(PStream& _out, string _title, unsigned long the_maxpos)
+ProgressBar::ProgressBar(PStream& _out, string _title, uint32_t the_maxpos)
     :title(_title),currentpos(0), maxpos(the_maxpos),closed(false)
 {
     if (plugin == NULL)
@@ -137,7 +137,7 @@
 #endif
 }
 
-void TextProgressBarPlugin::update(ProgressBar * pb,unsigned long newpos)
+void TextProgressBarPlugin::update(ProgressBar * pb, uint32_t newpos)
 {
 #if USING_MPI
     if(PLMPI::rank==0)
@@ -198,7 +198,7 @@
 }
 
 
-void RemoteProgressBarPlugin::update(ProgressBar* pb, unsigned long newpos)
+void RemoteProgressBarPlugin::update(ProgressBar* pb, uint32_t newpos)
 {
     // this handles the case where we reuse the same progress bar
     if(newpos < pb->currentpos)
@@ -242,7 +242,7 @@
 void LineOutputProgressBarPlugin::addProgressBar(ProgressBar* pb)
 { out << "In progress: " << pbInfo(pb) << endl; }
 
-void LineOutputProgressBarPlugin::update(ProgressBar* pb, unsigned long newpos)
+void LineOutputProgressBarPlugin::update(ProgressBar* pb, uint32_t newpos)
 {
     // this handles the case where we reuse the same progress bar
     if(newpos < pb->currentpos)

Modified: trunk/plearn/base/ProgressBar.h
===================================================================
--- trunk/plearn/base/ProgressBar.h	2007-07-10 00:23:26 UTC (rev 7732)
+++ trunk/plearn/base/ProgressBar.h	2007-07-10 01:19:04 UTC (rev 7733)
@@ -71,7 +71,7 @@
     virtual ~ProgressBarPlugin() {}
     virtual void addProgressBar(ProgressBar * pb){};
     virtual void killProgressBar(ProgressBar * pb){};
-    virtual void update(ProgressBar * pb, unsigned long newpos){};
+    virtual void update(ProgressBar * pb, uint32_t newpos){};
 };
 
 
@@ -82,7 +82,7 @@
     PStream out;
 public:
     virtual void addProgressBar(ProgressBar * pb);
-    virtual void update(ProgressBar * pb, unsigned long newpos);
+    virtual void update(ProgressBar * pb, uint32_t newpos);
 
     TextProgressBarPlugin(ostream& _out);
     TextProgressBarPlugin(PStream& _out);
@@ -97,7 +97,7 @@
 {
 public:
     virtual void addProgressBar(ProgressBar* pb);
-    virtual void update(ProgressBar* pb, unsigned long newpos);
+    virtual void update(ProgressBar* pb, uint32_t newpos);
 
     RemoteProgressBarPlugin(ostream& _out, unsigned int nticks_= 20);
     RemoteProgressBarPlugin(PStream& _out, unsigned int nticks_= 20);
@@ -119,7 +119,7 @@
 {
 public:
     virtual void addProgressBar(ProgressBar* pb);
-    virtual void update(ProgressBar* pb, unsigned long newpos);
+    virtual void update(ProgressBar* pb, uint32_t newpos);
 
     LineOutputProgressBarPlugin(ostream& _out, unsigned int nticks_= 100);
     LineOutputProgressBarPlugin(PStream& _out, unsigned int nticks_= 100);
@@ -154,20 +154,20 @@
 {
 public:
     string title;
-    unsigned long currentpos; // current position
-    unsigned long maxpos;
+    uint32_t currentpos; // current position
+    uint32_t maxpos;
 
     // creates a new progressbar with the given title and maxpos
     // *** Note, for now, ignore the stream (someday, remove this argument for 
     // every progressBar creation in PLearn)
-    ProgressBar(string _title, unsigned long the_maxpos);
-    ProgressBar(ostream& _out,string _title, unsigned long the_maxpos);
-    ProgressBar(PStream& _out,string _title, unsigned long the_maxpos);
+    ProgressBar(string _title, uint32_t the_maxpos);
+    ProgressBar(ostream& _out,string _title, uint32_t the_maxpos);
+    ProgressBar(PStream& _out,string _title, uint32_t the_maxpos);
 
     // moves the progressbar up to position newpos
-    void operator()(unsigned long newpos){plugin->update(this,newpos);}
+    void operator()(uint32_t newpos){plugin->update(this,newpos);}
 
-    void update(unsigned long newpos){plugin->update(this,newpos);}
+    void update(uint32_t newpos){plugin->update(this,newpos);}
 
     // this function assumes plugin is always a valid object (it is created statically in the .cc)
     static void setPlugin(PP<ProgressBarPlugin> plugin_) { plugin = plugin_; }

Modified: trunk/plearn/io/PStream.cc
===================================================================
--- trunk/plearn/io/PStream.cc	2007-07-10 00:23:26 UTC (rev 7732)
+++ trunk/plearn/io/PStream.cc	2007-07-10 01:19:04 UTC (rev 7733)
@@ -1842,7 +1842,7 @@
     return *this;
 }
 
-//* Commented out because "long" has not the same size on every platform
+/* Commented out because "long" has not the same size on every platform
 PStream& PStream::operator<<(long x) 
 { 
     switch(outmode)
@@ -1915,7 +1915,7 @@
     return *this;
 }
 
-//*
+/*
 PStream& PStream::operator<<(unsigned long x) 
 { 
     switch(outmode)

Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2007-07-10 00:23:26 UTC (rev 7732)
+++ trunk/plearn/io/PStream.h	2007-07-10 01:19:04 UTC (rev 7733)
@@ -493,8 +493,8 @@
     PStream& operator<<(bool x);  
     PStream& operator<<(int x);
     PStream& operator<<(unsigned int x);
-    PStream& operator<<(long x);
-    PStream& operator<<(unsigned long x);
+    //PStream& operator<<(long x);
+    //PStream& operator<<(unsigned long x);
     PStream& operator<<(int64_t x);
     PStream& operator<<(uint64_t x);
     //PStream& operator<<(long long x);

Modified: trunk/plearn/math/PRandom.cc
===================================================================
--- trunk/plearn/math/PRandom.cc	2007-07-10 00:23:26 UTC (rev 7732)
+++ trunk/plearn/math/PRandom.cc	2007-07-10 01:19:04 UTC (rev 7733)
@@ -53,7 +53,7 @@
 /////////////
 // PRandom //
 /////////////
-PRandom::PRandom(long seed):
+PRandom::PRandom(int32_t seed):
     exponential_distribution(0),
     normal_distribution(0),
     uniform_01(0),
@@ -174,7 +174,7 @@
         this->manual_seed_(seed_);
     else
         PLERROR("In PRandom::build_ - The only value allowed for the seed are "
-                "-1, 0 or a strictly positive long integer");
+                "-1, 0 or a strictly positive int32_t integer");
 }
 
 ////////////
@@ -279,7 +279,7 @@
 /////////////////
 // manual_seed //
 /////////////////
-void PRandom::manual_seed(long x)
+void PRandom::manual_seed(int32_t x)
 {
     if (fixed_seed)
         PLERROR("In PRandom::manual_seed - You are not allowed to change the seed "
@@ -291,9 +291,9 @@
 //////////////////
 // manual_seed_ //
 //////////////////
-void PRandom::manual_seed_(long x)
+void PRandom::manual_seed_(int32_t x)
 {
-    the_seed = boost::uint32_t(x);
+    the_seed = uint32_t(x);
     rgen.seed(the_seed);
     if (uniform_01) {
         // The boost::uniform_01 object must be re-constructed from the updated
@@ -349,7 +349,7 @@
     struct tm *today;
     time(&ltime);
     today = localtime(&ltime);
-    manual_seed_((long)today->tm_sec+
+    manual_seed_((int32_t)today->tm_sec+
                  60*today->tm_min+
                  60*60*today->tm_hour+
                  60*60*24*today->tm_mday);

Modified: trunk/plearn/math/PRandom.h
===================================================================
--- trunk/plearn/math/PRandom.h	2007-07-10 00:23:26 UTC (rev 7732)
+++ trunk/plearn/math/PRandom.h	2007-07-10 01:19:04 UTC (rev 7733)
@@ -83,14 +83,13 @@
     boost::uniform_01<boost::mt19937>* uniform_01;
 
     //! The actual seed used by the random number generator.
-    boost::uint32_t the_seed;
-    
+    uint32_t the_seed;
+
     // *********************
     // * protected options *
     // *********************
 
-    //long fixed_seed;
-    int fixed_seed;
+    int32_t fixed_seed;
 
 public:
 
@@ -98,15 +97,14 @@
     // * public build options *
     // ************************
 
-    //long seed_; // CAUSES PROBLEMS WITH PYTHON SERVER INTERFACE
-    int seed_;
+    int32_t seed_;
 
     // ****************
     // * Constructors *
     // ****************
 
     //! Constructor from a given seed.
-    PRandom(long seed = -1);
+    PRandom(int32_t seed = -1);
 
     //! Copy constructor.
     //! This constructor ensures that no deep-copy is needed. All fields are
@@ -133,17 +131,17 @@
     { return normal_distribution; }
     boost::uniform_01<boost::mt19937>*        get_uniform_01()               const
     { return uniform_01; }
-	
-    boost::uint32_t get_the_seed()   const { return the_seed; }
-    long            get_fixed_seed() const { return fixed_seed; }
-    long            get_seed()       const { return seed_; }
 
-private: 
+    uint32_t get_the_seed()   const { return the_seed; }
+    int32_t  get_fixed_seed() const { return fixed_seed; }
+    int32_t  get_seed()       const { return seed_; }
 
+private:
+
     //! This does the actual building. 
     void build_();
 
-protected: 
+protected:
 
     //! Declares this class' options.
     static void declareOptions(OptionList& ol);
@@ -152,9 +150,9 @@
     //! This is an internal method that does not update the 'seed' option.
     void time_seed_();
 
-    //! Initialize the random number generator with the given long 'x'.
+    //! Initialize the random number generator with the given int32_t 'x'.
     //! This is an internal method that does not update the 'seed' option.
-    void manual_seed_(long x);
+    void manual_seed_(int32_t x);
 
     //! Ensure the 'uniform_01' member is correctly initialized.
     //! This method is called in build(), so it should not be needed to call it
@@ -199,7 +197,7 @@
     //! accordingly.
     //! 'x' may be -1 to initialize from the current CPU time, or 0 to make
     //! no initialization.
-    void manual_seed(long x);
+    void manual_seed(int32_t x);
 
     //! Initialize the random number generator with the CPU time.
     inline void time_seed() { manual_seed(-1); }

Modified: trunk/plearn/math/random.cc
===================================================================
--- trunk/plearn/math/random.cc	2007-07-10 00:23:26 UTC (rev 7732)
+++ trunk/plearn/math/random.cc	2007-07-10 01:19:04 UTC (rev 7733)
@@ -52,8 +52,8 @@
     The static data to store the seed used by the random number generators.
 */
 
-static long  the_seed=0;
-static int   iset=0;
+static int32_t  the_seed=0;
+static int      iset=0;
 static real gset;
 
 /*  
@@ -182,7 +182,7 @@
     Rem: - The stored value is negative.
 */
 
-void  manual_seed(long x)
+void  manual_seed(int32_t x)
 {
     the_seed = - labs(x);
     iset     = 0;
@@ -198,7 +198,7 @@
     struct  tm *today;
     time(&ltime);
     today = localtime(&ltime);
-    manual_seed((long)today->tm_sec+
+    manual_seed((int32_t)today->tm_sec+
                 60*today->tm_min+
                 60*60*today->tm_hour+
                 60*60*24*today->tm_mday);
@@ -208,9 +208,9 @@
     get_seed(): returns the current value of the 'seed'.
 */
 
-long  get_seed()
+int32_t get_seed()
 {
-    long seed = the_seed;
+    int32_t seed = the_seed;
     return seed;
 }
 
@@ -248,10 +248,10 @@
 real uniform_sample()  
 {
     int j;
-    long k;
-    static long idum2=123456789;
-    static long iy=0;
-    static long iv[NTAB];
+    int32_t k;
+    static int32_t idum2=123456789;
+    static int32_t iy=0;
+    static int32_t iv[NTAB];
     real temp;
 
     if (the_seed <= 0) {

Modified: trunk/plearn/math/random.h
===================================================================
--- trunk/plearn/math/random.h	2007-07-10 00:23:26 UTC (rev 7732)
+++ trunk/plearn/math/random.h	2007-07-10 01:19:04 UTC (rev 7733)
@@ -72,10 +72,10 @@
 
 /*!   initializes the random number generator with the cpu time   */
 void  seed();
-/*!   initialzes the random number generator with the given long "x"   */
-void  manual_seed(long x);
+/*!   initialzes the random number generator with the given int32_t "x"   */
+void  manual_seed(int32_t x);
 /*!   returns the current seed used by the random number generator   */
-long  get_seed();
+int32_t  get_seed();
 
 /*!   returns a random number uniformly distributed between 0 and 1   */
 real  uniform_sample();

Modified: trunk/plearn/misc/vmatmain.cc
===================================================================
--- trunk/plearn/misc/vmatmain.cc	2007-07-10 00:23:26 UTC (rev 7732)
+++ trunk/plearn/misc/vmatmain.cc	2007-07-10 01:19:04 UTC (rev 7733)
@@ -627,14 +627,14 @@
         if(vvm!=NULL)
         {
             pout<< "Last modification (including dependencies of .vmat): "
-                << long(vvm->getMtime()) << endl;
+                << int32_t(vvm->getMtime()) << endl;
             bool ispre=vvm->isPrecomputedAndUpToDate();
             pout<<"precomputed && uptodate : ";
             if(ispre)
             {
                 pout <<"yes : " << vvm->getPrecomputedDataName()<<endl;
                 pout<< "timestamp of precom. data : "
-                    << long(getDataSetDate(vvm->getPrecomputedDataName()))
+                    << int32_t(getDataSetDate(vvm->getPrecomputedDataName()))
                     << endl;
             }
             else pout <<"no"<<endl;

Modified: trunk/plearn/vmat/AddMissingVMatrix.h
===================================================================
--- trunk/plearn/vmat/AddMissingVMatrix.h	2007-07-10 00:23:26 UTC (rev 7732)
+++ trunk/plearn/vmat/AddMissingVMatrix.h	2007-07-10 01:19:04 UTC (rev 7733)
@@ -73,7 +73,7 @@
   bool add_missing_target;
   real missing_prop;
   int  only_on_first;
-  long seed;
+  int32_t seed;
   //! Columns which will be filled with missing values
   TVec<int> missing_values_columns;
 

Modified: trunk/plearn/vmat/BootstrapVMatrix.cc
===================================================================
--- trunk/plearn/vmat/BootstrapVMatrix.cc	2007-07-10 00:23:26 UTC (rev 7732)
+++ trunk/plearn/vmat/BootstrapVMatrix.cc	2007-07-10 01:19:04 UTC (rev 7733)
@@ -66,7 +66,7 @@
 {}
 
 BootstrapVMatrix::BootstrapVMatrix(VMat m, real the_frac, bool the_shuffle,
-                                   long the_seed, bool allow_rep):
+                                   int32_t the_seed, bool allow_rep):
     rgen(new PRandom()),
     frac(the_frac),
     n_elems(-1),
@@ -94,7 +94,7 @@
     // We obtain the seed value that was actually used to initialize the Boost
     // random number generator, to ensure this VMat is always the same after
     // consecutive builds.
-    seed = long(the_rgen->get_the_seed());
+    seed = int32_t(the_rgen->get_the_seed());
     this->source = m;
     build();
 }

Modified: trunk/plearn/vmat/BootstrapVMatrix.h
===================================================================
--- trunk/plearn/vmat/BootstrapVMatrix.h	2007-07-10 00:23:26 UTC (rev 7732)
+++ trunk/plearn/vmat/BootstrapVMatrix.h	2007-07-10 01:19:04 UTC (rev 7733)
@@ -61,8 +61,8 @@
 
     real frac;
     int n_elems;
-    long own_seed;
-    long seed;
+    int32_t own_seed;
+    int32_t seed;
     bool shuffle;
     bool allow_repetitions;
 
@@ -73,7 +73,7 @@
 
     //! Construct a boostrap of another VMatrix.
     BootstrapVMatrix(VMat m, real frac, bool shuffle = false,
-                     long the_seed = 1827,
+                     int32_t the_seed = 1827,
                      bool allow_rep= false);
 
     //! Constructor which takes directly a PRandom object instead of a seed.

Modified: trunk/plearn/vmat/ClassSeparationSplitter.h
===================================================================
--- trunk/plearn/vmat/ClassSeparationSplitter.h	2007-07-10 00:23:26 UTC (rev 7732)
+++ trunk/plearn/vmat/ClassSeparationSplitter.h	2007-07-10 01:19:04 UTC (rev 7733)
@@ -81,7 +81,7 @@
     bool append_train;
 
     //! Seed of random generator
-    long seed;
+    int32_t seed;
 
 public:
     //#####  Public Member Functions  #########################################

Modified: trunk/plearn/vmat/MultiToUniInstanceSelectRandomVMatrix.h
===================================================================
--- trunk/plearn/vmat/MultiToUniInstanceSelectRandomVMatrix.h	2007-07-10 00:23:26 UTC (rev 7732)
+++ trunk/plearn/vmat/MultiToUniInstanceSelectRandomVMatrix.h	2007-07-10 01:19:04 UTC (rev 7733)
@@ -67,7 +67,7 @@
     // ************************
 
     //! Random number generator seed
-    long seed;
+    int32_t seed;
 
     // ****************
     // * Constructors *

Modified: trunk/plearn/vmat/RandomSamplesFromVMatrix.h
===================================================================
--- trunk/plearn/vmat/RandomSamplesFromVMatrix.h	2007-07-10 00:23:26 UTC (rev 7732)
+++ trunk/plearn/vmat/RandomSamplesFromVMatrix.h	2007-07-10 01:19:04 UTC (rev 7733)
@@ -64,7 +64,7 @@
     //! If provided, will overwrite length by flength * source->length()
     real flength;
     //! Random number generator's seed
-    long seed;
+    int32_t seed;
 
 public:
     //#####  Public Member Functions  #########################################

Modified: trunk/plearn/vmat/RandomSamplesVMatrix.h
===================================================================
--- trunk/plearn/vmat/RandomSamplesVMatrix.h	2007-07-10 00:23:26 UTC (rev 7732)
+++ trunk/plearn/vmat/RandomSamplesVMatrix.h	2007-07-10 01:19:04 UTC (rev 7733)
@@ -64,7 +64,7 @@
 
     string is_preserved;
     int n_non_preserved;
-    long seed;
+    int32_t seed;
 
     //! ### declare public option fields (such as build options) here
     //! Start your comments with Doxygen-compatible comments such as //!

Modified: trunk/plearn/vmat/RepeatSplitter.h
===================================================================
--- trunk/plearn/vmat/RepeatSplitter.h	2007-07-10 00:23:26 UTC (rev 7732)
+++ trunk/plearn/vmat/RepeatSplitter.h	2007-07-10 01:19:04 UTC (rev 7733)
@@ -72,7 +72,7 @@
     bool do_not_shuffle_first;
     real force_proportion;
     int n;
-    long seed;
+    int32_t seed;
     int shuffle;
     PP<Splitter> to_repeat;
 

Modified: trunk/plearn/vmat/SelectRowsMultiInstanceVMatrix.h
===================================================================
--- trunk/plearn/vmat/SelectRowsMultiInstanceVMatrix.h	2007-07-10 00:23:26 UTC (rev 7732)
+++ trunk/plearn/vmat/SelectRowsMultiInstanceVMatrix.h	2007-07-10 01:19:04 UTC (rev 7733)
@@ -69,7 +69,7 @@
     //! Start your comments with Doxygen-compatible comments such as //!
 
     //! random number generator seed
-    long seed;
+    int32_t seed;
     TVec<int> indices; // indices of the kept rows
     TVec<int> mi_info; // bag info for the reduced lookup set.
     PP<PLearner> multi_nnet;

Modified: trunk/plearn/vmat/ShuffleColumnsVMatrix.h
===================================================================
--- trunk/plearn/vmat/ShuffleColumnsVMatrix.h	2007-07-10 00:23:26 UTC (rev 7732)
+++ trunk/plearn/vmat/ShuffleColumnsVMatrix.h	2007-07-10 01:19:04 UTC (rev 7733)
@@ -69,7 +69,7 @@
     // ************************
 
     bool only_shuffle_inputs;
-    long seed;
+    int32_t seed;
 
     // ****************
     // * Constructors *

Modified: trunk/plearn/vmat/TrainTestSplitter.h
===================================================================
--- trunk/plearn/vmat/TrainTestSplitter.h	2007-07-10 00:23:26 UTC (rev 7732)
+++ trunk/plearn/vmat/TrainTestSplitter.h	2007-07-10 01:19:04 UTC (rev 7733)
@@ -57,7 +57,7 @@
     // *********************
 
     // ### declare protected option fields (such as learnt parameters) here
-    
+
     TVec<int> train_indices, test_indices;
 
 public:
@@ -70,7 +70,7 @@
     real test_fraction; // the fraction of the dataset to be used as test (hese will be the last few samples of the dataset)
     bool calc_with_pct;
     int test_fraction_abs;
-    long shuffle_seed;
+    int32_t shuffle_seed;
 
     // ****************
     // * Constructors *

Modified: trunk/plearn/vmat/VMatrixFromDistribution.h
===================================================================
--- trunk/plearn/vmat/VMatrixFromDistribution.h	2007-07-10 00:23:26 UTC (rev 7732)
+++ trunk/plearn/vmat/VMatrixFromDistribution.h	2007-07-10 01:19:04 UTC (rev 7733)
@@ -69,7 +69,7 @@
     string mode; // one of "sample" "density" "log_density"
 
     // for "sample" mode
-    long generator_seed; // the generator_seed to initialize the generator
+    int32_t generator_seed; // the generator_seed to initialize the generator
     int nsamples; // number of samples to draw
 
     // for density mode:

Modified: trunk/plearn_learners/generic/BestAveragingPLearner.cc
===================================================================
--- trunk/plearn_learners/generic/BestAveragingPLearner.cc	2007-07-10 00:23:26 UTC (rev 7732)
+++ trunk/plearn_learners/generic/BestAveragingPLearner.cc	2007-07-10 01:19:04 UTC (rev 7733)
@@ -186,7 +186,7 @@
                     __FUNCTION__);
 
         const int N = m_total_learner_num;
-        long cur_seed = m_initial_seed;
+        int32_t cur_seed = m_initial_seed;
         m_learner_set.resize(N);
         for (int i=0 ; i<N ; ++i) {
             PP<PLearner> new_learner = PLearn::deepCopy(m_learner_template);

Modified: trunk/plearn_learners/generic/BestAveragingPLearner.h
===================================================================
--- trunk/plearn_learners/generic/BestAveragingPLearner.h	2007-07-10 00:23:26 UTC (rev 7732)
+++ trunk/plearn_learners/generic/BestAveragingPLearner.h	2007-07-10 01:19:04 UTC (rev 7733)
@@ -98,7 +98,7 @@
      *  that is being instantiated.  If this value is <= 0, it is used as-is
      *  without being incremented.
      */
-    long m_initial_seed;
+    int32_t m_initial_seed;
 
     /**
      *  Use in conjunction with 'initial_seed'; option name pointing to the

Modified: trunk/plearn_learners/online/ModuleTester.cc
===================================================================
--- trunk/plearn_learners/online/ModuleTester.cc	2007-07-10 00:23:26 UTC (rev 7732)
+++ trunk/plearn_learners/online/ModuleTester.cc	2007-07-10 01:19:04 UTC (rev 7733)
@@ -53,7 +53,7 @@
 );
 
 ModuleTester::ModuleTester():
-    seeds(TVec<long>(1, long(1827))),
+    seeds(TVec<int32_t>(1, int32_t(1827))),
     default_length(10),
     default_width(5),
     max_in(1),
@@ -191,7 +191,7 @@
     TVec<Mat> mats(max_mats_size);
 
     PP<PRandom> sub_rng = NULL;
-    long default_seed = 1827;
+    int32_t default_seed = 1827;
     if (!module->random_gen) {
         // The module needs to be provided a random generator.
         sub_rng = new PRandom();

Modified: trunk/plearn_learners/online/ModuleTester.h
===================================================================
--- trunk/plearn_learners/online/ModuleTester.h	2007-07-10 00:23:26 UTC (rev 7732)
+++ trunk/plearn_learners/online/ModuleTester.h	2007-07-10 01:19:04 UTC (rev 7733)
@@ -66,7 +66,7 @@
     TVec< map<string, TVec<string> > > configurations;
     map< string, PP<VMatrix> > sampling_data;
 
-    TVec<long> seeds;
+    TVec<int32_t> seeds;
 
     int default_length;
     int default_width;

Modified: trunk/plearn_learners_experimental/SurfaceTemplate/ScoreLayerVariable.h
===================================================================
--- trunk/plearn_learners_experimental/SurfaceTemplate/ScoreLayerVariable.h	2007-07-10 00:23:26 UTC (rev 7732)
+++ trunk/plearn_learners_experimental/SurfaceTemplate/ScoreLayerVariable.h	2007-07-10 01:19:04 UTC (rev 7733)
@@ -79,7 +79,7 @@
     int n_active_templates;
     int n_inactive_templates;
     bool normalize_by_n_features;
-    long seed_;
+    int32_t seed_;
     bool simple_mixture;
     VMat templates_source;
 



From lamblin at mail.berlios.de  Tue Jul 10 04:04:17 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 10 Jul 2007 04:04:17 +0200
Subject: [Plearn-commits] r7734 - trunk/plearn/python/test
Message-ID: <200707100204.l6A24HYM008188@sheep.berlios.de>

Author: lamblin
Date: 2007-07-10 04:04:15 +0200 (Tue, 10 Jul 2007)
New Revision: 7734

Modified:
   trunk/plearn/python/test/BasicIdentityCallsTest.cc
   trunk/plearn/python/test/InterfunctionXchgTest.cc
   trunk/plearn/python/test/MemoryStressTest.cc
Log:
Also update tests


Modified: trunk/plearn/python/test/BasicIdentityCallsTest.cc
===================================================================
--- trunk/plearn/python/test/BasicIdentityCallsTest.cc	2007-07-10 01:19:04 UTC (rev 7733)
+++ trunk/plearn/python/test/BasicIdentityCallsTest.cc	2007-07-10 02:04:15 UTC (rev 7734)
@@ -198,7 +198,7 @@
          << python->invoke("unary_int", 42).as<int>() << endl;
 
     cout << "Calling unary_long(42L)     : "
-         << python->invoke("unary_long", 42L).as<long>() << endl;
+         << python->invoke("unary_long", 42L).as<int32_t>() << endl;
 
     cout << "Calling unary_float(42.01)  : "
          << python->invoke("unary_float", 42.01).as<double>() << endl;
@@ -241,13 +241,13 @@
                                    .as< vector<string> >() ))
          << endl;
 
-    map<string,long> mapsd;
+    map<string,int32_t> mapsd;
     string str_mapsd = "{ Oui:16 il:32 est:64 juste:128 et:256 bon:512 }";
     PStream is_mapsd = openString(str_mapsd, PStream::plearn_ascii);
     is_mapsd >> mapsd;
 
     cout << "Calling unary_dict(mapsd)   : "
-         << tostring( python->invoke("unary_dict", mapsd).as< map<string,long> >() )
+         << tostring( python->invoke("unary_dict", mapsd).as< map<string,int32_t> >() )
          << endl;
 }
 

Modified: trunk/plearn/python/test/InterfunctionXchgTest.cc
===================================================================
--- trunk/plearn/python/test/InterfunctionXchgTest.cc	2007-07-10 01:19:04 UTC (rev 7733)
+++ trunk/plearn/python/test/InterfunctionXchgTest.cc	2007-07-10 02:04:15 UTC (rev 7734)
@@ -198,7 +198,7 @@
 
 
     try {
-        map<string,long> mapsd;
+        map<string,int32_t> mapsd;
         string str_mapsd = "{ Oui:16 il:32 est:64 juste:128 et:256 bon:512 }";
         PStream is_mapsd = openString(str_mapsd, PStream::plearn_ascii);
         is_mapsd >> mapsd;
@@ -206,7 +206,7 @@
         python_other->setGlobalObject("some_global_map", PythonObjectWrapper(mapsd));
         cout << "Associated 'some_global_map' with: " << tostring(mapsd) << endl;
         cout << "Read back from Python environment: "
-             << tostring(python_other->getGlobalObject("some_global_map").as< map<string,long> >())
+             << tostring(python_other->getGlobalObject("some_global_map").as< map<string,int32_t> >())
              << endl;
         python_other->invoke("print_global_map");
 

Modified: trunk/plearn/python/test/MemoryStressTest.cc
===================================================================
--- trunk/plearn/python/test/MemoryStressTest.cc	2007-07-10 01:19:04 UTC (rev 7733)
+++ trunk/plearn/python/test/MemoryStressTest.cc	2007-07-10 02:04:15 UTC (rev 7734)
@@ -190,10 +190,10 @@
 
 void MemoryStressTest::unary(const PythonCodeSnippet* python)
 {
-    int i    = python->invoke("unary_int", 42).as<int>();
-    long l   = python->invoke("unary_long", 42L).as<long>();
-    double d = python->invoke("unary_float", 42.01).as<double>();
-    string s = python->invoke("unary_str", "Hello").as<string>();
+    int i       = python->invoke("unary_int", 42).as<int>();
+    int32_t l   = python->invoke("unary_long", 42L).as<int32_t>();
+    double d    = python->invoke("unary_float", 42.01).as<double>();
+    string s    = python->invoke("unary_str", "Hello").as<string>();
 
     i = i;
     l = l;
@@ -218,12 +218,12 @@
     TVec<string> py_tvs    = python->invoke("unary_list_str", tvs).as< TVec<string> >();
     vector<string> py_vecs = python->invoke("unary_list_str", vecs).as< vector<string> >();
 
-    map<string,long> mapsd;
+    map<string,int32_t> mapsd;
     string str_mapsd = "{ Oui:16 il:32 est:64 juste:128 et:256 bon:512 }";
     PStream is_mapsd = openString(str_mapsd, PStream::plearn_ascii);
     is_mapsd >> mapsd;
 
-    map<string,long> py_mapsd = python->invoke("unary_dict", mapsd).as< map<string,long> >();
+    map<string,int32_t> py_mapsd = python->invoke("unary_dict", mapsd).as< map<string,int32_t> >();
 }
 
 void MemoryStressTest::binary(const PythonCodeSnippet* python)



From lamblin at mail.berlios.de  Tue Jul 10 06:27:02 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 10 Jul 2007 06:27:02 +0200
Subject: [Plearn-commits] r7735 - in trunk/plearn/python: . test
Message-ID: <200707100427.l6A4R2kc018479@sheep.berlios.de>

Author: lamblin
Date: 2007-07-10 06:27:01 +0200 (Tue, 10 Jul 2007)
New Revision: 7735

Modified:
   trunk/plearn/python/PythonObjectWrapper.cc
   trunk/plearn/python/PythonObjectWrapper.h
   trunk/plearn/python/test/BasicIdentityCallsTest.cc
   trunk/plearn/python/test/InterfunctionXchgTest.cc
   trunk/plearn/python/test/MemoryStressTest.cc
Log:
More fixes to the python stuff


Modified: trunk/plearn/python/PythonObjectWrapper.cc
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.cc	2007-07-10 02:04:15 UTC (rev 7734)
+++ trunk/plearn/python/PythonObjectWrapper.cc	2007-07-10 04:27:01 UTC (rev 7735)
@@ -113,6 +113,7 @@
     return static_cast<unsigned int>(PyInt_AsUnsignedLongMask(pyobj));
 }
 
+/*
 long ConvertFromPyObject<long>::convert(PyObject* pyobj, bool print_traceback)
 {
     PLASSERT( pyobj );
@@ -128,7 +129,24 @@
         PLPythonConversionError("ConvertFromPyObject<unsigned long>", pyobj, print_traceback);
     return PyInt_AsUnsignedLongMask(pyobj);
 }
+*/
 
+int64_t ConvertFromPyObject<int64_t>::convert(PyObject* pyobj, bool print_traceback)
+{
+    PLASSERT( pyobj );
+    if (! PyLong_Check(pyobj))
+        PLPythonConversionError("ConvertFromPyObject<int64_t>", pyobj, print_traceback);
+    return PyLong_AsLongLong(pyobj);
+}
+
+uint64_t ConvertFromPyObject<uint64_t>::convert(PyObject* pyobj, bool print_traceback)
+{
+    PLASSERT( pyobj );
+    if (! PyInt_Check(pyobj))
+        PLPythonConversionError("ConvertFromPyObject<uint64_t>", pyobj, print_traceback);
+    return PyInt_AsUnsignedLongLongMask(pyobj);
+}
+
 real ConvertFromPyObject<real>::convert(PyObject* pyobj, bool print_traceback)
 {
     PLASSERT( pyobj );
@@ -687,7 +705,8 @@
 {
     return PyLong_FromUnsignedLong(static_cast<unsigned long>(x));
 }
-    
+
+/*
 PyObject* ConvertToPyObject<long>::newPyObject(const long& x)
 {
     return PyLong_FromLong(x);
@@ -697,7 +716,18 @@
 {
     return PyLong_FromUnsignedLong(x);
 }
-    
+*/
+
+PyObject* ConvertToPyObject<int64_t>::newPyObject(const int64_t& x)
+{
+    return PyLong_FromLongLong(x);
+}
+
+PyObject* ConvertToPyObject<uint64_t>::newPyObject(const uint64_t& x)
+{
+    return PyLong_FromUnsignedLongLong(x);
+}
+
 PyObject* ConvertToPyObject<double>::newPyObject(const double& x)
 {
     return PyFloat_FromDouble(x);

Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2007-07-10 02:04:15 UTC (rev 7734)
+++ trunk/plearn/python/PythonObjectWrapper.h	2007-07-10 04:27:01 UTC (rev 7735)
@@ -131,6 +131,7 @@
     static unsigned int convert(PyObject*, bool print_traceback);
 };
 
+/*
 template <>
 struct ConvertFromPyObject<long>
 {
@@ -142,9 +143,22 @@
 {
     static unsigned long convert(PyObject*, bool print_traceback);
 };
+*/
 
+template <>
+struct ConvertFromPyObject<int64_t>
+{
+    static int64_t convert(PyObject*, bool print_traceback);
+};
 
 template <>
+struct ConvertFromPyObject<uint64_t>
+{
+    static uint64_t convert(PyObject*, bool print_traceback);
+};
+
+
+template <>
 struct ConvertFromPyObject<real>
 {
     static real convert(PyObject*, bool print_traceback);
@@ -268,17 +282,24 @@
 
 template<> struct ConvertToPyObject<bool>
 { static PyObject* newPyObject(const bool& x); };
-    
+
 template<> struct ConvertToPyObject<int>
 { static PyObject* newPyObject(const int& x); };
 template<> struct ConvertToPyObject<unsigned int>
 { static PyObject* newPyObject(const unsigned int& x); };
-    
+
+template<> struct ConvertToPyObject<int64_t>
+{ static PyObject* newPyObject(const int64_t& x); };
+template<> struct ConvertToPyObject<uint64_t>
+{ static PyObject* newPyObject(const uint64_t& x); };
+
+/*
 template<> struct ConvertToPyObject<long>
 { static PyObject* newPyObject(const long& x); };
 template<> struct ConvertToPyObject<unsigned long>
 { static PyObject* newPyObject(const unsigned long& x); };
-    
+*/
+
 template<> struct ConvertToPyObject<double>
 { static PyObject* newPyObject(const double& x); };
 
@@ -290,13 +311,13 @@
 
 template<size_t N> struct ConvertToPyObject<char[N]>
 { static PyObject* newPyObject(const char x[N]); };
-    
+
 template<> struct ConvertToPyObject<string>
 { static PyObject* newPyObject(const string& x); };
 
 template<> struct ConvertToPyObject<PPath>
 { static PyObject* newPyObject(const PPath& x); };
-  
+
 //! PLearn Vec: use numarray
 template<> struct ConvertToPyObject<Vec>
 { static PyObject* newPyObject(const Vec&); };

Modified: trunk/plearn/python/test/BasicIdentityCallsTest.cc
===================================================================
--- trunk/plearn/python/test/BasicIdentityCallsTest.cc	2007-07-10 02:04:15 UTC (rev 7734)
+++ trunk/plearn/python/test/BasicIdentityCallsTest.cc	2007-07-10 04:27:01 UTC (rev 7735)
@@ -198,7 +198,7 @@
          << python->invoke("unary_int", 42).as<int>() << endl;
 
     cout << "Calling unary_long(42L)     : "
-         << python->invoke("unary_long", 42L).as<int32_t>() << endl;
+         << python->invoke("unary_long", 42L).as<int64_t>() << endl;
 
     cout << "Calling unary_float(42.01)  : "
          << python->invoke("unary_float", 42.01).as<double>() << endl;

Modified: trunk/plearn/python/test/InterfunctionXchgTest.cc
===================================================================
--- trunk/plearn/python/test/InterfunctionXchgTest.cc	2007-07-10 02:04:15 UTC (rev 7734)
+++ trunk/plearn/python/test/InterfunctionXchgTest.cc	2007-07-10 04:27:01 UTC (rev 7735)
@@ -198,7 +198,7 @@
 
 
     try {
-        map<string,int32_t> mapsd;
+        map<string,int64_t> mapsd;
         string str_mapsd = "{ Oui:16 il:32 est:64 juste:128 et:256 bon:512 }";
         PStream is_mapsd = openString(str_mapsd, PStream::plearn_ascii);
         is_mapsd >> mapsd;
@@ -206,7 +206,7 @@
         python_other->setGlobalObject("some_global_map", PythonObjectWrapper(mapsd));
         cout << "Associated 'some_global_map' with: " << tostring(mapsd) << endl;
         cout << "Read back from Python environment: "
-             << tostring(python_other->getGlobalObject("some_global_map").as< map<string,int32_t> >())
+             << tostring(python_other->getGlobalObject("some_global_map").as< map<string,int64_t> >())
              << endl;
         python_other->invoke("print_global_map");
 

Modified: trunk/plearn/python/test/MemoryStressTest.cc
===================================================================
--- trunk/plearn/python/test/MemoryStressTest.cc	2007-07-10 02:04:15 UTC (rev 7734)
+++ trunk/plearn/python/test/MemoryStressTest.cc	2007-07-10 04:27:01 UTC (rev 7735)
@@ -191,14 +191,14 @@
 void MemoryStressTest::unary(const PythonCodeSnippet* python)
 {
     int i       = python->invoke("unary_int", 42).as<int>();
-    int32_t l   = python->invoke("unary_long", 42L).as<int32_t>();
+    int64_t l   = python->invoke("unary_long", 42L).as<int64_t>();
     double d    = python->invoke("unary_float", 42.01).as<double>();
     string s    = python->invoke("unary_str", "Hello").as<string>();
 
     i = i;
     l = l;
     d = d;
-    
+
     Vec v;
     string str_v = "[2,3,5,7,11,13,17,19,23]";
     PStream is = openString(str_v, PStream::plearn_ascii);
@@ -218,12 +218,12 @@
     TVec<string> py_tvs    = python->invoke("unary_list_str", tvs).as< TVec<string> >();
     vector<string> py_vecs = python->invoke("unary_list_str", vecs).as< vector<string> >();
 
-    map<string,int32_t> mapsd;
+    map<string,int64_t> mapsd;
     string str_mapsd = "{ Oui:16 il:32 est:64 juste:128 et:256 bon:512 }";
     PStream is_mapsd = openString(str_mapsd, PStream::plearn_ascii);
     is_mapsd >> mapsd;
 
-    map<string,int32_t> py_mapsd = python->invoke("unary_dict", mapsd).as< map<string,int32_t> >();
+    map<string,int64_t> py_mapsd = python->invoke("unary_dict", mapsd).as< map<string,int64_t> >();
 }
 
 void MemoryStressTest::binary(const PythonCodeSnippet* python)



From lamblin at mail.berlios.de  Tue Jul 10 07:19:11 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 10 Jul 2007 07:19:11 +0200
Subject: [Plearn-commits] r7736 - in trunk/plearn/python/test: .
	.pytest/EmbeddedPython_BasicIdentityCalls/expected_results
Message-ID: <200707100519.l6A5JBTw018119@sheep.berlios.de>

Author: lamblin
Date: 2007-07-10 07:19:02 +0200 (Tue, 10 Jul 2007)
New Revision: 7736

Modified:
   trunk/plearn/python/test/.pytest/EmbeddedPython_BasicIdentityCalls/expected_results/RUN.log
   trunk/plearn/python/test/BasicIdentityCallsTest.cc
   trunk/plearn/python/test/MemoryStressTest.cc
Log:
Update tests so they don't use "long" anymore


Modified: trunk/plearn/python/test/.pytest/EmbeddedPython_BasicIdentityCalls/expected_results/RUN.log
===================================================================
--- trunk/plearn/python/test/.pytest/EmbeddedPython_BasicIdentityCalls/expected_results/RUN.log	2007-07-10 04:27:01 UTC (rev 7735)
+++ trunk/plearn/python/test/.pytest/EmbeddedPython_BasicIdentityCalls/expected_results/RUN.log	2007-07-10 05:19:02 UTC (rev 7736)
@@ -49,14 +49,14 @@
 def quaternary(a,b,c,d):
     return a,b,c,d
 <<<
-isInvokable(nullary)        : 1
-Calling nullary             : Called nullary()
-Calling unary_int(42)       : 42
-Calling unary_long(42L)     : 42
-Calling unary_float(42.01)  : 42.01
-Calling unary_str('Hello')  : Hello
-Calling unary_vec(v)        : 2 3 5 7 11 13 17 19 23 
-Calling unary_mat(m)        : 
+isInvokable(nullary)           : 1
+Calling nullary                : Called nullary()
+Calling unary_int(42)          : 42
+Calling unary_long(int64_t(42)): 42
+Calling unary_float(42.01)     : 42.01
+Calling unary_str('Hello')     : Hello
+Calling unary_vec(v)           : 2 3 5 7 11 13 17 19 23 
+Calling unary_mat(m)           : 
 Called unary_mat with:
 [[  2.   3.   5.]
  [  7.  11.  13.]
@@ -65,7 +65,7 @@
 7	11	13	
 17	19	23	
 
-Calling unary_mat(m)        : 
+Calling unary_mat(m)           : 
 Called unary_mat with:
 [[  3.   5.]
  [ 11.  13.]
@@ -74,9 +74,9 @@
 11	13	
 19	23	
 
-Calling unary_list_str(tvs) : Cela est juste et bon 
-Calling unary_list_str(vecs): Cela est juste et bon 
-Calling unary_dict(mapsd)   : {Oui: 16, bon: 512, est: 64, et: 256, il: 32, juste: 128}
-Calling binary(2,4)         : 2 4 
-Calling ternary(2,4,8)      : 2 4 8 
-Calling quaternary(2,4,8,16): 2 4 8 16 
+Calling unary_list_str(tvs)    : Cela est juste et bon 
+Calling unary_list_str(vecs)   : Cela est juste et bon 
+Calling unary_dict(mapsd)      : {Oui: 16, bon: 512, est: 64, et: 256, il: 32, juste: 128}
+Calling binary(2,4)            : 2 4 
+Calling ternary(2,4,8)         : 2 4 8 
+Calling quaternary(2,4,8,16)   : 2 4 8 16 

Modified: trunk/plearn/python/test/BasicIdentityCallsTest.cc
===================================================================
--- trunk/plearn/python/test/BasicIdentityCallsTest.cc	2007-07-10 04:27:01 UTC (rev 7735)
+++ trunk/plearn/python/test/BasicIdentityCallsTest.cc	2007-07-10 05:19:02 UTC (rev 7736)
@@ -187,23 +187,23 @@
 
 void BasicIdentityCallsTest::nullary(const PythonCodeSnippet* python)
 {
-    cout << "isInvokable(nullary)        : " << python->isInvokable("nullary") << endl;
-    cout << "Calling nullary             : " << flush;
+    cout << "isInvokable(nullary)           : " << python->isInvokable("nullary") << endl;
+    cout << "Calling nullary                : " << flush;
     python->invoke("nullary");
 }
 
 void BasicIdentityCallsTest::unary(const PythonCodeSnippet* python)
 {
-    cout << "Calling unary_int(42)       : "
+    cout << "Calling unary_int(42)          : "
          << python->invoke("unary_int", 42).as<int>() << endl;
 
-    cout << "Calling unary_long(42L)     : "
-         << python->invoke("unary_long", 42L).as<int64_t>() << endl;
+    cout << "Calling unary_long(int64_t(42)): "
+         << python->invoke("unary_long", int64_t(42)).as<int64_t>() << endl;
 
-    cout << "Calling unary_float(42.01)  : "
+    cout << "Calling unary_float(42.01)     : "
          << python->invoke("unary_float", 42.01).as<double>() << endl;
 
-    cout << "Calling unary_str('Hello')  : "
+    cout << "Calling unary_str('Hello')     : "
          << python->invoke("unary_str", "Hello").as<string>() << endl;
 
     Vec v;
@@ -213,17 +213,17 @@
     Mat m(3,3);
     m.toVec() << v;
 
-    cout << "Calling unary_vec(v)        : "
+    cout << "Calling unary_vec(v)           : "
          << tostring( python->invoke("unary_vec", v).as<Vec>() )
          << endl;
 
     // Test full matrix (mod == width)
-    cout << "Calling unary_mat(m)        : " << endl;
+    cout << "Calling unary_mat(m)           : " << endl;
     cout << tostring( python->invoke("unary_mat", m).as<Mat>() )
          << endl;
 
     // Test sliced matrix (mod > width)
-    cout << "Calling unary_mat(m)        : " << endl;
+    cout << "Calling unary_mat(m)           : " << endl;
     cout << tostring( python->invoke("unary_mat", m.subMatColumns(1,2)).as<Mat>() )
          << endl;
 
@@ -233,10 +233,10 @@
     is_tvs >> tvs;
     vector<string> vecs(tvs.begin(), tvs.end());
 
-    cout << "Calling unary_list_str(tvs) : "
+    cout << "Calling unary_list_str(tvs)    : "
          << tostring( python->invoke("unary_list_str", tvs).as< TVec<string> >() )
          << endl;
-    cout << "Calling unary_list_str(vecs): "
+    cout << "Calling unary_list_str(vecs)   : "
          << tostring( TVec<string>(python->invoke("unary_list_str", vecs)
                                    .as< vector<string> >() ))
          << endl;
@@ -246,28 +246,28 @@
     PStream is_mapsd = openString(str_mapsd, PStream::plearn_ascii);
     is_mapsd >> mapsd;
 
-    cout << "Calling unary_dict(mapsd)   : "
+    cout << "Calling unary_dict(mapsd)      : "
          << tostring( python->invoke("unary_dict", mapsd).as< map<string,int32_t> >() )
          << endl;
 }
 
 void BasicIdentityCallsTest::binary(const PythonCodeSnippet* python)
 {
-    cout << "Calling binary(2,4)         : "
+    cout << "Calling binary(2,4)            : "
          << tostring( python->invoke("binary",2,4).as< TVec<int> >())
          << endl;
 }
 
 void BasicIdentityCallsTest::ternary(const PythonCodeSnippet* python)
 {
-    cout << "Calling ternary(2,4,8)      : "
+    cout << "Calling ternary(2,4,8)         : "
          << tostring( python->invoke("ternary",2,4,8).as< TVec<int> >())
          << endl;
 }
 
 void BasicIdentityCallsTest::quaternary(const PythonCodeSnippet* python)
 {
-    cout << "Calling quaternary(2,4,8,16): "
+    cout << "Calling quaternary(2,4,8,16)   : "
          << tostring( python->invoke("quaternary",2,4,8,16).as< TVec<int> >())
          << endl;
 }

Modified: trunk/plearn/python/test/MemoryStressTest.cc
===================================================================
--- trunk/plearn/python/test/MemoryStressTest.cc	2007-07-10 04:27:01 UTC (rev 7735)
+++ trunk/plearn/python/test/MemoryStressTest.cc	2007-07-10 05:19:02 UTC (rev 7736)
@@ -191,7 +191,7 @@
 void MemoryStressTest::unary(const PythonCodeSnippet* python)
 {
     int i       = python->invoke("unary_int", 42).as<int>();
-    int64_t l   = python->invoke("unary_long", 42L).as<int64_t>();
+    int64_t l   = python->invoke("unary_long", int64_t(42)).as<int64_t>();
     double d    = python->invoke("unary_float", 42.01).as<double>();
     string s    = python->invoke("unary_str", "Hello").as<string>();
 



From nouiz at mail.berlios.de  Tue Jul 10 16:14:35 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 10 Jul 2007 16:14:35 +0200
Subject: [Plearn-commits] r7737 - trunk/scripts
Message-ID: <200707101414.l6AEEZSC013858@sheep.berlios.de>

Author: nouiz
Date: 2007-07-10 16:14:35 +0200 (Tue, 10 Jul 2007)
New Revision: 7737

Modified:
   trunk/scripts/collectres
Log:
print the file name where an error occured


Modified: trunk/scripts/collectres
===================================================================
--- trunk/scripts/collectres	2007-07-10 05:19:02 UTC (rev 7736)
+++ trunk/scripts/collectres	2007-07-10 14:14:35 UTC (rev 7737)
@@ -116,7 +116,7 @@
                            load_pmat_as_array(filename))
       all_results.append([file_res,filename])
     except ValueError,v:
-      print >>sys.stderr, "caught ValueError exception!"
+      print >>sys.stderr, "caught ValueError exception in", filename
       print >>sys.stderr, v
   return all_results
 



From nouiz at mail.berlios.de  Tue Jul 10 17:12:15 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 10 Jul 2007 17:12:15 +0200
Subject: [Plearn-commits] r7738 - in trunk: . python_modules/plearn/pymake
Message-ID: <200707101512.l6AFCFix017835@sheep.berlios.de>

Author: nouiz
Date: 2007-07-10 17:12:15 +0200 (Tue, 10 Jul 2007)
New Revision: 7738

Modified:
   trunk/pymake.config.model
   trunk/python_modules/plearn/pymake/pymake.py
Log:
Added pymakeLinkOption that do not modify the objs dir but modify the executable filename
To do this, I added an in_output_filename attribute to PymakeOption


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2007-07-10 14:14:35 UTC (rev 7737)
+++ trunk/pymake.config.model	2007-07-10 15:12:15 UTC (rev 7738)
@@ -423,8 +423,9 @@
   # [ 'throwerrors', 'exiterrors' ],
   [ 'python23', 'python24', 'python25', 'nopython' ],
   
-  [ 'blas', 'p3blas','p4blas','athlonblas','pentiumblas', 'mammouthblas',
-    'noblas', 'veclib', 'scs', 'goto', 'lisa' ],
+  [ 'blas', 'noblas' ],
+  [ 'defblas', 'nolibblas', 'p3blas','p4blas','athlonblas','pentiumblas',
+    'mammouthblas', 'veclib', 'scs', 'goto', 'lisa' ],
   
   [ 'logging=dbg', 'logging=mand', 'logging=imp', 'logging=normal',
     'logging=extreme', 'logging=dbg-profile' ]
@@ -655,66 +656,67 @@
     lapack_linkeroptions = ''
 
 pymakeOption( name = 'noblas',
-              description = 'compilation and linking without BLAS',
+              description = 'compilation without BLAS',
               )
 
 pymakeOption( name = 'blas',
-              description = 'compilation and linking basic BLAS',
-              cpp_definitions = ['USE_BLAS_SPECIALISATIONS'],
+              description = 'compilation with BLAS',
+              cpp_definitions = ['USE_BLAS_SPECIALISATIONS']
+              )
+
+cpp_variables += ['USE_BLAS_SPECIALISATIONS']
+
+pymakeLinkOption( name = 'defblas',
+              description = 'linking with default BLAS',
               linkeroptions = lapack_linkeroptions + ' ' + blas_linkeroptions
               )
 
-pymakeOption( name = 'pentiumblas',
-              description = 'compilation and linking BLAS for Intel Pentium processor',
-              cpp_definitions = ['USE_BLAS_SPECIALISATIONS'],
+pymakeLinkOption( name = 'nolibblas',
+              description = 'linking without BLAS',
+              )
+
+pymakeLinkOption( name = 'pentiumblas',
+              description = 'linking BLAS for Intel Pentium processor',
               linkeroptions = '-L' + libdir +'/intelmkl/lib/32 -lmkl -lvml -lpthread -lg2c' )
 
-pymakeOption( name = 'p3blas',
-              description = 'compilation and linking BLAS for Intel Pentium 3 processor',
+pymakeLinkOption( name = 'p3blas',
+              description = 'linking BLAS for Intel Pentium 3 processor',
               linkeroptions = '-L'+ libdir + '/intelmkl/lib/32 -lmkl_p3 -lmkl_vml_p3 -lpthread -lg2c' )
 
-pymakeOption( name = 'athlonblas',
-              description = 'compilation and linking atlas BLAS for AMD Athlon processor',
-              cpp_definitions = ['USE_BLAS_SPECIALISATIONS'],
+pymakeLinkOption( name = 'athlonblas',
+              description = 'linking atlas BLAS for AMD Athlon processor',
               linkeroptions = '-L' + libdir + '/atlas_athlon256 -lcblas -lf77blas -latlas -lg2c' )
 
-pymakeOption( name = 'p4blas',
-              description = 'compilation and linking BLAS for Intel Pentium 4 processor',
-              cpp_definitions = ['USE_BLAS_SPECIALISATIONS'],
+pymakeLinkOption( name = 'p4blas',
+              description = 'linking BLAS for Intel Pentium 4 processor',
               linkeroptions = '-L' + libdir + '/intelmkl/lib/32 -lmkl_p4 -lmkl_vml_p4 -lpthread -lg2c' )
 
-pymakeOption( name = 'mammouthblas',
-              description = 'compilation and linking BLAS for P4 Mammouth-Serie cluster',
-              cpp_definitions = ['USE_BLAS_SPECIALISATIONS'],
+pymakeLinkOption( name = 'mammouthblas',
+              description = 'linking BLAS for P4 Mammouth-Serie cluster',
               linkeroptions = '-L/opt/mkl/lib/32 -lmkl_p4 -lmkl_vml_p4 -lpthread -lmkl_lapack' )
 
-pymakeOption( name = 'veclib',
+pymakeLinkOption( name = 'veclib',
               description = "Apple's vecLib library, a version of the BLAS library for the G4 and G5 under OS X",
-              cpp_definitions = ['USE_BLAS_SPECIALISATIONS'],
               linkeroptions = '-framework vecLib' )
 
-pymakeOption( name = 'scs',
+pymakeLinkOption( name = 'scs',
               description = "BLAS and lapack intel super optimized library",
-              cpp_definitions = ['USE_BLAS_SPECIALISATIONS'],
               linkeroptions = '-lscs -lpthread' )
 
-pymakeOption( name = 'goto',
-              description = 'compilation and linking using GOTO for BLAS',
-              cpp_definitions = ['USE_BLAS_SPECIALISATIONS'],
+pymakeLinkOption( name = 'goto',
+              description = 'linking using GOTO lib for BLAS',
               linkeroptions = '-L' + libdir +'goto -llapack -lgoto -lgfortran'
               )
 
 ## We must link again the static version of lapack as the dynamic version is linked again the default version of blas
 ## and we don't want to link again it. Also, we must remove fonction from lapack as some of them are also in GOTO
-pymakeOption( name = 'lisa',
-              description = 'compilation and linking using GOTO for BLAS',
-              cpp_definitions = ['USE_BLAS_SPECIALISATIONS'],
+pymakeLinkOption( name = 'lisa',
+              description = 'linking using recommended BLAS at LISA laboratory',
               linkeroptions = '-L' + libdir +'goto -llapack -lgoto -lgfortran'
               #lapack_for_goto.a is currently bugged
               #linkeroptions = '-L' + libdir +'goto /u/lisa/local/'+target_platform+'/lib/lapack_for_goto.a  -lgoto -lpthread -lgfortran -lg2c'
               )
 
-cpp_variables += ['USE_BLAS_SPECIALISATIONS']
 
 
 #####  Logging Behavior  ####################################################

Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2007-07-10 14:14:35 UTC (rev 7737)
+++ trunk/python_modules/plearn/pymake/pymake.py	2007-07-10 15:12:15 UTC (rev 7738)
@@ -478,7 +478,7 @@
 pymake_options_defs = {}
 
 class PymakeOption:
-    def __init__(self, name, description, compiler, compileroptions, cpp_definitions, linker, linkeroptions):
+    def __init__(self, name, description, compiler, compileroptions, cpp_definitions, linker, linkeroptions, in_output_dirname):
         self.name = name
         self.description = description
         self.compiler = compiler
@@ -486,12 +486,17 @@
         self.cpp_definitions = cpp_definitions
         self.linker = linker
         self.linkeroptions = linkeroptions
+        self.in_output_dirname = in_output_dirname
 
 # adds a possible option to the pymake_options_defs
-def pymakeOption( name, description, compiler='', compileroptions='', cpp_definitions=[], linker='', linkeroptions='' ):
-    pymake_options_defs[name] = PymakeOption(name, description, compiler, compileroptions, cpp_definitions, linker, linkeroptions)
+def pymakeOption( name, description, compiler='', compileroptions='', cpp_definitions=[], linker='', linkeroptions='', in_output_dirname=True):
+    pymake_options_defs[name] = PymakeOption(name, description, compiler, compileroptions, cpp_definitions, linker, linkeroptions, in_output_dirname)
 
 
+# adds a possible option to the pymake_options_defs that modify only the linker step
+def pymakeLinkOption( name, description, triggers='', linker='', linkeroptions='' ):
+    pymakeOption(name=name, description=description, linker=linker, linkeroptions=linkeroptions, in_output_dirname=False)
+    
 optional_libraries_defs = []
 
 # It is now possible to use lists in the linker and compiler options of an optional library.
@@ -1637,6 +1642,12 @@
 
             elif self.hasmain:
                 self.corresponding_output = join(self.filedir, objsdir, self.filebase)
+                # We append options to the file name if they are not appended to the objsdir name
+                for opt in options:
+                    pyopt = pymake_options_defs[opt]
+                    if not pyopt.in_output_dirname:
+                    #if objsdir.find('_' + opt) == -1: # if not found
+                        self.corresponding_output = self.corresponding_output + '_' + opt
 
         else:
             raise 'Attempting to build a FileInfo from a file that is not a .cc or .h or similar file ('+self.filepath+')'
@@ -1861,7 +1872,7 @@
         if symlink_source_basename is not None:
             symlink_to_base = symlink_source_basename
         else:
-            symlink_to_base = self.filebase
+            symlink_to_base = self.corresponding_output
 
         if not symlink_to:
             symlink_to = join(objsdir, symlink_to_base)
@@ -2739,9 +2750,15 @@
 
         # Building name of object subdirectory
         if  objspolicy== 1:
-            objsdir = join('OBJS', target_platform + '__' + string.join(options,'_'))
+            objsdir = join('OBJS', target_platform + '__')
         elif objspolicy == 2:
-            objsdir = join(objsdir, target_platform + '__' + string.join(options,'_'))
+            objsdir = join(objsdir, target_platform + '__')
+        # We append options name to the objsdir name if they modify the compiled objects file
+        # Otherwise we append them to the target_name
+        for opt in options:
+            pyopt = pymake_options_defs[opt]
+            if pyopt.in_output_dirname:
+                objsdir = objsdir + '_' + opt
 
         print '*** Running pymake on '+os.path.basename(target)+' using configuration file: ' + configpath
         print '*** Running pymake on '+os.path.basename(target)+' using options: ' + string.join(map(lambda o: '-'+o, options))



From nouiz at mail.berlios.de  Tue Jul 10 17:27:48 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 10 Jul 2007 17:27:48 +0200
Subject: [Plearn-commits] r7739 - trunk
Message-ID: <200707101527.l6AFRmNH018784@sheep.berlios.de>

Author: nouiz
Date: 2007-07-10 17:27:48 +0200 (Tue, 10 Jul 2007)
New Revision: 7739

Modified:
   trunk/pymake.config.model
Log:
Modif so that compiling and linking require only the -noblas option


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2007-07-10 15:12:15 UTC (rev 7738)
+++ trunk/pymake.config.model	2007-07-10 15:27:48 UTC (rev 7739)
@@ -674,6 +674,8 @@
 pymakeLinkOption( name = 'nolibblas',
               description = 'linking without BLAS',
               )
+if 'noblas' in optionargs and not 'nolibblas' in optionargs:
+    optionargs.append('nolibblas')
 
 pymakeLinkOption( name = 'pentiumblas',
               description = 'linking BLAS for Intel Pentium processor',



From larocheh at mail.berlios.de  Tue Jul 10 18:40:32 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Tue, 10 Jul 2007 18:40:32 +0200
Subject: [Plearn-commits] r7740 - trunk/plearn_learners_experimental
Message-ID: <200707101640.l6AGeWH3010730@sheep.berlios.de>

Author: larocheh
Date: 2007-07-10 18:40:25 +0200 (Tue, 10 Jul 2007)
New Revision: 7740

Modified:
   trunk/plearn_learners_experimental/StackedSVDNet.cc
   trunk/plearn_learners_experimental/StackedSVDNet.h
Log:
blu


Modified: trunk/plearn_learners_experimental/StackedSVDNet.cc
===================================================================
--- trunk/plearn_learners_experimental/StackedSVDNet.cc	2007-07-10 15:27:48 UTC (rev 7739)
+++ trunk/plearn_learners_experimental/StackedSVDNet.cc	2007-07-10 16:40:25 UTC (rev 7740)
@@ -32,13 +32,14 @@
 // This file is part of the PLearn library. For more information on the PLearn
 // library, go to the PLearn Web site at www.plearn.org
 
-// Authors: Pascal Lamblin
+// Authors: Hugo Larochelle
 
 /*! \file StackedSVDNet.cc */
 
 
 #define PL_LOG_MODULE_NAME "StackedSVDNet"
 #include <plearn/io/pl_log.h>
+#include <plearn/math/plapack.h>
 
 #include "StackedSVDNet.h"
 
@@ -58,8 +59,7 @@
     fine_tuning_decrease_ct( 0. ),
     batch_size(50),
     minimum_relative_improvement(1e-3),
-    n_layers( 0 ),
-    currently_trained_layer( 0 )
+    n_layers( 0 )
 {
     // random_gen will be initialized in PLearner::build_()
     random_gen = new PRandom();
@@ -158,9 +158,9 @@
             PLERROR("StackedSVDNet::build_layers_and_connections() - \n"
                     "layers[0] should have a size of %d.\n",
                     inputsize_);
-    
-        activations.resize( n_layers );
-        expectations.resize( n_layers );
+
+        reconstruction_costs(batch_size,1);    
+
         activation_gradients.resize( n_layers );
         expectation_gradients.resize( n_layers );
 
@@ -176,8 +176,6 @@
                 PLERROR("In StackedSVDNet::build()_: "
                     "layers must have decreasing sizes from bottom to top.");
                 
-            activations[i].resize( batch_size, layers[i]->size );
-            expectations[i].resize( batch_size, layers[i]->size );
             activation_gradients[i].resize( batch_size, layers[i]->size );
             expectation_gradients[i].resize( batch_size, layers[i]->size );
         }
@@ -186,7 +184,10 @@
             PLERROR("StackedSVDNet::build_costs() - \n"
                     "final_cost should be provided.\n");
 
-        final_cost_gradient.resize( final_cost->input_size );
+        final_cost_inputs.resize( batch_size, final_cost->input_size );
+        final_cost_value.resize( final_cost->output_size );
+        final_cost_values.resize( batch_size, final_cost->output_size );
+        final_cost_gradients.resize( batch_size, final_cost->input_size );
         final_cost->setLearningRate( fine_tuning_learning_rate );
 
         if( !(final_cost->random_gen) )
@@ -238,31 +239,25 @@
 
     // deepCopyField(, copies);
 
-    deepCopyField(training_schedule, copies);
     deepCopyField(layers, copies);
-    deepCopyField(connections, copies);
-    deepCopyField(reconstruction_connections, copies);
     deepCopyField(final_module, copies);
     deepCopyField(final_cost, copies);
-    deepCopyField(partial_costs, copies);
-    deepCopyField(partial_costs_weights, copies);
-    deepCopyField(activations, copies);
-    deepCopyField(expectations, copies);
+    deepCopyField(connections, copies);
     deepCopyField(activation_gradients, copies);
     deepCopyField(expectation_gradients, copies);
-    deepCopyField(reconstruction_activations, copies);
-    deepCopyField(reconstruction_expectations, copies);
+    deepCopyField(reconstruction_layer, copies);
+    deepCopyField(reconstruction_targets, copies);
+    deepCopyField(reconstruction_costs, copies);
+    deepCopyField(reconstruction_activation_gradient, copies);
     deepCopyField(reconstruction_activation_gradients, copies);
-    deepCopyField(reconstruction_expectation_gradients, copies);
-    deepCopyField(partial_costs_positions, copies);
-    deepCopyField(partial_cost_value, copies);
-    deepCopyField(final_cost_input, copies);
+    deepCopyField(reconstruction_input_gradients, copies);
+    deepCopyField(final_cost_inputs, copies);
     deepCopyField(final_cost_value, copies);
-    deepCopyField(final_cost_gradient, copies);
-    deepCopyField(greedy_stages, copies);
+    deepCopyField(final_cost_values, copies);
+    deepCopyField(final_cost_gradients, copies);
     
-    PLERROR("In StackedSVDNet::makeDeepCopyFromShallowCopy(): "
-            "not implemented yet.");
+    //PLERROR("In StackedSVDNet::makeDeepCopyFromShallowCopy(): "
+    //        "not implemented yet.");
 }
 
 
@@ -276,6 +271,7 @@
     inherited::forget();
 
     connections.resize(0);
+    rbm_connections.resize(0);
     
     final_module->forget();
     final_cost->forget();
@@ -286,19 +282,17 @@
 void StackedSVDNet::train()
 {
     MODULE_LOG << "train() called " << endl;
-    MODULE_LOG << "  training_schedule = " << training_schedule << endl;
 
     Vec input( inputsize() );
     Vec target( targetsize() );
     real weight; // unused
+    Mat inputs( batch_size, inputsize() );
+    Mat targets( batch_size, targetsize() );
 
     TVec<string> train_cost_names = getTrainCostNames() ;
     Vec train_costs( train_cost_names.length() );
     train_costs.fill(MISSING_VALUE) ;
 
-    int nsamples = train_set->length();
-    int sample;
-
     PP<ProgressBar> pb;
 
     // clear stats of previous epoch
@@ -311,6 +305,7 @@
     if(stage == 0)
     {
         connections.resize(n_layers-1);
+        rbm_connections.resize(n_layers-1);
         TVec< Vec > biases(n_layers-1);
         for( int i=0 ; i<n_layers-1 ; i++ )
         {
@@ -324,63 +319,82 @@
             for(int j=0; j < layers[i]->size; j++)
                 connections[i]->weights(j,j) = 0;
 
+            rbm_connections[i] = (RBMMatrixConnection *) connections[i];
+
+            CopiesMap map;
+            reconstruction_layer = layers[ i ]->deepCopy( map );
+            reconstruction_targets.resize( batch_size, layers[ i ]->size );
+            reconstruction_activation_gradient.resize( layers[ i ]->size );
+            reconstruction_activation_gradients.resize( 
+                batch_size, layers[ i ]->size );
+            reconstruction_input_gradients.resize( 
+                batch_size, layers[ i ]->size );
+
             lr = greedy_learning_rate;
-            layers[i]->setLearningRate( lr );
             connections[i]->setLearningRate( lr );
-            layers[i+1]->setLearningRate( lr );
+            reconstruction_layer->setLearningRate( lr );
 
-            real cost = 30;
-            real last_cost = 100;
+            real cost = 0;
+            real last_cost = 0;
             int nupdates = 0;
             int nepochs = 0;
             while( nepochs < 2 ||
                    (last_cost - cost) / last_cost >= minimum_relative_improvement )
             {
                 train_stats->forget();
-                for(int sample = 0; sample < train_set.length(); sample++)
+                for(int sample = 0; sample < train_set.length()/batch_size; 
+                    sample++)
                 {
                     if( !fast_exact_is_equal( greedy_decrease_ct , 0 ) )
                     {
                         lr = greedy_learning_rate/(1 + greedy_decrease_ct 
                                                    * nupdates);
-                        layers[i]->setLearningRate( lr );
                         connections[i]->setLearningRate( lr );
-                        reconstruction_connections[i]->setLearningRate( lr );
-                        layers[i+1]->setLearningRate( lr );                
+                        reconstruction_layer->setLearningRate( lr );                
                     }
-
-                    train_set->getExample(sample, input, target, weight);
-                    greedyStep( input, target, sample, train_costs );
+                    
+                    for(int j=0; j<batch_size; j++)
+                    {
+                        train_set->getExample(sample*batch_size + j, 
+                                              input, target, weight);
+                        inputs(j) << input;
+                        targets(j) << target;
+                    }
+                    greedyStep( inputs, targets, i, train_costs );
                     nupdates++;
                     train_stats->update( train_costs );
                 }
                 train_stats->finalize();
                 nepochs++;
                 last_cost = cost;
-                cost = train_stats->mean()[0];
+                cost = train_stats->getMean()[0];
             }
-            Mat A,U,S,Vt;
-            A.resize(layers[i]->size,layers[i]->size+1);
-            A.column(0) << layers[i]->bias;
-            A.subMat(0,1,layers[i]->size,layers[i]->size) << 
+            Mat A,U,Vt;
+            Vec S;
+            A.resize( reconstruction_layer->size, reconstruction_layer->size+1);
+            A.column( 0 ) << reconstruction_layer->bias;
+            A.subMat( 0, 1, reconstruction_layer->size, 
+                      reconstruction_layer->size ) << 
                 connections[i]->weights;
-            SVD(connections[i]->weights,U,S,V);
-            connections[i]->up_size = layers[i+1]->size;
-            connections[i]->down_size = layers[i]->size;
-            connections[i]->build();
-            connection[i]->weights << Vt.subRows(0,layers[i+1]->size);
-            biases[i].resize(layers[i+1]->size);
-            biases[i] << Vt.column(0).subVec(0,layers[i+1]->size);
-            for(int j=0; j<connections[i]->up_size; j++)
+            SVD( A, U, S, Vt );
+            connections[ i ]->up_size = layers[ i+1 ]->size;
+            connections[ i ]->down_size = layers[ i ]->size;
+            connections[ i ]->build();
+            connections[ i ]->weights << Vt.subMat( 
+                0, 0, layers[i+1]->size, Vt.width() );
+            biases[ i ].resize( layers[i+1]->size );
+            biases[ i ] << Vt.column( 0 ).toVec().subVec( 
+                0, layers[i+1]->size );
+            for(int j=0; j<connections[ i ]->up_size; j++)
             {
-                connections[i]->weights(j) *= S(j,j);
-                biases[i][j] *= S(j,j);
+                connections[ i ]->weights( j ) *= S[ j ];
+                biases[ i ][ j ] *= S[ j ];
             }
         }
         stage++;
         for(int i=0; i<biases.length(); i++)
         {
-            layers[i]->bias << biases[i];
+            layers[ i+1 ]->bias << biases[ i ];
         }
     }
 
@@ -402,158 +416,134 @@
 
         setLearningRate( fine_tuning_learning_rate );
         train_costs.fill(MISSING_VALUE);
+
         for( ; stage<nstages ; stage++ )
         {
-            sample = stage % nsamples;
-            if( !fast_exact_is_equal( fine_tuning_decrease_ct, 0. ) )
-                setLearningRate( fine_tuning_learning_rate
-                                 / (1. + fine_tuning_decrease_ct * stage ) );
+            for( int sample = 0; sample<train_set->length()/batch_size; sample++)
+            {
+                if( !fast_exact_is_equal( fine_tuning_decrease_ct, 0. ) )
+                    setLearningRate( fine_tuning_learning_rate
+                                     / (1. + fine_tuning_decrease_ct * stage ) );
 
-            train_set->getExample( sample, input, target, weight );
-            fineTuningStep( input, target, train_costs );
-            train_stats->update( train_costs );
-
-            if( pb )
-                pb->update( stage - init_stage + 1 );
+                for(int j=0; j<batch_size; j++)
+                {
+                    train_set->getExample(sample*batch_size + j, 
+                                          input, target, weight);
+                    inputs(j) << input;
+                    targets(j) << target;
+                }
+                fineTuningStep( inputs, targets, train_costs );
+                train_stats->update( train_costs );
+                
+                if( pb )
+                    pb->update( stage - init_stage + 1 );
+            }
         }
     }
     
     train_stats->finalize();
 }
 
-void StackedSVDNet::greedyStep( const Vec& input, const Vec& target, int index, Vec train_costs )
+void StackedSVDNet::greedyStep( const Mat& inputs, const Mat& targets, int index, Vec train_costs )
 {
     PLASSERT( index < n_layers );
 
-    expectations[0] << input;
-    for( int i=0 ; i<index + 1; i++ )
+    layers[ 0 ]->setExpectations( inputs );
+    
+    for( int i=0 ; i<index ; i++ )
     {
-        connections[i]->fprop( expectations[i], activations[i+1] );
-        layers[i+1]->fprop(activations[i+1],expectations[i+1]);
+        connections[ i ]->setAsDownInputs( layers[i]->getExpectations() );
+        layers[ i+1 ]->getAllActivations( rbm_connections[i], 0, true );
+        layers[ i+1 ]->computeExpectations();
     }
-
-    reconstruction_connections[ index ]->fprop( expectations[ index + 1],
-                                                reconstruction_activations);
-    layers[ index ]->fprop( reconstruction_activations,
-                            layers[ index ]->expectation);
+    reconstruction_targets << layers[ index ]->getExpectations();
     
-    layers[ index ]->expectation_is_up_to_date = true;
-    train_costs[index] = layers[ index ]->fpropNLL(expectations[index]);
+    connections[ index ]->setAsDownInputs( layers[ index ]->getExpectations() );
+    reconstruction_layer->getAllActivations( rbm_connections[ index ], 0, true );
+    reconstruction_layer->computeExpectations();
+    
+    reconstruction_layer->fpropNLL( layers[ index ]->getExpectations(), 
+                                    reconstruction_costs);
+    train_costs[index] = sum( reconstruction_costs )/batch_size;
 
-    layers[ index ]->bpropNLL(expectations[index], train_costs[index],
-                                  reconstruction_activation_gradients);
+    reconstruction_layer->bpropNLL( 
+        layers[ index ]->getExpectations(), reconstruction_costs,
+        reconstruction_activation_gradients );
 
-    layers[ index ]->update(reconstruction_activation_gradients);
+    columnMean( reconstruction_activation_gradients, 
+                reconstruction_activation_gradient );
+    reconstruction_layer->update( reconstruction_activation_gradient );
 
-    // // This is a bad update! Propagates gradient through sigmoid again!
-    // layers[ index ]->bpropUpdate( reconstruction_activations, 
-    //                                   layers[ index ]->expectation,
-    //                                   reconstruction_activation_gradients,
-    //                                   reconstruction_expectation_gradients);
-
-    reconstruction_connections[ index ]->bpropUpdate( 
-        expectations[ index + 1], 
-        reconstruction_activations, 
-        reconstruction_expectation_gradients, //reused
-        reconstruction_activation_gradients);
-
-    if(!fast_exact_is_equal(l1_neuron_decay,0))
-    {
-        // Compute L1 penalty gradient on neurons
-        real* hid = expectations[ index + 1 ].data();
-        real* grad = reconstruction_expectation_gradients.data();
-        int len = expectations[ index + 1 ].length();
-        for(int i=0; i<len; i++)
-        {
-            if(*hid > l1_neuron_decay_center)
-                *grad -= l1_neuron_decay;
-            else if(*hid < l1_neuron_decay_center)
-                *grad += l1_neuron_decay;
-            hid++;
-            grad++;
-        }
-    }
-
-    // Update hidden layer bias and weights
-    layers[ index+1 ]->bpropUpdate( activations[ index + 1 ],
-                                    expectations[ index + 1 ],
-                                    reconstruction_activation_gradients, // reused
-                                    reconstruction_expectation_gradients);    
-
     connections[ index ]->bpropUpdate( 
-        expectations[ index ],
-        activations[ index + 1 ],
-        reconstruction_expectation_gradients, //reused
+        layers[ index ]->getExpectations(), 
+        layers[ index ]->activations, 
+        reconstruction_input_gradients, 
         reconstruction_activation_gradients);
 
-    // Set diagonal to zero!!!
+    // Set diagonal to zero
+    for(int i=0; i<connections[ index ]->up_size; i++)
+        connections[ index ]->weights(i,i) = 0;
 }
 
-void StackedSVDNet::fineTuningStep( const Vec& input, const Vec& target,
+void StackedSVDNet::fineTuningStep( const Mat& inputs, const Mat& targets,
                                     Vec& train_costs )
 {
     // fprop
-    expectations[0] << input;
-    for( int i=0 ; i<n_layers-1; i++ )
+    layers[ 0 ]->setExpectations( inputs );
+    
+    for( int i=0 ; i<n_layers-1 ; i++ )
     {
-        connections[i]->fprop( expectations[i], activations[i+1] );
-        layers[i+1]->fprop(activations[i+1],expectations[i+1]);
+        connections[ i ]->setAsDownInputs( layers[i]->getExpectations() );
+        layers[ i+1 ]->getAllActivations( rbm_connections[i], 0, true );
+        layers[ i+1 ]->computeExpectations();
     }
 
-    final_module->fprop( expectations[ n_layers-1 ],
-                         final_cost_input );
-    final_cost->fprop( final_cost_input, target, final_cost_value );
+    final_module->fprop( layers[ n_layers-1 ]->getExpectations(),
+                         final_cost_inputs );
+    final_cost->fprop( final_cost_inputs, targets, final_cost_values );
 
-    train_costs.subVec(train_costs.length()-final_cost_value.length(),
-                       final_cost_value.length()) <<
+    columnMean( final_cost_values, 
+                final_cost_value );
+    train_costs.subVec(train_costs.length()-final_cost_value.length()) << 
         final_cost_value;
 
-    final_cost->bpropUpdate( final_cost_input, target,
-                             final_cost_value[0],
-                             final_cost_gradient );
-    final_module->bpropUpdate( expectations[ n_layers-1 ],
-                               final_cost_input,
+    final_cost->bpropUpdate( final_cost_inputs, targets,
+                             final_cost_values,
+                             final_cost_gradients );
+    final_module->bpropUpdate( layers[ n_layers-1 ]->getExpectations(),
+                               final_cost_inputs,
                                expectation_gradients[ n_layers-1 ],
-                               final_cost_gradient );
+                               final_cost_gradients );
 
     for( int i=n_layers-1 ; i>0 ; i-- )
     {
-        layers[i]->bpropUpdate( activations[i],
-                                expectations[i],
-                                activation_gradients[i],
-                                expectation_gradients[i] );
+        layers[ i ]->bpropUpdate( layers[ i ]->activations,
+                                  layers[ i ]->getExpectations(),
+                                  activation_gradients[ i ],
+                                  expectation_gradients[ i ] );
 
-        connections[i-1]->bpropUpdate( expectations[i-1],
-                                       activations[i],
-                                       expectation_gradients[i-1],
-                                       activation_gradients[i] );
+        connections[ i-1 ]->bpropUpdate( layers[ i-1 ]->getExpectations(),
+                                         layers[ i ]->activations,
+                                         expectation_gradients[ i-1 ],
+                                         activation_gradients[ i ] );
     }
 }
 
 void StackedSVDNet::computeOutput(const Vec& input, Vec& output) const
 {
     // fprop
-
-    expectations[0] << input;
-
-    for(int i=0 ; i<currently_trained_layer-1 ; i++ )
+    layers[ 0 ]->expectation <<  input ;
+    layers[ 0 ]->expectation_is_up_to_date = true;
+    
+    for( int i=0 ; i<n_layers-1 ; i++ )
     {
-        connections[i]->fprop( expectations[i], activations[i+1] );
-        layers[i+1]->fprop(activations[i+1],expectations[i+1]);
+        connections[ i ]->setAsDownInput( layers[i]->expectation );
+        layers[ i+1 ]->getAllActivations( rbm_connections[i], 0, true );
+        layers[ i+1 ]->computeExpectation();
     }
 
-    if( currently_trained_layer<n_layers )
-    {
-        connections[currently_trained_layer-1]->fprop( 
-            expectations[currently_trained_layer-1], 
-            activations[currently_trained_layer] );
-        layers[currently_trained_layer]->fprop(
-            activations[currently_trained_layer],
-            output);
-    }
-    else        
-        final_module->fprop( expectations[ currently_trained_layer - 1],
-                             output );
+    final_module->fprop( layers[ n_layers-1 ]->expectation,
+                         output );
 }
 
 void StackedSVDNet::computeCostsFromOutputs(const Vec& input, const Vec& output,
@@ -563,60 +553,11 @@
 
     costs.resize( getTestCostNames().length() );
     costs.fill( MISSING_VALUE );
-
-    if(compute_all_test_costs)
-    {
-        for(int i=0; i<currently_trained_layer-1; i++)
-        {
-            reconstruction_connections[ i ]->fprop( expectations[ i+1 ],
-                                                    reconstruction_activations);
-            layers[ i ]->fprop( reconstruction_activations,
-                                    layers[ i ]->expectation);
-            
-            layers[ i ]->expectation_is_up_to_date = true;
-            costs[i] = layers[ i ]->fpropNLL(expectations[ i ]);
-            
-            if( partial_costs && partial_costs[i])
-            {
-                partial_costs[ i ]->fprop( expectations[ i + 1],
-                                           target, partial_cost_value );
-                costs.subVec(partial_costs_positions[i],
-                             partial_cost_value.length()) << 
-                    partial_cost_value;
-            }
-        }
-    }
-
-    if( currently_trained_layer<n_layers )
-    {
-        reconstruction_connections[ currently_trained_layer-1 ]->fprop( 
-            output,
-            reconstruction_activations);
-        layers[ currently_trained_layer-1 ]->fprop( 
-            reconstruction_activations,
-            layers[ currently_trained_layer-1 ]->expectation);
-        
-        layers[ currently_trained_layer-1 ]->expectation_is_up_to_date = true;
-        costs[ currently_trained_layer-1 ] = 
-            layers[ currently_trained_layer-1 ]->fpropNLL(
-                expectations[ currently_trained_layer-1 ]);
-
-        if( partial_costs && partial_costs[ currently_trained_layer-1 ] )
-        {
-            partial_costs[ currently_trained_layer-1 ]->fprop( 
-                output,
-                target, partial_cost_value );
-            costs.subVec(partial_costs_positions[currently_trained_layer-1],
-                         partial_cost_value.length()) << partial_cost_value;
-        }
-    }
-    else
-    {
-        final_cost->fprop( output, target, final_cost_value );        
-        costs.subVec(costs.length()-final_cost_value.length(),
-                     final_cost_value.length()) <<
-            final_cost_value;
-    }
+    
+    final_cost->fprop( output, target, final_cost_value );
+    costs.subVec(costs.length()-final_cost_value.length(),
+                 final_cost_value.length()) <<
+        final_cost_value;
 }
 
 TVec<string> StackedSVDNet::getTestCostNames() const
@@ -630,14 +571,6 @@
     for( int i=0; i<layers.size()-1; i++)
         cost_names.push_back("reconstruction_error_" + tostring(i+1));
     
-    for( int i=0 ; i<partial_costs.size() ; i++ )
-    {
-        TVec<string> cost_names = partial_costs[i]->name();
-        for(int j=0; j<cost_names.length(); j++)
-            cost_names.push_back("partial_cost_" + tostring(i+1) + "_" + 
-                cost_names[j]);
-    }
-
     cost_names.append( final_cost->name() );
 
     return cost_names;

Modified: trunk/plearn_learners_experimental/StackedSVDNet.h
===================================================================
--- trunk/plearn_learners_experimental/StackedSVDNet.h	2007-07-10 15:27:48 UTC (rev 7739)
+++ trunk/plearn_learners_experimental/StackedSVDNet.h	2007-07-10 16:40:25 UTC (rev 7740)
@@ -1,6 +1,6 @@
 // -*- C++ -*-
 
-// StackedAutoassociatorsNet.h
+// StackedSVDNet.h
 //
 // Copyright (C) 2007 Hugo Larochelle
 //
@@ -32,16 +32,17 @@
 // This file is part of the PLearn library. For more information on the PLearn
 // library, go to the PLearn Web site at www.plearn.org
 
-// Authors: Pascal Lamblin
+// Authors: Hugo Larochelle
 
-/*! \file StackedAutoassociatorsNet.h */
+/*! \file StackedSVDNet.h */
 
 
-#ifndef StackedAutoassociatorsNet_INC
-#define StackedAutoassociatorsNet_INC
+#ifndef StackedSVDNet_INC
+#define StackedSVDNet_INC
 
 #include <plearn_learners/generic/PLearner.h>
 #include <plearn_learners/online/OnlineLearningModule.h>
+#include <plearn_learners/online/CostModule.h>
 #include <plearn_learners/online/RBMLayer.h>
 #include <plearn_learners/online/RBMConnection.h>
 #include <plearn_learners/online/RBMMatrixConnection.h>
@@ -52,7 +53,7 @@
 /**
  * Neural net, initialized with SVDs of logistic auto-regressions.
  */
-class StackedAutoassociatorsNet : public PLearner
+class StackedSVDNet : public PLearner
 {
     typedef PLearner inherited;
 
@@ -100,6 +101,10 @@
     //! The weights of the connections between the layers
     TVec< PP<RBMMatrixConnection> > connections;
 
+    //! View of connections as RBMConnection pointers (for compatibility
+    //! with RBM function calls)
+    TVec< PP<RBMConnection> > rbm_connections;
+
     //! Number of layers
     int n_layers;
 
@@ -107,7 +112,7 @@
     //#####  Public Member Functions  #########################################
 
     //! Default constructor
-    StackedAutoassociatorsNet();
+    StackedSVDNet();
 
 
     //#####  PLearner Member Functions  #######################################
@@ -142,10 +147,10 @@
     virtual TVec<std::string> getTrainCostNames() const;
 
 
-    void greedyStep( const Vec& input, const Vec& target, int index, 
+    void greedyStep( const Mat& inputs, const Mat& targets, int index, 
                      Vec train_costs );
 
-    void fineTuningStep( const Vec& input, const Vec& target,
+    void fineTuningStep( const Mat& inputs, const Mat& targets,
                          Vec& train_costs );
 
     //#####  PLearn::Object Protocol  #########################################
@@ -153,7 +158,7 @@
     // Declares other standard object methods.
     // ### If your class is not instantiatable (it has pure virtual methods)
     // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
-    PLEARN_DECLARE_OBJECT(StackedAutoassociatorsNet);
+    PLEARN_DECLARE_OBJECT(StackedSVDNet);
 
     // Simply calls inherited::build() then build_()
     virtual void build();
@@ -165,14 +170,6 @@
 protected:
     //#####  Not Options  #####################################################
 
-    //! Stores the activations of the input and hidden layers
-    //! (at the input of the layers)
-    mutable TVec<Mat> activations;
-
-    //! Stores the expectations of the input and hidden layers
-    //! (at the output of the layers)
-    mutable TVec<Mat> expectations;
-
     //! Stores the gradient of the cost wrt the activations of 
     //! the input and hidden layers
     //! (at the input of the layers)
@@ -183,37 +180,36 @@
     //! (at the output of the layers)
     mutable TVec<Mat> expectation_gradients;
 
-    //! Reconstruction activations
-    mutable Mat reconstruction_activations;
+    //! Reconstruction layer
+    mutable PP<RBMLayer> reconstruction_layer;
     
-    //! Reconstruction expectations
-    mutable Mat reconstruction_expectations;
-    
-    //! Reconstruction activations
+    //! Reconstruction target
+    mutable Mat reconstruction_targets;
+
+    //! Reconstruction costs
+    mutable Mat reconstruction_costs;
+
+    //! Reconstruction activation gradient
+    mutable Vec reconstruction_activation_gradient;
+
+    //! Reconstruction activation gradients
     mutable Mat reconstruction_activation_gradients;
     
-    //! Reconstruction expectations
-    mutable Mat reconstruction_expectation_gradients;
+    //! Reconstruction activations
+    mutable Mat reconstruction_input_gradients;
 
-    //! Input of the final_cost
-    mutable Vec final_cost_input;
+    //! Inputs of the final_cost
+    mutable Mat final_cost_inputs;
 
     //! Cost value of final_cost
     mutable Vec final_cost_value;
 
-    //! Stores the gradient of the cost at the input of final_cost
-    mutable Vec final_cost_gradient;
+    //! Cost values of final_cost
+    mutable Mat final_cost_values;
 
-    //! Currently trained layer (1 means the first hidden layer,
-    //! n_layers means the output layer)
-    int currently_trained_layer;
+    //! Stores the gradients of the cost at the inputs of final_cost
+    mutable Mat final_cost_gradients;
 
-    //! Indication whether final_module has learning rate
-    bool final_module_has_learning_rate;
-    
-    //! Indication whether final_cost has learning rate
-    bool final_cost_has_learning_rate;
-    
 protected:
     //#####  Protected Member Functions  ######################################
 
@@ -241,7 +237,7 @@
 };
 
 // Declares a few other classes and functions related to this class
-DECLARE_OBJECT_PTR(StackedAutoassociatorsNet);
+DECLARE_OBJECT_PTR(StackedSVDNet);
 
 } // end of namespace PLearn
 



From lamblin at mail.berlios.de  Wed Jul 11 00:09:11 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 11 Jul 2007 00:09:11 +0200
Subject: [Plearn-commits] r7741 - trunk/plearn/io
Message-ID: <200707102209.l6AM9B1O029954@sheep.berlios.de>

Author: lamblin
Date: 2007-07-11 00:09:10 +0200 (Wed, 11 Jul 2007)
New Revision: 7741

Modified:
   trunk/plearn/io/PStream.cc
   trunk/plearn/io/PStream.h
Log:
Minor cosmetic changes


Modified: trunk/plearn/io/PStream.cc
===================================================================
--- trunk/plearn/io/PStream.cc	2007-07-10 16:40:25 UTC (rev 7740)
+++ trunk/plearn/io/PStream.cc	2007-07-10 22:09:10 UTC (rev 7741)
@@ -8,18 +8,18 @@
 //
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
-// 
+//
 //  1. Redistributions of source code must retain the above copyright
 //     notice, this list of conditions and the following disclaimer.
-// 
+//
 //  2. Redistributions in binary form must reproduce the above copyright
 //     notice, this list of conditions and the following disclaimer in the
 //     documentation and/or other materials provided with the distribution.
-// 
+//
 //  3. The name of the authors may not be used to endorse or promote
 //     products derived from this software without specific prior written
 //     permission.
-// 
+//
 // THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
 // IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 // OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
@@ -30,7 +30,7 @@
 // LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 // NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 // SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-// 
+//
 // This file is part of the PLearn library. For more information on the PLearn
 // library, go to the PLearn Web site at www.plearn.org
 
@@ -61,7 +61,7 @@
 PStream& get_pnull()
 {
     static PStream pnull = new NullPStreamBuf();
-    return pnull;    
+    return pnull;
 }
 
 PStream pnull = get_pnull();
@@ -174,8 +174,8 @@
 
 PStream::PStream()
     :inherited(0),
-     inmode(plearn_ascii), 
-     outmode(plearn_ascii), 
+     inmode(plearn_ascii),
+     outmode(plearn_ascii),
      format_float (format_float_default),
      format_double(format_double_default),
      implicit_storage(true),
@@ -185,8 +185,8 @@
 
 PStream::PStream(streambuftype* sb)
     :inherited(sb),
-     inmode(plearn_ascii), 
-     outmode(plearn_ascii), 
+     inmode(plearn_ascii),
+     outmode(plearn_ascii),
      format_float (format_float_default),
      format_double(format_double_default),
      implicit_storage(true),
@@ -198,7 +198,7 @@
 //! ctor. from an istream (I)
 PStream::PStream(istream* pin_, bool own_pin_)
     :inherited(new StdPStreamBuf(pin_,own_pin_)),
-     inmode(plearn_ascii), 
+     inmode(plearn_ascii),
      outmode(plearn_ascii),
      format_float (format_float_default),
      format_double(format_double_default),
@@ -210,7 +210,7 @@
 
 PStream::PStream(ostream* pout_, bool own_pout_)
     :inherited(new StdPStreamBuf(pout_,own_pout_)),
-     inmode(plearn_ascii), 
+     inmode(plearn_ascii),
      outmode(plearn_ascii),
      format_float (format_float_default),
      format_double(format_double_default),
@@ -222,7 +222,7 @@
 //! ctor. from an iostream (IO)
 PStream::PStream(iostream* pios_, bool own_pios_)
     :inherited(new StdPStreamBuf(pios_,own_pios_)),
-     inmode(plearn_ascii), 
+     inmode(plearn_ascii),
      outmode(plearn_ascii),
      format_float (format_float_default),
      format_double(format_double_default),
@@ -234,7 +234,7 @@
 //! ctor. from an istream and an ostream (IO)
 PStream::PStream(istream* pin_, ostream* pout_, bool own_pin_, bool own_pout_)
     :inherited(new StdPStreamBuf(pin_,pout_,own_pin_,own_pout_)),
-     inmode(plearn_ascii), 
+     inmode(plearn_ascii),
      outmode(plearn_ascii),
      format_float (format_float_default),
      format_double(format_double_default),
@@ -247,8 +247,8 @@
 PStream::~PStream()
 { }
 
-PStream::mode_t PStream::switchToPLearnOutMode() 
-{ 
+PStream::mode_t PStream::switchToPLearnOutMode()
+{
     mode_t oldmode = outmode;
     switch(outmode)
     {
@@ -290,7 +290,7 @@
 
 void PStream::readExpected(char expect)
 {
-    int c = get(); 
+    int c = get();
     if(c!=expect)
         PLERROR("In readExpected : expected %c, but read %c",expect,c);
 }
@@ -298,7 +298,7 @@
 void PStream::readExpected(char* expect)
 {
     for(char c = *expect; c!=0; c=*expect++)
-        readExpected(c);    
+        readExpected(c);
 }
 
 void PStream::readExpected(const string& expect)
@@ -308,13 +308,13 @@
         readExpected(expect[i]);
 }
 
-  
+
 streamsize PStream::readUntil(char* buf, streamsize n, char stop_char)
 {
     streamsize nread = 0;
 
     while(nread<n)
-    {        
+    {
         int c = get();
         if(c==EOF)
             break;
@@ -335,7 +335,7 @@
     streamsize nread = 0;
 
     while(nread<n)
-    {        
+    {
         int c = get();
         if(c==EOF)
             break;
@@ -365,28 +365,28 @@
             if(characters_read.length() == characters_read.capacity())
                 characters_read.reserve(characters_read.length()*2); //don't realloc&copy every time a char is appended...
             characters_read+= static_cast<char>(c);
-          
+
             switch(c)
             {
             case '(':
                 smartReadUntilNext(")", characters_read, ignore_brackets, skip_comments);
-                characters_read+= ')';          
+                characters_read+= ')';
                 break;
             case '[':
                 if(!ignore_brackets)
                 {
                     smartReadUntilNext("]", characters_read, ignore_brackets, skip_comments);
-                    characters_read+= ']';          
+                    characters_read+= ']';
                 }
                 break;
             case '{':
                 smartReadUntilNext("}", characters_read, ignore_brackets, skip_comments);
-                characters_read+= '}';          
+                characters_read+= '}';
                 break;
             case '"':
                 smartReadUntilNext("\"", characters_read, ignore_brackets, false);
-                characters_read+= '"';          
-                break;          
+                characters_read+= '"';
+                break;
             }
         }
     }
@@ -424,9 +424,9 @@
 {
     int c = get();
     while(c!=EOF)
-    { 
+    {
         if(c=='#')
-            skipRestOfLine();     
+            skipRestOfLine();
         else if(c!=' ' && c!='\t' && c!='\n' && c!='\r')
             break;
         c = get();
@@ -439,9 +439,9 @@
 {
     int c = get();
     while(c!=EOF)
-    { 
+    {
         if(c=='#')
-            skipRestOfLine();     
+            skipRestOfLine();
         else if(c!=' ' && c!='\t' && c!='\n' && c!='\r' && c!=';' && c!=',')
             break;
         c = get();
@@ -533,7 +533,7 @@
     if(x>=zero)
         writeAsciiNum((unsigned long long)x);
     else
-    {        
+    {
         put('-');
         writeAsciiNum((unsigned long long) -x);
     }
@@ -656,7 +656,7 @@
     }
     else if (c == '+')
         c = get();
-  
+
     if(!isdigit(c))
         PLERROR("In readAsciiNum: not a valid ascii number, expected a digit, but read %c (ascii code %d)",c,c);
 
@@ -697,7 +697,7 @@
     }
     else if (c == '+')
         c = get();
-  
+
     if(!isdigit(c))
         PLERROR("In readAsciiNum: not a valid ascii number, expected a digit, but read %c (ascii code %d)",c,c);
 
@@ -740,7 +740,7 @@
     skipBlanks();
     int l=0;
     bool opposite = false;
-  
+
     char c = get();
     if (c == '-') {
         tmpbuf[l++] = c;
@@ -776,11 +776,11 @@
         }
         else
             PLERROR(error_msg);
-        break ; 
+        break ;
     default:
-        while(isdigit(c) 
+        while(isdigit(c)
               || ((c=='e' || c=='E') && !E_seen) //only one E
-              || ((c=='-' || c=='+') && E_seen && !sign_seen)//one sign, after E 
+              || ((c=='-' || c=='+') && E_seen && !sign_seen)//one sign, after E
               || (c=='.' && !E_seen && !dot_seen))//one dot, before E
         {
             if(c=='e' || c=='E') E_seen= true;
@@ -797,7 +797,7 @@
 }
 
 PStream& PStream::operator=(const PStream& pios)
-{ 
+{
     if(this != &pios)
     {
         inherited::operator=((const inherited&)pios);
@@ -841,7 +841,7 @@
     }
     return *this;
 }
- 
+
 PStream& PStream::operator>>(signed char &x)
 {
     char c;
@@ -1008,7 +1008,7 @@
     break;
     case PStream::plearn_ascii:
     case PStream::plearn_binary:
-    { 
+    {
         skipBlanksAndComments();
         int c = peek();
         if(c=='"') // it's a quoted string "..."
@@ -1021,7 +1021,7 @@
                 if(c=='\\') // escaped character
                 {
                     c = get();
-                    switch (c)             
+                    switch (c)
                     {
                     case 'n':
                         x += '\n';
@@ -1066,7 +1066,7 @@
         }
         else // it's a single word without quotes
         {
-            x.resize(0);      
+            x.resize(0);
             c= get();
             while(c != EOF && wordseparators().find(c)==string::npos) // as long as we don't meet a wordseparator (or eof)...
             {
@@ -1105,7 +1105,7 @@
         if(c==0x07 || c==0x08 || c==0x0B || c==0x0C )  // plearn_binary
         {
             read(reinterpret_cast<char*>(&x),sizeof(int));
-            if( (c==0x07 && byte_order()==BIG_ENDIAN_ORDER) 
+            if( (c==0x07 && byte_order()==BIG_ENDIAN_ORDER)
                 || (c==0x08 && byte_order()==LITTLE_ENDIAN_ORDER) )
                 endianswap(&x);
         }
@@ -1143,7 +1143,7 @@
         if(c==0x0B || c==0x0C || c==0x07 || c==0x08)  // plearn_binary unsigned int or int
         {
             read(reinterpret_cast<char*>(&x),sizeof(unsigned int));
-            if( (c==0x0B && byte_order()==BIG_ENDIAN_ORDER) 
+            if( (c==0x0B && byte_order()==BIG_ENDIAN_ORDER)
                 || (c==0x0C && byte_order()==LITTLE_ENDIAN_ORDER) )
                 endianswap(&x);
         }
@@ -1182,14 +1182,14 @@
         if(c==0x07 || c==0x08)  // plearn_binary
         {
             read(reinterpret_cast<char*>(&x),sizeof(long));
-            if( (c==0x07 && byte_order()==BIG_ENDIAN_ORDER) 
+            if( (c==0x07 && byte_order()==BIG_ENDIAN_ORDER)
                 || (c==0x08 && byte_order()==LITTLE_ENDIAN_ORDER) )
                 endianswap(&x);
         }
         else if(c==0x16 || c==0x17)  // plearn_binary
         {
             read(reinterpret_cast<char*>(&x),sizeof(long));
-            if( (c==0x16 && byte_order()==BIG_ENDIAN_ORDER) 
+            if( (c==0x16 && byte_order()==BIG_ENDIAN_ORDER)
                 || (c==0x17 && byte_order()==LITTLE_ENDIAN_ORDER) )
                 endianswap(&x);
         }
@@ -1229,7 +1229,7 @@
         {
             int32_t y;
             read(reinterpret_cast<char*>(&y),sizeof(int32_t));
-            if( (c==0x07 && byte_order()==BIG_ENDIAN_ORDER) 
+            if( (c==0x07 && byte_order()==BIG_ENDIAN_ORDER)
                 || (c==0x08 && byte_order()==LITTLE_ENDIAN_ORDER) )
                 endianswap(&y);
             x = y;
@@ -1237,7 +1237,7 @@
         else if(c==0x16 || c==0x17)  // plearn_binary 64 bits integer
         {
             read(reinterpret_cast<char*>(&x),sizeof(int64_t));
-            if( (c==0x16 && byte_order()==BIG_ENDIAN_ORDER) 
+            if( (c==0x16 && byte_order()==BIG_ENDIAN_ORDER)
                 || (c==0x17 && byte_order()==LITTLE_ENDIAN_ORDER) )
                 endianswap(&x);
         }
@@ -1276,7 +1276,7 @@
         if(c==0x0B || c==0x0C)  // plearn_binary
         {
             read(reinterpret_cast<char*>(&x),sizeof(unsigned long));
-            if( (c==0x0B && byte_order()==BIG_ENDIAN_ORDER) 
+            if( (c==0x0B && byte_order()==BIG_ENDIAN_ORDER)
                 || (c==0x0C && byte_order()==LITTLE_ENDIAN_ORDER) )
                 endianswap(&x);
         }
@@ -1316,7 +1316,7 @@
         {
             uint32_t y;
             read(reinterpret_cast<char*>(&y),sizeof(uint32_t));
-            if( (c==0x0B && byte_order()==BIG_ENDIAN_ORDER) 
+            if( (c==0x0B && byte_order()==BIG_ENDIAN_ORDER)
                 || (c==0x0C && byte_order()==LITTLE_ENDIAN_ORDER) )
                 endianswap(&y);
             x = y;
@@ -1363,7 +1363,7 @@
         if(c==0x16 || c==0x17)  // plearn_binary
         {
             read(reinterpret_cast<char*>(&x),sizeof(long long));
-            if( (c==0x16 && byte_order()==BIG_ENDIAN_ORDER) 
+            if( (c==0x16 && byte_order()==BIG_ENDIAN_ORDER)
                 || (c==0x17 && byte_order()==LITTLE_ENDIAN_ORDER) )
                 endianswap(&x);
         }
@@ -1380,7 +1380,7 @@
     }
     return *this;
 }
-  
+
 PStream& PStream::operator>>(unsigned long long &x)
 {
     switch(inmode)
@@ -1401,7 +1401,7 @@
         if(c==0x18 || c==0x19)  // plearn_binary
         {
             read(reinterpret_cast<char*>(&x),sizeof(unsigned long long));
-            if( (c==0x18 && byte_order()==BIG_ENDIAN_ORDER) 
+            if( (c==0x18 && byte_order()==BIG_ENDIAN_ORDER)
                 || (c==0x19 && byte_order()==LITTLE_ENDIAN_ORDER) )
                 endianswap(&x);
         }
@@ -1440,7 +1440,7 @@
         if(c==0x03 || c==0x04)  // plearn_binary
         {
             read(reinterpret_cast<char*>(&x),sizeof(short));
-            if( (c==0x03 && byte_order()==BIG_ENDIAN_ORDER) 
+            if( (c==0x03 && byte_order()==BIG_ENDIAN_ORDER)
                 || (c==0x04 && byte_order()==LITTLE_ENDIAN_ORDER) )
                 endianswap(&x);
         }
@@ -1457,7 +1457,7 @@
     }
     return *this;
 }
-  
+
 PStream& PStream::operator>>(unsigned short &x)
 {
     switch(inmode)
@@ -1478,7 +1478,7 @@
         if(c==0x05 || c==0x06)  // plearn_binary
         {
             read(reinterpret_cast<char*>(&x),sizeof(unsigned short));
-            if( (c==0x05 && byte_order()==BIG_ENDIAN_ORDER) 
+            if( (c==0x05 && byte_order()==BIG_ENDIAN_ORDER)
                 || (c==0x06 && byte_order()==LITTLE_ENDIAN_ORDER) )
                 endianswap(&x);
         }
@@ -1518,18 +1518,18 @@
 
         else if(c=='T')
         {
-            char r = get();        
-            char u = get();        
+            char r = get();
+            char u = get();
             char e = get();
             if ( r == 'r' && u == 'u' && e == 'e' )
                 parsed = 1;
         }
-      
+
         else if(c=='F')
         {
-            char a = get();        
+            char a = get();
             char l = get();
-            char s = get();        
+            char s = get();
             char e = get();
             if ( a == 'a' && l == 'l' && s == 's' && e == 'e' )
                 parsed = 0;
@@ -1569,7 +1569,7 @@
         if(c==0x0E || c==0x0F)  // plearn_binary
         {
             read(reinterpret_cast<char*>(&x),sizeof(float));
-            if( (c==0x0E && byte_order()==BIG_ENDIAN_ORDER) 
+            if( (c==0x0E && byte_order()==BIG_ENDIAN_ORDER)
                 || (c==0x0F && byte_order()==LITTLE_ENDIAN_ORDER) )
                 endianswap(&x);
         }
@@ -1607,7 +1607,7 @@
         if(c==0x10 || c==0x11)  // plearn_binary
         {
             read(reinterpret_cast<char*>(&x),sizeof(double));
-            if( (c==0x10 && byte_order()==BIG_ENDIAN_ORDER) 
+            if( (c==0x10 && byte_order()==BIG_ENDIAN_ORDER)
                 || (c==0x11 && byte_order()==LITTLE_ENDIAN_ORDER) )
                 endianswap(&x);
         }
@@ -1628,8 +1628,8 @@
 
 // Implementation of operator<<'s
 
-PStream& PStream::operator<<(char x) 
-{ 
+PStream& PStream::operator<<(char x)
+{
     switch(outmode)
     {
     case raw_ascii:
@@ -1654,14 +1654,14 @@
     return *this;
 }
 
-PStream& PStream::operator<<(signed char x) 
+PStream& PStream::operator<<(signed char x)
 {
     operator<<(char(x));
     return *this;
 }
 
-PStream& PStream::operator<<(unsigned char x) 
-{ 
+PStream& PStream::operator<<(unsigned char x)
+{
     switch(outmode)
     {
     case raw_ascii:
@@ -1698,7 +1698,7 @@
     case PStream::pretty_ascii:
     case PStream::raw_binary:
         write(x);
-        break;      
+        break;
 
     case PStream::plearn_ascii:
     case PStream::plearn_binary:
@@ -1755,8 +1755,8 @@
     return *this;
 }
 
-PStream& PStream::operator<<(bool x) 
-{ 
+PStream& PStream::operator<<(bool x)
+{
     switch(outmode)
     {
     case plearn_ascii:
@@ -1774,7 +1774,7 @@
             put('1');
         else
             put('0');
-        break;    
+        break;
     default:
         PLERROR("In PStream::operator<<  unknown outmode!!!!!!!!!");
         break;
@@ -1782,8 +1782,8 @@
     return *this;
 }
 
-PStream& PStream::operator<<(int x) 
-{ 
+PStream& PStream::operator<<(int x)
+{
     switch(outmode)
     {
     case raw_binary:
@@ -1812,8 +1812,8 @@
     return *this;
 }
 
-PStream& PStream::operator<<(unsigned int x) 
-{ 
+PStream& PStream::operator<<(unsigned int x)
+{
     switch(outmode)
     {
     case raw_binary:
@@ -1843,8 +1843,8 @@
 }
 
 /* Commented out because "long" has not the same size on every platform
-PStream& PStream::operator<<(long x) 
-{ 
+PStream& PStream::operator<<(long x)
+{
     switch(outmode)
     {
     case raw_binary:
@@ -1883,7 +1883,7 @@
     }
     return *this;
 }
-//*/
+*/
 
 PStream& PStream::operator<<(int64_t x)
 {
@@ -1916,8 +1916,8 @@
 }
 
 /*
-PStream& PStream::operator<<(unsigned long x) 
-{ 
+PStream& PStream::operator<<(unsigned long x)
+{
     switch(outmode)
     {
     case raw_binary:
@@ -1945,7 +1945,7 @@
     }
     return *this;
 }
-//*/
+*/
 
 PStream& PStream::operator<<(uint64_t x)
 {
@@ -1978,8 +1978,8 @@
 }
 
 /*
-PStream& PStream::operator<<(long long x) 
-{ 
+PStream& PStream::operator<<(long long x)
+{
     switch(outmode)
     {
     case raw_binary:
@@ -2008,8 +2008,8 @@
     return *this;
 }
 
-PStream& PStream::operator<<(unsigned long long x) 
-{ 
+PStream& PStream::operator<<(unsigned long long x)
+{
     switch(outmode)
     {
     case raw_binary:
@@ -2039,8 +2039,8 @@
 }
 */
 
-PStream& PStream::operator<<(short x) 
-{ 
+PStream& PStream::operator<<(short x)
+{
     switch(outmode)
     {
     case raw_binary:
@@ -2069,8 +2069,8 @@
     return *this;
 }
 
-PStream& PStream::operator<<(unsigned short x) 
-{ 
+PStream& PStream::operator<<(unsigned short x)
+{
     switch(outmode)
     {
     case raw_binary:
@@ -2160,11 +2160,11 @@
 }
 
 
-void binread_(PStream& in, bool* x,                
-              unsigned int n, unsigned char typecode)  
-{                                                      
-    if(typecode!=TypeTraits<bool>::little_endian_typecode()) 
-        PLERROR("In binread_ incompatible typecode");      
+void binread_(PStream& in, bool* x,
+              unsigned int n, unsigned char typecode)
+{
+    if(typecode!=TypeTraits<bool>::little_endian_typecode())
+        PLERROR("In binread_ incompatible typecode");
 
     while(n--)
     {
@@ -2214,23 +2214,23 @@
 //! The binread_ for float and double are special
 
 void binread_(PStream& in, double* x, unsigned int n, unsigned char typecode)
-{ 
+{
     if(typecode==TypeTraits<double>::little_endian_typecode())
     {
-        in.read((char*)x, streamsize(n*sizeof(double))); 
+        in.read((char*)x, streamsize(n*sizeof(double)));
 #ifdef BIGENDIAN
-        endianswap(x,n); 
-#endif      
+        endianswap(x,n);
+#endif
     }
     else if(typecode==TypeTraits<double>::big_endian_typecode())
     {
-        in.read((char*)x, streamsize(n*sizeof(double))); 
+        in.read((char*)x, streamsize(n*sizeof(double)));
 #ifdef LITTLEENDIAN
-        endianswap(x,n); 
+        endianswap(x,n);
 #endif
     }
     else if(typecode==TypeTraits<float>::little_endian_typecode())
-    {    
+    {
         float val;
         while(n--)
         {
@@ -2242,7 +2242,7 @@
         }
     }
     else if(typecode==TypeTraits<float>::big_endian_typecode())
-    {    
+    {
         float val;
         while(n--)
         {
@@ -2260,23 +2260,23 @@
 
 
 void binread_(PStream& in, float* x, unsigned int n, unsigned char typecode)
-{ 
+{
     if(typecode==TypeTraits<float>::little_endian_typecode())
     {
-        in.read((char*)x, streamsize(n*sizeof(float))); 
+        in.read((char*)x, streamsize(n*sizeof(float)));
 #ifdef BIGENDIAN
-        endianswap(x,n); 
-#endif      
+        endianswap(x,n);
+#endif
     }
     else if(typecode==TypeTraits<float>::big_endian_typecode())
     {
-        in.read((char*)x, streamsize(n*sizeof(float))); 
+        in.read((char*)x, streamsize(n*sizeof(float)));
 #ifdef LITTLEENDIAN
-        endianswap(x,n); 
+        endianswap(x,n);
 #endif
     }
     else if(typecode==TypeTraits<double>::little_endian_typecode())
-    {    
+    {
         double val;
         while(n--)
         {
@@ -2288,7 +2288,7 @@
         }
     }
     else if(typecode==TypeTraits<double>::big_endian_typecode())
-    {    
+    {
         double val;
         while(n--)
         {

Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2007-07-10 16:40:25 UTC (rev 7740)
+++ trunk/plearn/io/PStream.h	2007-07-10 22:09:10 UTC (rev 7741)
@@ -1,5 +1,5 @@
 // -*- C++ -*-
- 
+
 // PStream.h
 // Copyright (C) 1998 Pascal Vincent
 // Copyright (C) 1999-2001 Pascal Vincent, Yoshua Bengio and University of Montreal
@@ -8,18 +8,18 @@
 //
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
-//   
+//
 //  1. Redistributions of source code must retain the above copyright
 //     notice, this list of conditions and the following disclaimer.
-// 
+//
 //  2. Redistributions in binary form must reproduce the above copyright
 //     notice, this list of conditions and the following disclaimer in the
 //     documentation and/or other materials provided with the distribution.
-// 
+//
 //  3. The name of the authors may not be used to endorse or promote
 //     products derived from this software without specific prior written
 //     permission.
-// 
+//
 // THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
 // IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 // OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
@@ -30,7 +30,7 @@
 // LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 // NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 // SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-// 
+//
 // This file is part of the PLearn library. For more information on the PLearn
 // library, go to the PLearn Web site at www.plearn.org
 
@@ -93,7 +93,7 @@
      *  format to write stuff.  On input however, they are equivalent, as the
      *  right format is automatically detected.
      */
-    enum mode_t 
+    enum mode_t
     {
         plearn_ascii,    //!< PLearn ascii serialization format (can be mixed with plearn_binary)
         plearn_binary,   //!< PLearn binary serialization format (can be mixed with plearn_ascii)
@@ -101,18 +101,18 @@
         raw_binary,      //!< Simply writes the bytes as they are in memory.
         pretty_ascii     //!< Ascii pretty print (in particular for Vec and Mat, formatted output without size info)
     };
-  
 
+
     //! Compression mode (mostly used by binary serialization of sequences of floats or doubles, such as TMat<real>)
     //! (Used on output only; autodetect on read).
-    enum compr_mode_t { 
+    enum compr_mode_t {
         compr_none,            //!< No compression.
         compr_double_as_float, //!< In plearn_binary mode, store doubles as float
-        compr_sparse,          //!< PLearn 
-        compr_lossy_sparse     //!< Also stores double as float 
+        compr_sparse,          //!< PLearn
+        compr_lossy_sparse     //!< Also stores double as float
     };
 
-public:  
+public:
     mode_t inmode;              //!< mode for input formatting
     // bitset<32> pl_stream_flags_in;  //!< format flags for input
     map<unsigned int, void *> copies_map_in; //!< copies map for input
@@ -135,7 +135,7 @@
 
     //! Default format string for doubles
     static const char* format_double_default;
-    
+
 public:
     //! If true, then Mat and Vec will be serialized with their elements in place,
     //! If false, they will have an explicit pointer to a storage
@@ -144,12 +144,12 @@
     //! Determines the way data is compressed, if any.
     compr_mode_t compression_mode;
 
-    //! Should be true if this stream is used to communicate 
+    //! Should be true if this stream is used to communicate
     //! with a remote PLearn host.  Will serialize options
     //! accordingly.
     bool remote_plearn_comm;
 
-public:  
+public:
 
     PStream();
 
@@ -186,7 +186,7 @@
 
     //! if outmode is raw_ascii or raw_binary t will be switched to
     //! corresponding plearn_ascii, resp. plearn_binary.
-    //! The old mode will be returned, so that you can call setOutMode 
+    //! The old mode will be returned, so that you can call setOutMode
     //! to revert to the old mode when finished
     mode_t switchToPLearnOutMode();
 
@@ -235,7 +235,7 @@
     //! Writes the corresponding 2 hex digits (ex: 0A )
     void writeAsciiHexNum(unsigned char x);
 
-    inline bool eof() const 
+    inline bool eof() const
     { return ptr->eof(); }
 
     bool good() const
@@ -260,23 +260,23 @@
     const char* getDoubleFormat() const { return format_double; }
     void setFloatFormat(const char* f)  { format_float = f;  }
     void setDoubleFormat(const char* f) { format_double = f; }
-    
+
     /**
      * The folowing methods are 'forwarded' from {i|o}stream.
      */
-    inline int get() 
+    inline int get()
     { return ptr->get(); }
 
     //! The folowing methods are 'forwarded' from {i|o}stream.
-    inline PStream& get(char& c) 
-    { 
+    inline PStream& get(char& c)
+    {
         c = (char)ptr->get();
-        return *this; 
+        return *this;
     }
 
-    //! Delimitor is read from stream but not appended to string. 
+    //! Delimitor is read from stream but not appended to string.
     inline PStream& getline(string& line, char delimitor='\n')
-    { 
+    {
         line.clear();
         int c = get();
         while (c != EOF && c != delimitor)
@@ -284,7 +284,7 @@
             line += (char)c;
             c = get();
         }
-        return *this; 
+        return *this;
     }
 
     inline string getline()
@@ -293,20 +293,20 @@
     //! The folowing methods are 'forwarded' from {i|o}stream.
     //! It read from the stream without advancing in it.
     //! I.E. Multiple successive peek() will return the same data.
-    inline int peek() 
+    inline int peek()
     { return ptr->peek(); }
-  
+
     //! If you put back the result of a call to get(), make sure it is not EOF.
-    inline PStream& putback(char c) 
-    { 
+    inline PStream& putback(char c)
+    {
         ptr->putback(c);
-        return *this; 
+        return *this;
     }
 
     //! Put back the last character read by the get() or read() methods.
     //! You can only call this method once (use the unread() method if you want
     //! to put back more than one character).
-    inline PStream& unget() 
+    inline PStream& unget()
     {
         ptr->unget();
         return *this;
@@ -325,13 +325,13 @@
     { return unread(s.data(), streamsize(s.length())); }
 
 
-    inline PStream& read(char* s, streamsize n) 
-    { 
+    inline PStream& read(char* s, streamsize n)
+    {
         ptr->read(s,n);
         return *this;
     }
 
-    inline PStream& read(string& s, streamsize n) 
+    inline PStream& read(string& s, streamsize n)
     {
         char* buf = new char[n];
         string::size_type nread = ptr->read(buf, n);
@@ -372,14 +372,14 @@
     //! The stopping character met is not extracted from the stream.
     streamsize readUntil(char* buf, streamsize n, const char* stop_chars);
 
-    inline PStream& write(const char* s, streamsize n) 
-    { 
+    inline PStream& write(const char* s, streamsize n)
+    {
         ptr->write(s,n);
-        return *this; 
+        return *this;
     }
 
-    inline PStream& put(char c) 
-    { 
+    inline PStream& put(char c)
+    {
         ptr->put(c);
         return *this;
     }
@@ -391,10 +391,10 @@
     }
     inline PStream& put(int x) { return put((char)x); }
 
-    inline PStream& flush() 
-    { 
+    inline PStream& flush()
+    {
         ptr->flush();
-        return *this; 
+        return *this;
     }
 
     inline PStream& endl()
@@ -405,16 +405,16 @@
     }
 
     // These are convenient method for writing raw strings (whatever the outmode):
-    inline PStream& write(const char* s) 
-    { 
+    inline PStream& write(const char* s)
+    {
         write(s, streamsize(strlen(s)));
-        return *this; 
+        return *this;
     }
 
-    inline PStream& write(const string& s) 
-    { 
+    inline PStream& write(const string& s)
+    {
         write(s.data(), streamsize(s.length()));
-        return *this; 
+        return *this;
     }
 
     // Useful skip functions
@@ -435,10 +435,10 @@
     void skipAll(const char* chars_to_skip);
 
     //! Reads characters from stream, until we meet one of the stopping symbols at the current "level".
-    //! i.e. any opening parenthesis, bracket, brace or quote will open a next level and we'll 
-    //! be back to the current level only *after* we meet the corresponding closing parenthesis, 
+    //! i.e. any opening parenthesis, bracket, brace or quote will open a next level and we'll
+    //! be back to the current level only *after* we meet the corresponding closing parenthesis,
     //! bracket, brace or quote.
-    //! All characters read, except the stoppingsymbol, will be *appended* to characters_read 
+    //! All characters read, except the stoppingsymbol, will be *appended* to characters_read
     //! The stoppingsymbol is read and returned, but not appended to characters_read.
     //! Comments starting with # until the end of line may be skipped (as if they were not part of the stream)
     int smartReadUntilNext(const string& stoppingsymbols, string& characters_read,
@@ -453,7 +453,7 @@
     PStream& operator>>(double &x);
     PStream& operator>>(string &x);
     PStream& operator>>(char* x); // read string in already allocated char[]
-    PStream& operator>>(char &x); 
+    PStream& operator>>(char &x);
     PStream& operator>>(signed char &x);
     PStream& operator>>(unsigned char &x);
     PStream& operator>>(int &x);
@@ -484,13 +484,13 @@
     //! (unless you're in raw_ascii or raw_binary mode!)
     PStream& operator<<(const string &x);
 
-    PStream& operator<<(char x); 
+    PStream& operator<<(char x);
     PStream& operator<<(signed char x);
     PStream& operator<<(unsigned char x);
 
     // Note: If you get mysterious mesages of problems with const bool resolutions,
     // then a workaround might be to not declare <<(bool) as a method, but as an inline function
-    PStream& operator<<(bool x);  
+    PStream& operator<<(bool x);
     PStream& operator<<(int x);
     PStream& operator<<(unsigned int x);
     //PStream& operator<<(long x);
@@ -502,7 +502,7 @@
     PStream& operator<<(short x);
     PStream& operator<<(unsigned short x);
     PStream& operator<<(pl_pstream_manip func) { return (*func)(*this); }
- 
+
 };
 
 /*! PStream objects to replace the standard cout, cin, ... */
@@ -534,12 +534,12 @@
 //!  after removing any trailing '\r' and/or '\n'
 string pgetline(PStream& in);
 
-  
+
 /*****
  * op>> & op<< for generic pointers
  */
 
-template <class T> 
+template <class T>
 inline PStream& operator>>(PStream& in, T*& x)
 {
 
@@ -556,7 +556,7 @@
         else
         {
             in.skipBlanksAndCommentsAndSeparators();
-            if (in.peek() == '-') 
+            if (in.peek() == '-')
             {
                 in.get(); // Eat '-'
                 char cc = in.get();
@@ -571,8 +571,8 @@
                 //don't skip blanks before we need to read something else (read might block).
                 //in.skipBlanksAndCommentsAndSeparators();
                 in.copies_map_in[id]= x;
-            } 
-            else 
+            }
+            else
             {
                 // Find it in map and return ptr;
                 map<unsigned int, void *>::iterator it = in.copies_map_in.find(id);
@@ -582,25 +582,25 @@
                 x= static_cast<T *>(it->second);
             }
         }
-    } 
+    }
     else
     {
         in >> *x;
         //don't skip blanks before we need to read something else (read might block).
         //in.skipBlanksAndCommentsAndSeparators();
     }
-    
+
     return in;
 }
 
 
-template <class T> 
+template <class T>
 inline PStream& operator<<(PStream& out, T const * const & x)
 {
     if(x)
     {
         map<void *, unsigned int>::iterator it = out.copies_map_out.find(const_cast<T*&>(x));
-        if (it == out.copies_map_out.end()) 
+        if (it == out.copies_map_out.end())
         {
             int id = (int)out.copies_map_out.size()+1;
             out.put('*');
@@ -609,7 +609,7 @@
             out.copies_map_out[const_cast<T*&>(x)] = id;
             out << *x;
         }
-        else 
+        else
         {
             out.put('*');
             out << it->second;
@@ -621,7 +621,7 @@
     return out;
 }
 
-template <class T> 
+template <class T>
 inline PStream& operator>>(PStream& in, PP<T> &o)
 {
     T *ptr;
@@ -634,7 +634,7 @@
     return in;
 }
 
-template <class T> 
+template <class T>
 inline PStream& operator<<(PStream& out, const PP<T> &o)
 {
     T *ptr = static_cast<T *>(o);
@@ -642,20 +642,20 @@
     return out;
 }
 
-template <class T> 
+template <class T>
 inline PStream& operator<<(PStream& out, T*& ptr)
 {
     out << const_cast<T const * const &>(ptr);
     return out;
-}  
+}
 
 
-// Serialization of pairs in the form:   
+// Serialization of pairs in the form:
 // first : second
 
 template<class A,class B>
-inline PStream& operator<<(PStream& out, const pair<A,B>& x) 
-{ 
+inline PStream& operator<<(PStream& out, const pair<A,B>& x)
+{
     // new format (same as for tuple)
     out.put('(');
     out << x.first;
@@ -687,9 +687,9 @@
     return out;
 }
 
-template <typename S, typename T> 
-inline PStream& operator>>(PStream& in, pair<S, T> &x) 
-{ 
+template <typename S, typename T>
+inline PStream& operator>>(PStream& in, pair<S, T> &x)
+{
     in.skipBlanksAndCommentsAndSeparators();
     int c = in.peek();
     if(c==0x16) // binary pair
@@ -724,7 +724,7 @@
 
 template<class MapT>
 void writeMap(PStream& out, const MapT& m)
-{  
+{
     typename MapT::const_iterator it = m.begin();
     typename MapT::const_iterator itend = m.end();
 
@@ -776,7 +776,7 @@
         in.skipBlanksAndCommentsAndSeparators();
         c = in.peek(); // do we have a '}' ?
     }
-    in.get(); // eat the '}'  
+    in.get(); // eat the '}'
 }
 
 template<class Key, class Value, class Compare, class Alloc>
@@ -828,7 +828,7 @@
         out << *it;
         ++it;
     }
-    out.outmode = outmode; // restore previous outmode 
+    out.outmode = outmode; // restore previous outmode
 }
 
 inline void binwrite_(PStream& out, const bool* x, unsigned int n)
@@ -842,71 +842,71 @@
     }
 }
 
-inline void binwrite_(PStream& out, const char* x, unsigned int n) 
+inline void binwrite_(PStream& out, const char* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(char))); }
-inline void binwrite_(PStream& out, char* x, unsigned int n) 
+inline void binwrite_(PStream& out, char* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(char))); }
 
-inline void binwrite_(PStream& out, const signed char* x, unsigned int n) 
+inline void binwrite_(PStream& out, const signed char* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(signed char))); }
-inline void binwrite_(PStream& out, signed char* x, unsigned int n) 
+inline void binwrite_(PStream& out, signed char* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(signed char))); }
 
-inline void binwrite_(PStream& out, const unsigned char* x, unsigned int n) 
+inline void binwrite_(PStream& out, const unsigned char* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(unsigned char))); }
-inline void binwrite_(PStream& out, unsigned char* x, unsigned int n) 
+inline void binwrite_(PStream& out, unsigned char* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(unsigned char))); }
 
-inline void binwrite_(PStream& out, const short* x, unsigned int n) 
+inline void binwrite_(PStream& out, const short* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(short))); }
-inline void binwrite_(PStream& out, short* x, unsigned int n) 
+inline void binwrite_(PStream& out, short* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(short))); }
 
-inline void binwrite_(PStream& out, const unsigned short* x, unsigned int n) 
+inline void binwrite_(PStream& out, const unsigned short* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(unsigned short))); }
-inline void binwrite_(PStream& out, unsigned short* x, unsigned int n) 
+inline void binwrite_(PStream& out, unsigned short* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(unsigned short))); }
 
-inline void binwrite_(PStream& out, const int* x, unsigned int n) 
+inline void binwrite_(PStream& out, const int* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(int))); }
-inline void binwrite_(PStream& out, int* x, unsigned int n) 
+inline void binwrite_(PStream& out, int* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(int))); }
 
-inline void binwrite_(PStream& out, const unsigned int* x, unsigned int n) 
+inline void binwrite_(PStream& out, const unsigned int* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(unsigned int))); }
-inline void binwrite_(PStream& out, unsigned int* x, unsigned int n) 
+inline void binwrite_(PStream& out, unsigned int* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(unsigned int))); }
 
 /*
-inline void binwrite_(PStream& out, const long* x, unsigned int n) 
+inline void binwrite_(PStream& out, const long* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(long))); }
-inline void binwrite_(PStream& out, long* x, unsigned int n) 
+inline void binwrite_(PStream& out, long* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(long))); }
 
-inline void binwrite_(PStream& out, const unsigned long* x, unsigned int n) 
+inline void binwrite_(PStream& out, const unsigned long* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(unsigned long))); }
-inline void binwrite_(PStream& out, unsigned long* x, unsigned int n) 
+inline void binwrite_(PStream& out, unsigned long* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(unsigned long))); }
 */
 
-inline void binwrite_(PStream& out, const int64_t* x, unsigned int n) 
+inline void binwrite_(PStream& out, const int64_t* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(int64_t))); }
-inline void binwrite_(PStream& out, int64_t* x, unsigned int n) 
+inline void binwrite_(PStream& out, int64_t* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(int64_t))); }
 
-inline void binwrite_(PStream& out, const uint64_t* x, unsigned int n) 
+inline void binwrite_(PStream& out, const uint64_t* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(uint64_t))); }
-inline void binwrite_(PStream& out, uint64_t* x, unsigned int n) 
+inline void binwrite_(PStream& out, uint64_t* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(uint64_t))); }
 
-inline void binwrite_(PStream& out, const float* x, unsigned int n) 
+inline void binwrite_(PStream& out, const float* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(float))); }
-inline void binwrite_(PStream& out, float* x, unsigned int n) 
+inline void binwrite_(PStream& out, float* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(float))); }
 
-inline void binwrite_(PStream& out, const double* x, unsigned int n) 
+inline void binwrite_(PStream& out, const double* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(double))); }
-inline void binwrite_(PStream& out, double* x, unsigned int n) 
+inline void binwrite_(PStream& out, double* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(double))); }
 
 // The typecode indicates the type and format of the elements in the stream
@@ -927,14 +927,14 @@
 void binread_(PStream& in, bool* x, unsigned int n, unsigned char typecode);
 
 inline void binread_(PStream& in, char* x,
-                     unsigned int n, unsigned char typecode)  
-{                                                      
+                     unsigned int n, unsigned char typecode)
+{
     // big endian and little endian have the same typecodes
     // so we need to check only one for consistency
 
     if(typecode!=TypeTraits<char>::little_endian_typecode()
-       && typecode!=TypeTraits<unsigned char>::little_endian_typecode()) 
-        PLERROR("In binread_ incompatible typecode");      
+       && typecode!=TypeTraits<unsigned char>::little_endian_typecode())
+        PLERROR("In binread_ incompatible typecode");
 
     in.read((char*)x, n);
 }
@@ -963,10 +963,10 @@
     // norman: added explicit cast
     unsigned int n = (unsigned int)seq.size();
     typename SequenceType::const_iterator it = seq.begin();
-  
+
     switch(out.outmode)
     {
-    case PStream::raw_ascii:      
+    case PStream::raw_ascii:
         while(n--)
         {
             out << *it;
@@ -974,7 +974,7 @@
             ++it;
         }
         break;
-      
+
     case PStream::pretty_ascii:
         out.write("[ ");
         while(n--)
@@ -987,7 +987,7 @@
         out.write(" ] ");
         break;
 
-    case PStream::raw_binary: 
+    case PStream::raw_binary:
         binwrite_(out, it, n);
         break;
 
@@ -1007,7 +1007,7 @@
         unsigned char typecode;
         if(byte_order()==LITTLE_ENDIAN_ORDER)
         {
-            out.put((char)0x12); // 1D little-endian 
+            out.put((char)0x12); // 1D little-endian
             typecode = TypeTraits<typename SequenceType::value_type>::little_endian_typecode();
         }
         else
@@ -1018,15 +1018,15 @@
 
         // write typecode
         out.put(typecode);
-        
-        // write length in raw_binary 
+
+        // write length in raw_binary
         out.write((char*)&n, sizeof(n));
-        
+
         // write the data
         binwrite_(out, it, n);
     }
     break;
-      
+
     default:
         PLERROR("In PStream::writeSequence(Iterator& it, int n)  unknown outmode!!!!!!!!!");
         break;
@@ -1036,7 +1036,7 @@
 
 //! Reads in a sequence type from a PStream.
 /*! For this to work with the current implementation, the SequenceType must have:
-  - typedefs defining (SequenceType::...) value_type, size_type, iterator 
+  - typedefs defining (SequenceType::...) value_type, size_type, iterator
   - a begin() method that returns a proper iterator,
   - a size_type size() method returning the size of the current container
   - a resize(size_type n) method that allows to change the size of the container
@@ -1056,7 +1056,7 @@
         while(n--)
         {
             in.skipBlanks();
-            in >> *it; 
+            in >> *it;
             //don't skip blanks before we need to read something else (read might block).
             //in.skipBlanks();
             ++it;
@@ -1069,7 +1069,7 @@
         typename SequenceType::iterator it = seq.begin();
         while(n--)
         {
-            in >> *it; 
+            in >> *it;
             ++it;
         }
     }
@@ -1130,11 +1130,11 @@
         else if(c==0x12 || c==0x13) // it's a generic binary 1D sequence
         {
             in.get(); // eat c
-            unsigned char typecode = in.get(); 
+            unsigned char typecode = in.get();
             unsigned int l;
             in.read((char*)&l,sizeof(l));
 
-            bool inverted_byte_order = (    (c==0x12 && byte_order()==BIG_ENDIAN_ORDER) 
+            bool inverted_byte_order = (    (c==0x12 && byte_order()==BIG_ENDIAN_ORDER)
                                             || (c==0x13 && byte_order()==LITTLE_ENDIAN_ORDER) );
 
             if(inverted_byte_order)
@@ -1151,28 +1151,28 @@
         PLERROR("In PStream::operator>>  unknown inmode!!!!!!!!!");
         break;
     }
-    
+
 }
 
 // Default behavior for write() and read() is
 // to call corresponding operator<<() or operator>>()
 // on PStream.
 
-template<class T> 
+template<class T>
 inline void write(ostream& out_, const T& o)
 {
     PStream out(&out_);
     out << o;
 }
 
-template<class T> 
+template<class T>
 inline void read(istream& in_, T& o)
 {
     PStream in(&in_);
     in >> o;
 }
 
-template<class T> 
+template<class T>
 inline void read(const string& stringval, T& x)
 {
     istringstream in_(stringval);
@@ -1195,7 +1195,7 @@
 
 template<class SetT>
 void writeSet(PStream& out, const SetT& s)
-{  
+{
     typename SetT::const_iterator it = s.begin();
     typename SetT::const_iterator itend = s.end();
 
@@ -1245,7 +1245,7 @@
 {
 public:
     PIFStream(const string& fname, ios_base::openmode m = ios_base::in)
-        :PStream(new ifstream(fname.c_str()),true) 
+        :PStream(new ifstream(fname.c_str()),true)
     {
         PLDEPRECATED("PIFStream is deprecated. Use the openFile function instead.");
     }
@@ -1256,7 +1256,7 @@
 {
 public:
     POFStream(const string& fname, ios_base::openmode m = ios_base::out | ios_base::trunc)
-        :PStream(new ofstream(fname.c_str()),true) 
+        :PStream(new ofstream(fname.c_str()),true)
     {
         PLDEPRECATED("POFStream is deprecated. Use the openFile function instead.");
     }
@@ -1268,7 +1268,7 @@
 {
 public:
     PIStringStream(const string& s)
-        :PStream(new istringstream(s), true /* own it */) 
+        :PStream(new istringstream(s), true /* own it */)
     {
         PLDEPRECATED("PIStringStream is deprecated. Use the openString function instead.");
     }



From nouiz at mail.berlios.de  Wed Jul 11 16:24:14 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 11 Jul 2007 16:24:14 +0200
Subject: [Plearn-commits] r7742 - trunk/python_modules/plearn/pymake
Message-ID: <200707111424.l6BEOEYC009161@sheep.berlios.de>

Author: nouiz
Date: 2007-07-11 16:24:14 +0200 (Wed, 11 Jul 2007)
New Revision: 7742

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
-modified order of parameter to the linker so that we can better see the parameter when there is too much objects file


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2007-07-10 22:09:10 UTC (rev 7741)
+++ trunk/python_modules/plearn/pymake/pymake.py	2007-07-11 14:24:14 UTC (rev 7742)
@@ -2228,7 +2228,7 @@
             if optdef.linker:
                 linker = optdef.linker
 
-        command = linker + so_options + ' -o ' + self.corresponding_output + ' ' + string.join(self.objsfilelist,' ') + ' ' + self.get_optional_libraries_linkeroptions() + ' ' + linkeroptions + ' ' + linkeroptions_tail
+        command = linker + ' ' + string.join(self.objsfilelist,' ') + so_options + ' -o ' + self.corresponding_output + ' ' + self.get_optional_libraries_linkeroptions() + ' ' + linkeroptions + ' ' + linkeroptions_tail
 
         return command
 



From nouiz at mail.berlios.de  Wed Jul 11 16:26:41 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 11 Jul 2007 16:26:41 +0200
Subject: [Plearn-commits] r7743 - trunk/python_modules/plearn/parallel
Message-ID: <200707111426.l6BEQfpx009405@sheep.berlios.de>

Author: nouiz
Date: 2007-07-11 16:26:41 +0200 (Wed, 11 Jul 2007)
New Revision: 7743

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
Moved the condor log in the log_dir


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-07-11 14:24:14 UTC (rev 7742)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-07-11 14:26:41 UTC (rev 7743)
@@ -359,7 +359,8 @@
 
     def __init__( self, commands, **args ):
         DBIBase.__init__(self, commands, **args)
-
+        os.mkdir(self.log_dir) # condor log are always generated
+        
         if not os.path.exists(self.tmp_dir):
             os.mkdir(self.tmp_dir)
             
@@ -471,10 +472,13 @@
                 executable     = %s/launch.sh
                 universe       = vanilla
                 requirements   = %s
-                output         = main.%s.%s.$(Process).out
-                error          = main.%s.%s.$(Process).error
-                log            = main.%s.log
-                ''' % (self.tmp_dir,req,self.targetcondorplatform,task.unique_id,self.targetcondorplatform,task.unique_id,self.targetcondorplatform)))
+                output         = %s/condor.%s.%s.$(Process).out
+                error          = %s/condor.%s.%s.$(Process).error
+                log            = %s/condor.%s.log
+                ''' % (self.tmp_dir,req,
+                       self.log_dir,self.targetcondorplatform,task.unique_id,
+                       self.log_dir,self.targetcondorplatform,task.unique_id,
+                       self.log_dir,self.targetcondorplatform)))
 #                preBatch = ''' + pre_batch_command + '''
 #                postBatch = ''' + post_batch_command +'''
 



From yoshua at mail.berlios.de  Wed Jul 11 19:17:14 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Wed, 11 Jul 2007 19:17:14 +0200
Subject: [Plearn-commits] r7744 - trunk/plearn_learners/online/EXPERIMENTAL
Message-ID: <200707111717.l6BHHEAL012725@sheep.berlios.de>

Author: yoshua
Date: 2007-07-11 19:17:13 +0200 (Wed, 11 Jul 2007)
New Revision: 7744

Modified:
   trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
   trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.h
Log:


Modified: trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-11 14:26:41 UTC (rev 7743)
+++ trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-11 17:17:13 UTC (rev 7744)
@@ -530,13 +530,14 @@
     if (using_reconstruction_connection)
     {
         PLASSERT( reconstruction_connection );
-        reconstruction_connection->setAsDownInputs(hidden);
+        reconstruction_connection->setAsUpInputs(hidden);
         visible_layer->getAllActivations(reconstruction_connection, 0, true);
     }
     else
     {
         if(weights && !weights->isEmpty())
         {
+            PLASSERT( connection->classname() == "RBMMatrixConnection" );
             Mat old_weights;
             Vec old_activation;
             connection->getAllWeights(old_weights);
@@ -621,11 +622,11 @@
     Mat* reconstruction_error = 0;
     if(reconstruction_connection)
     {
-        visible_reconstruction = 
-            ports_value[getPortIndex("visible_reconstruction.state")]; 
-        visible_reconstruction_activations = 
+        visible_reconstruction =
+            ports_value[getPortIndex("visible_reconstruction.state")];
+        visible_reconstruction_activations =
             ports_value[getPortIndex("visible_reconstruction_activations.state")];
-        reconstruction_error = 
+        reconstruction_error =
             ports_value[getPortIndex("reconstruction_error.state")];
     }
     Mat* contrastive_divergence = 0;
@@ -634,15 +635,15 @@
     Mat* negative_phase_hidden_activations = NULL;
     if (compute_contrastive_divergence)
     {
-        contrastive_divergence = ports_value[getPortIndex("contrastive_divergence")]; 
+        contrastive_divergence = ports_value[getPortIndex("contrastive_divergence")];
         if (!contrastive_divergence || !contrastive_divergence->isEmpty())
             PLERROR("In KLp0p1RBMModule::fprop - When option "
                     "'compute_contrastive_divergence' is 'true', the "
                     "'contrastive_divergence' port should be provided, as an "
                     "output.");
-        negative_phase_visible_samples = 
+        negative_phase_visible_samples =
             ports_value[getPortIndex("negative_phase_visible_samples.state")];
-        negative_phase_hidden_expectations = 
+        negative_phase_hidden_expectations =
             ports_value[getPortIndex("negative_phase_hidden_expectations.state")];
         negative_phase_hidden_activations =
             ports_value[getPortIndex("negative_phase_hidden_activations.state")];
@@ -656,12 +657,13 @@
     {
         // When an input is provided, that would restart the chain for
         // unconditional sampling, from that example.
-        Gibbs_step = 0; 
-        visible_layer->setExpectations(*visible);
+        Gibbs_step = 0;
+        visible_layer->samples.resize(visible->length(),visible->width());
+        visible_layer->samples << *visible;
     }
 
     // COMPUTE ENERGY
-    if (energy) 
+    if (energy)
     {
         PLASSERT_MSG( energy->isEmpty(), 
                       "KLp0p1RBMModule: the energy port can only be an output port\n" );
@@ -685,6 +687,7 @@
         }
         found_a_valid_configuration = true;
     }
+    // COMPUTE NLL
     if (neg_log_likelihood && neg_log_likelihood->isEmpty() && compute_log_likelihood)
     {
         if (partition_function_is_stale && !during_training)
@@ -758,7 +761,7 @@
             computeEnergy(*visible,*hidden,*neg_log_likelihood);
             *neg_log_likelihood += log_partition_function;
         }
-        else if (visible && !visible->isEmpty()) 
+        else if (visible && !visible->isEmpty())
         {
             // neg-log-likelihood(visible) = free_energy(visible) + log(partition_function)
             computeFreeEnergyOfVisible(*visible,*neg_log_likelihood);
@@ -770,9 +773,9 @@
             computeFreeEnergyOfHidden(*hidden,*neg_log_likelihood);
             *neg_log_likelihood += log_partition_function;
         }
-        else PLERROR("KLp0p1RBMModule: neg_log_likelihood currently computable only of the visible as inputs");
+        else PLERROR("RBMModule: neg_log_likelihood currently computable only of the visible as inputs");
     }
-    
+
     // REGULAR FPROP
     // we are given the visible units and we want to compute the hidden
     // activation and/or the hidden expectation
@@ -794,8 +797,79 @@
         // Since we return below, the other ports must be unused.
         //PLASSERT( !visible_sample && !hidden_sample );
         found_a_valid_configuration = true;
-    } 
+    }
 
+    // COMPUTE AUTOASSOCIATOR RECONSTRUCTION ERROR
+    if ( visible && !visible->isEmpty() &&
+         ( ( visible_reconstruction && visible_reconstruction->isEmpty() ) ||
+           ( visible_reconstruction_activations &&
+             visible_reconstruction_activations->isEmpty() ) ||
+           ( reconstruction_error && reconstruction_error->isEmpty() ) ) )
+    {
+        // Autoassociator reconstruction cost
+        PLASSERT( ports_value.length() == nPorts() );
+        if(!hidden_expectations_are_computed)
+        {
+            computePositivePhaseHiddenActivations(*visible);
+            hidden_layer->computeExpectations();
+            hidden_expectations_are_computed=true;
+        }
+
+        // Don't need to verify if they are asked in a port, this was done previously
+
+        computeVisibleActivations(hidden_layer->getExpectations(), true);
+        if(visible_reconstruction_activations)
+        {
+            PLASSERT( visible_reconstruction_activations->isEmpty() );
+            const Mat& to_store = visible_layer->activations;
+            visible_reconstruction_activations->resize(to_store.length(),
+                                                       to_store.width());
+            *visible_reconstruction_activations << to_store;
+        }
+        if (visible_reconstruction || reconstruction_error)
+        {
+            visible_layer->computeExpectations();
+            if(visible_reconstruction)
+            {
+                PLASSERT( visible_reconstruction->isEmpty() );
+                const Mat& to_store = visible_layer->getExpectations();
+                visible_reconstruction->resize(to_store.length(),
+                                               to_store.width());
+                *visible_reconstruction << to_store;
+            }
+            if(reconstruction_error)
+            {
+                PLASSERT( reconstruction_error->isEmpty() );
+                reconstruction_error->resize(visible->length(),1);
+                visible_layer->fpropNLL(*visible,
+                                        *reconstruction_error);
+            }
+        }
+        found_a_valid_configuration = true;
+    }
+    // COMPUTE VISIBLE GIVEN HIDDEN
+    else if ( visible_reconstruction && visible_reconstruction->isEmpty()
+         && hidden && !hidden->isEmpty())
+    {
+        // Don't need to verify if they are asked in a port, this was done previously
+        computeVisibleActivations(*hidden,true);
+        if(visible_reconstruction_activations)
+        {
+            PLASSERT( visible_reconstruction_activations->isEmpty() );
+            const Mat& to_store = visible_layer->activations;
+            visible_reconstruction_activations->resize(to_store.length(),
+                                                       to_store.width());
+            *visible_reconstruction_activations << to_store;
+        }
+        visible_layer->computeExpectations();
+        PLASSERT( visible_reconstruction->isEmpty() );
+        const Mat& to_store = visible_layer->getExpectations();
+        visible_reconstruction->resize(to_store.length(),
+                                       to_store.width());
+        *visible_reconstruction << to_store;
+        found_a_valid_configuration = true;
+    }
+
     // compute KLp0p1 cost, given visible input
     if (KLp0p1 && KLp0p1->isEmpty() && visible && !visible->isEmpty())
     {
@@ -807,8 +881,11 @@
         PLASSERT_MSG(n>0,"KLp0p1RBMModule: training_set must have n>0 rows");
 
         // compute all P(hidden_i=1|x^k) for all x^k in training set
+        hidden_layer->setBatchSize(n);
+        visible_layer->setBatchSize(n);
         const Mat& ph=hidden_layer->getExpectations();
         training_set->getMat(0,0,visible_layer->getExpectations());
+        connection->setAsDownInputs(visible_layer->getExpectations());
         hidden_layer->getAllActivations(connection,0,true);
         hidden_layer->computeExpectations();
 
@@ -831,8 +908,9 @@
             }
         }
         // compute all P(visible_i=1|h) for each h configuration
-        visible_layer->getAllActivations(connection,0,true);
-        visible_layer->computeExpectations();
+        connection->setAsUpInputs(conf_hidden_layer->samples);
+        conf_visible_layer->getAllActivations(connection,0,true);
+        conf_visible_layer->computeExpectations();
 
         for (int c=0;c<n_configurations;c++)
         {
@@ -863,6 +941,9 @@
                     (*KLp0p1)(t,0) = logadd((*KLp0p1)(t,0), conf_visible_layer->fpropNLL((*visible)(t)) + log_sum_ph_given_xk);
         }
         *KLp0p1 *= -1;
+        // reset sizes as before
+        hidden_layer->setBatchSize(mbs);
+        visible_layer->setBatchSize(mbs);
     }
 
     // SAMPLING
@@ -879,24 +960,29 @@
         else if (visible_sample && !visible_sample->isEmpty()) // if an input is provided, sample hidden conditionally
         {
             sampleHiddenGivenVisible(*visible_sample);
+            hidden_activations_are_computed = false;
             Gibbs_step=0;
-            //cout << "sampling hidden from (discrete) visible" << endl;
+            //cout << "sampling hidden from visible" << endl;
         }
-        else if (visible && !visible->isEmpty()) // if an input is provided, sample hidden conditionally
+        else if (visible_expectation && !visible_expectation->isEmpty())
         {
-            visible_layer->generateSamples(); // WHY THIS LINE????
-            sampleHiddenGivenVisible(visible_layer->samples);
-            Gibbs_step=0;
-            //cout << "sampling hidden from visible expectation" << endl;
-        }
-        else if (visible_expectation && !visible_expectation->isEmpty()) 
-        {
              PLERROR("In KLp0p1RBMModule::fprop visible_expectation can only be an output port (use visible as input port");
         }
         else // sample unconditionally: Gibbs sample after k steps
         {
             // the visible_layer->expectations contain the "state" from which we
             // start or continue the chain
+            if (visible_layer->samples.isEmpty())
+            {
+                if (visible && !visible->isEmpty())
+                    visible_layer->samples << *visible;
+                else if (!visible_layer->getExpectations().isEmpty())
+                    visible_layer->samples << visible_layer->getExpectations();
+                else if (!hidden_layer->samples.isEmpty())
+                    sampleVisibleGivenHidden(hidden_layer->samples);    
+                else if (!hidden_layer->getExpectations().isEmpty())
+                    sampleVisibleGivenHidden(hidden_layer->getExpectations());    
+            }
             int min_n = max(Gibbs_step+n_Gibbs_steps_per_generated_sample,
                             min_n_Gibbs_steps);
             //cout << "Gibbs sampling " << Gibbs_step+1;
@@ -905,7 +991,8 @@
                 sampleHiddenGivenVisible(visible_layer->samples);
                 sampleVisibleGivenHidden(hidden_layer->samples);
             }
-              //cout << " -> " << Gibbs_step << endl;
+            hidden_activations_are_computed = false;
+            //cout << " -> " << Gibbs_step << endl;
         }
 
         if ( hidden && hidden->isEmpty())   // fill hidden.state with expectations
@@ -933,9 +1020,21 @@
                                         to_store.width());
             *visible_expectation << to_store;
         }
+        if (hidden && hidden->isEmpty())
+        {
+            hidden->resize(hidden_layer->samples.length(),
+                           hidden_layer->samples.width());
+            *hidden << hidden_layer->samples;
+        }
+        if (hidden_act && hidden_act->isEmpty())
+        {
+            hidden_act->resize(hidden_layer->samples.length(),
+                               hidden_layer->samples.width());
+            *hidden_act << hidden_layer->getExpectations();
+        }
         found_a_valid_configuration = true;
     }// END SAMPLING
-    
+
     // COMPUTE CONTRASTIVE DIVERGENCE CRITERION
     if (contrastive_divergence)
     {
@@ -951,15 +1050,16 @@
             {
                 PLASSERT(!hidden_act);
                 computePositivePhaseHiddenActivations(*visible);
-                
+
                 // we need to save the hidden activations somewhere
                 hidden_act_store.resize(mbs,hidden_layer->size);
                 hidden_act_store << hidden_layer->activations;
                 h_act = &hidden_act_store;
-            } else 
+            }
+            else
             {
                 // hidden_act must have been computed above if they were requested on port
-                PLASSERT(hidden_act && !hidden_act->isEmpty()); 
+                PLASSERT(hidden_act && !hidden_act->isEmpty());
                 h_act = hidden_act;
             }
             if (!hidden_expectations_are_computed) // it must be because hidden outputs were not asked
@@ -971,7 +1071,8 @@
                 hidden_exp_store.resize(mbs,hidden_layer->size);
                 hidden_exp_store << hidden_expectations;
                 h = &hidden_exp_store;
-            } else
+            }
+            else
             {
                 // hidden exp. must have been computed above if they were requested on port
                 PLASSERT(hidden && !hidden->isEmpty());
@@ -985,6 +1086,7 @@
                 sampleVisibleGivenHidden(hidden_layer->samples);
                 // compute corresponding hidden expectations.
                 computeHiddenActivations(visible_layer->samples);
+                hidden_activations_are_computed = false;
                 hidden_layer->computeExpectations();
             }
             PLASSERT(negative_phase_visible_samples);
@@ -1006,19 +1108,19 @@
             PLASSERT(hidden_layer->classname()=="RBMBinomialLayer");
 
             // note that h_act and h may point to hidden_act_store and hidden_exp_store
-            PLASSERT(h_act && !h_act->isEmpty()); 
+            PLASSERT(h_act && !h_act->isEmpty());
             PLASSERT(h && !h->isEmpty());
 
             contrastive_divergence->resize(hidden_expectations.length(),1);
             // compute contrastive divergence itself
             for (int i=0;i<mbs;i++)
             {
-                (*contrastive_divergence)(i,0) = 
+                (*contrastive_divergence)(i,0) =
                     // positive phase energy
                     visible_layer->energy((*visible)(i))
                     - dot((*h)(i),(*h_act)(i))
                     // minus
-                    - 
+                    -
                     // negative phase energy
                     (visible_layer->energy(visible_layer->samples(i))
                      - dot(hidden_expectations(i),hidden_layer->activations(i)));
@@ -1029,83 +1131,8 @@
                     "only possible if only visible are provided in input).\n");
         found_a_valid_configuration = true;
     }
-    
 
-    
-    
-    // COMPUTE AUTOASSOCIATOR RECONSTRUCTION ERROR
-    if ( visible && !visible->isEmpty() && 
-         ( ( visible_reconstruction && visible_reconstruction->isEmpty() ) || 
-           ( visible_reconstruction_activations && 
-             visible_reconstruction_activations->isEmpty() ) ||
-           ( reconstruction_error && reconstruction_error->isEmpty() ) ) ) 
-    {        
-        // Autoassociator reconstruction cost
-        PLASSERT( ports_value.length() == nPorts() );
-        computePositivePhaseHiddenActivations(*visible); 
-        if(!hidden_expectations_are_computed)
-        {
-            hidden_layer->computeExpectations();
-            hidden_expectations_are_computed=true;
-        }
 
-        // Don't need to verify if they are asked in a port, this was done previously
-        
-        computeVisibleActivations(hidden_layer->getExpectations(),true);
-        if(visible_reconstruction_activations) 
-        {
-            PLASSERT( visible_reconstruction_activations->isEmpty() );
-            const Mat& to_store = visible_layer->activations;
-            visible_reconstruction_activations->resize(to_store.length(), 
-                                                       to_store.width());
-            *visible_reconstruction_activations << to_store;
-        }
-        if (visible_reconstruction || reconstruction_error)
-        {        
-            visible_layer->computeExpectations();
-            if(visible_reconstruction)
-            {
-                PLASSERT( visible_reconstruction->isEmpty() );
-                const Mat& to_store = visible_layer->getExpectations();
-                visible_reconstruction->resize(to_store.length(), 
-                                                           to_store.width());
-                *visible_reconstruction << to_store;
-            }
-            if(reconstruction_error)
-            {
-                PLASSERT( reconstruction_error->isEmpty() );
-                reconstruction_error->resize(visible->length(),1);
-                visible_layer->fpropNLL(*visible,
-                                        *reconstruction_error);
-            }
-        }
-        found_a_valid_configuration = true;
-    }
-    // COMPUTE VISIBLE GIVEN HIDDEN
-    else if ( visible_reconstruction && visible_reconstruction->isEmpty() 
-         && hidden && !hidden->isEmpty())
-           
-    {        
-        // Don't need to verify if they are asked in a port, this was done previously
-        
-	computeVisibleActivations(*hidden,true);
-        if(visible_reconstruction_activations)
-        {
-            PLASSERT( visible_reconstruction_activations->isEmpty() );
-            const Mat& to_store = visible_layer->activations;
-            visible_reconstruction_activations->resize(to_store.length(), 
-                                                       to_store.width());
-            *visible_reconstruction_activations << to_store;
-        }      
-        visible_layer->computeExpectations();
-        PLASSERT( visible_reconstruction->isEmpty() );
-        const Mat& to_store = visible_layer->getExpectations();
-        visible_reconstruction->resize(to_store.length(), 
-                                       to_store.width());
-        *visible_reconstruction << to_store;
-        found_a_valid_configuration = true;
-    }
-
     // Reset some class fields to ensure they are not reused by mistake.
     hidden_act = NULL;
     hidden_bias = NULL;
@@ -1129,7 +1156,7 @@
             cout << "visible_expectation_empty : "<< (bool) visible_expectation->isEmpty() << endl;
 
         */
-        PLERROR("In KLp0p1RBMModule::fprop - Unknown port configuration for module %s", name.c_str());
+        PLERROR("In RBMModule::fprop - Unknown port configuration for module %s", name.c_str());
     }
 
     checkProp(ports_value);
@@ -1151,8 +1178,8 @@
     hidden_act = ports_value[getPortIndex("hidden_activations.state")];
     Mat* reconstruction_error_grad = 0;
     Mat* hidden_bias_grad = ports_gradient[getPortIndex("hidden_bias")];
-    weights = ports_value[getPortIndex("weights")]; 
-    Mat* weights_grad = ports_gradient[getPortIndex("weights")];    
+    weights = ports_value[getPortIndex("weights")];
+    Mat* weights_grad = ports_gradient[getPortIndex("weights")];
     hidden_bias = ports_value[getPortIndex("hidden_bias")];
     Mat* contrastive_divergence_grad = NULL;
     Mat* KLp0p1 = ports_value[getPortIndex("KLp0p1")];
@@ -1169,7 +1196,7 @@
     }
 
     if(reconstruction_connection)
-        reconstruction_error_grad = 
+        reconstruction_error_grad =
             ports_gradient[getPortIndex("reconstruction_error.state")];
 
     // Ensure the visible gradient is not provided as input. This is because we
@@ -1179,7 +1206,7 @@
 
     bool compute_visible_grad = visible_grad && visible_grad->isEmpty();
     bool compute_weights_grad = weights_grad && weights_grad->isEmpty();
-    
+
     int mbs = (visible && !visible->isEmpty()) ? visible->length() : -1;
 
     if (hidden_grad && !hidden_grad->isEmpty())
@@ -1191,67 +1218,67 @@
         // Note: we need to perform the following steps even if the gradient
         // learning rate is equal to 0. This is because we must propagate the
         // gradient to the visible layer, even though no update is required.
-            setAllLearningRates(grad_learning_rate);
-            PLASSERT( hidden && hidden_act );
-            // Compute gradient w.r.t. activations of the hidden layer.
-            hidden_layer->bpropUpdate(
-                    *hidden_act, *hidden, hidden_act_grad, *hidden_grad,
-                    false);
-            if (hidden_bias_grad)
+        setAllLearningRates(grad_learning_rate);
+        PLASSERT( hidden && hidden_act );
+        // Compute gradient w.r.t. activations of the hidden layer.
+        hidden_layer->bpropUpdate(
+                *hidden_act, *hidden, hidden_act_grad, *hidden_grad,
+                false);
+        if (hidden_bias_grad)
+        {
+            PLASSERT( hidden_bias_grad->isEmpty() &&
+                      hidden_bias_grad->width() == hidden_layer->size );
+            hidden_bias_grad->resize(mbs,hidden_layer->size);
+            *hidden_bias_grad += hidden_act_grad;
+        }
+        // Compute gradient w.r.t. expectations of the visible layer (=
+        // inputs).
+        Mat* store_visible_grad = NULL;
+        if (compute_visible_grad) {
+            PLASSERT( visible_grad->width() == visible_layer->size );
+            store_visible_grad = visible_grad;
+        } else {
+            // We do not actually need to store the gradient, but since it
+            // is required in bpropUpdate, we provide a dummy matrix to
+            // store it.
+            store_visible_grad = &visible_exp_grad;
+        }
+        store_visible_grad->resize(mbs,visible_layer->size);
+
+        if (weights)
+        {
+            int up = connection->up_size;
+            int down = connection->down_size;
+            PLASSERT( !weights->isEmpty() &&
+                      weights_grad && weights_grad->isEmpty() &&
+                      weights_grad->width() == up * down );
+            weights_grad->resize(mbs, up * down);
+            Mat w, wg;
+            Vec v,h,vg,hg;
+            for(int i=0; i<mbs; i++)
             {
-                PLASSERT( hidden_bias_grad->isEmpty() &&
-                          hidden_bias_grad->width() == hidden_layer->size );
-                hidden_bias_grad->resize(mbs,hidden_layer->size);
-                *hidden_bias_grad += hidden_act_grad;
+                w = Mat(up, down,(*weights)(i));
+                wg = Mat(up, down,(*weights_grad)(i));
+                v = (*visible)(i);
+                h = (*hidden_act)(i);
+                vg = (*store_visible_grad)(i);
+                hg = hidden_act_grad(i);
+                connection->petiteCulotteOlivierUpdate(
+                    v,
+                    w,
+                    h,
+                    vg,
+                    wg,
+                    hg,true);
             }
-            // Compute gradient w.r.t. expectations of the visible layer (=
-            // inputs).
-            Mat* store_visible_grad = NULL;
-            if (compute_visible_grad) {
-                PLASSERT( visible_grad->width() == visible_layer->size );
-                store_visible_grad = visible_grad;
-            } else {
-                // We do not actually need to store the gradient, but since it
-                // is required in bpropUpdate, we provide a dummy matrix to
-                // store it.
-                store_visible_grad = &visible_exp_grad;
-            }
-            store_visible_grad->resize(mbs,visible_layer->size);
-            
-            if (weights)
-            {
-                int up = connection->up_size;
-                int down = connection->down_size;
-                PLASSERT( !weights->isEmpty() &&
-                          weights_grad && weights_grad->isEmpty() &&
-                          weights_grad->width() == up * down );
-                weights_grad->resize(mbs, up * down);
-                Mat w, wg;
-                Vec v,h,vg,hg;
-                for(int i=0; i<mbs; i++)
-                {
-                    w = Mat(up, down,(*weights)(i));
-                    wg = Mat(up, down,(*weights_grad)(i));
-                    v = (*visible)(i);
-                    h = (*hidden_act)(i);
-                    vg = (*store_visible_grad)(i);
-                    hg = hidden_act_grad(i);
-                    connection->petiteCulotteOlivierUpdate(
-                        v,
-                        w,
-                        h,
-                        vg,
-                        wg,
-                        hg,true);
-                }
-            }
-            else
-            {
-                connection->bpropUpdate(
-                    *visible, *hidden_act, *store_visible_grad,
-                    hidden_act_grad, true);
-            }
-            partition_function_is_stale = true;
+        }
+        else
+        {
+            connection->bpropUpdate(
+                *visible, *hidden_act, *store_visible_grad,
+                hidden_act_grad, true);
+        }
+        partition_function_is_stale = true;
     }
 
     if (cd_learning_rate > 0 && minimize_log_likelihood) {
@@ -1331,7 +1358,7 @@
         // Perform a step of contrastive divergence.
         PLASSERT( visible && !visible->isEmpty() );
         setAllLearningRates(cd_learning_rate);
-        Mat* negative_phase_visible_samples = 
+        Mat* negative_phase_visible_samples =
             compute_contrastive_divergence?ports_value[getPortIndex("negative_phase_visible_samples.state")]:0;
         const Mat* negative_phase_hidden_expectations =
             compute_contrastive_divergence ?
@@ -1341,7 +1368,7 @@
             compute_contrastive_divergence ?
                 ports_value[getPortIndex("negative_phase_hidden_activations.state")]
                 : NULL;
-        
+
         PLASSERT( visible && hidden );
         PLASSERT( !negative_phase_visible_samples ||
                   !negative_phase_visible_samples->isEmpty() );
@@ -1550,14 +1577,24 @@
             visible_act_grad(t) *= (*reconstruction_error_grad)(t,0);
 
         // Visible bias update
-        columnSum(visible_act_grad,visible_bias_grad);
+        columnMean(visible_act_grad, visible_bias_grad);
         visible_layer->update(visible_bias_grad);
 
         // Reconstruction connection update
-        reconstruction_connection->bpropUpdate(
-            *hidden, *visible_reconstruction_activations,
-            hidden_exp_grad, visible_act_grad, false);
-        
+        hidden_exp_grad.resize(mbs, hidden_layer->size);
+        hidden_exp_grad.clear();
+        hidden_exp_grad.resize(0, hidden_layer->size);
+
+        TVec<Mat*> rec_ports_value(2);
+        rec_ports_value[0] = visible_reconstruction_activations;
+        rec_ports_value[1] = hidden;
+        TVec<Mat*> rec_ports_gradient(2);
+        rec_ports_gradient[0] = &visible_act_grad;
+        rec_ports_gradient[1] = &hidden_exp_grad;
+
+        reconstruction_connection->bpropAccUpdate( rec_ports_value,
+                                                   rec_ports_gradient );
+
         // Hidden layer bias update
         hidden_layer->bpropUpdate(*hidden_act,
                                   *hidden, hidden_act_grad,
@@ -1585,7 +1622,7 @@
         }
         else
         {
-            visible_exp_grad.resize(mbs,visible_layer->size);        
+            visible_exp_grad.resize(mbs,visible_layer->size);
             connection->bpropUpdate(
                 *visible, *hidden_act,
                 visible_exp_grad, hidden_act_grad, true);

Modified: trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.h
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.h	2007-07-11 14:26:41 UTC (rev 7743)
+++ trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.h	2007-07-11 17:17:13 UTC (rev 7744)
@@ -78,12 +78,12 @@
 
     bool compute_contrastive_divergence;
 
-    //! Number of Gibbs sampling steps in negative phase 
+    //! Number of Gibbs sampling steps in negative phase
     //! of contrastive divergence.
     int n_Gibbs_steps_CD;
 
     //! used to generate samples from the RBM
-    int min_n_Gibbs_steps; 
+    int min_n_Gibbs_steps;
     int n_Gibbs_steps_per_generated_sample;
 
     bool compute_log_likelihood;
@@ -98,8 +98,8 @@
     bool standard_cd_grad;
     bool standard_cd_bias_grad;
     bool standard_cd_weights_grad;
-    
 
+
 public:
     //#####  Public Member Functions  #########################################
 
@@ -249,7 +249,7 @@
     Mat hidden_exp_store;
     Mat hidden_act_store;
     Mat* hidden_act;
-    bool hidden_activations_are_computed;    
+    bool hidden_activations_are_computed;
 
     //! Used to store the contrastive divergence gradient w.r.t. weights.
     Mat store_weights_grad;
@@ -319,7 +319,7 @@
     //! it in the 'energy' matrix.
     //! The 'positive_phase' boolean is used to save computations when we know
     //! we are in the positive phase of fprop.
-    void computeEnergy(const Mat& visible, const Mat& hidden, Mat& energy, 
+    void computeEnergy(const Mat& visible, const Mat& hidden, Mat& energy,
                        bool positive_phase = true);
 
 private:



From lamblin at mail.berlios.de  Wed Jul 11 22:00:39 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 11 Jul 2007 22:00:39 +0200
Subject: [Plearn-commits] r7745 -
	tags/after_energy_sign_changes/plearn_learners/online
Message-ID: <200707112000.l6BK0d5P014467@sheep.berlios.de>

Author: lamblin
Date: 2007-07-11 22:00:39 +0200 (Wed, 11 Jul 2007)
New Revision: 7745

Modified:
   tags/after_energy_sign_changes/plearn_learners/online/RBMBinomialLayer.cc
Log:
Correct sign of gradient in bpropNLL


Modified: tags/after_energy_sign_changes/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- tags/after_energy_sign_changes/plearn_learners/online/RBMBinomialLayer.cc	2007-07-11 17:17:13 UTC (rev 7744)
+++ tags/after_energy_sign_changes/plearn_learners/online/RBMBinomialLayer.cc	2007-07-11 20:00:39 UTC (rev 7745)
@@ -425,8 +425,8 @@
     PLASSERT( target.size() == input_size );
     bias_gradient.resize( size );
 
-    // bias_gradient = target - expectation
-    substract(target, expectation, bias_gradient);
+    // bias_gradient = expectation - target
+    substract(expectation, target, bias_gradient);
 }
 
 void RBMBinomialLayer::bpropNLL(const Mat& targets, const Mat& costs_column,
@@ -440,8 +440,8 @@
     PLASSERT( costs_column.length() == batch_size );
     bias_gradients.resize( batch_size, size );
 
-    // bias_gradients = targets - expectations
-    substract(targets, expectations, bias_gradients);
+    // bias_gradients = expectations - targets
+    substract(expectations, targets, bias_gradients);
 }
 
 void RBMBinomialLayer::declareOptions(OptionList& ol)



From lamblin at mail.berlios.de  Wed Jul 11 23:04:14 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 11 Jul 2007 23:04:14 +0200
Subject: [Plearn-commits] r7746 -
	tags/after_energy_sign_changes/plearn_learners/online
Message-ID: <200707112104.l6BL4ECo019096@sheep.berlios.de>

Author: lamblin
Date: 2007-07-11 23:04:13 +0200 (Wed, 11 Jul 2007)
New Revision: 7746

Modified:
   tags/after_energy_sign_changes/plearn_learners/online/RBMMultinomialLayer.cc
Log:
Correct sign of gradient in bpropNLL


Modified: tags/after_energy_sign_changes/plearn_learners/online/RBMMultinomialLayer.cc
===================================================================
--- tags/after_energy_sign_changes/plearn_learners/online/RBMMultinomialLayer.cc	2007-07-11 20:00:39 UTC (rev 7745)
+++ tags/after_energy_sign_changes/plearn_learners/online/RBMMultinomialLayer.cc	2007-07-11 21:04:13 UTC (rev 7746)
@@ -377,7 +377,8 @@
     PLASSERT( target.size() == input_size );
     bias_gradient.resize( size );
 
-    substract(target, expectation, bias_gradient);
+    // bias_gradient = expectation - target
+    substract(expectation, target, bias_gradient);
 }
 
 void RBMMultinomialLayer::bpropNLL(const Mat& targets, const Mat& costs_column,
@@ -391,7 +392,8 @@
     PLASSERT( costs_column.length() == batch_size );
     bias_gradients.resize( batch_size, size );
 
-    substract(targets, expectations, bias_gradients);
+    // bias_gradients = expectations - targets
+    substract(expectations, targets, bias_gradients);
 }
 
 void RBMMultinomialLayer::declareOptions(OptionList& ol)



From lamblin at mail.berlios.de  Wed Jul 11 23:17:20 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 11 Jul 2007 23:17:20 +0200
Subject: [Plearn-commits] r7747 -
	tags/after_energy_sign_changes/plearn_learners/online
Message-ID: <200707112117.l6BLHKgW020162@sheep.berlios.de>

Author: lamblin
Date: 2007-07-11 23:17:20 +0200 (Wed, 11 Jul 2007)
New Revision: 7747

Modified:
   tags/after_energy_sign_changes/plearn_learners/online/RBMGaussianLayer.cc
Log:
Correct sign of gradient in bpropNLL


Modified: tags/after_energy_sign_changes/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- tags/after_energy_sign_changes/plearn_learners/online/RBMGaussianLayer.cc	2007-07-11 21:04:13 UTC (rev 7746)
+++ tags/after_energy_sign_changes/plearn_learners/online/RBMGaussianLayer.cc	2007-07-11 21:17:20 UTC (rev 7747)
@@ -713,8 +713,8 @@
     PLASSERT( target.size() == input_size );
     bias_gradient.resize( size );
 
-    // bias_gradient = target - expectation
-    substract(target, expectation, bias_gradient);
+    // bias_gradient = expectation - target
+    substract(expectation, target, bias_gradient);
 }
 
 void RBMGaussianLayer::bpropNLL(const Mat& targets, const Mat& costs_column,
@@ -728,8 +728,8 @@
     PLASSERT( costs_column.length() == batch_size );
     bias_gradients.resize( batch_size, size );
 
-    // bias_gradients = targets - expectations
-    substract(targets, expectations, bias_gradients);
+    // bias_gradients = expectations - targets
+    substract(expectations, targets, bias_gradients);
 }
 
 



From lamblin at mail.berlios.de  Wed Jul 11 23:35:30 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 11 Jul 2007 23:35:30 +0200
Subject: [Plearn-commits] r7748 - trunk/plearn_learners/online
Message-ID: <200707112135.l6BLZUk1021941@sheep.berlios.de>

Author: lamblin
Date: 2007-07-11 23:35:30 +0200 (Wed, 11 Jul 2007)
New Revision: 7748

Modified:
   trunk/plearn_learners/online/RBMBinomialLayer.cc
   trunk/plearn_learners/online/RBMGaussianLayer.cc
   trunk/plearn_learners/online/RBMMultinomialLayer.cc
Log:
Merge the last modifications into the trunk


Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-07-11 21:17:20 UTC (rev 7747)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-07-11 21:35:30 UTC (rev 7748)
@@ -425,8 +425,8 @@
     PLASSERT( target.size() == input_size );
     bias_gradient.resize( size );
 
-    // bias_gradient = target - expectation
-    substract(target, expectation, bias_gradient);
+    // bias_gradient = expectation - target
+    substract(expectation, target, bias_gradient);
 }
 
 void RBMBinomialLayer::bpropNLL(const Mat& targets, const Mat& costs_column,
@@ -440,8 +440,8 @@
     PLASSERT( costs_column.length() == batch_size );
     bias_gradients.resize( batch_size, size );
 
-    // bias_gradients = targets - expectations
-    substract(targets, expectations, bias_gradients);
+    // bias_gradients = expectations - targets
+    substract(expectations, targets, bias_gradients);
 }
 
 void RBMBinomialLayer::declareOptions(OptionList& ol)

Modified: trunk/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-07-11 21:17:20 UTC (rev 7747)
+++ trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-07-11 21:35:30 UTC (rev 7748)
@@ -713,8 +713,8 @@
     PLASSERT( target.size() == input_size );
     bias_gradient.resize( size );
 
-    // bias_gradient = target - expectation
-    substract(target, expectation, bias_gradient);
+    // bias_gradient = expectation - target
+    substract(expectation, target, bias_gradient);
 }
 
 void RBMGaussianLayer::bpropNLL(const Mat& targets, const Mat& costs_column,
@@ -728,8 +728,8 @@
     PLASSERT( costs_column.length() == batch_size );
     bias_gradients.resize( batch_size, size );
 
-    // bias_gradients = targets - expectations
-    substract(targets, expectations, bias_gradients);
+    // bias_gradients = expectations - targets
+    substract(expectations, targets, bias_gradients);
 }
 
 

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-07-11 21:17:20 UTC (rev 7747)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-07-11 21:35:30 UTC (rev 7748)
@@ -377,7 +377,8 @@
     PLASSERT( target.size() == input_size );
     bias_gradient.resize( size );
 
-    substract(target, expectation, bias_gradient);
+    // bias_gradient = expectation - target
+    substract(expectation, target, bias_gradient);
 }
 
 void RBMMultinomialLayer::bpropNLL(const Mat& targets, const Mat& costs_column,
@@ -391,7 +392,8 @@
     PLASSERT( costs_column.length() == batch_size );
     bias_gradients.resize( batch_size, size );
 
-    substract(targets, expectations, bias_gradients);
+    // bias_gradients = expectations - targets
+    substract(expectations, targets, bias_gradients);
 }
 
 void RBMMultinomialLayer::declareOptions(OptionList& ol)



From yoshua at mail.berlios.de  Thu Jul 12 00:09:14 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Thu, 12 Jul 2007 00:09:14 +0200
Subject: [Plearn-commits] r7749 - trunk/plearn_learners/online/EXPERIMENTAL
Message-ID: <200707112209.l6BM9ExA024012@sheep.berlios.de>

Author: yoshua
Date: 2007-07-12 00:09:14 +0200 (Thu, 12 Jul 2007)
New Revision: 7749

Modified:
   trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
Log:
fixed KLp0p1 computation and gradient


Modified: trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-11 21:35:30 UTC (rev 7748)
+++ trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-11 22:09:14 UTC (rev 7749)
@@ -884,7 +884,8 @@
         hidden_layer->setBatchSize(n);
         visible_layer->setBatchSize(n);
         const Mat& ph=hidden_layer->getExpectations();
-        training_set->getMat(0,0,visible_layer->getExpectations());
+        const Mat& X=visible_layer->getExpectations();
+        training_set->getMat(0,0,X);
         connection->setAsDownInputs(visible_layer->getExpectations());
         hidden_layer->getAllActivations(connection,0,true);
         hidden_layer->computeExpectations();
@@ -920,20 +921,16 @@
             Vec h = conf_hidden_layer->samples(c);
             for (int k=0;k<n;k++)
             {
-                real lp=h[0]==1?safelog(ph(k,0)):safelog(1-ph(k,0));
-                for (int i=1;i<hidden_layer->size;i++)
-                {
-                    real p_hi_given_xk = h[i]==1?safelog(ph(k,i)):safelog(1-ph(k,i)); 
-                    lp = logadd(lp,p_hi_given_xk);
-                }
+                real lp=0;
+                for (int i=0;i<hidden_layer->size;i++)
+                    lp += h[i]==1?safelog(ph(k,i)):safelog(1-ph(k,i)); 
                 // now lp = log P(h|x^k)
                 if (k==0)
                     log_sum_ph_given_xk = lp;
                 else
                     log_sum_ph_given_xk = logadd(log_sum_ph_given_xk,lp);
             }
-            log_sum_ph_given_xk -= logn;
-            // now log_sum_ph_given_xk = log (1/n) sum_k P(h|x^k)
+            // now log_sum_ph_given_xk = log sum_k P(h|x^k)
             for (int t=0;t<mbs;t++)
                 if (c==0)
                     (*KLp0p1)(t,0) = conf_visible_layer->fpropNLL((*visible)(t)) + log_sum_ph_given_xk;
@@ -941,6 +938,7 @@
                     (*KLp0p1)(t,0) = logadd((*KLp0p1)(t,0), conf_visible_layer->fpropNLL((*visible)(t)) + log_sum_ph_given_xk);
         }
         *KLp0p1 *= -1;
+        *KLp0p1 += logn;
         // reset sizes as before
         hidden_layer->setBatchSize(mbs);
         visible_layer->setBatchSize(mbs);
@@ -1650,6 +1648,8 @@
         int n=training_set.length();
         PLASSERT(connection->classname()=="RBMMatrixConnection");
         PP<RBMMatrixConnection> matrix_connection = PP<RBMMatrixConnection>(connection);
+        hidden_layer->setBatchSize(n);
+        visible_layer->setBatchSize(n);
         Mat& W = matrix_connection->weights;
         Vec& hidden_bias = hidden_layer->bias;
         Vec& visible_bias = visible_layer->bias;
@@ -1657,6 +1657,7 @@
         Vec phi_given_xk(hidden_layer->size);
         const Mat& ph_given_Xk=hidden_layer->getExpectations();
         const Mat& pvisible_given_H=conf_visible_layer->getExpectations();
+        const Mat& X=visible_layer->getExpectations();
         int n_configurations = 1 << hidden_layer->size; // = 2^{hidden_layer->size}
         real logn=safelog(n);
         for (int t=0;t<mbs;t++)
@@ -1665,7 +1666,7 @@
             for (int k=0;k<n;k++)
             {
                 Vec ph_given_xk = ph_given_Xk(k);
-                Vec xk = (*visible)(k);
+                Vec xk = X(k);
                 for (int c=0;c<n_configurations;c++)
                 {
                     Vec h = conf_hidden_layer->samples(c);
@@ -1673,12 +1674,13 @@
                     real lp = (*KLp0p1)(t,0) - logn; // lp = log (1/(n P1(x^t)))
                     // compute and multiply by P(h|x^k)
                     for (int i=0;i<hidden_layer->size;i++)
-                        lp = logadd(lp,phi_given_xk[i]=(h[i]==1?safelog(ph_given_xk[i]):safelog(1-ph_given_xk[i]))); 
+                        lp += (phi_given_xk[i]=(h[i]==1?safelog(ph_given_xk[i]):safelog(1-ph_given_xk[i]))); 
                     // now lp = log ( (1/(n P1(x^t))) P(h|x^k) )
 
                     // compute and multiply by P(x^t|h)
                     for (int j=0;j<visible_layer->size;j++)
-                        lp = logadd(lp,pxtj_given_h[j]=(xt[j]*safelog(pvisible_given_h[j])+(1-xt[j])*safelog(1-pvisible_given_h[j])));
+                        lp += pxtj_given_h[j]=(xt[j]*safelog(pvisible_given_h[j])+
+                                               (1-xt[j])*safelog(1-pvisible_given_h[j]));
                     // now lp = log ( (1/(n P1(x^t))) P(h|x^k)  P(x^t|h) )
                     real coeff = exp(lp);
                     for (int j=0;j<visible_layer->size;j++)
@@ -1687,7 +1689,8 @@
                     {
                         hidden_bias[i] -= klp0p1_learning_rate*coeff*(h[i]-phi_given_xk[i]);
                         for (int j=0;j<visible_layer->size;j++)
-                            W(i,j) -= klp0p1_learning_rate*coeff*(h[i]*(xt[j]-pxtj_given_h[j])-xk[j]*(h[i]-phi_given_xk[i]));
+                            W(i,j) -= klp0p1_learning_rate*coeff*
+                                (h[i]*(xt[j]-pxtj_given_h[j])-xk[j]*(h[i]-phi_given_xk[i]));
                     }
                 }
             }



From nouiz at mail.berlios.de  Thu Jul 12 18:41:38 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 12 Jul 2007 18:41:38 +0200
Subject: [Plearn-commits] r7750 - trunk/python_modules/plearn/learners
Message-ID: <200707121641.l6CGfcwS003204@sheep.berlios.de>

Author: nouiz
Date: 2007-07-12 18:41:36 +0200 (Thu, 12 Jul 2007)
New Revision: 7750

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:
-The function test_model, do_simple_validation and do_cross_validation now return a dict of cost
-Added a list that keep the result for each parameter


Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2007-07-11 22:09:14 UTC (rev 7749)
+++ trunk/python_modules/plearn/learners/SVM.py	2007-07-12 16:41:36 UTC (rev 7750)
@@ -132,6 +132,7 @@
                         'valid_error_rate',
                         'best_parameters',
                         'tried_parameters',
+                        'result_list',
                         'save_filename'
                         ]
        
@@ -147,6 +148,7 @@
           self.best_parameters      = None  
           self.best_model           = None
           self.tried_parameters     = {}
+          self.result_list          = {}
           
           self.save_filename        = None
           
@@ -159,6 +161,7 @@
           self.POLY_expert.reset()
           
           self.tried_parameters = {}
+          self.result_list          = {}
           #if self.best_parameters != None:
           #   self.add_parameter_to_tried_list(self.best_parameters[0], self.best_parameters[1:])
           self.error_rate       = 1.
@@ -168,8 +171,14 @@
           if self.tried_parameters.has_key(kernel):
              self.tried_parameters[kernel]+=[kernel_parameters]
           else:
-             self.tried_parameters[kernel] =[kernel_parameters] 
+             self.tried_parameters[kernel] =[kernel_parameters]
 
+      def add_result_to_result_list(self, kernel, kernel_parameters, error_rate):
+          if self.result_list.has_key(kernel):
+             self.result_list[kernel]+= kernel_parameters, error_rate
+          else:
+             self.result_list[kernel] = kernel_parameters, error_rate
+             
       def train_and_test(self, samples_target_list):
           check_samples_target_list(samples_target_list)
 
@@ -177,17 +186,16 @@
           best_parameters = best_expert.best_parameters
           param = best_expert.get_svm_parameter( best_parameters )
           if len(samples_target_list) == 1: # cross-validation
-             error_rate = do_cross_validation(samples_target_list[0][0], samples_target_list[0][1], param, self.nr_fold)
+             costs = do_cross_validation(samples_target_list[0][0], samples_target_list[0][1], param, self.nr_fold)
           else:
-             error_rate = do_simple_validation(samples_target_list[0][0] , samples_target_list[0][1] , samples_target_list[1][0] , samples_target_list[1][1], param)
-          return error_rate
+             costs = do_simple_validation(samples_target_list[0][0] , samples_target_list[0][1] , samples_target_list[1][0] , samples_target_list[1][1], param)
+          return costs
 
       def test(self, samples_target_list):
           check_samples_target_list(samples_target_list)
           if len(samples_target_list) <> 1:
-             raise TypeError, "in SVM::test(), samples_target_list must be [[samples],[targets]] (list of list)"
-          error_rate = test_model(model, samples_target_list[0][0], samples_target_list[0][1])
-          return error_rate
+             raise TypeError, "in SVM::test(), samples_target_list must be [[samples],[targets]] (list of list)"          
+          return test_model(self.best_model, samples_target_list[0][0], samples_target_list[0][1])
 
 
       def train_and_tune(self, kernel_type, samples_target_list):
@@ -219,7 +227,7 @@
                   else:
                      train_problem = svm_problem( samples_target_list[0][1] , samples_target_list[0][0] )
                      model = svm_model(train_problem, param)
-                     error_rate = test_model(model, samples_target_list[1][0], samples_target_list[1][1])
+                     error_rate = test_model(model, samples_target_list[1][0], samples_target_list[1][1])['error_rate']
                      
                      if self.save_filename != None:
                         try:
@@ -229,8 +237,9 @@
                            FID.write(' --> Error rate = '+str(error_rate)+'\n')
                            FID.close()
                         except:
-                           print "COULD not write in save_filename"   
-                                  
+                           print "COULD not write in save_filename"
+
+                  self.add_result_to_result_list(kernel_type, parameters, error_rate)
                   if error_rate < best_error_rate:
                      best_parameters = parameters
                      best_error_rate = error_rate
@@ -244,7 +253,7 @@
                 self.best_parameters = [kernel_type, best_parameters]
                 self.valid_error_rate = best_error_rate
                 if len(samples_target_list) == 3: # train-valid-test
-                   self.error_rate = test_model(self.best_model, samples_target_list[2][0], samples_target_list[2][1])
+                   self.error_rate = test_model(self.best_model, samples_target_list[2][0], samples_target_list[2][1])['error_rate']
                 else:
                    self.error_rate = self.valid_error_rate
           
@@ -288,21 +297,37 @@
         for j in range(0,i)+range(i+1,nr_fold):
             train_samples += samples_subsets[j]
             train_targets += targets_subsets[j]
-        cum_error_rate += do_simple_validation(train_samples, train_targets, test_samples, test_targets, param)
-    return cum_error_rate / nr_fold
+        cum_error_rate += do_simple_validation(train_samples, train_targets, test_samples, test_targets, param)['error_rate']
+    ret = cum_error_rate / nr_fold
+    return {'error_rate':err}
         
 def test_model(model, samples, targets):
     N = len(samples)
-    total_correct = 0
+    diffs = {}
     for i in range(N):
-        if model.predict(samples[i]) == targets[i]:
-           total_correct = total_correct + 1
-    return ( ( N - total_correct )*1. / N)
+          diff = abs(model.predict(samples[i]) - targets[i])
+          if diffs.has_key(diff):
+                diffs[diff] += 1
+          else:
+                diffs[diff] = 1
+    error_rate = 0
+    linear_class_error = 0
+    square_class_error = 0
+    for diff, nbdiff in diffs.iteritems():
+          if not diff == 0:
+                error_rate += 1*nbdiff
+          linear_class_error += diff*nbdiff
+          square_class_error += (diff*diff)*nbdiff
+    error_rate = float(error_rate) / N
+    linear_class_error = float(linear_class_error) / N
+    square_class_error = float(square_class_error) / N
+    
+    return {'error_rate':error_rate, 'linear_class_error':linear_class_error, 'square_class_error':square_class_error }
 
 def do_simple_validation(train_samples, train_targets, test_samples, test_targets, param):
     train_problem = svm_problem( train_targets, train_samples )
     model = svm_model(train_problem, param)
-    test_model(model,test_samples,test_targets)
+    return test_model(model,test_samples,test_targets)
 
 
 



From nouiz at mail.berlios.de  Thu Jul 12 21:03:11 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 12 Jul 2007 21:03:11 +0200
Subject: [Plearn-commits] r7751 - trunk/plearn/vmat
Message-ID: <200707121903.l6CJ3BmO003906@sheep.berlios.de>

Author: nouiz
Date: 2007-07-12 21:03:10 +0200 (Thu, 12 Jul 2007)
New Revision: 7751

Modified:
   trunk/plearn/vmat/VariableDeletionVMatrix.cc
Log:
Speed up optimisation


Modified: trunk/plearn/vmat/VariableDeletionVMatrix.cc
===================================================================
--- trunk/plearn/vmat/VariableDeletionVMatrix.cc	2007-07-12 16:41:36 UTC (rev 7750)
+++ trunk/plearn/vmat/VariableDeletionVMatrix.cc	2007-07-12 19:03:10 UTC (rev 7751)
@@ -230,6 +230,12 @@
     }
     if(!source)
         PLERROR("In VariableDeletionVMatrix::build_ - The source VMat do not exist!");
+
+    int is = source->inputsize();
+    if (is < 0)
+        PLERROR("In VariableDeletionVMatrix::build_ - The source VMat must "
+                "have an inputsize defined");
+
     VMat the_train_source = train_set ? train_set : source;
     if (number_of_train_samples > 0 &&
         number_of_train_samples < the_train_source->length())
@@ -237,44 +243,45 @@
                                           number_of_train_samples,
                                           the_train_source->width());
         
-    TVec<StatsCollector> stats =
-        PLearn::computeStats(the_train_source, -1, false);
-    PLASSERT( stats.length() == source->width() );
-    int is = source->inputsize();
-    if (is < 0)
-        PLERROR("In VariableDeletionVMatrix::build_ - The source VMat must "
-                "have an inputsize defined");
+    TVec<StatsCollector> stats;
+    if(min_non_missing_threshold > 0 || max_constant_threshold > 0){
+        stats = PLearn::computeStats(the_train_source, -1, false);
+        PLASSERT( stats.length() == source->width() );
+    }
 
     indices.resize(0);
 
     // First remove columns that have too many missing values.
-    int min_non_missing =
-        int(round(min_non_missing_threshold * the_train_source->length()));
-    for (int i = 0; i < is; i++)
-        if (stats[i].nnonmissing() >= min_non_missing)
+    if (min_non_missing_threshold > 0){
+        int min_non_missing =
+            int(round(min_non_missing_threshold * the_train_source->length()));
+        for (int i = 0; i < is; i++)
+            if (stats[i].nnonmissing() >= min_non_missing)
+                indices.append(i);
+    } else
+        for (int i = 0; i < is; i++)
             indices.append(i);
-
     // Then remove columns that are too constant.
     TVec<int> final_indices;
-    for (int k = 0; k < indices.length(); k++) {
-        int i = indices[k];
-        int max_constant_absolute =
-            int(round(max_constant_threshold * stats[i].nnonmissing()));
-        map<real, StatsCollectorCounts>* counts = stats[i].getCounts();
-        map<real, StatsCollectorCounts>::const_iterator it;
-        bool ok = true;
-        if (max_constant_threshold > 0)
+    if (max_constant_threshold > 0){
+        for (int k = 0; k < indices.length(); k++) {
+            int i = indices[k];
+            int max_constant_absolute =
+                int(round(max_constant_threshold * stats[i].nnonmissing()));
+            map<real, StatsCollectorCounts>* counts = stats[i].getCounts();
+            map<real, StatsCollectorCounts>::const_iterator it;
+            bool ok = true;
             for (it = counts->begin(); ok && it != counts->end(); it++) {
                 int n = int(round(it->second.n));
                 if (n >= max_constant_absolute)
                     ok = false;
             }
-        if (ok)
-            final_indices.append(i);
+            if (ok)
+                final_indices.append(i);
+        }
+        indices.resize(final_indices.length());
+        indices << final_indices;
     }
-    indices.resize(final_indices.length());
-    indices << final_indices;
-
     // Define sizes.
     inputsize_  = indices.length();
     targetsize_ = source->targetsize();



From tihocan at mail.berlios.de  Thu Jul 12 21:25:45 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 12 Jul 2007 21:25:45 +0200
Subject: [Plearn-commits] r7752 - trunk/plearn_learners/online/EXPERIMENTAL
Message-ID: <200707121925.l6CJPjP1005195@sheep.berlios.de>

Author: tihocan
Date: 2007-07-12 21:25:44 +0200 (Thu, 12 Jul 2007)
New Revision: 7752

Modified:
   trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
Log:
Fixed some mistakes in the gradient computation

Modified: trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-12 19:03:10 UTC (rev 7751)
+++ trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-12 19:25:44 UTC (rev 7752)
@@ -62,7 +62,7 @@
     "layer RBMs here."
     "The gradient on the weight Wij is"
     "  dC(x)/dWij = (1/(n P1(x))) "
-    "       sum_{k=1}^n sum_h P(x|h) P(h|x^k) (h_i(x_j - P(x_j|h)) + x_j^k(h_i - P(h_i|x^k)))"
+    "       sum_{k=1}^n sum_h P(x|h) P(h|x^k) (h_i(x_j - P(x_j=1|h)) + x_j^k(h_i - P(h_i=1|x^k)))"
     "Apart from the KLp0p1 output port, and the fact that CD learning is replaced by minimization"
     "of KLp0p1, this module acts like a regular RBMModule."
 );
@@ -1641,7 +1641,7 @@
         //
         // We want to compute
         //   dC(x)/dWij = (1/(n P1(x))) 
-        //       sum_{k=1}^n sum_h P(x|h) P(h|x^k) (h_i(x_j - P(x_j|h)) + x_j^k(h_i - P(h_i|x^k)))
+        //       sum_{k=1}^n sum_h P(x|h) P(h|x^k) (h_i(x_j - P(x_j=1|h)) + x_j^k(h_i - P(h_i=1|x^k)))
         //
         PLASSERT_MSG(KLp0p1 && !KLp0p1->isEmpty(), "Must compute KLp0p1 in order to compute its gradient, connect that port!");
         int mbs=visible->length();
@@ -1653,8 +1653,6 @@
         Mat& W = matrix_connection->weights;
         Vec& hidden_bias = hidden_layer->bias;
         Vec& visible_bias = visible_layer->bias;
-        Vec pxtj_given_h(visible_layer->size);
-        Vec phi_given_xk(hidden_layer->size);
         const Mat& ph_given_Xk=hidden_layer->getExpectations();
         const Mat& pvisible_given_H=conf_visible_layer->getExpectations();
         const Mat& X=visible_layer->getExpectations();
@@ -1674,23 +1672,24 @@
                     real lp = (*KLp0p1)(t,0) - logn; // lp = log (1/(n P1(x^t)))
                     // compute and multiply by P(h|x^k)
                     for (int i=0;i<hidden_layer->size;i++)
-                        lp += (phi_given_xk[i]=(h[i]==1?safelog(ph_given_xk[i]):safelog(1-ph_given_xk[i]))); 
+                        lp += (h[i]==1?safelog(ph_given_xk[i]):safelog(1-ph_given_xk[i])); 
                     // now lp = log ( (1/(n P1(x^t))) P(h|x^k) )
 
                     // compute and multiply by P(x^t|h)
                     for (int j=0;j<visible_layer->size;j++)
-                        lp += pxtj_given_h[j]=(xt[j]*safelog(pvisible_given_h[j])+
+                        lp += (xt[j]*safelog(pvisible_given_h[j])+
                                                (1-xt[j])*safelog(1-pvisible_given_h[j]));
                     // now lp = log ( (1/(n P1(x^t))) P(h|x^k)  P(x^t|h) )
                     real coeff = exp(lp);
                     for (int j=0;j<visible_layer->size;j++)
-                        visible_bias[j] -= klp0p1_learning_rate*coeff*(xt[j]-pxtj_given_h[j]);
+                        visible_bias[j] -=
+                            klp0p1_learning_rate*coeff*(xt[j]-pvisible_given_h[j]);
                     for (int i=0;i<hidden_layer->size;i++)
                     {
-                        hidden_bias[i] -= klp0p1_learning_rate*coeff*(h[i]-phi_given_xk[i]);
+                        hidden_bias[i] -= klp0p1_learning_rate*coeff*(h[i]-ph_given_xk[i]);
                         for (int j=0;j<visible_layer->size;j++)
                             W(i,j) -= klp0p1_learning_rate*coeff*
-                                (h[i]*(xt[j]-pxtj_given_h[j])-xk[j]*(h[i]-phi_given_xk[i]));
+                                (h[i]*(xt[j]-pvisible_given_h[j])-xk[j]*(h[i]-ph_given_xk[i]));
                     }
                 }
             }



From yoshua at mail.berlios.de  Thu Jul 12 22:50:04 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Thu, 12 Jul 2007 22:50:04 +0200
Subject: [Plearn-commits] r7753 - trunk/plearn_learners/online/EXPERIMENTAL
Message-ID: <200707122050.l6CKo4J4009246@sheep.berlios.de>

Author: yoshua
Date: 2007-07-12 22:50:04 +0200 (Thu, 12 Jul 2007)
New Revision: 7753

Modified:
   trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
Log:
The cost explodes...


Modified: trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-12 19:25:44 UTC (rev 7752)
+++ trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-12 20:50:04 UTC (rev 7753)
@@ -883,7 +883,7 @@
         // compute all P(hidden_i=1|x^k) for all x^k in training set
         hidden_layer->setBatchSize(n);
         visible_layer->setBatchSize(n);
-        const Mat& ph=hidden_layer->getExpectations();
+        const Mat& ha=hidden_layer->activations;
         const Mat& X=visible_layer->getExpectations();
         training_set->getMat(0,0,X);
         connection->setAsDownInputs(visible_layer->getExpectations());
@@ -911,7 +911,6 @@
         // compute all P(visible_i=1|h) for each h configuration
         connection->setAsUpInputs(conf_hidden_layer->samples);
         conf_visible_layer->getAllActivations(connection,0,true);
-        conf_visible_layer->computeExpectations();
 
         for (int c=0;c<n_configurations;c++)
         {
@@ -923,7 +922,12 @@
             {
                 real lp=0;
                 for (int i=0;i<hidden_layer->size;i++)
-                    lp += h[i]==1?safelog(ph(k,i)):safelog(1-ph(k,i)); 
+                {
+                    real act=ha(k,i);
+                    // note that log sigmoid(act) = -softplus(-act)
+                    // and       log(1 - sigmoid(act)) = -act -softplus(-act)
+                    lp += h[i]==1?-softplus(-act):-act-softplus(-act); 
+                }
                 // now lp = log P(h|x^k)
                 if (k==0)
                     log_sum_ph_given_xk = lp;
@@ -931,6 +935,7 @@
                     log_sum_ph_given_xk = logadd(log_sum_ph_given_xk,lp);
             }
             // now log_sum_ph_given_xk = log sum_k P(h|x^k)
+            conf_visible_layer->activation << conf_visible_layer->activations(c);
             for (int t=0;t<mbs;t++)
                 if (c==0)
                     (*KLp0p1)(t,0) = conf_visible_layer->fpropNLL((*visible)(t)) + log_sum_ph_given_xk;
@@ -1628,7 +1633,7 @@
         partition_function_is_stale = true;
     }
 
-    // compute KLp0p1 cost, given visible input
+    // compute gradient of KLp0p1 cost, given visible input
     if (klp0p1_learning_rate>0 && visible && !visible->isEmpty())
     {
         // WE ASSUME THAT THIS BPROP IS CALLED JUST AFTER THE CORRESPONDING FPROP!!!
@@ -1653,34 +1658,46 @@
         Mat& W = matrix_connection->weights;
         Vec& hidden_bias = hidden_layer->bias;
         Vec& visible_bias = visible_layer->bias;
-        const Mat& ph_given_Xk=hidden_layer->getExpectations();
-        const Mat& pvisible_given_H=conf_visible_layer->getExpectations();
         const Mat& X=visible_layer->getExpectations();
         int n_configurations = 1 << hidden_layer->size; // = 2^{hidden_layer->size}
         real logn=safelog(n);
+        // we only computed the activations in the fprop
+        conf_visible_layer->computeExpectations(); 
+        const Mat& pvisible_given_H = conf_visible_layer->getExpectations();
+        const Mat& ph_given_X = hidden_layer->getExpectations();
         for (int t=0;t<mbs;t++)
         {
             Vec xt = (*visible)(t);
             for (int k=0;k<n;k++)
             {
-                Vec ph_given_xk = ph_given_Xk(k);
+                Vec ah_given_xk = hidden_layer->activations(k);
+                Vec ph_given_xk = ph_given_X(k);
                 Vec xk = X(k);
                 for (int c=0;c<n_configurations;c++)
                 {
                     Vec h = conf_hidden_layer->samples(c);
-                    Vec pvisible_given_h=pvisible_given_H(c);
+                    Vec avisible_given_h=conf_visible_layer->activations(c);
                     real lp = (*KLp0p1)(t,0) - logn; // lp = log (1/(n P1(x^t)))
                     // compute and multiply by P(h|x^k)
                     for (int i=0;i<hidden_layer->size;i++)
-                        lp += (h[i]==1?safelog(ph_given_xk[i]):safelog(1-ph_given_xk[i])); 
+                    {
+                        real act=ah_given_xk[i];
+                        // note that log sigmoid(act) = -softplus(-act)
+                        // and       log(1 - sigmoid(act)) = -act -softplus(-act)
+                        // so h*log(sigmoid(act))+(1-h)*log(sigmoid(act)) = act*h-act-softplus(act)
+                        lp += h[i]*act-act-softplus(-act);
+                    }
                     // now lp = log ( (1/(n P1(x^t))) P(h|x^k) )
 
                     // compute and multiply by P(x^t|h)
                     for (int j=0;j<visible_layer->size;j++)
-                        lp += (xt[j]*safelog(pvisible_given_h[j])+
-                                               (1-xt[j])*safelog(1-pvisible_given_h[j]));
+                    {
+                        real act=avisible_given_h[j];
+                        lp += act*xt[j] - act - softplus(-act);
+                    }
                     // now lp = log ( (1/(n P1(x^t))) P(h|x^k)  P(x^t|h) )
                     real coeff = exp(lp);
+                    Vec pvisible_given_h=pvisible_given_H(c);
                     for (int j=0;j<visible_layer->size;j++)
                         visible_bias[j] -=
                             klp0p1_learning_rate*coeff*(xt[j]-pvisible_given_h[j]);



From yoshua at mail.berlios.de  Fri Jul 13 00:14:44 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Fri, 13 Jul 2007 00:14:44 +0200
Subject: [Plearn-commits] r7754 - trunk/plearn/vmat
Message-ID: <200707122214.l6CMEi9T015201@sheep.berlios.de>

Author: yoshua
Date: 2007-07-13 00:14:43 +0200 (Fri, 13 Jul 2007)
New Revision: 7754

Added:
   trunk/plearn/vmat/BinaryNumbersVMatrix.cc
   trunk/plearn/vmat/BinaryNumbersVMatrix.h
Log:
New VMatrix class that handles many types of files with binary encoding of numbers.


Added: trunk/plearn/vmat/BinaryNumbersVMatrix.cc
===================================================================
--- trunk/plearn/vmat/BinaryNumbersVMatrix.cc	2007-07-12 20:50:04 UTC (rev 7753)
+++ trunk/plearn/vmat/BinaryNumbersVMatrix.cc	2007-07-12 22:14:43 UTC (rev 7754)
@@ -0,0 +1,187 @@
+// -*- C++ -*-
+
+// BinaryNumbersVMatrix.cc
+//
+// Copyright (C) 2007 Yoshua Bengio
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Yoshua Bengio
+
+/*! \file BinaryNumbersVMatrix.cc */
+
+
+#include "BinaryNumbersVMatrix.h"
+
+
+namespace PLearn {
+using namespace std;
+
+
+PLEARN_IMPLEMENT_OBJECT(
+    BinaryNumbersVMatrix,
+    "VMatrix reading file with numbers in various possible binary formats",
+    "VMatrix that can take its values from a possibly large file (greater than 2Gig)\n"
+    "containing numbers in a user-given binary format, preceded by an arbitrary header whose\n"
+    "length is user-given. The user must also specify the dimensions of the matrix\n"
+    "(length and width).\n"
+    );
+
+BinaryNumbersVMatrix::BinaryNumbersVMatrix()
+    : format("u1"), header_size(0), file_is_bigendian(false), f(0), buffer(0)
+{
+}
+
+void BinaryNumbersVMatrix::getNewRow(int i, const Vec& v) const
+{
+    PLASSERT_MSG(v.length()==width_,"BinaryNumbersVMatrix::getNewRow(i,v) with v.length!= vmatrix width");
+    PR_Seek64(f,header_size+i*row_size,PR_SEEK_SET);
+    PR_Read(f,buffer,row_size);
+    bool swap_endian=false;
+#ifdef LITTLEENDIAN
+    if(file_is_bigendian)
+        swap_endian=true;
+#endif
+#ifdef BIGENDIAN
+    if(!file_is_bigendian)
+        swap_endian=true;
+#endif
+    
+    if (format=="u1") 
+        for (int i=0;i<v.length();i++)
+            v[i] = (real)((unsigned char*)buffer)[i];
+    else if (format=="u2") {
+        if (swap_endian)
+            endianswap2(buffer,width_);
+        for (int i=0;i<v.length();i++)
+            v[i] = (real)((unsigned short*)buffer)[i];
+    }
+    else if (format=="i4") {
+        if (swap_endian)
+            endianswap4(buffer,width_);
+        for (int i=0;i<v.length();i++)
+            v[i] = (real)((int*)buffer)[i];
+    }
+    else if (format=="f4") {
+        if (swap_endian)
+            endianswap4(buffer,width_);
+        for (int i=0;i<v.length();i++)
+            v[i] = (real)((float*)buffer)[i];
+    }
+    else if (format=="f8") {
+        if (swap_endian)
+            endianswap8(buffer,width_);
+        for (int i=0;i<v.length();i++)
+            v[i] = (real)((double*)buffer)[i];
+    }
+    else
+        PLERROR("BinaryNumbersVMatrix: unknown format = %s\n",format.c_str());
+}
+
+void BinaryNumbersVMatrix::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "filename", &BinaryNumbersVMatrix::filename,
+                   OptionBase::buildoption,
+                  "Name of file to be read.");
+
+    declareOption(ol, "format", &BinaryNumbersVMatrix::format,
+                   OptionBase::buildoption,
+                  "2-character specification of binary format of the numbers in the file:\n"
+                  "  u1 = 1-byte unsigned integers\n"
+                  "  u2 = 1-byte unsigned integers\n"
+                  "  i4 = 4-byte signed integers\n"
+                  "  f4 = 4-byte floating point\n"
+                  "  f8 = 8-byte floating point\n");
+
+    declareOption(ol, "header_size", &BinaryNumbersVMatrix::header_size,
+                   OptionBase::buildoption,
+                  "Number of bytes of header at beginning of file.");
+
+    declareOption(ol, "file_is_bigendian", &BinaryNumbersVMatrix::file_is_bigendian,
+                   OptionBase::buildoption,
+                  "Whether the byte order is 'BIG ENDIAN' or not.");
+
+    declareOption(ol, "row_size", &BinaryNumbersVMatrix::row_size,
+                   OptionBase::learntoption,
+                  "Number of bytes in each row");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void BinaryNumbersVMatrix::build_()
+{
+    if (!f)
+        PR_Close(f);
+    f = PR_Open(filename.c_str(), PR_RDONLY, 0666);
+    if (width_>0)
+    {
+        if (buffer) 
+            delete[] (char*)buffer;
+        row_size = width_*(format[1]-'0');
+        buffer = (void*) (new char[row_size]);
+    }
+}
+
+// ### Nothing to add here, simply calls build_
+void BinaryNumbersVMatrix::build()
+{
+    inherited::build();
+    build_();
+}
+
+void BinaryNumbersVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+}
+
+
+BinaryNumbersVMatrix::~BinaryNumbersVMatrix()
+{
+    if (buffer) delete[] (char*)buffer;
+    buffer=0;
+    if (f) PR_Close(f);
+    f=0;
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/vmat/BinaryNumbersVMatrix.h
===================================================================
--- trunk/plearn/vmat/BinaryNumbersVMatrix.h	2007-07-12 20:50:04 UTC (rev 7753)
+++ trunk/plearn/vmat/BinaryNumbersVMatrix.h	2007-07-12 22:14:43 UTC (rev 7754)
@@ -0,0 +1,163 @@
+// -*- C++ -*-
+
+// BinaryNumbersVMatrix.h
+//
+// Copyright (C) 2007 Yoshua Bengio
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Yoshua Bengio
+
+/*! \file BinaryNumbersVMatrix.h */
+
+
+#ifndef BinaryNumbersVMatrix_INC
+#define BinaryNumbersVMatrix_INC
+
+#include <plearn/vmat/RowBufferedVMatrix.h>
+#include <plearn/io/fileutils.h>
+#include <plearn/io/pl_NSPR_io.h>
+
+namespace PLearn {
+
+/**
+ * VMatrix that can take its values from a possibly large file (greater than 2Gig)
+ * containing numbers in a user-given binary format, preceded by an arbitrary header whose 
+ * length is user-given. 
+ *
+ */
+class BinaryNumbersVMatrix : public RowBufferedVMatrix
+{
+    typedef RowBufferedVMatrix inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+
+    PPath filename;
+    // u1 = 1-byte unsigned integers
+    // u2 = 1-byte unsigned integers
+    // i4 = 4-byte signed integers
+    // f4 = 4-byte floating point
+    // f8 = 8-byte floating point
+    string format;
+
+    // length of header in bytes
+    int header_size;
+
+    // endian-swapping option
+    bool file_is_bigendian;
+
+    // learnt options
+
+    // number of bytes of each row
+    int row_size;
+
+protected:
+    // NON-OPTIONS
+
+    PRFileDesc *f;
+    void* buffer;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    BinaryNumbersVMatrix();
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(BinaryNumbersVMatrix);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList& ol);
+
+    //! Fill the vector 'v' with the content of the i-th row.
+    //! 'v' is assumed to be the right size.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void getNewRow(int i, const Vec& v) const;
+
+    //! destructor releases buffer and file pointer
+    virtual ~BinaryNumbersVMatrix();
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(BinaryNumbersVMatrix);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From saintmlx at mail.berlios.de  Fri Jul 13 00:49:53 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Fri, 13 Jul 2007 00:49:53 +0200
Subject: [Plearn-commits] r7755 - trunk/plearn/base
Message-ID: <200707122249.l6CMnrBV015885@sheep.berlios.de>

Author: saintmlx
Date: 2007-07-13 00:49:52 +0200 (Fri, 13 Jul 2007)
New Revision: 7755

Modified:
   trunk/plearn/base/Object.cc
Log:
- remote-declared macroLoadObject



Modified: trunk/plearn/base/Object.cc
===================================================================
--- trunk/plearn/base/Object.cc	2007-07-12 22:14:43 UTC (rev 7754)
+++ trunk/plearn/base/Object.cc	2007-07-12 22:49:52 UTC (rev 7755)
@@ -1026,6 +1026,15 @@
                             "file containing the object to load"),
                      RetDoc ("newly created object")));
 
+    declareFunction("macroLoadObject", static_cast<Object* (*)(const PPath&,map<string,string>&)>(&macroLoadObject),
+                    (BodyDoc("Returns PLearn object from a file describing it,"
+                             " after macro-processing.\n"),
+                     ArgDoc("filename", 
+                            "file containing the object to load"),
+                     ArgDoc("vars", 
+                            "map of vars to values."),
+                     RetDoc ("newly created object")));
+
     declareFunction("deepCopy", &remote_deepCopy,
                     (BodyDoc("Returns deep copy of a PLearn object.\n"),
                      ArgDoc ("source", "object to be deep-copied"),



From yoshua at mail.berlios.de  Fri Jul 13 16:46:49 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Fri, 13 Jul 2007 16:46:49 +0200
Subject: [Plearn-commits] r7756 - trunk/plearn_learners/online/EXPERIMENTAL
Message-ID: <200707131446.l6DEknWi016444@sheep.berlios.de>

Author: yoshua
Date: 2007-07-13 16:46:49 +0200 (Fri, 13 Jul 2007)
New Revision: 7756

Modified:
   trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
Log:


Modified: trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-12 22:49:52 UTC (rev 7755)
+++ trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-13 14:46:49 UTC (rev 7756)
@@ -926,7 +926,8 @@
                     real act=ha(k,i);
                     // note that log sigmoid(act) = -softplus(-act)
                     // and       log(1 - sigmoid(act)) = -act -softplus(-act)
-                    lp += h[i]==1?-softplus(-act):-act-softplus(-act); 
+                    // and  h log(sigm(act))+(1-h)log(1-sigm(act)) = act*h-softplus(act)
+                    lp += h[i]*act-softplus(act); 
                 }
                 // now lp = log P(h|x^k)
                 if (k==0)
@@ -1684,8 +1685,8 @@
                         real act=ah_given_xk[i];
                         // note that log sigmoid(act) = -softplus(-act)
                         // and       log(1 - sigmoid(act)) = -act -softplus(-act)
-                        // so h*log(sigmoid(act))+(1-h)*log(sigmoid(act)) = act*h-act-softplus(act)
-                        lp += h[i]*act-act-softplus(-act);
+                        // so h*log(sigmoid(act))+(1-h)*log(sigmoid(act)) = act*h-softplus(act)
+                        lp += h[i]*act-softplus(act);
                     }
                     // now lp = log ( (1/(n P1(x^t))) P(h|x^k) )
 
@@ -1693,7 +1694,7 @@
                     for (int j=0;j<visible_layer->size;j++)
                     {
                         real act=avisible_given_h[j];
-                        lp += act*xt[j] - act - softplus(-act);
+                        lp += act*xt[j] - softplus(act);
                     }
                     // now lp = log ( (1/(n P1(x^t))) P(h|x^k)  P(x^t|h) )
                     real coeff = exp(lp);



From saintmlx at mail.berlios.de  Fri Jul 13 17:13:30 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Fri, 13 Jul 2007 17:13:30 +0200
Subject: [Plearn-commits] r7757 - trunk/plearn/python
Message-ID: <200707131513.l6DFDUxc019215@sheep.berlios.de>

Author: saintmlx
Date: 2007-07-13 17:13:29 +0200 (Fri, 13 Jul 2007)
New Revision: 7757

Modified:
   trunk/plearn/python/PythonCodeSnippet.cc
Log:
- don't re-inject global PLearn funcs. into python if already done



Modified: trunk/plearn/python/PythonCodeSnippet.cc
===================================================================
--- trunk/plearn/python/PythonCodeSnippet.cc	2007-07-13 14:46:49 UTC (rev 7756)
+++ trunk/plearn/python/PythonCodeSnippet.cc	2007-07-13 15:13:29 UTC (rev 7757)
@@ -549,13 +549,18 @@
     map<string, PyObject*> global_map= 
         wrapped_globals.as<map<string, PyObject*> >();
 
-    //inject global funcs
-    map<string, PyObject*>::iterator it_gf= 
-        global_map.find("pl_global_funcs");
-    if(it_gf == global_map.end())
-        PLERROR("in PythonCodeSnippet::compileGlobalCode : "
-                "plearn.pybridge.pl_global_funcs not present in global env!");
-    injectPLearnGlobalFunctions(it_gf->second);//inject in global_funcs module
+    //inject global funcs, if not already done
+    static bool global_funcs_injected= false;
+    if(!global_funcs_injected)
+    {
+        map<string, PyObject*>::iterator it= 
+            global_map.find("pl_global_funcs");
+        if(it == global_map.end())
+            PLERROR("in PythonCodeSnippet::compileGlobalCode : "
+                    "plearn.pybridge.pl_global_funcs not present in global env!");
+        injectPLearnGlobalFunctions(it->second);//inject in pl_global_funcs module
+        global_funcs_injected= true;
+    }
 
     //try to find an EmbeddedCodeSnippet to instantiate
     PyObject* snippet_found= 0;



From yoshua at mail.berlios.de  Fri Jul 13 17:32:51 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Fri, 13 Jul 2007 17:32:51 +0200
Subject: [Plearn-commits] r7758 - trunk/plearn_learners/online/EXPERIMENTAL
Message-ID: <200707131532.l6DFWpHY020477@sheep.berlios.de>

Author: yoshua
Date: 2007-07-13 17:32:50 +0200 (Fri, 13 Jul 2007)
New Revision: 7758

Modified:
   trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
Log:
Fixed more errors in the gradient of KLp0p1


Modified: trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-13 15:13:29 UTC (rev 7757)
+++ trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-13 15:32:50 UTC (rev 7758)
@@ -944,7 +944,7 @@
                     (*KLp0p1)(t,0) = logadd((*KLp0p1)(t,0), conf_visible_layer->fpropNLL((*visible)(t)) + log_sum_ph_given_xk);
         }
         *KLp0p1 *= -1;
-        *KLp0p1 += logn;
+        *KLp0p1 += 2*logn;
         // reset sizes as before
         hidden_layer->setBatchSize(mbs);
         visible_layer->setBatchSize(mbs);
@@ -1700,14 +1700,14 @@
                     real coeff = exp(lp);
                     Vec pvisible_given_h=pvisible_given_H(c);
                     for (int j=0;j<visible_layer->size;j++)
-                        visible_bias[j] -=
+                        visible_bias[j] +=
                             klp0p1_learning_rate*coeff*(xt[j]-pvisible_given_h[j]);
                     for (int i=0;i<hidden_layer->size;i++)
                     {
-                        hidden_bias[i] -= klp0p1_learning_rate*coeff*(h[i]-ph_given_xk[i]);
+                        hidden_bias[i] += klp0p1_learning_rate*coeff*(h[i]-ph_given_xk[i]);
                         for (int j=0;j<visible_layer->size;j++)
-                            W(i,j) -= klp0p1_learning_rate*coeff*
-                                (h[i]*(xt[j]-pvisible_given_h[j])-xk[j]*(h[i]-ph_given_xk[i]));
+                            W(i,j) += klp0p1_learning_rate*coeff*
+                                (h[i]*(xt[j]-pvisible_given_h[j])+xk[j]*(h[i]-ph_given_xk[i]));
                     }
                 }
             }



From tihocan at mail.berlios.de  Fri Jul 13 17:35:13 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 13 Jul 2007 17:35:13 +0200
Subject: [Plearn-commits] r7759 - trunk/plearn_learners/online/EXPERIMENTAL
Message-ID: <200707131535.l6DFZD0P020742@sheep.berlios.de>

Author: tihocan
Date: 2007-07-13 17:35:13 +0200 (Fri, 13 Jul 2007)
New Revision: 7759

Modified:
   trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
Log:
Fixed mistake in help for gradient computation. Did not verify whether the code is correctly implementing it yet

Modified: trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-13 15:32:50 UTC (rev 7758)
+++ trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-13 15:35:13 UTC (rev 7759)
@@ -61,7 +61,7 @@
     "input-to-hidden conditional distribution. Both are the usual found in Binomial"
     "layer RBMs here."
     "The gradient on the weight Wij is"
-    "  dC(x)/dWij = (1/(n P1(x))) "
+    "  dC(x)/dWij = (-1/(n P1(x))) "
     "       sum_{k=1}^n sum_h P(x|h) P(h|x^k) (h_i(x_j - P(x_j=1|h)) + x_j^k(h_i - P(h_i=1|x^k)))"
     "Apart from the KLp0p1 output port, and the fact that CD learning is replaced by minimization"
     "of KLp0p1, this module acts like a regular RBMModule."
@@ -1646,7 +1646,7 @@
         //   * -log P1(x^t) for each input visible(t) in KLp0p1(t,0)
         //
         // We want to compute
-        //   dC(x)/dWij = (1/(n P1(x))) 
+        //   dC(x)/dWij = (-1/(n P1(x))) 
         //       sum_{k=1}^n sum_h P(x|h) P(h|x^k) (h_i(x_j - P(x_j=1|h)) + x_j^k(h_i - P(h_i=1|x^k)))
         //
         PLASSERT_MSG(KLp0p1 && !KLp0p1->isEmpty(), "Must compute KLp0p1 in order to compute its gradient, connect that port!");



From dorionc at mail.berlios.de  Fri Jul 13 17:45:35 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Fri, 13 Jul 2007 17:45:35 +0200
Subject: [Plearn-commits] r7760 - in trunk/python_modules/plearn: math pytest
Message-ID: <200707131545.l6DFjZAW021534@sheep.berlios.de>

Author: dorionc
Date: 2007-07-13 17:45:34 +0200 (Fri, 13 Jul 2007)
New Revision: 7760

Modified:
   trunk/python_modules/plearn/math/arrays.py
   trunk/python_modules/plearn/pytest/programs.py
   trunk/python_modules/plearn/pytest/tests.py
Log:
- Added a 'dependencies' option to PyTest's Program instances
- Added a stripMissings() function in arrays.py


Modified: trunk/python_modules/plearn/math/arrays.py
===================================================================
--- trunk/python_modules/plearn/math/arrays.py	2007-07-13 15:35:13 UTC (rev 7759)
+++ trunk/python_modules/plearn/math/arrays.py	2007-07-13 15:45:34 UTC (rev 7760)
@@ -210,6 +210,10 @@
 def replace_nans(a, repl_with=0.0):
     return choose(isNotNaN(a), (repl_with, a))
 
+def stripMissings(vec):    
+    nan_filter = isNotNaN(vec)
+    return vec[nan_filter]
+
 def average(x, axis=0):
     arrx = array(x)
     return sum(arrx,axis) / arrx.shape[axis]

Modified: trunk/python_modules/plearn/pytest/programs.py
===================================================================
--- trunk/python_modules/plearn/pytest/programs.py	2007-07-13 15:35:13 UTC (rev 7759)
+++ trunk/python_modules/plearn/pytest/programs.py	2007-07-13 15:45:34 UTC (rev 7760)
@@ -81,11 +81,18 @@
     #######  Options  #############################################################
 
     name = PLOption(None)
+    
     compiler = PLOption(None)
+
     compile_options = PLOption(None)
-    no_plearn_options = PLOption(False)
 
+    no_plearn_options = PLOption(False,
+        doc="PLearn commands usually receive the --no-progess and --no-version options. "
+            "If that option is False though, this won't be the case for this Program instance.")
 
+    dependencies = PLOption([], doc="A list of programs on which this one depends.")
+    
+
     #######  Class Variables  #####################################################
 
     # Default compiler: for programs assumed to be compilable but for which
@@ -162,6 +169,8 @@
             return ""
         elif optname=="no_plearn_options" and not val:
             return "" # Don't print the default value
+        elif optname=="dependencies" and not val:
+            return "" # Don't print the default value
         else:
             return super(Program, self)._optionFormat(option_pair, indent_level, inner_repr)
 
@@ -173,9 +182,13 @@
             raise core.PyTestUsageError(
                 "Called PyTest with --no-compile option but %s "
                 "was not previously compiled." % self.getInternalExecPath())
-            
-        return no_need_to_compile or (self.__attempted_to_compile and exec_exists)
 
+        # Account for dependencies
+        success = no_need_to_compile or (self.__attempted_to_compile and exec_exists)
+        for dep in self.dependencies:
+            success = (success and dep.compilationSucceeded())
+        return success
+
     def compile(self, publish_dirpath=""):
         # Remove old compile log if any
         publish_target = os.path.join(publish_dirpath, os.path.basename(self.__log_file_path))
@@ -194,13 +207,18 @@
 
         # First compilation attempt
         else:
-            succeeded = self.__first_compilation_attempt()       
+            succeeded = self.__first_compilation_attempt()               
 
         # Publish the compile log
         if succeeded and publish_dirpath:
             logging.debug("Publishing the compile log %s"%self.__log_file_path)
             toolkit.symlink(self.__log_file_path,
                             moresh.relative_path(publish_target))
+
+        # Account for dependencies
+        for dep in self.dependencies:
+            succeeded = (succeeded and dep.compile(publish_dirpath))
+        
         return succeeded
         
     def __first_compilation_attempt(self):
@@ -347,6 +365,10 @@
             # Otherwise assumed to be non-compilable
             else:
                 self.__is_compilable = False
+                for dep in dependencies:
+                    if dep.isCompilable():
+                        self.__is_compilable = True
+                        break
 
             # It is now cached... 
             return self.__is_compilable

Modified: trunk/python_modules/plearn/pytest/tests.py
===================================================================
--- trunk/python_modules/plearn/pytest/tests.py	2007-07-13 15:35:13 UTC (rev 7759)
+++ trunk/python_modules/plearn/pytest/tests.py	2007-07-13 15:45:34 UTC (rev 7760)
@@ -691,7 +691,7 @@
         PyTestObject.__init__(self, **overrides) 
         os.chdir( self.test.directory() )
         
-    def compile_program(self):
+    def compileProgram(self):
         if not self.test.program.isCompilable():
             return True
 
@@ -712,7 +712,7 @@
 class CompilationRoutine(Routine):
     """Launches the compilation of target tests' compilable files."""    
     def start(self):
-        if self.compile_program():
+        if self.compileProgram():
             self.test.setStatus("PASSED")
 
 
@@ -735,7 +735,7 @@
                 os.remove( fname )
 
     def run_test(self, results):
-        if not self.compile_program():
+        if not self.compileProgram():
             logging.debug("%s bails out." % self.classname())
             return 
 



From yoshua at mail.berlios.de  Fri Jul 13 17:53:04 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Fri, 13 Jul 2007 17:53:04 +0200
Subject: [Plearn-commits] r7761 - trunk/plearn_learners/online/EXPERIMENTAL
Message-ID: <200707131553.l6DFr4Ho021912@sheep.berlios.de>

Author: yoshua
Date: 2007-07-13 17:53:04 +0200 (Fri, 13 Jul 2007)
New Revision: 7761

Modified:
   trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
Log:
Trying to make KLp0p1 positive!


Modified: trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-13 15:45:34 UTC (rev 7760)
+++ trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-13 15:53:04 UTC (rev 7761)
@@ -54,14 +54,15 @@
     "This criterion is described and justified in the paper by Le Roux and Bengio entitled"
     "'Representational Power of Restricted Boltzmann Machines and Deep Belief Networks'."
     "The exact and very inefficient implementation of this criterion is done here."
+    "  KL(p0||p1) = sum_x p0(x) log p0(x)/p1(x) = - sum_i (1/n) log p1(x_i) + sum_i (1/n) log(1/n)"
     "For an example x the cost is:"
-    "  C(x) = - log P1(x) = - log (1/n)sum_{k=1}^n sum_h P(x|h) P(h|x^k)"
+    "  C(x) = - log p1(x) -(1/n) log n = - log (1/n)sum_{k=1}^n sum_h P(x|h) P(h|x^k) -(1/n)log n"
     "where {x^1, ... x^n} is the training set of examples x^k, h is a hidden layer bit vector,"
     "P(x|h) is the hidden-to-visible conditional distribution and P(h|x) is the"
     "input-to-hidden conditional distribution. Both are the usual found in Binomial"
     "layer RBMs here."
     "The gradient on the weight Wij is"
-    "  dC(x)/dWij = (-1/(n P1(x))) "
+    "  dC(x)/dWij = (-1/(n p1(x))) "
     "       sum_{k=1}^n sum_h P(x|h) P(h|x^k) (h_i(x_j - P(x_j=1|h)) + x_j^k(h_i - P(h_i=1|x^k)))"
     "Apart from the KLp0p1 output port, and the fact that CD learning is replaced by minimization"
     "of KLp0p1, this module acts like a regular RBMModule."
@@ -892,7 +893,7 @@
 
         PLASSERT_MSG(hidden_layer->size<32,"To compute KLp0p1 of an RBM, hidden_layer->size must be <32");
         PLASSERT(hidden_layer->classname()=="RBMBinomialLayer");
-        real logn=safelog(n);
+        real logn=safelog((real)n);
         // assuming a binary hidden we sum over all bit configurations
         int n_configurations = 1 << hidden_layer->size; // = 2^{hidden_layer->size}
         // put all h configurations in the hidden_layer->samples
@@ -944,7 +945,7 @@
                     (*KLp0p1)(t,0) = logadd((*KLp0p1)(t,0), conf_visible_layer->fpropNLL((*visible)(t)) + log_sum_ph_given_xk);
         }
         *KLp0p1 *= -1;
-        *KLp0p1 += 2*logn;
+        *KLp0p1 += logn*((real)1.0 - (real)1.0/(real)n);
         // reset sizes as before
         hidden_layer->setBatchSize(mbs);
         visible_layer->setBatchSize(mbs);



From yoshua at mail.berlios.de  Fri Jul 13 19:48:26 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Fri, 13 Jul 2007 19:48:26 +0200
Subject: [Plearn-commits] r7762 - trunk/plearn_learners/online/EXPERIMENTAL
Message-ID: <200707131748.l6DHmQ0G011096@sheep.berlios.de>

Author: yoshua
Date: 2007-07-13 19:48:26 +0200 (Fri, 13 Jul 2007)
New Revision: 7762

Modified:
   trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
Log:
Now the KL is positive...


Modified: trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-13 15:53:04 UTC (rev 7761)
+++ trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-13 17:48:26 UTC (rev 7762)
@@ -915,8 +915,8 @@
 
         for (int c=0;c<n_configurations;c++)
         {
-            //  C(x) = - log P1(x) = - log (1/n)sum_{k=1}^n sum_h P(x|h) P(h|x^k)
-            //                     = - log sum_h P(x|h) (sum_k P(h|x^k))/n
+            //  C(x) = - log P1(x) =  (1-1/n)log n - log sum_{k=1}^n sum_h P(x|h) P(h|x^k)
+            //                     =  (1-1/n)log n - log sum_h P(x|h) (sum_k P(h|x^k))/n
             real log_sum_ph_given_xk = 0;
             Vec h = conf_hidden_layer->samples(c);
             for (int k=0;k<n;k++)
@@ -940,13 +940,13 @@
             conf_visible_layer->activation << conf_visible_layer->activations(c);
             for (int t=0;t<mbs;t++)
                 if (c==0)
-                    (*KLp0p1)(t,0) = conf_visible_layer->fpropNLL((*visible)(t)) + log_sum_ph_given_xk;
+                    (*KLp0p1)(t,0) = -conf_visible_layer->fpropNLL((*visible)(t)) + log_sum_ph_given_xk;
                 else
-                    (*KLp0p1)(t,0) = logadd((*KLp0p1)(t,0), conf_visible_layer->fpropNLL((*visible)(t)) + log_sum_ph_given_xk);
+                    (*KLp0p1)(t,0) = logadd((*KLp0p1)(t,0), -conf_visible_layer->fpropNLL((*visible)(t)) + log_sum_ph_given_xk);
         }
         *KLp0p1 *= -1;
         *KLp0p1 += logn*((real)1.0 - (real)1.0/(real)n);
-        // reset sizes as before
+        // going in the other direction just for the fun of it:
         hidden_layer->setBatchSize(mbs);
         visible_layer->setBatchSize(mbs);
     }



From yoshua at mail.berlios.de  Fri Jul 13 19:49:46 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Fri, 13 Jul 2007 19:49:46 +0200
Subject: [Plearn-commits] r7763 - trunk/plearn/vmat
Message-ID: <200707131749.l6DHnkU1011188@sheep.berlios.de>

Author: yoshua
Date: 2007-07-13 19:49:46 +0200 (Fri, 13 Jul 2007)
New Revision: 7763

Modified:
   trunk/plearn/vmat/BinaryNumbersVMatrix.cc
Log:
Fixed typo causing crash.


Modified: trunk/plearn/vmat/BinaryNumbersVMatrix.cc
===================================================================
--- trunk/plearn/vmat/BinaryNumbersVMatrix.cc	2007-07-13 17:48:26 UTC (rev 7762)
+++ trunk/plearn/vmat/BinaryNumbersVMatrix.cc	2007-07-13 17:49:46 UTC (rev 7763)
@@ -137,7 +137,7 @@
 
 void BinaryNumbersVMatrix::build_()
 {
-    if (!f)
+    if (f)
         PR_Close(f);
     f = PR_Open(filename.c_str(), PR_RDONLY, 0666);
     if (width_>0)
@@ -166,7 +166,8 @@
 {
     if (buffer) delete[] (char*)buffer;
     buffer=0;
-    if (f) PR_Close(f);
+    if (f) 
+        PR_Close(f);
     f=0;
 }
 



From yoshua at mail.berlios.de  Fri Jul 13 20:11:22 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Fri, 13 Jul 2007 20:11:22 +0200
Subject: [Plearn-commits] r7764 - trunk/plearn_learners/online/EXPERIMENTAL
Message-ID: <200707131811.l6DIBMBo012420@sheep.berlios.de>

Author: yoshua
Date: 2007-07-13 20:11:21 +0200 (Fri, 13 Jul 2007)
New Revision: 7764

Modified:
   trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
Log:
Fixed something in gradient computation of KLp0p1


Modified: trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-13 17:49:46 UTC (rev 7763)
+++ trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-13 18:11:21 UTC (rev 7764)
@@ -915,8 +915,10 @@
 
         for (int c=0;c<n_configurations;c++)
         {
-            //  C(x) = - log P1(x) =  (1-1/n)log n - log sum_{k=1}^n sum_h P(x|h) P(h|x^k)
-            //                     =  (1-1/n)log n - log sum_h P(x|h) (sum_k P(h|x^k))/n
+            //  C(x) =  -log p1(x) -(1/n)logn
+            //       =  (1-1/n)log n - log sum_{k=1}^n sum_h P(x|h) P(h|x^k)
+            //       =  (1-1/n)log n - log sum_h P(x|h) (sum_k P(h|x^k))/n
+
             real log_sum_ph_given_xk = 0;
             Vec h = conf_hidden_layer->samples(c);
             for (int k=0;k<n;k++)
@@ -1679,7 +1681,8 @@
                 {
                     Vec h = conf_hidden_layer->samples(c);
                     Vec avisible_given_h=conf_visible_layer->activations(c);
-                    real lp = (*KLp0p1)(t,0) - logn; // lp = log (1/(n P1(x^t)))
+                    // KLp0p1(x) = -log p1(x) -(1/n)logn
+                    real lp = -(*KLp0p1)(t,0) - (1+1/(real)n)*logn; // lp = log (1/(n P1(x^t)))
                     // compute and multiply by P(h|x^k)
                     for (int i=0;i<hidden_layer->size;i++)
                     {



From yoshua at mail.berlios.de  Fri Jul 13 20:57:23 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Fri, 13 Jul 2007 20:57:23 +0200
Subject: [Plearn-commits] r7765 - trunk/commands
Message-ID: <200707131857.l6DIvNOW014541@sheep.berlios.de>

Author: yoshua
Date: 2007-07-13 20:57:23 +0200 (Fri, 13 Jul 2007)
New Revision: 7765

Modified:
   trunk/commands/plearn_light_inc.h
Log:
Added BinaryNumbersVMatrix


Modified: trunk/commands/plearn_light_inc.h
===================================================================
--- trunk/commands/plearn_light_inc.h	2007-07-13 18:11:21 UTC (rev 7764)
+++ trunk/commands/plearn_light_inc.h	2007-07-13 18:57:23 UTC (rev 7765)
@@ -271,6 +271,7 @@
 #include <plearn/vmat/DatedJoinVMatrix.h>
 // #include <plearn/vmat/DictionaryVMatrix.h>
 #include <plearn/vmat/DisregardRowsVMatrix.h>
+#include <plearn/vmat/BinaryNumbersVMatrix.h>
 #include <plearn/vmat/ExtractNNetParamsVMatrix.h>
 #include <plearn/vmat/FilteredVMatrix.h>
 #include <plearn/vmat/FinancePreprocVMatrix.h>



From yoshua at mail.berlios.de  Fri Jul 13 21:16:41 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Fri, 13 Jul 2007 21:16:41 +0200
Subject: [Plearn-commits] r7766 - trunk/commands
Message-ID: <200707131916.l6DJGf0W015573@sheep.berlios.de>

Author: yoshua
Date: 2007-07-13 21:16:40 +0200 (Fri, 13 Jul 2007)
New Revision: 7766

Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
Added BinaryNumbersVMatrix


Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2007-07-13 18:57:23 UTC (rev 7765)
+++ trunk/commands/plearn_noblas_inc.h	2007-07-13 19:16:40 UTC (rev 7766)
@@ -288,6 +288,7 @@
 #include <plearn/vmat/DatedJoinVMatrix.h>
 // #include <plearn/vmat/DictionaryVMatrix.h>
 #include <plearn/vmat/DisregardRowsVMatrix.h>
+#include <plearn/vmat/BinaryNumbersVMatrix.h>
 #include <plearn/vmat/ExtractNNetParamsVMatrix.h>
 #include <plearn/vmat/FilteredVMatrix.h>
 #include <plearn/vmat/FinancePreprocVMatrix.h>



From nouiz at mail.berlios.de  Fri Jul 13 21:35:53 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 13 Jul 2007 21:35:53 +0200
Subject: [Plearn-commits] r7767 - trunk/python_modules/plearn/parallel
Message-ID: <200707131935.l6DJZrUd016386@sheep.berlios.de>

Author: nouiz
Date: 2007-07-13 21:35:52 +0200 (Fri, 13 Jul 2007)
New Revision: 7767

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
BUGFIX we create a directory only if it don't existe


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-07-13 19:16:40 UTC (rev 7766)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-07-13 19:35:52 UTC (rev 7767)
@@ -359,7 +359,8 @@
 
     def __init__( self, commands, **args ):
         DBIBase.__init__(self, commands, **args)
-        os.mkdir(self.log_dir) # condor log are always generated
+        if not os.path.exists(self.log_dir):
+            os.mkdir(self.log_dir) # condor log are always generated
         
         if not os.path.exists(self.tmp_dir):
             os.mkdir(self.tmp_dir)



From nouiz at mail.berlios.de  Fri Jul 13 21:39:28 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 13 Jul 2007 21:39:28 +0200
Subject: [Plearn-commits] r7768 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200707131939.l6DJdSjq016662@sheep.berlios.de>

Author: nouiz
Date: 2007-07-13 21:39:28 +0200 (Fri, 13 Jul 2007)
New Revision: 7768

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
Log:
made NatGradNNet work with minibatch for calculating the cost and output


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-07-13 19:35:52 UTC (rev 7767)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-07-13 19:39:28 UTC (rev 7768)
@@ -164,7 +164,8 @@
 
     declareOption(ol, "minibatch_size", &NatGradNNet::minibatch_size,
                   OptionBase::buildoption,
-                  "Update the parameters only so often (number of examples).\n");
+                  "Update the parameters only so often (number of examples).\n"
+                  "Must be greater or equal to test_minibatch_size\n");
 
     declareOption(ol, "neurons_natgrad_template", &NatGradNNet::neurons_natgrad_template,
                   OptionBase::buildoption,
@@ -625,9 +626,9 @@
         int b = stage % minibatch_size;
         Vec input = neuron_outputs_per_layer[0](b);
         Vec target = targets(b);
-        Profiler::pl_profile_start("getting_data");
+        Profiler::pl_profile_start("NatGradNNet::getting_data");
         train_set->getExample(sample, input, target, example_weights[b]);
-        Profiler::pl_profile_end("getting_data");
+        Profiler::pl_profile_end("NatGradNNet::getting_data");
         if (b+1==minibatch_size) // do also special end-case || stage+1==nstages)
         {
             onlineStep(stage, targets, train_costs, example_weights );
@@ -876,11 +877,11 @@
 
 void NatGradNNet::computeOutput(const Vec& input, Vec& output) const
 {
-    Profiler::pl_profile_start("computeOutput");
+    Profiler::pl_profile_start("NatGradNNet::computeOutput");
     neuron_outputs_per_layer[0](0) << input;
     fpropNet(1,false);
     output << neuron_outputs_per_layer[n_layers-1](0);
-    Profiler::pl_profile_end("computeOutput");
+    Profiler::pl_profile_end("NatGradNNet::computeOutput");
 }
 
 //! compute (pre-final-non-linearity) network top-layer output given input
@@ -1072,7 +1073,48 @@
     Mat costsM = costs.toMat(1,costs.length());
     fbpropLoss(outputM,targetM,w,costsM);
 }
+/*
+void NatGradNNet::computeOutput(const Vec& input, Vec& output)
+{
+    Profiler::pl_profile_start("computeOutput");
+    neuron_outputs_per_layer[0](0) << input;
+    fpropNet(1,false);
+    output << neuron_outputs_per_layer[n_layers-1](0);
+    Profiler::pl_profile_end("computeOutput");
+    }
+void PLearner::computeOutputAndCosts(const Vec& input, const Vec& target, 
+                                     Vec& output, Vec& costs) const
+{
+    computeOutput(input, output);
+    computeCostsFromOutputs(input, output, target, costs);
+}
+*/
 
+void NatGradNNet::computeOutputs(const Mat& input, Mat& output) const
+{
+    Profiler::pl_profile_start("NatGradNNet::computeOutputs");
+    PLASSERT(test_minibatch_size<=minibatch_size);
+    neuron_outputs_per_layer[0].subMat(0,0,input.length(),input.width()) << input;
+    fpropNet(input.length(),false);
+    output << neuron_outputs_per_layer[n_layers-1].subMat(0,0,output.length(),output.width());
+    Profiler::pl_profile_end("NatGradNNet::computeOutputs");
+}
+void NatGradNNet::computeOutputsAndCosts(const Mat& input, const Mat& target, 
+                                      Mat& output, Mat& costs) const
+{//TODO
+    Profiler::pl_profile_start("NatGradNNet::computeOutputsAndCosts");
+
+    int n=input.length();
+    PLASSERT(target.length()==n);
+    output.resize(n,outputsize());
+    costs.resize(n,nTestCosts());
+    computeOutputs(input,output);
+
+    Vec w(n);
+    w.fill(1);
+    fbpropLoss(output,target,w,costs);
+    Profiler::pl_profile_end("NatGradNNet::computeOutputsAndCosts");
+    }
 TVec<string> NatGradNNet::getTestCostNames() const
 {
     TVec<string> costs;

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-07-13 19:35:52 UTC (rev 7767)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-07-13 19:39:28 UTC (rev 7768)
@@ -206,7 +206,12 @@
     //! Computes the output from the input.
     // (PLEASE IMPLEMENT IN .cc)
     virtual void computeOutput(const Vec& input, Vec& output) const;
+    virtual void computeOutputs(const Mat& input, Mat& output) const;
 
+    virtual void computeOutputsAndCosts(const Mat& input, const Mat& target, 
+                                        Mat& output, Mat& costs) const;
+
+
     //! Computes the costs from already computed output.
     // (PLEASE IMPLEMENT IN .cc)
     virtual void computeCostsFromOutputs(const Vec& input, const Vec& output,



From nouiz at mail.berlios.de  Fri Jul 13 22:55:51 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 13 Jul 2007 22:55:51 +0200
Subject: [Plearn-commits] r7769 - trunk/plearn_learners/generic
Message-ID: <200707132055.l6DKtpep021275@sheep.berlios.de>

Author: nouiz
Date: 2007-07-13 22:55:51 +0200 (Fri, 13 Jul 2007)
New Revision: 7769

Modified:
   trunk/plearn_learners/generic/AddCostToLearner.cc
   trunk/plearn_learners/generic/AddCostToLearner.h
   trunk/plearn_learners/generic/EmbeddedLearner.cc
   trunk/plearn_learners/generic/EmbeddedLearner.h
Log:
-Made AddCostToLearner and EmbeddedLearner use the computOutputsAndCosts from sub learner so that they respect the test_minibatch_size flag
-In AddCostToLearner their is a squellette that is not finished that add an option to select the threshold between class


Modified: trunk/plearn_learners/generic/AddCostToLearner.cc
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.cc	2007-07-13 19:39:28 UTC (rev 7768)
+++ trunk/plearn_learners/generic/AddCostToLearner.cc	2007-07-13 20:55:51 UTC (rev 7769)
@@ -94,7 +94,8 @@
       to_max(1),
       to_min(0),
       n_classes(-1),
-      confusion_matrix_target(0)
+      confusion_matrix_target(0),
+      find_class_threshold(0)
 {}
 
 ////////////////////
@@ -177,6 +178,11 @@
                   OptionBase::buildoption,
         "Index of the target for which the confusion matrix is computed.");
 
+    declareOption(ol, "find_class_threshold",
+                  &AddCostToLearner::find_class_threshold,
+                  OptionBase::buildoption,
+        "0 if we don't find the best threshold between classes.\n"
+        "Otherwise we find the best threshold between classes");
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -267,18 +273,20 @@
 // computeCostsFromOutputs //
 /////////////////////////////
 void AddCostToLearner::computeCostsFromOutputs(const Vec& input, const Vec& output, 
-                                               const Vec& target, Vec& costs) const
+                                                const Vec& target, Vec& costs,
+                                                const bool add_sub_learner_costs) const
 {
     int n_original_costs = learner_->nTestCosts();
     // We give only costs.subVec to the sub-learner because it may want to resize it.
     costs.resize(nTestCosts());
     Vec sub_costs = costs.subVec(0, n_original_costs);
     int target_length = target.length();
-    if (compute_costs_on_bags) {
-        learner_->computeCostsFromOutputs(input, output, target.subVec(0, target_length - 1), sub_costs);
-    } else {
-        learner_->computeCostsFromOutputs(input, output, target, sub_costs);
-    }
+    if(add_sub_learner_costs)
+        if (compute_costs_on_bags) {
+            learner_->computeCostsFromOutputs(input, output, target.subVec(0, target_length - 1), sub_costs);
+        } else {
+            learner_->computeCostsFromOutputs(input, output, target, sub_costs);
+        }
 
     if (compute_costs_on_bags) {
         // We only need to compute the costs when the whole bag has been seen,
@@ -327,7 +335,8 @@
                 PLERROR("In AddCostToLearner::computeCostsFromOutputs - Unknown value for 'combine_bag_outputs_method'");
             }
             // We re-compute the sub-learner's costs with the brand new combined bag output.
-            learner_->computeCostsFromOutputs(input, combined_output, target.subVec(0, target_length - 1), sub_costs);
+            if(add_sub_learner_costs)
+                learner_->computeCostsFromOutputs(input, combined_output, target.subVec(0, target_length - 1), sub_costs);
         } else {
             costs.fill(MISSING_VALUE);
             return;
@@ -543,7 +552,10 @@
             int output_length = sub_learner_output.length();
             int local_ind = 0;
             if (output_length == target_length) {
-                local_ind = ind_cost + int(round(sub_learner_output[confusion_matrix_target]))
+                int sub_learner_out = int(round(sub_learner_output[confusion_matrix_target]));
+//if outside allowd range, will access the wrong element in the cost vector
+                PLASSERT(sub_learner_out>=n_classes || sub_learner_out<0);
+                local_ind = ind_cost + sub_learner_out
                     + int(round(desired_target[confusion_matrix_target]))*n_classes;
             } else if (target_length == 1){
                 local_ind = ind_cost + argmax(sub_learner_output) + int(round(desired_target[confusion_matrix_target]))*n_classes;
@@ -563,8 +575,89 @@
             PLERROR("In AddCostToLearner::computeCostsFromOutputs - Unknown cost");
         }
     }
-}                                
+}
 
+///////////
+// train //
+///////////
+void AddCostToLearner::train()
+{
+    Profiler::pl_profile_start("AddCostToLearner::train");
+
+    int find_threshold = -1;
+    if(find_class_threshold){
+        for (int i = 0; i < this->costs.length(); i++) {
+            if(costs[i]=="square_class_error" || costs[i]=="linear_class_error" || costs[i]=="class_error" )
+                find_threshold = i;
+            break;
+        }
+    }
+    if(find_class_threshold != 0)
+        PLASSERT_MSG(-1 != find_threshold , "We where asked to find the threashold and no *class_error costs are selected.\n"
+                     "We use the first *class_error cost to select the threshold");
+
+    PLASSERT( learner_ );
+    learner_->nstages = nstages;
+    learner_->train();
+    stage = learner_->stage;
+
+    if(-1 != find_threshold){
+        
+        Vec input;
+        Vec target;
+        Vec output;
+        Vec outcosts;
+        real weight;
+        output.resize(learner_->outputsize());
+        outcosts.resize(learner_->nTestCosts());
+        class_threshold.resize(n_classes);
+        Vec test_threshold;
+        Vec best_threshold;
+        test_threshold.resize(n_classes);
+        best_threshold.resize(n_classes);
+        double best_class_error = -1;
+        int costs_index = -1;
+        TVec<string> costsnames=getTestCostNames();
+        Vec paramtotry;
+        for(float f=0;f<3;f+=0.1)
+            paramtotry.append(f);
+
+        //find the index of the costs to use.
+        for(int i=0;i<costsnames.size();i++){
+            string str1 = costsnames[i];
+            string str2 = costs[find_threshold];
+            if( str1 == str2){
+                costs_index = i;
+                break;
+            }
+        }
+
+        for(int a=0;a<paramtotry.size();a++){
+            for(int b=a+1;b<paramtotry.size();b++){
+                test_threshold[0] = paramtotry[a];
+                test_threshold[1]  = paramtotry[b];
+                double cum_class_error = 0;
+                for(int i=0;i<train_set->length();i++){
+                    learner_->getTrainingSet().getExample(i, input, target, weight);
+                    computeOutputAndCosts(input, target, output, outcosts);
+                    cum_class_error += outcosts[costs_index];
+                }
+                if(best_class_error == -1 || best_class_error > cum_class_error){
+                    best_threshold << test_threshold;
+                    best_class_error = cum_class_error;
+                }
+            }
+        }
+        class_threshold << best_threshold;
+        if(verbosity >=2)
+            for(int i=0;i<class_threshold.size();i++)
+                cout << "class_threshold[" << i << "] = " <<class_threshold[i] << endl;
+
+    }
+    Profiler::pl_profile_end("AddCostToLearner::train");
+
+}
+
 ///////////////////////////
 // computeOutputAndCosts //
 ///////////////////////////
@@ -573,7 +666,24 @@
     PLearner::computeOutputAndCosts(input, target, output, costs);
 }
 
-
+///////////////////////////
+// computeOutputsAndCosts //
+///////////////////////////
+void AddCostToLearner::computeOutputsAndCosts(const Mat& input, const Mat& target,
+                                             Mat& output, Mat& costs) const {
+    PLASSERT( learner_ );
+    Mat sub_costs = costs.subMatColumns(0, learner_->nTestCosts());
+    learner_->computeOutputsAndCosts(input, target, output, sub_costs);
+    for (int i=0;i<input.length();i++)
+    {
+        Vec in_i = input(i);
+        Vec out_i = output(i); 
+        Vec target_i = target(i);
+        Vec c_i = costs(i);
+        computeCostsFromOutputs(in_i,out_i,target_i,c_i,false);
+    }
+    
+}
 ////////////
 // forget //
 ////////////

Modified: trunk/plearn_learners/generic/AddCostToLearner.h
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.h	2007-07-13 19:39:28 UTC (rev 7768)
+++ trunk/plearn_learners/generic/AddCostToLearner.h	2007-07-13 20:55:51 UTC (rev 7769)
@@ -104,6 +104,9 @@
     //! Its value is desired_target[0].
     Var target_var;
 
+    //! The threshold between class
+    Vec class_threshold;
+
 public:
 
     // ************************
@@ -123,6 +126,7 @@
     real to_min;
     int n_classes;
     int confusion_matrix_target;
+    int find_class_threshold;
 
     // ****************
     // * Constructors *
@@ -159,23 +163,35 @@
     // Declares other standard object methods.
     PLEARN_DECLARE_OBJECT(AddCostToLearner);
 
-
     // **************************
     // **** PLearner methods ****
     // **************************
 
+    virtual void train();
+
     //! (Re-)initializes the PLearner in its fresh state (that state may depend on the 'seed' option)
     //! And sets 'stage' back to 0   (this is the stage of a fresh learner!).
     virtual void forget();
 
-    //! Computes the costs from already computed output. 
+    //! Computes our and from the sub_learner costs from already computed output. 
     virtual void computeCostsFromOutputs(const Vec& input, const Vec& output, 
-                                         const Vec& target, Vec& costs) const;
+                                         const Vec& target, Vec& costs,
+                                         const bool add_sub_learner_costs) const;
 
+    //! Computes our and from the sub_learner costs from already computed output. 
+    void computeCostsFromOutputs(const Vec& input, const Vec& output, 
+                                         const Vec& target, Vec& costs) const{
+        computeCostsFromOutputs(input,output,target,costs,true);
+    }
+
     //! Overridden to use default PLearner behavior.
     virtual void computeOutputAndCosts(const Vec& input, const Vec& target,
                                        Vec& output, Vec& costs) const;
 
+    //! Overridden to use the sublearner version and complete it
+    virtual void computeOutputsAndCosts(const Mat& input, const Mat& target,
+                                       Mat& output, Mat& costs) const;
+
     //! Returns the names of the costs computed by computeCostsFromOutpus (and thus the test method).
     virtual TVec<string> getTestCostNames() const;
 

Modified: trunk/plearn_learners/generic/EmbeddedLearner.cc
===================================================================
--- trunk/plearn_learners/generic/EmbeddedLearner.cc	2007-07-13 19:39:28 UTC (rev 7768)
+++ trunk/plearn_learners/generic/EmbeddedLearner.cc	2007-07-13 20:55:51 UTC (rev 7769)
@@ -208,6 +208,13 @@
     learner_->computeOutputAndCosts(input, target, output, costs); 
 }
 
+void EmbeddedLearner::computeOutputsAndCosts(const Mat& input, const Mat& target, 
+                                            Mat& output, Mat& costs) const
+{ 
+    PLASSERT( learner_ );
+    learner_->computeOutputsAndCosts(input, target, output, costs); 
+}
+
 bool EmbeddedLearner::computeConfidenceFromOutput(
     const Vec& input, const Vec& output,
     real probability, TVec< pair<real,real> >& intervals) const

Modified: trunk/plearn_learners/generic/EmbeddedLearner.h
===================================================================
--- trunk/plearn_learners/generic/EmbeddedLearner.h	2007-07-13 19:39:28 UTC (rev 7768)
+++ trunk/plearn_learners/generic/EmbeddedLearner.h	2007-07-13 20:55:51 UTC (rev 7769)
@@ -137,6 +137,10 @@
                                        Vec& output, Vec& costs) const;
 
     //! Forwarded to inner learner
+    virtual void computeOutputsAndCosts(const Mat& input, const Mat& target,
+                                       Mat& output, Mat& costs) const;
+
+    //! Forwarded to inner learner
     virtual
     bool computeConfidenceFromOutput(const Vec& input, const Vec& output,
                                      real probability,



From nouiz at mail.berlios.de  Fri Jul 13 23:03:54 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 13 Jul 2007 23:03:54 +0200
Subject: [Plearn-commits] r7770 - trunk/plearn_learners/generic
Message-ID: <200707132103.l6DL3s3e021708@sheep.berlios.de>

Author: nouiz
Date: 2007-07-13 23:03:54 +0200 (Fri, 13 Jul 2007)
New Revision: 7770

Modified:
   trunk/plearn_learners/generic/AddCostToLearner.cc
Log:
Added missing include


Modified: trunk/plearn_learners/generic/AddCostToLearner.cc
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.cc	2007-07-13 20:55:51 UTC (rev 7769)
+++ trunk/plearn_learners/generic/AddCostToLearner.cc	2007-07-13 21:03:54 UTC (rev 7770)
@@ -48,6 +48,7 @@
 #include <plearn/var/SumOverBagsVariable.h>   //!< For the bag signal constants.
 #include <plearn/var/VarArray.h>
 #include <plearn/var/VecElementVariable.h>
+#include <plearn/sys/Profiler.h>
 
 namespace PLearn {
 using namespace std;



From yoshua at mail.berlios.de  Fri Jul 13 23:51:45 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Fri, 13 Jul 2007 23:51:45 +0200
Subject: [Plearn-commits] r7771 - trunk/plearn/vmat
Message-ID: <200707132151.l6DLpj2C024198@sheep.berlios.de>

Author: yoshua
Date: 2007-07-13 23:51:45 +0200 (Fri, 13 Jul 2007)
New Revision: 7771

Modified:
   trunk/plearn/vmat/BinaryNumbersVMatrix.cc
Log:
Do not use int to address more than 2Gig!


Modified: trunk/plearn/vmat/BinaryNumbersVMatrix.cc
===================================================================
--- trunk/plearn/vmat/BinaryNumbersVMatrix.cc	2007-07-13 21:03:54 UTC (rev 7770)
+++ trunk/plearn/vmat/BinaryNumbersVMatrix.cc	2007-07-13 21:51:45 UTC (rev 7771)
@@ -61,7 +61,10 @@
 void BinaryNumbersVMatrix::getNewRow(int i, const Vec& v) const
 {
     PLASSERT_MSG(v.length()==width_,"BinaryNumbersVMatrix::getNewRow(i,v) with v.length!= vmatrix width");
-    PR_Seek64(f,header_size+i*row_size,PR_SEEK_SET);
+    PRInt64 offset = i;
+    offset *= row_size;
+    offset += header_size;
+    PR_Seek64(f,offset,PR_SEEK_SET);
     PR_Read(f,buffer,row_size);
     bool swap_endian=false;
 #ifdef LITTLEENDIAN



From lamblin at mail.berlios.de  Sat Jul 14 05:31:34 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Sat, 14 Jul 2007 05:31:34 +0200
Subject: [Plearn-commits] r7772 - trunk/plearn_learners/generic
Message-ID: <200707140331.l6E3VYMh001055@sheep.berlios.de>

Author: lamblin
Date: 2007-07-14 05:31:34 +0200 (Sat, 14 Jul 2007)
New Revision: 7772

Modified:
   trunk/plearn_learners/generic/PLearner.cc
Log:
Initialize field names of stats collectors


Modified: trunk/plearn_learners/generic/PLearner.cc
===================================================================
--- trunk/plearn_learners/generic/PLearner.cc	2007-07-13 21:51:45 UTC (rev 7771)
+++ trunk/plearn_learners/generic/PLearner.cc	2007-07-14 03:31:34 UTC (rev 7772)
@@ -869,7 +869,7 @@
 //////////
 // test //
 //////////
-void PLearner::test(VMat testset, PP<VecStatsCollector> test_stats, 
+void PLearner::test(VMat testset, PP<VecStatsCollector> test_stats,
                     VMat testoutputs, VMat testcosts) const
 {
     int len = testset.length();
@@ -886,9 +886,12 @@
         test_stats->update(costs);
     }
 
+    if (test_stats)
+        // Set names of test_stats costs
+        test_stats->setFieldNames(getTestCostNames());
 
     PP<ProgressBar> pb;
-    if (report_progress) 
+    if (report_progress)
         pb = new ProgressBar("Testing learner", len);
 
     PP<PRandom> copy_random_gen=0;
@@ -905,7 +908,7 @@
     const int chunksize= 2500;//nb. rows in each chunk sent to a remote server
     const int chunks_per_server= 3;//ideal nb. chunks per server
     int nservers= min(len/(chunks_per_server*chunksize), service.availableServers());
-    
+
     if(nservers > 1 && parallelize_here && !isStatefulLearner())
     {// parallel test
         CopiesMap copies;
@@ -1144,7 +1147,7 @@
 {
     string warn_msg = "In PLearner::initTrain (called by '" +
         this->classname() + "') - ";
-    
+
     // Check 'nstages' is valid.
     if (nstages < 0) {
         PLWARNING((warn_msg + "Option nstages (set to " + tostring(nstages)
@@ -1181,6 +1184,9 @@
     if (!train_stats)
         train_stats = new VecStatsCollector();
 
+    // Set names of train_stats costs
+    train_stats->setFieldNames(getTrainCostNames());
+
     // Everything is fine.
     return true;
 }
@@ -1274,7 +1280,7 @@
     use(inputs,outputs);
     return outputs;
 }
-    
+
 //! Version of computeOutputAndCosts that's called by RMI
 
 tuple<Vec,Vec> PLearner::remote_computeOutputAndCosts(const Vec& input, const Vec& target) const



From lamblin at mail.berlios.de  Sat Jul 14 10:37:22 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Sat, 14 Jul 2007 10:37:22 +0200
Subject: [Plearn-commits] r7773 - in trunk/plearn_learners: generic
	regressors
Message-ID: <200707140837.l6E8bMVK008475@sheep.berlios.de>

Author: lamblin
Date: 2007-07-14 10:37:22 +0200 (Sat, 14 Jul 2007)
New Revision: 7773

Modified:
   trunk/plearn_learners/generic/PLearner.cc
   trunk/plearn_learners/regressors/LinearRegressor.cc
Log:
Initialize field names of stats collectors


Modified: trunk/plearn_learners/generic/PLearner.cc
===================================================================
--- trunk/plearn_learners/generic/PLearner.cc	2007-07-14 03:31:34 UTC (rev 7772)
+++ trunk/plearn_learners/generic/PLearner.cc	2007-07-14 08:37:22 UTC (rev 7773)
@@ -567,7 +567,10 @@
 
 
 void PLearner::setTrainStatsCollector(PP<VecStatsCollector> statscol)
-{ train_stats = statscol; }
+{
+    train_stats = statscol;
+    train_stats->setFieldNames(getTrainCostNames());
+}
 
 
 int PLearner::inputsize() const

Modified: trunk/plearn_learners/regressors/LinearRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/LinearRegressor.cc	2007-07-14 03:31:34 UTC (rev 7772)
+++ trunk/plearn_learners/regressors/LinearRegressor.cc	2007-07-14 08:37:22 UTC (rev 7773)
@@ -220,8 +220,8 @@
     resetAccumulators();
     resid_variance.resize(0);
 }
-  
 
+
 void LinearRegressor::train()
 {
     if(targetsize()<=0)
@@ -245,7 +245,8 @@
     if(!train_stats)  // make a default stats collector, in case there's none
         train_stats = new VecStatsCollector();
 
-    train_stats->forget(); 
+    train_stats->setFieldNames(getTrainCostNames());
+    train_stats->forget();
 
     // Compute training inputs and targets; take into account optional bias
     real squared_error=0;



From lamblin at mail.berlios.de  Sat Jul 14 10:40:15 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Sat, 14 Jul 2007 10:40:15 +0200
Subject: [Plearn-commits] r7774 - trunk/python_modules/plearn/learners/autolr
Message-ID: <200707140840.l6E8eFTQ008544@sheep.berlios.de>

Author: lamblin
Date: 2007-07-14 10:40:15 +0200 (Sat, 14 Jul 2007)
New Revision: 7774

Modified:
   trunk/python_modules/plearn/learners/autolr/__init__.py
Log:
Also report approximate training error


Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-07-14 08:37:22 UTC (rev 7773)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-07-14 08:40:15 UTC (rev 7774)
@@ -8,18 +8,23 @@
 
 
 if plearn.bridgemode.useserver:
-    servers_lock = Lock() # Lock on the servers list so we don't have race conditions
-    servers = [[serv, 0, Lock()]] # List of [server, amount_of_jobs_server_is_running, lock_on_the_server] lists
-    servers_max = 1e10 # Default maximal number of servers we are willing to run
+    # Lock on the servers list so we don't have race conditions
+    servers_lock = Lock()
+    # List of [server, amount_of_jobs_server_is_running, lock_on_the_server]
+    # lists
+    servers = [[serv, 0, Lock()]]
+    # Default maximal number of servers we are willing to run
+    servers_max = 1e10
 
 def execute(object, tasks, use_threads = False):
     def job(object):
         lock = [servinfo[2] for servinfo in servers if servinfo[0] is object.server][0]
-        lock.acquire() # we will only send our job to the server if nothing is already running
+        # we will only send our job to the server if nothing is already running
+        lock.acquire()
         for method, args in tasks: # do each task sequentially
             getattr(object, method)(*args)
         lock.release()
-        
+
     if plearn.bridgemode.useserver and use_threads:
         t = Thread(target = job, args = (object,))
         return t
@@ -47,7 +52,9 @@
     if nservers < servers_max:
         command = plearn.bridgemode.server_exe + ' server'
         server = launch_plearn_server(command = command)
-        time.sleep(0.5) # give some time to the server to get born and well alive (taken from bridge.py)
+        # give some time to the server to get born and well alive (taken from
+        # bridge.py)
+        time.sleep(0.5)
         lock = Lock()
         servers.append([server, 1, lock])
     else:
@@ -64,7 +71,8 @@
         for servinfo in servers:
             server_, njobs, lock = servinfo
             if server_ is server: # == doesn't work
-                servinfo[1] -= 1 # this server is done so we reduce its job count
+                # this server is done so we reduce its job count
+                servinfo[1] -= 1
                 break
         servers_lock.release()
 
@@ -84,15 +92,17 @@
     with one column per schedule but a unified sequence of stages.
     Each schedule is a Nx2 array, with the first column containing number of
     stages (examples) and the second column containing corresponding learning
-    rates. N may vary across schedules. If we have a schedule first row with [10000 0.01]
-    and a second row with [30000 0.001], it means that from the learner's current stage
-    to stage 10000 (excluded) we should use a learning rate of 0.01, and from stage 10000
-    to 30000 (excluded) we should use a learning rate of 0.001. The different
-    schedules do not have to have the same N nor the same maximum stage. They
-    will be merged in one big schedule ranging from the mininum to the maximum
-    of the stages found in all the schedules.
+    rates. N may vary across schedules. If we have a schedule first row with
+    [10000 0.01] and a second row with [30000 0.001], it means that from the
+    learner's current stage to stage 10000 (excluded) we should use a learning
+    rate of 0.01, and from stage 10000 to 30000 (excluded) we should use a
+    learning rate of 0.001. The different schedules do not have to have the
+    same N nor the same maximum stage. They will be merged in one big schedule
+    ranging from the mininum to the maximum of the stages found in all the
+    schedules.
     The result is an array with the stages in the first column and one
-    additional column (with a sequence of learning rates) for each input schedule.
+    additional column (with a sequence of learning rates) for each input
+    schedule.
     """
     n_schedules = len(schedules)
     stages = []
@@ -121,14 +131,14 @@
     res[:,0]=stages
     res[:,1:]=learning_rates
     return res
-    
 
+
 def train_with_schedule(learner,
                         lr_options,
                         schedules,
-                        trainset,testsets,expdir,
+                        trainset, testsets, expdir,
                         cost_to_select_best=0,
-                        selected_costnames = False,
+                        selected_costnames = None,
                         logfile=None):
     """Train a learner with one or more schedules of learning rates.
 lr_options is a list of list of option strings. Each list in lr_options
@@ -140,19 +150,26 @@
 column and sequences of learning rates (one sequence per group) in
 each of the other columns (just like the result of the call to merge_schedules).
 """
-    costnames = learner.getTestCostNames()
-    if not selected_costnames:
-        # use all cost names if not user-provided
-        selected_costnames=costnames
-    cost_indices = [costnames.index(name) for name in selected_costnames]
+    train_costnames = learner.getTrainCostNames()
+    test_costnames = learner.getTestCostNames()
+
+    # Filter out unwanted costnames
+    if selected_costnames is not None:
+        train_costnames = [ name for name in train_costnames
+                            if name in selected_costnames ]
+        test_costnames = [ name for name in test_costnames
+                           if name in selected_costnames ]
+
+    n_train_costs = len(train_costnames)
+    n_test_costs = len(test_costnames)
+
     learner.setTrainingSet(trainset,False)
     stages = schedules[:,0]
     learning_rates = schedules[:,1:]
     n_train = len(stages)
     n_schedules = len(lr_options)
     n_tests = len(testsets)
-    n_costs = len(costnames)
-    results = zeros([n_train,2+n_tests*n_costs],Float)
+    results = zeros([n_train, 2 + n_train_costs + n_tests*n_test_costs], Float)
     best_err = 1e10
     if plearn.bridgemode.interactive:
         clf()
@@ -169,32 +186,60 @@
         results[i,0] = learner.stage
         for s in range(n_schedules):
             results[i,1+s] = learning_rates[i][s]
-        for j in range(0,n_tests):
+
+        # Report approximate training error
+        train_vsc = learner.getTrainStatsCollector()
+        if train_vsc.fieldnames == []:
+            # learner did not set train_stats fieldnames
+            train_vsc.fieldnames = learner.getTrainCostNames()
+        if logfile:
+            print >>logfile, "At stage ", learner.stage, " train :",
+        for k, costname in zip(range(n_train_costs), train_costnames):
+            err = train_vsc.getStat('E['+costname+']')
+            results[i, k+1+n_schedules] = err
+            if logfile:
+                print >>logfile, costname, '=', err,
+            if plearn.bridgemode.interactive:
+                plot(   results[0:i+1, 0],
+                        results[0:i+1, 1+n_schedules+k],
+                        colors[k%7]+styles[0],
+                        label='train:'+costname)
+        if logfile:
+            print >>logfile
+            logfile.flush()
+
+        # Report error on test sets
+        for j in range(n_tests):
             ts = pl.VecStatsCollector()
             if plearn.bridgemode.useserver:
                 ts=serv.new(ts)
             learner.test(testsets[j],ts,0,0)
             if logfile:
-                print >>logfile, "At stage ",learner.stage," test" + str(j+1),": ",
-            for k in range(0,n_costs):
-                err = ts.getStat("E["+str(k)+"]")
-                results[i,j*n_costs+k+1+n_schedules]=err
-                costname = costnames[cost_indices[k]]
+                print >>logfile, "At stage ", learner.stage, " test" + str(j+1),": ",
+            for k, costname in zip(range(n_test_costs), test_costnames):
+                err = ts.getStat("E["+costname+"]")
+                results[i, 1+n_schedules+n_train_costs+(j*n_test_costs)+k] = err
                 if logfile:
                     print >>logfile, costname, "=", err,
                 if k==cost_to_select_best and j==0 and err < best_err:
                     best_err = err
                     learner.save(expdir+"/"+"best_learner.psave","plearn_ascii")
                 if plearn.bridgemode.interactive:
-                    plot(results[0:i+1,0],results[0:i+1,
-                         j*n_costs+k+1+n_schedules],colors[k%7]+styles[j%15],
-                         label='test'+str(j+1)+':'+costname)
+                    plot(results[0:i+1,0],
+                            results[0:i+1, 1+n_schedules+n_train_costs+(j*n_test_costs)+k],
+                            colors[k%7]+styles[(j+1)%15],
+                            label='test'+str(j+1)+':'+costname)
             if logfile:
                 print >>logfile
                 logfile.flush()
         if plearn.bridgemode.interactive and i==0:
             legend()
-    return (['stage']+selected_costnames,results)
+    # Return headers for the result matrix, and the results themselves
+    return (['stage', 'learning_rate']
+            + ['train.' + costname for costname in train_costnames]
+            + ['test'+str(j+1)+'.'+costname for j in range(n_tests)
+                                            for costname in test_costnames],
+            results)
 
 
 def choose_initial_lr(initial_learner,trainset,testset,lr_options,
@@ -205,10 +250,11 @@
                       lr_steps=exp(log(10)/2),
                       logfile=None):
     """
-Optimize initial learning rate by exploring greedily from a given initial learning rate
-If call_forget then the provided initial_learner is changed (and not necessarily the
-optimal one) upon return. But if not call_forget then the initial_learner is unchanged
-(we make deep copies internally).
+Optimize initial learning rate by exploring greedily from a given initial
+learning rate.
+If call_forget then the provided initial_learner is changed (and not
+necessarily the optimal one) upon return. But if not call_forget then the
+initial_learner is unchanged (we make deep copies internally).
 """
     log_initial_lr=log(initial_lr)
     log_steps=log(lr_steps)
@@ -225,7 +271,7 @@
     def perf(i):
         global best_err
         global best_learner
-        
+
         if call_forget:
             learner = initial_learner
         else:
@@ -259,7 +305,7 @@
         best=bottom
     else:
         best=top
-    
+
     while top-bottom<2 or (best==top or best==bottom):
         if best==top:
             top+=1
@@ -275,40 +321,59 @@
 
 def train_adapting_lr(learner,
                       trainset,testsets,expdir,
-                      lr_options, # List of lists of options (one list/group of options with a given schedule)
-                      optimized_group=0, # Group of options that is actually optimized (if -1, then no optimization is performed)
-                      schedules=None, # Matrix of schedules (number of columns = 1 (stage) + number of groups)
-                      nstages=None, # Used to construct default schedule if 'schedules' is None
+                      # List of lists of options (one list/group of options
+                      # with a given schedule)
+                      lr_options,
+                      # Group of options that is actually optimized
+                      # (if -1, then no optimization is performed)
+                      optimized_group=0,
+                      # Matrix of schedules
+                      # (number of columns = 1 (stage) + number of groups)
+                      schedules=None,
+                      # Used to construct default schedule
+                      # if 'schedules' is None
+                      nstages=None,
                       epoch=None,   # ""
-                      initial_lr=0.1, # Starting value for group being optimized
-                      nskip=2,   # Number of epochs after which we add/remove candidates
-                      cost_to_select_best=0, # Index of cost being optimized
-                      return_best_model=False, # o/w return final model
-                      save_best=False, # for paranoids: save best model every save_best epochs, or not at all (if = False)
-                      selected_costnames = False,
+                      # Starting value for group being optimized
+                      initial_lr=0.1,
+                      # Number of epochs after which we add/remove candidates
+                      nskip=2,
+                      # Index of cost being optimized
+                      cost_to_select_best=0,
+                      # o/w return final model
+                      return_best_model=False,
+                      # for paranoids: save best model every save_best epochs,
+                      # or not at all (if = False)
+                      save_best=False,
+                      selected_costnames = None,
                       min_epochs_to_delete = 2,
-                      lr_steps=exp(log(10)/2), # Scaling coefficient when modifying learning rates
+                      # Scaling coefficient when modifying learning rates
+                      lr_steps=exp(log(10)/2),
                       logfile=False,
-                      min_lr=1e-6, # do not try to go below this learning rate
-                      keep_lr=2, # Learning rate interval for heuristic
+                      # do not try to go below this learning rate
+                      min_lr=1e-6,
+                      # Learning rate interval for heuristic
+                      keep_lr=2,
                       use_threads=False):
 
     if plearn.bridgemode.useserver:
         servers[0][1] = 1 # learner
 
-    min_epochs_to_delete = max(1,min_epochs_to_delete) # although 1 is probably too small
+    # although 1 is probably too small
+    min_epochs_to_delete = max(1, min_epochs_to_delete)
 
     def error_curve(active,start_t,current_t):
         delta_t = current_t+1-start_t
-        # in all_results, column 0 is stage, column 1 is option value (learning rate)
-        # and column 2+test*n_costs+cost is the cost value for cost number 'cost'
-        # (index in the selected_costnames list), in testset 'test'.
+        # in all_results, column 0 is stage, column 1 is option value
+        # (learning rate) and column 2+test*n_costs+cost is the cost value for
+        # cost number 'cost' (index in the selected_costnames list),
+        # in testset 'test'.
         # And testset 0 is the one used for selection.
         return all_results[active][start_t:current_t+1,2+cost_to_select_best]
-        
+
     def error_curve_dominates(c1,c2,t):
-        """curve1 has a lower last error than curve2, but
-           will curve2 eventually cross curve1? if yes return False o/w return True"""
+        """curve1 has a lower last error than curve2, but will curve2
+        eventually cross curve1? if yes return False o/w return True"""
 
         start_t = max(all_start[c1],all_start[c2])
         curve1 = error_curve(c1,start_t,t)
@@ -319,29 +384,42 @@
         slope2=curve2[-1]-curve2[-2]
         if  slope1 >= slope2:
             return False
-        #check to see if c2 is alone with its learning rate (or nearby); if yes keep it
+
+        # check to see if c2 is alone with its learning rate (or nearby);
+        # if yes keep it
         alone=True
         c2lr=all_lr[c2]
         c2err=all_last_err[c2]
         for a in actives:
             if all_lr[a]==c2lr and all_last_err[a]<=c2err:
-                return True # throw it away if worse than other actives of same lr
-            # say that it is alone if there are no other actives with nearby and greater lr
+                # throw it away if worse than other actives of same lr
+                return True
+
+            # say that it is alone if there are no other actives with nearby
+            # and greater lr
             if a!=c2 and all_lr[a]>c2lr and abs(log(all_lr[a]/c2lr))<keep_lr*log(lr_steps):
                 alone=False
         c1lr=all_lr[c1]
-        if alone and c2lr>c1lr: # and slope2<0: 
+        if alone and c2lr>c1lr: # and slope2<0:
             # keep if alone and a larger learning rate and improving
             return False
         return True
-    
-    costnames = learner.getTestCostNames()
-    if not selected_costnames:
-        # use all cost names if not user-provided
-        selected_costnames=costnames
-    cost_indices = [costnames.index(name) for name in selected_costnames]
+
+    train_costnames = learner.getTestCostNames()
+    test_costnames = learner.getTestCostNames()
+    if selected_costnames is not None:
+        # Filter out unwanted costnames
+        train_costnames = [ name for name in train_costnames
+                            if name in selected_costnames ]
+        test_costnames = [ name for name in test_costnames
+                           if name in selected_costnames ]
+
+    #cost_indices = [costnames.index(name) for name in selected_costnames]
     n_tests = len(testsets)
-    n_costs = len(cost_indices)
+    #n_costs = len(cost_indices)
+    n_train_costs = len(train_costnames)
+    n_test_costs = len(test_costnames)
+
     if schedules:
         stages = schedules[:,0]
         learning_rates = schedules[:,1:]
@@ -358,13 +436,14 @@
             raise Error
         if len(lr_options)!=1:
             lr_options=[lr_options[0]]
+
     n_train = len(stages)
     n_schedules = len(lr_options)
     assert n_schedules==learning_rates.shape[1]
     best_err = 1e10
     previous_best_err = best_err
     best_active = -1
-    all_results = [1e10*ones([n_train,2+n_tests*n_costs],Float)]
+    all_results = [1e10*ones([n_train,2+n_train_costs+n_tests*n_test_costs],Float)]
     all_candidates = [learner]
     all_last_err = [best_err]
     all_lr = [initial_lr]
@@ -398,12 +477,17 @@
                         options[lr_option]=str(learning_rates[t,s])
             candidate.changeOptions(options)
             candidate.setTrainingSet(trainset,False)
+            train_vsc = pl.VecStatsCollector();
+            if plearn.bridgemode.useserver:
+                train_vsc = candidate.server.new(train_vsc)
+            candidate.setTrainStatsCollector(train_vsc)
             tasks = [('train', ())]
             stats = []
             for j in range(0,n_tests):
                 ts = pl.VecStatsCollector()
                 if plearn.bridgemode.useserver:
-                    ts = candidate.server.new(ts) # no threads are running so we don't need to lock here
+                    # no threads are running so we don't need to lock here
+                    ts = candidate.server.new(ts)
                 stats.append(ts)
                 tasks.append(('test', (testsets[j], ts, 0, 0)))
             active_stats.append(stats)
@@ -424,18 +508,28 @@
         for active, stats in zip(actives, active_stats):
             candidate = all_candidates[active]
             results = all_results[active]
-        
+
             results[t,0] = candidate.stage
             results[t,1] = all_lr[active]
             if logfile:
                 print >>logfile, "candidate ",active,":",
+
+            # Report approximate training statistics
+            if logfile:
+                print >>logfile, 'train :',
+            for k, costname in zip(range(n_train_costs), train_costnames):
+                err = ts.getStat('E['+costname+']')
+                results[t, 2+k] = err
+                if logfile:
+                    print >>logfile, costname, '=', err,
+
+            # Report testing statistics
             for j, ts in zip(range(0,n_tests), stats):
                 if logfile:
                     print >>logfile, " test" + str(j+1),": ",
-                for k in range(0,n_costs):
-                    err = ts.getStat("E["+str(cost_indices[k])+"]")
-                    results[t,j*n_costs+k+2]=err
-                    costname = costnames[cost_indices[k]]
+                for k, costname in zip(range(n_test_costs), test_costnames):
+                    err = ts.getStat("E["+costname+"]")
+                    results[t, 2+n_train_costs+(j*n_test_costs)+k] = err
                     if logfile:
                         print >>logfile, costname, "=", err,
                     if k==cost_to_select_best and j==0:
@@ -443,12 +537,14 @@
                             start = all_start[active]
                             if start==t:
                                 plot(results[start:t+1,0],
-                                     results[start:t+1,j*n_costs+k+2],colors[active%7]+styles[j%15],
+                                     results[start:t+1, 2+n_train_costs+(j*n_test_costs)+k],
+                                     colors[active%7]+styles[j%15],
                                      label='candidate'+str(active)+':'+costname)
-                                legend() 
+                                legend()
                             else:
                                 plot(results[start:t+1,0],
-                                     results[start:t+1,j*n_costs+k+2],colors[active%7]+styles[j%15])
+                                     results[start:t+1, 2+n_train_costs+(j*n_test_costs)+k],
+                                     colors[active%7]+styles[j%15])
 
                         all_last_err[active]=err
                         if err < best_err:
@@ -456,7 +552,8 @@
                             best_active = active
                             best_early_stop = stage
                             if return_best_model:
-                                best_candidate = deepcopy(candidate, use_threads)
+                                best_candidate = deepcopy(candidate,
+                                                          use_threads)
             if logfile:
                 print >>logfile
                 logfile.flush()
@@ -467,16 +564,18 @@
             if logfile:
                 print >>logfile,"BEST to now is candidate ",best_active," with err=",best_err
                 print >>logfile, "stage\tl.rate\t",
-                for cost_index in cost_indices:
-                    costname = costnames[cost_index]
-                    print >>logfile, costname+"\t",
+                for costname in train_costnames:
+                    print >>logfile, 'train.'+costname+"\t",
                 print >>logfile
+                for costname in test_costnames:
+                    print >>logfile, 'test.'+costname+'\t',
+                print >>logfile
                 for row in all_results[best_active][0:t+1,:]:
                     for val in row:
                         print >>logfile,val,"\t",
                     print >>logfile
                 print >>logfile
-                
+
         else:
             if logfile:
                 print >>logfile, "THE BEST ACTIVE HAS GOTTEN WORSE!!!!"
@@ -490,14 +589,18 @@
                     if logfile:
                         print >>logfile,"REMOVE candidate ",a
                     release_server(all_candidates[a], use_threads)
-                    all_candidates[a]=None # hopefully this destroys the candidate
+                    # hopefully this destroys the candidate
+                    all_candidates[a]=None
                     del actives[j-ndeleted]
                     ndeleted+=1
-            # add a candidate with slightly lower learning rate than best_active, starting from it
-            new_lr=all_lr[best_active]/lr_steps # only try a smaller learning rate
+            # add a candidate with slightly lower learning rate than
+            # best_active, starting from it
+            # only try a smaller learning rate
+            new_lr=all_lr[best_active]/lr_steps
             if new_lr>=min_lr:
                 all_lr.append(new_lr)
-                new_candidate = deepcopy(all_candidates[best_active], use_threads)
+                new_candidate = deepcopy(all_candidates[best_active],
+                                         use_threads)
                 new_a = len(all_candidates)
                 actives.append(new_a)
                 all_candidates.append(new_candidate)
@@ -515,11 +618,21 @@
         schedules[:,1+optimized_group]=all_results[best_active][:,1]
     if logfile and best_err < all_last_err[best_active]:
         print >>logfile, "WARNING: best performing model would have stopped early at stage ",best_early_stop
-    return (final_model,   # Learner
-            schedules,     # Matrix of schedules (including the one that was optimized)
-            all_results[best_active], # Error curve (matrix) for best model
-            all_results, # List of all error curve matrices
-            all_last_err, # List of all last errors recorded for each candidate (not necessarily at last epoch)
-            all_start,  # Epoch index where each candidate was created (not necessarily the first epoch)
-            best_early_stop) # Timestep (in stages) at which early-stopping should have happened (i.e. stage of best error found)
+    return (# Learner
+            final_model,
+            # Matrix of schedules (including the one that was optimized)
+            schedules,
+            # Error curve (matrix) for best model
+            all_results[best_active],
+            # List of all error curve matrices
+            all_results,
+            # List of all last errors recorded for each candidate
+            # (not necessarily at last epoch)
+            all_last_err,
+            # Epoch index where each candidate was created (not necessarily
+            # the first epoch)
+            all_start,
+            # Timestep (in stages) at which early-stopping should have happened
+            # (i.e. stage of best error found)
+            best_early_stop)
 



From dorionc at mail.berlios.de  Sun Jul 15 07:34:41 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Sun, 15 Jul 2007 07:34:41 +0200
Subject: [Plearn-commits] r7775 - trunk/python_modules/plearn/pytest
Message-ID: <200707150534.l6F5Yfhm028480@sheep.berlios.de>

Author: dorionc
Date: 2007-07-15 07:34:40 +0200 (Sun, 15 Jul 2007)
New Revision: 7775

Modified:
   trunk/python_modules/plearn/pytest/programs.py
Log:
Sorry for the mistake...

Modified: trunk/python_modules/plearn/pytest/programs.py
===================================================================
--- trunk/python_modules/plearn/pytest/programs.py	2007-07-14 08:40:15 UTC (rev 7774)
+++ trunk/python_modules/plearn/pytest/programs.py	2007-07-15 05:34:40 UTC (rev 7775)
@@ -365,7 +365,7 @@
             # Otherwise assumed to be non-compilable
             else:
                 self.__is_compilable = False
-                for dep in dependencies:
+                for dep in self.dependencies:
                     if dep.isCompilable():
                         self.__is_compilable = True
                         break



From yoshua at mail.berlios.de  Sun Jul 15 16:30:13 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Sun, 15 Jul 2007 16:30:13 +0200
Subject: [Plearn-commits] r7776 - trunk/plearn_learners/online/EXPERIMENTAL
Message-ID: <200707151430.l6FEUDSD002004@sheep.berlios.de>

Author: yoshua
Date: 2007-07-15 16:30:13 +0200 (Sun, 15 Jul 2007)
New Revision: 7776

Modified:
   trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
Log:
Corrections to scaling of gradient and KLp0p1


Modified: trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-15 05:34:40 UTC (rev 7775)
+++ trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-15 14:30:13 UTC (rev 7776)
@@ -56,7 +56,7 @@
     "The exact and very inefficient implementation of this criterion is done here."
     "  KL(p0||p1) = sum_x p0(x) log p0(x)/p1(x) = - sum_i (1/n) log p1(x_i) + sum_i (1/n) log(1/n)"
     "For an example x the cost is:"
-    "  C(x) = - log p1(x) -(1/n) log n = - log (1/n)sum_{k=1}^n sum_h P(x|h) P(h|x^k) -(1/n)log n"
+    "  C(x) = - log p1(x) - log n = - log sum_{k=1}^n sum_h P(x|h) P(h|x^k)"
     "where {x^1, ... x^n} is the training set of examples x^k, h is a hidden layer bit vector,"
     "P(x|h) is the hidden-to-visible conditional distribution and P(h|x) is the"
     "input-to-hidden conditional distribution. Both are the usual found in Binomial"
@@ -915,9 +915,11 @@
 
         for (int c=0;c<n_configurations;c++)
         {
-            //  C(x) =  -log p1(x) -(1/n)logn
-            //       =  (1-1/n)log n - log sum_{k=1}^n sum_h P(x|h) P(h|x^k)
-            //       =  (1-1/n)log n - log sum_h P(x|h) (sum_k P(h|x^k))/n
+            // KL(p0|p1) = sum_t (1/n) log ((1/n) / p1(x_t)) = (1/n) sum_t C(x_t)
+            //  p1(x) = sum_k (1/n) sum_h P(x|h) P(h|x_k)
+            //  C(x) =  -log p1(x) - log n
+            //       =  log n - log sum_{k=1}^n sum_h P(x|h) P(h|x^k)  - log n
+            //       =  - log sum_h P(x|h) sum_k P(h|x^k)
 
             real log_sum_ph_given_xk = 0;
             Vec h = conf_hidden_layer->samples(c);
@@ -947,8 +949,6 @@
                     (*KLp0p1)(t,0) = logadd((*KLp0p1)(t,0), -conf_visible_layer->fpropNLL((*visible)(t)) + log_sum_ph_given_xk);
         }
         *KLp0p1 *= -1;
-        *KLp0p1 += logn*((real)1.0 - (real)1.0/(real)n);
-        // going in the other direction just for the fun of it:
         hidden_layer->setBatchSize(mbs);
         visible_layer->setBatchSize(mbs);
     }
@@ -1648,8 +1648,9 @@
         //   * x^t for every t in the input visible, in *visible
         //   * -log P1(x^t) for each input visible(t) in KLp0p1(t,0)
         //
+        // Since C(x) = - log sum_h P(x|h) sum_k P(h|x^k), dC/dsum = -1/sum = -1/exp(-C)=-exp(C)
         // We want to compute
-        //   dC(x)/dWij = (-1/(n P1(x))) 
+        //   dC(x)/dWij = (-exp(C(x)))
         //       sum_{k=1}^n sum_h P(x|h) P(h|x^k) (h_i(x_j - P(x_j=1|h)) + x_j^k(h_i - P(h_i=1|x^k)))
         //
         PLASSERT_MSG(KLp0p1 && !KLp0p1->isEmpty(), "Must compute KLp0p1 in order to compute its gradient, connect that port!");
@@ -1681,9 +1682,8 @@
                 {
                     Vec h = conf_hidden_layer->samples(c);
                     Vec avisible_given_h=conf_visible_layer->activations(c);
-                    // KLp0p1(x) = -log p1(x) -(1/n)logn
-                    real lp = -(*KLp0p1)(t,0) - (1+1/(real)n)*logn; // lp = log (1/(n P1(x^t)))
-                    // compute and multiply by P(h|x^k)
+                    real lp = (*KLp0p1)(t,0); // lp = log (exp(C(x^t)))
+                    // compute and multiply exp(lp) by P(h|x^k)
                     for (int i=0;i<hidden_layer->size;i++)
                     {
                         real act=ah_given_xk[i];
@@ -1692,7 +1692,7 @@
                         // so h*log(sigmoid(act))+(1-h)*log(sigmoid(act)) = act*h-softplus(act)
                         lp += h[i]*act-softplus(act);
                     }
-                    // now lp = log ( (1/(n P1(x^t))) P(h|x^k) )
+                    // now lp = log ( exp(C(x^t)) P(h|x^k) )
 
                     // compute and multiply by P(x^t|h)
                     for (int j=0;j<visible_layer->size;j++)
@@ -1700,7 +1700,7 @@
                         real act=avisible_given_h[j];
                         lp += act*xt[j] - softplus(act);
                     }
-                    // now lp = log ( (1/(n P1(x^t))) P(h|x^k)  P(x^t|h) )
+                    // now lp = log ( exp(C(x^t)) P(h|x^k)  P(x^t|h) )
                     real coeff = exp(lp);
                     Vec pvisible_given_h=pvisible_given_H(c);
                     for (int j=0;j<visible_layer->size;j++)



From yoshua at mail.berlios.de  Sun Jul 15 16:48:14 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Sun, 15 Jul 2007 16:48:14 +0200
Subject: [Plearn-commits] r7777 - trunk/plearn_learners/online/EXPERIMENTAL
Message-ID: <200707151448.l6FEmEcd002702@sheep.berlios.de>

Author: yoshua
Date: 2007-07-15 16:48:14 +0200 (Sun, 15 Jul 2007)
New Revision: 7777

Modified:
   trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
Log:
Adding warning for negative KLp0p1


Modified: trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-15 14:30:13 UTC (rev 7776)
+++ trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-15 14:48:14 UTC (rev 7777)
@@ -949,6 +949,9 @@
                     (*KLp0p1)(t,0) = logadd((*KLp0p1)(t,0), -conf_visible_layer->fpropNLL((*visible)(t)) + log_sum_ph_given_xk);
         }
         *KLp0p1 *= -1;
+        for (int t=0;t<mbs;t++)
+            if ((*KLp0p1)(t,0) < 0)
+                PLWARNING("KLp0p1: training example %d is getting mass > 1/n!",t);
         hidden_layer->setBatchSize(mbs);
         visible_layer->setBatchSize(mbs);
     }



From yoshua at mail.berlios.de  Sun Jul 15 17:57:16 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Sun, 15 Jul 2007 17:57:16 +0200
Subject: [Plearn-commits] r7778 - trunk/plearn_learners/online/EXPERIMENTAL
Message-ID: <200707151557.l6FFvGiA005808@sheep.berlios.de>

Author: yoshua
Date: 2007-07-15 17:57:15 +0200 (Sun, 15 Jul 2007)
New Revision: 7778

Modified:
   trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
Log:
fixed bug due to non-sharing of biases in KLp0p1


Modified: trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-15 14:48:14 UTC (rev 7777)
+++ trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-15 15:57:15 UTC (rev 7778)
@@ -238,9 +238,15 @@
     if(visible_layer)
         visible_bias_grad.resize(visible_layer->size);
 
+    // copy layers to allow different storage of activations and samples
+    // but keep the same parameters 
     conf_visible_layer = PLearn::deepCopy(visible_layer);
+    // (this pointing of bias would not suffice with RBMGaussianLayer, which has other params)
+    conf_visible_layer->bias = visible_layer->bias;
     conf_hidden_layer = PLearn::deepCopy(hidden_layer);
+    conf_hidden_layer->bias = hidden_layer->bias;
 
+
     // Forward random generator to underlying modules.
     if (random_gen) {
         if (hidden_layer && !hidden_layer->random_gen) {
@@ -943,15 +949,18 @@
             // now log_sum_ph_given_xk = log sum_k P(h|x^k)
             conf_visible_layer->activation << conf_visible_layer->activations(c);
             for (int t=0;t<mbs;t++)
-                if (c==0)
+                if (c==0) // at this point we accumulate log sum_h P(x_t|h) P(h|x_k) in KLp0p1
                     (*KLp0p1)(t,0) = -conf_visible_layer->fpropNLL((*visible)(t)) + log_sum_ph_given_xk;
-                else
+                else {
                     (*KLp0p1)(t,0) = logadd((*KLp0p1)(t,0), -conf_visible_layer->fpropNLL((*visible)(t)) + log_sum_ph_given_xk);
+                    //if ((*KLp0p1)(t,0) > 0)
+                    // PLWARNING("KLp0p1: training example %d is getting mass > 1/n! KL=%g after getting to configuration %d",t,(double)(*KLp0p1)(t,0),c);
+                }
         }
         *KLp0p1 *= -1;
-        for (int t=0;t<mbs;t++)
-            if ((*KLp0p1)(t,0) < 0)
-                PLWARNING("KLp0p1: training example %d is getting mass > 1/n!",t);
+        //for (int t=0;t<mbs;t++)
+        //    if ((*KLp0p1)(t,0) < 0)
+        //        PLWARNING("KLp0p1: training example %d is getting mass > 1/n! KL=%g",t,(double)(*KLp0p1)(t,0));
         hidden_layer->setBatchSize(mbs);
         visible_layer->setBatchSize(mbs);
     }



From yoshua at mail.berlios.de  Mon Jul 16 03:31:39 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Mon, 16 Jul 2007 03:31:39 +0200
Subject: [Plearn-commits] r7779 - trunk/plearn_learners/online/EXPERIMENTAL
Message-ID: <200707160131.l6G1VduG003255@sheep.berlios.de>

Author: yoshua
Date: 2007-07-16 03:31:38 +0200 (Mon, 16 Jul 2007)
New Revision: 7779

Modified:
   trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
Log:
still getting negative KL and total probability mass > 1...


Modified: trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-15 15:57:15 UTC (rev 7778)
+++ trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-16 01:31:38 UTC (rev 7779)
@@ -919,6 +919,8 @@
         connection->setAsUpInputs(conf_hidden_layer->samples);
         conf_visible_layer->getAllActivations(connection,0,true);
 
+        //Vec check_sum_to_one(n);
+
         for (int c=0;c<n_configurations;c++)
         {
             // KL(p0|p1) = sum_t (1/n) log ((1/n) / p1(x_t)) = (1/n) sum_t C(x_t)
@@ -940,6 +942,12 @@
                     // and  h log(sigm(act))+(1-h)log(1-sigm(act)) = act*h-softplus(act)
                     lp += h[i]*act-softplus(act); 
                 }
+#if 0
+                if (c==0)
+                    check_sum_to_one[k]=lp;
+                else
+                    check_sum_to_one[k]=logadd(check_sum_to_one[k],lp);
+#endif
                 // now lp = log P(h|x^k)
                 if (k==0)
                     log_sum_ph_given_xk = lp;
@@ -948,19 +956,45 @@
             }
             // now log_sum_ph_given_xk = log sum_k P(h|x^k)
             conf_visible_layer->activation << conf_visible_layer->activations(c);
+            real log_sum_p_xt = 0;
             for (int t=0;t<mbs;t++)
-                if (c==0) // at this point we accumulate log sum_h P(x_t|h) P(h|x_k) in KLp0p1
-                    (*KLp0p1)(t,0) = -conf_visible_layer->fpropNLL((*visible)(t)) + log_sum_ph_given_xk;
+            {
+                real log_p_xt = -conf_visible_layer->fpropNLL((*visible)(t));
+                //if (t==0) // check if sum_xt p(xt|h) = 1 (when testing with the full set of possible inputs)
+                //    log_sum_p_xt = log_p_xt;
+                //else
+                //    log_sum_p_xt = logadd(log_sum_p_xt,log_p_xt);
+                if (c==0) // at this point we accumulate log sum_h P(x_t|h) sum_k P(h|x_k) in KLp0p1
+                    (*KLp0p1)(t,0) = log_p_xt + log_sum_ph_given_xk;
                 else {
-                    (*KLp0p1)(t,0) = logadd((*KLp0p1)(t,0), -conf_visible_layer->fpropNLL((*visible)(t)) + log_sum_ph_given_xk);
+                    (*KLp0p1)(t,0) = logadd((*KLp0p1)(t,0), log_p_xt + log_sum_ph_given_xk);
                     //if ((*KLp0p1)(t,0) > 0)
                     // PLWARNING("KLp0p1: training example %d is getting mass > 1/n! KL=%g after getting to configuration %d",t,(double)(*KLp0p1)(t,0),c);
                 }
+            }
+            //if (!during_training)
+            //    cout << "sum_t(p(x_t|h)) = " << exp(log_sum_p_xt) << endl;
         }
+#if 0 
+        for (int k=0;k<n;k++)
+        {
+            real p_k=exp(check_sum_to_one[k]);
+            if (fabs(p_k-1)>1e-6)
+                PLWARNING("Probabilities that do not sum to 1!");
+        }
+#endif
         *KLp0p1 *= -1;
-        //for (int t=0;t<mbs;t++)
-        //    if ((*KLp0p1)(t,0) < 0)
-        //        PLWARNING("KLp0p1: training example %d is getting mass > 1/n! KL=%g",t,(double)(*KLp0p1)(t,0));
+        if (!during_training)
+        {
+            real sum_pxt=0;
+            for (int t=0;t<mbs;t++)
+            {
+                sum_pxt += exp(-(*KLp0p1)(t,0) -logn);
+                if ((*KLp0p1)(t,0) < 0)
+                    PLWARNING("KLp0p1: training example %d is getting mass = %g > 1/n!",t,(double)exp(-(*KLp0p1)(t,0)-logn));
+            }
+            cout << "sum_t p1(x_t) = " << sum_pxt << endl;
+        }
         hidden_layer->setBatchSize(mbs);
         visible_layer->setBatchSize(mbs);
     }



From nouiz at mail.berlios.de  Mon Jul 16 15:28:08 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 16 Jul 2007 15:28:08 +0200
Subject: [Plearn-commits] r7780 - trunk/scripts
Message-ID: <200707161328.l6GDS86K009556@sheep.berlios.de>

Author: nouiz
Date: 2007-07-16 15:28:08 +0200 (Mon, 16 Jul 2007)
New Revision: 7780

Modified:
   trunk/scripts/cdispatch
Log:
Corrected example of requirement for condor


Modified: trunk/scripts/cdispatch
===================================================================
--- trunk/scripts/cdispatch	2007-07-16 01:31:38 UTC (rev 7779)
+++ trunk/scripts/cdispatch	2007-07-16 13:28:08 UTC (rev 7780)
@@ -73,9 +73,9 @@
 
 The optional parameter '--req="CONDOR_REQUIREMENT"' make that cdispatch send additional option to DBI that will be used to generate addtional requirement for condor. CONDOR_REQUIREMENT must follow the syntax of requirement for condor with one exception. The symbol '"' must be escaped 3 times! So the requirement (Machine == "computer.example.com") must be writed like that:
 
-cdispatch "--req=Machine\\\\\\"computer.example.com\\\\\\""
+cdispatch "--req=Machine==\\\\\\"computer.example.com\\\\\\""
 or
-cdispatch '--req=Machine\\"computer.example.com\\"'
+cdispatch '--req=Machine==\\"computer.example.com\\"'
 
 It the the optinal parameter '--file=FILEPATH' is set, the script will take commands from the file instead of taking one as parameter. This file must have one command to execute by line and each of them will be expended.
 



From nouiz at mail.berlios.de  Mon Jul 16 15:29:38 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 16 Jul 2007 15:29:38 +0200
Subject: [Plearn-commits] r7781 - trunk/scripts
Message-ID: <200707161329.l6GDTcJ1009732@sheep.berlios.de>

Author: nouiz
Date: 2007-07-16 15:29:38 +0200 (Mon, 16 Jul 2007)
New Revision: 7781

Modified:
   trunk/scripts/collectres
Log:
Correctly test if the number of parameter is valid


Modified: trunk/scripts/collectres
===================================================================
--- trunk/scripts/collectres	2007-07-16 13:28:08 UTC (rev 7780)
+++ trunk/scripts/collectres	2007-07-16 13:29:38 UTC (rev 7781)
@@ -118,6 +118,7 @@
     except ValueError,v:
       print >>sys.stderr, "caught ValueError exception in", filename
       print >>sys.stderr, v
+      print >>sys.stderr, ""
   return all_results
 
 def compare_res(x,y):
@@ -214,7 +215,7 @@
 
 if __name__=='__main__':
   args = sys.argv[:]
-  if len(args)==1:
+  if len(args)<=3:
     print "Usage: collectres <outputfile> <spec> <file1.pmat> <file2.pmat> ..."
     print 
     print "The <spec> can be the following (note how the <spec> has to be surrounded by quotes):"



From yoshua at mail.berlios.de  Mon Jul 16 16:45:20 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Mon, 16 Jul 2007 16:45:20 +0200
Subject: [Plearn-commits] r7782 - trunk/plearn_learners/online/EXPERIMENTAL
Message-ID: <200707161445.l6GEjKUD015620@sheep.berlios.de>

Author: yoshua
Date: 2007-07-16 16:45:19 +0200 (Mon, 16 Jul 2007)
New Revision: 7782

Modified:
   trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
Log:
KLp0p1 finally works!


Modified: trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-16 13:29:38 UTC (rev 7781)
+++ trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-16 14:45:19 UTC (rev 7782)
@@ -984,6 +984,7 @@
         }
 #endif
         *KLp0p1 *= -1;
+#if 0 
         if (!during_training)
         {
             real sum_pxt=0;
@@ -995,6 +996,7 @@
             }
             cout << "sum_t p1(x_t) = " << sum_pxt << endl;
         }
+#endif
         hidden_layer->setBatchSize(mbs);
         visible_layer->setBatchSize(mbs);
     }



From larocheh at mail.berlios.de  Mon Jul 16 19:01:13 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Mon, 16 Jul 2007 19:01:13 +0200
Subject: [Plearn-commits] r7783 - trunk/plearn_learners/online
Message-ID: <200707161701.l6GH1DVD030110@sheep.berlios.de>

Author: larocheh
Date: 2007-07-16 19:01:11 +0200 (Mon, 16 Jul 2007)
New Revision: 7783

Modified:
   trunk/plearn_learners/online/RBMMatrixConnection.cc
   trunk/plearn_learners/online/RBMMatrixConnection.h
Log:
Added L1 and L2 weight decay, similarly to the way it is coded in GradNNetLayerModule...


Modified: trunk/plearn_learners/online/RBMMatrixConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.cc	2007-07-16 14:45:19 UTC (rev 7782)
+++ trunk/plearn_learners/online/RBMMatrixConnection.cc	2007-07-16 17:01:11 UTC (rev 7783)
@@ -52,7 +52,9 @@
 RBMMatrixConnection::RBMMatrixConnection( real the_learning_rate ) :
     inherited(the_learning_rate),
     gibbs_ma_increment(0.1),
-    gibbs_initial_ma_coefficient(0.1)
+    gibbs_initial_ma_coefficient(0.1),
+    L1_penalty_factor(0),
+    L2_penalty_factor(0)
 {
 }
 
@@ -70,15 +72,36 @@
                   "its inverse sigmoid by gibbs_ma_increment). After the last\n"
                   "increase has been made, the moving average coefficient stays constant.\n");
 
-    declareOption(ol, "gibbs_ma_increment", &RBMMatrixConnection::gibbs_ma_increment,
+    declareOption(ol, "gibbs_ma_increment", 
+                  &RBMMatrixConnection::gibbs_ma_increment,
                   OptionBase::buildoption,
-                  "The increment in the inverse sigmoid of the moving average coefficient\n"
-                  "to apply after the number of updates reaches an element of the gibbs_ma_schedule.\n");
+                  "The increment in the inverse sigmoid of the moving "
+                  "average coefficient\n"
+                  "to apply after the number of updates reaches an element "
+                  "of the gibbs_ma_schedule.\n");
 
-    declareOption(ol, "gibbs_initial_ma_coefficient", &RBMMatrixConnection::gibbs_initial_ma_coefficient,
+    declareOption(ol, "gibbs_initial_ma_coefficient", 
+                  &RBMMatrixConnection::gibbs_initial_ma_coefficient,
                   OptionBase::buildoption,
-                  "Initial moving average coefficient for the negative phase statistics in the Gibbs chain.\n");
+                  "Initial moving average coefficient for the negative phase "
+                  "statistics in the Gibbs chain.\n");
 
+    declareOption(ol, "L1_penalty_factor", 
+                  &RBMMatrixConnection::L1_penalty_factor,
+                  OptionBase::buildoption,
+                  "Optional (default=0) factor of L1 regularization term, i.e.\n"
+                  "minimize L1_penalty_factor * sum_{ij} |weights(i,j)| "
+                  "during training.\n");
+
+    declareOption(ol, "L2_penalty_factor", 
+                  &RBMMatrixConnection::L2_penalty_factor,
+                  OptionBase::buildoption,
+                  "Optional (default=0) factor of L2 regularization term, i.e.\n"
+                  "minimize 0.5 * L2_penalty_factor * sum_{ij} weights(i,j)^2 "
+                  "during training.\n");
+
+
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -217,6 +240,9 @@
             }
     }
 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+        applyWeightPenalty();
+
     clearStats();
 }
 
@@ -272,6 +298,9 @@
                 w_i[j] += winc_i[j];
             }
     }
+
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+        applyWeightPenalty();
 }
 
 void RBMMatrixConnection::update( const Mat& pos_down_values, // v_0
@@ -323,6 +352,9 @@
             }
          */
     }
+
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+        applyWeightPenalty();
 }
 
 
@@ -361,6 +393,9 @@
     transposeProductScaleAcc(weights, cd_neg_up_values, cd_neg_down_values,
         -learning_rate*(1-background_gibbs_update_ratio)*normalize_factor,
         real(1));
+
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+        applyWeightPenalty();
 }
 
 void RBMMatrixConnection::updateGibbs( const Mat& pos_down_values,
@@ -403,6 +438,9 @@
     transposeProductScaleAcc(weights, pos_up_values, pos_down_values,
                              learning_rate*normalize_factor, real(1));
     multiplyAcc(weights, weights_neg_stats, -learning_rate);
+
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+        applyWeightPenalty();
 }
 
 ////////////////
@@ -549,6 +587,9 @@
 
     // weights -= learning_rate * output_gradient * input'
     externalProductScaleAcc( weights, output_gradient, input, -learning_rate );
+
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+        applyWeightPenalty();
 }
 
 void RBMMatrixConnection::bpropUpdate(const Mat& inputs, const Mat& outputs,
@@ -579,6 +620,9 @@
     // weights -= learning_rate/n * output_gradients' * inputs
     transposeProductScaleAcc(weights, output_gradients, inputs,
                              -learning_rate / inputs.length(), real(1));
+
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+        applyWeightPenalty();
 }
 
 void RBMMatrixConnection::petiteCulotteOlivierUpdate(
@@ -618,6 +662,8 @@
                          input);
     }
 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+        addWeightPenalty(rbm_weights, rbm_weights_gradient);
 }
 
 
@@ -686,6 +732,9 @@
     else
         PLCHECK_MSG( false,
                      "Unknown port configuration" );
+
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+        applyWeightPenalty();
 }
 
 
@@ -717,6 +766,9 @@
             for( int j=0 ; j<w ; j++ )
                 w_i[j] = wns_i[j]/pos_count - wps_i[j]/neg_count;
     }
+
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+        addWeightPenalty(weights, weights_gradient);
 }
 
 // Instead of using the statistics, we assume we have only one markov chain
@@ -755,8 +807,62 @@
             for( int j=0 ; j<w ; j++ )
                 w_i[j] =  *nuv_i * ndv[j] - *puv_i * pdv[j] ;
     }
+
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+        addWeightPenalty(weights, weights_gradient);
 }
 
+// Applies penalty (decay) on weights
+void RBMMatrixConnection::applyWeightPenalty()
+{
+    real delta_L1 = learning_rate * L1_penalty_factor;
+    real delta_L2 = learning_rate * L2_penalty_factor;
+    for( int i=0; i<up_size; i++)
+    {
+        real* w_ = weights[i];
+        for( int j=0; j<down_size; j++ )
+        {
+            if( delta_L2 != 0. )
+                w_[j] *= (1 - delta_L2);
+        
+            if( delta_L1 != 0. )
+            {
+                if( w_[j] > delta_L1 )
+                    w_[j] -= delta_L1;
+                else if( w_[j] < -delta_L1 )
+                    w_[j] += delta_L1;
+                else
+                    w_[j] = 0.;
+            }
+        }
+    }
+}
+
+// Adds penalty (decay) gradient
+void RBMMatrixConnection::addWeightPenalty(Mat weights, Mat weight_gradients)
+{
+    real delta_L1 = L1_penalty_factor;
+    real delta_L2 = L2_penalty_factor;
+    for( int i=0; i<weights.length(); i++)
+    {
+        real* w_ = weights[i];
+        real* gw_ = weight_gradients[i];
+        for( int j=0; j<weights.width(); j++ )
+        {
+            if( delta_L2 != 0. )
+                gw_[j] += delta_L2*w_[j];
+        
+            if( delta_L1 != 0. )
+            {
+                if( w_[j] > 0 )
+                    gw_[j] += delta_L1;
+                else if( w_[j] < 0 )
+                    gw_[j] -= delta_L1;
+            }
+        }
+    }
+}
+
 ////////////
 // forget //
 ////////////

Modified: trunk/plearn_learners/online/RBMMatrixConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.h	2007-07-16 14:45:19 UTC (rev 7782)
+++ trunk/plearn_learners/online/RBMMatrixConnection.h	2007-07-16 17:01:11 UTC (rev 7783)
@@ -68,6 +68,12 @@
 
     //#####  Learned Options  #################################################
 
+    //! Optional (default=0) factor of L1 regularization term
+    real L1_penalty_factor;
+
+    //! Optional (default=0) factor of L2 regularization term
+    real L2_penalty_factor;
+
     //! Matrix containing unit-to-unit weights (output_size ? input_size)
     Mat weights;
 
@@ -226,6 +232,12 @@
                                         Mat& weights_gradient,
                                         bool accumulate = false);
 
+    //! Applies penalty (decay) on weights
+    virtual void applyWeightPenalty();
+
+    //! Adds penalty (decay) gradient
+    virtual void addWeightPenalty(Mat weights, Mat weight_gradients);
+
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from
     //! build().



From chrish at mail.berlios.de  Mon Jul 16 20:20:59 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Mon, 16 Jul 2007 20:20:59 +0200
Subject: [Plearn-commits] r7784 - trunk/python_modules/plearn/vmat
Message-ID: <200707161820.l6GIKxX4011096@sheep.berlios.de>

Author: chrish
Date: 2007-07-16 20:20:59 +0200 (Mon, 16 Jul 2007)
New Revision: 7784

Modified:
   trunk/python_modules/plearn/vmat/PMat.py
   trunk/python_modules/plearn/vmat/__init__.py
Log:
Make it possible to import plearn.vmat.PMat without having other dependencies.


Modified: trunk/python_modules/plearn/vmat/PMat.py
===================================================================
--- trunk/python_modules/plearn/vmat/PMat.py	2007-07-16 17:01:11 UTC (rev 7783)
+++ trunk/python_modules/plearn/vmat/PMat.py	2007-07-16 18:20:59 UTC (rev 7784)
@@ -33,7 +33,13 @@
 # Author: Pascal Vincent
 
 import numarray, sys, os, os.path
-from plearn.pyplearn.plearn_repr import plearn_repr, format_list_elements
+pyplearn_import_failed = False
+try:
+    from plearn.pyplearn.plearn_repr import plearn_repr, format_list_elements
+except ImportError:
+    pyplearn_import_failed = True
+    
+                             
 
 def array_columns( a, cols ):
     indices = None
@@ -400,25 +406,26 @@
     def __len__(self):
         return self.length
 
-    def __str__( self ):
-        return plearn_repr(self, indent_level=0)
+    if not pyplearn_import_failed:
+        def __str__( self ):
+            return plearn_repr(self, indent_level=0)
     
-    def plearn_repr( self, indent_level=0, inner_repr=plearn_repr ):
-        # asking for plearn_repr could be to send specification over
-        # to another prg so that will open the .pmat
-        # So we make sure data is flushed to disk.
-        self.flush()
-
-        def elem_format( elem ):
-            k, v = elem
-            return '%s = %s' % ( k, inner_repr(v, indent_level+1) )
-
-        options = [ ( 'filename',   self.fname      ),
-                    ( 'inputsize',  self.inputsize  ), 
-                    ( 'targetsize', self.targetsize ),
-                    ( 'weightsize', self.weightsize ) ]
-        return 'FileVMatrix(%s)' % format_list_elements( options, elem_format, indent_level+1 )
-        
+        def plearn_repr( self, indent_level=0, inner_repr=plearn_repr ):
+            # asking for plearn_repr could be to send specification over
+            # to another prg so that will open the .pmat
+            # So we make sure data is flushed to disk.
+            self.flush()
+    
+            def elem_format( elem ):
+                k, v = elem
+                return '%s = %s' % ( k, inner_repr(v, indent_level+1) )
+    
+            options = [ ( 'filename',   self.fname      ),
+                        ( 'inputsize',  self.inputsize  ), 
+                        ( 'targetsize', self.targetsize ),
+                        ( 'weightsize', self.weightsize ) ]
+            return 'FileVMatrix(%s)' % format_list_elements( options, elem_format, indent_level+1 )
+            
 if __name__ == '__main__':
     pmat = PMat( 'tmp.pmat', 'w', fieldnames=['F1', 'F2'] )
     pmat.append( [1, 2] )

Modified: trunk/python_modules/plearn/vmat/__init__.py
===================================================================
--- trunk/python_modules/plearn/vmat/__init__.py	2007-07-16 17:01:11 UTC (rev 7783)
+++ trunk/python_modules/plearn/vmat/__init__.py	2007-07-16 18:20:59 UTC (rev 7784)
@@ -1,17 +1,23 @@
-from plearn.pyplearn import pl
 
-__all__ = [ "AutoVMatrix", "vmat" ]
+pyplearn_import_failed = False
+try:
+    from plearn.pyplearn import pl
+except ImportError:
+    pyplearn_import_failed = True
 
-def AutoVMatrix( specification, inputsize=-1, targetsize=-1, weightsize=-1 ):
-    return pl.AutoVMatrix( specification = specification,
-                           inputsize     = inputsize, 
-                           targetsize    = targetsize,
-                           weightsize    = weightsize   )
-
-def vmat( vmatrix ):
-    ## Consider it as a path.
-    if isinstance( vmatrix, str ):
-        return AutoVMatrix( vmatrix )
-
-    ## For now, consider it as a snippet.
-    return vmatrix
+if not pyplearn_import_failed:
+    __all__ = [ "AutoVMatrix", "vmat" ]
+    
+    def AutoVMatrix( specification, inputsize=-1, targetsize=-1, weightsize=-1 ):
+        return pl.AutoVMatrix( specification = specification,
+                               inputsize     = inputsize, 
+                               targetsize    = targetsize,
+                               weightsize    = weightsize   )
+    
+    def vmat( vmatrix ):
+        ## Consider it as a path.
+        if isinstance( vmatrix, str ):
+            return AutoVMatrix( vmatrix )
+    
+        ## For now, consider it as a snippet.
+        return vmatrix



From chapados at mail.berlios.de  Tue Jul 17 16:20:46 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Tue, 17 Jul 2007 16:20:46 +0200
Subject: [Plearn-commits] r7785 - trunk/plearn/base
Message-ID: <200707171420.l6HEKkcZ000680@sheep.berlios.de>

Author: chapados
Date: 2007-07-17 16:20:46 +0200 (Tue, 17 Jul 2007)
New Revision: 7785

Modified:
   trunk/plearn/base/RemoteMethodDoc.cc
Log:
Slightly more descriptive error message

Modified: trunk/plearn/base/RemoteMethodDoc.cc
===================================================================
--- trunk/plearn/base/RemoteMethodDoc.cc	2007-07-16 18:20:59 UTC (rev 7784)
+++ trunk/plearn/base/RemoteMethodDoc.cc	2007-07-17 14:20:46 UTC (rev 7785)
@@ -43,7 +43,8 @@
     int argdocsize = m_args_doc.size();
     int argtypesize = m_args_type.size();
     if(argdocsize != argtypesize)
-        PLERROR("Number of ArgDoc (%d) inconsistent with number of arguments (%d)",argdocsize,argtypesize);
+        PLERROR("For function '%s', number of ArgDoc (%d) inconsistent with number of arguments (%d)",
+                m_name.c_str(), argdocsize, argtypesize);
 }
     
 //! Returns a string repretenting the "prototype" (signature) of the function in the doc.



From chrish at mail.berlios.de  Tue Jul 17 18:00:14 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Tue, 17 Jul 2007 18:00:14 +0200
Subject: [Plearn-commits] r7786 - trunk/plearn_learners/testers
Message-ID: <200707171600.l6HG0Ech007479@sheep.berlios.de>

Author: chrish
Date: 2007-07-17 18:00:14 +0200 (Tue, 17 Jul 2007)
New Revision: 7786

Modified:
   trunk/plearn_learners/testers/PTester.cc
Log:
Add comment documenting race condition in expdir creation in PTester.

Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2007-07-17 14:20:46 UTC (rev 7785)
+++ trunk/plearn_learners/testers/PTester.cc	2007-07-17 16:00:14 UTC (rev 7786)
@@ -436,6 +436,13 @@
         if (pathexists(expdir) && enforce_clean_expdir)
             PLERROR("Directory (or file) %s already exists.\n"
                     "First move it out of the way.", expdir.c_str());
+        // This code looks like it's guaranteeing that we get an expdir that we
+        // create when enforce_clean_expdir is True, but this is not the case. 
+        // Let's say some other process (from a parallel dispatch of many
+        // PLearn processes, say) is trying to create an expdir with the same
+        // name, and that other process succeeds in creating it when we are
+        // here. Then, given that force_mkdir() returns true if the directory
+        // already exists, this is a textbook example of a race condition.
         if (!force_mkdir(expdir))
             PLERROR("In PTester Could not create experiment directory %s",expdir.c_str());
         expdir = expdir.absolute() / "";



From nouiz at mail.berlios.de  Tue Jul 17 20:07:11 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 17 Jul 2007 20:07:11 +0200
Subject: [Plearn-commits] r7787 - trunk/plearn/misc
Message-ID: <200707171807.l6HI7Bxr032159@sheep.berlios.de>

Author: nouiz
Date: 2007-07-17 20:07:10 +0200 (Tue, 17 Jul 2007)
New Revision: 7787

Modified:
   trunk/plearn/misc/vmatmain.cc
Log:
Added args verification


Modified: trunk/plearn/misc/vmatmain.cc
===================================================================
--- trunk/plearn/misc/vmatmain.cc	2007-07-17 16:00:14 UTC (rev 7786)
+++ trunk/plearn/misc/vmatmain.cc	2007-07-17 18:07:10 UTC (rev 7787)
@@ -532,6 +532,11 @@
     {
         string source = argv[2];
         string destination = argv[3];
+        if(argc<4)
+        {
+            cerr<<"usage vmat convert <source> <destination> [--cols=col1,col2,col3,...]\n";
+            exit(1);
+        }
         VMat vm = getVMat(source, indexf);
 
         /**



From louradou at mail.berlios.de  Tue Jul 17 20:33:36 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Tue, 17 Jul 2007 20:33:36 +0200
Subject: [Plearn-commits] r7788 - trunk/plearn_learners/online
Message-ID: <200707171833.l6HIXau0001573@sheep.berlios.de>

Author: louradou
Date: 2007-07-17 20:33:35 +0200 (Tue, 17 Jul 2007)
New Revision: 7788

Modified:
   trunk/plearn_learners/online/RBMBinomialLayer.cc
   trunk/plearn_learners/online/RBMBinomialLayer.h
   trunk/plearn_learners/online/RBMLayer.cc
   trunk/plearn_learners/online/RBMLayer.h
   trunk/plearn_learners/online/RBMMixedLayer.cc
   trunk/plearn_learners/online/RBMMixedLayer.h
   trunk/plearn_learners/online/RBMModule.cc
   trunk/plearn_learners/online/RBMModule.h
   trunk/plearn_learners/online/RBMMultinomialLayer.cc
   trunk/plearn_learners/online/RBMMultinomialLayer.h
Log:
Compute the likelihood of the RBM visible inputs with Binomial layers as well as
Multinomial and Mixed layers.



Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-07-17 18:07:10 UTC (rev 7787)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-07-17 18:33:35 UTC (rev 7788)
@@ -476,6 +476,22 @@
     return -dot(unit_values, bias);
 }
 
+int RBMBinomialLayer::getConfigurationCount()
+{
+    return size < 31 ? 1<<size : INFINITE_CONFIGURATIONS;
+}
+
+void RBMBinomialLayer::getConfiguration(int conf_index, Vec& output)
+{
+    PLASSERT( output.length() == size );
+    PLASSERT( conf_index >= 0 && conf_index < getConfigurationCount() );
+
+    for ( int i = 0; i < size; ++i ) {
+        output[i] = conf_index & 1;
+        conf_index >>= 1;
+    }    
+}
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/online/RBMBinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.h	2007-07-17 18:07:10 UTC (rev 7787)
+++ trunk/plearn_learners/online/RBMBinomialLayer.h	2007-07-17 18:33:35 UTC (rev 7788)
@@ -120,6 +120,10 @@
     //! compute -bias' unit_values
     virtual real energy(const Vec& unit_values) const;
 
+    virtual int getConfigurationCount();
+
+    virtual void getConfiguration(int conf_index, Vec& output);
+
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.

Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2007-07-17 18:07:10 UTC (rev 7787)
+++ trunk/plearn_learners/online/RBMLayer.cc	2007-07-17 18:33:35 UTC (rev 7788)
@@ -631,6 +631,17 @@
     return 0;
 }
 
+int RBMLayer::getConfigurationCount()
+{
+    PLERROR("RBMLayer::getConfigurationCount() not implemented in subclass %s\n",classname().c_str());
+    return 0;
+}
+
+void RBMLayer::getConfiguration(int conf_index, Vec& output)
+{
+    PLERROR("RBMLayer::getConfiguration(int, Vec) not implemented in subclass %s\n",classname().c_str());
+}
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2007-07-17 18:07:10 UTC (rev 7787)
+++ trunk/plearn_learners/online/RBMLayer.h	2007-07-17 18:33:35 UTC (rev 7788)
@@ -113,6 +113,8 @@
     //! computed values of activations.
     bool expectations_are_up_to_date;
 
+    static const int INFINITE_CONFIGURATIONS = 0x7fffffff;
+
 public:
     //#####  Public Member Functions  #########################################
 
@@ -265,6 +267,12 @@
 
     virtual real energy(const Vec& unit_values) const;
 
+    //! Returns a number of different configurations the layer can be in.
+    virtual int getConfigurationCount();
+
+    //! Computes the conf_index configuration of the layer.
+    virtual void getConfiguration(int conf_index, Vec& output);
+
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.

Modified: trunk/plearn_learners/online/RBMMixedLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.cc	2007-07-17 18:07:10 UTC (rev 7787)
+++ trunk/plearn_learners/online/RBMMixedLayer.cc	2007-07-17 18:33:35 UTC (rev 7788)
@@ -374,6 +374,8 @@
         int size_i = sub_layers[i]->size;
         sub_layers[i]->fpropNLL( targets.subMatColumns(begin, size_i),
                                  mat_nlls.column(i) );
+        for( int j=0; j < batch_size; ++j )
+            costs_column(j,0) += mat_nlls(j, i);
     }
 }
 
@@ -629,7 +631,51 @@
     deepCopyField(mat_nlls,         copies);
 }
 
+real RBMMixedLayer::energy(const Vec& unit_values) const
+{
+    real energy = 0;
 
+    for ( int i = 0; i < n_layers; ++i ) {
+        int begin = init_positions[i];
+        int size_i = sub_layers[i]->size;
+        Vec values = unit_values.subVec( begin, size_i );
+        energy += sub_layers[i]->energy(values);
+    }
+
+    return energy;
+}
+
+int RBMMixedLayer::getConfigurationCount()
+{
+    int count = 1;
+
+    for ( int i = 0; i < n_layers; ++i ) {
+        int cc_layer_i = sub_layers[i]->getConfigurationCount();
+        // Avoiding overflow
+        if ( INFINITE_CONFIGURATIONS/cc_layer_i <= count )
+            return INFINITE_CONFIGURATIONS;
+        count *= cc_layer_i;
+    }
+
+    return count;
+}
+
+void RBMMixedLayer::getConfiguration(int conf_index, Vec& output)
+{
+    PLASSERT( output.length() == size );
+    PLASSERT( conf_index >= 0 && conf_index < getConfigurationCount() );
+
+    int conf_i = conf_index;
+    for ( int i = 0; i < n_layers; ++i ) {
+        int conf_layer_i = sub_layers[i]->getConfigurationCount();
+        int begin = init_positions[i];
+        int size_i = sub_layers[i]->size;
+        Vec output_i = output.subVec( begin, size_i );
+        sub_layers[i]->getConfiguration(conf_i % conf_layer_i, output_i);
+        conf_i /= conf_layer_i;
+    }    
+}
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/online/RBMMixedLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.h	2007-07-17 18:07:10 UTC (rev 7787)
+++ trunk/plearn_learners/online/RBMMixedLayer.h	2007-07-17 18:33:35 UTC (rev 7788)
@@ -166,7 +166,13 @@
     //! forgets everything
     virtual void forget();
 
+    //! Compute -bias' unit_values
+    virtual real energy(const Vec& unit_values) const;
 
+    virtual int getConfigurationCount();
+
+    virtual void getConfiguration(int conf_index, Vec& output);
+
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-07-17 18:07:10 UTC (rev 7787)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-07-17 18:33:35 UTC (rev 7788)
@@ -603,6 +603,59 @@
     PLERROR("In RBMModule::fprop - Not implemented");
 }
 
+void RBMModule::computePartitionFunction()
+{
+	int hidden_configurations = hidden_layer->getConfigurationCount();
+	int visible_configurations = visible_layer->getConfigurationCount();
+	cout << "vc: " << visible_configurations << endl;
+	cout << "hc: " << hidden_configurations << endl;
+	const int MAX_CONFIGURATIONS = 1<<30;
+	
+	PLASSERT_MSG(hidden_configurations < MAX_CONFIGURATIONS || visible_configurations < MAX_CONFIGURATIONS,
+		"To compute exact log-likelihood of an RBM maximum configurations of hidden "
+				"or visible layer must be less than 2^30.");
+	
+	// Compute partition function
+	if (hidden_configurations > visible_configurations)
+		// do it by log-summing minus-free-energy of visible configurations
+	{
+		energy_inputs.resize(1, visible_layer->size);
+		Vec input = energy_inputs(0);
+		// COULD BE DONE MORE EFFICIENTLY BY DOING MANY CONFIGURATIONS
+		// AT ONCE IN A 'MINIBATCH'
+		Mat free_energy(1, 1);
+		log_partition_function = 0;
+		for (int c = 0; c < visible_configurations; c++)
+		{
+			visible_layer->getConfiguration(c, input);
+			computeFreeEnergyOfVisible(energy_inputs, free_energy, false);
+			if (c==0)
+				log_partition_function = -free_energy(0,0);
+			else
+				log_partition_function = logadd(log_partition_function,-free_energy(0,0));
+		}
+	}
+	else
+		// do it by summing free-energy of hidden configurations
+	{
+		energy_inputs.resize(1, hidden_layer->size);
+		Vec input = energy_inputs(0);
+		// COULD BE DONE MORE EFFICIENTLY BY DOING MANY CONFIGURATIONS
+		// AT ONCE IN A 'MINIBATCH'
+		Mat free_energy(1, 1);
+		log_partition_function = 0;
+		for (int c = 0; c < hidden_configurations; c++)
+		{
+			hidden_layer->getConfiguration(c, input);
+			computeFreeEnergyOfHidden(energy_inputs, free_energy);
+			if (c==0)
+				log_partition_function = -free_energy(0,0);
+			else
+				log_partition_function = logadd(log_partition_function,-free_energy(0,0));
+		}
+	}
+}
+
 void RBMModule::fprop(const TVec<Mat*>& ports_value)
 {
 
@@ -696,7 +749,8 @@
     {
         if (partition_function_is_stale && !during_training)
         {
-            PLASSERT_MSG(hidden_layer->size<32 || visible_layer->size<32,
+		
+            /*PLASSERT_MSG(hidden_layer->size<32 || visible_layer->size<32,
                          "To compute exact log-likelihood of an RBM, hidden_layer->size "
                          "or visible_layer->size must be <32");
             // recompute partition function
@@ -755,7 +809,8 @@
                     else
                         log_partition_function = logadd(log_partition_function,-free_energy(0,0));
                 }
-            }
+	    }*/
+	    computePartitionFunction();
             partition_function_is_stale=false;
         }
         if (visible && !visible->isEmpty()

Modified: trunk/plearn_learners/online/RBMModule.h
===================================================================
--- trunk/plearn_learners/online/RBMModule.h	2007-07-17 18:07:10 UTC (rev 7787)
+++ trunk/plearn_learners/online/RBMModule.h	2007-07-17 18:33:35 UTC (rev 7788)
@@ -315,6 +315,8 @@
     void computeEnergy(const Mat& visible, const Mat& hidden, Mat& energy,
                        bool positive_phase = true);
 
+	void computePartitionFunction();
+
 private:
     //#####  Private Member Functions  ########################################
 

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-07-17 18:07:10 UTC (rev 7787)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-07-17 18:33:35 UTC (rev 7788)
@@ -423,7 +423,27 @@
     inherited::makeDeepCopyFromShallowCopy(copies);
 }
 
+real RBMMultinomialLayer::energy(const Vec& unit_values) const
+{
+    return -dot(unit_values, bias);
+}
 
+int RBMMultinomialLayer::getConfigurationCount()
+{
+    return size;
+}
+
+void RBMMultinomialLayer::getConfiguration(int conf_index, Vec& output)
+{
+    PLASSERT( output.length() == size );
+    PLASSERT( conf_index >= 0 && conf_index < getConfigurationCount() );
+
+    for ( int i = 0; i < size; ++i ) {
+        output[i] = i == conf_index ? 1 : 0;
+    }    
+}
+
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.h	2007-07-17 18:07:10 UTC (rev 7787)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.h	2007-07-17 18:33:35 UTC (rev 7788)
@@ -116,6 +116,13 @@
     virtual void bpropNLL(const Mat& targets, const Mat& costs_column,
                           Mat& bias_gradients);
 
+    // Compute -bias' unit_values
+    virtual real energy(const Vec& unit_values) const;
+
+    virtual int getConfigurationCount();
+
+    virtual void getConfiguration(int conf_index, Vec& output);
+
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.



From tihocan at mail.berlios.de  Tue Jul 17 21:19:31 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 17 Jul 2007 21:19:31 +0200
Subject: [Plearn-commits] r7789 - trunk/plearn_learners/online/EXPERIMENTAL
Message-ID: <200707171919.l6HJJVv4004074@sheep.berlios.de>

Author: tihocan
Date: 2007-07-17 21:19:31 +0200 (Tue, 17 Jul 2007)
New Revision: 7789

Modified:
   trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
Log:
- Added safety call to setBatchSize on the visible and hidden layers after computing the gradient of the KL(p0|p1)
- Added commented out code that allows quick testing of the gradient
- Commented some code to get rid of compiler's warnings


Modified: trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-17 18:33:35 UTC (rev 7788)
+++ trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-17 19:19:31 UTC (rev 7789)
@@ -39,6 +39,7 @@
 
 
 #include "KLp0p1RBMModule.h"
+//#include <plearn/vmat/AutoVMatrix.h>
 #include <plearn/vmat/VMat.h>
 #include <plearn_learners/online/RBMMatrixConnection.h>
 
@@ -883,7 +884,13 @@
         int mbs=visible->length();
         KLp0p1->resize(mbs,1);
         KLp0p1->clear();
-        PLASSERT_MSG(training_set,"KLp0p1RBMModule: training_set must be provided");
+#if 0
+        if (!training_set) {
+            training_set = new AutoVMatrix("/u/delallea/tmp/kl/data.amat");
+        } else
+#else
+            PLASSERT_MSG(training_set,"KLp0p1RBMModule: training_set must be provided");
+#endif
         int n=training_set.length();
         PLASSERT_MSG(n>0,"KLp0p1RBMModule: training_set must have n>0 rows");
 
@@ -893,13 +900,21 @@
         const Mat& ha=hidden_layer->activations;
         const Mat& X=visible_layer->getExpectations();
         training_set->getMat(0,0,X);
+        PP<RBMMatrixConnection> matrix_connection = NULL;
+#if 0
+        if (weights) {
+            matrix_connection = PP<RBMMatrixConnection>(connection);
+            matrix_connection->weights << (*weights)(0);
+            pout << "Weights: " << endl << matrix_connection->weights << endl;
+        }
+#endif
         connection->setAsDownInputs(visible_layer->getExpectations());
         hidden_layer->getAllActivations(connection,0,true);
         hidden_layer->computeExpectations();
 
         PLASSERT_MSG(hidden_layer->size<32,"To compute KLp0p1 of an RBM, hidden_layer->size must be <32");
         PLASSERT(hidden_layer->classname()=="RBMBinomialLayer");
-        real logn=safelog((real)n);
+        //real logn=safelog((real)n);
         // assuming a binary hidden we sum over all bit configurations
         int n_configurations = 1 << hidden_layer->size; // = 2^{hidden_layer->size}
         // put all h configurations in the hidden_layer->samples
@@ -956,7 +971,7 @@
             }
             // now log_sum_ph_given_xk = log sum_k P(h|x^k)
             conf_visible_layer->activation << conf_visible_layer->activations(c);
-            real log_sum_p_xt = 0;
+            //real log_sum_p_xt = 0;
             for (int t=0;t<mbs;t++)
             {
                 real log_p_xt = -conf_visible_layer->fpropNLL((*visible)(t));
@@ -1708,12 +1723,12 @@
         PP<RBMMatrixConnection> matrix_connection = PP<RBMMatrixConnection>(connection);
         hidden_layer->setBatchSize(n);
         visible_layer->setBatchSize(n);
-        Mat& W = matrix_connection->weights;
+        Mat& W = /* weights ? *weights :*/ matrix_connection->weights;
         Vec& hidden_bias = hidden_layer->bias;
         Vec& visible_bias = visible_layer->bias;
         const Mat& X=visible_layer->getExpectations();
         int n_configurations = 1 << hidden_layer->size; // = 2^{hidden_layer->size}
-        real logn=safelog(n);
+        //real logn=safelog(n);
         // we only computed the activations in the fprop
         conf_visible_layer->computeExpectations(); 
         const Mat& pvisible_given_H = conf_visible_layer->getExpectations();
@@ -1730,6 +1745,7 @@
                 {
                     Vec h = conf_hidden_layer->samples(c);
                     Vec avisible_given_h=conf_visible_layer->activations(c);
+                    // KLp0p1(x) = -log p1(x) - logn
                     real lp = (*KLp0p1)(t,0); // lp = log (exp(C(x^t)))
                     // compute and multiply exp(lp) by P(h|x^k)
                     for (int i=0;i<hidden_layer->size;i++)
@@ -1752,18 +1768,36 @@
                     real coeff = exp(lp);
                     Vec pvisible_given_h=pvisible_given_H(c);
                     for (int j=0;j<visible_layer->size;j++)
+                    {
                         visible_bias[j] +=
                             klp0p1_learning_rate*coeff*(xt[j]-pvisible_given_h[j]);
+                    }
                     for (int i=0;i<hidden_layer->size;i++)
                     {
                         hidden_bias[i] += klp0p1_learning_rate*coeff*(h[i]-ph_given_xk[i]);
                         for (int j=0;j<visible_layer->size;j++)
-                            W(i,j) += klp0p1_learning_rate*coeff*
-                                (h[i]*(xt[j]-pvisible_given_h[j])+xk[j]*(h[i]-ph_given_xk[i]));
+                        {
+                            real grad = - coeff *
+                                (  xk[j] * (h[i]  - ph_given_xk[i])
+                                 + h[i]  * (xt[j] - pvisible_given_h[j]));
+
+#if 0
+                            if (compute_weights_grad) {
+                                weights_grad->resize(mbs,
+                                        weights_grad->width());
+                                (*weights_grad)(0, i * visible_layer->size + j)
+                                    += grad;
+                            }
+#else
+                            W(i,j) -= klp0p1_learning_rate * grad;
+#endif
+                        }
                     }
                 }
             }
         }
+        hidden_layer->setBatchSize(mbs);
+        visible_layer->setBatchSize(mbs);
     }
 
     // Explicit error message in the case of the 'visible' port.



From nouiz at mail.berlios.de  Tue Jul 17 22:04:26 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 17 Jul 2007 22:04:26 +0200
Subject: [Plearn-commits] r7790 - trunk/python_modules/plearn/parallel
Message-ID: <200707172004.l6HK4Q2Q007264@sheep.berlios.de>

Author: nouiz
Date: 2007-07-17 22:04:26 +0200 (Tue, 17 Jul 2007)
New Revision: 7790

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
In condor mode, we o not create one file for each experiment to start when their is no log of generated


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-07-17 19:19:31 UTC (rev 7789)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-07-17 20:04:26 UTC (rev 7790)
@@ -438,24 +438,24 @@
             return #no task to run
         # create the bqsubmit.dat, with
         condor_datas = []
-        for task in self.tasks:
-            condor_data = os.path.join(self.tmp_dir, task.unique_id + '.data')
-            condor_datas.append(condor_data)
-            self.temp_files.append(condor_data)
-            param_dat = open(condor_data, 'w')
-            
-            param_dat.write( dedent('''\
-            #!/bin/bash
-            %s''' %('\n'.join(task.commands))))
-            param_dat.close()
+         if len(self.tasks)>1:
+             for task in self.tasks:
+                 condor_data = os.path.join(self.tmp_dir,self.unique_id +'.'+ task.unique_id + '.data')
+                 condor_datas.append(condor_data)
+                 self.temp_files.append(condor_data)
+                 param_dat = open(condor_data, 'w')
+                
+                 param_dat.write( dedent('''\
+                 #!/bin/bash
+                 %s''' %('\n'.join(task.commands))))
+                 param_dat.close()
         
 
-        condor_file = os.path.join(self.tmp_dir, task.unique_id + ".condor")
+        condor_file = os.path.join(self.tmp_dir, self.unique_id + ".condor")
         self.temp_files.append(condor_file)
         condor_dat = open( condor_file, 'w' )
         
         req=""
-        u=get_username()
         if self.targetcondorplatform == 'BOTH':
             req="((Arch == \"INTEL\")||(Arch == \"X86_64\"))"
         elif self.targetcondorplatform == 'INTEL':
@@ -477,14 +477,18 @@
                 error          = %s/condor.%s.%s.$(Process).error
                 log            = %s/condor.%s.log
                 ''' % (self.tmp_dir,req,
-                       self.log_dir,self.targetcondorplatform,task.unique_id,
-                       self.log_dir,self.targetcondorplatform,task.unique_id,
+                       self.log_dir,self.targetcondorplatform,self.unique_id,
+                       self.log_dir,self.targetcondorplatform,self.unique_id,
                        self.log_dir,self.targetcondorplatform)))
 #                preBatch = ''' + pre_batch_command + '''
 #                postBatch = ''' + post_batch_command +'''
 
-        for i in condor_datas:
-            condor_dat.write("arguments      = sh "+i+" $$(Arch) \nqueue\n")
+        if len(condor_datas)==0:
+            for i in condor_datas:
+                condor_dat.write("arguments      = sh "+i+" $$(Arch) \nqueue\n")
+        else:
+            for task in self.tasks:
+                condor_dat.write("arguments      = %s \nqueue\n" %(' ; '.join(task.commands)))
         condor_dat.close()
 
         launch_file = os.path.join(self.tmp_dir, 'launch.sh')
@@ -514,13 +518,13 @@
                     echo "PATH: $PATH" 1>&2
                     echo "PYTHONPATH: $PYTHONPATH" 1>&2
                     echo "LD_LIBRARY_PATH: $LD_LIBRARY_PATH" 1>&2
-                    which python 1>&2
-                    echo -n python version: 1>&2
-                    python -V 1>&2
-                    echo -n /usr/bin/python version: 1>&2
-                    /usr/bin/python -V 1>&2
+                    #which python 1>&2
+                    #echo -n python version: 1>&2
+                    #python -V 1>&2
+                    #echo -n /usr/bin/python version: 1>&2
+                    #/usr/bin/python -V 1>&2
                     echo ${PROGRAM} $@ 1>&2
-                    $PROGRAM $@'''))
+                    ${PROGRAM} "$@"'''))
             launch_dat.close()
             os.chmod(launch_file, 0755)
 



From nouiz at mail.berlios.de  Wed Jul 18 15:43:05 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 18 Jul 2007 15:43:05 +0200
Subject: [Plearn-commits] r7791 - trunk/python_modules/plearn/parallel
Message-ID: <200707181343.l6IDh5E3023166@sheep.berlios.de>

Author: nouiz
Date: 2007-07-18 15:43:02 +0200 (Wed, 18 Jul 2007)
New Revision: 7791

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
Solve indentation problem


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-07-17 20:04:26 UTC (rev 7790)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-07-18 13:43:02 UTC (rev 7791)
@@ -438,17 +438,17 @@
             return #no task to run
         # create the bqsubmit.dat, with
         condor_datas = []
-         if len(self.tasks)>1:
-             for task in self.tasks:
-                 condor_data = os.path.join(self.tmp_dir,self.unique_id +'.'+ task.unique_id + '.data')
-                 condor_datas.append(condor_data)
-                 self.temp_files.append(condor_data)
-                 param_dat = open(condor_data, 'w')
+        if len(self.tasks)>1:
+            for task in self.tasks:
+                condor_data = os.path.join(self.tmp_dir,self.unique_id +'.'+ task.unique_id + '.data')
+                condor_datas.append(condor_data)
+                self.temp_files.append(condor_data)
+                param_dat = open(condor_data, 'w')
                 
-                 param_dat.write( dedent('''\
-                 #!/bin/bash
-                 %s''' %('\n'.join(task.commands))))
-                 param_dat.close()
+                param_dat.write( dedent('''\
+                #!/bin/bash
+                %s''' %('\n'.join(task.commands))))
+                param_dat.close()
         
 
         condor_file = os.path.join(self.tmp_dir, self.unique_id + ".condor")



From tihocan at mail.berlios.de  Wed Jul 18 17:19:17 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 18 Jul 2007 17:19:17 +0200
Subject: [Plearn-commits] r7792 - trunk/plearn_learners/online
Message-ID: <200707181519.l6IFJHFP031475@sheep.berlios.de>

Author: tihocan
Date: 2007-07-18 17:19:15 +0200 (Wed, 18 Jul 2007)
New Revision: 7792

Modified:
   trunk/plearn_learners/online/OnlineLearningModule.cc
Log:
Slightly more efficient implementation of getPortLength

Modified: trunk/plearn_learners/online/OnlineLearningModule.cc
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.cc	2007-07-18 13:43:02 UTC (rev 7791)
+++ trunk/plearn_learners/online/OnlineLearningModule.cc	2007-07-18 15:19:15 UTC (rev 7792)
@@ -398,11 +398,12 @@
 ///////////////////
 int OnlineLearningModule::getPortLength(const string& port)
 {
-    if (getPortIndex(port)<0)
-        PLERROR("Port named %s not known by module %s of class %s\n",
-                port.c_str(),name.c_str(),classname().c_str());
-    PLASSERT( getPortIndex(port) >= 0 );
-    return getPortSizes()(getPortIndex(port), 0);
+    int port_index = getPortIndex(port);
+    if (port_index < 0)
+        PLERROR("In OnlineLearningModule::getPortLength - Port '%s' not known "
+                "by module '%s' of class '%s'",
+                port.c_str(), name.c_str(), classname().c_str());
+    return getPortSizes()(port_index, 0);
 }
 
 //////////////////



From sakenasv at mail.berlios.de  Wed Jul 18 19:06:20 2007
From: sakenasv at mail.berlios.de (sakenasv at mail.berlios.de)
Date: Wed, 18 Jul 2007 19:06:20 +0200
Subject: [Plearn-commits] r7793 - trunk/plearn_learners/online
Message-ID: <200707181706.l6IH6KBn017943@sheep.berlios.de>

Author: sakenasv
Date: 2007-07-18 19:06:05 +0200 (Wed, 18 Jul 2007)
New Revision: 7793

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
Cosmetic changes to the partition function calculation.

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-07-18 15:19:15 UTC (rev 7792)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-07-18 17:06:05 UTC (rev 7793)
@@ -607,13 +607,11 @@
 {
 	int hidden_configurations = hidden_layer->getConfigurationCount();
 	int visible_configurations = visible_layer->getConfigurationCount();
-	cout << "vc: " << visible_configurations << endl;
-	cout << "hc: " << hidden_configurations << endl;
-	const int MAX_CONFIGURATIONS = 1<<30;
 	
-	PLASSERT_MSG(hidden_configurations < MAX_CONFIGURATIONS || visible_configurations < MAX_CONFIGURATIONS,
+	PLASSERT_MSG(hidden_configurations != RBMLayer::INFINITE_CONFIGURATIONS ||
+                    visible_configurations != RBMLayer::INFINITE_CONFIGURATIONS,
 		"To compute exact log-likelihood of an RBM maximum configurations of hidden "
-				"or visible layer must be less than 2^30.");
+				"or visible layer must be less than 2^31.");
 	
 	// Compute partition function
 	if (hidden_configurations > visible_configurations)
@@ -749,67 +747,6 @@
     {
         if (partition_function_is_stale && !during_training)
         {
-		
-            /*PLASSERT_MSG(hidden_layer->size<32 || visible_layer->size<32,
-                         "To compute exact log-likelihood of an RBM, hidden_layer->size "
-                         "or visible_layer->size must be <32");
-            // recompute partition function
-            if (hidden_layer->size > visible_layer->size)
-                // do it by log-summing minus-free-energy of visible configurations
-            {
-                PLASSERT(visible_layer->classname()=="RBMBinomialLayer");
-                // assuming a binary input we sum over all bit configurations
-                int n_configurations = 1 << visible_layer->size; // = 2^{visible_layer->size}
-                energy_inputs.resize(1, visible_layer->size);
-                Vec input = energy_inputs(0);
-                // COULD BE DONE MORE EFFICIENTLY BY DOING MANY CONFIGURATIONS
-                // AT ONCE IN A 'MINIBATCH'
-                Mat free_energy(1, 1);
-                log_partition_function = 0;
-                for (int c=0;c<n_configurations;c++)
-                {
-                    // convert integer c into a bit-wise visible representation
-                    int x=c;
-                    for (int i=0;i<visible_layer->size;i++)
-                    {
-                        input[i]= x & 1; // take least significant bit
-                        x >>= 1; // and shift right (divide by 2)
-                    }
-                    computeFreeEnergyOfVisible(energy_inputs,free_energy,false);
-                    if (c==0)
-                        log_partition_function = -free_energy(0,0);
-                    else
-                        log_partition_function = logadd(log_partition_function,-free_energy(0,0));
-                }
-            }
-            else
-                // do it by summing free-energy of hidden configurations
-            {
-                PLASSERT(hidden_layer->classname()=="RBMBinomialLayer");
-                // assuming a binary hidden we sum over all bit configurations
-                int n_configurations = 1 << hidden_layer->size; // = 2^{hidden_layer->size}
-                energy_inputs.resize(1, hidden_layer->size);
-                Vec input = energy_inputs(0);
-                // COULD BE DONE MORE EFFICIENTLY BY DOING MANY CONFIGURATIONS
-                // AT ONCE IN A 'MINIBATCH'
-                Mat free_energy(1,1);
-                log_partition_function = 0;
-                for (int c=0;c<n_configurations;c++)
-                {
-                    // convert integer c into a bit-wise hidden representation
-                    int x=c;
-                    for (int i=0;i<hidden_layer->size;i++)
-                    {
-                        input[i]= x & 1; // take least significant bit
-                        x >>= 1; // and shift right (divide by 2)
-                    }
-                    computeFreeEnergyOfHidden(energy_inputs, free_energy);
-                    if (c==0)
-                        log_partition_function = -free_energy(0,0);
-                    else
-                        log_partition_function = logadd(log_partition_function,-free_energy(0,0));
-                }
-	    }*/
 	    computePartitionFunction();
             partition_function_is_stale=false;
         }



From saintmlx at mail.berlios.de  Wed Jul 18 19:27:36 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Wed, 18 Jul 2007 19:27:36 +0200
Subject: [Plearn-commits] r7794 - in trunk/plearn: base python
Message-ID: <200707181727.l6IHRaMs019663@sheep.berlios.de>

Author: saintmlx
Date: 2007-07-18 19:27:35 +0200 (Wed, 18 Jul 2007)
New Revision: 7794

Modified:
   trunk/plearn/base/Object.h
   trunk/plearn/python/PythonObjectWrapper.cc
   trunk/plearn/python/PythonObjectWrapper.h
Log:
- fixed PLearn/Python bridge:
  - classes deriving from PP<T> (merci Pascal)
  - non-PP pointers to Objects, enums, etc. (merci Nicolas)



Modified: trunk/plearn/base/Object.h
===================================================================
--- trunk/plearn/base/Object.h	2007-07-18 17:06:05 UTC (rev 7793)
+++ trunk/plearn/base/Object.h	2007-07-18 17:27:35 UTC (rev 7794)
@@ -48,7 +48,10 @@
 #define Object_INC
 
 // Python stuff must be included first
-#include <plearn/python/PythonIncludes.h>
+#ifdef PL_PYTHON_VERSION 
+#  include <plearn/python/PythonIncludes.h>
+#  include <plearn/python/PythonObjectWrapper.h>
+#endif //ifdef PL_PYTHON_VERSION 
 
 #include <map>
 #include <string>
@@ -504,7 +507,7 @@
  *  used to serialize the new smart pointer type
  */
 
-#define DECLARE_OBJECT_PP(PPCLASSTYPE, CLASSTYPE)                       \
+#define DECLARE_OBJECT_PP_SERIALIZE(PPCLASSTYPE, CLASSTYPE)             \
         inline PStream &operator>>(PStream &in, PPCLASSTYPE &o)         \
           { Object *ptr = 0;                                            \
             in >> ptr;                                                  \
@@ -514,7 +517,27 @@
           { out << static_cast<const PP<CLASSTYPE> &>(o); return out; } \
         DECLARE_TYPE_TRAITS(PPCLASSTYPE)
 
+#ifdef PL_PYTHON_VERSION 
+#define DECLARE_OBJECT_PP(PPCLASSTYPE, CLASSTYPE)                       \
+        struct ConvertFromPyObject<PPCLASSTYPE>                         \
+        {                                                               \
+            static PPCLASSTYPE convert(PyObject* o,                     \
+                                       bool print_traceback)            \
+            { return PPCLASSTYPE(ConvertFromPyObject<PP<CLASSTYPE> >    \
+                                  ::convert(o, print_traceback)); }     \
+        };                                                              \
+        template<> struct ConvertToPyObject<PPCLASSTYPE>                \
+        {                                                               \
+            static PyObject* newPyObject(const PPCLASSTYPE& x)          \
+            {return ConvertToPyObject<PP<CLASSTYPE> >::newPyObject(x);} \
+        };                                                              \
+        DECLARE_OBJECT_PP_SERIALIZE(PPCLASSTYPE, CLASSTYPE)
+#else //def PL_PYTHON_VERSION 
+#define DECLARE_OBJECT_PP(PPCLASSTYPE, CLASSTYPE)                       \
+        DECLARE_OBJECT_PP_SERIALIZE(PPCLASSTYPE, CLASSTYPE)
+#endif //def PL_PYTHON_VERSION 
 
+
 //#####  PLearn::Object  ######################################################
 
 /**

Modified: trunk/plearn/python/PythonObjectWrapper.cc
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.cc	2007-07-18 17:06:05 UTC (rev 7793)
+++ trunk/plearn/python/PythonObjectWrapper.cc	2007-07-18 17:27:35 UTC (rev 7794)
@@ -260,7 +260,8 @@
     return m;
 }
 
-VMat ConvertFromPyObject<VMat>::convert(PyObject* pyobj, bool print_traceback)
+//VMat ConvertFromPyObject<VMat>::convert(PyObject* pyobj, bool print_traceback)
+PP<VMatrix> ConvertFromPyObject<PP<VMatrix> >::convert(PyObject* pyobj, bool print_traceback)
 {
     PLASSERT(pyobj);
     if(PyObject_HasAttrString(pyobj, "_cptr"))
@@ -268,7 +269,7 @@
             ConvertFromPyObject<Object*>::convert(pyobj, print_traceback));
     Mat m;
     ConvertFromPyObject<Mat>::convert(pyobj, m, print_traceback);
-    return m;
+    return VMat(m);
 }
 
 PythonObjectWrapper ConvertFromPyObject<PythonObjectWrapper>::convert(PyObject* pyobj, bool print_traceback)
@@ -802,12 +803,13 @@
     return (PyObject*)pyarr;
 }
 
-PyObject* ConvertToPyObject<VMat>::newPyObject(const VMat& vm)
+//PyObject* ConvertToPyObject<VMat>::newPyObject(const VMat& vm)
+PyObject* ConvertToPyObject<PP<VMatrix> >::newPyObject(const PP<VMatrix>& vm)
 {
     if (vm.isNull())
         return ConvertToPyObject<Mat>::newPyObject(Mat());
     else
-        return ConvertToPyObject<Mat>::newPyObject(vm.toMat());
+        return ConvertToPyObject<Mat>::newPyObject(vm->toMat());
     //return ConvertToPyObject<Object*>::newPyObject(static_cast<Object*>(vm));
 }
 

Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2007-07-18 17:06:05 UTC (rev 7793)
+++ trunk/plearn/python/PythonObjectWrapper.h	2007-07-18 17:27:35 UTC (rev 7794)
@@ -61,6 +61,11 @@
 #include <plearn/base/TypeTraits.h>
 #include <plearn/base/tuple.h>
 
+// from boost
+#include <boost/static_assert.hpp>
+#include <boost/type_traits.hpp>
+
+
 #ifdef USEFLOAT
 #define tReal tFloat32
 #else
@@ -71,7 +76,7 @@
 
 class PythonObjectWrapper;                   // Forward-declare
 class Object;
-class VMat;
+class VMatrix;
 
 //! Used for error reporting.  If 'print_traceback' is true, a full
 //! Python traceback is printed to stderr.  Otherwise, raise PLERROR.
@@ -97,13 +102,7 @@
 template <class T>
 struct ConvertFromPyObject
 { 
-    static T convert(PyObject*, bool print_traceback)
-    {
-        PLERROR("Cannot convert this object by value from python (type=%s).",
-                TypeTraits<T>::name().c_str());
-        return T();//to silence compiler
-    }
-
+    static T convert(PyObject* x, bool print_traceback);
 };
 
 template <>
@@ -190,6 +189,32 @@
     static Object* convert(PyObject*, bool print_traceback);
 };
 
+
+///***///***
+// PARTIAL specialisation from T*.  Assume Object*.
+// TODO: fix this assumption
+template <class T>
+struct ConvertFromPyObject<T*>
+{
+    static T* convert(PyObject* pyobj, bool print_traceback)
+    {
+        // Compile-time assertion:
+        BOOST_STATIC_ASSERT((boost::is_base_of<Object, T>::value));
+        //N.B.: If this assertion fails, it probably means that you are trying
+        //      to retrieve a pointer to something that is not an Object from
+        //      python.  Only Object pointers are supported.
+
+        Object* obj = ConvertFromPyObject<Object*>::convert(pyobj, print_traceback);
+        if (T* tobj = dynamic_cast<T*>(obj))
+            return tobj;
+        else
+            PLERROR("Cannot convert object from python (type='%s').",
+                    TypeTraits<T*>::name().c_str());
+        return 0;                            // Silence compiler
+    }
+};
+///***///***
+
 template <>
 struct ConvertFromPyObject<Vec>
 {
@@ -213,10 +238,10 @@
 };
 
 template <>
-struct ConvertFromPyObject<VMat>
+struct ConvertFromPyObject<PP<VMatrix> >
 {
     // Return new MemoryVMatrix
-    static VMat convert(PyObject*, bool print_traceback);
+    static PP<VMatrix> convert(PyObject*, bool print_traceback);
 };
 
 template <>
@@ -271,15 +296,40 @@
 {
     static PyObject* newPyObject(const T& x)
     {
-        PLERROR("Cannot convert type %s by value to python",
+        PLERROR("Cannot convert type %s by value to python.",
                 TypeTraits<T>::name().c_str());
         return 0;//shut up compiler
     }
 };
 
+// Specialization for Object*
 template<> struct ConvertToPyObject<Object*>
 { static PyObject* newPyObject(const Object* x); };
 
+///***///***
+// Specialization for General T*.  Attempt to cast into Object*.  If that works
+// we're all set; for specific pointer types (e.g.  map<U,V>* and vector<T>*,
+// below, since they are more specialized they should kick in before this one.
+template <typename T>
+struct ConvertToPyObject<T*>
+{
+    static PyObject* newPyObject(const T* x)
+    {
+        if(!x) // null ptr. becomes None
+            return PythonObjectWrapper::newPyObject();
+
+        if (const Object* objx = dynamic_cast<const Object*>(x))
+            return ConvertToPyObject<Object*>::newPyObject(objx);
+
+        PLERROR("Cannot convert type %s by value to python",
+                TypeTraits<T*>::name().c_str());
+        return 0;//shut up compiler
+    }
+};
+
+// Other specializations
+///***///***
+
 template<> struct ConvertToPyObject<bool>
 { static PyObject* newPyObject(const bool& x); };
 
@@ -331,8 +381,8 @@
 //! are lost when converting to Python.
 //!
 //! @TODO  Must provide a complete Python wrapper over VMatrix objects
-template<> struct ConvertToPyObject<VMat>
-{ static PyObject* newPyObject(const VMat& vm); };
+template<> struct ConvertToPyObject<PP<VMatrix> >
+{ static PyObject* newPyObject(const PP<VMatrix>& vm); };
     
 //! Generic PP: wrap pointed object
 template<class T> struct ConvertToPyObject<PP<T> >
@@ -624,7 +674,44 @@
 
 //#####  ConvertFromPyObject Implementations  #################################
 
+template<class U, bool is_enum> 
+struct StaticConvertEnumFromPyObject 
+{
+    static U convert(PyObject* x, bool print_traceback)
+    {
+        PLERROR("Cannot convert this object by value from python (type=%s).",
+                TypeTraits<U>::name().c_str());
+        return U();//to silence compiler
+    }
+};
+    
+template<class U>
+struct StaticConvertEnumFromPyObject<U, true>
+{
+    static U convert(PyObject* x, bool print_traceback)
+    {
+        return static_cast<U>(
+            ConvertFromPyObject<int>::convert(x, print_traceback));
+    }
+};
+
 template <class T>
+T ConvertFromPyObject<T>::convert(PyObject* x, bool print_traceback)
+{
+    return StaticConvertEnumFromPyObject<T, boost::is_enum<T>::value>
+        ::convert(x, print_traceback);
+    /*
+    if(boost::is_enum<T>::value)
+        return ConvertFromPyObject<int>::convert(x, print_traceback);
+    
+    PLERROR("Cannot convert this object by value from python (type=%s).",
+            TypeTraits<T>::name().c_str());
+    return T();//to silence compiler
+    */
+}
+
+
+template <class T>
 PP<T> ConvertFromPyObject<PP<T> >::convert(PyObject* pyobj, bool print_traceback)
 {
     PLASSERT( pyobj );



From lamblin at mail.berlios.de  Wed Jul 18 20:49:56 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 18 Jul 2007 20:49:56 +0200
Subject: [Plearn-commits] r7795 - trunk/plearn/base
Message-ID: <200707181849.l6IInuCL024827@sheep.berlios.de>

Author: lamblin
Date: 2007-07-18 20:49:56 +0200 (Wed, 18 Jul 2007)
New Revision: 7795

Removed:
   trunk/plearn/base/pstdint.h
Log:
This file was not needed in fact.


Deleted: trunk/plearn/base/pstdint.h
===================================================================
--- trunk/plearn/base/pstdint.h	2007-07-18 17:27:35 UTC (rev 7794)
+++ trunk/plearn/base/pstdint.h	2007-07-18 18:49:56 UTC (rev 7795)
@@ -1,679 +0,0 @@
-/*  A portable stdint.h
- *
- *  Copyright (c) 2005-2007 Paul Hsieh
- *
- *  Redistribution and use in source and binary forms, with or without
- *  modification, are permitted provided that the following conditions
- *  are met:
- *
- *      Redistributions of source code must retain the above copyright
- *      notice, this list of conditions and the following disclaimer.
- *
- *      Redistributions in binary form must not misrepresent the orignal
- *      source in the documentation and/or other materials provided
- *      with the distribution.
- *
- *      The names of the authors nor its contributors may be used to
- *      endorse or promote products derived from this software without
- *      specific prior written permission.
- *
- *  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- *  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- *  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
- *  FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
- *  COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
- *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
- *  (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
- *  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
- *  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
- *  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- *  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
- *  OF THE POSSIBILITY OF SUCH DAMAGE.
- *
- ****************************************************************************
- *
- *  Version 0.1.8
- *
- *  The ANSI C standard committee, for the C99 standard, specified the
- *  inclusion of a new standard include file called stdint.h.  This is
- *  a very useful and long desired include file which contains several
- *  very precise definitions for integer scalar types that is
- *  critically important for making portable several classes of
- *  applications including cryptography, hashing, variable length
- *  integer libraries and so on.  But for most developers its likely
- *  useful just for programming sanity.
- *
- *  The problem is that most compiler vendors have decided not to
- *  implement the C99 standard, and the next C++ language standard
- *  (which has a lot more mindshare these days) will be a long time in
- *  coming and its unknown whether or not it will include stdint.h or
- *  how much adoption it will have.  Either way, it will be a long time
- *  before all compilers come with a stdint.h and it also does nothing
- *  for the extremely large number of compilers available today which
- *  do not include this file, or anything comparable to it.
- *
- *  So that's what this file is all about.  Its an attempt to build a
- *  single universal include file that works on as many platforms as
- *  possible to deliver what stdint.h is supposed to.  A few things
- *  that should be noted about this file:
- *
- *    1) It is not guaranteed to be portable and/or present an identical
- *       interface on all platforms.  The extreme variability of the
- *       ANSI C standard makes this an impossibility right from the
- *       very get go. Its really only meant to be useful for the vast
- *       majority of platforms that possess the capability of
- *       implementing usefully and precisely defined, standard sized
- *       integer scalars.  Systems which are not intrinsically 2s
- *       complement may produce invalid constants.
- *
- *    2) There is an unavoidable use of non-reserved symbols.
- *
- *    3) Other standard include files are invoked.
- *
- *    4) This file may come in conflict with future platforms that do
- *       include stdint.h.  The hope is that one or the other can be
- *       used with no real difference.
- *
- *    5) In the current verison, if your platform can't represent
- *       int32_t, int16_t and int8_t, it just dumps out with a compiler
- *       error.
- *
- *    6) 64 bit integers may or may not be defined.  Test for their
- *       presence with the test: #ifdef INT64_MAX or #ifdef UINT64_MAX.
- *       Note that this is different from the C99 specification which
- *       requires the existence of 64 bit support in the compiler.  If
- *       this is not defined for your platform, yet it is capable of
- *       dealing with 64 bits then it is because this file has not yet
- *       been extended to cover all of your system's capabilities.
- *
- *    7) (u)intptr_t may or may not be defined.  Test for its presence
- *       with the test: #ifdef PTRDIFF_MAX.  If this is not defined
- *       for your platform, then it is because this file has not yet
- *       been extended to cover all of your system's capabilities, not
- *       because its optional.
- *
- *    8) The following might not been defined even if your platform is
- *       capable of defining it:
- *
- *       WCHAR_MIN
- *       WCHAR_MAX
- *       (u)int64_t
- *       PTRDIFF_MIN
- *       PTRDIFF_MAX
- *       (u)intptr_t
- *
- *    9) The following have not been defined:
- *
- *       WINT_MIN
- *       WINT_MAX
- *
- *   10) The criteria for defining (u)int_least(*)_t isn't clear,
- *       except for systems which don't have a type that precisely
- *       defined 8, 16, or 32 bit types (which this include file does
- *       not support anyways). Default definitions have been given.
- *
- *   11) The criteria for defining (u)int_fast(*)_t isn't something I
- *       would trust to any particular compiler vendor or the ANSI C
- *       committee.  It is well known that "compatible systems" are
- *       commonly created that have very different performance
- *       characteristics from the systems they are compatible with,
- *       especially those whose vendors make both the compiler and the
- *       system.  Default definitions have been given, but its strongly
- *       recommended that users never use these definitions for any
- *       reason (they do *NOT* deliver any serious guarantee of
- *       improved performance -- not in this file, nor any vendor's
- *       stdint.h).
- *
- *   12) The following macros:
- *
- *       PRINTF_INTMAX_MODIFIER
- *       PRINTF_INT64_MODIFIER
- *       PRINTF_INT32_MODIFIER
- *       PRINTF_INT16_MODIFIER
- *       PRINTF_LEAST64_MODIFIER
- *       PRINTF_LEAST32_MODIFIER
- *       PRINTF_LEAST16_MODIFIER
- *       PRINTF_INTPTR_MODIFIER
- *
- *       are strings which have been defined as the modifiers required
- *       for the "d", "u" and "x" printf formats to correctly output
- *       (u)intmax_t, (u)int64_t, (u)int32_t, (u)int16_t, (u)least64_t,
- *       (u)least32_t, (u)least16_t and (u)intptr_t types respectively.
- *       PRINTF_INTPTR_MODIFIER is not defined for some systems which
- *       provide their own stdint.h.  PRINTF_INT64_MODIFIER is not
- *       defined if INT64_MAX is not defined.  These are an extension
- *       beyond what C99 specifies must be in stdint.h.
- *
- *       In addition, the following macros are defined:
- *
- *       PRINTF_INTMAX_HEX_WIDTH
- *       PRINTF_INT64_HEX_WIDTH
- *       PRINTF_INT32_HEX_WIDTH
- *       PRINTF_INT16_HEX_WIDTH
- *       PRINTF_INT8_HEX_WIDTH
- *       PRINTF_INTMAX_DEC_WIDTH
- *       PRINTF_INT64_DEC_WIDTH
- *       PRINTF_INT32_DEC_WIDTH
- *       PRINTF_INT16_DEC_WIDTH
- *       PRINTF_INT8_DEC_WIDTH
- *
- *       Which specifies the maximum number of characters required to
- *       print the number of that type in either hexadecimal or decimal.
- *       These are an extension beyond what C99 specifies must be in
- *       stdint.h.
- *
- *  Compilers tested (all with 0 warnings at their highest respective
- *  settings): Borland Turbo C 2.0, WATCOM C/C++ 11.0 (16 bits and 32
- *  bits), Microsoft Visual C++ 6.0 (32 bit), Microsoft Visual Studio
- *  .net (VC7), Intel C++ 4.0, GNU gcc v3.3.3
- *
- *  This file should be considered a work in progress.  Suggestions for
- *  improvements, especially those which increase coverage are strongly
- *  encouraged.
- *
- *  Acknowledgements
- *
- *  The following people have made significant contributions to the
- *  development and testing of this file:
- *
- *  Chris Howie
- *  John Steele Scott
- *  Dave Thorup
- *
- */
-
-#include <stddef.h>
-#include <limits.h>
-#include <signal.h>
-
-/*
- *  For gcc with _STDINT_H, fill in the PRINTF_INT*_MODIFIER macros, and
- *  do nothing else.  On the Mac OS X version of gcc this is _STDINT_H_.
- */
-
-#if ((defined(__STDC__) && __STDC__ && __STDC_VERSION__ >= 199901L) || (defined (__WATCOMC__) && (defined (_STDINT_H_INCLUDED) || __WATCOMC__ >= 1250)) || (defined(__GNUC__) && (defined(_STDINT_H) || defined(_STDINT_H_)) )) && !defined (_PSTDINT_H_INCLUDED)
-#include <stdint.h>
-#define _PSTDINT_H_INCLUDED
-# ifndef PRINTF_INT64_MODIFIER
-#  define PRINTF_INT64_MODIFIER "ll"
-# endif
-# ifndef PRINTF_INT32_MODIFIER
-#  define PRINTF_INT32_MODIFIER "l"
-# endif
-# ifndef PRINTF_INT16_MODIFIER
-#  define PRINTF_INT16_MODIFIER "h"
-# endif
-# ifndef PRINTF_INTMAX_MODIFIER
-#  define PRINTF_INTMAX_MODIFIER PRINTF_INT64_MODIFIER
-# endif
-# ifndef PRINTF_INT64_HEX_WIDTH
-#  define PRINTF_INT64_HEX_WIDTH "16"
-# endif
-# ifndef PRINTF_INT32_HEX_WIDTH
-#  define PRINTF_INT32_HEX_WIDTH "8"
-# endif
-# ifndef PRINTF_INT16_HEX_WIDTH
-#  define PRINTF_INT16_HEX_WIDTH "4"
-# endif
-# ifndef PRINTF_INT8_HEX_WIDTH
-#  define PRINTF_INT8_HEX_WIDTH "2"
-# endif
-# ifndef PRINTF_INT64_DEC_WIDTH
-#  define PRINTF_INT64_DEC_WIDTH "20"
-# endif
-# ifndef PRINTF_INT32_DEC_WIDTH
-#  define PRINTF_INT32_DEC_WIDTH "10"
-# endif
-# ifndef PRINTF_INT16_DEC_WIDTH
-#  define PRINTF_INT16_DEC_WIDTH "5"
-# endif
-# ifndef PRINTF_INT8_DEC_WIDTH
-#  define PRINTF_INT8_DEC_WIDTH "3"
-# endif
-# ifndef PRINTF_INTMAX_HEX_WIDTH
-#  define PRINTF_INTMAX_HEX_WIDTH PRINTF_INT64_HEX_WIDTH
-# endif
-# ifndef PRINTF_INTMAX_DEC_WIDTH
-#  define PRINTF_INTMAX_DEC_WIDTH PRINTF_INT64_DEC_WIDTH
-# endif
-#endif
-
-#ifndef _PSTDINT_H_INCLUDED
-#define _PSTDINT_H_INCLUDED
-
-#ifndef SIZE_MAX
-# define SIZE_MAX (~(size_t)0)
-#endif
-
-/*
- *  Deduce the type assignments from limits.h under the assumption that
- *  integer sizes in bits are powers of 2, and follow the ANSI
- *  definitions.
- */
-
-#ifndef UINT8_MAX
-# define UINT8_MAX 0xff
-#endif
-#ifndef uint8_t
-# if (UCHAR_MAX == UINT8_MAX) || defined (S_SPLINT_S)
-    typedef unsigned char uint8_t;
-#   define UINT8_C(v) ((uint8_t) v)
-# else
-#   error "Platform not supported"
-# endif
-#endif
-
-#ifndef INT8_MAX
-# define INT8_MAX 0x7f
-#endif
-#ifndef INT8_MIN
-# define INT8_MIN INT8_C(0x80)
-#endif
-#ifndef int8_t
-# if (SCHAR_MAX == INT8_MAX) || defined (S_SPLINT_S)
-    typedef signed char int8_t;
-#   define INT8_C(v) ((int8_t) v)
-# else
-#   error "Platform not supported"
-# endif
-#endif
-
-#ifndef UINT16_MAX
-# define UINT16_MAX 0xffff
-#endif
-#ifndef uint16_t
-#if (UINT_MAX == UINT16_MAX) || defined (S_SPLINT_S)
-  typedef unsigned int uint16_t;
-# ifndef PRINTF_INT16_MODIFIER
-#  define PRINTF_INT16_MODIFIER ""
-# endif
-# define UINT16_C(v) ((uint16_t) (v))
-#elif (USHRT_MAX == UINT16_MAX)
-  typedef unsigned short uint16_t;
-# define UINT16_C(v) ((uint16_t) (v))
-# ifndef PRINTF_INT16_MODIFIER
-#  define PRINTF_INT16_MODIFIER "h"
-# endif
-#else
-#error "Platform not supported"
-#endif
-#endif
-
-#ifndef INT16_MAX
-# define INT16_MAX 0x7fff
-#endif
-#ifndef INT16_MIN
-# define INT16_MIN INT16_C(0x8000)
-#endif
-#ifndef int16_t
-#if (INT_MAX == INT16_MAX) || defined (S_SPLINT_S)
-  typedef signed int int16_t;
-# define INT16_C(v) ((int16_t) (v))
-# ifndef PRINTF_INT16_MODIFIER
-#  define PRINTF_INT16_MODIFIER ""
-# endif
-#elif (SHRT_MAX == INT16_MAX)
-  typedef signed short int16_t;
-# define INT16_C(v) ((int16_t) (v))
-# ifndef PRINTF_INT16_MODIFIER
-#  define PRINTF_INT16_MODIFIER "h"
-# endif
-#else
-#error "Platform not supported"
-#endif
-#endif
-
-#ifndef UINT32_MAX
-# define UINT32_MAX (0xffffffffUL)
-#endif
-#ifndef uint32_t
-#if (ULONG_MAX == UINT32_MAX) || defined (S_SPLINT_S)
-  typedef unsigned long uint32_t;
-# define UINT32_C(v) v ## UL
-# ifndef PRINTF_INT32_MODIFIER
-#  define PRINTF_INT32_MODIFIER "l"
-# endif
-#elif (UINT_MAX == UINT32_MAX)
-  typedef unsigned int uint32_t;
-# ifndef PRINTF_INT32_MODIFIER
-#  define PRINTF_INT32_MODIFIER ""
-# endif
-# define UINT32_C(v) v ## U
-#elif (USHRT_MAX == UINT32_MAX)
-  typedef unsigned short uint32_t;
-# define UINT32_C(v) ((unsigned short) (v))
-# ifndef PRINTF_INT32_MODIFIER
-#  define PRINTF_INT32_MODIFIER ""
-# endif
-#else
-#error "Platform not supported"
-#endif
-#endif
-
-#ifndef INT32_MAX
-# define INT32_MAX (0x7fffffffL)
-#endif
-#ifndef INT32_MIN
-# define INT32_MIN INT32_C(0x80000000)
-#endif
-#ifndef int32_t
-#if (LONG_MAX == INT32_MAX) || defined (S_SPLINT_S)
-  typedef signed long int32_t;
-# define INT32_C(v) v ## L
-# ifndef PRINTF_INT32_MODIFIER
-#  define PRINTF_INT32_MODIFIER "l"
-# endif
-#elif (INT_MAX == INT32_MAX)
-  typedef signed int int32_t;
-# define INT32_C(v) v
-# ifndef PRINTF_INT32_MODIFIER
-#  define PRINTF_INT32_MODIFIER ""
-# endif
-#elif (SHRT_MAX == INT32_MAX)
-  typedef signed short int32_t;
-# define INT32_C(v) ((short) (v))
-# ifndef PRINTF_INT32_MODIFIER
-#  define PRINTF_INT32_MODIFIER ""
-# endif
-#else
-#error "Platform not supported"
-#endif
-#endif
-
-/*
- *  The macro stdint_int64_defined is temporarily used to record
- *  whether or not 64 integer support is available.  It must be
- *  defined for any 64 integer extensions for new platforms that are
- *  added.
- */
-
-#undef stdint_int64_defined
-#if (defined(__STDC__) && defined(__STDC_VERSION__)) || defined (S_SPLINT_S)
-# if (__STDC__ && __STDC_VERSION >= 199901L) || defined (S_SPLINT_S)
-#  define stdint_int64_defined
-   typedef long long int64_t;
-   typedef unsigned long long uint64_t;
-#  define UINT64_C(v) v ## ULL
-#  define  INT64_C(v) v ## LL
-#  ifndef PRINTF_INT64_MODIFIER
-#   define PRINTF_INT64_MODIFIER "ll"
-#  endif
-# endif
-#endif
-
-#if !defined (stdint_int64_defined)
-# if defined(__GNUC__)
-#  define stdint_int64_defined
-   __extension__ typedef long long int64_t;
-   __extension__ typedef unsigned long long uint64_t;
-#  define UINT64_C(v) v ## ULL
-#  define  INT64_C(v) v ## LL
-#  ifndef PRINTF_INT64_MODIFIER
-#   define PRINTF_INT64_MODIFIER "ll"
-#  endif
-# elif defined(__MWERKS__) || defined (__SUNPRO_C) || defined (__SUNPRO_CC) || defined (__APPLE_CC__) || defined (_LONG_LONG) || defined (_CRAYC) || defined (S_SPLINT_S)
-#  define stdint_int64_defined
-   typedef long long int64_t;
-   typedef unsigned long long uint64_t;
-#  define UINT64_C(v) v ## ULL
-#  define  INT64_C(v) v ## LL
-#  ifndef PRINTF_INT64_MODIFIER
-#   define PRINTF_INT64_MODIFIER "ll"
-#  endif
-# elif (defined(__WATCOMC__) && defined(__WATCOM_INT64__)) || (defined(_MSC_VER) && _INTEGRAL_MAX_BITS >= 64) || (defined (__BORLANDC__) && __BORLANDC__ > 0x460) || defined (__alpha) || defined (__DECC)
-#  define stdint_int64_defined
-   typedef __int64 int64_t;
-   typedef unsigned __int64 uint64_t;
-#  define UINT64_C(v) v ## UI64
-#  define  INT64_C(v) v ## I64
-#  ifndef PRINTF_INT64_MODIFIER
-#   define PRINTF_INT64_MODIFIER "I64"
-#  endif
-# endif
-#endif
-
-#if !defined (LONG_LONG_MAX) && defined (INT64_C)
-# define LONG_LONG_MAX INT64_C (9223372036854775807)
-#endif
-#ifndef ULONG_LONG_MAX
-# define ULONG_LONG_MAX UINT64_C (18446744073709551615)
-#endif
-
-#if !defined (INT64_MAX) && defined (INT64_C)
-# define INT64_MAX INT64_C (9223372036854775807)
-#endif
-#if !defined (INT64_MIN) && defined (INT64_C)
-# define INT64_MIN INT64_C (-9223372036854775808)
-#endif
-#if !defined (UINT64_MAX) && defined (INT64_C)
-# define UINT64_MAX UINT64_C (18446744073709551615)
-#endif
-
-/*
- *  Width of hexadecimal for number field.
- */
-
-#ifndef PRINTF_INT64_HEX_WIDTH
-# define PRINTF_INT64_HEX_WIDTH "16"
-#endif
-#ifndef PRINTF_INT32_HEX_WIDTH
-# define PRINTF_INT32_HEX_WIDTH "8"
-#endif
-#ifndef PRINTF_INT16_HEX_WIDTH
-# define PRINTF_INT16_HEX_WIDTH "4"
-#endif
-#ifndef PRINTF_INT8_HEX_WIDTH
-# define PRINTF_INT8_HEX_WIDTH "2"
-#endif
-
-#ifndef PRINTF_INT64_DEC_WIDTH
-# define PRINTF_INT64_DEC_WIDTH "20"
-#endif
-#ifndef PRINTF_INT32_DEC_WIDTH
-# define PRINTF_INT32_DEC_WIDTH "10"
-#endif
-#ifndef PRINTF_INT16_DEC_WIDTH
-# define PRINTF_INT16_DEC_WIDTH "5"
-#endif
-#ifndef PRINTF_INT8_DEC_WIDTH
-# define PRINTF_INT8_DEC_WIDTH "3"
-#endif
-
-/*
- *  Ok, lets not worry about 128 bit integers for now.  Moore's law says
- *  we don't need to worry about that until about 2040 at which point
- *  we'll have bigger things to worry about.
- */
-
-#ifdef stdint_int64_defined
-  typedef int64_t intmax_t;
-  typedef uint64_t uintmax_t;
-# define  INTMAX_MAX   INT64_MAX
-# define  INTMAX_MIN   INT64_MIN
-# define UINTMAX_MAX  UINT64_MAX
-# define UINTMAX_C(v) UINT64_C(v)
-# define  INTMAX_C(v)  INT64_C(v)
-# ifndef PRINTF_INTMAX_MODIFIER
-#   define PRINTF_INTMAX_MODIFIER PRINTF_INT64_MODIFIER
-# endif
-# ifndef PRINTF_INTMAX_HEX_WIDTH
-#  define PRINTF_INTMAX_HEX_WIDTH PRINTF_INT64_HEX_WIDTH
-# endif
-# ifndef PRINTF_INTMAX_DEC_WIDTH
-#  define PRINTF_INTMAX_DEC_WIDTH PRINTF_INT64_DEC_WIDTH
-# endif
-#else
-  typedef int32_t intmax_t;
-  typedef uint32_t uintmax_t;
-# define  INTMAX_MAX   INT32_MAX
-# define UINTMAX_MAX  UINT32_MAX
-# define UINTMAX_C(v) UINT32_C(v)
-# define  INTMAX_C(v)  INT32_C(v)
-# ifndef PRINTF_INTMAX_MODIFIER
-#   define PRINTF_INTMAX_MODIFIER PRINTF_INT32_MODIFIER
-# endif
-# ifndef PRINTF_INTMAX_HEX_WIDTH
-#  define PRINTF_INTMAX_HEX_WIDTH PRINTF_INT32_HEX_WIDTH
-# endif
-# ifndef PRINTF_INTMAX_DEC_WIDTH
-#  define PRINTF_INTMAX_DEC_WIDTH PRINTF_INT32_DEC_WIDTH
-# endif
-#endif
-
-/*
- *  Because this file currently only supports platforms which have
- *  precise powers of 2 as bit sizes for the default integers, the
- *  least definitions are all trivial.  Its possible that a future
- *  version of this file could have different definitions.
- */
-
-#ifndef stdint_least_defined
-  typedef   int8_t   int_least8_t;
-  typedef  uint8_t  uint_least8_t;
-  typedef  int16_t  int_least16_t;
-  typedef uint16_t uint_least16_t;
-  typedef  int32_t  int_least32_t;
-  typedef uint32_t uint_least32_t;
-# define PRINTF_LEAST32_MODIFIER PRINTF_INT32_MODIFIER
-# define PRINTF_LEAST16_MODIFIER PRINTF_INT16_MODIFIER
-# define  UINT_LEAST8_MAX  UINT8_MAX
-# define   INT_LEAST8_MAX   INT8_MAX
-# define UINT_LEAST16_MAX UINT16_MAX
-# define  INT_LEAST16_MAX  INT16_MAX
-# define UINT_LEAST32_MAX UINT32_MAX
-# define  INT_LEAST32_MAX  INT32_MAX
-# define   INT_LEAST8_MIN   INT8_MIN
-# define  INT_LEAST16_MIN  INT16_MIN
-# define  INT_LEAST32_MIN  INT32_MIN
-# ifdef stdint_int64_defined
-    typedef  int64_t  int_least64_t;
-    typedef uint64_t uint_least64_t;
-#   define PRINTF_LEAST64_MODIFIER PRINTF_INT64_MODIFIER
-#   define UINT_LEAST64_MAX UINT64_MAX
-#   define  INT_LEAST64_MAX  INT64_MAX
-#   define  INT_LEAST64_MIN  INT64_MIN
-# endif
-#endif
-#undef stdint_least_defined
-
-/*
- *  The ANSI C committee pretending to know or specify anything about
- *  performance is the epitome of misguided arrogance.  The mandate of
- *  this file is to *ONLY* ever support that absolute minimum
- *  definition of the fast integer types, for compatibility purposes.
- *  No extensions, and no attempt to suggest what may or may not be a
- *  faster integer type will ever be made in this file.  Developers are
- *  warned to stay away from these types when using this or any other
- *  stdint.h.
- */
-
-typedef   int_least8_t   int_fast8_t;
-typedef  uint_least8_t  uint_fast8_t;
-typedef  int_least16_t  int_fast16_t;
-typedef uint_least16_t uint_fast16_t;
-typedef  int_least32_t  int_fast32_t;
-typedef uint_least32_t uint_fast32_t;
-#define  UINT_FAST8_MAX  UINT_LEAST8_MAX
-#define   INT_FAST8_MAX   INT_LEAST8_MAX
-#define UINT_FAST16_MAX UINT_LEAST16_MAX
-#define  INT_FAST16_MAX  INT_LEAST16_MAX
-#define UINT_FAST32_MAX UINT_LEAST32_MAX
-#define  INT_FAST32_MAX  INT_LEAST32_MAX
-#define   INT_FAST8_MIN   IN_LEASTT8_MIN
-#define  INT_FAST16_MIN  INT_LEAST16_MIN
-#define  INT_FAST32_MIN  INT_LEAST32_MIN
-#ifdef stdint_int64_defined
-  typedef  int_least64_t  int_fast64_t;
-  typedef uint_least64_t uint_fast64_t;
-# define UINT_FAST64_MAX UINT_LEAST64_MAX
-# define  INT_FAST64_MAX  INT_LEAST64_MAX
-# define  INT_FAST64_MIN  INT_LEAST64_MIN
-#endif
-
-#undef stdint_int64_defined
-
-/*
- *  Whatever piecemeal, per compiler thing we can do about the wchar_t
- *  type limits.
- */
-
-#if defined(__WATCOMC__) || defined(_MSC_VER) || defined (__GNUC__)
-# include <wchar.h>
-# ifndef WCHAR_MIN
-#  define WCHAR_MIN 0
-# endif
-# ifndef WCHAR_MAX
-#  define WCHAR_MAX ((wchar_t)-1)
-# endif
-#endif
-
-/*
- *  Whatever piecemeal, per compiler/platform thing we can do about the
- *  (u)intptr_t types and limits.
- */
-
-#if defined (_MSC_VER) && defined (_UINTPTR_T_DEFINED)
-# define STDINT_H_UINTPTR_T_DEFINED
-#endif
-
-#ifndef STDINT_H_UINTPTR_T_DEFINED
-# if defined (__alpha__) || defined (__ia64__) || defined (__x86_64__) || defined (_WIN64)
-#  define stdint_intptr_bits 64
-# elif defined (__WATCOMC__) || defined (__TURBOC__)
-#  if defined(__TINY__) || defined(__SMALL__) || defined(__MEDIUM__)
-#    define stdint_intptr_bits 16
-#  else
-#    define stdint_intptr_bits 32
-#  endif
-# elif defined (__i386__) || defined (_WIN32) || defined (WIN32)
-#  define stdint_intptr_bits 32
-# elif defined (__INTEL_COMPILER)
-/* TODO -- what will Intel do about x86-64? */
-# endif
-
-# ifdef stdint_intptr_bits
-#  define stdint_intptr_glue3_i(a,b,c)  a##b##c
-#  define stdint_intptr_glue3(a,b,c)    stdint_intptr_glue3_i(a,b,c)
-#  ifndef PRINTF_INTPTR_MODIFIER
-#    define PRINTF_INTPTR_MODIFIER      stdint_intptr_glue3(PRINTF_INT,stdint_intptr_bits,_MODIFIER)
-#  endif
-#  ifndef PTRDIFF_MAX
-#    define PTRDIFF_MAX                 stdint_intptr_glue3(INT,stdint_intptr_bits,_MAX)
-#  endif
-#  ifndef PTRDIFF_MIN
-#    define PTRDIFF_MIN                 stdint_intptr_glue3(INT,stdint_intptr_bits,_MIN)
-#  endif
-#  ifndef UINTPTR_MAX
-#    define UINTPTR_MAX                 stdint_intptr_glue3(UINT,stdint_intptr_bits,_MAX)
-#  endif
-#  ifndef INTPTR_MAX
-#    define INTPTR_MAX                  stdint_intptr_glue3(INT,stdint_intptr_bits,_MAX)
-#  endif
-#  ifndef INTPTR_MIN
-#    define INTPTR_MIN                  stdint_intptr_glue3(INT,stdint_intptr_bits,_MIN)
-#  endif
-#  ifndef INTPTR_C
-#    define INTPTR_C(x)                 stdint_intptr_glue3(INT,stdint_intptr_bits,_C)(x)
-#  endif
-#  ifndef UINTPTR_C
-#    define UINTPTR_C(x)                stdint_intptr_glue3(UINT,stdint_intptr_bits,_C)(x)
-#  endif
-  typedef stdint_intptr_glue3(uint,stdint_intptr_bits,_t) uintptr_t;
-  typedef stdint_intptr_glue3( int,stdint_intptr_bits,_t)  intptr_t;
-# else
-/* TODO -- This following is likely wrong for some platforms, and does
-   nothing for the definition of uintptr_t. */
-  typedef ptrdiff_t intptr_t;
-# endif
-# define STDINT_H_UINTPTR_T_DEFINED
-#endif
-
-/*
- *  Assumes sig_atomic_t is signed and we have a 2s complement machine.
- */
-
-#ifndef SIG_ATOMIC_MAX
-# define SIG_ATOMIC_MAX ((((sig_atomic_t) 1) << (sizeof (sig_atomic_t)*CHAR_BIT-1)) - 1)
-#endif
-
-#endif



From tihocan at mail.berlios.de  Wed Jul 18 21:54:21 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 18 Jul 2007 21:54:21 +0200
Subject: [Plearn-commits] r7796 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200707181954.l6IJsLRd028583@sheep.berlios.de>

Author: tihocan
Date: 2007-07-18 21:54:20 +0200 (Wed, 18 Jul 2007)
New Revision: 7796

Added:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.h
Log:
SMP implementation of stochastic gradient

Added: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2007-07-18 18:49:56 UTC (rev 7795)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2007-07-18 19:54:20 UTC (rev 7796)
@@ -0,0 +1,1329 @@
+// -*- C++ -*-
+
+// NatGradSMPNNet.cc
+//
+// Copyright (C) 2007 Yoshua Bengio
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Yoshua Bengio
+
+/*! \file NatGradSMPNNet.cc */
+
+
+#include "NatGradSMPNNet.h"
+#include <plearn/math/pl_erf.h>
+
+#include <sys/ipc.h>
+#include <sys/sem.h>
+#include <sys/shm.h>
+
+namespace PLearn {
+using namespace std;
+
+union semun {
+    int val;                    /*!< value for SETVAL */
+    struct semid_ds *buf;       /*!< buffer for IPC_STAT, IPC_SET */
+    unsigned short int *array;  /*!< array for GETALL, SETALL */
+    struct seminfo *__buf;      /*!< buffer for IPC_INFO */
+};
+
+PLEARN_IMPLEMENT_OBJECT(
+    NatGradSMPNNet,
+    "Multi-layer neural network trained with an efficient Natural Gradient optimization",
+    "A separate covariance matrix is estimated for the gradients associated with the\n"
+    "the input weights of each neuron, and a covariance matrix between the gradients\n"
+    "on the neurons is also computed. These are combined to obtained an adjusted gradient\n"
+    "on all the parameters. The class GradientCorrector embodies the adjustment algorithm.\n"
+    "Users may specify different options for the estimator that is used for correcting\n"
+    "the neurons gradients and for the estimator that is used for correcting the\n"
+    "parameters gradients (separately for each neuron).\n"
+    );
+
+NatGradSMPNNet::NatGradSMPNNet()
+    : noutputs(-1),
+      params_averaging_coeff(1.0),
+      params_averaging_freq(5),
+      init_lrate(0.01),
+      lrate_decay(0),
+      output_layer_L1_penalty_factor(0.0),
+      output_layer_lrate_scale(1),
+      minibatch_size(1),
+      output_type("NLL"),
+      input_size_lrate_normalization_power(0),
+      lrate_scale_factor(3),
+      lrate_scale_factor_max_power(0),
+      lrate_scale_factor_min_power(0),
+      self_adjusted_scaling_and_bias(false),
+      target_mean_activation(-4), // 
+      target_stdev_activation(3), // 2.5% of the time we are above 1
+      verbosity(0),
+      //corr_profiling_start(0), 
+      //corr_profiling_end(0),
+      use_pvgrad(false),
+      pv_initial_stepsize(1e-6),
+      pv_acceleration(2),
+      pv_min_samples(2),
+      pv_required_confidence(0.80),
+      pv_random_sample_step(false),
+      pv_gradstats(new VecStatsCollector()),
+      n_layers(-1),
+      cumulative_training_time(0),
+      params_ptr(NULL),
+      params_id(-1),
+      nsteps(0),
+      semaphore_id(-1)
+{
+    random_gen = new PRandom();
+}
+
+void NatGradSMPNNet::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "noutputs", &NatGradSMPNNet::noutputs,
+                  OptionBase::buildoption,
+                  "Number of outputs of the neural network, which can be derived from  output_type and targetsize_\n");
+
+    declareOption(ol, "n_layers", &NatGradSMPNNet::n_layers,
+                  OptionBase::learntoption,
+                  "Number of layers of weights (ie. 2 for a neural net with one hidden layer).\n"
+                  "Needs not be specified explicitly (derived from hidden_layer_sizes).\n");
+
+    declareOption(ol, "hidden_layer_sizes", &NatGradSMPNNet::hidden_layer_sizes,
+                  OptionBase::buildoption,
+                  "Defines the architecture of the multi-layer neural network by\n"
+                  "specifying the number of hidden units in each hidden layer.\n");
+
+    declareOption(ol, "layer_sizes", &NatGradSMPNNet::layer_sizes,
+                  OptionBase::learntoption,
+                  "Derived from hidden_layer_sizes, inputsize_ and noutputs\n");
+
+    declareOption(ol, "cumulative_training_time", &NatGradSMPNNet::cumulative_training_time,
+                  OptionBase::learntoption,
+                  "Cumulative training time since age=0, in seconds.\n");
+
+    declareOption(ol, "layer_params", &NatGradSMPNNet::layer_params,
+                  OptionBase::learntoption,
+                  "Parameters used while training, for each layer, organized as follows: layer_params[i] \n"
+                  "is a matrix of dimension layer_sizes[i+1] x (layer_sizes[i]+1)\n"
+                  "containing the neuron biases in its first column.\n");
+
+    declareOption(ol, "activations_scaling", &NatGradSMPNNet::activations_scaling,
+                  OptionBase::learntoption,
+                  "Scaling coefficients for each neuron of each layer, if self_adjusted_scaling_and_bias:\n"
+                  " output = tanh(activations_scaling[layer][neuron] * (biases[layer][neuron] + weights[layer]*input[layer-1])\n");
+
+    declareOption(ol, "layer_mparams", &NatGradSMPNNet::layer_mparams,
+                  OptionBase::learntoption,
+                  "Test parameters for each layer, organized like layer_params.\n"
+                  "This is a moving average of layer_params, computed with\n"
+                  "coefficient params_averaging_coeff. Thus the mparams are\n"
+                  "a smoothed version of the params, and they are used only\n"
+                  "during testing.\n");
+
+    declareOption(ol, "params_averaging_coeff", &NatGradSMPNNet::params_averaging_coeff,
+                  OptionBase::buildoption,
+                  "Coefficient used to control how fast we forget old parameters\n"
+                  "in the moving average performed as follows:\n"
+                  "mparams <-- (1-params_averaging_coeff)mparams + params_averaging_coeff*params\n");
+
+    declareOption(ol, "params_averaging_freq", &NatGradSMPNNet::params_averaging_freq,
+                  OptionBase::buildoption,
+                  "How often (in terms of number of minibatches, i.e. weight updates)\n"
+                  "do we perform the moving average update calculation\n"
+                  "mparams <-- (1-params_averaging_coeff)mparams + params_averaging_coeff*params\n");
+
+    declareOption(ol, "init_lrate", &NatGradSMPNNet::init_lrate,
+                  OptionBase::buildoption,
+                  "Initial learning rate\n");
+
+    declareOption(ol, "lrate_decay", &NatGradSMPNNet::lrate_decay,
+                  OptionBase::buildoption,
+                  "Learning rate decay factor\n");
+
+    declareOption(ol, "output_layer_L1_penalty_factor",
+                  &NatGradSMPNNet::output_layer_L1_penalty_factor,
+                  OptionBase::buildoption,
+                  "Optional (default=0) factor of L1 regularization term, i.e.\n"
+                  "minimize L1_penalty_factor * sum_{ij} |weights(i,j)| during training.\n"
+                  "Gets multiplied by the learning rate. Only on output layer!!");
+
+    declareOption(ol, "output_layer_lrate_scale", &NatGradSMPNNet::output_layer_lrate_scale,
+                  OptionBase::buildoption,
+                  "Scaling factor of the learning rate for the output layer. Values less than 1"
+                  "mean that the output layer parameters have a smaller learning rate than the others.\n");
+
+    declareOption(ol, "minibatch_size", &NatGradSMPNNet::minibatch_size,
+                  OptionBase::buildoption,
+                  "Update the parameters only so often (number of examples).\n");
+
+    declareOption(ol, "neurons_natgrad_template", &NatGradSMPNNet::neurons_natgrad_template,
+                  OptionBase::buildoption,
+                  "Optional template GradientCorrector for the neurons gradient.\n"
+                  "If not provided, then the natural gradient correction\n"
+                  "on the neurons gradient is not performed.\n");
+
+    declareOption(ol, "neurons_natgrad_per_layer", 
+                  &NatGradSMPNNet::neurons_natgrad_per_layer,
+                  OptionBase::learntoption,
+                  "Vector of GradientCorrector objects for the gradient on the neurons of each layer.\n"
+                  "They are copies of the neuron_natgrad_template provided by the user.\n");
+
+    declareOption(ol, "params_natgrad_template", 
+                  &NatGradSMPNNet::params_natgrad_template,
+                  OptionBase::buildoption,
+                  "Optional template GradientCorrector object for the gradient of the parameters inside each neuron\n"
+                  "It is replicated in the params_natgrad vector, for each neuron\n"
+                  "If not provided, then the neuron-specific natural gradient estimator is not used.\n");
+
+    declareOption(ol, "params_natgrad_per_input_template",
+                  &NatGradSMPNNet::params_natgrad_per_input_template,
+                  OptionBase::buildoption,
+                  "Optional template GradientCorrector object for the gradient of the parameters of the first layer\n"
+                  "grouped based upon their input. It is replicated in the params_natgrad_per_group vector, for each group.\n"
+                  "If provided, overides the params_natgrad_template for the parameters of the first layer.\n");
+
+    declareOption(ol, "params_natgrad_per_group", 
+                    &NatGradSMPNNet::params_natgrad_per_group,
+                    OptionBase::learntoption,
+                    "Vector of GradientCorrector objects for the gradient inside groups of parameters.\n"
+                    "They are copies of the params_natgrad_template and params_natgrad_per_input_template\n"
+                    "templates provided by the user.\n");
+
+    declareOption(ol, "full_natgrad", &NatGradSMPNNet::full_natgrad,
+                  OptionBase::buildoption,
+                  "GradientCorrector for all the parameter gradients simultaneously.\n"
+                  "This should not be set if neurons_natgrad or params_natgrad_template\n"
+                  "is provided. If none of the GradientCorrectors is provided, then\n"
+                  "regular stochastic gradient is performed.\n");
+
+    declareOption(ol, "output_type", 
+                  &NatGradSMPNNet::output_type,
+                  OptionBase::buildoption,
+                  "type of output cost: 'cross_entropy' for binary classification,\n"
+                  "'NLL' for classification problems, or 'MSE' for regression.\n");
+
+    declareOption(ol, "input_size_lrate_normalization_power", 
+                  &NatGradSMPNNet::input_size_lrate_normalization_power, 
+                  OptionBase::buildoption,
+                  "Scale the learning rate neuron-wise (or layer-wise actually, here):\n"
+                  "-1 scales by 1 / ||x||^2, where x is the 1-extended input vector of the neuron\n"
+                  "0 does not scale the learning rate\n"
+                  "1 scales it by 1 / the nb of inputs of the neuron\n"
+                  "2 scales it by 1 / sqrt(the nb of inputs of the neuron), etc.\n");
+
+    declareOption(ol, "lrate_scale_factor",
+                  &NatGradSMPNNet::lrate_scale_factor,
+                  OptionBase::buildoption,
+                  "scale the learning rate in different neurons by a factor\n"
+                  "taken randomly as follows: choose integer n uniformly between\n"
+                  "lrate_scale_factor_min_power and lrate_scale_factor_max_power\n"
+                  "inclusively, and then scale learning rate by lrate_scale_factor^n.\n");
+
+    declareOption(ol, "lrate_scale_factor_max_power",
+                  &NatGradSMPNNet::lrate_scale_factor_max_power,
+                  OptionBase::buildoption,
+                  "See help on lrate_scale_factor\n");
+
+    declareOption(ol, "lrate_scale_factor_min_power",
+                  &NatGradSMPNNet::lrate_scale_factor_min_power,
+                  OptionBase::buildoption,
+                  "See help on lrate_scale_factor\n");
+
+    declareOption(ol, "self_adjusted_scaling_and_bias",
+                  &NatGradSMPNNet::self_adjusted_scaling_and_bias,
+                  OptionBase::buildoption,
+                  "If true, let each neuron self-adjust its bias and scaling factor\n"
+                  "of its activations so that the mean and standard deviation of the\n"
+                  "activations reach the target_mean_activation and target_stdev_activation.\n"
+                  "The activations mean and variance are estimated by a moving average with\n"
+                  "coefficient given by activations_statistics_moving_average_coefficient\n");
+
+    declareOption(ol, "target_mean_activation",
+                  &NatGradSMPNNet::target_mean_activation,
+                  OptionBase::buildoption,
+                  "See help on self_adjusted_scaling_and_bias\n");
+
+    declareOption(ol, "target_stdev_activation",
+                  &NatGradSMPNNet::target_stdev_activation,
+                  OptionBase::buildoption,
+                  "See help on self_adjusted_scaling_and_bias\n");
+
+    declareOption(ol, "activation_statistics_moving_average_coefficient",
+                  &NatGradSMPNNet::activation_statistics_moving_average_coefficient,
+                  OptionBase::buildoption,
+                  "The activations mean and variance used for self_adjusted_scaling_and_bias\n"
+                  "are estimated by a moving average with this coefficient:\n"
+                  "   xbar <-- coefficient * xbar + (1-coefficient) x\n"
+                  "where x could be the activation or its square\n");
+
+    //declareOption(ol, "corr_profiling_start",
+    //              &NatGradSMPNNet::corr_profiling_start,
+    //              OptionBase::buildoption,
+    //              "Stage to start the profiling of the gradients' and the\n"
+    //              "natural gradients' correlation.\n");
+
+    //declareOption(ol, "corr_profiling_end",
+    //              &NatGradSMPNNet::corr_profiling_end,
+    //              OptionBase::buildoption,
+    //              "Stage to end the profiling of the gradients' and the\n"
+    //              "natural gradients' correlations.\n");
+
+    declareOption(ol, "use_pvgrad",
+                  &NatGradSMPNNet::use_pvgrad,
+                  OptionBase::buildoption,
+                  "Use Pascal Vincent's gradient technique.\n"
+                  "All options specific to this technique start with pv_...\n"
+                  "This is currently very experimental. Current code is \n"
+                  "NOT YET optimised for speed (nor supports minibatch).");
+
+    declareOption(ol, "pv_initial_stepsize",
+                  &NatGradSMPNNet::pv_initial_stepsize,
+                  OptionBase::buildoption,
+                  "Initial size of steps in parameter space");
+
+    declareOption(ol, "pv_acceleration",
+                  &NatGradSMPNNet::pv_acceleration,
+                  OptionBase::buildoption,
+                  "Coefficient by which to multiply/divide the step sizes");
+
+    declareOption(ol, "pv_min_samples",
+                  &NatGradSMPNNet::pv_min_samples,
+                  OptionBase::buildoption,
+                  "PV's minimum number of samples to estimate gradient sign.\n"
+                  "This should at least be 2.");
+
+    declareOption(ol, "pv_required_confidence",
+                  &NatGradSMPNNet::pv_required_confidence,
+                  OptionBase::buildoption,
+                  "Minimum required confidence (probability of being positive or negative) for taking a step.");
+
+    declareOption(ol, "pv_random_sample_step",
+                  &NatGradSMPNNet::pv_random_sample_step,
+                  OptionBase::buildoption,
+                  "If this is set to true, then we will randomly choose the step sign\n"
+                  "for each parameter based on the estimated probability of it being\n"
+                  "positive or negative.");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void NatGradSMPNNet::build_()
+{
+    if (!train_set)
+        return;
+    inputsize_ = train_set->inputsize();
+    if (output_type=="MSE")
+    {
+        if (noutputs<0) noutputs = targetsize_;
+        else PLASSERT_MSG(noutputs==targetsize_,"NatGradSMPNNet: noutputs should be -1 or match data's targetsize");
+    }
+    else if (output_type=="NLL")
+    {
+        if (noutputs<0)
+            PLERROR("NatGradSMPNNet: if output_type=NLL (classification), one \n"
+                    "should provide noutputs = number of classes, or possibly\n"
+                    "1 when 2 classes\n");
+    }
+    else if (output_type=="cross_entropy")
+    {
+        if(noutputs!=1)
+            PLERROR("NatGradSMPNNet: if output_type=cross_entropy, then \n"
+                    "noutputs should be 1.\n");
+    }
+    else PLERROR("NatGradSMPNNet: output_type should be cross_entropy, NLL or MSE\n");
+
+    if( output_layer_L1_penalty_factor < 0. )
+        PLWARNING("NatGradSMPNNet::build_ - output_layer_L1_penalty_factor is negative!\n");
+
+    if(use_pvgrad && minibatch_size!=1)
+        PLERROR("PV's gradient technique (triggered by use_pvgrad): support for minibatch not yet implemented (must have minibatch_size=1)");
+    
+    while (hidden_layer_sizes.length()>0 && hidden_layer_sizes[hidden_layer_sizes.length()-1]==0)
+        hidden_layer_sizes.resize(hidden_layer_sizes.length()-1);
+    n_layers = hidden_layer_sizes.length()+2;
+    layer_sizes.resize(n_layers);
+    layer_sizes.subVec(1,n_layers-2) << hidden_layer_sizes;
+    layer_sizes[0]=inputsize_;
+    layer_sizes[n_layers-1]=noutputs;
+    if (!layer_params.isEmpty())
+        PLERROR("In NatGradSMPNNet::build_ - Currently, one can only build "
+                "a network from scratch");
+    layer_params.resize(n_layers-1);
+    layer_mparams.resize(n_layers-1);
+    layer_params_delta.resize(n_layers-1);
+    layer_params_gradient.resize(n_layers-1);
+    biases.resize(n_layers-1);
+    activations_scaling.resize(n_layers-1);
+    weights.resize(n_layers-1);
+    mweights.resize(n_layers-1);
+    mean_activations.resize(n_layers-1);
+    var_activations.resize(n_layers-1);
+    int n_neurons=0;
+    int n_params=0;
+    for (int i=0;i<n_layers-1;i++)
+    {
+        n_neurons+=layer_sizes[i+1];
+        n_params+=layer_sizes[i+1]*(1+layer_sizes[i]);
+    }
+
+    // Allocate shared memory for parameters.
+    // First deallocate memory if needed.
+    if (params_ptr) {
+        shmctl(params_id, IPC_RMID, 0);
+        params_ptr = NULL;
+    }
+    long total_memory_needed = long(n_params) * sizeof(real);
+    params_id = shmget(IPC_PRIVATE, total_memory_needed, 0666 | IPC_CREAT);
+    PLCHECK( params_id != -1 );
+    params_ptr = (real*) shmat(params_id, 0, 0);
+    assert( params_ptr );
+    // We should have copied data from 'all_params' first if there were some!
+    PLCHECK_MSG( all_params.isEmpty(), "Multiple builds not implemented yet" );
+    all_params = Vec(n_params, params_ptr);
+
+    all_params.resize(n_params);
+    all_mparams.resize(n_params);
+    all_params_gradient.resize(n_params);
+    all_params_delta.resize(n_params);
+
+    // depending on how parameters are grouped on the first layer
+    int n_groups = params_natgrad_per_input_template ? (n_neurons-layer_sizes[1]+layer_sizes[0]+1) : n_neurons;
+    group_params.resize(n_groups);
+    group_params_delta.resize(n_groups);
+    group_params_gradient.resize(n_groups);
+
+    for (int i=0,k=0,p=0;i<n_layers-1;i++)
+    {
+        int np=layer_sizes[i+1]*(1+layer_sizes[i]);
+        // First layer has natural gradient applied on groups of parameters
+        // linked to the same input -> parameters must be stored TRANSPOSED!
+        if( i==0 && params_natgrad_per_input_template ) {
+            PLERROR("This should not be executed");
+            layer_params[i]=all_params.subVec(p,np).toMat(layer_sizes[i]+1,layer_sizes[i+1]);
+            layer_mparams[i]=all_mparams.subVec(p,np).toMat(layer_sizes[i]+1,layer_sizes[i+1]);
+            biases[i]=layer_params[i].subMatRows(0,1);
+            weights[i]=layer_params[i].subMatRows(1,layer_sizes[i]); //weights[0] from layer 0 to layer 1
+            mweights[i]=layer_mparams[i].subMatRows(1,layer_sizes[i]); //weights[0] from layer 0 to layer 1
+            layer_params_gradient[i]=all_params_gradient.subVec(p,np).toMat(layer_sizes[i]+1,layer_sizes[i+1]);
+            layer_params_delta[i]=all_params_delta.subVec(p,np);
+            for (int j=0;j<layer_sizes[i]+1;j++,k++)   // include a bias input 
+            {
+                group_params[k]=all_params.subVec(p,layer_sizes[i+1]);
+                group_params_delta[k]=all_params_delta.subVec(p,layer_sizes[i+1]);
+                group_params_gradient[k]=all_params_gradient.subVec(p,layer_sizes[i+1]);
+                p+=layer_sizes[i+1];
+            }
+        // Usual parameter storage
+        }   else    {
+            layer_params[i]=all_params.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
+            layer_mparams[i]=all_mparams.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
+            biases[i]=layer_params[i].subMatColumns(0,1);
+            weights[i]=layer_params[i].subMatColumns(1,layer_sizes[i]); // weights[0] from layer 0 to layer 1
+            mweights[i]=layer_mparams[i].subMatColumns(1,layer_sizes[i]); // weights[0] from layer 0 to layer 1
+            layer_params_gradient[i]=all_params_gradient.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
+            layer_params_delta[i]=all_params_delta.subVec(p,np);
+            for (int j=0;j<layer_sizes[i+1];j++,k++)
+            {
+                group_params[k]=all_params.subVec(p,1+layer_sizes[i]);
+                group_params_delta[k]=all_params_delta.subVec(p,1+layer_sizes[i]);
+                group_params_gradient[k]=all_params_gradient.subVec(p,1+layer_sizes[i]);
+                p+=1+layer_sizes[i];
+            }
+        }
+        activations_scaling[i].resize(layer_sizes[i+1]);
+        mean_activations[i].resize(layer_sizes[i+1]);
+        var_activations[i].resize(layer_sizes[i+1]);
+    }
+    if (params_natgrad_template || params_natgrad_per_input_template)
+    {
+        int n_input_groups=0;
+        int n_neuron_groups=0;
+        if(params_natgrad_template)
+            n_neuron_groups = n_neurons;
+        if( params_natgrad_per_input_template ) {
+            n_input_groups = layer_sizes[0]+1;
+            if(params_natgrad_template) // override first layer groups if present
+                n_neuron_groups -= layer_sizes[1];
+        }
+        params_natgrad_per_group.resize(n_input_groups+n_neuron_groups);
+        for (int i=0;i<n_input_groups;i++)
+            params_natgrad_per_group[i] = PLearn::deepCopy(params_natgrad_per_input_template);
+        for (int i=n_input_groups; i<n_input_groups+n_neuron_groups;i++)
+            params_natgrad_per_group[i] = PLearn::deepCopy(params_natgrad_template);
+    }
+    if (neurons_natgrad_template && neurons_natgrad_per_layer.length()==0)
+    {
+        neurons_natgrad_per_layer.resize(n_layers); // 0 not used
+        for (int i=1;i<n_layers;i++) // no need for correcting input layer
+            neurons_natgrad_per_layer[i] = PLearn::deepCopy(neurons_natgrad_template);
+    }
+    neuron_gradients.resize(minibatch_size,n_neurons);
+    neuron_outputs_per_layer.resize(n_layers); // layer 0 = input, layer n_layers-1 = output
+    neuron_extended_outputs_per_layer.resize(n_layers); // layer 0 = input, layer n_layers-1 = output
+    neuron_gradients_per_layer.resize(n_layers); // layer 0 not used
+    neuron_extended_outputs_per_layer[0].resize(minibatch_size,1+layer_sizes[0]);
+    neuron_outputs_per_layer[0]=neuron_extended_outputs_per_layer[0].subMatColumns(1,layer_sizes[0]);
+    neuron_extended_outputs_per_layer[0].column(0).fill(1.0); // for biases
+    for (int i=1,k=0;i<n_layers;k+=layer_sizes[i],i++)
+    {
+        neuron_extended_outputs_per_layer[i].resize(minibatch_size,1+layer_sizes[i]);
+        neuron_outputs_per_layer[i]=neuron_extended_outputs_per_layer[i].subMatColumns(1,layer_sizes[i]);
+        neuron_extended_outputs_per_layer[i].column(0).fill(1.0); // for biases
+        neuron_gradients_per_layer[i] = 
+            neuron_gradients.subMatColumns(k,layer_sizes[i]);
+    }
+    example_weights.resize(minibatch_size);
+    TVec<string> train_cost_names = getTrainCostNames() ;
+    train_costs.resize(minibatch_size,train_cost_names.length()-2 );
+
+    Profiler::activate();
+
+    // Gradient correlation profiling
+    //if( corr_profiling_start != corr_profiling_end )  {
+    //    PLASSERT( (0<=corr_profiling_start) && (corr_profiling_start<corr_profiling_end) );
+    //    cout << "n_params " << n_params << endl;
+    //    // Build the names.
+    //    stringstream ss_suffix;
+    //    for (int i=0;i<n_layers;i++)    {
+    //        ss_suffix << "_" << layer_sizes[i];
+    //    }
+    //    ss_suffix << "_stages_" << corr_profiling_start << "_" << corr_profiling_end;
+    //    string str_gc_name = "gCcorr" + ss_suffix.str();
+    //    string str_ngc_name;
+    //    if( full_natgrad )  {
+    //        str_ngc_name = "ngCcorr_full" + ss_suffix.str();
+    //    }   else if (params_natgrad_template)   {
+    //        str_ngc_name = "ngCcorr_params" + ss_suffix.str();
+    //    }
+    //    // Build the profilers.
+    //    g_corrprof = new CorrelationProfiler( n_params, str_gc_name);
+    //    g_corrprof->build();
+    //    ng_corrprof = new CorrelationProfiler( n_params, str_ngc_name);
+    //    ng_corrprof->build();
+    //}
+
+}
+
+// ### Nothing to add here, simply calls build_
+void NatGradSMPNNet::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void NatGradSMPNNet::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(hidden_layer_sizes, copies);
+    deepCopyField(layer_params, copies);
+    deepCopyField(layer_mparams, copies);
+    deepCopyField(biases, copies);
+    deepCopyField(weights, copies);
+    deepCopyField(mweights, copies);
+    deepCopyField(activations_scaling, copies);
+    deepCopyField(neurons_natgrad_template, copies);
+    deepCopyField(neurons_natgrad_per_layer, copies);
+    deepCopyField(params_natgrad_template, copies);
+    deepCopyField(params_natgrad_per_input_template, copies);
+    deepCopyField(params_natgrad_per_group, copies);
+    deepCopyField(full_natgrad, copies);
+    deepCopyField(layer_sizes, copies);
+    deepCopyField(targets, copies);
+    deepCopyField(example_weights, copies);
+    deepCopyField(train_costs, copies);
+    deepCopyField(neuron_outputs_per_layer, copies);
+    deepCopyField(neuron_extended_outputs_per_layer, copies);
+    deepCopyField(all_params, copies);
+    deepCopyField(all_mparams, copies);
+    deepCopyField(all_params_gradient, copies);
+    deepCopyField(layer_params_gradient, copies);
+    deepCopyField(neuron_gradients, copies);
+    deepCopyField(neuron_gradients_per_layer, copies);
+    deepCopyField(all_params_delta, copies);
+    deepCopyField(group_params, copies);
+    deepCopyField(group_params_gradient, copies);
+    deepCopyField(group_params_delta, copies);
+    deepCopyField(layer_params_delta, copies);
+
+    deepCopyField(pv_gradstats, copies);
+    deepCopyField(pv_stepsizes, copies);
+    deepCopyField(pv_stepsigns, copies);
+
+    if (params_ptr)
+        PLERROR("In NatGradSMPNNet::makeDeepCopyFromShallowCopy - Deep copy of"
+                " 'params_ptr' not implemented");
+
+
+/*
+    deepCopyField(, copies);
+*/
+}
+
+
+int NatGradSMPNNet::outputsize() const
+{
+    return noutputs;
+}
+
+void NatGradSMPNNet::forget()
+{
+    //! (Re-)initialize the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!)
+    inherited::forget();
+    for (int i=0;i<n_layers-1;i++)
+    {
+        real delta = 1/sqrt(real(layer_sizes[i]));
+        random_gen->fill_random_uniform(weights[i],-delta,delta);
+        biases[i].clear();
+        activations_scaling[i].fill(1.0);
+        mean_activations[i].clear();
+        var_activations[i].fill(1.0);
+    }
+    stage = 0;
+    cumulative_training_time=0;
+    if (params_averaging_coeff!=1.0)
+        all_mparams << all_params;
+    
+    if(use_pvgrad)
+    {
+        pv_gradstats->forget();
+        int n = all_params.length();
+        pv_stepsizes.resize(n);
+        pv_stepsizes.fill(pv_initial_stepsize);
+        pv_stepsigns.resize(n);
+        pv_stepsigns.fill(true);
+    }
+
+    nsteps = 0;
+}
+
+void NatGradSMPNNet::train()
+{
+
+    if (inputsize_<0)
+        build();
+
+    targets.resize(minibatch_size,targetsize());  // the train_set's targetsize()
+
+    if(!train_set)
+        PLERROR("In NNet::train, you did not setTrainingSet");
+    
+    if(!train_stats)
+        setTrainStatsCollector(new VecStatsCollector());
+
+    train_costs.fill(MISSING_VALUE) ;
+
+    train_stats->forget();
+
+    PP<ProgressBar> pb;
+
+    Profiler::reset("training");
+    Profiler::start("training");
+    Profiler::pl_profile_start("Totaltraining");
+    if( report_progress && stage < nstages )
+        pb = new ProgressBar( "Training "+classname(),
+                              nstages - stage );
+
+    Vec costs_plus_time(train_costs.width()+2);
+    costs_plus_time[train_costs.width()] = MISSING_VALUE;
+    costs_plus_time[train_costs.width()+1] = MISSING_VALUE;
+    Vec costs = costs_plus_time.subVec(0,train_costs.width());
+    int nsamples = train_set->length();
+
+    // Obtain the number of CPUs we want to use.
+    char* ncpus_ptr = getenv("NCPUS");
+    if (!ncpus_ptr)
+        PLERROR("In NatGradSMPNNet::train - The environment variable 'NCPUS' "
+                "must be set (to the number of CPUs being used)");
+    int ncpus = atoi(ncpus_ptr);
+
+    // Semaphore to know which cpu should be updating weights next.
+    if (semaphore_id >= 0) {
+        // First get rid of existing semaphore.
+        int success = semctl(semaphore_id, 0, IPC_RMID);
+        if (success < 0)
+            PLERROR("In NatGradSMPNNet::train - Could not remove previous "
+                    "semaphore (errno = %d)", errno);
+        semaphore_id = -1;
+    }
+    // The semaphore has 'ncpus' + 1 values.
+    // The first one is the index of the CPU that will be next to update
+    // weights.
+    // The other ones are 0/1 values that are initialized with 0, and take 1
+    // once the corresponding CPU has finished all updates for this training
+    // period.
+    semaphore_id = semget(IPC_PRIVATE, ncpus + 1, 0666 | IPC_CREAT);
+    if (semaphore_id == -1)
+        PLERROR("In NatGradSMPNNet::train - Could not create semaphore "
+                "(errno = %d)", errno);
+    // Initialize all values in the semaphore to zero.
+    semun semun_v;
+    semun_v.val = 0;
+    for (int i = 0; i < ncpus + 1; i++) {
+        int success = semctl(semaphore_id, i, SETVAL, semun_v);
+        if (success != 0)
+            PLERROR("In NatGradSMPNNet::train - Could not initialize semaphore"
+                    " value (errno = %d)", errno);
+    }
+
+    // Fork one process/cpu.
+    int iam = 0;
+    for (int cpu = 1; cpu < ncpus ; cpu++)
+        if (fork() == 0) {
+            iam = cpu;
+            break;
+        }
+
+    // Each processor computes gradient over its own subset of samples (between
+    // indices 'start' and 'end' in the training set).
+    int start = (nsamples / ncpus) * iam;
+    int end = iam == ncpus - 1 ? nsamples
+                               : (nsamples / ncpus) * (iam + 1);
+    int my_n_samples = end - start;
+
+    int stage_incr = nstages - stage;
+    int stage_incr_per_cpu = stage_incr / ncpus;
+    int stage_incr_left = stage_incr % ncpus;
+    int my_stage_incr = iam >= stage_incr_left ? stage_incr_per_cpu
+                                               : stage_incr_per_cpu + 1;
+
+    for(int i = 0; i < my_stage_incr; i++)
+    {
+        int sample = start + i % my_n_samples;
+        int b = i % minibatch_size;
+        Vec input = neuron_outputs_per_layer[0](b);
+        Vec target = targets(b);
+        //Profiler::pl_profile_start("getting_data");
+        train_set->getExample(sample, input, target, example_weights[b]);
+        //Profiler::pl_profile_end("getting_data");
+        if (b+1==minibatch_size) // do also special end-case || stage+1==nstages)
+        {
+            onlineStep(stage, targets, train_costs, example_weights );
+            nsteps++;
+            /*
+            for (int i=0;i<minibatch_size;i++)
+            {
+                costs << train_costs(b);
+                train_stats->update( costs_plus_time );
+            }
+            */
+            // Update weights if it is this cpu's turn.
+            int sem_value = semctl(semaphore_id, 0, GETVAL);
+            if (sem_value == iam) {
+                printf("CPU %d updating (nsteps =%d)\n", iam, nsteps);
+                sem_value = (sem_value + 1) % ncpus;
+                semun_v.val = sem_value;
+                semctl(semaphore_id, 0, SETVAL, semun_v);
+                nsteps = 0;
+                // TODO Perform update.
+            } else {
+#if 0
+                printf("CPU %d NOT updating (sem_value = %d)\n",
+                        iam, sem_value);
+#endif
+            }
+        }
+        /*
+        if (params_averaging_coeff!=1.0 && 
+            b==minibatch_size-1 && 
+            (stage+1)%(minibatch_size*params_averaging_freq)==0)
+        {
+            PLERROR("Not implemented for SMP");
+            multiplyScaledAdd(all_params, 1-params_averaging_coeff,
+                    params_averaging_coeff, all_mparams);
+        }
+        if( pb ) {
+            PLERROR("Progress bar not implemented for SMP");
+            pb->update( stage + 1 );
+        }
+        */
+    }
+
+    // Wait until it is our turn.
+    while (true) {
+        int sem_value = semctl(semaphore_id, 0, GETVAL);
+        if (sem_value == iam || iam == 0) {
+            if (sem_value == iam) {
+                if (nsteps >  0) {
+                    // TODO Update weights at the end of training.
+                    printf("CPU %d final updating (nsteps =%d)\n", iam, nsteps);
+                    nsteps = 0;
+                }
+                // Indicate this CPU is done.
+                semun_v.val = 1;
+                semctl(semaphore_id, iam + 1, SETVAL, semun_v);
+                if (iam != 0) {
+                    // Exit additional processes after training.
+                    printf("CPU %d exiting\n", iam);
+                    exit(0);
+                }
+            }
+            // The master process is controlling the counter, to ensure all
+            // processes will correctly exit.
+            if (semctl(semaphore_id, sem_value + 1, GETVAL) == 0) {
+                // The next process is not done yet: we need to wait.
+#if 0
+                printf("Main CPU (%d) still waiting on CPU %d\n", iam,
+                        sem_value);
+#endif
+                continue;
+            }
+
+            // Check if all CPUs are done.
+            bool finished = true;
+            for (int i = 0; i < ncpus; i++) {
+                if (semctl(semaphore_id, i + 1, GETVAL) == 0) {
+                    printf("Main CPU still waiting on CPU %d (GETVAL => %d)\n",
+                            i, semctl(semaphore_id, i + 1, GETVAL));
+                    finished = false;
+                    break;
+                }
+            }
+            if (finished) {
+                printf("Main CPU ready to finish (all ready!)\n");
+                break;
+            }
+
+            // Next CPU!
+            sem_value = (sem_value + 1) % ncpus;
+            semun_v.val = sem_value;
+            semctl(semaphore_id, 0, SETVAL, semun_v);
+        }
+    }
+
+    // Free semaphore's ressources.
+    if (semaphore_id >= 0) {
+        int success = semctl(semaphore_id, 0, IPC_RMID);
+        if (success < 0)
+            PLERROR("In NatGradSMPNNet::train - Could not remove previous "
+                    "semaphore (errno = %d)", errno);
+        semaphore_id = -1;
+    }
+
+    // Update the learner's stage.
+    stage = nstages;
+
+    Profiler::end("training");
+    Profiler::pl_profile_end("Totaltraining");
+    if (verbosity>0)
+        Profiler::report(cout);
+    const Profiler::Stats& stats = Profiler::getStats("training");
+    costs.fill(MISSING_VALUE);
+    real ticksPerSec = Profiler::ticksPerSecond();
+    real cpu_time = (stats.user_duration+stats.system_duration)/ticksPerSec;
+    cumulative_training_time += cpu_time;
+    costs_plus_time[train_costs.width()] = cpu_time;
+    costs_plus_time[train_costs.width()+1] = cumulative_training_time;
+    train_stats->update( costs_plus_time );
+    train_stats->finalize(); // finalize statistics for this epoch
+
+    // profiling gradient correlation
+    //if( g_corrprof )    {
+    //    PLASSERT( corr_profiling_end <= nstages );
+    //    g_corrprof->printAndReset();
+    //    ng_corrprof->printAndReset();
+    //}
+
+}
+
+void NatGradSMPNNet::onlineStep(int tutu, const Mat& targets,
+                             Mat& train_costs, Vec example_weights)
+{
+    // Simply crash right now (easy!) if one tries to use a decrease constant.
+    if (!fast_exact_is_equal(lrate_decay, 0))
+        PLERROR("In NatGradSMPNNet::onlineStep - Learning rate decay not "
+                "implemented");
+    // mean gradient over minibatch_size examples has less variance, can afford larger learning rate
+    // TODO Note that this scaling formula is disabled to avoid confusion about
+    // what learning rates are being used in experiments.
+    real lrate = /*sqrt(real(minibatch_size))* */ init_lrate/(1 + 0*lrate_decay);
+    PLASSERT(targets.length()==minibatch_size && train_costs.length()==minibatch_size && example_weights.length()==minibatch_size);
+    fpropNet(minibatch_size, true);
+    fbpropLoss(neuron_outputs_per_layer[n_layers-1],targets,example_weights,train_costs);
+    for (int i=n_layers-1;i>0;i--)
+    {
+        // here neuron_gradients_per_layer[i] contains the gradient on activations (weighted sums)
+        //      (minibatch_size x layer_size[i])
+
+        Mat previous_neurons_gradient = neuron_gradients_per_layer[i-1];
+        Mat next_neurons_gradient = neuron_gradients_per_layer[i];
+        Mat previous_neurons_output = neuron_outputs_per_layer[i-1];
+        real layer_lrate_factor = (i==n_layers-1)?output_layer_lrate_scale:1;
+        if (self_adjusted_scaling_and_bias && i+1<n_layers-1)
+            for (int k=0;k<minibatch_size;k++)
+            {
+                Vec g=next_neurons_gradient(k);
+                g*=activations_scaling[i-1]; // pass gradient through scaling
+            }
+        if (input_size_lrate_normalization_power==-1)
+            layer_lrate_factor /= sumsquare(neuron_extended_outputs_per_layer[i-1]);
+        else if (input_size_lrate_normalization_power==-2)
+            layer_lrate_factor /= sqrt(sumsquare(neuron_extended_outputs_per_layer[i-1]));
+        else if (input_size_lrate_normalization_power!=0)
+        {
+            int fan_in = neuron_extended_outputs_per_layer[i-1].length();
+            if (input_size_lrate_normalization_power==1)
+                layer_lrate_factor/=fan_in;
+            else if (input_size_lrate_normalization_power==2)
+                layer_lrate_factor/=sqrt(real(fan_in));
+            else layer_lrate_factor/=pow(fan_in,1.0/input_size_lrate_normalization_power);
+        }
+        // optionally correct the gradient on neurons using their covariance
+        if (neurons_natgrad_template && neurons_natgrad_per_layer[i])
+        {
+            static Vec tmp;
+            tmp.resize(layer_sizes[i]);
+            for (int k=0;k<minibatch_size;k++)
+            {
+                Vec g_k = next_neurons_gradient(k);
+                PLERROR("Not implemented (t not available)");
+                //(*neurons_natgrad_per_layer[i])(t-minibatch_size+1+k,g_k,tmp);
+                g_k << tmp;
+            }
+        }
+        if (i>1) // compute gradient on previous layer
+        {
+            // propagate gradients
+            //Profiler::pl_profile_start("ProducScaleAccOnlineStep");
+            productScaleAcc(previous_neurons_gradient,next_neurons_gradient,false,
+                            weights[i-1],false,1,0);
+            //Profiler::pl_profile_end("ProducScaleAccOnlineStep");
+            // propagate through tanh non-linearity
+            for (int j=0;j<previous_neurons_gradient.length();j++)
+            {
+                real* grad = previous_neurons_gradient[j];
+                real* out = previous_neurons_output[j];
+                for (int k=0;k<previous_neurons_gradient.width();k++,out++)
+                    grad[k] *= (1 - *out * *out); // gradient through tanh derivative
+            }
+        }
+        // compute gradient on parameters, possibly update them
+        if (use_pvgrad)
+        {
+            PLERROR("What is this?");
+            productScaleAcc(layer_params_gradient[i-1],next_neurons_gradient,true,
+                            neuron_extended_outputs_per_layer[i-1],false,1,0);
+        }
+        else if (full_natgrad || params_natgrad_template || params_natgrad_per_input_template) 
+        {
+//alternate
+            PLERROR("No, I just want stochastic gradient!");
+            if( params_natgrad_per_input_template && i==1 ){ // parameters are transposed
+                Profiler::pl_profile_start("ProducScaleAccOnlineStep");
+                productScaleAcc(layer_params_gradient[i-1],
+                            neuron_extended_outputs_per_layer[i-1], true,
+                            next_neurons_gradient, false, 
+                            1, 0);
+                Profiler::pl_profile_end("ProducScaleAccOnlineStep");
+            }else{
+                Profiler::pl_profile_start("ProducScaleAccOnlineStep");
+                productScaleAcc(layer_params_gradient[i-1],next_neurons_gradient,true,
+                            neuron_extended_outputs_per_layer[i-1],false,1,0);
+                Profiler::pl_profile_end("ProducScaleAccOnlineStep");
+            }
+            layer_params_gradient[i-1] *= 1.0/minibatch_size; // use the MEAN gradient
+        } else {// just regular stochastic gradient
+            // compute gradient on weights and update them in one go (more efficient)
+            // mean gradient has less variance, can afford larger learning rate
+            //Profiler::pl_profile_start("ProducScaleAccOnlineStep");
+            productScaleAcc(layer_params[i-1],next_neurons_gradient,true,
+                            neuron_extended_outputs_per_layer[i-1],false,
+                            -layer_lrate_factor*lrate /* /minibatch_size */, 1);
+            //Profiler::pl_profile_end("ProducScaleAccOnlineStep");
+        }
+    }
+    if (use_pvgrad)
+    {
+        PLERROR("What is this?");
+        pvGradUpdate();
+    }
+    else if (full_natgrad)
+    {
+        PLERROR("Not implemented (t not available)");
+        //(*full_natgrad)(t/minibatch_size,all_params_gradient,all_params_delta); // compute update direction by natural gradient
+        if (output_layer_lrate_scale!=1.0)
+            layer_params_delta[n_layers-2] *= output_layer_lrate_scale; // scale output layer's learning rate
+        multiplyAcc(all_params,all_params_delta,-lrate); // update
+        // Hack to apply batch gradient even in this case (used for profiling
+        // the gradient correlations)
+        //if (output_layer_lrate_scale!=1.0)
+        //      layer_params_gradient[n_layers-2] *= output_layer_lrate_scale; // scale output layer's learning rate
+        //  multiplyAcc(all_params,all_params_gradient,-lrate); // update
+
+    } else if (params_natgrad_template || params_natgrad_per_input_template)
+    {
+        PLERROR("Not implemented (t not available)");
+        for (int i=0;i<params_natgrad_per_group.length();i++)
+        {
+            //GradientCorrector& neuron_natgrad = *(params_natgrad_per_group[i]);
+            //neuron_natgrad(t/minibatch_size,group_params_gradient[i],group_params_delta[i]); // compute update direction by natural gradient
+        }
+//alternate
+        if (output_layer_lrate_scale!=1.0)
+            layer_params_delta[n_layers-2] *= output_layer_lrate_scale; // scale output layer's learning rate 
+        multiplyAcc(all_params,all_params_delta,-lrate); // update
+    }
+
+    // profiling gradient correlation
+    //if( (t>=corr_profiling_start) && (t<=corr_profiling_end) && g_corrprof )    {
+    //    (*g_corrprof)(all_params_gradient);
+    //    (*ng_corrprof)(all_params_delta);
+    //}
+
+    // Output layer L1 regularization
+    if( output_layer_L1_penalty_factor != 0. )    {
+        PLERROR("Not implemented");
+        real L1_delta = lrate * output_layer_L1_penalty_factor;
+        real* m_i = layer_params[n_layers-2].data();
+
+        for(int i=0; i<layer_params[n_layers-2].length(); i++,m_i+=layer_params[n_layers-2].mod())  {
+            for(int j=0; j<layer_params[n_layers-2].width(); j++)   {
+                if( m_i[j] > L1_delta )
+                    m_i[j] -= L1_delta;
+                else if( m_i[j] < -L1_delta )
+                    m_i[j] += L1_delta;
+                else
+                    m_i[j] = 0.;
+            }
+        }
+    }
+
+}
+
+void NatGradSMPNNet::pvGradUpdate()
+{
+    int n = all_params_gradient.length();
+    if(pv_stepsizes.length()==0)
+    {
+        pv_stepsizes.resize(n);
+        pv_stepsizes.fill(pv_initial_stepsize);
+        pv_stepsigns.resize(n);
+        pv_stepsigns.fill(true);
+    }
+    pv_gradstats->update(all_params_gradient);
+    real pv_deceleration = 1.0/pv_acceleration;
+    for(int k=0; k<n; k++)
+    {
+        StatsCollector& st = pv_gradstats->getStats(k);
+        int n = (int)st.nnonmissing();
+        if(n>pv_min_samples)
+        {
+            real m = st.mean();
+            real e = st.stderror();
+            real prob_pos = gauss_01_cum(m/e);
+            real prob_neg = 1.-prob_pos;
+            if(!pv_random_sample_step)
+            {
+                if(prob_pos>=pv_required_confidence)
+                {
+                    all_params[k] += pv_stepsizes[k];
+                    pv_stepsizes[k] *= (pv_stepsigns[k]?pv_acceleration:pv_deceleration);
+                    pv_stepsigns[k] = true;
+                    st.forget();
+                }
+                else if(prob_neg>=pv_required_confidence)
+                {
+                    all_params[k] -= pv_stepsizes[k];
+                    pv_stepsizes[k] *= ((!pv_stepsigns[k])?pv_acceleration:pv_deceleration);
+                    pv_stepsigns[k] = false;
+                    st.forget();
+                }
+            }
+            else  // random sample update direction (sign)
+            {
+                bool ispos = (random_gen->binomial_sample(prob_pos)>0);
+                if(ispos) // picked positive
+                    all_params[k] += pv_stepsizes[k];
+                else  // picked negative
+                    all_params[k] -= pv_stepsizes[k];
+                pv_stepsizes[k] *= (pv_stepsigns[k]==ispos) ?pv_acceleration :pv_deceleration;
+                pv_stepsigns[k] = ispos;
+                st.forget();
+            }
+        }
+    }
+}
+
+void NatGradSMPNNet::computeOutput(const Vec& input, Vec& output) const
+{
+    Profiler::pl_profile_start("computeOutput");
+    neuron_outputs_per_layer[0](0) << input;
+    fpropNet(1,false);
+    output << neuron_outputs_per_layer[n_layers-1](0);
+    Profiler::pl_profile_end("computeOutput");
+}
+
+//! compute (pre-final-non-linearity) network top-layer output given input
+void NatGradSMPNNet::fpropNet(int n_examples, bool during_training) const
+{
+    PLASSERT_MSG(n_examples<=minibatch_size,"NatGradSMPNNet::fpropNet: nb input vectors treated should be <= minibatch_size\n");
+    for (int i=0;i<n_layers-1;i++)
+    {
+        Mat prev_layer = (self_adjusted_scaling_and_bias && i+1<n_layers-1)?
+            neuron_outputs_per_layer[i]:neuron_extended_outputs_per_layer[i];
+        Mat next_layer = neuron_outputs_per_layer[i+1];
+        if (n_examples!=minibatch_size)
+        {
+            prev_layer = prev_layer.subMatRows(0,n_examples);
+            next_layer = next_layer.subMatRows(0,n_examples);
+        }
+//alternate
+        // Are the input weights transposed? (because of ...)
+        bool tw = true;
+        if( params_natgrad_per_input_template && i==0 )
+            tw = false;
+
+        // try to use BLAS for the expensive operation
+        if (self_adjusted_scaling_and_bias && i+1<n_layers-1){
+            if (during_training)
+                Profiler::pl_profile_start("ProducScaleAccFpropTrain");
+            else
+                Profiler::pl_profile_start("ProducScaleAccFpropNoTrain");
+            productScaleAcc(next_layer, prev_layer, false, 
+                            (during_training || params_averaging_coeff==1.0)?
+                            weights[i]:mweights[i], 
+                            tw, 1, 0);
+            if (during_training)
+                Profiler::pl_profile_end("ProducScaleAccFpropTrain");
+            else
+                Profiler::pl_profile_end("ProducScaleAcccFpropNoTrain");
+        }else{
+            if (during_training)
+                Profiler::pl_profile_start("ProducScaleAccFpropTrain");
+            else
+                Profiler::pl_profile_start("ProducScaleAcccFpropNoTrain");
+            productScaleAcc(next_layer, prev_layer, false, 
+                            (during_training || params_averaging_coeff==1.0)?
+                            layer_params[i]:layer_mparams[i], 
+                            tw, 1, 0);
+            if (during_training)
+                Profiler::pl_profile_end("ProducScaleAccFpropTrain");
+            else
+                Profiler::pl_profile_end("ProducScaleAcccFpropNoTrain");
+        }
+        // compute layer's output non-linearity
+        if (i+1<n_layers-1)
+            for (int k=0;k<n_examples;k++)
+            {
+                Vec L=next_layer(k);
+                if (self_adjusted_scaling_and_bias)
+                {
+                    real* m=mean_activations[i].data();
+                    real* v=var_activations[i].data();
+                    real* a=L.data();
+                    real* s=activations_scaling[i].data();
+                    real* b=biases[i].data(); // biases[i] is a 1-column matrix
+                    int bmod = biases[i].mod();
+                    for (int j=0;j<layer_sizes[i+1];j++,b+=bmod,m++,v++,a++,s++)
+                    {
+                        if (during_training)
+                        {
+                            real diff = *a - *m;
+                            *v = (1-activation_statistics_moving_average_coefficient) * *v
+                                + activation_statistics_moving_average_coefficient * diff*diff;
+                            *m = (1-activation_statistics_moving_average_coefficient) * *m
+                                + activation_statistics_moving_average_coefficient * *a;
+                            *b = target_mean_activation - *m;
+                            if (*v<100*target_stdev_activation*target_stdev_activation)
+                                *s = target_stdev_activation/sqrt(*v);
+                            else // rescale the weights and the statistics for that neuron
+                            {
+                                real rescale_factor = target_stdev_activation/sqrt(*v);
+                                Vec w = weights[i](j);
+                                w *= rescale_factor;
+                                *b *= rescale_factor;
+                                *s = 1;
+                                *m *= rescale_factor;
+                                *v *= rescale_factor*rescale_factor;
+                            }
+                        }
+                        Profiler::pl_profile_start("activation function");
+                        *a = tanh((*a + *b) * *s);
+                        Profiler::pl_profile_end("activation function");
+                    }
+                }
+                else{
+                    Profiler::pl_profile_start("activation function");
+                    compute_tanh(L,L);
+                    Profiler::pl_profile_end("activation function");
+                }
+            }
+        else if (output_type=="NLL")
+            for (int k=0;k<n_examples;k++)
+            {
+                Vec L=next_layer(k);
+                Profiler::pl_profile_start("activation function");
+                log_softmax(L,L);
+                Profiler::pl_profile_end("activation function");
+            }
+        else if (output_type=="cross_entropy")  {
+            for (int k=0;k<n_examples;k++)
+            {
+                Vec L=next_layer(k);
+                Profiler::pl_profile_start("activation function");
+                log_sigmoid(L,L);
+                Profiler::pl_profile_end("activation function");
+            }
+         }
+    }
+}
+
+//! compute train costs given the (pre-final-non-linearity) network top-layer output
+void NatGradSMPNNet::fbpropLoss(const Mat& output, const Mat& target, const Vec& example_weight, Mat& costs) const
+{
+    int n_examples = output.length();
+    Mat out_grad = neuron_gradients_per_layer[n_layers-1];
+    if (n_examples!=minibatch_size)
+        out_grad = out_grad.subMatRows(0,n_examples);
+    if (output_type=="NLL")
+    {
+        for (int i=0;i<n_examples;i++)
+        {
+            int target_class = int(round(target(i,0)));
+            Vec outp = output(i);
+            Vec grad = out_grad(i);
+            exp(outp,grad); // map log-prob to prob
+            costs(i,0) = -outp[target_class];
+            costs(i,1) = (target_class == argmax(outp))?0:1;
+            grad[target_class]-=1;
+            if (example_weight[i]!=1.0)
+                costs(i,0) *= example_weight[i];
+        }
+    }
+    else if(output_type=="cross_entropy")   {
+        for (int i=0;i<n_examples;i++)
+        {
+            int target_class = int(round(target(i,0)));
+            Vec outp = output(i);
+            Vec grad = out_grad(i);
+            exp(outp,grad); // map log-prob to prob
+            if( target_class == 1 ) {
+                costs(i,0) = - outp[0];
+                costs(i,1) = (grad[0]>0.5)?0:1;
+            }   else    {
+                costs(i,0) = - pl_log( 1.0 - grad[0] );
+                costs(i,1) = (grad[0]>0.5)?1:0;
+            }
+            grad[0] -= (real)target_class;
+            if (example_weight[i]!=1.0)
+                costs(i,0) *= example_weight[i];
+        }
+//cout << "costs\t" << costs(0) << endl;
+//cout << "gradient\t" << out_grad(0) << endl;
+
+    }
+    else // if (output_type=="MSE")
+    {
+        substract(output,target,out_grad);
+        for (int i=0;i<n_examples;i++)
+        {
+            costs(i,0) = pownorm(out_grad(i));
+            if (example_weight[i]!=1.0)
+            {
+                out_grad(i) *= example_weight[i];
+                costs(i,0) *= example_weight[i];
+            }
+        }
+    }
+}
+
+void NatGradSMPNNet::computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                           const Vec& target, Vec& costs) const
+{
+    Vec w(1);
+    w[0]=1;
+    Mat outputM = output.toMat(1,output.length());
+    Mat targetM = target.toMat(1,output.length());
+    Mat costsM = costs.toMat(1,costs.length());
+    fbpropLoss(outputM,targetM,w,costsM);
+}
+
+TVec<string> NatGradSMPNNet::getTestCostNames() const
+{
+    TVec<string> costs;
+    if (output_type=="NLL")
+    {
+        costs.resize(2);
+        costs[0]="NLL";
+        costs[1]="class_error";
+    }
+    else if (output_type=="cross_entropy")  {
+        costs.resize(2);
+        costs[0]="cross_entropy";
+        costs[1]="class_error";
+    }
+    else if (output_type=="MSE")
+    {
+        costs.resize(1);
+        costs[0]="MSE";
+    }
+    return costs;
+}
+
+TVec<string> NatGradSMPNNet::getTrainCostNames() const
+{
+    TVec<string> costs = getTestCostNames();
+    costs.append("train_seconds");
+    costs.append("cum_train_seconds");
+    return costs;
+}
+
+NatGradSMPNNet::~NatGradSMPNNet()
+{
+    if (params_ptr) {
+        shmctl(params_id, IPC_RMID, 0);
+        params_ptr = NULL;
+    }
+    if (semaphore_id >= 0) {
+        int success = semctl(semaphore_id, 0, IPC_RMID);
+        if (success < 0)
+            PLERROR("In NatGradSMPNNet::train - Could not remove previous "
+                    "semaphore (errno = %d)", errno);
+        semaphore_id = -1;
+    }
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.h	2007-07-18 18:49:56 UTC (rev 7795)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.h	2007-07-18 19:54:20 UTC (rev 7796)
@@ -0,0 +1,359 @@
+// -*- C++ -*-
+
+// NatGradSMPNNet.h
+//
+// Copyright (C) 2007 Yoshua Bengio
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Yoshua Bengio
+
+/*! \file NatGradSMPNNet.h */
+
+
+#ifndef NatGradSMPNNet_INC
+#define NatGradSMPNNet_INC
+
+#include <plearn_learners/generic/PLearner.h>
+#include <plearn_learners/generic/GradientCorrector.h>
+#include <plearn/sys/Profiler.h>
+//#include "CorrelationProfiler.h"
+
+namespace PLearn {
+
+/**
+ * Multi-layer neural network trained with an efficient Natural Gradient optimization.
+ */
+class NatGradSMPNNet : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    int noutputs;
+
+    //! sizes of hidden layers, provided by the user.
+    TVec<int> hidden_layer_sizes;
+
+    //! layer_params[i] is a matrix of dimension layer_sizes[i+1] x (layer_sizes[i]+1)
+    //! containing the neuron biases in its first column.
+    TVec<Mat> layer_params;
+    //! mean layer_params, averaged over past updates (moving average)
+    TVec<Mat> layer_mparams;
+
+    //! mparams <-- (1-params_averaging_coeff)mparams + params_averaging_coeff*params
+    real params_averaging_coeff;
+    //! how often (in terms of minibatches, i.e. weight updates) do we perform the above?
+    int params_averaging_freq;
+
+    //! initial learning rate
+    real init_lrate;
+
+    //! learning rate decay factor
+    real lrate_decay;
+
+    //! L1 penalty applied to the output layer's parameters
+    real output_layer_L1_penalty_factor;
+
+    //! scaling factor of the learning rate for the output layer
+    real output_layer_lrate_scale;
+
+    //! update the parameters only so often
+    int minibatch_size;
+
+    //! natural gradient estimator for neurons
+    //! (if 0 then do not correct the gradient on neurons)
+    PP<GradientCorrector> neurons_natgrad_template;
+    TVec<PP<GradientCorrector> > neurons_natgrad_per_layer;
+
+    //! natural gradient estimator for the parameters within each neuron
+    //! (if 0 then do not correct the gradient on each neuron weight)
+    PP<GradientCorrector> params_natgrad_template;
+    //! natural gradient estimator solely for the parameters of the first
+    //! layer. If present, performs over groups of parameters related to the
+    //! same input (this includes the additional bias input).
+    //! Has precedence over params_natgrad_template, ie if present, there is
+    //! no natural gradient performed on the groups of a neuron's parameters:
+    //! params_natgrad_template is not applied for the first hidden layer's
+    //! parameters). 
+    PP<GradientCorrector> params_natgrad_per_input_template;
+
+    //! the above templates are used by the user to specifiy all the elements of the vector below
+    TVec<PP<GradientCorrector> > params_natgrad_per_group;
+
+    //! optionally, if neurons_natgrad==0 and params_natgrad_template==0, one can
+    //! have regular stochastic gradient descent, or full-covariance natural gradient
+    //! using the natural gradient estimator below
+    PP<GradientCorrector> full_natgrad;
+
+    //! type of output cost: "NLL" for classification problems, "MSE" for regression
+    string output_type;
+
+    //! 0 does not scale the learning rate
+    //! 1 scales it by 1 / the nb of inputs of the neuron
+    //! 2 scales it by 1 / sqrt(the nb of inputs of the neuron)
+    //! etc.
+    real input_size_lrate_normalization_power;
+
+    //! scale the learning rate in different neurons by a factor
+    //! taken randomly as follows: choose integer n uniformly between 
+    //! lrate_scale_factor_min_power and lrate_scale_factor_max_power
+    //! inclusively, and then scale learning rate by lrate_scale_factor^n.
+    real lrate_scale_factor;
+    int lrate_scale_factor_max_power;
+    int lrate_scale_factor_min_power;
+
+    //! Let each neuron self-adjust its bias and scaling factor of its activations
+    //! so that the mean and standard deviation of the activations reach 
+    //! the target_mean_activation and target_stdev_activation.
+    bool self_adjusted_scaling_and_bias;
+    real target_mean_activation;
+    real target_stdev_activation;
+    // the mean and variance of the activations is estimated by a moving
+    // average with this coefficient (near 0 for very slow averaging)
+    real activation_statistics_moving_average_coefficient;
+
+    int verbosity;
+
+    //! Stages for profiling the correlation between the gradients' elements
+    //int corr_profiling_start, corr_profiling_end;
+
+public:
+    //*************************************************************
+    //*** Members used for Pascal Vincent's gradient technique  ***
+
+    //! Use Pascal's gradient 
+    bool use_pvgrad;
+
+    //! Initial size of steps in parameter space
+    real pv_initial_stepsize;
+
+    //! Coefficient by which to multiply/divide the step sizes  
+    real pv_acceleration;
+
+    //! PV's gradient minimum number of samples to estimate confidence
+    int pv_min_samples;
+
+    //! Minimum required confidence (probability of being positive or negative) for taking a step. 
+    real pv_required_confidence;
+
+    //! If this is set to true, then we will randomly choose the step sign for
+    // each parameter based on the estimated probability of it being positive or
+    // negative.
+    bool pv_random_sample_step;
+    
+
+protected:
+    //! accumulated statistics of gradients on each parameter.
+    PP<VecStatsCollector> pv_gradstats;
+
+    //! The step size (absolute value) to be taken for each parameter.
+    Vec pv_stepsizes;
+
+    //! Indicates whether the previous step was positive (true) or negative (false)
+    TVec<bool> pv_stepsigns;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    NatGradSMPNNet();
+
+    //! Destructor (to free shared memory).
+    virtual ~NatGradSMPNNet();
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Returns the size of this learner's output, (which typically
+    //! may depend on its inputsize(), targetsize() and set options).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual int outputsize() const;
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training costs
+    //! measured on-line in the process.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void train();
+
+    //! Computes the output from the input.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeOutput(const Vec& input, Vec& output) const;
+
+    //! Computes the costs from already computed output.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                         const Vec& target, Vec& costs) const;
+
+    //! Returns the names of the costs computed by computeCostsFromOutpus (and
+    //! thus the test method).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec<std::string> getTestCostNames() const;
+
+    //! Returns the names of the objective costs that the train method computes
+    //! and  for which it updates the VecStatsCollector train_stats.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec<std::string> getTrainCostNames() const;
+
+
+    // *** SUBCLASS WRITING: ***
+    // While in general not necessary, in case of particular needs
+    // (efficiency concerns for ex) you may also want to overload
+    // some of the following methods:
+    // virtual void computeOutputAndCosts(const Vec& input, const Vec& target,
+    //                                    Vec& output, Vec& costs) const;
+    // virtual void computeCostsOnly(const Vec& input, const Vec& target,
+    //                               Vec& costs) const;
+    // virtual void test(VMat testset, PP<VecStatsCollector> test_stats,
+    //                   VMat testoutputs=0, VMat testcosts=0) const;
+    // virtual int nTestCosts() const;
+    // virtual int nTrainCosts() const;
+    // virtual void resetInternalState();
+    // virtual bool isStatefulLearner() const;
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(NatGradSMPNNet);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+
+    //! number of layers of weights (2 for a neural net with one hidden layer)
+    int n_layers;
+
+    //! layer sizes (derived from hidden_layer_sizes, inputsize_ and outputsize_)
+    TVec<int> layer_sizes;
+
+    //! pointers into the layer_params
+    TVec<Mat> biases;
+    TVec<Mat> weights,mweights;
+    TVec<Vec> activations_scaling; // output = tanh(activations_scaling[layer][neuron] * (biases[layer][neuron] + weights[layer]*input[layer-1])
+    TVec<Vec> mean_activations;
+    TVec<Vec> var_activations;
+    real cumulative_training_time;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList& ol);
+
+    //! one minibatch training step
+    void onlineStep(int t, const Mat& targets, Mat& train_costs, Vec example_weights);
+
+    //! compute a minibatch of size n_examples network top-layer output given layer 0 output (= network input)
+    //! (note that log-probabilities are computed for classification tasks, output_type=NLL)
+    void fpropNet(int n_examples, bool during_training) const;
+
+    //! compute train costs given the network top-layer output
+    //! and write into neuron_gradients_per_layer[n_layers-2], gradient on pre-non-linearity activation
+    void fbpropLoss(const Mat& output, const Mat& target, const Vec& example_weights, Mat& train_costs) const;
+
+    //! gradient computation and weight update in Pascal Vincent's gradient technique
+    void pvGradUpdate();
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+
+    Vec all_params; // all the parameters in one vector
+    Vec all_params_delta; // update direction
+    Vec all_params_gradient; // all the parameter gradients in one vector
+    Vec all_mparams; // mean parameters (moving-averaged over past values)
+    TVec<Mat> layer_params_gradient;
+    TVec<Vec> layer_params_delta;
+    TVec<Vec> group_params; // params of each group (pointing in all_params)
+    TVec<Vec> group_params_delta; // params_delta of each group (pointing in all_params_delta)
+    TVec<Vec> group_params_gradient; // params_delta of each group (pointing in all_params_gradient)
+    Mat neuron_gradients; // one row per example of a minibatch, has concatenation of layer 0, layer 1, ... gradients.
+    TVec<Mat> neuron_gradients_per_layer; // pointing into neuron_gradients (one row per example of a minibatch)
+    mutable TVec<Mat> neuron_outputs_per_layer;  // same structure
+    mutable TVec<Mat> neuron_extended_outputs_per_layer;  // with 1's in the first pseudo-neuron, for doing biases
+    Mat targets; // one target row per example in a minibatch
+    Vec example_weights; // one element per example in a minibatch
+    Mat train_costs; // one row per example in a minibatch
+
+    real* params_ptr; // Raw pointer to the (shared) parameters.
+    int params_id; // Shared memory id for parameters.
+
+    //! Number of online steps performed since the last global parameter update.
+    int nsteps;
+
+    //! Semaphore used to control which CPU must perform an update.
+    int semaphore_id;
+
+    //PP<CorrelationProfiler> g_corrprof, ng_corrprof;    // for optional gradient correlation profiling
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(NatGradSMPNNet);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From lamblin at mail.berlios.de  Wed Jul 18 21:57:47 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 18 Jul 2007 21:57:47 +0200
Subject: [Plearn-commits] r7797 - trunk/plearn/io
Message-ID: <200707181957.l6IJvl5H028721@sheep.berlios.de>

Author: lamblin
Date: 2007-07-18 21:57:47 +0200 (Wed, 18 Jul 2007)
New Revision: 7797

Modified:
   trunk/plearn/io/PStream.h
Log:
Use boost types for fixed-size integers


Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2007-07-18 19:54:20 UTC (rev 7796)
+++ trunk/plearn/io/PStream.h	2007-07-18 19:57:47 UTC (rev 7797)
@@ -41,7 +41,7 @@
 #include <set>
 #include <sstream>
 #include <fstream>
-#include <stdint.h>
+#include <boost/cstdint.hpp>
 #include <plearn/base/byte_order.h>
 #include <plearn/base/pl_hash_fun.h>
 #include <plearn/base/plerror.h>
@@ -53,6 +53,17 @@
 
 using namespace std;
 
+// inject principal boost integer types into namespace PLearn
+// @TODO: use them all?
+using boost::int8_t;
+using boost::uint8_t;
+using boost::int16_t;
+using boost::uint16_t;
+using boost::int32_t;
+using boost::uint32_t;
+using boost::int64_t;
+using boost::uint64_t;
+
 /*!
  * PStream:
  *  This class defines a type of stream that should be used for all I/O within PLearn.



From saintmlx at mail.berlios.de  Thu Jul 19 00:30:32 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 19 Jul 2007 00:30:32 +0200
Subject: [Plearn-commits] r7798 - in trunk/plearn: io python
Message-ID: <200707182230.l6IMUWjN005109@sheep.berlios.de>

Author: saintmlx
Date: 2007-07-19 00:30:31 +0200 (Thu, 19 Jul 2007)
New Revision: 7798

Modified:
   trunk/plearn/io/PStream.h
   trunk/plearn/python/PythonObjectWrapper.h
Log:
- allow receiving Object*'s from python



Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2007-07-18 19:57:47 UTC (rev 7797)
+++ trunk/plearn/io/PStream.h	2007-07-18 22:30:31 UTC (rev 7798)
@@ -633,6 +633,13 @@
 }
 
 template <class T>
+inline PStream& operator>>(PStream& in, const T*& x)
+{
+    PLERROR("operator>>(PStream&, const T*&) should never be used! (object pointed is const)");
+    return in;
+}
+
+template <class T>
 inline PStream& operator>>(PStream& in, PP<T> &o)
 {
     T *ptr;

Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2007-07-18 19:57:47 UTC (rev 7797)
+++ trunk/plearn/python/PythonObjectWrapper.h	2007-07-18 22:30:31 UTC (rev 7798)
@@ -199,7 +199,8 @@
     static T* convert(PyObject* pyobj, bool print_traceback)
     {
         // Compile-time assertion:
-        BOOST_STATIC_ASSERT((boost::is_base_of<Object, T>::value));
+        BOOST_STATIC_ASSERT((boost::is_base_of<Object, typename boost::remove_cv<T>::type>::value)
+                            || (boost::is_same<Object, typename boost::remove_cv<T>::type>::value));
         //N.B.: If this assertion fails, it probably means that you are trying
         //      to retrieve a pointer to something that is not an Object from
         //      python.  Only Object pointers are supported.



From lysiane at mail.berlios.de  Thu Jul 19 17:15:49 2007
From: lysiane at mail.berlios.de (lysiane at BerliOS)
Date: Thu, 19 Jul 2007 17:15:49 +0200
Subject: [Plearn-commits] r7799 -
	trunk/plearn_learners/distributions/EXPERIMENTAL
Message-ID: <200707191515.l6JFFn3f024917@sheep.berlios.de>

Author: lysiane
Date: 2007-07-19 17:15:44 +0200 (Thu, 19 Jul 2007)
New Revision: 7799

Modified:
   trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc
   trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h
Log:
Transformation Learner, PDistribution version, for digit experiences


Modified: trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc
===================================================================
--- trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc	2007-07-18 22:30:31 UTC (rev 7798)
+++ trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc	2007-07-19 15:15:44 UTC (rev 7799)
@@ -1,8 +1,7 @@
-// -*- C++ -*-
+    // -*- C++ -*-
 
 // TransformationLearner.cc
 //
-//version 5
 // Copyright (C) 2007 Lysiane Bouchard
 //
 // Redistribution and use in source and binary forms, with or without
@@ -40,51 +39,52 @@
 
 #include "TransformationLearner.h"
 
-//C++ 
-#include <math.h>
-
-
-//Plearn
-#include <plearn/math/TMat_maths.h>
-#include <plearn/math/PRandom.h>
-#include <plearn/math/plapack.h>
-
-
 namespace PLearn {
 using namespace std;
 
 PLEARN_IMPLEMENT_OBJECT(
     TransformationLearner,
-    "ONE LINE DESCRIPTION",
-    "MULTI-LINE \nHELP");
+    "ONE LINE DESCR",
+    "NO HELP"
+);
 
-//TO TEST : OK
+//////////////////
+// TransformationLearner //
+//////////////////
 TransformationLearner::TransformationLearner():
-/* ### Initialize all fields to their default value here */
-    seed(1827),
-    transformFamily(TRANSFORM_FAMILY_LINEAR),
-    noiseVariance(1.0),
-    transformsVariance(0.5),
-    nbTransforms(5),
-    nbNeighbors(5),
-    epsilonInitWeight(0.01)
+    behavior(BEHAVIOR_LEARNER),
+    minimumProba(0.0001),
+    transformFamily(TRANSFORM_FAMILY_LINEAR_INCREMENT),
+    withBias(false),
+    learnNoiseVariance(false),
+    regOnNoiseVariance(false),
+    learnTransformDistribution(false),
+    regOnTransformDistribution(false),
+    initializationMode(INIT_MODE_DEFAULT),
+    largeEStepAPeriod(UNDEFINED),
+    largeEStepAOffset(UNDEFINED),
+    largeEStepBPeriod(UNDEFINED),
+    largeEStepBOffset(UNDEFINED),
+    noiseVariancePeriod(UNDEFINED),
+    noiseVarianceOffset(UNDEFINED),
+    noiseAlpha(NOISE_ALPHA_NO_REG),
+    noiseBeta(NOISE_BETA_NO_REG),
+    transformDistributionPeriod(UNDEFINED),
+    transformDistributionOffset(UNDEFINED),
+    transformDistributionAlpha(TRANSFORM_DISTRIBUTION_ALPHA_NO_REG),
+    transformsPeriod(1),
+    transformsOffset(0),
+    noiseVariance(UNDEFINED),
+    transformsVariance(1.0),
+    nbTransforms(2),
+    nbNeighbors(2)
 {
-    // ...
-    //pout << "TransformationLearner()" << endl;
-    random_gen = new PRandom();
-    
-
-    // ### You may (or not) want to call build_() to finish building the object
-    // ### (doing so assumes the parent classes' build_() have been called too
-    // ### in the parent classes' constructors, something that you must ensure)
-
-    // ### If this learner needs to generate random numbers, uncomment the
-    // ### line below to enable the use of the inherited PRandom object.
-    // random_gen = new PRandom();
+    pout << "hello\n";
 }
 
-
-//TO TEST
+////////////////////
+// declareOptions //
+////////////////////
 void TransformationLearner::declareOptions(OptionList& ol)
 {
     // ### Declare all of this object's options here.
@@ -100,323 +100,577 @@
     //               OptionBase::buildoption,
     //               "Help text describing this option");
     // ...
-    
-    declareOption(ol, 
-                  "seed", 
-                  &TransformationLearner::seed, 
+
+
+    //buildoption
+    pout << "declare options\n";
+
+    declareOption(ol,
+                  "behavior",
+                  &TransformationLearner::behavior,
                   OptionBase::buildoption,
-                  "seed of the random generator");
-    declareOption(ol, 
-                  "transformFamily", 
-                  &TransformationLearner::transformFamily, 
+                  "a transformationLearner might behave as a learner or as a generator");
+    declareOption(ol,
+                  "minimumProba",
+                  &TransformationLearner::minimumProba,
                   OptionBase::buildoption,
-                  "transformation function Family");
-    declareOption(ol, 
-                  "noiseVariance", 
-                  &TransformationLearner::noiseVariance, 
+                  "initial weight that will be needed sometimes");
+    declareOption(ol,
+                  "transformFamily",
+                  &TransformationLearner::transformFamily,
                   OptionBase::buildoption,
-                  "variance on the noise r.v. (normaly distributed with mean 0)");
+                  "global form of the transformation functions");
     declareOption(ol,
-                  "transformsVariance", 
-                  &TransformationLearner::transformsVariance, 
+                  "withBias",
+                  &TransformationLearner::withBias,
                   OptionBase::buildoption,
-                  "variance on the transformation parameters r.vs"
-                  "(normaly distributed with mean 0)");
+                  "yes/no: add a bias to the transformation function ?");
     declareOption(ol,
-                  "nbTransforms", 
-                  &TransformationLearner::nbTransforms, 
+                  "learnNoiseVariance",
+                  &TransformationLearner::learnNoiseVariance,
                   OptionBase::buildoption,
-                  "number of transformations");
+                  "the noise variance is ...fixed/learned ?");
     declareOption(ol,
-                  "nbNeighbors", 
-                  &TransformationLearner::nbNeighbors, 
+                  "regOnNoiseVariance",
+                  &TransformationLearner::regOnNoiseVariance,
                   OptionBase::buildoption,
-                  "number of neighbors");
+                  "yes/no: prior assumptions on the noise variance?");
     declareOption(ol,
-                  "epsilonInitWeight", 
-                  &TransformationLearner::epsilonInitWeight, 
+                  "learnTransformDistribution",
+                  &TransformationLearner::learnTransformDistribution,
                   OptionBase::buildoption,
-                  "smallest amount of weight we can give to a choosen \n"
-                  "generation candidate at initialization of the \n "
-                  "generation set");
-
+                  "the transformation distribution is ... fixed/learned ?");
     declareOption(ol,
-                  "transformDistribution", 
-                  &TransformationLearner::transformDistribution, 
+                  "regOnTransformDistribution",
+                  &TransformationLearner::regOnTransformDistribution,
                   OptionBase::buildoption,
-                  "a multinomial distribution for the transformations\n"
-                  "i.e. p(kth transformation) = transformDistribution[k] \n ");
-
+                  "yes/no: prior assumptions on the transformation distribution ?");
+    
     declareOption(ol,
-                  "inputSpaceDim", 
-                  &TransformationLearner::inputSpaceDim, 
-                  OptionBase::learntoption,
-                  "dimension of the training set input space");
+                  "initializationMode",
+                  &TransformationLearner::initializationMode,
+                  OptionBase::buildoption,
+                  "how the initial values of the parameters to learn are choosen?");
+    
     declareOption(ol,
-                  "nbGenerationCandidatesPerTarget", 
-                  &TransformationLearner::nbGenerationCandidatesPerTarget, 
-                  OptionBase::learntoption,
-                  "number of generation candidates per target"); 
+                  "largeEStepAPeriod",
+                  &TransformationLearner::largeEStepAPeriod,
+                  OptionBase::buildoption,
+                  "time interval between two updates of the reconstruction set\n"
+                  "(version A, method largeEStepA())");
     declareOption(ol,
-                  "nbGenerationCandidates", 
-                  &TransformationLearner::nbGenerationCandidates, 
-                  OptionBase::learntoption,
-                  "number of generation candidates in the generation set");
+                  "largeEStepAOffset",
+                  &TransformationLearner::largeEStepAOffset,
+                  OptionBase::buildoption,
+                  "time of the first update of the reconstruction set"
+                  "(version A, method largeEStepA())");
     declareOption(ol,
-                  "nbTrainingInput", 
-                  &TransformationLearner::nbTrainingInput, 
-                  OptionBase::learntoption,
-                  "number of input given in the training set");  
+                  "largeEStepBPeriod",
+                  &TransformationLearner::largeEStepBPeriod,
+                  OptionBase::buildoption,
+                  "time interval between two updates of the reconstruction set\n"
+                  "(version  B, method largeEStepB())"); 
     declareOption(ol,
-                  "trainsformsSet", 
-                  &TransformationLearner::transformsSet, 
-                  OptionBase::learntoption,
-                  "set of transformations");
+                  "noiseVariancePeriod",
+                  &TransformationLearner::noiseVariancePeriod,
+                  OptionBase::buildoption,
+                  "time interval between two updates of the noise variance");
     declareOption(ol,
-                  "transforms", 
-                  &TransformationLearner::transforms, 
-                  OptionBase::learntoption,
-                  "views on the transformation set");
-    
+                  "noiseVarianceOffset",
+                  &TransformationLearner::noiseVarianceOffset,
+                  OptionBase::buildoption,
+                  "time of the first update of the noise variance");
     declareOption(ol,
-                  "generationSet", 
-                  &TransformationLearner::generationSet, 
-                  OptionBase::learntoption,
-                  "set of generation candidates"
-                  "i.e. triples (target, neighbor, transformation)");
-    declareOption(ol,
-                  "lambda", 
-                  &TransformationLearner::lambda, 
-                  OptionBase::learntoption,
-                  "weight decay");
+                  "noiseAlpha",
+                  &TransformationLearner::noiseAlpha,
+                  OptionBase::buildoption,
+                  "parameter of the prior distribution of the noise variance");
+   declareOption(ol,
+                 "noiseBeta",
+                 &TransformationLearner::noiseBeta,
+                 OptionBase::buildoption,
+                 "parameter of the prior distribution of the noise variance");
+   declareOption(ol,
+                 "transformDistributionPeriod",
+                 &TransformationLearner::transformDistributionPeriod,
+                 OptionBase::buildoption,
+                 "time interval between two updates of the transformation distribution");
+   declareOption(ol, 
+                 "transformDistributionOffset",
+                 &TransformationLearner::transformDistributionOffset,
+                 OptionBase::buildoption,
+                 "time of the first update of the transformation distribution");
+   declareOption(ol, 
+                 "transformDistributionAlpha",
+                 &TransformationLearner::transformDistributionAlpha,
+                 OptionBase::buildoption,
+                 "parameter of the prior distribution of the transformation distribution");
+   declareOption(ol,
+                 "transformsPeriod",
+                 &TransformationLearner::transformsPeriod,
+                 OptionBase::buildoption,
+                 "time interval between two updates of the transformations parameters");
+   declareOption(ol,
+                 "transformsOffset",
+                 &TransformationLearner::transformsOffset,
+                 OptionBase::buildoption,
+                 "time of the first update of the transformations parameters");
 
-    declareOption(ol,
-                  "noiseVarianceFactor", 
-                  &TransformationLearner::noiseVarianceFactor, 
-                  OptionBase::learntoption,
-                  "factor used in computation of generation weights"
-                  " 1/(2*noise variance)");
+   declareOption(ol, 
+                 "noiseVariance",
+                 &TransformationLearner::noiseVariance,
+                 OptionBase::buildoption,
+                 "noise variance (noise = random variable normally distributed)");
+   declareOption(ol, 
+                 "transformsVariance",
+                 &TransformationLearner::transformsVariance,
+                 OptionBase::buildoption,
+                 "variance on the transformation parameters (normally distributed)");
+   declareOption(ol, 
+                 "nbTransforms",
+                 &TransformationLearner::nbTransforms,
+                 OptionBase::buildoption,
+                 "how many transformations?");
+   declareOption(ol, 
+                 "nbNeighbors",
+                 &TransformationLearner::nbNeighbors,
+                 OptionBase::buildoption,
+                 "how many neighbors?");
+   declareOption(ol, 
+                 "transformDistribution",
+                 &TransformationLearner::transformDistribution,
+                 OptionBase::buildoption,
+                 "transformation distribution");
+   
+   //learntoption
+   declareOption(ol,
+                 "transformsSet",
+                 &TransformationLearner::transformsSet,
+                 OptionBase::learntoption,
+                 "set of transformations \n)"
+                 "implemented as a mdXd matrix,\n"
+                 "     where m is the number of transformations\n"
+                 "           and d is dimensionality of the input space");
+   declareOption(ol,
+                 "transforms",
+                 &TransformationLearner::transforms,
+                 OptionBase::learntoption,
+                 "set of transformations\n"
+                 "vector form of the previous set:\n)"
+                 "    kth element of the vector = view on the kth sub-matrix");
+   declareOption(ol,
+                 "biasSet",
+                 &TransformationLearner::biasSet,
+                 OptionBase::learntoption,
+                 "set of bias (one by transformation)");
+   declareOption(ol,
+                 "inputSpaceDim",
+                 &TransformationLearner::inputSpaceDim,
+                 OptionBase::learntoption,
+                 "dimensionality of the input space");
+   declareOption(ol,
+                 "nbTargetReconstructions",
+                 &TransformationLearner::nbTargetReconstructions,
+                 OptionBase::learntoption,
+                 "number of reconstructions of the same target");
+   declareOption(ol,
+                 "nbReconstructions",
+                 &TransformationLearner::nbReconstructions,
+                 OptionBase::learntoption,
+                 "total number of reconstructions");
+   declareOption(ol,
+                 "trainingSetLength",
+                 &TransformationLearner::trainingSetLength,
+                 OptionBase::learntoption,
+                 "number of samples in the training" );
+   declareOption(ol,
+                 "transformsSD",
+                 &TransformationLearner::transformsSD,
+                 OptionBase::learntoption,
+                 "standard deviation of the transformations parameters");
+   declareOption(ol,
+                 "targetReconstructionSet",
+                 &TransformationLearner::targetReconstructionSet,
+                 OptionBase::learntoption,
+                 "will be used to store a view on the reconstructions of a same target");
+   declareOption(ol,
+                 "B_C",
+                 &TransformationLearner::B_C,
+                 OptionBase::learntoption,
+                 "storage space needed in the maximization step (to update transformations parameters)\n"
+                 " - 2mdXd matrix, m=number of transformations, d = dimensionality of the input space");
+   declareOption(ol,
+                 "B",
+                 &TransformationLearner::B,
+                 OptionBase::learntoption,
+                 "views on m first dxd submatrices of B_C \n"
+                 "(vector form)");
+   declareOption(ol,
+                 "C",
+                 &TransformationLearner::C,
+                 OptionBase::learntoption,
+                 "views on m last dxd submatrices of B_C \n"
+                 "(vector form)");
+   declareOption(ol,
+                 "target",
+                 &TransformationLearner::target,
+                 OptionBase::learntoption,
+                 "to store a view on a training sample");
+   declareOption(ol,
+                 "neighbor",
+                 &TransformationLearner::neighbor,
+                 OptionBase::learntoption,
+                 "to store a view on a training sample");
 
-    declareOption(ol,
-                  "noiseStDev", 
-                  &TransformationLearner::noiseStDev, 
-                  OptionBase::learntoption,
-                  "standard deviation on noise distribution");
-    declareOption(ol,
-                  "transformsStDev", 
-                  &TransformationLearner::transformsStDev, 
-                  OptionBase::learntoption,
-                  "standard deviation on transformation parameters");
+   // Now call the parent class' declareOptions().
+   inherited::declareOptions(ol);
+}
 
-    // Now call the parent class' declareOptions
-    
+void TransformationLearner::declareMethods(RemoteMethodMap& rmm){
 
 
-    inherited::declareOptions(ol);
-}
-
-
-//TO TEST
-void TransformationLearner::declareMethods(RemoteMethodMap& rmm)
-{
-    declareMethod(rmm, "largeEStepA", &TransformationLearner::largeEStepA,
-                  (BodyDoc("Performs a large update of the generation set (expectation step)"  
-                           "For each target, we take the best generation candidates among all the possibilities \n")));
+    pout << "declare methods\n";
+    rmm.inherited(inherited::_getRemoteMethodMap_());
     
-    declareMethod(rmm, "initEStep", &TransformationLearner::initEStep,
-                  (BodyDoc("Initialization of the generation set (expectation step)\n" 
-                           "For each possible couple (target,neighbor), we take the best transformations to form \n" 
-                           "the generation candidates")));
+    declareMethod(rmm, 
+                  "initTransformsParameters",
+                  &TransformationLearner::initTransformsParameters,
+                  (BodyDoc("initializes the transformation parameters randomly \n"
+                           "  (all parameters are a priori independent and normally distributed)")));
+   
+    declareMethod(rmm, 
+                  "setTransformsParameters",
+                  &TransformationLearner::setTransformsParameters,
+                  (BodyDoc("initializes the transformation parameters with the given values"),
+                   ArgDoc("TVec<Mat> transforms", "initial transformation matrices"),
+                   ArgDoc("Mat  biasSet","initial bias (one by transformation) (optional)")));
+    declareMethod(rmm, 
+                  "initNoiseVariance",
+                  &TransformationLearner::initNoiseVariance,
+                  (BodyDoc("initializes the noise variance randomly (gamma distribution)")));
+    declareMethod(rmm, 
+                  "setNoiseVariance",
+                  &TransformationLearner::setNoiseVariance,
+                  (BodyDoc("initializes the noise variance to the given value"),
+                   ArgDoc("real nv","noise variance")));
+    declareMethod(rmm, 
+                  "initTransformDistribution",
+                  &TransformationLearner::initTransformDistribution,
+                  (BodyDoc("initializes the transformation distribution randomly \n"
+                           "-we use a dirichlet distribution \n"
+                           "-we store log-probabilities instead probabilities")));
+    declareMethod(rmm, 
+                  "setTransformDistribution",
+                  &TransformationLearner::setTransformDistribution,
+                  (BodyDoc("initializes the transformation distribution with the given values \n"
+                           " -the given values might represent log-probabilities"),
+                   ArgDoc("Vec td","initial values of the transformation distribution")));
     
-    declareMethod(rmm, "smallEStep",&TransformationLearner::smallEStep,
-                  (BodyDoc("Update of the generation set (expectation step) \n"
-                           "we update the weights of the generation candidates while keeping them fixed")));
-    
-    declareMethod(rmm, "MStep", &TransformationLearner::MStep,
-                  (BodyDoc("Updating the transformation parameters (maximization step)\n")));
+    declareMethod(rmm,
+                  "returnPredictedFrom",
+                  &TransformationLearner::returnPredictedFrom,
+                  (BodyDoc("generates a sample data point from a source data point and returns it \n"
+                           " - a specific transformation is used"),
+                   ArgDoc("const Vec source","source data point"),
+                   ArgDoc("int transformIdx","index of the transformation (optional)"),
+                   RetDoc("Vec")));
 
-    declareMethod(rmm, "largeEStepB", &TransformationLearner::largeEStepB,
-                  (BodyDoc("Performs a large update of the")));
- 
-    declareMethod(rmm, "returnReproductionSources", &TransformationLearner::returnReproductionSources,
-                  (BodyDoc("Returns the generation candidates associated to a specific target "),
-                   ArgDoc ("targetIdx", "Index of the target data point in the training set"),
-                   RetDoc ("A vector of tuples (target index, neighbor index, transformation index, weight )")));
-
-    declareMethod(rmm, "returnReproductions", &TransformationLearner::returnReproductions,
-                  (BodyDoc("Computes the reproductions of the target from his generation candidates "),
-                   ArgDoc("targetIdx","Index of the target data point in the training set"),
-                   RetDoc("A matrix of data points (reproductions of the target)")));
-
-    declareMethod(rmm, "returnTransform", &TransformationLearner::returnTransform,
-                  (BodyDoc("Returns the parameters of a transformation"),
-                   ArgDoc("transformIdx"," Index of the transformation"),
-                   RetDoc("a dXd matrix, d = dimension of input space")));
-    declareMethod(rmm, "returnAllTransforms",&TransformationLearner::returnAllTransforms,
-                  (BodyDoc("Returns all the transformation parameters"),
-                   RetDoc("a kdXd matrix,  k = nb transformations \n" 
-                          "                d = dimension of input space")));
-    declareMethod(rmm, 
-                  "returnGeneratedSamplesFrom", 
+    declareMethod(rmm,
+                  "returnGeneratedSamplesFrom",
                   &TransformationLearner::returnGeneratedSamplesFrom,
-                  (BodyDoc("returns samples data point generated from\n"
-                           "a center data point"),
-                   ArgDoc("Vec center, int n"," center data point"),
-                   ArgDoc("int n", " number of sample data points to generate"),
-                   RetDoc("a nXd matrix, the center is included in the dataset")));
+                  (BodyDoc("generates samples data points form a source data point and return them \n"
+                           "    -we use a specific transformation"),
+                   ArgDoc("Vec source","source data point"),
+                   ArgDoc("int n","number of samples"),
+                   ArgDoc("int transformIdx", "index of the transformation (optional)"),
+                   RetDoc("nXd matrix (one row = one sample)")));
     declareMethod(rmm,
-                  "returnGeneratedDataSet",
-                  &TransformationLearner::returnGeneratedDataSet,
-                  (BodyDoc("returns a data set with respect to the current\n"
-                           "distribution paramaters\n"
-                           "We use a tree generation process (see createDataSet)\n"
-                           "i.e.: each new data point is used to generate a fix number of data points "),
-                   ArgDoc("Vec root","initial data point"),
-                   ArgDoc("int nbGenerations","deepness of the tree"),
-                   ArgDoc("int GenerationLen","number of child for interior nodes"),
-                   RetDoc("a nXd matrix, where n = number of nodes in the tree (root = part of the dataSet)")));
-    
+                  "pickTransformIdx",
+                  &TransformationLearner::pickTransformIdx,
+                  (BodyDoc("select a transformation ramdomly"),
+                   RetDoc("int (index of the choosen transformation)")));
+               
     declareMethod(rmm,
-                  "returnSequentiallyGeneratedDataSet",
-                  &TransformationLearner::returnSequentiallyGeneratedDataSet,
-                  (BodyDoc("returns a data set with respect to the current\n"
-                           "distribution parameters\n"
-                           "We use a sequential generation process\n"
-                           "i.e: each new data point is used to generate the next data point"),
-                   ArgDoc("Vec root","initial data point"),
-                   ArgDoc("int n","number of data points to generate"),  
-                   RetDoc("a nXd matrix (the root is included in the data set)")));
-    
-    inherited::declareMethods(rmm);
+                  "pickNeighborIdx",
+                  &TransformationLearner::pickNeighborIdx,
+                  (BodyDoc("select a neighbor among the data points in the training set"),
+                   RetDoc("int (index of the data point in the training set)")));
+    declareMethod(rmm,
+                  "returnTreeDataSet",
+                  &TransformationLearner::returnTreeDataSet,
+                  (BodyDoc("creates and returns a data set using a 'tree generation process'\n"
+                           " see 'treeDataSet()' implantation for more details"),
+                   ArgDoc("Vec root","data point from which all the other data points will derive (directly or indirectly)"),
+                   ArgDoc("int deepness","deepness of the tree reprenting the samples created"),
+                   ArgDoc("int branchingFactor","branching factor of the tree representing the samples created"),
+                   RetDoc("Mat (one row = one sample)")));
+    declareMethod(rmm,
+                  "returnSequenceDataSet",
+                  &TransformationLearner::returnSequenceDataSet,
+                  (BodyDoc("creates and returns a data set using a 'sequential procedure' \n"
+                           "see 'sequenceDataSet()' implantation for more details"),
+                   ArgDoc("const Vec start","data point from which all the other data points will derice (directly or indirectly)"),
+                   ArgDoc("int n","number of sample data points to generate"),
+                   RetDoc("nXd matrix (one row = one sample)")));
+    declareMethod(rmm,
+                  "returnTrainingPoint",
+                  &TransformationLearner::returnTrainingPoint,
+                  (BodyDoc("returns the 'idx'th data point in the training set"),
+                   ArgDoc("int idx","index of the data point in the training set"),
+                   RetDoc("Vec")));
+    declareMethod(rmm,
+                  "returnReconstructionCandidates",
+                  &TransformationLearner::returnReconstructionCandidates,
+                  (BodyDoc("return all the reconstructions candidates associated to a given target"),
+                   ArgDoc("int targetIdx","index of the target data point in the training set"),
+                   RetDoc("TVec<ReconstructionCandidate>")));
+    declareMethod(rmm,
+                  "returnReconstructions",
+                  &TransformationLearner::returnReconstructions,
+                  (BodyDoc("returns the reconstructions of the 'targetIdx'th data point in the training set \n"
+                           "(one reconstruction per reconstruction candidate)"),
+                   ArgDoc("int targetIdx","index of the target data point in the training set"),
+                   RetDoc("Mat (ith row = reconstruction associated to the ith reconstruction candidate)")));
+    declareMethod(rmm,
+                  "returnNeighbors",
+                  &TransformationLearner::returnNeighbors,
+                  (BodyDoc("returns the choosen neighbors of the target\n"
+                           "  (one neighbor per reconstruction candidate)"),
+                   ArgDoc("int targetIdx","index of the target in the training set"),
+                   RetDoc("Mat (ith row = neighbor associated to the ith reconstruction candidate)")));
+    declareMethod(rmm,
+                  "returnTransform",
+                  &TransformationLearner::returnTransform,
+                  (BodyDoc("returns the parameters of the 'transformIdx'th transformation"),
+                   ArgDoc("int transformIdx","index of the transformation"),
+                   RetDoc("Mat")));
+    declareMethod(rmm,
+                  "returnAllTransforms",
+                  &TransformationLearner::returnAllTransforms,
+                  (BodyDoc("returns the parameters of each transformation"),
+                   RetDoc("mdXd matrix, m = number of transformations \n"
+                          "             d = dimensionality of the input space")));
+    declareMethod(rmm,
+                  "trainBuild",
+                  &TransformationLearner::trainBuild,
+                  (BodyDoc("training specific initialization operations")));
+    declareMethod(rmm,
+                  "generatorBuild",
+                  &TransformationLearner::generatorBuild,
+                  (BodyDoc("generator specific initialization operations"),
+                   ArgDoc("int inputSpaceDim","dimensionality of the input space"),
+                   ArgDoc("TVec<Mat> transforms_", "transformations matrices"),
+                   ArgDoc("Mat biasSet_","transformations bias"),
+                   ArgDoc("real noiseVariance_","noise variance"),
+                   ArgDoc("transformDistribution_", "transformation distribution")));
+    declareMethod(rmm,
+                  "gamma_sample",
+                  &TransformationLearner::gamma_sample,
+                  (BodyDoc("returns a pseudo-random positive real value using the distribution p(x)=Gamma(x |alpha,beta)"),
+                   ArgDoc("real alpha",">=1"),
+                   ArgDoc("real beta",">= 0 (optional: default value==1)"),
+                   RetDoc("real >=0")));
+    declareMethod(rmm,
+                  "return_dirichlet_sample",
+                  &TransformationLearner::return_dirichlet_sample,
+                  (BodyDoc("returns a pseudo-random positive real vector using the distribution p(x)=Dirichlet(x|alpha)"),
+                   ArgDoc("real alpha","all the parameters of the distribution are equal to 'alpha'"),
+                   RetDoc("Vec (each element is between 0 and 1 , the elements sum to one)")));
+/* declareMethod(rmm,
+   "return_dirichlet_sample",
+   &TransformationLearner::return_dirichlet_sample,
+   (BodyDoc("returns a pseudo-random positive real vector using the distribution p(x)=Dirichlet(x|alphas)"),
+   ArgDoc("Vec alphas","parameters of the distribution"),
+   RetDoc("Vec (each element is between 0 and 1, the elements sum to one )"))); */
+    declareMethod(rmm,
+                  "initEStep",
+                  &TransformationLearner::initEStep,
+                  (BodyDoc("initial expectation step")));
+    declareMethod(rmm,
+                  "EStep",
+                  &TransformationLearner::EStep,
+                  (BodyDoc("coordination of the different kinds of expectation steps")));
+    declareMethod(rmm,
+                  "largeEStepA",
+                  &TransformationLearner::largeEStepA,
+                  (BodyDoc("update the reconstruction set \n"
+                           "for each target, keeps the most probable <neighbor, transformation> pairs")));
+    declareMethod(rmm,
+                  "largeEStepB",
+                  &TransformationLearner::largeEStepB,
+                  (BodyDoc("update the reconstruction set \n"
+                           "for each <target,transformation> pairs,choose the most probable neighbors ")));
+    declareMethod(rmm,
+                  "smallEStep",
+                  &TransformationLearner::smallEStep,
+                  (BodyDoc("update the weights of the reconstruction candidates")));
+    declareMethod(rmm,
+                  "MStep",
+                  &TransformationLearner::MStep,
+                  (BodyDoc("coordination of the different kinds of maximization step")));
+    declareMethod(rmm,
+                  "MStepTransformDistribution",
+                  &TransformationLearner::MStepTransformDistribution,
+                  (BodyDoc("maximization step with respect to transformation distribution parameters")));
+    declareMethod(rmm,
+                  "MStepTransformations",
+                  &TransformationLearner::MStepTransformations,
+                  (BodyDoc("maximization step with respect to transformation parameters (MAP version)")));
+    declareMethod(rmm,
+                  "MStepNoiseVariance",
+                  &TransformationLearner::MStepNoiseVariance,
+                  (BodyDoc("maximization step with respect to noise variance")));
+    declareMethod(rmm,
+                  "stoppingCriterionReached",
+                  &TransformationLearner::stoppingCriterionReached,
+                  (BodyDoc("stages == nstages?")));
+    declareMethod(rmm,
+                  "nextStage",
+                  &TransformationLearner::nextStage,
+                  (BodyDoc("increment 'stage' by one")));
+
 }
 
 
-
-//TO TEST
-//do the building operations related to the generation process
-//warning: we suppose the transformation parameters are set 
-void TransformationLearner::generatorInit(){
-    
-    inputSpaceDim = transformsSet.width();
-    
-
+///////////
+// build //
+///////////
+void TransformationLearner::build()
+{
+    pout << "build\n";
+    // ### Nothing to add here, simply calls build_().
+    inherited::build();
+    build_();
 }
 
+////////////
+// build_ //
+////////////
+void TransformationLearner::build_()
+{
+    // ### This method should do the real building of the object,
+    // ### according to set 'options', in *any* situation.
+    // ### Typical situations include:
+    // ###  - Initial building of an object from a few user-specified options
+    // ###  - Building of a "reloaded" object: i.e. from the complete set of
+    // ###    all serialised options.
+    // ###  - Updating or "re-building" of an object after a few "tuning"
+    // ###    options have been modified.
+    // ### You should assume that the parent class' build_() has already been
+    // ### called.
 
-//TO TEST
-//do the building operations related to training
-//warning: we suppose the training set has been transmitted
-//         before calling the method
-void TransformationLearner::trainInit(){
-   
-    //DIMENSION VARIABLES
-    
-    //dimension of the input space
-    inputSpaceDim = train_set->inputsize();
-      
-    //number of samples given in the training set
-    nbTrainingInput = train_set->length();
+    // ### In general, you will want to call this class' specific methods for
+    // ### conditional distributions.
+    // TransformationLearner::setPredictorPredictedSizes(predictor_size,
+    //                                          predicted_size,
+    //                                          false);
+    // TransformationLearner::setPredictor(predictor_part, false);
 
-    
-    //number of generation candidates related to a specific target in the 
-    //generation set.   
-    nbGenerationCandidatesPerTarget = nbNeighbors * nbTransforms;
-
-   //total number of generation candidates in the generation set
-    nbGenerationCandidates = nbTrainingInput * nbGenerationCandidatesPerTarget;
-
-    
-    //LEARNED MODEL PARAMETERS
-    
-    //set of transformations (represented as a single matrix)
-    transformsSet = Mat(nbTransforms * inputSpaceDim, inputSpaceDim);
-    
-    //view on the set of transformations (vector)
-    //    each transformation = one matrix 
-    transforms.resize(nbTransforms);
-    for(int k = 0; k< nbTransforms; k++){
-        transforms[k] = transformsSet.subMatRows(k * inputSpaceDim, inputSpaceDim);       
+    pout << "build_\n";
+    if(behavior == BEHAVIOR_LEARNER){
+        trainBuild();
     }
-    
-     //generation set and weights of the entries in the generation set
-    generationSet.resize(nbGenerationCandidates);
-
-    //OTHER VARIABLES
-    
-    //weight decay
-    lambda = noiseVariance/transformsVariance;
    
+    else{
+        generatorBuild(); //initialization of the parameters with all the default values
+    }
     
+ 
+}
 
-    //factor used in the computation of the generation weights
-    noiseVarianceFactor = 1/(2*noiseVariance);
-    
+/////////
+// cdf //
+/////////
+real TransformationLearner::cdf(const Vec& y) const
+{
+    PLERROR("cdf not implemented for TransformationLearner"); return 0;
+}
 
-    //to store a view on the generation set 
-    //   (entries related to a specific target)
-    targetGenerationSet.resize(nbGenerationCandidatesPerTarget);
+/////////////////
+// expectation //
+/////////////////
+void TransformationLearner::expectation(Vec& mu) const
+{
+    PLERROR("expectation not implemented for TransformationLearner");
+}
 
-    //Storage space used in the update of the transformation parameters
-    B_C = Mat(2 * nbTransforms * inputSpaceDim , inputSpaceDim);
-
-    B.resize(nbTransforms);
-    C.resize(nbTransforms);
-    for(int k=0; k<nbTransforms; k++){
-        B[k]= B_C.subMatRows(k*inputSpaceDim, inputSpaceDim);
-    }
-    for(int k= nbTransforms ; k<2*nbTransforms ; k++){
-        C[(k % nbTransforms)] = B_C.subMatRows(k*inputSpaceDim, inputSpaceDim);
-    }
+// ### Remove this method if your distribution does not implement it.
+////////////
+// forget //
+////////////
+void TransformationLearner::forget()
+{
     
+    /*!
+      A typical forget() method should do the following:
+      - initialize a random number generator with the seed option
+      - initialize the learner's parameters, using this random generator
+      - stage = 0
+    */    
+    //PLERROR("forget method not implemented for TransformationLearner");
     
-    target.resize(inputSpaceDim);
-    neighbor.resize(inputSpaceDim);
-
+    inherited::forget();
+    stage = 0;
+    trainBuild();
+    
 }
 
+//////////////
+// generate //
+//////////////
 
-//TO TEST 
-void TransformationLearner::build_(){
+//!generate a point using the training set: 
+//! - choose ramdomly a neighbor among data points in the training set
+//! - choose randomly a transformation 
+//! - apply the transformation on the choosen neighbor
+//! - add some noise 
+void TransformationLearner::generate(Vec & y) //const
+{
+    //PLERROR("generate not implemented for TransformationLearner");
+    PLASSERT(y.length() == inputSpaceDim);
+    int neighborIdx ;
+    neighborIdx=pickNeighborIdx();
+    seeNeighbor(neighborIdx);
+    generatePredictedFrom(neighbor, y);
+}
 
-  
-    if(transformDistribution.length() == 0){
-        transformDistribution.resize(nbTransforms);
-        transformDistribution.fill(1.0/nbTransforms);
-    }
-    else{
-        PLASSERT(transformDistribution.length() == nbTransforms);
-        real sum =0;
-        for(int i=0; i<nbTransforms; i++){
-            sum += transformDistribution[i];
-        }
-        PLASSERT(sum == 1);  
-    }
-    
-   
-
-    // ### This method should do the real building of the object,
-    // ### according to set 'options', in *any* situation.
-    // ### Typical situations include:
-    // ###  - Initial building of an object from a few user-specified options
-    // ###  - Building of a "reloaded" object: i.e. from the complete set of
-    // ###    all serialised options.
-    // ###  - Updating or "re-building" of an object after a few "tuning"
-    // ###    options have been modified.
-    // ### You should assume that the parent class' build_() has already been
-    // ### called.
+// ### Default version of inputsize returns learner->inputsize()
+// ### If this is not appropriate, you should uncomment this and define
+// ### it properly here:
+int TransformationLearner::inputsize() const {
+    return inputSpaceDim;
 }
 
-//TO TEST
-// ### Nothing to add here, simply calls build_
-void TransformationLearner::build()
+/////////////////
+// log_density //
+/////////////////
+real TransformationLearner::log_density(const Vec& y) //const
 {
-
-    // pout << "build()" << endl;
-    inherited::build();
-    build_(); 
+    PLASSERT(y.length() == inputSpaceDim);
+    real weight;
+    real totalWeight = INIT_weight(0);
+    real scalingFactor = -1*(pl_log(pow(2*Pi*noiseVariance, inputSpaceDim/2.0)) 
+                             +
+                             pl_log(trainingSetLength));
+    for(int neighborIdx=0; neighborIdx<trainingSetLength; neighborIdx++){
+        for(int transformIdx=0 ; transformIdx<nbTransforms ; transformIdx++){
+            weight = computeReconstructionWeight(y,
+                                                 neighborIdx,
+                                                 transformIdx);
+            weight = MULT_weights(weight,
+                                  transformDistribution[transformIdx]);
+            totalWeight = SUM_weights(weight,totalWeight);
+        }  
+    }
+    totalWeight = MULT_weights(totalWeight, scalingFactor);
+    return totalWeight;
+    
+    /*PLERROR("density not implemented for TransformationLearner"); return 0;*/
 }
 
-
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
 void TransformationLearner::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
@@ -431,40 +685,60 @@
     PLERROR("TransformationLearner::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
 }
 
-/******** LEARNING MODULE ******************************************************/
+////////////////////
+// resetGenerator //
+////////////////////
+/*void TransformationLearner::resetGenerator(long g_seed) const
+{
+    PLERROR("resetGenerator not implemented for TransformationLearner");
+}
+*/
 
+//////////////////
+// setPredictor //
+//////////////////
+void TransformationLearner::setPredictor(const Vec& predictor, bool call_parent) const
+{
+    if (call_parent)
+        inherited::setPredictor(predictor, true);
+    // ### Add here any specific code required by your subclass.
+}
 
-//TO DO
-int TransformationLearner::outputsize() const
+////////////////////////////////
+// setPredictorPredictedSizes //
+////////////////////////////////
+bool TransformationLearner::setPredictorPredictedSizes(int the_predictor_size,
+                                               int the_predicted_size,
+                                               bool call_parent)
 {
-    return 0;
-    // Compute and return the size of this learner's output (which typically
-    // may depend on its inputsize(), targetsize() and set options).
+    bool sizes_have_changed = false;
+    if (call_parent)
+        sizes_have_changed = inherited::setPredictorPredictedSizes(
+                the_predictor_size, the_predicted_size, true);
+
+    // ### Add here any specific code required by your subclass.
+
+    // Returned value.
+    return sizes_have_changed;
 }
 
-
-//TO DO
-void TransformationLearner::forget()
+/////////////////
+// survival_fn //
+/////////////////
+real TransformationLearner::survival_fn(const Vec& y) const
 {
-    //! (Re-)initialize the PLearner in its fresh state (that state may depend
-    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
-    //! a fresh learner!)
-    /*!
-      A typical forget() method should do the following:
-      - call inherited::forget() to initialize its random number generator
-        with the 'seed' option
-      - initialize the learner's parameters, using this random generator
-      - stage = 0
-    */
-    inherited::forget();
+    PLERROR("survival_fn not implemented for TransformationLearner"); return 0;
 }
 
-//TO DO
+// ### Remove this method, if your distribution does not implement it.
+///////////
+// train //
+///////////
 void TransformationLearner::train()
 {
-
-    trainInit();
-
+    
+  
+    //PLERROR("train method not implemented for TransformationLearner");
     // The role of the train method is to bring the learner up to
     // stage==nstages, updating train_stats with training costs measured
     // on-line in the process.
@@ -496,446 +770,922 @@
         train_stats->finalize(); // finalize statistics for this epoch
     }
     */
-}
 
-//TO DO
-void TransformationLearner::computeOutput(const Vec& input, Vec& output) const
-{
-    // Compute the output from the input.
-    // int nout = outputsize();
-    // output.resize(nout);
-    // ...
+    initEStep();
+    while(!stoppingCriterionReached()){
+        MStep();
+        EStep();
+        stage ++;
+    }
+    
 }
 
-//TO DO
-void TransformationLearner::computeCostsFromOutputs(const Vec& input, const Vec& output,
-                                           const Vec& target, Vec& costs) const
+//////////////
+// variance //
+//////////////
+void TransformationLearner::variance(Mat& covar) const
 {
-// Compute the costs from *already* computed output.
-// ...
+    PLERROR("variance not implemented for TransformationLearner");
 }
 
-//TO DO
-TVec<string> TransformationLearner::getTestCostNames() const
-{
-    // Return the names of the costs computed by computeCostsFromOutputs
-    // (these may or may not be exactly the same as what's returned by
-    // getTrainCostNames).
-    // ..
+
+
+//INITIALIZATION METHODS 
+
+
+//! initialization operations that have to be done before the training
+//!WARNING: the trainset ("train_set") must be given
+void TransformationLearner::trainBuild(){
     
-    return TVec<string>();
-}
+    
+    transformsSD = sqrt(transformsVariance);
+    
+    //DIMENSION VARIABLES
+    
+    //dimension of the input space
+    inputSpaceDim = train_set->inputsize();
+      
+    //number of samples given in the training set
+    trainingSetLength = train_set->length();
+    
+    
+    //number of reconstruction candidates related to a specific target in the 
+    //reconstruction set.   
+    nbTargetReconstructions = nbNeighbors * nbTransforms;
 
-//TO DO
-TVec<string> TransformationLearner::getTrainCostNames() const
-{
-    // Return the names of the objective costs that the train method computes
-    // and for which it updates the VecStatsCollector train_stats
-    // (these may or may not be exactly the same as what's returned by
-    // getTestCostNames).
-    // ...
-    return TVec<string>();
+    //total number of reconstruction candidates in the reconstruction set
+    nbReconstructions = trainingSetLength * nbTargetReconstructions;
+    
+    
+    //LEARNED MODEL PARAMETERS
+    
+    //set of transformations (represented as a single matrix)
+    transformsSet = Mat(nbTransforms * inputSpaceDim, inputSpaceDim);
+    
+    //view on the set of transformations (vector)
+    //    each transformation = one matrix 
+    transforms.resize(nbTransforms);
+    for(int k = 0; k< nbTransforms; k++){
+        transforms[k] = transformsSet.subMatRows(k * inputSpaceDim, inputSpaceDim);       
+    }
+    
+    if(withBias){
+        biasSet = Mat(nbTransforms,inputSpaceDim);
+    }
+    
+    initTransformsParameters();
+
+   
+    if(transformsPeriod == UNDEFINED || transformsOffset == UNDEFINED){
+        if(learnNoiseVariance){
+            transformsPeriod = 2;
+            transformsOffset = 1;
+        }
+        else{
+            transformsPeriod = 1;
+            transformsOffset = 0;
+        }
+    }
+
+    //training parameters for noise variance
+    if(learnNoiseVariance){
+        if(noiseVariancePeriod == UNDEFINED || noiseVarianceOffset == UNDEFINED){
+            noiseVariancePeriod = 2;
+            noiseVarianceOffset = 1;
+        }
+        if(regOnNoiseVariance){
+            if(noiseAlpha < 1)
+                noiseAlpha = 1;
+            if(noiseBeta <= 0){
+                noiseBeta = 1;
+            }
+        }
+        else{
+            noiseAlpha = NOISE_ALPHA_NO_REG;
+            noiseBeta = NOISE_BETA_NO_REG;
+        }
+    }
+    
+    //initialize the noise variance
+     if(noiseVariance == UNDEFINED){
+        if(learnNoiseVariance && regOnNoiseVariance){
+            initNoiseVariance();
+        }
+        else{
+            noiseVariance = 1.0;
+        }
+     }
+    
+     //training parameters for transformation distribution
+     if(learnTransformDistribution){
+         if(transformDistributionPeriod == UNDEFINED || transformDistributionOffset == UNDEFINED){
+             transformDistributionPeriod = 1;
+             transformDistributionOffset = 0;
+         }
+         if(regOnTransformDistribution){
+             if(transformDistributionAlpha<=0){
+                 transformDistributionAlpha =10;
+             }
+             else{
+                 transformDistributionAlpha = TRANSFORM_DISTRIBUTION_ALPHA_NO_REG;
+             }
+         }
+     }
+
+
+    //transformDistribution
+    if(transformDistribution.length() == 0){
+        if(learnTransformDistribution && regOnTransformDistribution)
+            initTransformDistribution();
+        else{
+            transformDistribution.resize(nbTransforms);
+            real w = INIT_weight(1.0/nbTransforms);
+            for(int k=0; k<nbTransforms ; k++){
+                transformDistribution[k] = w;
+            }
+        }       
+    }
+    else{
+        PLASSERT(transformDistribution.length() == nbTransforms);
+        PLASSERT(isWellDefined(transformDistribution));
+    }
+
+    //reconstruction set 
+    reconstructionSet.resize(nbReconstructions);
+    
+    
+
+    //OTHER VARIABLES
+    
+    
+    //to store a view on the generation set 
+    //   (entries related to a specific target)
+    targetReconstructionSet.resize(nbTargetReconstructions);
+    
+    //Storage space used in the update of the transformation parameters
+    B_C = Mat(2 * nbTransforms * inputSpaceDim , inputSpaceDim);
+    
+    B.resize(nbTransforms);
+    C.resize(nbTransforms);
+    for(int k=0; k<nbTransforms; k++){
+        B[k]= B_C.subMatRows(k*inputSpaceDim, inputSpaceDim);
+    }
+    for(int k= nbTransforms ; k<2*nbTransforms ; k++){
+        C[(k % nbTransforms)] = B_C.subMatRows(k*inputSpaceDim, inputSpaceDim);
+    }
+    
+    
+    target.resize(inputSpaceDim);
+    neighbor.resize(inputSpaceDim);
 }
 
- /*********GENERATION MODULE *****************************************/
 
-    //The object is the representation of a learned distribution
-    //Are are methods to ensure the "generative behavior" of the object
-    //(Once the distribution is learned, we might be able to generate
-    // samples from it)
+//! initialization operations that have to be done before a generation process
+//! (all the undefined parameters will be initialized  randomly)
+void TransformationLearner::generatorBuild( int inputSpaceDim_,
+                                            TVec<Mat> transforms_,
+                                            Mat biasSet_,
+                                            real noiseVariance_,
+                                            Vec transformDistribution_){
+    
+    inputSpaceDim = inputSpaceDim_;
+    transformsSD = sqrt(transformsVariance);
+    
 
+    //transformations parameters
 
+    
+    transformsSet = Mat(nbTransforms * inputSpaceDim, inputSpaceDim);
+    transforms.resize(nbTransforms);
+    for(int k = 0; k< nbTransforms; k++){
+        transforms[k] = transformsSet.subMatRows(k * inputSpaceDim, inputSpaceDim);       
+    }
+    
+    if(withBias){
+        biasSet = Mat(nbTransforms,inputSpaceDim);
+    }
 
-// TO TEST
-//Chooses the transformation parameters using a 
-//normal distribution with mean 0 and variance "transformsVariance"
-//(call generatorBuild() after)
-void TransformationLearner::buildTransformationParametersNormal(){
+    if(transforms_.length() == 0){
+        initTransformsParameters();
+    }
+    else{
+        setTransformsParameters(transforms_,biasSet_);
+    }
+    
+
+    //noise variance
+    if(noiseAlpha < 1){
+            noiseAlpha = 1;
+        }
+    if(noiseBeta <= 0){
+        noiseBeta = 1;
+    }
+    if(noiseVariance_ == UNDEFINED){
+        initNoiseVariance();
+    }
+    else{
+        setNoiseVariance(noiseVariance_);
+    }
+    //transformation distribution
+    if(transformDistributionAlpha <=0)
+        transformDistributionAlpha = 10;
+    if(transformDistribution_.length()==0){
+        initTransformDistribution();
+    }
+    else{
+        setTransformDistribution(transformDistribution_);
+    }
+}
+
+
+//!initializes the transformation parameters randomly 
+//!(prior distribution= Normal(0,transformsVariance))
+void TransformationLearner::initTransformsParameters()
+{
+    
     transformsSet.resize(nbTransforms*inputSpaceDim, inputSpaceDim);
     transforms.resize(nbTransforms);
+    for(int k = 0; k< nbTransforms; k++){
+        transforms[k] = transformsSet.subMatRows(k * inputSpaceDim, inputSpaceDim);       
+    }
+    int idx = 0;
     for(int t=0; t<nbTransforms ; t++){
-        random_gen->fill_random_normal(transforms[t], 0 , transformsStDev);
+        transforms[t] = transformsSet.subMatRows(idx,inputSpaceDim);
+        idx += inputSpaceDim;
+        random_gen->fill_random_normal(transforms[t], 0 , transformsSD);
     }
+    if(withBias){
+        biasSet = Mat(nbTransforms,inputSpaceDim);
+        random_gen->fill_random_normal(biasSet, 0,transformsSD);
+    }
+    if(transformFamily == TRANSFORM_FAMILY_LINEAR){
+        for(int t=0; t<nbTransforms;t++){
+            addToDiagonal(transforms[t],1.0);
+        }
+    }
 }
 
-
-//TO TEST
-//set the transformation parameters to the specified values
-//(call generatorBuild() after)
-void TransformationLearner::setTransformationParameters(TVec<Mat> & transforms_){
-     
+//!initializes the transformation parameters to the given values
+//!(bias are set to 0)
+void TransformationLearner::setTransformsParameters(TVec<Mat> transforms_,
+                                                    Mat biasSet_)
+{
     PLASSERT(transforms_.length() == nbTransforms);
-    inputSpaceDim = transforms_[0].width();
     
     int nbRows = inputSpaceDim*nbTransforms;
-    transformsSet = Mat(nbRows,inputSpaceDim);
+    transformsSet.resize(nbRows,inputSpaceDim);
     transforms.resize(nbTransforms);
-  
+    for(int k = 0; k< nbTransforms; k++){
+        transforms[k] = transformsSet.subMatRows(k * inputSpaceDim, inputSpaceDim);       
+    }
+
+
     int rowIdx = 0;
     for(int t=0; t<nbTransforms; t++){
+        PLASSERT(transforms_[t].width() == inputSpaceDim);
+        PLASSERT(transforms_[t].length() == inputSpaceDim);
         transformsSet.subMatRows(rowIdx,inputSpaceDim) << transforms_[t];
         transforms[t]= transformsSet.subMatRows(rowIdx,inputSpaceDim);
         rowIdx += inputSpaceDim;
     }
-}
-
-//TO TEST
-//creates a data set
-//
-//     Consists in building a tree of deepness d = "nbGenerations" and
-//     constant branch factor n = "generationLength"
-//
-//            0      1        2     ...         
-//  
-//            r - child1  - child1  ...       
-//                        - child2  ...
-//                            ...   ...
-//                        - childn  ...
-//
-//              - child2  - child1  ...
-//                        - child2  ...
-//                            ...   ...
-//                        - childn  ...
-//                     ...
-//             - childn   - child1  ...
-//                        - child2  ...
-//                            ...   ...
-//                        - childn  ... 
-//
-
-// all the childs are choosen following the same process:
-// 1) choose a transformation  
-// 2) apply the transformation to the parent
-// 3) add noise to the result 
-void TransformationLearner::createDataSet(Vec & root,
-                                          int nbGenerations,
-                                          int generationLength,
-                                          Mat & dataPoints){
-   
-    PLASSERT(root.length() == inputSpaceDim);
- 
-    //we look at the length of the given matrix dataPoint ;.  
-    int nbDataPoints = int(pow(1.0*nbGenerations,1.0*generationLength)) + 1;
-    dataPoints.resize(nbDataPoints,inputSpaceDim);
+    if(withBias){    
+        PLASSERT(biasSet_.length() == nbTransforms);
+        PLASSERT(biasSet_.width() == inputSpaceDim);
+        biasSet = Mat(nbTransforms, inputSpaceDim);
+        biasSet << biasSet_;
+    }
     
-    //root = first element in the matrix dataPoints
-    dataPoints(0) << root;
-  
-    //generate the other data points 
-    int centerIdx=0 ;
-    for(int dataIdx=1; dataIdx < nbDataPoints ; dataIdx+=generationLength){
 
-        Vec v = dataPoints(centerIdx);
-        Mat m = dataPoints.subMatRows(dataIdx, generationLength);
-        batchGenerateFrom(v,m); 
-        centerIdx ++ ;
-    }
 }
 
+//!initializes the noise variance randomly
+//!(gamma distribution)
+void TransformationLearner::initNoiseVariance()
+{
+    real noisePrecision = gamma_sample(noiseAlpha, noiseBeta);
+    PLASSERT(noisePrecision != 0);
+    noiseVariance = 1.0/noisePrecision;
+}
 
-//TO TEST
-//create a dataset using the same tree generation process as
-//createDataSet, except the number of child per parent is fixed to 1,
-//   root -> 1st point -> 2nd point ... -> nth point 
-void TransformationLearner::createDataSetSequentially(Vec & root,
-                                                      int n,
-                                                      Mat & dataPoints){
-    createDataSet(root, n-1, 1, dataPoints);
+//!initializes the noise variance with the given value
+void TransformationLearner::setNoiseVariance(real nv)
+{
+    PLASSERT(nv > 0);
+    noiseVariance = nv;
 }
 
 
-//TO TEST
-//Select a transformation randomly (with respect ot our transformation
-//distribution)
-int TransformationLearner::pickTransformIdx(){
-     return random_gen->multinomial_sample(transformDistribution);
+//!initializes the transformation distribution randomly
+//!(dirichlet distribution)
+void TransformationLearner::initTransformDistribution()
+{
+    
+    transformDistribution.resize(nbTransforms);
+    dirichlet_sample(transformDistributionAlpha, transformDistribution);
+    for(int i=0; i<nbTransforms ;i++){
+        transformDistribution[i] = INIT_weight(transformDistribution[i]);
+    } 
 }
 
-  
-//here is the generation process for a given center data point 
-//  1) choose a transformation
-//  2) apply it on the center data point
-//  3) add noise
+//!initializes the transformation distribution with the given values
+void TransformationLearner::setTransformDistribution(Vec td)
+{
+    PLASSERT(td.length() == nbTransforms);
+    PLASSERT(isWellDefined(td));
+    transformDistribution.resize(nbTransforms);
+    transformDistribution << td;
+}
 
-//TO TEST
-//generates a sample data point  from a  given center data point 
-void  TransformationLearner::generateFrom(Vec & center, Vec & sample){
+
+//GENERATION
+
+//!generates a sample data point from a source data point
+void TransformationLearner::generatePredictedFrom(const Vec & source,
+                                                  Vec & sample)const
+{
+    
     int transformIdx = pickTransformIdx();
-    generateFrom(center, sample, transformIdx);
+    generatePredictedFrom(source, sample, transformIdx);
 }
 
-//TO TEST
-//generates a sample data point from a given center data point
-void TransformationLearner::generateFrom(Vec & center,
-                                         Vec & sample, 
-                                         int transformIdx){
-    int d = center.length();
+//!generates a sample data point from a source data point with a specific transformation
+void TransformationLearner::generatePredictedFrom(const Vec & source,
+                                                  Vec & sample,
+                                                  int transformIdx)const
+{
+    //TODO
+    real noiseSD = pow(noiseVariance,0.5);
+    int d = source.length();
     PLASSERT(d == inputSpaceDim);
+    PLASSERT(sample.length() == inputSpaceDim);
+    PLASSERT(0<=transformIdx<nbTransforms);
     
-    sample.resize(inputSpaceDim);
-    
     //apply the transformation
-    applyTransformationOn(transformIdx,center,sample);
+    applyTransformationOn(transformIdx,source,sample);
     
     //add noise
     for(int i=0; i<d; i++){
-        sample[i] += random_gen->gaussian_mu_sigma(0, noiseStDev);
+        sample[i] += random_gen->gaussian_mu_sigma(0, noiseSD);
     } 
 }
 
-//TO TEST
-//fill the matrix "samples" with sample data points obtained from
-// a given center data point.
-void TransformationLearner::batchGenerateFrom(Vec & center, Mat & samples){
+//!generates a sample data point from a source data point and returns it
+//! (if transformIdx >= 0 , we use the corresponding transformation )
+Vec TransformationLearner::returnPredictedFrom(Vec source,
+                                               int transformIdx)
+{
+    Vec sample;
+    sample.resize(inputSpaceDim);
+    if(transformIdx <0)
+        generatePredictedFrom(source,sample);
+    else
+        generatePredictedFrom(source,sample,transformIdx);
+    return sample;
+}
 
+//!fill the matrix "samples" with data points obtained from a given center data point
+void TransformationLearner::batchGeneratePredictedFrom(const Vec & center,
+                                                        Mat & samples)const
+{
     PLASSERT(center.length() ==inputSpaceDim);
-    PLASSERT(samples.width()==inputSpaceDim);
+    PLASSERT(samples.width() ==inputSpaceDim);
     int l = samples.length();
     for(int i=0; i<l; i++)
     {
         Vec v = samples(i);
-        generateFrom(center, v);
+        generatePredictedFrom(center, v);
     }
 }
 
+//!fill the matrix "samples" with data points obtained form a given center data point
+//!    - we use a specific transformation
+void TransformationLearner::batchGeneratePredictedFrom(const Vec & center,
+                                                        Mat & samples,
+                                                        int transformIdx)const
+{
+    PLASSERT(center.length() ==inputSpaceDim);
+    PLASSERT(samples.width() ==inputSpaceDim);
+    int l = samples.length();
+    for(int i=0; i<l; i++)
+    {
+        Vec v = samples(i);
+        generatePredictedFrom(center, v,transformIdx);
+    }  
+}
 
+//Generates n samples from center and returns them stored in a matrix
+//    (generation process = 1) choose a transformation (*),
+//                          2) apply it on center
+//                          3) add noise)
+// - (*) if transformIdx>=0, we always use the corresponding transformation
+Mat TransformationLearner::returnGeneratedSamplesFrom(Vec center,
+                                                      int n,
+                                                      int transformIdx)
+{
+    Mat samples = Mat(n,inputSpaceDim);
+    if(transformIdx<0)
+        batchGeneratePredictedFrom(center,samples);
+    else
+        batchGeneratePredictedFrom(center,samples,transformIdx);
+    return samples;
+}
 
-//-------------EXTERNAL ACCESS ---------------------------
+//!select a transformation randomly (with respect to our multinomial distribution)
+int TransformationLearner::pickTransformIdx() const
+{
+    
+    Vec probaTransformDistribution ;
+    probaTransformDistribution.resize(nbTransforms);
+    for(int i=0; i<nbTransforms; i++){
+        probaTransformDistribution[i]=PROBA_weight(transformDistribution[i]);
+    }
+    return random_gen->multinomial_sample(probaTransformDistribution);
+}
 
-//TO TEST
-//Returns a copy of the generation candidates associated to a given target
-TVec<GenerationCandidate> TransformationLearner::returnReproductionSources
-(int targetIdx){
-    int startIdx = targetIdx * nbGenerationCandidatesPerTarget;
-    int endIdx = startIdx + nbGenerationCandidatesPerTarget;
-    return generationSet.subVec(startIdx, endIdx).copy();
+//!Select a neighbor in the training set randomly
+//!(return his index in the training set)
+//!We suppose all data points in the training set are equiprobables
+int TransformationLearner::pickNeighborIdx() const
+{
+    
+    return random_gen->uniform_multinomial_sample(trainingSetLength);
 }
 
-//TO TEST
-//Returns the parameters of a given transformation
-Mat TransformationLearner::returnTransform(int transformIdx){
-    return transforms[transformIdx].copy();   
+
+ //!creates a data set:
+//!     equivalent in building a tree with fixed deepness and constant branching factor
+//!
+//!            0      1        2     ...         
+//!  
+//!            r -> child1  -> child1  ...       
+//!                         -> child2  ...
+//!                             ...    ...
+//!                         -> childn  ...
+//!
+//!              -> child2  -> child1  ...
+//!                         -> child2  ...
+//!                              ...   ...
+//!                         -> childn  ...
+//!                      ...
+//!              -> childn  -> child1  ...
+//!                         -> child2  ...
+//!                              ...   ...
+//!                         -> childn  ... 
+//!
+//!(where "a -> b" stands for "a generate b")
+//!all the child are generated by the same following process:
+//! 1) choose a transformation  
+//! 2) apply the transformation to the parent
+//! 3) add noise to the result 
+void TransformationLearner::treeDataSet(const Vec & root,
+                                        int deepness,
+                                        int branchingFactor,
+                                        Mat & dataPoints)
+{
+
+    PLASSERT(root.length() == inputSpaceDim);
+
+    //we look at the length of the given matrix dataPoint ;
+    int nbDataPoints;
+    if(branchingFactor == 1)
+        nbDataPoints = deepness + 1;  
+    else nbDataPoints = int((1- pow(1.0*branchingFactor,deepness + 1.0))
+                            /
+                            (1 - branchingFactor));
+    dataPoints.resize(nbDataPoints,inputSpaceDim);
+    
+    //root = first element in the matrix dataPoints
+    dataPoints(0) << root;
+  
+    //generate the other data points 
+    int centerIdx=0 ;
+    for(int dataIdx=1; dataIdx < nbDataPoints ; dataIdx+=branchingFactor){
+        
+        Vec v = dataPoints(centerIdx);
+        Mat m = dataPoints.subMatRows(dataIdx, branchingFactor);
+        batchGeneratePredictedFrom(v,m); 
+        centerIdx ++ ;
+    }  
 }
 
-//TO TEST
-//Returns all the transformation parameters
-Mat TransformationLearner::returnAllTransforms(){
-    return transformsSet;
+Mat TransformationLearner::returnTreeDataSet(Vec root,
+                                             int deepness,
+                                             int branchingFactor)
+{
+    Mat dataPoints;
+    treeDataSet(root,deepness,branchingFactor, dataPoints);
+    return dataPoints;
 }
 
 
-//TO TEST
-//From the subset ofgeneration candidate associated to the target,
-//builds and returns the corresponding subset of generated data points . 
-Mat TransformationLearner::returnReproductions(int targetIdx){
-    Mat reproductions = Mat(nbGenerationCandidatesPerTarget,inputSpaceDim);
-    int candidateIdx = targetIdx*nbGenerationCandidatesPerTarget;
-    int neighborIdx, transformIdx;
-    for(int i=0; i<nbGenerationCandidatesPerTarget; i++){
-        neighborIdx = generationSet[candidateIdx].neighborIdx;
-        transformIdx= generationSet[candidateIdx].transformIdx;
-        getNeighborFromTrainingSet(neighborIdx);
-        Vec v = reproductions(i);
-        applyTransformationOn(transformIdx, neighbor, v);
-        candidateIdx ++;
-    }
-    return reproductions;
+//!create a "sequential" dataset:
+//!  start -> second point -> third point ... ->nth point
+//! (where "->" stands for : "generate the")
+void TransformationLearner::sequenceDataSet(const Vec & start,
+                                            int n,
+                                            Mat & dataPoints)
+{
+    treeDataSet(start,n-1,1,dataPoints);
 }
 
-
-//TO TEST
-//Generates n samples from center and returns them
-//    (generation process = 1) choose a transformation,
-//                          2) apply it on center
-//                          3) add noise)
-Mat TransformationLearner::returnGeneratedSamplesFrom(Vec center, int n){
-    int d = center.length();
-    PLASSERT(d == inputSpaceDim);
-    Mat m = Mat(n,d);
-    batchGenerateFrom(center, m);
-    return m;
+Mat TransformationLearner::returnSequenceDataSet(Vec start,
+                                                 int n)
+{
+    Mat dataPoints;
+    sequenceDataSet(start,n,dataPoints);
+    return dataPoints;
 }
 
-//TO TEST
-//Generates a data set and returns it
-//(tree generation process: see createDataSet for more details)
-Mat TransformationLearner::returnGeneratedDataSet(Vec root,
-                                                  int nbGenerations,
-                                                  int generationLength){
- 
-    int n = int(pow(1.0*nbGenerations, 1.0*generationLength)) + 1;
-    int d = root.length();
-    PLASSERT(d == inputSpaceDim);
 
-    Mat dataSet = Mat(n,d);
-    createDataSet(root,nbGenerations,generationLength,dataSet);
-    return dataSet;
+//! COPIES OF THE STRUCTURES
+
+
+//!returns the "idx"th data point in the training set
+Vec TransformationLearner::returnTrainingPoint(int idx)
+{
+    
+    Vec v,temp;
+    real w;
+    v.resize(inputSpaceDim);
+    train_set->getExample(idx, v, temp, w);
+    return v;
+    
 }
+ 
 
-//TO TEST
-//Generates a data set and returns it
-//(sequential generation process: see createDataSetSequentially for more details)
-Mat TransformationLearner::returnSequentiallyGeneratedDataSet(Vec root,int n){ 
-    return returnGeneratedDataSet(root, n-1,1);
+//!returns all the reconstructions candidates associated to a given target
+TVec<ReconstructionCandidate> TransformationLearner::returnReconstructionCandidates(int targetIdx)
+{
+   
+    int startIdx = targetIdx * nbTargetReconstructions;  
+    return reconstructionSet.subVec(startIdx, 
+                                    nbTargetReconstructions).copy();
 }
 
 
-// ----------GENERAL USE--------------------------------------------------
+//!returns the reconstructions of the "targetIdx"th data point value in the training set
+//!(one reconstruction for each reconstruction candidate)
+Mat TransformationLearner::returnReconstructions(int targetIdx)
+{
+    Mat reconstructions = Mat(nbTargetReconstructions,inputSpaceDim);
+    int candidateIdx = targetIdx*nbTargetReconstructions;
+    int neighborIdx, transformIdx;
+    for(int i=0; i<nbTargetReconstructions; i++){
+        neighborIdx = reconstructionSet[candidateIdx].neighborIdx;
+        transformIdx= reconstructionSet[candidateIdx].transformIdx;
+        seeNeighbor(neighborIdx);
+        Vec v = reconstructions(i);
+        applyTransformationOn(transformIdx, neighbor, v);
+        candidateIdx ++;
+    }
+    return reconstructions; 
+}
 
+//!returns the neighbors choosen to reconstruct the target
+//!(one choosen neighbor for each reconstruction candidate associated to the target)
+Mat TransformationLearner::returnNeighbors(int targetIdx)
+{
+    int candidateIdx = targetIdx*nbTargetReconstructions;
+    int neighborIdx;
+    Mat neighbors = Mat(nbTargetReconstructions, inputSpaceDim);
+    for(int i=0; i<nbTargetReconstructions; i++){
+        neighborIdx = reconstructionSet[candidateIdx].neighborIdx;
+        seeNeighbor(neighborIdx);
+        neighbors(i) << neighbor;
+        candidateIdx++;
+    }
+    return neighbors;
+}
 
 
-//REFERENCE OPERATIONS ON GENERATION SET AND TRAINING SET  
+//!returns the parameters of the "transformIdx"th transformation
+Mat TransformationLearner::returnTransform(int transformIdx)
+{
+    return transforms[transformIdx].copy();    
+}
 
-
-//TO TEST
-// stores a view on the subset of generation set related to the specified
-// target (into the variable "targetGenerationSet" )
-void TransformationLearner::getViewOnTargetGenerationCandidates(int targetIdx){
-    int startIdx = targetIdx * nbGenerationCandidatesPerTarget;
-    int endIdx = startIdx + nbGenerationCandidatesPerTarget;
-    targetGenerationSet = generationSet.subVec(startIdx, 
-                                               endIdx);
-    
+//!returns the parameters of each transformation
+//!(as an KdXd matrix, K = number of transformations,
+//!                    d = dimension of input space)
+Mat TransformationLearner::returnAllTransforms()
+{
+    return transformsSet.copy();    
 }
 
 
+//! VIEWS ON RECONSTRUCTION SET AND TRAINING SET
 
 
 
-// stores the "targetIdx"th input in the training set into the variable
+//! stores a VIEW on the reconstruction candidates related to the specified
+//! target (into the variable "targetReconstructionSet" )
+void TransformationLearner::seeTargetReconstructionSet(int targetIdx)
+{
+    int startIdx = targetIdx *nbTargetReconstructions;
+    targetReconstructionSet = reconstructionSet.subVec(startIdx, 
+                                                       nbTargetReconstructions); 
+}
+
+// stores the "targetIdx"th point in the training set into the variable
 // "target"
-void TransformationLearner::getTargetFromTrainingSet(int targetIdx){
+void TransformationLearner::seeTarget(const int targetIdx)
+{
     Vec v;
     real w;
     train_set->getExample(targetIdx,target,v,w);
-
-    //TO TEST : OK
+    
 }
 
 // stores the "neighborIdx"th input in the training set into the variable
 // "neighbor" 
-void TransformationLearner::getNeighborFromTrainingSet(int neighborIdx){
+void TransformationLearner::seeNeighbor(const int neighborIdx)
+{
     Vec v;
     real w;
-    train_set->getExample(neighborIdx,neighbor,v,w);
+    train_set->getExample(neighborIdx, neighbor,v,w);
+}
+
+
+//! GENERATE GAMMA RANDOM VARIABLES
+
+//!source of the algorithm: http://oldmill.uchicago.edu/~wilder/Code/random/Papers/Marsaglia_00_SMGGV.pdf
     
-    //TO TEST : OK
+
+//!returns a pseudo-random positive real number x  
+//!using the distribution p(x)=Gamma(alpha,beta)
+real TransformationLearner::gamma_sample(real alpha, real beta)
+{
+  real c,x,u,d,v;
+  do{
+      c = 1.0/3.0;
+      x = random_gen->gaussian_01();
+      u = random_gen->uniform_sample();
+      d = alpha - c ;
+      v = pow((1 + x/(pow(9*d , 0.5)))  ,3.0);
+  }
+  while(pl_log(u) < 0.5*pow(x,2) + d - d*v + d*pl_log(v));
+  return d*v/beta;   
 }
 
 
-//OPERATIONS RELATED TO GENERATION WEIGHTS
 
-//normalizes the generation weights related to a given target. 
-void TransformationLearner::normalizeTargetGenerationWeights(int targetIdx, 
-                                                             real totalWeight){
+
+//! GENERATE DIRICHLET RANDOM VARIABLES
+
+
+ //!source of the algorithm: WIKIPEDIA
+    
+
+//!returns a pseudo-random positive real vector x 
+//!using the distribution p(x) = Dirichlet(x| all the parameters = alpha)
+//!-all the element of the vector are between 0 and 1,
+//!-the elements of the vector sum to 1
+void TransformationLearner::dirichlet_sample(real alpha, Vec & sample){
+    int d = sample.length();
+    real sum = 0;
+    for(int i=0;i<d;i++){
+        sample[i]=gamma_sample(alpha);
+        sum += sample[i];
+    }
+    for(int i=0;i<d;i++){
+        sample[i]/=sum;
+    }
+}
+Vec TransformationLearner::return_dirichlet_sample(real alpha)
+{
+    Vec sample ;
+    sample.resize(inputSpaceDim);
+    dirichlet_sample(alpha, sample);
+    return sample;
+}
+
+
+
+/*void TransformationLearner::dirichlet_sample(const Vec & alphas,
+                                        Vec & samples)
+{
+    //TODO
+}
+Vec TransformationLearner::return_dirichlet_sample(Vec alphas)
+{
+    //TODO
+    return Vec();
+}
+*/
+
+
+
+//! OPERATIONS ON WEIGHTS
+
+
+//!normalizes the reconstruction weights related to a given target.
+void TransformationLearner::normalizeTargetWeights(int targetIdx,
+                                                   real totalWeight)const
+{
     real w;
-    int startIdx = targetIdx * nbGenerationCandidatesPerTarget;
-    int endIdx = startIdx + nbGenerationCandidatesPerTarget;
+    int startIdx = targetIdx * nbTargetReconstructions;
+    int endIdx = startIdx + nbTargetReconstructions;
     for(int candidateIdx =startIdx; candidateIdx<endIdx; candidateIdx++){
-        w = generationSet[candidateIdx].weight;
-        generationSet[candidateIdx].weight =  DIV_weights(w,totalWeight);
+        w = reconstructionSet[candidateIdx].weight;
+        reconstructionSet[candidateIdx].weight =  DIV_weights(w,totalWeight);
     }
+}
 
-    //TO TEST : OK
+//!returns a random weight 
+real TransformationLearner::randomWeight()const
+{  
+    real w = random_gen->uniform_sample();
+    return INIT_weight((w + minimumProba)/(1.0 + minimumProba));
 }
-    
-//returns a random positive weight 
-real TransformationLearner::randomPositiveGenerationWeight(){
-    return  random_gen->uniform_sample() + epsilonInitWeight;
 
-    //TO TEST : OK
+//!arithmetic operations on  reconstruction weights : CONSTRUCTOR
+//!proba->weight
+real TransformationLearner::INIT_weight(real initValue)const
+{
+    return pl_log(initValue);
 }
-  
 
-//arithmetic operations on  generation weights
-real TransformationLearner::DIV_weights(real numWeight,    //DIVISION
-                                        real denomWeight){ 
-    return numWeight/denomWeight;
+//!arithmetic operations on  reconstruction weights :GET CORRESPONDING PROBABILITY 
+//! weight->proba
+real TransformationLearner::PROBA_weight(real weight)const
+{
+    return exp(weight); 
+}
 
-    //TO TEST : OK
+//!arithmetic operations on  reconstruction weights : DIVISION
+//! In our particular case:
+//!  numWeight = log(w1)
+//!  denomWeight = log(w2)
+//! and we want weight = log(w1/w2) = log(w1) - log(w2) 
+//!                             = numweight - denomWeight
+real TransformationLearner::DIV_weights(real numWeight,
+                                        real denomWeight)const
+{
+    return numWeight - denomWeight;
 }
-real TransformationLearner::MULT_INVERSE_weight(real weight){//MULTIPLICATIVE INVERSE
-    return -weight;
 
-    //TO TEST : OK
+
+//!arithmetic operations on  reconstruction weights :MULTIPLICATIVE INVERSE
+//! weight = log(p)
+//!we want : weight' = log(1/p) = log(1) - log(p)
+//!                             =     0 -  log(p)
+//!                             = -weight
+real TransformationLearner::MULT_INVERSE_weight(real weight)const
+{
+    
+    return -1*weight;
 }
-real TransformationLearner::MULT_weights(real weight1, real weight2){ //MULTIPLICATION
-    return weight1*weight2;
+
+//!arithmetic operations on  reconstruction weights: MULTIPLICATION
+//! weight1 = log(p1)
+//! weight2 = log(p2)
+//! we want weight3 = log(p1*p2) = log(p1) + log(p2)
+//!                              = weight1 + weight2
+real TransformationLearner::MULT_weights(real weight1,real weight2)const
+{
     
-    //TO TEST : OK
+    return weight1 + weight2 ;
 }
-real TransformationLearner::SUM_weights(real weight1, real weight2){ //SUM
-    return weight1 + weight2;
 
-    //TO TEST : OK
-} 
-
-//TO TEST
-//update/compute the weight of a generation candidate with
-//the actual transformation parameters
-real TransformationLearner::updateGenerationWeight(int candidateIdx){
+//!arithmetic operations on  reconstruction weights : SUM 
+//! weight1 = log(p1)
+//! weight2 = log(p2)
+//! we want : weight3 = log(p1 + p2) = logAdd(weight1, weight2)
+real TransformationLearner::SUM_weights(real weight1,
+                                        real weight2)const
+{
     
-    GenerationCandidate * gc = & generationSet[candidateIdx];
-    
-    real w = computeGenerationWeight(gc->targetIdx,
-                                     gc->neighborIdx,
-                                     gc->transformIdx);
-    pout << "weigth:"<< w <<endl;
-    gc->weight = w;
-    return w;
+    return logadd(weight1,weight2);
 }
 
-//TO TEST
-real TransformationLearner::computeGenerationWeight(GenerationCandidate & gc){
-    return computeGenerationWeight(gc.targetIdx,
-                                   gc.neighborIdx,
-                                   gc.transformIdx);
 
-   
+
+//!update/compute the weight of a reconstruction candidate with
+//!the actual transformation parameters
+real TransformationLearner::updateReconstructionWeight(int candidateIdx)
+{
+    int targetIdx = reconstructionSet[candidateIdx].targetIdx;
+    int neighborIdx = reconstructionSet[candidateIdx].neighborIdx;
+    int transformIdx = reconstructionSet[candidateIdx].transformIdx;
+    
+    real w = computeReconstructionWeight(targetIdx,
+                                         neighborIdx,
+                                         transformIdx);
+    reconstructionSet[candidateIdx].weight = w;
+    return w; 
 }
-
-//TO TEST
-real TransformationLearner::computeGenerationWeight(int targetIdx, 
-                                                    int neighborIdx, 
-                                                    int transformIdx){
-  
-
-    getTargetFromTrainingSet(targetIdx);
-    getNeighborFromTrainingSet(neighborIdx);
+real TransformationLearner::computeReconstructionWeight(const ReconstructionCandidate & gc)
+{
+    return computeReconstructionWeight(gc.targetIdx,
+                                       gc.neighborIdx,
+                                       gc.transformIdx);
+}
+real TransformationLearner::computeReconstructionWeight(int targetIdx,
+                                                        int neighborIdx,
+                                                        int transformIdx)
+{
+    seeTarget(targetIdx);
+    return computeReconstructionWeight(target,
+                                       neighborIdx,
+                                       transformIdx);
+}
+real TransformationLearner::computeReconstructionWeight(const Vec & target_,
+                                                        int neighborIdx,
+                                                        int transformIdx)
+{
+    seeNeighbor(neighborIdx);
     Vec predictedTarget ;
     predictedTarget.resize(inputSpaceDim);
     applyTransformationOn(transformIdx, neighbor, predictedTarget);
-    return exp(noiseVarianceFactor * powdistance(target, predictedTarget));
-
-    
+    real factor = -1/(2*noiseVariance);
+    real w = factor*powdistance(target_, predictedTarget);
+    return MULT_weights(w, transformDistribution[transformIdx]); 
 }
 
-
-//applies "transformIdx"th transformation on data point "src"
-void TransformationLearner::applyTransformationOn(int transformIdx, Vec & src, Vec &  dst){
-    if(transformFamily == TRANSFORM_FAMILY_LINEAR){
+//!applies "transformIdx"th transformation on data point "src"
+void TransformationLearner::applyTransformationOn(int transformIdx,
+                                                 const Vec & src,
+                                                 Vec & dst)const
+{
+    if(transformFamily==TRANSFORM_FAMILY_LINEAR){
         Mat m  = transforms[transformIdx];
-        product(dst,m,src);
+        transposeProduct(dst,m,src); 
+        if(withBias){
+            dst += biasSet(transformIdx);
+        }
     }
-    else{
+    else{ //transformFamily == TRANSFORM_FAMILY_LINEAR_INCREMENT
         Mat m = transforms[transformIdx];
-        product(dst, m, src);
+        transposeProduct(dst,m,src);
         dst += src;
+        if(withBias){
+            dst += biasSet(transformIdx);
+        }
     }
 }
 
-//-------- INITIAL E STEP --------------------------------------------
+//! verify if the multinomial distribution given is well-defined
+//! i.e. verify that the weights represent probabilities, and that 
+//! those probabilities sum to 1 . 
+//!(typical case: the distribution is represented as a set of weights, which are typically
+//! log-probabilities)
+bool  TransformationLearner::isWellDefined(Vec & distribution)
+{  
+    if(nbTransforms != distribution.length()){
+        return false;
+    }
+    real sum = 0;
+    real proba;
+    for(int i=0; i<nbTransforms;i++){
+        proba = PROBA_weight(distribution[i]);
+        if(proba < 0 || proba > 1){
+            return false;
+        }
+        sum += proba;
+    }
+    return sum == 1;    
+}
 
-//initialization of the generation set 
+
+//! INITIAL E STEP
+
+//!initialization of the reconstruction set
 void TransformationLearner::initEStep(){
+    if(initializationMode == INIT_MODE_DEFAULT){
+        initEStepA();
+    }
+    else
+        initEStepB();
+}
+
+//!initialization of the reconstruction set, version B
+//!we suppose that all the parameters to learn are already initialized to some value
+//1)for each target,
+//  a)find the neighbors
+//  b)for each neighbor, consider all the possible transformations
+//2)compute the weights of all the reconstruction candidates using 
+//  the current value of the parameters to learn 
+void TransformationLearner::initEStepB(){
+    initEStepA();
+    smallEStep();    
+}
+
+
+//!initialization of the reconstruction set, version A
+//for each target:
+//1)find the neighbors (we use euclidean distance as an heuristic)
+//2)for each neighbor, assign a random weight to each possible transformation
+void TransformationLearner::initEStepA()
+{
+   
     priority_queue< pair< real,int > > pq = priority_queue< pair< real,int > >();
     
     real totalWeight;
     int candidateIdx=0,targetStartIdx, neighborIdx;
     
     //for each point in the training set i.e. for each target point,
-    for(int targetIdx = 0; targetIdx < nbTrainingInput ;targetIdx++){
-    
+    for(int targetIdx = 0; targetIdx < trainingSetLength ;targetIdx++){
+        
         //finds the nearest neighbors and keep them in a priority queue 
         findNearestNeighbors(targetIdx, pq);
         
@@ -943,67 +1693,70 @@
         //(i.e. for each neighbor, creates one entry per transformation and 
         //assignsit a positive random weight)
         
-        totalWeight =0;
+        totalWeight = INIT_weight(0);
         targetStartIdx = candidateIdx;
         for(int k = 0; k < nbNeighbors; k++){
             neighborIdx = pq.top().second;
             pq.pop();
-            totalWeight =SUM_weights(expandTargetNeighborPairInGenerationSet(targetIdx, 
-                                                                             neighborIdx,
-                                                                             candidateIdx),
-                                     totalWeight);
+            totalWeight =
+                SUM_weights(totalWeight,
+                            expandTargetNeighborPairInReconstructionSet(targetIdx, 
+                                                                        neighborIdx,
+                                                                        candidateIdx));
             candidateIdx += nbTransforms;
         }
         //normalizes the  weights of all the entries created for the target 
         //point
-        normalizeTargetGenerationWeights(targetIdx,totalWeight);
+        normalizeTargetWeights(targetIdx,totalWeight);
     }
 
-    //TO TEST
 }
-    
-//auxialiary function of "initEStep" . 
-//    for a given pair (target, neighbor), creates all the associated 
-//    generation candidates (entries) in the data set. 
-//returns the total weight of the generation candidates created
-real TransformationLearner::expandTargetNeighborPairInGenerationSet(int targetIdx,
-                                                                    int neighborIdx,
-                                                                    int candidateIdx){
-    real weight, totalWeight = 0;  
+
+
+//!auxialiary function of "initEStep" . 
+//!    for a given pair (target, neighbor), creates all the  
+//!    possible reconstruction candidates. 
+//!returns the total weight of the reconstruction candidates created
+real TransformationLearner::expandTargetNeighborPairInReconstructionSet(int targetIdx,
+                                                                        int neighborIdx,
+                                                                        int candidateStartIdx)
+{
+    int candidateIdx = candidateStartIdx;
+    real weight, totalWeight = INIT_weight(0);  
     for(int transformIdx=0; transformIdx<nbTransforms; transformIdx ++){
-        //choose a random positive weight
-        weight = randomPositiveGenerationWeight(); 
-        totalWeight = SUM_weights(weight,totalWeight);
-        generationSet[candidateIdx] = GenerationCandidate(targetIdx, 
-                                                          neighborIdx,
-                                                          transformIdx,
-                                                          weight);
+       
+        weight = randomWeight(); 
+        totalWeight = SUM_weights(totalWeight,weight);
+        reconstructionSet[candidateIdx] = ReconstructionCandidate(targetIdx, 
+                                                                  neighborIdx,
+                                                                  transformIdx,
+                                                                  weight);
     
         candidateIdx ++;
     }
     return totalWeight;    
-
-    //TO TEST
 }
 
-//auxiliary function of initEStep
-//    keeps the nearest neighbors for a given target point in a priority
-//    queue.
-void TransformationLearner::findNearestNeighbors (int targetIdx,
-                                                  priority_queue< pair< real, int > > & pq){
+
+//!auxiliary function of initEStep
+//!    stores the nearest neighbors for a given target point in a priority
+//!    queue.
+void TransformationLearner::findNearestNeighbors(int targetIdx,
+                                                 priority_queue< pair< real, int > > & pq)
+{
     
     //we want an empty queue
     PLASSERT(pq.empty()); 
   
     //capture the target from his index in the training set
-    getTargetFromTrainingSet(targetIdx);
+    seeTarget(targetIdx);
      
     //for each potential neighbor,
     real dist;    
-    for(int i=0; i<nbTrainingInput; i++){
+    for(int i=0; i<trainingSetLength; i++){
         if(i != targetIdx){ //(the target cannot be his own neighbor)
             //computes the distance to the target
-            getNeighborFromTrainingSet(i);
+            seeNeighbor(i);
             dist = powdistance(target, neighbor); 
             //if the distance is among "nbNeighbors" smallest distances seen,
             //keep it until to see a closer neighbor. 
@@ -1014,34 +1767,53 @@
                 pq.pop();
                 pq.push(pair<real,int>(dist,i));
             }
+            else if(dist == pq.top().first){
+                if(random_gen->uniform_sample() >0.5){
+                    pq.pop();
+                    pq.push(pair<real,int>(dist,i));
+                }
+            }
         }
     }    
+}
 
-    //TO TEST
+//! ESTEP 
+
+//!coordination of the different kinds of expectation steps
+//!  -which are : largeEStepA, largeEStepB, smallEStep
+void TransformationLearner::EStep()
+{
+    if(largeEStepAPeriod > 0  && stage % largeEStepAPeriod == largeEStepAOffset){
+        largeEStepA();
+    }
+    if(largeEStepBPeriod>0 && stage % largeEStepBPeriod == largeEStepBOffset){
+        largeEStepB();
+    }
+    smallEStep(); 
 }
 
 
-//-------- LARGE E STEP : VERSION A --------------------------------
+//! LARGE E STEP : VERSION A (expectation step)
 
-//full update of the generation set
-//for each target, keeps the top km most probable <neighbor, transformation> 
-//pairs (k = nb neighbors, m= nb transformations)
-void TransformationLearner::largeEStepA(){
-    
-    priority_queue< GenerationCandidate > pq =  priority_queue< GenerationCandidate >();
-    real totalWeight=0;
+//!full update of the reconstruction set
+//!for each target, keeps the km most probable <neighbor, transformation> 
+//!pairs (k = nb neighbors, m= nb transformations)
+void TransformationLearner::largeEStepA()
+{
+    priority_queue< ReconstructionCandidate > pq =  
+        priority_queue< ReconstructionCandidate >();
+    real totalWeight= INIT_weight(0);
     int candidateIdx=0;
     
     //for each point in the training set i.e. for each target point,
-    for(int targetIdx = 0; targetIdx < nbTrainingInput ; targetIdx++){
+    for(int targetIdx = 0; targetIdx < trainingSetLength ; targetIdx++){
         
         //finds the best weighted triples and keep them in a priority queue 
-        findBestTargetCandidates(targetIdx, pq);
-        
+        findBestTargetReconstructionCandidates(targetIdx, pq);
         //store those triples in the dataset:
-        totalWeight = 0;
-        for(int k=0; k < nbGenerationCandidatesPerTarget; k++){
-            generationSet[candidateIdx] = pq.top(); 
+        totalWeight = INIT_weight(0);
+        for(int k=0; k < nbTargetReconstructions; k++){
+            reconstructionSet[candidateIdx] = pq.top(); 
             totalWeight = SUM_weights(pq.top().weight, totalWeight);
             pq.pop();         
             candidateIdx ++;
@@ -1049,203 +1821,328 @@
         
         //normalizes the  weights of all the entries created for the 
         //target point;
-        normalizeTargetGenerationWeights(targetIdx,totalWeight);
-    }
-
-    //TO TEST
+        normalizeTargetWeights(targetIdx,totalWeight);
+    } 
 }
 
-//auxiliary function of largeEStepA()
-//   for a given target, keeps the top km most probable neighbors,
-//   transformation pairs in a priority queue 
-//   (k = nb neighbors, m = nb transformations)
-void  TransformationLearner::findBestTargetCandidates
-(int targetIdx,
- priority_queue< GenerationCandidate > & pq){
-    
+
+//!auxiliary function of largeEStepA()
+//!   for a given target, stores the km most probable (neighbors,
+//!   transformation) pairs in a priority queue 
+//!   (k = nb neighbors, m = nb transformations)
+void TransformationLearner::findBestTargetReconstructionCandidates(int targetIdx,
+                                                                   priority_queue< ReconstructionCandidate > & pq)
+{
     //we want an empty queue
     PLASSERT(pq.empty()); 
+    
     real weight;
 
     //for each potential neighbor
-    for(int neighborIdx=0; neighborIdx<nbTrainingInput; neighborIdx++){
+    for(int neighborIdx=0; neighborIdx<trainingSetLength; neighborIdx++){
         if(neighborIdx != targetIdx){
             for(int transformIdx=0; transformIdx<nbTransforms; transformIdx++){
-                weight = computeGenerationWeight(targetIdx, 
-                                                 neighborIdx, 
-                                                 transformIdx);
+                weight = computeReconstructionWeight(targetIdx, 
+                                                     neighborIdx, 
+                                                     transformIdx);
                 
                 //if the weight is among "nbEntries" biggest weight seen,
                 //keep it until to see a bigger neighbor. 
-                if(int(pq.size()) < nbGenerationCandidatesPerTarget){
-                    pq.push(GenerationCandidate(targetIdx,
-                                                neighborIdx,
-                                                transformIdx,
-                                                weight));  
+                if(int(pq.size()) < nbTargetReconstructions){
+                    pq.push(ReconstructionCandidate(targetIdx,
+                                                    neighborIdx,
+                                                    transformIdx,
+                                                    weight));  
                 }
-                else if (weight > pq.top().weight){
+                else if (weight > pq.top().weight){ 
                     pq.pop();
-                    pq.push(GenerationCandidate(targetIdx,
-                                                neighborIdx,
-                                                transformIdx,
-                                                weight));
+                    pq.push(ReconstructionCandidate(targetIdx,
+                                                    neighborIdx,
+                                                    transformIdx,
+                                                    weight));
                 }
+                else if (weight == pq.top().weight){
+                    if(random_gen->uniform_sample()>0.5){
+                        pq.pop();
+                        pq.push(ReconstructionCandidate(targetIdx,
+                                                        neighborIdx,
+                                                        transformIdx,
+                                                        weight));
+                    }
+                }
             }
         }     
     }
-
-    //TO TEST
 }
 
 
-//-------- LARGE E STEP : VERSION B --------------------------------
 
-//full update of the generation set
-//   for each given pair (target, transformation), find the best
-//   weighted neighbors  
-void  TransformationLearner::largeEStepB(){
-    priority_queue< GenerationCandidate > pq;
+//! LARGE E STEP : VERSION B (expectation step)
+
+
+//!full update of the reconstruction set
+//!   for each given pair (target, transformation), find the best
+//!   weighted neighbors  
+void TransformationLearner::largeEStepB()
+{
+    priority_queue< ReconstructionCandidate > pq;
     
-  real totalWeight=0 , weight;
-  int candidateIdx=0 ;
-  
-  //for each point in the training set i.e. for each target point,
-  for(int targetIdx =0; targetIdx<nbTrainingInput ;targetIdx++){
-  
-      totalWeight = 0;
-      for(int transformIdx=0; transformIdx < nbTransforms; transformIdx ++){
-          //finds the best weighted triples   them in a priority queue 
-          findBestWeightedNeighbors(targetIdx,transformIdx, pq);
-          
-          //store those neighbors in the dataset
-          for(int k=0; k<nbNeighbors; k++){
-              generationSet[candidateIdx] = pq.top();
-              weight = pq.top().weight;
-              totalWeight = SUM_weights( weight, totalWeight);
-              pq.pop();
-              candidateIdx ++;
-          }
-      }
+    real totalWeight , weight;
+    int candidateIdx=0 ;
+    
+    //for each point in the training set i.e. for each target point,
+    for(int targetIdx =0; targetIdx<trainingSetLength ;targetIdx++){
+        
+        totalWeight = INIT_weight(0);
+        for(int transformIdx=0; transformIdx < nbTransforms; transformIdx ++){
+            //finds the best weighted triples   them in a priority queue 
+            findBestWeightedNeighbors(targetIdx,transformIdx, pq);
+            //store those neighbors in the dataset
+            for(int k=0; k<nbNeighbors; k++){
+                reconstructionSet[candidateIdx] = pq.top();
+                weight = pq.top().weight;
+                totalWeight = SUM_weights( weight, totalWeight);
+                pq.pop();
+                candidateIdx ++;
+            }
+        }
       //normalizes the  weights of all the entries created for the target 
       //point;
-      normalizeTargetGenerationWeights(targetIdx,totalWeight);
-  }
-
-  // TO TEST
+        normalizeTargetWeights(targetIdx,totalWeight);
+    }
 }
+   
 
-    
-//auxiliary function of largeEStepB()
-//   for a given target x and a given transformationt , keeps the best
-//   weighted triples (x, neighbor, t) in a priority queue .
-void  TransformationLearner::findBestWeightedNeighbors
-(int targetIdx,
- int transformIdx,
- priority_queue< GenerationCandidate > & pq){
- 
+//!auxiliary function of largeEStepB()
+//!   for a given target x and a given transformation t , stores the best
+//!    weighted triples (x, neighbor, t) in a priority queue .
+void TransformationLearner::findBestWeightedNeighbors(int targetIdx,
+                                                      int transformIdx,
+                                                      priority_queue< ReconstructionCandidate > & pq)
+{
     //we want an empty queue
     PLASSERT(pq.empty()); 
-
+    
     real weight; 
     
     //for each potential neighbor
-    for(int neighborIdx=0; neighborIdx<nbTrainingInput; neighborIdx++){
+    for(int neighborIdx=0; neighborIdx<trainingSetLength; neighborIdx++){
         if(neighborIdx != targetIdx){ //(the target cannot be his own neighbor)
-            
-	  weight=  computeGenerationWeight(targetIdx, 
-                                           neighborIdx, 
-                                           transformIdx);
-	  //if the weight of the triple is among the "nbNeighbors" biggest 
-	  //seen,keep it until see a bigger weight. 
+          
+            weight = computeReconstructionWeight(targetIdx, 
+                                                 neighborIdx, 
+                                                 transformIdx);
+            //if the weight of the triple is among the "nbNeighbors" biggest 
+            //seen,keep it until see a bigger weight. 
             if(int(pq.size()) < nbNeighbors){
-                pq.push(GenerationCandidate(targetIdx,
-                                            neighborIdx, 
-                                            transformIdx,
-                                            weight));
+                pq.push(ReconstructionCandidate(targetIdx,
+                                                neighborIdx, 
+                                                transformIdx,
+                                                weight));
             }
-            else if (weight >  pq.top().weight){
+            else if (weight > pq.top().weight){
                 pq.pop();
-                pq.push(GenerationCandidate(targetIdx,
-                                            neighborIdx,
-                                            transformIdx,
-                                            weight));
+                pq.push(ReconstructionCandidate(targetIdx,
+                                                neighborIdx,
+                                                transformIdx,
+                                                weight));
             }
+            else if (weight == pq.top().weight){
+                if(random_gen->uniform_sample() > 0.5){
+                    pq.pop();
+                    pq.push(ReconstructionCandidate(targetIdx,
+                                                    neighborIdx,
+                                                    transformIdx,
+                                                    weight));
+                }
+            }
         }
-    } 
-
-    //TO TEST
+    }   
 }
 
 
-//-------- SMALL E STEP --------------------------------------------- 
 
+//! SMALL E STEP (expectation step)
 
-//updating the weights while keeping the candidate neighbor set fixed
-void TransformationLearner::smallEStep(){
+
+//!updating the weights while keeping the candidate neighbor set fixed
+void TransformationLearner::smallEStep()
+{
+    int candidateIdx =0;
+    int  targetIdx = reconstructionSet[candidateIdx].targetIdx;
+    real totalWeight = INIT_weight(0);
     
-    int candidateIdx =0, startCandidateIdx=0;
-    int startTargetIdx = generationSet[startCandidateIdx].targetIdx;
-    int  targetIdx;
-    real totalWeight = 0;
-  
-    while(candidateIdx < nbGenerationCandidates){
-    
-        totalWeight = SUM_weights(updateGenerationWeight(candidateIdx),
-                                  totalWeight);
+    while(candidateIdx < nbReconstructions){
+        
+        totalWeight = SUM_weights(totalWeight,
+                                  updateReconstructionWeight(candidateIdx));
         candidateIdx ++;
     
-        targetIdx = generationSet[candidateIdx].targetIdx; 
-    
-        if(candidateIdx > nbGenerationCandidates || targetIdx != startTargetIdx){
-            normalizeTargetGenerationWeights(startTargetIdx, totalWeight);
-            totalWeight = 0;
-            startTargetIdx = targetIdx;
+        if(candidateIdx == nbReconstructions)
+            normalizeTargetWeights(targetIdx,totalWeight);
+        else if(targetIdx != reconstructionSet[candidateIdx].targetIdx){
+            normalizeTargetWeights(targetIdx, totalWeight);
+            totalWeight = INIT_weight(0);
+            targetIdx = reconstructionSet[candidateIdx].targetIdx;
         }
     }    
+}
 
-    //TO TEST
+// M STEP
+
+
+//!coordination of the different kinds of maximization step
+//!(i.e.: we optimize with respect to which parameter?)
+void TransformationLearner::MStep()
+{
+    if(noiseVariancePeriod > 0 && stage%noiseVariancePeriod == noiseVarianceOffset)
+        MStepNoiseVariance();
+    if(transformDistributionPeriod > 0 && 
+       stage % transformDistributionPeriod == transformDistributionOffset)
+        MStepTransformDistribution();
+    if(stage % transformsPeriod == transformsOffset)
+        MStepTransformations();
+    
 }
+
+//!maximization step  with respect to  transformation distribution
+//!parameters
+void TransformationLearner::MStepTransformDistribution()
+{
+    MStepTransformDistributionMAP(transformDistributionAlpha);
+}
+
+//!maximization step  with respect to transformation distribution
+//!parameters
+//!(MAP version, alpha = dirichlet prior distribution parameter)
+//!NOTE :  alpha =1 ->  no regularization
+void TransformationLearner::MStepTransformDistributionMAP(real alpha)
+{
+   
+    Vec newDistribution ;
+    newDistribution.resize(nbTransforms);
     
+    for(int k=0; k<nbTransforms ; k++){
+        newDistribution[k] = INIT_weight(0);
+    }
+    
+    int transformIdx;
+    real weight;
+    for(int idx =0 ;idx < nbReconstructions ; idx ++){
+        transformIdx = reconstructionSet[idx].transformIdx;
+        weight = reconstructionSet[idx].weight;
+        newDistribution[transformIdx] = 
+            SUM_weights(newDistribution[transformIdx],
+                        weight);
+    }
 
-//-------- M STEP ---------------------------------------------   
+    real addFactor = INIT_weight(alpha - 1);
+    real divisionFactor = INIT_weight(nbTransforms*(alpha - 1) + trainingSetLength); 
+
+    for(int k=0; k<nbTransforms ; k++){
+        newDistribution[k]= DIV_weights(SUM_weights(addFactor,
+                                                    newDistribution[k]),
+                                        divisionFactor);
+    }
+    transformDistribution << newDistribution ;
+}
+
+//!maximization step with respect to transformation parameters
+//!(MAP version)
+void TransformationLearner::MStepTransformations()
+{
     
-
-//updating the transformation parameters
-void TransformationLearner::MStep(){
     //set the m dXd matrices Ck and Bk , k in{1, ...,m} to 0.
     B_C.clear();
-  
-    for(int idx=0 ; idx<nbGenerationCandidates ; idx++){
     
+    real lambda = 1.0*noiseVariance/transformsVariance;
+    Vec v;
+    for(int idx=0 ; idx<nbReconstructions ; idx++){
+        
         //catch a view on the next entry of our dataset, that is, a  triple:
         //(target_idx, neighbor_idx, transformation_idx)
-        GenerationCandidate * gc = &generationSet[idx];
         
-        real w = gc->weight;
+        real p = PROBA_weight(reconstructionSet[idx].weight);
   
         //catch the target and neighbor points from the training set
-        getTargetFromTrainingSet(gc->targetIdx);
-        getNeighborFromTrainingSet(gc->neighborIdx);
+        seeTarget(reconstructionSet[idx].targetIdx);
+        seeNeighbor(reconstructionSet[idx].neighborIdx);
         
-        int t = gc->transformIdx;
+        int t = reconstructionSet[idx].transformIdx;
         
-        externalProductScaleAcc(C[t], target, target, w);
-        if(transformFamily == TRANSFORM_FAMILY_LINEAR){
-            externalProductScaleAcc(B[t], target, neighbor,w);
+        v.resize(inputSpaceDim);
+        v << target;
+        if(transformFamily == TRANSFORM_FAMILY_LINEAR_INCREMENT){
+            v = v - neighbor;
         }
-        else
-            externalProductScaleAcc(B[t], target, (target - neighbor), w); 
+        if(withBias){
+            v = v - biasSet(t);
+        }
+        externalProductScaleAcc(C[t], neighbor, neighbor, p);
+        
+        externalProductScaleAcc(B[t], neighbor, v,p); 
     }
     for(int t=0; t<nbTransforms; t++){
         addToDiagonal(C[t],lambda);
-        transforms[t] << solveLinearSystem(C[t], B[t]);
+        transforms[t] << solveLinearSystem(C[t], B[t]);  
+    }  
+}
+ 
+
+//!maximization step with respect to noise variance
+void TransformationLearner::MStepNoiseVariance()
+{
+    MStepNoiseVarianceMAP(noiseAlpha,noiseBeta);
+}
+
+//!maximization step with respect to noise variance
+//!(MAP version, alpha and beta = gamma prior distribution parameters)
+//!NOTE : alpha=1, beta=0 -> no regularization
+void TransformationLearner::MStepNoiseVarianceMAP(real alpha, real beta)
+{
+    
+    Vec total_k;
+    total_k.resize(nbTransforms);
+    int transformIdx;
+    real proba;
+    for(int idx=0; idx < nbReconstructions; idx++){
+        transformIdx = reconstructionSet[idx].transformIdx;
+        proba = PROBA_weight(reconstructionSet[idx].weight);
+        total_k[transformIdx]+=(proba * reconstructionEuclideanDistance(idx));
     }
+    noiseVariance = (2*beta + sum(total_k))/(2*alpha - 2 + trainingSetLength*inputSpaceDim);  
+}
+ 
+//!returns the distance between the reconstruction and the target
+//!for the 'candidateIdx'th reconstruction candidate
+real TransformationLearner::reconstructionEuclideanDistance(int candidateIdx){
+    seeTarget(reconstructionSet[candidateIdx].targetIdx);
+    seeNeighbor(reconstructionSet[candidateIdx].neighborIdx);
+    Vec reconstruction;
+    reconstruction.resize(inputSpaceDim);
+    applyTransformationOn(reconstructionSet[candidateIdx].transformIdx,
+                          neighbor,
+                          reconstruction);
+    return powdistance(target, reconstruction);
+}
 
-    //TO TEST
 
+//!STOPPING CRITERION
+
+
+//!stages == nstages?
+bool TransformationLearner::stoppingCriterionReached()
+{
+   
+    return stage==nstages;
 }
 
+//!increments the variable 'stage' of 1 
+void TransformationLearner::nextStage(){
+    stage ++;
+}
 
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h
===================================================================
--- trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h	2007-07-18 22:30:31 UTC (rev 7798)
+++ trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h	2007-07-19 15:15:44 UTC (rev 7799)
@@ -2,8 +2,6 @@
 
 // TransformationLearner.h
 //
-//version 5 
-//
 // Copyright (C) 2007 Lysiane Bouchard
 //
 // Redistribution and use in source and binary forms, with or without
@@ -34,7 +32,7 @@
 // This file is part of the PLearn library. For more information on the PLearn
 // library, go to the PLearn Web site at www.plearn.org
 
-// Authors: Lysiane Bouchard
+// Authors: Lysiane Bouchard, Pascal Vincent
 
 /*! \file TransformationLearner.h */
 
@@ -42,123 +40,259 @@
 #ifndef TransformationLearner_INC
 #define TransformationLearner_INC
 
-//C++
-#include <utility>
-#include <queue>
-
-//Plearn
-#include <plearn_learners/generic/PLearner.h>
+//PLEARN
+#include <plearn_learners/distributions/PDistribution.h> 
+#include <plearn/math/plapack.h>
+#include <plearn/math/pl_math.h>
+#include <plearn_learners/distributions/PDistribution.h>
 #include <plearn/math/TMat_maths.h>
 #include <plearn/math/PRandom.h>
 #include <plearn/base/tuple.h>
 
+//C++
+#include <utility>
+#include <queue>
+#include <math.h>
 
 #define TRANSFORM_FAMILY_LINEAR 0
 #define TRANSFORM_FAMILY_LINEAR_INCREMENT 1
+#define UNDEFINED -1
+#define INIT_MODE_DEFAULT 0
+#define INIT_MODE_RANDOM 1
+#define NOISE_ALPHA_NO_REG 1
+#define NOISE_BETA_NO_REG 0
+#define TRANSFORM_DISTRIBUTION_ALPHA_NO_REG 1
+#define BEHAVIOR_LEARNER 0
+#define BEHAVIOR_GENERATOR 1
 
+namespace PLearn {
 
 
-namespace PLearn {
 
 /**
- * The first sentence should be a BRIEF DESCRIPTION of what the class does.
- * Place the rest of the class programmer documentation here.  Doxygen supports
- * Javadoc-style comments.  See http://www.doxygen.org/manual.html
- *
- * @todo Write class to-do's here if there are any.
- *
- * @deprecated Write deprecated stuff here if there is any.  Indicate what else
- * should be used instead.
+description of the main class: TransformationLearner
+
+GENERATION PROCESS
+
+We suppose a new point v is obtained from a point x by :
+1) choosing a transformation t among a set of transformations T
+  (with probability p(t))
+2) applying the choosen transformation t on x
+3) add some noise in all the directions (noise = normally distributed random variable) 
+
+UNDERLYING PROBABILITY SPACE
+
+variables of the distribution : X : real vector
+                                V : real vector, neighbor of X
+                                T : transformation 
+P(x,v,t) = P(v is obtained by applying transformation t on x) 
+         = P(x,v|t)P(t)
+         = N( T(t)(x)|v,sigma)P(t)
+
+LEARNING BEHAVIOR
+ 
+The parameters of the distributions are learned using a variant
+of E.M.algorithm
+- learns  a finite set of transformations 
+- possibly:  learns the parameter sigma describing the noise distribution,
+             learns p(t), the transformation distribution.   
+
  */
 
-/***** GENERATION CANDIDATE ***********************************************/
 
 
-//Generate Candidate objects are basically 4-tuples with the following format: 
-//         nKm x4 matrix 
-//    ------C1----------|---- C2------------|-- C3-----------|-- C4-----------|
-//    index i  in the   | index j in the    | index t of a   | positive weight
-//    training set of a | training set of a | transformation | 
-//    target point      | a neighbour       |                |
-//                      | candidate         |                | 
 
-
-class GenerationCandidate
+/****************************************************************************
+ *AUXILIARY CLASS RECONSTRUCTION CANDIDATE : 
+ *
+ *
+ *
+ *Reconstruction Candidate objects are basically 4-tuples with the following format: 
+ *         nKm x4 matrix 
+ *    ------C1----------|---- C2------------|-- C3-----------|-- C4-----------|
+ *    index i  in the   | index j in the    | index t of a   | positive weight
+ *    training set of a | training set of a | transformation | 
+ *    target point      | a neighbour       |                |
+ *                      | candidate         |                | 
+ *
+ ***************************************************************************/ 
+class ReconstructionCandidate
 {
 public:
     int targetIdx, neighborIdx, transformIdx;
     real weight;
-
-    GenerationCandidate(int targetIdx_=-1, int neighborIdx_=-1, int transformIdx_=-1, real weight_=0){
+    
+    ReconstructionCandidate(int targetIdx_=-1, int neighborIdx_=-1, int transformIdx_=-1, real weight_=0){
         targetIdx =  targetIdx_;
         neighborIdx =  neighborIdx_;
         transformIdx =  transformIdx_ ;
         weight =  weight_;            
     }
 };
-inline bool operator<(const GenerationCandidate& o1 , const GenerationCandidate& o2)
+
+//Comparisons between ReconstructionCandidate objects 
+inline bool operator<(const ReconstructionCandidate& o1 ,
+                      const ReconstructionCandidate& o2)
 {
-    // we need the inverse comparison for the priority queue
-    return o2.weight>o1.weight;
+    //  Will be used in storage process, in a priority queue.
+    //  With the following  definitions, priority measure increases when weight
+    //  field decreases.
+    //  That is, we want to keep ReconstructionCandidate objects with lower 
+    //  weights on top of the priority queue 
+    return o2.weight<o1.weight;
 }
-
-inline bool operator==(const GenerationCandidate& o1, const GenerationCandidate& o2)
+inline bool operator==(const ReconstructionCandidate& o1,
+                       const ReconstructionCandidate& o2)
 {
-        return o1.weight==o2.weight;
+    return o1.weight==o2.weight;
 }
 
 
+//print/read ReconstructionCandidate objects
+inline PStream& operator<<(PStream& out, 
+                           const ReconstructionCandidate& x)
+{
+    out << tuple<int, int, int, real>(x.targetIdx, x.neighborIdx, x.transformIdx, x.weight);
+    return out;
+}
+inline PStream& operator>>(PStream& in, ReconstructionCandidate& x)
+{
+    tuple<int, int, int, real> t;
+    in >> t;
+    tie(x.targetIdx, x.neighborIdx, x.transformIdx, x.weight) = t;
+    return in;
+}
 
-inline PStream& operator<<(PStream& out, const GenerationCandidate& x)
-    {
-        out << tuple<int, int, int, real>(x.targetIdx, x.neighborIdx, x.transformIdx, x.weight);
-        return out;
-    }
 
-inline PStream& operator>>(PStream& in, GenerationCandidate& x)
-    {
-        tuple<int, int, int, real> t;
-        in >> t;
-        tie(x.targetIdx, x.neighborIdx, x.transformIdx, x.weight) = t;
-        return in;
-    }
 
-/********* END , GENERATION CANDIDATE *************************************/
-
-
-class TransformationLearner : public PLearner
+/***************************************************************************
+ * main class: TRANSFORMATION LEARNER
+ *
+ *
+ *
+ * Learns a finite set of linear transformations. That is, learns how to move from 
+ * one point to another.  
+ */
+class TransformationLearner : public PDistribution
 {
-    typedef PLearner inherited;
+    typedef PDistribution inherited;
 
 public:
     //#####  Public Build Options  ############################################
 
-    //! for random generators
-    long seed; 
+    //!A transformation learner might behave as a learner,as well as a generator
+    int behavior;
+
+
+    //!The following variable will be used to ensure p(x,v,t )>0 
+    //!at the beginning (see implantation of randomReconstuctionWeight()
+    //!for more details) 
+    real minimumProba;
     
-    //! set the family of transformation functions we are interested in
-    int transformFamily ;
     
-    //HYPER-PARAMETERS OF THE ALGORITHM
+    //WHICH KIND OF TRANSFORMATION FUNCTIONS ... 
     
-    //!variance of the NOISE, considered here as a random variable normaly 
-    //!distributed, with mean 0
+    //! what is the global form of the transformation functions used?
+    int transformFamily;
+    //! add a bias to the transformation function ?
+    bool withBias;
+    
+    //LEARNING MODE ...
+
+    //!is the variance(precision) of the noise random variable learned or fixed ? 
+    //!(recall that the precision = 1/variance) 
+    bool learnNoiseVariance;
+   
+    //!if we learn the noise variance, do we use the MAP estimator ? 
+    bool regOnNoiseVariance;
+    
+    //!is the transformation distribution learned or fixed?
+    bool learnTransformDistribution;
+    
+    //!if we learn the transformation distribution, do we use the MAP estimator ?
+    bool regOnTransformDistribution;
+
+    //!how the initial values of the parameters to learn are choosen?
+    int initializationMode;
+
+    //!For a given training point, we do not consider all the possibilities for the hidden variables.
+    //!We approximate EM by using only the hidden variables with higher probability.
+    //!That is, for each point in the training set, we keep a fixed number of
+    //!hidden variables combinations, the most probable ones.
+    //!We call that selection "large expection step".
+    //!There are 2 versions, A and B. The following variables tells us when to
+    //!perform each one. (see EStep() for more details)
+    int largeEStepAPeriod;
+    int largeEStepAOffset;
+    int largeEStepBPeriod;
+    int largeEStepBOffset;
+    
+    //!If the noise variance (precision) is learned, the following variables
+    //!tells us when to update the noise variance in the maximization steps:
+    //!(see MStep() for more details)
+    int noiseVariancePeriod;
+    int noiseVarianceOffset;                    
+    
+    //!These 2 parameters have to be defined if the noise variance is learned
+    //!using a MAP procedure.
+    //!We suppose that the prior distribution for the noise variance is a gamma
+    //!distribution with parameters alpha and beta:
+    //! p(x|alpha,beta)= x^(alpha-1)beta^(alpha)exp(-beta*x)/gamma(alpha)
+    //!Note : if alpha = 1, beta=0, all the possibilities are equiprobable 
+    //!      (no regularization effect)      
+    real noiseAlpha;
+    real noiseBeta;
+    
+    //!If the transformation distribution is learned, the following variables
+    //!tells us when to update it in the maximization steps:
+    //!(see MStep() for more details)
+    int transformDistributionPeriod;
+    int transformDistributionOffset;
+    
+    //!This parameter have to be defined if the transformation distribution
+    //!is learned using a MAP procedure. We suppose that this distribution have a a multinomial form
+    //(u1,u2,...,uK) with dirichlet prior probability : 
+    // p(u1,...,uK) = NormalisationCoeff(alpha)*u1^(alpha -1)*u2^(alpha -1)...*uK^(alpha  - 1)
+    //Note: if alpha = 1, it means all possibilities are equiprobable
+    //      (no regularization effect)  
+    real transformDistributionAlpha;
+
+
+    //!tells us when to update the transformation parameters
+    int transformsPeriod;
+    int transformsOffset;
+
+
+    //PARAMETERS OF THE DISTRIBUTION
+
+    //! variance of the NOISE random variable. 
+    //!(recall that this r.v. is normally distributed with mean 0).
+    //! -if it is a learned parameter, will be considered as the initial value 
+    //!  of the noise variance parameter.
+    //! -if it is not well defined (<=0), it will be redefined using its
+    //!  prior distribution (Gamma).
     real noiseVariance;
     
-    //!variance on the transformation parameters. (prior distribution = normal with
-    //!mean 0).
+    //! variance on the transformation parameters (prior distribution = normal with mean 0)
     real transformsVariance;
-
-    //number of transformations
+    
+    //!number of transformations
     int nbTransforms;
     
-    //number of neighbors
+    //!number of neighbors
     int nbNeighbors;
+    
+    //!multinomial distribution for the transformation:
+    //!(i.e. probabilit of kth transformation = transformDistriibution[k])
+    //!(might be learned or fixed)
+    //!-if it is a learned parameter, will be considered as the initial value
+    //! or the transformation distribution
+    //!-if it is not well defined (size, positivity, sum to 1), it will be 
+    //! redefined using its prior distribution (Dirichlet).
+    Vec transformDistribution; 
+   
+    
 
-    //minimum random weight to give to a  chosen generation candidate at
-    //initialization step
-    real epsilonInitWeight;
-
 public:
     //#####  Public Member Functions  #########################################
 
@@ -168,135 +302,105 @@
     TransformationLearner();
 
 
-    //#####  PLearner Member Functions  #######################################
+    //#####  PDistribution Member Functions  ##################################
 
-    //! Returns the size of this learner's output, (which typically
-    //! may depend on its inputsize(), targetsize() and set options).
-    // (PLEASE IMPLEMENT IN .cc)
-    virtual int outputsize() const;
+    //! Return log of probability density log(p(y | x)).
+    virtual real log_density(const Vec& y) ;//const;
 
-    //! (Re-)initializes the PLearner in its fresh state (that state may depend
-    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
-    //! a fresh learner!).
-    // (PLEASE IMPLEMENT IN .cc)
-    virtual void forget();
+    //! Return survival function: P(Y>y | x).
+    virtual real survival_fn(const Vec& y) const;
 
-    //! The role of the train method is to bring the learner up to
-    //! stage==nstages, updating the train_stats collector with training costs
-    //! measured on-line in the process.
-    // (PLEASE IMPLEMENT IN .cc)
-    virtual void train();
+    //! Return cdf: P(Y<y | x).
+    virtual real cdf(const Vec& y) const;
 
-    //! Computes the output from the input.
-    // (PLEASE IMPLEMENT IN .cc)
-    virtual void computeOutput(const Vec& input, Vec& output) const;
+    //! Return E[Y | x].
+    virtual void expectation(Vec& mu) const;
 
-    //! Computes the costs from already computed output.
-    // (PLEASE IMPLEMENT IN .cc)
-    virtual void computeCostsFromOutputs(const Vec& input, const Vec& output,
-                                         const Vec& target, Vec& costs) const;
+    //! Return Var[Y | x].
+    virtual void variance(Mat& cov) const;
 
-    //! Returns the names of the costs computed by computeCostsFromOutpus (and
-    //! thus the test method).
-    // (PLEASE IMPLEMENT IN .cc)
-    virtual TVec<std::string> getTestCostNames() const;
+    //! Return a pseudo-random sample generated from the conditional
+    //! distribution, of density p(y | x).
+    virtual void generate(Vec& y); //const ;
 
-    //! Returns the names of the objective costs that the train method computes
-    //! and  for which it updates the VecStatsCollector train_stats.
-    // (PLEASE IMPLEMENT IN .cc)
-    virtual TVec<std::string> getTrainCostNames() const;
+    //### Override this method if you need it (and if your distribution can
+    //### handle it. Default version calls PLERROR.
+    //! Generates a pseudo-random sample x from the reversed conditional
+    //! distribution, of density p(x | y) (and NOT p(y | x)).
+    //! i.e., generates a "predictor" part given a "predicted" part, regardless
+    //! of any previously set predictor.
+    // virtual void generatePredictorGivenPredicted(Vec& x, const Vec& y);
 
+    //### Override this method if you need it. Default version calls
+    //### random_gen->manual_seed(g_seed) if g_seed !=0
+    //! Reset the random number generator used by generate() using the
+    //! given seed.
+    // virtual void resetGenerator(long g_seed) const;
 
-    // *** SUBCLASS WRITING: ***
-    // While in general not necessary, in case of particular needs
-    // (efficiency concerns for ex) you may also want to overload
-    // some of the following methods:
-    // virtual void computeOutputAndCosts(const Vec& input, const Vec& target,
-    //                                    Vec& output, Vec& costs) const;
-    // virtual void computeCostsOnly(const Vec& input, const Vec& target,
-    //                               Vec& costs) const;
-    // virtual void test(VMat testset, PP<VecStatsCollector> test_stats,
-    //                   VMat testoutputs=0, VMat testcosts=0) const;
-    // virtual int nTestCosts() const;
-    // virtual int nTrainCosts() const;
-    // virtual void resetInternalState();
-    // virtual bool isStatefulLearner() const;
+    //! Set the 'predictor' and 'predicted' sizes for this distribution.
+    //### See help in PDistribution.h.
+    virtual bool setPredictorPredictedSizes(int the_predictor_size,
+                                            int the_predicted_size,
+                                            bool call_parent = true);
 
+    //! Set the value for the predictor part of a conditional probability.
+    //### See help in PDistribution.h.
+    virtual void setPredictor(const Vec& predictor, bool call_parent = true)
+                              const;
 
-    /*********GENERATION MODULE *****************************************/
+    // ### These methods may be overridden for efficiency purpose:
+    /*
+    //### Default version calls exp(log_density(y))
+    //! Return probability density p(y | x)
+    virtual real density(const Vec& y) const;
 
-    //The object is the representation of a learned distribution
-    //Are are methods to ensure the "generative behavior" of the object
-    //(Once the distribution is learned, we might be able to generate
-    // samples from it)
+    //### Default version calls setPredictorPredictedSises(0,-1) and generate
+    //! Generates a pseudo-random sample (x,y) from the JOINT distribution,
+    //! of density p(x, y)
+    //! i.e., generates a predictor and a predicted part, regardless of any
+    //! previously set predictor.
+    virtual void generateJoint(Vec& xy);
 
-    //Chooses the transformation parameters using a 
-    //normal distribution with mean 0 and variance "transformsVariance"
-    //(call generatorBuild() after)
-    void buildTransformationParametersNormal();
-   
-    //set the transformation parameters to the specified values
-    //(call generatorBuild() after)
-    void setTransformationParameters(TVec<Mat> & transforms);
+    //### Default version calls generateJoint and discards y
+    //! Generates a pseudo-random sample x from the marginal distribution of
+    //! predictors, of density p(x),
+    //! i.e., generates a predictor part, regardless of any previously set
+    //! predictor.
+    virtual void generatePredictor(Vec& x);
 
-    //creates a data set
-    //
-    //     Consists in building a tree of deepness d = "nbGenerations" and
-    //     constant branch factor n = "generationLength"
-    //
-    //            0      1        2     ...         
-    //  
-    //            r - child1  - child1  ...       
-    //                        - child2  ...
-    //                            ...   ...
-    //                        - childn  ...
-    //
-    //              - child2  - child1  ...
-    //                        - child2  ...
-    //                            ...   ...
-    //                        - childn  ...
-    //                     ...
-    //             - childn   - child1  ...
-    //                        - child2  ...
-    //                            ...   ...
-    //                        - childn  ... 
-    //
+    //### Default version calls generateJoint and discards x
+    //! Generates a pseudo-random sample y from the marginal distribution of
+    //! predicted parts, of density p(y) (and NOT p(y | x)).
+    //! i.e., generates a predicted part, regardless of any previously set
+    //! predictor.
+    virtual void generatePredicted(Vec& y);
+    */
 
-   // all the childs are choosen following the same process:
-   // 1) choose a transformation  
-   // 2) apply the transformation to the parent
-   // 3) add noise to the result 
-    void createDataSet(Vec & root,
-                       int nbGenerations,
-                       int generationLength,
-                       Mat & dataPoints);
-    
-    //create a dataset using the same tree generation process as
-    //createDataSet, except the number of child per parent is fixed to 1,
-    //   root -> 1st point -> 2nd point ... -> nth point 
-    void createDataSetSequentially(Vec & root,
-                                   int n,
-                                   Mat & dataPoints);
 
-    //Select a transformation randomly (with respect ot our transformation
-    //distribution)
-    int pickTransformIdx();
-    
-    
-    //here is the generation process for a given center data point 
-    //  1) choose a transformation
-    //  2) apply it on the center data point
-    //  3) add noise
+    //#####  PLearner Member Functions  #######################################
 
-    //generates a sample data point  from a  given center data point 
-    void generateFrom(Vec & center, Vec & sample);
-    //generates a sample data point from a given center data point
-    void generateFrom(Vec & center, Vec & sample, int transformIdx);
-    //fill the matrix "samples" with sample data points obtained from
-    // a given center data point.
-    void batchGenerateFrom( Vec & center, Mat & samples); 
-  
-   
+    // ### Default version of inputsize returns learner->inputsize()
+    // ### If this is not appropriate, you should uncomment this and define
+    // ### it properly in the .cc
+    virtual int inputsize() const;
+
+    /**
+     * (Re-)initializes the PDistribution in its fresh state (that state may
+     * depend on the 'seed' option).  And sets 'stage' back to 0 (this is the
+     * stage of a fresh learner!).
+     * ### You may remove this method if your distribution does not
+     * ### implement it.
+     */
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage == nstages, updating the train_stats collector with training
+    //! costs measured on-line in the process.
+    // ### You may remove this method if your distribution does not
+    // ### implement it.
+    virtual void train();
+
+
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.
@@ -304,275 +408,452 @@
     // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
     PLEARN_DECLARE_OBJECT(TransformationLearner);
 
-    // Simply calls inherited::build() then build_()
-    virtual void build();
 
-    //! Transforms a shallow copy into a deep copy
-    // (PLEASE IMPLEMENT IN .cc)
-    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+    //!INITIAL VALUES OF  THE PARAMETERS TO LEARN
+    
+    //!initializes the transformation parameters randomly 
+    //!(prior distribution= Normal(0,transformsVariance))
+    void initTransformsParameters();
 
+    //!initializes the transformation parameters to the given values
+    //!(bias are set to 0)
+    void setTransformsParameters(TVec<Mat>  transforms, Mat bias=Mat());
+    
+    
+    //!initializes the noise variance randomly
+    //!(gamma distribution)
+    void initNoiseVariance();
+    
+    //!initializes the noise variance with the given value
+    void setNoiseVariance(real nv);
+    
+    //!initializes the transformation distribution randomly
+    //!(dirichlet distribution)
+    void initTransformDistribution();
+    
+    //!initializes the transformation distribution with the given values
+    void setTransformDistribution(Vec td);
+    
+    
+    //!GENERATION FUNCTIONS
+    
+    //!generates a sample data point from a source data point
+    void generatePredictedFrom(const Vec & source, Vec & sample)const;
+    
+    //!generates a sample data point from a source data point with a specific transformation
+    void generatePredictedFrom(const Vec & source, Vec & sample, int transformIdx)const;
 
-    //---EXTERIOR ACCESS ON LEARNED VARIABLES -------------------------------
+    //!generates a sample data point from a source data point and returns it
+    //! (if transformIdx >= 0 , we use the corresponding transformation )
+    Vec returnPredictedFrom(Vec source, int transformIdx=-1);
     
-    //returns all the entries in the generation set with the 
-    //TARGET_IDX field fixed to "targetIdx"
-    //those entries represents ways to reproduct the 
-    //"targetIdx"th data point in the training set
-    TVec<GenerationCandidate> returnReproductionSources(int targetIdx); 
-  
-    //returns the reproductions of the "targetIdx"th data point in the
-    //training set
-    //(one reproduction by reproduction source)
-    Mat returnReproductions(int targetIdx);
 
-    //returns the parameter of the "transformIdx"th transformation
-    Mat returnTransform(int transformIdx);
+    //!fill the matrix "samples" with data points obtained from a given center data point
+    void batchGeneratePredictedFrom(const Vec & center,
+                                     Mat & samples)const;
+    
+    //!fill the matrix "samples" with data points obtained form a given center data point
+    //!    - we use a specific transformation
+    void batchGeneratePredictedFrom(const Vec & center,
+                                     Mat & samples,
+                                     int transformIdx)const ;
 
-    
-    //returns the paramter of each transformation
-    //(as an tdXd matrix, t = number of transformation,
-    //                    d = dimension of input space)
-    Mat returnAllTransforms();
-    
-    //Generates n samples from center and returns them
-    //    (generation process = 1) choose a transformation,
+    //Generates n samples from center and returns them stored in a matrix
+    //    (generation process = 1) choose a transformation (*),
     //                          2) apply it on center
     //                          3) add noise)
-    Mat returnGeneratedSamplesFrom(Vec center, int n);
+    // - (*) if transformIdx>=0, we always use the corresponding transformation
+    Mat returnGeneratedSamplesFrom(Vec center, int n, int transformIdx=-1);
     
-    //Creates a data set and returns it
-    //(see createDataSet for more details on the generation process)
-    Mat returnGeneratedDataSet(Vec root,
-                               int nbGenerations,
-                               int generationLength);
+    
+    //!select a transformation randomly (with respect to our multinomial distribution)
+    int pickTransformIdx() const;
 
-    //Generates a data set and returns it
-    //(sequential generation process: see createDataSetSequentially for more details)
-    Mat returnSequentiallyGeneratedDataSet(Vec root,int n);
+    //!Select a neighbor in the training set randomly
+    //!(return his index in the training set)
+    //!We suppose all data points in the training set are equiprobables
+    int pickNeighborIdx() const;
 
+    //!creates a data set:
+    //!     equivalent in building a tree with fixed deepness and constant branching factor
+    //!
+    //!            0      1        2     ...         
+    //!  
+    //!            r -> child1  -> child1  ...       
+    //!                         -> child2  ...
+    //!                             ...    ...
+    //!                         -> childn  ...
+    //!
+    //!              -> child2  -> child1  ...
+    //!                         -> child2  ...
+    //!                              ...   ...
+    //!                         -> childn  ...
+    //!                      ...
+    //!              -> childn  -> child1  ...
+    //!                         -> child2  ...
+    //!                              ...   ...
+    //!                         -> childn  ... 
+    //!
+    //!(where "a -> b" stands for "a generate b")
+    //!all the child are generated by the same following process:
+    //! 1) choose a transformation  
+    //! 2) apply the transformation to the parent
+    //! 3) add noise to the result 
+    void treeDataSet(const Vec &root,
+                     int deepness,
+                     int branchingFactor,
+                     Mat & dataPoints);
+    Mat returnTreeDataSet(Vec root,
+                          int deepness,
+                          int branchingFactor);
+    
 
-protected:
-    //#####  Protected Options  ###############################################
+    //!create a "sequential" dataset:
+    //!  start -> first point -> second point ... ->nth point
+    //! (where "->" stands for : "generate the")
+    void sequenceDataSet(const Vec & start,
+                         int n,
+                         Mat & dataPoints);
 
-    // ### Declare protected option fields (such as learned parameters) here
-    // ...
+    Mat returnSequenceDataSet(Vec start,int n);
 
-public:    
-    
-    //DIMENSION VARIABLES 
   
-    //dimension of the input space;
-    int inputSpaceDim;
+
     
-    //number of generation candidates related to a specific target in the 
-    //generation set. 
-    int nbGenerationCandidatesPerTarget;
 
-    //total number of generation candidates in the generation set
-    int nbGenerationCandidates;
 
-    //number of samples given in the training set
-    int nbTrainingInput;
 
-    //multinomial probability ditribution for the transformations
-    //(i.e. probability of kth transformation = transformDistribution[k])
-    Vec transformDistribution;
-    
+    //!COPIES OF THE STRUCTURES
 
-    //LEARNED MODEL PARAMETERS
+    //!returns the "idx"th data point in the training set
+    Vec returnTrainingPoint(int idx);
 
-    //set of transformations:
-    //mdxd matrix :  -where m = number of transformation,
-    //                      d = dimensionality of the input space
-    //               -rows kd to kd + d (exclusively) = sub-matrix = parameters of the
-    //                                                               kth transformation
-    //                                                               (0<=k<m)
-    Mat transformsSet ; 
-    TVec< Mat > transforms; //views on sub-matrices of the matrix transformsSet 
+    //!returns all the reconstructions candidates associated to a given target
+    TVec<ReconstructionCandidate> returnReconstructionCandidates(int targetIdx);
+
+    //!returns the reconstructions of the "targetIdx"th data point value in the training set
+    //!(one reconstruction for each reconstruction candidate)
+    Mat returnReconstructions(int targetIdx);
+
+    //!returns the neighbors choosen to reconstruct the target
+    //!(one choosen neighbor for each reconstruction candidate associated to the target)
+    Mat returnNeighbors(int targetIdx);
+
+    //!returns the parameters of the "transformIdx"th transformation
+    Mat returnTransform(int transformIdx);
+
+    //!returns the parameters of each transformation
+    //!(as an KdXd matrix, K = number of transformations,
+    //!                    d = dimension of input space)
+    Mat returnAllTransforms();
+
+
+    //OTHER BUILDING/INITIALIZATION METHODS 
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
     
-    //a generationSet D : 
-    //implemented as a vector of "GenerationCandidate" objects.
-    TVec< GenerationCandidate > generationSet; 
+    //! initialization operations that have to be done before the training
+    void trainBuild();
+    
+    //! initialization operations that have to be done before a generation process
+    //! (all the undefined parameters will be initialized  randomly)
+    void generatorBuild(int inputSpaceDim_=2,
+                        TVec<Mat> transforms_ =TVec<Mat>(),  
+                        Mat biasSet_ =Mat(),  
+                        real noiseVariance_ =-1.0,
+                        Vec transformDistribution_ =Vec());
+    
+    
 
 
-    //OTHER VARIABLES
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
-    //the weight decay
-    real lambda;
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
     
 
-    //factor used in the computation of generation weights :
-    // 1/2(noiseVariance)y
-    real noiseVarianceFactor;
+    //TRANSFORMATIONS
+    
+    //!set of transformations:
+    //!mdxd matrix :  -m = number of transformation,
+    //!               -d = dimensionality of the input space
+    //!               -rows kd to kd + d (exclusively) = sub-matrix = parameters of the
+    //!                                                               kth transformation
+    //!                                                               (0<=k<m)
+    Mat transformsSet;
+    TVec<Mat> transforms; //!views on sub-matrices of the matrix transformsSet
+    
+    //!set of bias (one by transformation)
+    //!-might be used only if the flag "withBias" is turned on
+    Mat biasSet;
+    
+    //SELECTED HIDDEN VARIABLES COMBINATIONS
 
-    //standard deviation for the noise distribution, and transformation
-    //parameters distributions:
-    real noiseStDev,transformsStDev;
+    //!a reconstruction set:
+    //!-choosen hidden variables combinations for each point in the training set 
+    //!-implemented as a vector of "ReconstructionCandidate" objects.
+    TVec< ReconstructionCandidate > reconstructionSet; 
+    
+    
+    //DIMENSION VARIABLES
 
-    //will be used to store a view on the generation set:
-    //that is, all the entries related to a specific target . 
-    TVec<GenerationCandidate>  targetGenerationSet; 
+    //!dimension of the input space
+    int inputSpaceDim;
+
+    //!number of hidden variables combinations keeped for a specific target 
+    //!in the reconstruction set.
+    //!(Those combinations might be seen like reconstructions of the target)
+    int nbTargetReconstructions;
     
-    //Storage space that will be used to update the transformation
-    //parameters. It represents a set of sub-matrices. There are exactly 2 
-    //sub-matrices by transformation.   
-    Mat B_C ;
+    //!total number of combinations (x,v,t) keeped in the reconstruction set
+    int nbReconstructions;
+    
+    //!number of samples given in the training set
+    int trainingSetLength;
+    
+    //USEFUL CONSTANTS
+    
+    
+    //!standard deviations for the transformation parameters:
+    real transformsSD;
+    
 
-    //Vectors of matrices that will be used to update the transformation 
-    //parameters. Each matrix is a view on a sub-matrix in B_C. 
-    TVec<Mat> C , B ;
+    //OTHERS
 
-    //to retrieve easily an input point from the training set 
+    
+    //! Will be used to store a view on the reconstructionSet.
+    //! The view will consist in all the entries related to a specific target
+    TVec<ReconstructionCandidate> targetReconstructionSet;
+    
+    //!Storage space that will be used in the maximization step, in transformation parameters
+    //! updating process.
+    //!It represents a set of sub-matrices.There are exactly 2 sub-matrices by transformation.
+    Mat B_C;
+    //!Vectors of matrices that will be used in transformations parameters updating process.
+    //!Each matrix is a view on a sub-matrix in th bigger matrix "B_C" described above.
+    TVec<Mat> B,C;
+    
+    //!To get easily a view on an input point from the training set
     Vec target, neighbor;
-
     
-
+    
 protected:
     //#####  Protected Member Functions  ######################################
 
     //! Declares the class options.
     // (PLEASE IMPLEMENT IN .cc)
     static void declareOptions(OptionList& ol);
-
-
-    //! Declare the methods that are remote-callable
+    //! Declares the methods that are remote-callable
     static void declareMethods(RemoteMethodMap& rmm);
 
-    //general building operations 
-    void build_();
-    
-    //do the building operations related to training
-    //warning: we suppose the training set has been transmitted
-    //         before calling the method
-    void trainInit();
-    //do the building operations related to the generation process
-    //warning: we suppose the transformation parameters are set 
-    void generatorInit();
 
 private:
     //#####  Private Member Functions  ########################################
 
     //! This does the actual building.
     // (PLEASE IMPLEMENT IN .cc)
- 
+    void build_();
 
-public:
 
-    // ----------GENERAL USE--------------------------------------------------
+    //!VIEWS ON RECONSTRUCTION SET AND TRAINING SET
     
-
-    //REFERENCE OPERATIONS ON GENERATION SET AND TRAINING SET  
-
-    // stores a view on the subset of generation set related to the specified
-    // target (into the variable "targetGenerationSet" )
-    void getViewOnTargetGenerationCandidates(int targetIdx);
-    // stores the "targetIdx"th input in the training set into the variable
+    //! stores a VIEW on the reconstruction candidates related to the specified
+    //! target (into the variable "targetReconstructionSet" )
+    void seeTargetReconstructionSet(int targetIdx) ;
+    // stores the "targetIdx"th point in the training set into the variable
     // "target"
-    void getTargetFromTrainingSet(int targetIdx);
+    void seeTarget(const int targetIdx) ;
     // stores the "neighborIdx"th input in the training set into the variable
     // "neighbor" 
-    void getNeighborFromTrainingSet(int neighborIdx);
+    void seeNeighbor(const int neighborIdx);
+
+
+    //!GENERATE GAMMA RANDOM VARIABLES
     
+    //!source of the algorithm: http://oldmill.uchicago.edu/~wilder/Code/random/Papers/Marsaglia_00_SMGGV.pdf
     
-    //OPERATIONS RELATED TO GENERATION WEIGHTS
+    //!returns a pseudo-random positive real number x  
+    //!using the distribution p(x)=Gamma(alpha,beta)
+    real gamma_sample(real alpha,real beta=1);
     
-    //normalizes the generation weights related to a given target. 
-    void normalizeTargetGenerationWeights(int targetIdx, real totalWeight);
     
-    //returns a random positive weight 
-    real randomPositiveGenerationWeight();
-  
-    //arithmetic operations on  generation weights
-    real DIV_weights(real numWeight, real denomWeight); //DIVISION
-    real MULT_INVERSE_weight(real weight);//MULTIPLICATIVE INVERSE
-    real MULT_weights(real weight1, real weight2); //MULTIPLICATION
-    real SUM_weights(real weight1, real weight2); //SUM 
+    //!GENERATE DIRICHLET RANDOM VARIABLES
+    //!source of the algorithm: WIKIPEDIA
     
-    //update/compute the weight of a generation candidate with
-    //the actual transformation parameters
-    real updateGenerationWeight(int candidateIdx);
-    real computeGenerationWeight(GenerationCandidate & gc);
-    real computeGenerationWeight(int targetIdx, 
-                                 int neighborIdx, 
-                                 int transformIdx);
+    //!returns a pseudo-random positive real vector x 
+    //!using the distribution p(x) = Dirichlet(x| all the parameters = alpha)
+    //!-all the element of the vector are between 0 and 1,
+    //!-the elements of the vector sum to 1
+    void dirichlet_sample(real alpha, Vec & sample);
+    Vec return_dirichlet_sample(real alpha);
+
     
-    //applies "transformIdx"th transformation on data point "src"
-    void applyTransformationOn(int transformIdx, Vec & src , Vec & dst);
   
+    
 
-   //-------- INITIAL E STEP --------------------------------------------
+    //!OPERATIONS ON WEIGHTS 
+    
+     //!normalizes the reconstruction weights related to a given target. 
+    void normalizeTargetWeights(int targetIdx, real totalWeight) const;
+    
+    //!returns a random weight 
+    real randomWeight() const;
+    
+    //!arithmetic operations on  reconstruction weights
+    real INIT_weight(real initValue) const; //!CONSTRUCTOR
+    real PROBA_weight(real weight) const; //!GET CORRESPONDING PROBABILITY 
+    real DIV_weights(real numWeight, real denomWeight) const; //!DIVISION
+    real MULT_INVERSE_weight(real weight) const ;//!MULTIPLICATIVE INVERSE
+    real MULT_weights(real weight1, real weight2) const ; //!MULTIPLICATION
+    real SUM_weights(real weight1, real weight2) const ; //!SUM 
+    
+    //!update/compute the weight of a reconstruction candidate with
+    //!the actual transformation parameters
+    real updateReconstructionWeight(int candidateIdx);
+    real computeReconstructionWeight(const ReconstructionCandidate & gc);
+    real computeReconstructionWeight(int targetIdx, 
+                                     int neighborIdx, 
+                                     int transformIdx);
+    real computeReconstructionWeight(const Vec & target,
+                                     int neighborIdx,
+                                     int transformIdx);
 
-    //initialization of the generation set 
+    //!applies "transformIdx"th transformation on data point "src"
+    void applyTransformationOn(int transformIdx, const Vec & src , Vec & dst) const ;
+
+    
+    //! verify if the multinomial distribution given is well-defined
+    //! i.e. verify that the weights represent probabilities, and that 
+    //! those probabilities sum to 1 . 
+    //!(the distribution is represented as a set of weights, which are typically
+    //! log-probabilities)
+    bool isWellDefined(Vec & distribution);
+
+    //!INITIAL E STEP 
+    
+    //!initialization of the reconstruction set
     void initEStep();
+
+    //!initialization of the reconstruction set, version A
+    //for each target:
+    // 1)find the neighbors (we use euclidean distance as an heuristic)
+    // 2)for each neighbor, assign a random weight to each possible transformation
+    void initEStepA();
+
+    //!initialization of the reconstruction set, version B
+
+    void initEStepB();
     
-    //auxialiary function of "initEStep" . 
-    //    for a given pair (target, neighbor), creates all the associated 
-    //    generation candidates (entries) in the data set. 
-    //returns the total weight of the generation candidates created
-    real expandTargetNeighborPairInGenerationSet(int targetIdx,
-                                                 int neighborIdx,
-                                                 int candidateStartIdx);
+
     
-    //auxiliary function of initEStep
-    //    keeps the nearest neighbors for a given target point in a priority
-    //    queue.
+    //!auxialiary function of "initEStep" . 
+    //!    for a given pair (target, neighbor), creates all the  
+    //!    possible reconstruction candidates. 
+    //!returns the total weight of the reconstruction candidates created
+    real expandTargetNeighborPairInReconstructionSet(int targetIdx,
+                                                     int neighborIdx,
+                                                     int candidateStartIdx);
+    
+    //!auxiliary function of initEStep
+    //!    stores the nearest neighbors for a given target point in a priority
+    //!    queue.
     void findNearestNeighbors(int targetIdx,
                               priority_queue< pair< real, int > > & pq);
     
+    
+    //!E STEP
 
-    //-------- LARGE E STEP : VERSION A --------------------------------
+    //!coordination of the different kinds of expectation steps
+    //!  -which are : largeEStepA, largeEStepB, smallEStep
+    void EStep();
+    
+    //!LARGE E STEP : VERSION A (expectation step)
 
-    //full update of the generation set
-    //for each target, keeps the top km most probable <neighbor, transformation> 
-    //pairs (k = nb neighbors, m= nb transformations)
+    //!full update of the reconstruction set
+    //!for each target, keeps the km most probable <neighbor, transformation> 
+    //!pairs (k = nb neighbors, m= nb transformations)
     void largeEStepA();
 
-    //auxiliary function of largeEStepA()
-    //   for a given target, keeps the top km most probable neighbors,
-    //   transformation pairs in a priority queue 
-    //   (k = nb neighbors, m = nb transformations)
-    void findBestTargetCandidates
+    //!auxiliary function of largeEStepA()
+    //!   for a given target, stores the km most probable (neighbors,
+    //!   transformation) pairs in a priority queue 
+    //!   (k = nb neighbors, m = nb transformations)
+    void findBestTargetReconstructionCandidates
     (int targetIdx,
-     priority_queue< GenerationCandidate > & pq);
+     priority_queue< ReconstructionCandidate > & pq);
     
-
-    //-------- LARGE E STEP : VERSION B --------------------------------
-
-    //full update of the generation set
-    //   for each given pair (target, transformation), find the best
-    //   weighted neighbors  
+    
+    //!LARGE E STEP : VERSION B (expectation step)
+    
+    //!full update of the reconstruction set
+    //!   for each given pair (target, transformation), find the best
+    //!   weighted neighbors  
     void largeEStepB();
-
     
-    //auxiliary function of largeEStepB()
-    //   for a given target x and a given transformationt , keeps the best
-    //   weighted triples (x, neighbor, t) in a priority queue .
+    
+    //!auxiliary function of largeEStepB()
+    //!   for a given target x and a given transformation t , stores the best
+    //!    weighted triples (x, neighbor, t) in a priority queue .
     void findBestWeightedNeighbors
     (int targetIdx,
      int transformIdx,
-     priority_queue< GenerationCandidate > & pq);
+     priority_queue< ReconstructionCandidate > & pq);
 
+    //!SMALL E STEP (expectation step)
 
+    //!updating the weights while keeping the candidate neighbor set fixed
+    void smallEStep();
+   
+    //!M STEP
+    
+    //!coordination of the different kinds of maximization step
+    //!(i.e.: we optimize with respect to which parameter?)
+    void MStep();
 
-    //-------- SMALL E STEP --------------------------------------------- 
+    //!maximization step  with respect to  transformation distribution
+    //!parameters
+    void MStepTransformDistribution();
+    
+    //!maximization step  with respect to transformation distribution
+    //!parameters
+    //!(MAP version, alpha = dirichlet prior distribution parameter)
+    //!NOTE :  alpha =1 ->  no regularization
+    void MStepTransformDistributionMAP(real alpha);
 
+    //!maximization step with respect to transformation parameters
+    //!(MAP version)
+    void MStepTransformations();
     
-    //updating the weights while keeping the candidate neighbor set fixed
-    void smallEStep();
+    //!maximization step with respect to noise variance
+    void MStepNoiseVariance();
     
-
-    //-------- M STEP ---------------------------------------------   
+    //!maximization step with respect to noise variance
+    //!(MAP version, alpha and beta = gamma prior distribution parameters)
+    //!NOTE : alpha=1, beta=0 -> no regularization   
+    void MStepNoiseVarianceMAP(real alpha, real beta);    
     
+    //!returns the distance between the reconstruction and the target
+    //!for the 'candidateIdx'th reconstruction candidate
+    real reconstructionEuclideanDistance(int candidateIdx);
+    
+    
+    //STOPPING CRITERION
+    //stage == nstages?
+    bool stoppingCriterionReached();
+    
+    //increment the variable 'stage' of 1
+    void nextStage();
 
-    //updating the transformation parameters
-    void MStep();
-
-
-
 private:
     //#####  Private Data Members  ############################################
 
     // The rest of the private stuff goes here
+   
 };
 
 // Declares a few other classes and functions related to this class



From tihocan at mail.berlios.de  Thu Jul 19 19:20:30 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 19 Jul 2007 19:20:30 +0200
Subject: [Plearn-commits] r7800 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200707191720.l6JHKUfc031696@sheep.berlios.de>

Author: tihocan
Date: 2007-07-19 19:20:30 +0200 (Thu, 19 Jul 2007)
New Revision: 7800

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.h
Log:
Added option 'delayed_update' to control when updates are performed

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2007-07-19 15:15:44 UTC (rev 7799)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2007-07-19 17:20:30 UTC (rev 7800)
@@ -66,8 +66,9 @@
     "parameters gradients (separately for each neuron).\n"
     );
 
-NatGradSMPNNet::NatGradSMPNNet()
-    : noutputs(-1),
+NatGradSMPNNet::NatGradSMPNNet():
+      delayed_update(true),
+      noutputs(-1),
       params_averaging_coeff(1.0),
       params_averaging_freq(5),
       init_lrate(0.01),
@@ -105,6 +106,12 @@
 
 void NatGradSMPNNet::declareOptions(OptionList& ol)
 {
+    declareOption(ol, "delayed_update", &NatGradSMPNNet::delayed_update,
+                  OptionBase::buildoption,
+        "If true, then each CPU's update will be delayed until it is its own\n"
+        "turn to update. This ensures no two CPUs are modifying parameters\n"
+        "at the same time.");
+
     declareOption(ol, "noutputs", &NatGradSMPNNet::noutputs,
                   OptionBase::buildoption,
                   "Number of outputs of the neural network, which can be derived from  output_type and targetsize_\n");
@@ -379,6 +386,7 @@
     layer_mparams.resize(n_layers-1);
     layer_params_delta.resize(n_layers-1);
     layer_params_gradient.resize(n_layers-1);
+    layer_params_update.resize(n_layers - 1);
     biases.resize(n_layers-1);
     activations_scaling.resize(n_layers-1);
     weights.resize(n_layers-1);
@@ -412,6 +420,7 @@
     all_mparams.resize(n_params);
     all_params_gradient.resize(n_params);
     all_params_delta.resize(n_params);
+    params_update.resize(n_params);
 
     // depending on how parameters are grouped on the first layer
     int n_groups = params_natgrad_per_input_template ? (n_neurons-layer_sizes[1]+layer_sizes[0]+1) : n_neurons;
@@ -427,6 +436,8 @@
         if( i==0 && params_natgrad_per_input_template ) {
             PLERROR("This should not be executed");
             layer_params[i]=all_params.subVec(p,np).toMat(layer_sizes[i]+1,layer_sizes[i+1]);
+            layer_params_update[i] = params_update.subVec(p,np).toMat(
+                    layer_sizes[i] + 1, layer_sizes[i+1]);
             layer_mparams[i]=all_mparams.subVec(p,np).toMat(layer_sizes[i]+1,layer_sizes[i+1]);
             biases[i]=layer_params[i].subMatRows(0,1);
             weights[i]=layer_params[i].subMatRows(1,layer_sizes[i]); //weights[0] from layer 0 to layer 1
@@ -443,6 +454,8 @@
         // Usual parameter storage
         }   else    {
             layer_params[i]=all_params.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
+            layer_params_update[i] = params_update.subVec(p, np).toMat(
+                    layer_sizes[i+1], layer_sizes[i] + 1);
             layer_mparams[i]=all_mparams.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
             biases[i]=layer_params[i].subMatColumns(0,1);
             weights[i]=layer_params[i].subMatColumns(1,layer_sizes[i]); // weights[0] from layer 0 to layer 1
@@ -578,6 +591,8 @@
     deepCopyField(pv_stepsizes, copies);
     deepCopyField(pv_stepsigns, copies);
 
+    PLCHECK_MSG(false, "Not fully implemented");
+
     if (params_ptr)
         PLERROR("In NatGradSMPNNet::makeDeepCopyFromShallowCopy - Deep copy of"
                 " 'params_ptr' not implemented");
@@ -625,6 +640,7 @@
     }
 
     nsteps = 0;
+    params_update.fill(0);
 }
 
 void NatGradSMPNNet::train()
@@ -740,7 +756,11 @@
             // Update weights if it is this cpu's turn.
             int sem_value = semctl(semaphore_id, 0, GETVAL);
             if (sem_value == iam) {
-                printf("CPU %d updating (nsteps =%d)\n", iam, nsteps);
+                //printf("CPU %d updating (nsteps =%d)\n", iam, nsteps);
+                if (delayed_update) {
+                    all_params += params_update;
+                    params_update.clear();
+                }
                 sem_value = (sem_value + 1) % ncpus;
                 semun_v.val = sem_value;
                 semctl(semaphore_id, 0, SETVAL, semun_v);
@@ -775,8 +795,11 @@
         if (sem_value == iam || iam == 0) {
             if (sem_value == iam) {
                 if (nsteps >  0) {
-                    // TODO Update weights at the end of training.
-                    printf("CPU %d final updating (nsteps =%d)\n", iam, nsteps);
+                    //printf("CPU %d final updating (nsteps =%d)\n", iam, nsteps);
+                    if (delayed_update) {
+                        all_params += params_update;
+                        params_update.clear();
+                    }
                     nsteps = 0;
                 }
                 // Indicate this CPU is done.
@@ -784,12 +807,10 @@
                 semctl(semaphore_id, iam + 1, SETVAL, semun_v);
                 if (iam != 0) {
                     // Exit additional processes after training.
-                    printf("CPU %d exiting\n", iam);
+                    //printf("CPU %d exiting\n", iam);
                     exit(0);
                 }
             }
-            // The master process is controlling the counter, to ensure all
-            // processes will correctly exit.
             if (semctl(semaphore_id, sem_value + 1, GETVAL) == 0) {
                 // The next process is not done yet: we need to wait.
 #if 0
@@ -803,14 +824,16 @@
             bool finished = true;
             for (int i = 0; i < ncpus; i++) {
                 if (semctl(semaphore_id, i + 1, GETVAL) == 0) {
+                    /*
                     printf("Main CPU still waiting on CPU %d (GETVAL => %d)\n",
                             i, semctl(semaphore_id, i + 1, GETVAL));
+                            */
                     finished = false;
                     break;
                 }
             }
             if (finished) {
-                printf("Main CPU ready to finish (all ready!)\n");
+                //printf("Main CPU ready to finish (all ready!)\n");
                 break;
             }
 
@@ -956,9 +979,18 @@
             // compute gradient on weights and update them in one go (more efficient)
             // mean gradient has less variance, can afford larger learning rate
             //Profiler::pl_profile_start("ProducScaleAccOnlineStep");
-            productScaleAcc(layer_params[i-1],next_neurons_gradient,true,
-                            neuron_extended_outputs_per_layer[i-1],false,
-                            -layer_lrate_factor*lrate /* /minibatch_size */, 1);
+            if (delayed_update) {
+                // Store updates in 'layer_params_update'.
+                productScaleAcc(layer_params_update[i - 1],
+                        next_neurons_gradient, true,
+                        neuron_extended_outputs_per_layer[i-1], false,
+                        -layer_lrate_factor*lrate /* /minibatch_size */, 1);
+            } else {
+                // Directly update the parameters.
+                productScaleAcc(layer_params[i-1],next_neurons_gradient,true,
+                        neuron_extended_outputs_per_layer[i-1],false,
+                        -layer_lrate_factor*lrate /* /minibatch_size */, 1);
+            }
             //Profiler::pl_profile_end("ProducScaleAccOnlineStep");
         }
     }

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.h	2007-07-19 15:15:44 UTC (rev 7799)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.h	2007-07-19 17:20:30 UTC (rev 7800)
@@ -57,6 +57,8 @@
 public:
     //#####  Public Build Options  ############################################
 
+    bool delayed_update;
+
     int noutputs;
 
     //! sizes of hidden layers, provided by the user.
@@ -335,6 +337,14 @@
     //! Semaphore used to control which CPU must perform an update.
     int semaphore_id;
 
+    //! Used to store the cumulative updates to the parameters, when the
+    //! 'delayed_update' option is set.
+    Vec params_update;
+
+    //! Used to store updates to the parameters of each layer (points into the
+    //! 'params_update' vector).
+    TVec<Mat> layer_params_update;
+
     //PP<CorrelationProfiler> g_corrprof, ng_corrprof;    // for optional gradient correlation profiling
 };
 



From tihocan at mail.berlios.de  Thu Jul 19 19:40:36 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 19 Jul 2007 19:40:36 +0200
Subject: [Plearn-commits] r7801 - trunk/plearn_learners/generic
Message-ID: <200707191740.l6JHea6X017115@sheep.berlios.de>

Author: tihocan
Date: 2007-07-19 19:40:31 +0200 (Thu, 19 Jul 2007)
New Revision: 7801

Modified:
   trunk/plearn_learners/generic/PLearner.cc
Log:
Minor moving of code to fix potential crash when using no test stats collector on an empty test set

Modified: trunk/plearn_learners/generic/PLearner.cc
===================================================================
--- trunk/plearn_learners/generic/PLearner.cc	2007-07-19 17:20:30 UTC (rev 7800)
+++ trunk/plearn_learners/generic/PLearner.cc	2007-07-19 17:40:31 UTC (rev 7801)
@@ -883,16 +883,17 @@
     Vec output(outputsize());
     Vec costs(nTestCosts());
 
-    if (len == 0) {
-        // Empty test set: we give -1 cost arbitrarily.
-        costs.fill(-1);
-        test_stats->update(costs);
-    }
-
-    if (test_stats)
+    if (test_stats) {
         // Set names of test_stats costs
         test_stats->setFieldNames(getTestCostNames());
 
+        if (len == 0) {
+            // Empty test set: we give -1 cost arbitrarily.
+            costs.fill(-1);
+            test_stats->update(costs);
+        }
+    }
+
     PP<ProgressBar> pb;
     if (report_progress)
         pb = new ProgressBar("Testing learner", len);



From tihocan at mail.berlios.de  Thu Jul 19 19:56:20 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 19 Jul 2007 19:56:20 +0200
Subject: [Plearn-commits] r7802 - trunk/plearn_learners/generic
Message-ID: <200707191756.l6JHuKGm018319@sheep.berlios.de>

Author: tihocan
Date: 2007-07-19 19:56:20 +0200 (Thu, 19 Jul 2007)
New Revision: 7802

Modified:
   trunk/plearn_learners/generic/AddCostToLearner.cc
Log:
Reverted behavior of the train() method to what it was previously (i.e. use the inner learner's nstages option)

Modified: trunk/plearn_learners/generic/AddCostToLearner.cc
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.cc	2007-07-19 17:40:31 UTC (rev 7801)
+++ trunk/plearn_learners/generic/AddCostToLearner.cc	2007-07-19 17:56:20 UTC (rev 7802)
@@ -597,11 +597,8 @@
         PLASSERT_MSG(-1 != find_threshold , "We where asked to find the threashold and no *class_error costs are selected.\n"
                      "We use the first *class_error cost to select the threshold");
 
-    PLASSERT( learner_ );
-    learner_->nstages = nstages;
-    learner_->train();
-    stage = learner_->stage;
-
+    inherited::train();
+    
     if(-1 != find_threshold){
         
         Vec input;



From tihocan at mail.berlios.de  Thu Jul 19 20:02:15 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 19 Jul 2007 20:02:15 +0200
Subject: [Plearn-commits] r7803 - trunk/plearn_learners/generic
Message-ID: <200707191802.l6JI2FGh018756@sheep.berlios.de>

Author: tihocan
Date: 2007-07-19 20:02:14 +0200 (Thu, 19 Jul 2007)
New Revision: 7803

Modified:
   trunk/plearn_learners/generic/AddCostToLearner.cc
   trunk/plearn_learners/generic/AddCostToLearner.h
Log:
- Field 'find_class_threshold' is now a boolean
- Added missing deep copy statement


Modified: trunk/plearn_learners/generic/AddCostToLearner.cc
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.cc	2007-07-19 17:56:20 UTC (rev 7802)
+++ trunk/plearn_learners/generic/AddCostToLearner.cc	2007-07-19 18:02:14 UTC (rev 7803)
@@ -96,7 +96,7 @@
       to_min(0),
       n_classes(-1),
       confusion_matrix_target(0),
-      find_class_threshold(0)
+      find_class_threshold(false)
 {}
 
 ////////////////////
@@ -182,8 +182,8 @@
     declareOption(ol, "find_class_threshold",
                   &AddCostToLearner::find_class_threshold,
                   OptionBase::buildoption,
-        "0 if we don't find the best threshold between classes.\n"
-        "Otherwise we find the best threshold between classes");
+        "If true, then during training we find the best threshold between\n"
+        "classes.");
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -592,10 +592,10 @@
                 find_threshold = i;
             break;
         }
+        PLASSERT_MSG(-1 != find_threshold , "We where asked to find the "
+                "threshold and no *class_error costs are selected.\n"
+                "We use the first *class_error cost to select the threshold");
     }
-    if(find_class_threshold != 0)
-        PLASSERT_MSG(-1 != find_threshold , "We where asked to find the threashold and no *class_error costs are selected.\n"
-                     "We use the first *class_error cost to select the threshold");
 
     inherited::train();
     
@@ -734,16 +734,17 @@
 void AddCostToLearner::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
-    deepCopyField(combined_output, copies);
-    deepCopyField(bag_outputs, copies);
-    deepCopyField(cross_entropy_prop, copies);
+    deepCopyField(combined_output,      copies);
+    deepCopyField(bag_outputs,          copies);
+    deepCopyField(cross_entropy_prop,   copies);
     varDeepCopyField(cross_entropy_var, copies);
-    deepCopyField(desired_target, copies);
-    varDeepCopyField(output_var, copies);
-    deepCopyField(sub_learner_output, copies);
-    deepCopyField(sub_input, copies);
-    varDeepCopyField(target_var, copies);
-    deepCopyField(costs, copies);
+    deepCopyField(desired_target,       copies);
+    varDeepCopyField(output_var,        copies);
+    deepCopyField(sub_learner_output,   copies);
+    deepCopyField(sub_input,            copies);
+    varDeepCopyField(target_var,        copies);
+    deepCopyField(class_threshold,      copies);
+    deepCopyField(costs,                copies);
 }
 
 ////////////////////

Modified: trunk/plearn_learners/generic/AddCostToLearner.h
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.h	2007-07-19 17:56:20 UTC (rev 7802)
+++ trunk/plearn_learners/generic/AddCostToLearner.h	2007-07-19 18:02:14 UTC (rev 7803)
@@ -126,7 +126,7 @@
     real to_min;
     int n_classes;
     int confusion_matrix_target;
-    int find_class_threshold;
+    bool find_class_threshold;
 
     // ****************
     // * Constructors *



From tihocan at mail.berlios.de  Thu Jul 19 20:15:47 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 19 Jul 2007 20:15:47 +0200
Subject: [Plearn-commits] r7804 - trunk/plearn/misc
Message-ID: <200707191815.l6JIFlnN020322@sheep.berlios.de>

Author: tihocan
Date: 2007-07-19 20:15:46 +0200 (Thu, 19 Jul 2007)
New Revision: 7804

Modified:
   trunk/plearn/misc/vmatmain.cc
Log:
Use PLERROR instead of explicit call to cerr and exit

Modified: trunk/plearn/misc/vmatmain.cc
===================================================================
--- trunk/plearn/misc/vmatmain.cc	2007-07-19 18:02:14 UTC (rev 7803)
+++ trunk/plearn/misc/vmatmain.cc	2007-07-19 18:15:46 UTC (rev 7804)
@@ -533,10 +533,9 @@
         string source = argv[2];
         string destination = argv[3];
         if(argc<4)
-        {
-            cerr<<"usage vmat convert <source> <destination> [--cols=col1,col2,col3,...]\n";
-            exit(1);
-        }
+            PLERROR("Usage: vmat convert <source> <destination> "
+                    "[--cols=col1,col2,col3,...]");
+
         VMat vm = getVMat(source, indexf);
 
         /**



From tihocan at mail.berlios.de  Thu Jul 19 20:17:41 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 19 Jul 2007 20:17:41 +0200
Subject: [Plearn-commits] r7805 - trunk/plearn_learners/online
Message-ID: <200707191817.l6JIHfw3020459@sheep.berlios.de>

Author: tihocan
Date: 2007-07-19 20:17:40 +0200 (Thu, 19 Jul 2007)
New Revision: 7805

Modified:
   trunk/plearn_learners/online/ModuleTester.cc
Log:
Added debug output

Modified: trunk/plearn_learners/online/ModuleTester.cc
===================================================================
--- trunk/plearn_learners/online/ModuleTester.cc	2007-07-19 18:15:46 UTC (rev 7804)
+++ trunk/plearn_learners/online/ModuleTester.cc	2007-07-19 18:17:40 UTC (rev 7805)
@@ -390,6 +390,7 @@
             // Continue only if accumulation test passed.
             if (!ok)
                 return;
+            DBG_MODULE_LOG << "Accumulation test successful" << endl;
             // Verify gradient is coherent with the input, through a subtle
             // perturbation of said input.
             // Save result of fprop.
@@ -400,6 +401,8 @@
                 PLASSERT( val && check );
                 check->resize(val->length(), val->width());
                 *check << *val;
+                DBG_MODULE_LOG << "Reference fprop data (" << all_out[k] << ")"
+                    << ":" << endl << *check << endl;
             }
             for (int k = 0; ok && k < in_grad.length(); k++) {
                 int idx = module->getPortIndex(in_grad[k]);
@@ -435,6 +438,11 @@
                                         (*out_prev)(oi, oj);
                                     (*grad)(p, q) +=
                                         diff * (*out_grad_)(oi, oj) / step;
+                                    DBG_MODULE_LOG << "  diff = " << diff <<
+                                        endl << "  step = " << step << endl <<
+                                        "  out_grad = " << (*out_grad_)(oi, oj)
+                                        << endl << "  grad = " << (*grad)(p, q)
+                                        << endl;
                                 }
                         }
                     }
@@ -450,8 +458,13 @@
                                 (*grad)(p,q) << ") != computed (" <<
                                 (*b_check)(p,q) << ")" << endl;
                             ok = false;
+                        } else {
+                            DBG_MODULE_LOG << "Gradient for port '" <<
+                                module->getPortName(idx) << "' was " <<
+                                "properly computed: finite difference (" <<
+                                (*grad)(p,q) << ") == computed (" <<
+                                (*b_check)(p,q) << ")" << endl;
                         }
-
             }
         }
     }



From nouiz at mail.berlios.de  Thu Jul 19 21:00:30 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 19 Jul 2007 21:00:30 +0200
Subject: [Plearn-commits] r7806 - trunk/python_modules/plearn/pymake
Message-ID: <200707191900.l6JJ0UXr023664@sheep.berlios.de>

Author: nouiz
Date: 2007-07-19 21:00:29 +0200 (Thu, 19 Jul 2007)
New Revision: 7806

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
BUGFIX: pymake -dependency now work 


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2007-07-19 18:17:40 UTC (rev 7805)
+++ trunk/python_modules/plearn/pymake/pymake.py	2007-07-19 19:00:29 UTC (rev 7806)
@@ -1643,10 +1643,9 @@
             elif self.hasmain:
                 self.corresponding_output = join(self.filedir, objsdir, self.filebase)
                 # We append options to the file name if they are not appended to the objsdir name
-                for opt in options:
+                for opt in getOptions(options_choices,optionargs):
                     pyopt = pymake_options_defs[opt]
                     if not pyopt.in_output_dirname:
-                    #if objsdir.find('_' + opt) == -1: # if not found
                         self.corresponding_output = self.corresponding_output + '_' + opt
 
         else:
@@ -2679,6 +2678,7 @@
 
     if 'dependency' in optionargs:
         if 1 <= len(otherargs) <= 2:
+            optionargs.remove('dependency')
             find_dependency(otherargs)
             sys.exit()
         else:



From tihocan at mail.berlios.de  Thu Jul 19 22:08:46 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 19 Jul 2007 22:08:46 +0200
Subject: [Plearn-commits] r7807 - trunk/plearn/base
Message-ID: <200707192008.l6JK8kIk027779@sheep.berlios.de>

Author: tihocan
Date: 2007-07-19 22:08:46 +0200 (Thu, 19 Jul 2007)
New Revision: 7807

Modified:
   trunk/plearn/base/Object.h
Log:
Fixed compilation crash with gcc 4.0.2

Modified: trunk/plearn/base/Object.h
===================================================================
--- trunk/plearn/base/Object.h	2007-07-19 19:00:29 UTC (rev 7806)
+++ trunk/plearn/base/Object.h	2007-07-19 20:08:46 UTC (rev 7807)
@@ -519,7 +519,7 @@
 
 #ifdef PL_PYTHON_VERSION 
 #define DECLARE_OBJECT_PP(PPCLASSTYPE, CLASSTYPE)                       \
-        struct ConvertFromPyObject<PPCLASSTYPE>                         \
+        template<> struct ConvertFromPyObject<PPCLASSTYPE>              \
         {                                                               \
             static PPCLASSTYPE convert(PyObject* o,                     \
                                        bool print_traceback)            \



From tihocan at mail.berlios.de  Thu Jul 19 22:13:10 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 19 Jul 2007 22:13:10 +0200
Subject: [Plearn-commits] r7808 - trunk/plearn/python
Message-ID: <200707192013.l6JKDATb028107@sheep.berlios.de>

Author: tihocan
Date: 2007-07-19 22:13:10 +0200 (Thu, 19 Jul 2007)
New Revision: 7808

Modified:
   trunk/plearn/python/PythonObjectWrapper.h
Log:
Fixed compilation crash with gcc 4.0.2

Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2007-07-19 20:08:46 UTC (rev 7807)
+++ trunk/plearn/python/PythonObjectWrapper.h	2007-07-19 20:13:10 UTC (rev 7808)
@@ -308,26 +308,6 @@
 { static PyObject* newPyObject(const Object* x); };
 
 ///***///***
-// Specialization for General T*.  Attempt to cast into Object*.  If that works
-// we're all set; for specific pointer types (e.g.  map<U,V>* and vector<T>*,
-// below, since they are more specialized they should kick in before this one.
-template <typename T>
-struct ConvertToPyObject<T*>
-{
-    static PyObject* newPyObject(const T* x)
-    {
-        if(!x) // null ptr. becomes None
-            return PythonObjectWrapper::newPyObject();
-
-        if (const Object* objx = dynamic_cast<const Object*>(x))
-            return ConvertToPyObject<Object*>::newPyObject(objx);
-
-        PLERROR("Cannot convert type %s by value to python",
-                TypeTraits<T*>::name().c_str());
-        return 0;//shut up compiler
-    }
-};
-
 // Other specializations
 ///***///***
 
@@ -672,7 +652,31 @@
     template<class T> friend class ConvertToPyObject;
 };
 
+// Specialization for General T*.  Attempt to cast into Object*.  If that works
+// we're all set; for specific pointer types (e.g.  map<U,V>* and vector<T>*),
+// above, since they are more specialized they should kick in before this one.
+// This specialization is not grouped with other specializations because it
+// makes explicit use of the 'newPyObject' method in the PythonObjectWrapper
+// class, and gcc 4.0.2 does not allow this until that class is properly
+// declared.
+template <typename T>
+struct ConvertToPyObject<T*>
+{
+    static PyObject* newPyObject(const T* x)
+    {
+        if(!x) // null ptr. becomes None
+            return PythonObjectWrapper::newPyObject();
 
+        if (const Object* objx = dynamic_cast<const Object*>(x))
+            return ConvertToPyObject<Object*>::newPyObject(objx);
+
+        PLERROR("Cannot convert type %s by value to python",
+                TypeTraits<T*>::name().c_str());
+        return 0;//shut up compiler
+    }
+};
+
+
 //#####  ConvertFromPyObject Implementations  #################################
 
 template<class U, bool is_enum> 



From louradou at mail.berlios.de  Fri Jul 20 01:10:44 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Fri, 20 Jul 2007 01:10:44 +0200
Subject: [Plearn-commits] r7809 - in trunk/python_modules/plearn/learners: .
	autolr modulelearners modulelearners/sampler
	modulelearners/sampler/example modulelearners/sampler/example/data
Message-ID: <200707192310.l6JNAi0I021380@sheep.berlios.de>

Author: louradou
Date: 2007-07-20 01:03:18 +0200 (Fri, 20 Jul 2007)
New Revision: 7809

Removed:
   trunk/python_modules/plearn/learners/modulelearners/examples/
Modified:
   trunk/python_modules/plearn/learners/SVM.py
   trunk/python_modules/plearn/learners/autolr/__init__.py
   trunk/python_modules/plearn/learners/modulelearners/__init__.py
   trunk/python_modules/plearn/learners/modulelearners/network_view.py
   trunk/python_modules/plearn/learners/modulelearners/sampler/__init__.py
   trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/DBN-3RBM.babyAI-1obj.psave
   trunk/python_modules/plearn/learners/modulelearners/sampler/example/do_sampling.py
   trunk/python_modules/plearn/learners/modulelearners/sampler/inputweights.py
   trunk/python_modules/plearn/learners/modulelearners/sampler/sample_from_hidden.py
   trunk/python_modules/plearn/learners/modulelearners/sampler/sample_from_visible.py
Log:
some clean up of the code



Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2007-07-19 20:13:10 UTC (rev 7808)
+++ trunk/python_modules/plearn/learners/SVM.py	2007-07-19 23:03:18 UTC (rev 7809)
@@ -132,8 +132,10 @@
                         'valid_error_rate',
                         'best_parameters',
                         'tried_parameters',
+                        'save_filename',
+			'best_model',
+			'nr_fold'
                         'result_list',
-                        'save_filename'
                         ]
        
       def __init__( self ):
@@ -192,10 +194,8 @@
           return costs
 
       def test(self, samples_target_list):
-          check_samples_target_list(samples_target_list)
-          if len(samples_target_list) <> 1:
-             raise TypeError, "in SVM::test(), samples_target_list must be [[samples],[targets]] (list of list)"          
-          return test_model(self.best_model, samples_target_list[0][0], samples_target_list[0][1])
+          check_samples_target_list([samples_target_list])
+          return test_model(self.best_model, samples_target_list[0], samples_target_list[1])
 
 
       def train_and_tune(self, kernel_type, samples_target_list):
@@ -232,8 +232,7 @@
                      if self.save_filename != None:
                         try:
                            FID=open(self.save_filename,'a')
-                           FID.write('------------\nTry with '+kernel_type+' kernel :\n')
-                           FID.write('parameters : '+str(parameters)+'\n')
+                           FID.write('------------\nTry with '+kernel_type+' kernel :( parameters : '+str(parameters)+' )\n')
                            FID.write(' --> Error rate = '+str(error_rate)+'\n')
                            FID.close()
                         except:
@@ -421,11 +420,11 @@
            if len(samples_target[0]) != len(samples_target[1]):
               raise ValueError, "ERROR: samples_target_list has an element that has an elements with different len. Len are: " + len(samples_target[0])+" and " + len(samples_target[1])
     if len(samples_target_list) == 1:
-       print "cross-validation"
+       print "\nCross-validation...\n"
     elif len(samples_target_list) == 2:
-       print "simple validation"
+       print "\nSimple validation...\n"
     elif len(samples_target_list) == 3:
-       print "validation + test"
+       print "\nValidation + test...\n"
     else:
        raise TypeError, "ERROR: samples_target_list have length "+str(len(samples_target_list))+" (not in [1,2,3])\n"+"samples_target_list has to be a list of [sample, target] arrays\n"+"for example :\n\t[[TrainSet, TrainLabels]]\n"+"\tor [[TrainSamples, TrainLabels], [ValidSamples, ValidLabels]]\n"+"\tor [[TrainSamples, TrainLabels], [ValidSamples, ValidLabels], [TestSamples, TestLabels]]\n"
 
@@ -455,10 +454,24 @@
     my_svm.save_filename = 'my_svm_results.txt'
    
 #<<<#
+#>>># Pre-processing your data : it is better to normalize...
+    
+    # Get the mean and standard deviation on the training set
+    # and normalize the training set (Mahalanobis)
+    #
+    mean, std = normalize(train_samples, None, None)
+    #
+    # DO NOT FORGET to apply the same normalization to other datasets
+    #
+    normalize(valid_samples, mean, std)
+    normalize(test_samples, mean, std)
+    
+#<<<#
 #>>># Defining train / valid data
     # - CROSS-VALIDATION
     
     DATA = [ [train_samples , train_targets] ]
+    svm.nr_fold = 5  #(will train on 4/5 of the data and test on 1/5: this will be done 5 times)
 
     # or...
     # - SIMPLE VALIDATION
@@ -478,10 +491,11 @@
     
     
 #<<<#
-#>>># Compute the accuracies (exploring a bit, each time, the space of parameters)
+#>>># Train several models with different sets of parameters and choosing the best set ("tuning"/"twicking")
     # - my_svm.error_rate indicates the current error rates.
     # - This error rate can only decrease while you run "train_and_tune"
     #   (as you are tuning parameters so as to improve the results)
+    # So one should run train_and_tune several times (as long as he can wait), at least for the LINEAR and RBF kernel
    
     my_svm.train_and_tune( 'LINEAR' ,  DATA )
     my_svm.train_and_tune( 'LINEAR' ,  DATA )
@@ -516,7 +530,8 @@
     #
     # or
     #
-    # If you want to try what give the best parameters (retrain the model on new train data)
+    # If you want to try what give the best parameters
+    # (retrain the model on new train data, but no search for better parameters)
     my_svm.train_and_test( NEW_DATA )
     
     valid_error_rate = my_svm.error_rate
@@ -524,7 +539,7 @@
 #<<<#
 #>>># To try the best trained model with new data (and obtain "fair" error rates)
    
-    TEST_DATA=[ [test_samples , test_targets] ]
+    TEST_DATA=[test_samples , test_targets]
     
     test_error_rate = my_svm.test( TEST_DATA )
     

Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-07-19 20:13:10 UTC (rev 7808)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-07-19 23:03:18 UTC (rev 7809)
@@ -349,7 +349,7 @@
                       min_epochs_to_delete = 2,
                       # Scaling coefficient when modifying learning rates
                       lr_steps=exp(log(10)/2),
-                      logfile=False,
+                      logfile=None,
                       # do not try to go below this learning rate
                       min_lr=1e-6,
                       # Learning rate interval for heuristic

Modified: trunk/python_modules/plearn/learners/modulelearners/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/__init__.py	2007-07-19 20:13:10 UTC (rev 7808)
+++ trunk/python_modules/plearn/learners/modulelearners/__init__.py	2007-07-19 23:03:18 UTC (rev 7809)
@@ -5,9 +5,7 @@
 from plearn.bridge import *
 #from plearn.pyplearn import *
 
-from plearn.learners.autolr import deepcopy
 
-
 tmp_file='/tmp/modulelearner.py'
 
 if plearn.bridgemode.useserver:
@@ -478,7 +476,7 @@
     setConnections(mynewObject, new_connections_list)
     setPorts(mynewObject, new_ports_list)
     setModules(mynewObject, new_modules_list)
-    return deepcopy( mynewObject )
+    mynewObject.build()
     return mynewObject
 
 

Modified: trunk/python_modules/plearn/learners/modulelearners/network_view.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/network_view.py	2007-07-19 20:13:10 UTC (rev 7808)
+++ trunk/python_modules/plearn/learners/modulelearners/network_view.py	2007-07-19 23:03:18 UTC (rev 7809)
@@ -1,30 +1,37 @@
 #!/usr/bin/env python
 
-try:
-  from plearn.pyext import *
-except:
-  from plearn.pymake.pymake import *
-  PLEARNDIR = os.environ.get('PLEARNDIR', os.getcwd())
-  PLEARNDIRpyext = os.path.join(PLEARNDIR,'python_modules','plearn','pyext')
-  PLEARNDIRpyextOBJ =  os.path.join(PLEARNDIRpyext,'OBJS')
-  DIRS=os.listdir(PLEARNDIRpyextOBJ)
-  l=len(DIRS)
-  for dirname in DIRS:
-      l -= 1
-      if l>0 and 'double' in dirname:
-         DIRS.append(dirname)
-         continue
-      elif l>0 and 'dbg' in dirname:
-         DIRS.append(dirname)
-         continue
-      dirname = os.path.join(PLEARNDIRpyextOBJ, dirname)      
-      if 'libplext.so' in os.listdir(dirname):
-         linux_command = 'ln -sf '+ os.path.join(dirname,'libplext.so') + ' ' + os.path.join(PLEARNDIRpyext, 'libplext.so') 
-         os.system(linux_command)
-      try :
-           from plearn.pyext import *
-	   break
-      except: pass
+import sys, os, os.path
+
+from plearn.pyplearn import *
+from plearn.learners.modulelearners import *
+import pydot
+
+
+#try:
+#  from plearn.pyext import *
+#except:
+#  from plearn.pymake.pymake import *
+#  PLEARNDIR = os.environ.get('PLEARNDIR', os.getcwd())
+#  PLEARNDIRpyext = os.path.join(PLEARNDIR,'python_modules','plearn','pyext')
+#  PLEARNDIRpyextOBJ =  os.path.join(PLEARNDIRpyext,'OBJS')
+#  DIRS=os.listdir(PLEARNDIRpyextOBJ)
+#  l=len(DIRS)
+#  for dirname in DIRS:
+#      l -= 1
+#      if l>0 and 'double' in dirname:
+#         DIRS.append(dirname)
+#         continue
+#      elif l>0 and '_dbg_' in dirname:
+#         DIRS.append(dirname)
+#         continue
+#      dirname = os.path.join(PLEARNDIRpyextOBJ, dirname)      
+#      if 'libplext.so' in os.listdir(dirname):
+#         linux_command = 'ln -sf '+ os.path.join(dirname,'libplext.so') + ' ' + os.path.join(PLEARNDIRpyext, 'libplext.so') 
+#         os.system(linux_command)
+#      try :
+#           from plearn.pyext import *
+#	   break
+#      except: pass
   
   
 
@@ -33,10 +40,6 @@
 printAllPorts=False
 
 
-from pyplearn_read import *
-
-import pydot
-
 # global variables:
 modules_dict = {}
 
@@ -89,23 +92,22 @@
     return name
 
 def formatPortNames(name,portname,modules_dict,printAllPorts):
-    if portname in ['input','target','cost','weight']:
-       return '*'+portname.lower()+'*'
-    elif portname in ['output']:
-       return formatModulesNames(name,modules_dict)
-    else:
-       if printAllPorts:
-          return '-*'+portname.lower()+'*'
-       else:
-          return formatModulesNames(name,modules_dict)
-       
+#    if portname in ['input','target','cost','weight']:
+#       return '*'+portname.lower()+'*'
+#    elif portname in ['output']:
+#       return formatModulesNames(name,modules_dict)
+#    else:
+#       if printAllPorts:
+#          return '-*'+portname.lower()+'*'
+#       else:
+#          return formatModulesNames(name,modules_dict)
+    return '*'+portname.lower()+'*'
 
 def checkName(ModuleName, ports_dict, modules_dict):
     if ports_dict.has_key(ModuleName):
        return formatPortNames(ModuleName,ports_dict[ModuleName],modules_dict,False)
     return formatModulesNames(ModuleName,modules_dict)
 
-
 def isModule(module,name):
     return name+'Module' in str(type(module))
      
@@ -240,10 +242,7 @@
     os.system('kuickshow '+output_name+' &')
 
 if __name__ == '__main__':
-    import sys
-    import os, os.path
 
-
     if len(sys.argv) <> 2:
        print "Usage:\n\tpython "+sys.argv[0]+" mylearner.ext"
        print "Purpose:\n\tDraw the graph of a network implemented/saved in a file mylearner.ext\n\twith extension (.ext) .py .pyplearn or .psave"

Modified: trunk/python_modules/plearn/learners/modulelearners/sampler/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/sampler/__init__.py	2007-07-19 20:13:10 UTC (rev 7808)
+++ trunk/python_modules/plearn/learners/modulelearners/sampler/__init__.py	2007-07-19 23:03:18 UTC (rev 7809)
@@ -1,4 +1,5 @@
 from pygame import *
+from numarray import *
 import math
 import sys
 
@@ -15,14 +16,23 @@
    try: return int(c.strip())
    except: return 0
 
-def init_screen(Nim,zoom_factor):
+def init_screen(input_size,zoom_factor):
     init()
-    width = int(math.sqrt(Nim*1.0))
-    if width**2 != Nim:
-       width = int(math.sqrt(Nim*1.0))+1
-#       raise TypeError, "This code only deals with square images\n(and image size "+str(Nim)+" is not the square of an integer)"
-    width *= zoom_factor
-    return display.set_mode([width, width])
+    if type(input_size)==int:
+      width = int(math.sqrt(input_size*1.0))
+      if width**2 != input_size:
+         width = math.ceil(math.sqrt(input_size*1.0))
+#       raise TypeError, "This code only deals with square images\n(and image size "+str(input_size)+" is not the square of an integer)"
+      width *= zoom_factor
+      height = width
+    elif type(input_size)==tuple or type(input_size)==list:
+      if len(input_size) <> 2:
+         raise ValueError, "the first argment of sampler::init_screen() must be of length 2 (if not an <int>)"
+      height = input_size[0]*zoom_factor
+      width  = input_size[1]*zoom_factor
+    else:
+       raise TypeError, "the first argument of sampler::init_screen() must be of type <int>, <tuple> or <list> (not "+str(type(input_size))+")"
+    return display.set_mode([height, width])
 
 def draw_image(values_in_01,screen,zoom_factor):
     """ Draw a 2D image where the gray level corresponds to a value scaled in [0,1]
@@ -33,17 +43,17 @@
     """
     GiveWarning=True
     
-    Nim=len(values_in_01)
-    width = int(math.sqrt(Nim*1.0))
-    if width**2 != Nim:
-       width += 1
-#       raise TypeError, "This code only deals with square images\n(and image size "+str(Nim)+" is not the square of an integer)"
-    width *= zoom_factor
+    width = screen.get_width()
+    height = screen.get_height()
+
     surface = Surface((width, width),0,8)
     surface.set_palette([(i,i,i) for i in range(2**8)])
     for x in range(width/zoom_factor):
-       for y in range(width/zoom_factor):
-           value = values_in_01[x*width/zoom_factor+y]
+       for y in range(height/zoom_factor):
+           if 'numarray' in str(type(values_in_01)) and len(values_in_01.shape)==2:
+              value = values_in_01[x,y]
+	   else:
+              value = values_in_01[x*width/zoom_factor+y]
 	   if value < 0. or value > 1.:
 	      if GiveWarning:
 	         GiveWarning=False
@@ -71,8 +81,12 @@
 	- zoom_factor : int > 0
     """
 
-    MAX=max(weights)
-    MIN=min(weights)
+    if 'numarray' in str(type(weights)):
+       MAX=weights.max()
+       MIN=weights.min()
+    else:
+       MAX=max(weights)
+       MIN=min(weights)
     MAX=2*max(MAX,-MIN)
 
     for i in range(len(weights)):
@@ -80,7 +94,6 @@
     return draw_image(weights,screen,zoom_factor)
 
 def max_matrix(array):
-    print array[0]
     raise SystemExit
     MAX = max(array[0])
     for vec in array:

Modified: trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/DBN-3RBM.babyAI-1obj.psave
===================================================================
(Binary files differ)

Modified: trunk/python_modules/plearn/learners/modulelearners/sampler/example/do_sampling.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/sampler/example/do_sampling.py	2007-07-19 20:13:10 UTC (rev 7808)
+++ trunk/python_modules/plearn/learners/modulelearners/sampler/example/do_sampling.py	2007-07-19 23:03:18 UTC (rev 7809)
@@ -7,8 +7,13 @@
 from plearn.learners.modulelearners.sampler.sample_from_visible import *
 from plearn.learners.modulelearners.sampler.sample_from_hidden import *
 
-learner_filename = os.path.dirname(os.path.abspath(sys.argv[0]))+'/data/DBN-3RBM.babyAI-1obj.psave'
-data_filename=os.path.dirname(os.path.abspath(sys.argv[0]))+'/data/babyAI-1obj.dmat'
+
+import os 
+PLEARNDIR = os.environ.get('PLEARNDIR', os.getcwd())
+default_DIRECTORY = os.path.join(PLEARNDIR,'python_modules','plearn','learners','modulelearners','sampler','example')
+
+learner_filename = default_DIRECTORY + '/data/DBN-3RBM.babyAI-1obj.psave'
+data_filename = default_DIRECTORY + '/data/babyAI-1obj.dmat'
 width = 32
 imageSize = width*width
 
@@ -21,14 +26,22 @@
   imageSize = int(sys.argv[3])
 
 if os.path.isfile(learner_filename) == False:
-   raise EOFError, "Cannot find file "+learner_filename
+   learner_filename2=learner_filename.replace(os.path.dirname(os.path.abspath(sys.argv[0])),os.path.abspath(sys.argv[0]))
+   if os.path.isfile(learner_filename2) == False:
+      raise EOFError, "Cannot find file "+learner_filename
+   else:
+      learner_filename=learner_filename2
 print " loading... "+learner_filename
 learner = loadObject(learner_filename)
 if 'HyperLearner' in str(type(learner)):
    learner=learner.learner
 
 if os.path.isfile(data_filename) == False and os.path.isdir(data_filename) == False:
-   raise EOFError, "Cannot find file or directory "+data_filename
+   data_filename2=data_filename.replace(os.path.dirname(os.path.abspath(sys.argv[0])),os.path.abspath(sys.argv[0]))
+   if os.path.isfile(data_filename) == False and os.path.isdir(data_filename) == False:
+      raise EOFError, "Cannot find file or directory "+data_filename
+   else:
+      data_filename=data_filename2
 print " loading... "+data_filename
 dataSet = pl.AutoVMatrix( specification = data_filename )
 
@@ -48,8 +61,8 @@
       print "\n---------------"
       print "-- MAIN MENU --"
       print "---------------"
-      print "1. *Sample visible units*: initialization of bottom RBM visible units with (randomly picked) real input image"
-      print "2. *Sample visible units*: initialization of top RBM hidden units (random binary vector)"
+      print "1. *Sample visible units*:\n   - initialization of bottom RBM visible units with (randomly picked) real input image"
+      print "2. *Sample visible units*:\n   - initialization of top RBM hidden units (random binary vector)"
       print "3. *Reconstruct* some input image"
       print "4. *Visualize weights* of the 1st RBM (input)"
       print "(to quit, type 'q' or 'Q')\n"

Modified: trunk/python_modules/plearn/learners/modulelearners/sampler/inputweights.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/sampler/inputweights.py	2007-07-19 20:13:10 UTC (rev 7808)
+++ trunk/python_modules/plearn/learners/modulelearners/sampler/inputweights.py	2007-07-19 23:03:18 UTC (rev 7809)
@@ -19,25 +19,52 @@
         image_RBM=learner.module.modules[i]
         break
   image_RBM_name=image_RBM.name
-
-  screen=init_screen(Nim,zoom_factor)
   
+  zoom_factor = globals()['zoom_factor']
   
-#  if 'RBMMatrixConnection' in str(type(image_RBM.connection)):
+  if 'RBMMatrixConnection' in str(type(image_RBM.connection)):
 
-  for i in range(len(image_RBM.connection.weights)):
-    weights=image_RBM.connection.weights[i]
-    print str(i+1)+"/"+str(len(image_RBM.connection.weights))
-    c = draw_normalized_image( weights, screen, zoom_factor )
-    if c==EXITCODE:
-       return
+    screen=init_screen(Nim,zoom_factor)
+    for i in range(len(image_RBM.connection.weights)):
+        weights=image_RBM.connection.weights[i]
+        print str(i+1)+"/"+str(len(image_RBM.connection.weights))
+        c = draw_normalized_image( weights, screen, zoom_factor )
+        if c==EXITCODE:
+           return
        
-#  elif 'RBMMixedConnection' in str(type(image_RBM.connection)):
+  elif 'RBMMixedConnection' in str(type(image_RBM.connection)):
 
+    N_filter = len(image_RBM.connection.sub_connections)
+    N_inputim = len(image_RBM.connection.sub_connections[0])
+    size_filter = image_RBM.connection.sub_connections[0][0].kernel.shape
+    zoom_factor **= 2
+
+    screen=init_screen( (size_filter[0]*N_inputim+(N_inputim-1) , size_filter[1]) , zoom_factor)
+    for i in range(N_filter):
+	weights = image_RBM.connection.sub_connections[i][0].kernel
+        print str(i+1)+"/"+str(N_filter)
+        for j in range(1,N_inputim):
+	   weights.resize( size_filter[0]*(j+1)+j, size_filter[1] )
+	   weights[size_filter[0]*j]=[0]*size_filter[1]
+	   weights[size_filter[0]*j+1:]=image_RBM.connection.sub_connections[i][j].kernel
+        c = draw_normalized_image( weights, screen, zoom_factor )
+        if c==EXITCODE:
+           return
+	   
+  else:
+     raise TypeError, "sampler::view_inputweights() not yet implemented for RBM connection of type "+str(type(image_RBM.connection))
+
+
 def inputweights_man():
      print "\nPlease type:"
      print ":    <ENTER>   : to continue Gibbs Sampling (same gibbs step)"
      print ":      q       : (quit) when you are fed up\n"
+     print "Meaning of gray levels (g):"
+     print "\tg = 127  <->  w = 0"
+     print "\tg > 127  <->  w > 0"
+     print "\tg < 127  <->  w < 0"
+     print "\tg = 255  <->  w = +max{ -min(w), max(w) }"
+     print "\tg = 0    <->  w = -max{ -min(w), max(w) }\n"
 
 if __name__ == "__main__":
 

Modified: trunk/python_modules/plearn/learners/modulelearners/sampler/sample_from_hidden.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/sampler/sample_from_hidden.py	2007-07-19 20:13:10 UTC (rev 7808)
+++ trunk/python_modules/plearn/learners/modulelearners/sampler/sample_from_hidden.py	2007-07-19 23:03:18 UTC (rev 7809)
@@ -7,28 +7,20 @@
 
 def view_sample_from_hidden(learner, Nim, init_gibbs_step):
 
-  print "\nChoose betweem these options:"
-  print "1.[default] Gibbs sampling in the top RBM + mean field"
-  print "2.                   ''                   + sample hidden->visible"
-  c = pause()
-  while c not in [0,1,2,EXITCODE]:
-        c = pause()
-  MeanField = False
-  if c==1:
-     MeanField = True
-  elif c==EXITCODE:
-     return
 
   print "analyzing learner..."
   #
   # Getting the RBMmodule which sees the image (looking at size of the down layer)
   #
+  nRBM=0 # The number of RBMs
   modules=getModules(learner)
   for i in range(len(modules)):
      module = modules[i]
-     if isModule(module,'RBM') and module.connection.down_size == Nim:
-        image_RBM=learner.module.modules[i]
-        break
+     if isModule(module,'RBM'):
+        nRBM += 1
+        if module.connection.down_size == Nim:
+           image_RBM=learner.module.modules[i]
+           break
   image_RBM_name=image_RBM.name
   #
   # Getting the top RBMmodule
@@ -39,6 +31,19 @@
   
   NH=top_RBM.connection.up_size
 
+  if nRBM == 1: MeanField=False
+  else:
+     print "\nChoose betweem these options:"
+     print "1.[default] Gibbs sampling in the top RBM + mean field"
+     print "2.                   ''                   + sample hidden->visible"
+     c = pause()
+     while c not in [0,1,2,EXITCODE]:
+           c = pause()
+     MeanField = False
+     if c==1:
+        MeanField = True
+     elif c==EXITCODE:
+        return
 
   if MeanField:
      ports = [ ('input', top_RBM_name+'.hidden_sample' ),

Modified: trunk/python_modules/plearn/learners/modulelearners/sampler/sample_from_visible.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/sampler/sample_from_visible.py	2007-07-19 20:13:10 UTC (rev 7808)
+++ trunk/python_modules/plearn/learners/modulelearners/sampler/sample_from_visible.py	2007-07-19 23:03:18 UTC (rev 7809)
@@ -7,28 +7,19 @@
 
 def view_sample_from_visible(learner, Nim, dataSet, init_gibbs_step):
 
-  print "\nChoose betweem these options:"
-  print "1.[default] Gibbs sampling in the top RBM + mean field"
-  print "2.                   ''                   + sample hidden<->visible"
-  c = pause()
-  while c not in [0,1,2,EXITCODE]:
-        c = pause()
-  MeanField = False
-  if c==1:
-     MeanField = True
-  elif c==EXITCODE:
-     return
-
   print "analyzing learner..."
   #
   # Getting the RBMmodule which sees the image (looking at size of the down layer)
   #
+  nRBM=0 # The number of RBMs
   modules=getModules(learner)
   for i in range(len(modules)):
      module = modules[i]
-     if isModule(module,'RBM') and module.connection.down_size == Nim:
-        image_RBM=learner.module.modules[i]
-        break
+     if isModule(module,'RBM'):
+        nRBM += 1
+        if module.connection.down_size == Nim:
+           image_RBM=learner.module.modules[i]
+           break
   image_RBM_name=image_RBM.name
   #
   # Getting the top RBMmodule
@@ -39,7 +30,20 @@
   
   NH=top_RBM.connection.up_size
 
-
+  if nRBM == 1: MeanField=False
+  else:
+    print "\nChoose betweem these options:"
+    print "1.[default] Gibbs sampling in the top RBM + mean field"
+    print "2.                   ''                   + sample hidden<->visible"
+    c = pause()
+    while c not in [0,1,2,EXITCODE]:
+        c = pause()
+    MeanField = False
+    if c==1:
+       MeanField = True
+    elif c==EXITCODE:
+       return
+  
   if MeanField:
      init_ports = [ ('input',  image_RBM_name+'.visible'),
                     ('output', top_RBM_name+'.hidden.state')



From nouiz at mail.berlios.de  Fri Jul 20 21:34:10 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 20 Jul 2007 21:34:10 +0200
Subject: [Plearn-commits] r7810 - trunk/plearn/math
Message-ID: <200707201934.l6KJYAsY012850@sheep.berlios.de>

Author: nouiz
Date: 2007-07-20 21:34:09 +0200 (Fri, 20 Jul 2007)
New Revision: 7810

Modified:
   trunk/plearn/math/TMat_maths_specialisation.h
Log:
added profiling of sgemm and dgemm


Modified: trunk/plearn/math/TMat_maths_specialisation.h
===================================================================
--- trunk/plearn/math/TMat_maths_specialisation.h	2007-07-19 23:03:18 UTC (rev 7809)
+++ trunk/plearn/math/TMat_maths_specialisation.h	2007-07-20 19:34:09 UTC (rev 7810)
@@ -46,6 +46,7 @@
 #define TMat_maths_specialisation_INC
 
 #include "TMat.h"
+#include <plearn/sys/Profiler.h>
 
 namespace PLearn {
 using namespace std;
@@ -193,6 +194,7 @@
                             const TMat<double>& B, bool transposeB,
                             double alpha, double beta)
 {
+    Profiler::pl_profile_start("productScaleAcc(dgemm)");
 #ifdef BOUNDCHECK
     int l2;
 #endif
@@ -251,6 +253,7 @@
 
     dgemm_(&transb, &transa, &w2, &l1, &w1, &alpha, B.data(), &ldb, A.data(),
            &lda, &beta, C.data(), &ldc);
+    Profiler::pl_profile_end("productScaleAcc(dgemm)");
 }
 
 //! y <- alpha A.x + beta y 
@@ -424,6 +427,8 @@
                             const TMat<float>& B, bool transposeB,
                             float alpha, float beta)
 {
+    Profiler::pl_profile_start("productScaleAcc(sgemm)");
+
 #ifdef BOUNDCHECK
     int l2;
 #endif
@@ -476,6 +481,7 @@
 
     sgemm_(&transb, &transa, &w2, &l1, &w1, &alpha, B.data(), &ldb, A.data(),
            &lda, &beta, C.data(), &ldc);
+    Profiler::pl_profile_end("productScaleAcc(sgemm)");
 }
 
 //! y <- alpha A.x + beta y



From nouiz at mail.berlios.de  Fri Jul 20 21:34:46 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 20 Jul 2007 21:34:46 +0200
Subject: [Plearn-commits] r7811 - trunk/plearn/math
Message-ID: <200707201934.l6KJYkJA012883@sheep.berlios.de>

Author: nouiz
Date: 2007-07-20 21:34:46 +0200 (Fri, 20 Jul 2007)
New Revision: 7811

Modified:
   trunk/plearn/math/pl_math.cc
Log:
removed gcc 4.2 warning


Modified: trunk/plearn/math/pl_math.cc
===================================================================
--- trunk/plearn/math/pl_math.cc	2007-07-20 19:34:09 UTC (rev 7810)
+++ trunk/plearn/math/pl_math.cc	2007-07-20 19:34:46 UTC (rev 7811)
@@ -49,10 +49,10 @@
 
 
 # ifdef BIGENDIAN
-_plearn_nan_type plearn_nan = { 0x7f, 0xc0, 0, 0 };
+_plearn_nan_type plearn_nan = { {0x7f, 0xc0, 0, 0} };
 # endif
 # ifdef LITTLEENDIAN
-_plearn_nan_type plearn_nan = { 0, 0, 0xc0, 0x7f };
+_plearn_nan_type plearn_nan = { {0, 0, 0xc0, 0x7f} };
 # endif
 
 float tanhtable[TANHTABLESIZE];



From nouiz at mail.berlios.de  Fri Jul 20 21:36:21 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 20 Jul 2007 21:36:21 +0200
Subject: [Plearn-commits] r7812 - trunk/plearn/vmat
Message-ID: <200707201936.l6KJaLDs012994@sheep.berlios.de>

Author: nouiz
Date: 2007-07-20 21:36:21 +0200 (Fri, 20 Jul 2007)
New Revision: 7812

Modified:
   trunk/plearn/vmat/SelectColumnsVMatrix.cc
   trunk/plearn/vmat/SelectColumnsVMatrix.h
Log:
Added the possibility to revert the selection.
That way we can tell the columns we don't want


Modified: trunk/plearn/vmat/SelectColumnsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SelectColumnsVMatrix.cc	2007-07-20 19:34:46 UTC (rev 7811)
+++ trunk/plearn/vmat/SelectColumnsVMatrix.cc	2007-07-20 19:36:21 UTC (rev 7812)
@@ -58,13 +58,15 @@
 //////////////////////////
 SelectColumnsVMatrix::SelectColumnsVMatrix()
     : extend_with_missing(false),
-      fields_partial_match(false)
+      fields_partial_match(false),
+      inverse_fields_selection(false)
 {}
 
 SelectColumnsVMatrix::SelectColumnsVMatrix(VMat the_source, TVec<string> the_fields, bool the_extend_with_missing)
     : extend_with_missing(the_extend_with_missing),
       fields(the_fields),
-      fields_partial_match(false)
+      fields_partial_match(false),
+      inverse_fields_selection(false)
 {
     source = the_source;
     build_();
@@ -76,7 +78,8 @@
     inherited(the_source, call_build_),
     extend_with_missing(false),
     indices(the_indices),
-    fields_partial_match(false)
+    fields_partial_match(false),
+    inverse_fields_selection(false)
 {
     if (call_build_)
         build_();
@@ -84,7 +87,8 @@
 
 SelectColumnsVMatrix::SelectColumnsVMatrix(VMat the_source, Vec the_indices)
     : extend_with_missing(false),
-      fields_partial_match(false)
+      fields_partial_match(false),
+      inverse_fields_selection(false)
 {
     source = the_source;
     indices.resize(the_indices.length());
@@ -137,6 +141,10 @@
     declareOption(ol, "extend_with_missing", &SelectColumnsVMatrix::extend_with_missing, OptionBase::buildoption,
                   "If set to 1, then fields specified in the 'fields' option that do not exist\n"
                   "in the source VMatrix will be filled with missing values.");
+    declareOption(ol, "inverse_fields_selection", &SelectColumnsVMatrix::inverse_fields_selection,
+                  OptionBase::buildoption,
+                  "If set to true, after all previous fields selection, we inverse the selection.\n"
+                  "This way we can specify the indicies we don't want.");
 
     inherited::declareOptions(ol);
 }
@@ -150,6 +158,7 @@
     deepCopyField(sinput, copies);
     deepCopyField(indices, copies);
     deepCopyField(fields, copies);
+    deepCopyField(inverse_fields_selection, copies);
 }
 
 ///////////
@@ -216,7 +225,22 @@
                         }
             }
         }
-
+        
+        //we inverse the selection
+        if(inverse_fields_selection){
+            TVec<int> newindices;
+            for (int i = 0;i<source.width();i++){
+                bool found=false;
+                for(int j=0;j<indices.size();j++)
+                    if(indices[j]==i){
+                        found = true;
+                        break;
+                    }
+                if(!found)
+                    newindices.append(i);
+            }
+            indices = newindices;
+        }
         // Copy matrix dimensions
         width_ = indices.length();
         length_ = source->length();

Modified: trunk/plearn/vmat/SelectColumnsVMatrix.h
===================================================================
--- trunk/plearn/vmat/SelectColumnsVMatrix.h	2007-07-20 19:34:46 UTC (rev 7811)
+++ trunk/plearn/vmat/SelectColumnsVMatrix.h	2007-07-20 19:36:21 UTC (rev 7812)
@@ -71,6 +71,7 @@
     TVec<int> indices;
     TVec<string> fields;
     bool fields_partial_match;
+    bool inverse_fields_selection;
 
 public:
 



From nouiz at mail.berlios.de  Fri Jul 20 21:37:35 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 20 Jul 2007 21:37:35 +0200
Subject: [Plearn-commits] r7813 - trunk/scripts
Message-ID: <200707201937.l6KJbZ2E013085@sheep.berlios.de>

Author: nouiz
Date: 2007-07-20 21:37:35 +0200 (Fri, 20 Jul 2007)
New Revision: 7813

Modified:
   trunk/scripts/multipymake
Log:
Better printing of information. This way we can see it more easily


Modified: trunk/scripts/multipymake
===================================================================
--- trunk/scripts/multipymake	2007-07-20 19:36:21 UTC (rev 7812)
+++ trunk/scripts/multipymake	2007-07-20 19:37:35 UTC (rev 7813)
@@ -65,8 +65,8 @@
 for i in "$@";
   do
   iname=${i//\ /_}
-  echo "Compiling \"${BASEPROG}${iname}.cc\" with command:"
+  echo -e "\nCompiling \"${BASEPROG}${iname}.cc\" with command:"
   echo " pymake $i ${PYMAKEFLAGS[@]} ${BASEPROG}${iname}.cc"
   pymake $i ${PYMAKEFLAGS[@]} ${BASEPROG}${iname}.cc || ( echo "Build failed for $i"; exit)
-  echo "Ended with status: $?"
+  echo -e "Ended with status: $?\n"
 done



From yoshua at mail.berlios.de  Sun Jul 22 04:29:09 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Sun, 22 Jul 2007 04:29:09 +0200
Subject: [Plearn-commits] r7814 - trunk/plearn_learners/online/EXPERIMENTAL
Message-ID: <200707220229.l6M2T9ea022775@sheep.berlios.de>

Author: yoshua
Date: 2007-07-22 04:29:08 +0200 (Sun, 22 Jul 2007)
New Revision: 7814

Modified:
   trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
Log:
Avoid recomputing hidden expectations.


Modified: trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-20 19:37:35 UTC (rev 7813)
+++ trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-22 02:29:08 UTC (rev 7814)
@@ -816,16 +816,25 @@
     {
         // Autoassociator reconstruction cost
         PLASSERT( ports_value.length() == nPorts() );
-        if(!hidden_expectations_are_computed)
-        {
-            computePositivePhaseHiddenActivations(*visible);
-            hidden_layer->computeExpectations();
-            hidden_expectations_are_computed=true;
+
+        // if hidden is provided, condition on it rather than
+        // use the P(h|visible) as hidden.
+        Mat h;
+        if (hidden && !hidden->isEmpty())
+            h = *hidden;
+        else {
+            if(!hidden_expectations_are_computed)
+            {
+                computePositivePhaseHiddenActivations(*visible);
+                hidden_layer->computeExpectations();
+                hidden_expectations_are_computed=true;
+            }
+            h = hidden_layer->getExpectations();
         }
 
         // Don't need to verify if they are asked in a port, this was done previously
 
-        computeVisibleActivations(hidden_layer->getExpectations(), true);
+        computeVisibleActivations(h, true);
         if(visible_reconstruction_activations)
         {
             PLASSERT( visible_reconstruction_activations->isEmpty() );



From yoshua at mail.berlios.de  Sun Jul 22 04:30:34 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Sun, 22 Jul 2007 04:30:34 +0200
Subject: [Plearn-commits] r7815 - trunk/plearn_learners/online
Message-ID: <200707220230.l6M2UY1C022883@sheep.berlios.de>

Author: yoshua
Date: 2007-07-22 04:30:33 +0200 (Sun, 22 Jul 2007)
New Revision: 7815

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
Better asserts and avoid some recomputations.


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-07-22 02:29:08 UTC (rev 7814)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-07-22 02:30:33 UTC (rev 7815)
@@ -691,11 +691,12 @@
     if (compute_contrastive_divergence)
     {
         contrastive_divergence = ports_value[getPortIndex("contrastive_divergence")];
-        if (!contrastive_divergence || !contrastive_divergence->isEmpty())
+/* YB: I don't agree with this error message: the behavior should be adapted to the provided ports. 
+      if (!contrastive_divergence || !contrastive_divergence->isEmpty())
             PLERROR("In RBMModule::fprop - When option "
                     "'compute_contrastive_divergence' is 'true', the "
                     "'contrastive_divergence' port should be provided, as an "
-                    "output.");
+                    "output.");*/
         negative_phase_visible_samples =
             ports_value[getPortIndex("negative_phase_visible_samples.state")];
         negative_phase_hidden_expectations =
@@ -703,7 +704,6 @@
         negative_phase_hidden_activations =
             ports_value[getPortIndex("negative_phase_hidden_activations.state")];
     }
-
     bool hidden_expectations_are_computed = false;
     hidden_activations_are_computed = false;
     bool found_a_valid_configuration = false;
@@ -804,16 +804,23 @@
     {
         // Autoassociator reconstruction cost
         PLASSERT( ports_value.length() == nPorts() );
-        if(!hidden_expectations_are_computed)
-        {
-            computePositivePhaseHiddenActivations(*visible);
-            hidden_layer->computeExpectations();
-            hidden_expectations_are_computed=true;
+
+        Mat h;
+        if (hidden && !hidden->isEmpty())
+            h = *hidden;
+        else {
+            if(!hidden_expectations_are_computed)
+            {
+                computePositivePhaseHiddenActivations(*visible);
+                hidden_layer->computeExpectations();
+                hidden_expectations_are_computed=true;
+            }
+            h = hidden_layer->getExpectations();
         }
 
         // Don't need to verify if they are asked in a port, this was done previously
 
-        computeVisibleActivations(hidden_layer->getExpectations(), true);
+        computeVisibleActivations(h, true);
         if(visible_reconstruction_activations)
         {
             PLASSERT( visible_reconstruction_activations->isEmpty() );
@@ -1102,9 +1109,12 @@
     Mat* weights_grad = ports_gradient[getPortIndex("weights")];
     hidden_bias = ports_value[getPortIndex("hidden_bias")];
     Mat* contrastive_divergence_grad = NULL;
+    Mat* contrastive_divergence = ports_value[getPortIndex("contrastive_divergence")];
+    bool computed_contrastive_divergence = compute_contrastive_divergence && 
+        contrastive_divergence && !contrastive_divergence->isEmpty();
 
     // Ensure the gradient w.r.t. contrastive divergence is 1 (if provided).
-    if (compute_contrastive_divergence) {
+    if (computed_contrastive_divergence) {
         contrastive_divergence_grad =
             ports_gradient[getPortIndex("contrastive_divergence")];
         if (contrastive_divergence_grad) {
@@ -1120,8 +1130,8 @@
 
     // Ensure the visible gradient is not provided as input. This is because we
     // accumulate more than once in 'visible_grad'.
-    PLASSERT_MSG( !visible_grad || visible_grad->isEmpty(), "Cannot provide "
-            "an input gradient w.r.t. visible units" );
+    PLASSERT_MSG( !visible_grad || visible_grad->isEmpty(), "If visible gradient is desired "
+                  " the corresponding matrix should have 0 length" );
 
     bool compute_visible_grad = visible_grad && visible_grad->isEmpty();
     bool compute_weights_grad = weights_grad && weights_grad->isEmpty();
@@ -1138,7 +1148,7 @@
         // learning rate is equal to 0. This is because we must propagate the
         // gradient to the visible layer, even though no update is required.
         setAllLearningRates(grad_learning_rate);
-        PLASSERT( hidden && hidden_act );
+        PLASSERT_MSG( hidden && hidden_act , "To compute gradients in bprop, the hidden_activations.state port must have been filled during fprop");
         // Compute gradient w.r.t. activations of the hidden layer.
         hidden_layer->bpropUpdate(
                 *hidden_act, *hidden, hidden_act_grad, *hidden_grad,
@@ -1278,13 +1288,13 @@
         PLASSERT( visible && !visible->isEmpty() );
         setAllLearningRates(cd_learning_rate);
         Mat* negative_phase_visible_samples =
-            compute_contrastive_divergence?ports_value[getPortIndex("negative_phase_visible_samples.state")]:0;
+            computed_contrastive_divergence?ports_value[getPortIndex("negative_phase_visible_samples.state")]:0;
         const Mat* negative_phase_hidden_expectations =
-            compute_contrastive_divergence ?
+            computed_contrastive_divergence ?
                 ports_value[getPortIndex("negative_phase_hidden_expectations.state")]
                 : NULL;
         Mat* negative_phase_hidden_activations =
-            compute_contrastive_divergence ?
+            computed_contrastive_divergence ?
                 ports_value[getPortIndex("negative_phase_hidden_activations.state")]
                 : NULL;
 
@@ -1304,7 +1314,7 @@
                 computeHiddenActivations(visible_layer->samples);
                 hidden_layer->computeExpectations();
             }
-            PLASSERT( !compute_contrastive_divergence );
+            PLASSERT( !computed_contrastive_divergence );
             PLASSERT( !negative_phase_hidden_expectations );
             PLASSERT( !negative_phase_hidden_activations );
             negative_phase_hidden_expectations = &(hidden_layer->getExpectations());



From yoshua at mail.berlios.de  Sun Jul 22 04:31:51 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Sun, 22 Jul 2007 04:31:51 +0200
Subject: [Plearn-commits] r7816 - trunk/plearn_learners/online
Message-ID: <200707220231.l6M2Vpds022961@sheep.berlios.de>

Author: yoshua
Date: 2007-07-22 04:31:51 +0200 (Sun, 22 Jul 2007)
New Revision: 7816

Modified:
   trunk/plearn_learners/online/OnlineLearningModule.cc
   trunk/plearn_learners/online/OnlineLearningModule.h
   trunk/plearn_learners/online/RBMBinomialLayer.cc
Log:
Using simpler formular for NLL in RBMBinomial layer, and new fprop/bprop python-accessible methods for OnlineLearningModules


Modified: trunk/plearn_learners/online/OnlineLearningModule.cc
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.cc	2007-07-22 02:30:33 UTC (rev 7815)
+++ trunk/plearn_learners/online/OnlineLearningModule.cc	2007-07-22 02:31:51 UTC (rev 7816)
@@ -42,6 +42,7 @@
 
 
 #include "OnlineLearningModule.h"
+#include <plearn/base/RemoteDeclareMethod.h>
 
 namespace PLearn {
 using namespace std;
@@ -323,6 +324,106 @@
     inherited::declareOptions(ol);
 }
 
+////////////////////
+// declareMethods //
+////////////////////
+void OnlineLearningModule::declareMethods(RemoteMethodMap& rmm)
+{
+    // Insert a backpointer to remote methods; note that this
+    // different than for declareOptions()
+    rmm.inherited(inherited::_getRemoteMethodMap_());
+
+    declareMethod(
+        rmm, "getPorts", &OnlineLearningModule::getPorts,
+        (BodyDoc("Return the list of port names of the module\n"),
+         RetDoc ("The list of port names")));
+
+    declareMethod(
+        rmm, "forget", &OnlineLearningModule::forget,
+        (BodyDoc("Reset the parameters to the state they would be before starting training.\n"
+                 "This may involve randomization using the random generator.\n")));
+
+    declareMethod(
+        rmm, "namedFprop", &OnlineLearningModule::namedFprop,
+        (BodyDoc("Perform the fprop computation on an OnlineLearningModule, which takes matrices\n"
+                  "in user-selected input ports and computes outputs in user-selected output-ports.\n"
+                  "The function actually computed by the module depends on the selected ports and\n"
+                  "on its internal state (options and parameters)\n"),
+         ArgDoc ("inputs", "A dictionary of input matrices (one for each input port), indexed by the port names,\n"),
+         ArgDoc ("wanted_outputs", "A list of wanted output port names,\n"),
+         RetDoc ("A dictionary of the input and output matrices (indexed by their name).\n")));
+
+    declareMethod(
+        rmm, "namedBpropAccUpdate", &OnlineLearningModule::namedBpropAccUpdate,
+        (BodyDoc("Perform the BpropAccUpdate computation on an OnlineLearningModule, which\n"
+                 "takes matrices in user-selected input ports, output ports, and output\n"
+                 "gradient ports and computes gradients for user-selected input ports.\n"
+                 "The function actually computed by the module depends on the selected ports\n"
+                 "and on its internal state (options and parameters)\n"),
+         ArgDoc ("values", "A dictionary of named input and output matrices that was\n"
+                 "returned by namedFprop (one entry for each input and output port used).\n"),
+         ArgDoc ("gradients", "A dictionary of named output (and possibly input) gradient\n"
+                 "matrices (the name indexing each matrix is the name of corresponding port).\n"
+                 "Output gradient matrices should be full, whereas input gradient matrices\n"
+                 "into which to accumulate should have lenght 0 and correct width.\n"),
+         ArgDoc ("additional_input_gradients", "A list of wanted input port names,\n"
+                 "for which the gradient is desired (no accumulation)\n"),
+         RetDoc ("A dictionary of all the input and output gradient matrices (indexed\n"
+                 "by their port name), including those in the gradients argument\n"
+                 "and those named in the additional_input_gradiaents argument.\n")));
+
+}
+
+map<string,Mat> OnlineLearningModule::namedFprop(map<string,Mat>& inputs, TVec<string> wanted_outputs)
+{
+    map<string,Mat> outputs;
+    TVec<string> port_names = getPorts();
+    TVec<Mat*> ports_value(nPorts());
+    map<string,Mat>::iterator it=inputs.begin();
+    for (;it!=inputs.end();++it)
+        ports_value[getPortIndex(it->first)]= &it->second;
+    for (int i=0;i<wanted_outputs.length();i++)
+        ports_value[getPortIndex(wanted_outputs[i])]= new Mat(0,0);
+    fprop(ports_value);
+    for (it=inputs.begin();it!=inputs.end();++it)
+        outputs[it->first]=it->second;
+    for (int i=0;i<wanted_outputs.length();i++)
+        outputs[wanted_outputs[i]]= *ports_value[getPortIndex(wanted_outputs[i])];
+    return outputs;
+}
+
+map<string,Mat> OnlineLearningModule::namedBpropAccUpdate(map<string,Mat>& values, 
+                                                          map<string,Mat>& gradients, 
+                                                          TVec<string> additional_input_gradients)
+{
+    map<string,Mat> all_gradients;
+    TVec<string> port_names = getPorts();
+    TVec<Mat*> ports_value(nPorts());
+    TVec<Mat*> ports_gradient(nPorts());
+    map<string,Mat>::iterator it=values.begin();
+    for (;it!=values.end();++it)
+        ports_value[getPortIndex(it->first)]= &it->second;
+    it=gradients.begin();
+    for (;it!=gradients.end();++it)
+        ports_gradient[getPortIndex(it->first)]= &it->second;
+    for (int i=0;i<additional_input_gradients.length();i++)
+    {
+        Mat port_value = values[additional_input_gradients[i]];
+        // the additional input gradients are to be initialized as zero matrices
+        Mat* port_gradient = new Mat(port_value.length(),port_value.width());
+        port_gradient->resize(0,port_value.width());
+        ports_gradient[getPortIndex(additional_input_gradients[i])]= port_gradient;
+    }
+    bpropAccUpdate(ports_value,ports_gradient);
+    for (it=gradients.begin();it!=gradients.end();++it)
+        all_gradients[it->first]=it->second;
+    for (int i=0;i<additional_input_gradients.length();i++)
+        all_gradients[additional_input_gradients[i]]= 
+            *ports_gradient[getPortIndex(additional_input_gradients[i])];
+    return all_gradients;
+}
+
+
 ////////////
 // build_ //
 ////////////

Modified: trunk/plearn_learners/online/OnlineLearningModule.h
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.h	2007-07-22 02:30:33 UTC (rev 7815)
+++ trunk/plearn_learners/online/OnlineLearningModule.h	2007-07-22 02:31:51 UTC (rev 7816)
@@ -130,6 +130,11 @@
     //! - crash otherwise
     virtual void fprop(const TVec<Mat*>& ports_value);
 
+    virtual map<string,Mat> namedFprop(map<string,Mat>& inputs, TVec<string> wanted_outputs);
+    virtual map<string,Mat> namedBpropAccUpdate(map<string,Mat>& values, 
+                                                map<string,Mat>& gradients, 
+                                                TVec<string> additional_input_gradients);
+
     //! SOON TO BE DEPRECATED, USE bpropAccUpdate(const TVec<Mat*>& ports_value,
     //!                                           const TVec<Mat*>& ports_gradient)
     //! Adapt based on the output gradient: this method should only
@@ -296,6 +301,9 @@
     //! Declares the class options.
     static void declareOptions(OptionList& ol);
 
+    //! Declare the methods that are remote-callable
+    static void declareMethods(RemoteMethodMap& rmm);
+
 protected:
 
     //! Used to store the size of each port (may be used in sub-classes).

Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-07-22 02:30:33 UTC (rev 7815)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-07-22 02:31:51 UTC (rev 7816)
@@ -334,34 +334,20 @@
         {
             target_i = target[i];
             activation_i = activation[i];
-            if(!fast_exact_is_equal(target_i,0.0))
-                // nll -= target[i] * pl_log(expectations[i]); 
-                // but it is numerically unstable, so use instead
-                // log (1/(1+exp(-x))) = -log(1+exp(-x)) = -softplus(-x)
-                ret += target_i * tabulated_softplus(-activation_i);
-            if(!fast_exact_is_equal(target_i,1.0))
-                // ret -= (1-target_i) * pl_log(1-expectation_i);
-                // log (1 - 1/(1+exp(-x))) = log(exp(-x)/(1+exp(-x)))
-                //                         = log(1/(1+exp(x)))
-                //                         = -log(1+exp(x)) = -softplus(x)
-                ret += (1-target_i) * tabulated_softplus(activation_i);
+            ret += tabulated_softplus(activation_i) - target_i * activation_i;
+            // nll -= target * pl_log(expectation); 
+            // but it is numerically unstable, so use instead the following identity:
+            // nll = - target*log(sigmoid(act)) -(1-target)*log(1-sigmoid(act))
+            //     = target*softplus(-act) +(1-target)*(act+softplus(-act))
+            //     = act + softplus(-act) - target*act 
+            //     = softplus(act) - target*act
         }
     } else {
         for( int i=0 ; i<size ; i++ )
         {
             target_i = target[i];
             activation_i = activation[i];
-            if(!fast_exact_is_equal(target_i,0.0))
-                // nll -= target[i] * pl_log(expectations[i]); 
-                // but it is numerically unstable, so use instead
-                // log (1/(1+exp(-x))) = -log(1+exp(-x)) = -softplus(-x)
-                ret += target_i * softplus(-activation_i);
-            if(!fast_exact_is_equal(target_i,1.0))
-                // ret -= (1-target_i) * pl_log(1-expectation_i);
-                // log (1 - 1/(1+exp(-x))) = log(exp(-x)/(1+exp(-x)))
-                //                         = log(1/(1+exp(x)))
-                //                         = -log(1+exp(x)) = -softplus(x)
-                ret += (1-target_i) * softplus(activation_i);
+            ret += softplus(activation_i) - target_i * activation_i;
         }
     }
     return ret;



From yoshua at mail.berlios.de  Sun Jul 22 14:13:28 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Sun, 22 Jul 2007 14:13:28 +0200
Subject: [Plearn-commits] r7817 - trunk/plearn_learners/online
Message-ID: <200707221213.l6MCDSRm017428@sheep.berlios.de>

Author: yoshua
Date: 2007-07-22 14:13:27 +0200 (Sun, 22 Jul 2007)
New Revision: 7817

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
Fixed bug due to missing check when !compute_contrastive_divergence


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-07-22 02:31:51 UTC (rev 7816)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-07-22 12:13:27 UTC (rev 7817)
@@ -1109,7 +1109,9 @@
     Mat* weights_grad = ports_gradient[getPortIndex("weights")];
     hidden_bias = ports_value[getPortIndex("hidden_bias")];
     Mat* contrastive_divergence_grad = NULL;
-    Mat* contrastive_divergence = ports_value[getPortIndex("contrastive_divergence")];
+    Mat* contrastive_divergence = NULL;
+    if (compute_contrastive_divergence)
+        contrastive_divergence = ports_value[getPortIndex("contrastive_divergence")];
     bool computed_contrastive_divergence = compute_contrastive_divergence && 
         contrastive_divergence && !contrastive_divergence->isEmpty();
 



From louradou at mail.berlios.de  Mon Jul 23 16:37:00 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Mon, 23 Jul 2007 16:37:00 +0200
Subject: [Plearn-commits] r7818 -
	trunk/python_modules/plearn/learners/modulelearners
Message-ID: <200707231437.l6NEb0Lg021915@sheep.berlios.de>

Author: louradou
Date: 2007-07-23 16:37:00 +0200 (Mon, 23 Jul 2007)
New Revision: 7818

Added:
   trunk/python_modules/plearn/learners/modulelearners/plugNetwork2SVM.py
Log:


Added: trunk/python_modules/plearn/learners/modulelearners/plugNetwork2SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/plugNetwork2SVM.py	2007-07-22 12:13:27 UTC (rev 7817)
+++ trunk/python_modules/plearn/learners/modulelearners/plugNetwork2SVM.py	2007-07-23 14:37:00 UTC (rev 7818)
@@ -0,0 +1,121 @@
+import sys, os, os.path
+from plearn.learners.modulelearners  import *
+from plearn.learners.SVM import *
+
+if __name__ == '__main__':
+
+    if len(sys.argv) <= 5:
+       print "Usage:\n\tpython "+sys.argv[0]+" learner train_data valid_data test_data ports_list\n"
+       print "Example:\n\tpython "+sys.argv[0]+" 3layer_DBN.psave trainData.dmat validData.dmat testData.dmat rbm3.hidden.state [rbm2.hidden.state ...]\n"
+       sys.exit(0)
+    
+    learner_filename       = sys.argv[1]
+    dataTrain_filename = sys.argv[2]
+    dataValid_filename = sys.argv[3]
+    dataTest_filename  = sys.argv[4]
+    ports_list    = sys.argv[5:]
+
+    if os.path.isfile(learner_filename) == False and os.path.isdir(learner_filename) == False:
+       raise EOFError, "ERROR : Learner file cannot be find\n\tCould not find file "+learner_filename
+    learner = loadModuleLearner(learner_filename)
+    learner_nickname = os.path.basename(learner_filename)+"_".join(ports_list).replace(".","")
+
+    result_dir = os.path.dirname(learner_filename)
+    output_filename = result_dir+'/SVM_results_'+"_"+learner_nickname+"-"+os.path.basename(dataTrain_filename).replace(".vmat","").replace(".amat","")
+    print
+    print "Results will be written in "+output_filename
+
+#               #
+#   MAIN PART   #
+#               #
+
+
+    new_learner = plug2output( learner, ports_list)
+
+    
+  
+    for typeDataSet in ['Train','Valid','Test']:
+        data_filename = globals()['data'+typeDataSet+'_filename']
+        if os.path.isfile(data_filename) == False and os.path.isdir(data_filename) == False:
+           raise EOFError, "Could not find "+data_filename
+           sys.exit(0)
+	dataSet = pl.AutoVMatrix( filename = data_filename )
+
+        print "CONVERSION "+data_filename
+        globals()[typeDataSet+'_outputs'], globals()[typeDataSet+'_targets'] = computeOutputsTargets( new_learner, dataSet)
+	#
+	# Normalizing the data (/!\ compute statistics on the training data and assumes it comes first)
+	#
+        if typeDataSet == 'Train':
+           mean, std = normalize(globals()[typeDataSet+'_outputs'],None,None)
+	else:
+	   normalize(globals()[typeDataSet+'_outputs'],mean,std)
+
+    my_SVM = SVM()
+    
+    print "Writing results in "+output_filename
+    if os.path.isfile(output_filename):
+       print "WARNING : output "+output_filename+" already exists"
+       FID = open(output_filename, 'a')
+       abspath = os.path.realpath(learner_filename)
+       FID.write('LEARNER.: '+abspath+'\n')
+       for i in range(3):
+           abspath = os.path.dirname(abspath)
+       global_results = abspath+'/global_stats.pmat'
+       if os.path.isfile(global_results):
+          os.system("echo   baseline test error rate : `plearn vmat cat "+global_results+" | tail -1 | awk '{print $NF}'` \%   >> "+output_filename )
+       else:
+          print "WARNING : could not find global_stats.pmat\n\t( "+abspath+"/global_stats.pmat )"
+       FID.write('Train...: '+os.path.realpath(dataTrain_filename)+'\n')
+       FID.write('Valid...: '+os.path.realpath(dataValid_filename)+'\n')
+       FID.write('Test....: '+os.path.realpath(dataTest_filename)+'\n')
+       FID.close()
+
+    # A log file where all the intermediate results will be stored
+    my_SVM.save_filename = output_filename
+    
+    # Trying the linear kernel
+    # with several values for C (i.e. bias-variance trade-off in SVM)
+    #
+    my_SVM.train_and_tune( 'LINEAR' ,     [[Train_outputs,Train_targets], [Valid_outputs,Valid_targets] ])
+    best_valid_error_rate = my_SVM.valid_error_rate
+    print
+    print "Tried parameters : "+str(my_SVM.tried_parameters)
+    print 'BEST ERROR RATE: '+str(best_valid_error_rate)+' (valid) for '+str(my_SVM.best_parameters)
+    
+    
+    # Trying the RBF (Gaussian) kernel
+    # with several values for C and 'gamma' (kernel width)
+    #
+#    my_SVM.train_and_tune( 'RBF' ,     [[Train_outputs,Train_targets], [Valid_outputs,Valid_targets] ])
+    best_valid_error_rate = my_SVM.valid_error_rate
+    print
+    print "Tried parameters : "+str(my_SVM.tried_parameters)
+    print 'BEST ERROR RATE: '+str(best_valid_error_rate)+' (valid) for '+str(my_SVM.best_parameters)
+    
+    # Trying the RBF kernel once more
+    # i.e. more precise tuning
+    # with more values for C and 'gamma' (kernel width)
+    #
+#    my_SVM.train_and_tune( 'RBF' ,     [[Train_outputs,Train_targets], [Valid_outputs,Valid_targets] ])
+    best_valid_error_rate = my_SVM.valid_error_rate
+    print
+    print "Tried parameters : "+str(my_SVM.tried_parameters)
+    print 'BEST ERROR RATE: '+str(best_valid_error_rate)+' (valid) for '+str(my_SVM.best_parameters)
+
+    # Trying the polynomial kernel
+    # with several values for C and the degree
+    #
+#    my_SVM.train_and_tune( 'POLY' ,     [[Train_outputs,Train_targets], [Valid_outputs,Valid_targets] ])
+    best_valid_error_rate = my_SVM.valid_error_rate
+    print
+    print "Tried parameters : "+str(my_SVM.tried_parameters)
+    print 'BEST ERROR RATE: '+str(best_valid_error_rate)+' (valid)  for '+str(my_SVM.best_parameters)
+
+    my_SVM.test( [Test_outputs,Test_targets] )
+
+    test_error_rate = my_SVM.error_rate
+    print "Test ERROR RATE with best model : "+str(test_error_rate)
+
+    print
+    print "Results written in "+output_filename



From tihocan at mail.berlios.de  Mon Jul 23 17:10:29 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 23 Jul 2007 17:10:29 +0200
Subject: [Plearn-commits] r7819 - trunk
Message-ID: <200707231510.l6NFATEX024368@sheep.berlios.de>

Author: tihocan
Date: 2007-07-23 17:10:28 +0200 (Mon, 23 Jul 2007)
New Revision: 7819

Modified:
   trunk/pymake.config.model
Log:
Default Python version is now automatically detected instead of being arbitrarily set to 2.3 (also doing automatic detection at the Lisa lab)

Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2007-07-23 14:37:00 UTC (rev 7818)
+++ trunk/pymake.config.model	2007-07-23 15:10:28 UTC (rev 7819)
@@ -248,15 +248,18 @@
 ### find a better way to know where to find the libraries.
 ### TODO Should find a way to automatically find python version ?
 if not 'nopython' in optionargs:
+    # First find which version of python is installed.
+    pyver = sys.version.split()[0][0:3]
+    pyoption = 'python%s' % pyver.replace('.', '')
     if domain_name.endswith('iro.umontreal.ca'):
-        optionargs += [ 'python24' ]
-        python_version = '2.4'
+        optionargs += [ pyoption ]
+        python_version = pyver
         python_lib_root = '/usr/lib'
         if platform == 'cygwin':
             numpy_includedirs = [ ]
-            numpy_site_packages = '$HOME/local-cygwin/lib/python2.4/site-packages/numarray'
+            numpy_site_packages = '$HOME/local-cygwin/lib/python%s/site-packages/numarray' % pyver
         elif platform == 'win32':
-            python_version = '24'
+            python_version = '2.4'
             python_lib_root = ''
             linkeroptions_tail += '-L/cygdrive/r/python2.4_mingw'
             numpy_site_packages = join(homedir, \
@@ -265,7 +268,7 @@
         else:
             numpy_includedirs = [ '/u/lisa/local/' + target_platform + '/include/' ]
             ### NB: The '-lutil' is necessary on i386 LISA computers.
-            numpy_site_packages = '/u/lisa/local/' + target_platform + '/lib/python2.4/site-packages/numarray -lutil'
+            numpy_site_packages = '/u/lisa/local/' + target_platform + '/lib/python%s/site-packages/numarray -lutil' % pyver
     elif domain_name.endswith('.ms'):
         numpy_includedirs = []
         numpy_site_packages = join(homedir, '../delallea/local/lib/python2.5/site-packages/numarray -lutil')
@@ -293,7 +296,7 @@
         elif 'python25' in optionargs:
             python_version = '2.5'
         else:
-            python_version = '2.3'
+            python_version = pyver
 
         python_lib_root = '/usr/lib'
         numpy_site_packages = '/usr/lib/python'+python_version+'/site-packages/numarray'
@@ -648,7 +651,7 @@
 #####  Mathematical Libraries  ##############################################
 
 lapack_linkeroptions = '-llapack'
-blas_linkeroptions   = '-lblas -lg2c'
+blas_linkeroptions   = '-lblas -lg2c '
 
 # Jasmin : In OS X, there is a framework with blas and lapack
 if platform=='darwin':



From tihocan at mail.berlios.de  Mon Jul 23 17:28:22 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 23 Jul 2007 17:28:22 +0200
Subject: [Plearn-commits] r7820 - trunk/plearn/python
Message-ID: <200707231528.l6NFSMvo025784@sheep.berlios.de>

Author: tihocan
Date: 2007-07-23 17:28:15 +0200 (Mon, 23 Jul 2007)
New Revision: 7820

Modified:
   trunk/plearn/python/PythonObjectWrapper.h
Log:
Fix for compilation with python 2.5 and gcc 4.1.2 on a 64 bits SMP computer

Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2007-07-23 15:10:28 UTC (rev 7819)
+++ trunk/plearn/python/PythonObjectWrapper.h	2007-07-23 15:28:15 UTC (rev 7820)
@@ -832,7 +832,11 @@
                                 print_traceback);
     
     PyObject *key, *value;
+#if PL_PYTHON_VERSION>=250
+    Py_ssize_t pos = 0;
+#else
     int pos = 0;
+#endif
     std::map<T,U> result;
 
     while (PyDict_Next(pyobj, &pos, &key, &value)) {



From lysiane at mail.berlios.de  Mon Jul 23 20:34:10 2007
From: lysiane at mail.berlios.de (lysiane at BerliOS)
Date: Mon, 23 Jul 2007 20:34:10 +0200
Subject: [Plearn-commits] r7821 -
	trunk/plearn_learners/distributions/EXPERIMENTAL
Message-ID: <200707231834.l6NIYAXo019036@sheep.berlios.de>

Author: lysiane
Date: 2007-07-23 20:34:10 +0200 (Mon, 23 Jul 2007)
New Revision: 7821

Modified:
   trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc
   trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h
Log:
Modification : more transformation families
new version is able to learn :- all the previous transformation families
                              - [all the previous transformation families] + bias
 


Modified: trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc
===================================================================
--- trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc	2007-07-23 15:28:15 UTC (rev 7820)
+++ trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc	2007-07-23 18:34:10 UTC (rev 7821)
@@ -72,8 +72,10 @@
     transformDistributionPeriod(UNDEFINED),
     transformDistributionOffset(UNDEFINED),
     transformDistributionAlpha(TRANSFORM_DISTRIBUTION_ALPHA_NO_REG),
-    transformsPeriod(1),
-    transformsOffset(0),
+    transformsPeriod(UNDEFINED),
+    transformsOffset(UNDEFINED),
+    biasPeriod(UNDEFINED),
+    biasOffset(UNDEFINED),
     noiseVariance(UNDEFINED),
     transformsVariance(1.0),
     nbTransforms(2),
@@ -209,13 +211,24 @@
                  "transformsPeriod",
                  &TransformationLearner::transformsPeriod,
                  OptionBase::buildoption,
-                 "time interval between two updates of the transformations parameters");
+                 "time interval between two updates of the transformations matrices");
    declareOption(ol,
                  "transformsOffset",
                  &TransformationLearner::transformsOffset,
                  OptionBase::buildoption,
-                 "time of the first update of the transformations parameters");
+                 "time of the first update of the transformations matrices");
+   declareOption(ol,
+                 "biasPeriod",
+                 &TransformationLearner::biasPeriod,
+                 OptionBase::buildoption,
+                 "time interval between 2 updates of the transformations bias (if any)");
 
+   declareOption(ol,
+                 "biasOffset",
+                 &TransformationLearner::biasOffset,
+                 OptionBase::buildoption,
+                 "time of the first update of the transformations bias (if any)");
+
    declareOption(ol, 
                  "noiseVariance",
                  &TransformationLearner::noiseVariance,
@@ -515,8 +528,12 @@
     declareMethod(rmm,
                   "MStepTransformations",
                   &TransformationLearner::MStepTransformations,
-                  (BodyDoc("maximization step with respect to transformation parameters (MAP version)")));
+                  (BodyDoc("maximization step with respect to transformation matrices (MAP version)")));
     declareMethod(rmm,
+                  "MStepBias",
+                  &TransformationLearner::MStepBias,
+                  (BodyDoc("maximization step with respect to transformation bias (MAP version)")));
+    declareMethod(rmm,
                   "MStepNoiseVariance",
                   &TransformationLearner::MStepNoiseVariance,
                   (BodyDoc("maximization step with respect to noise variance")));
@@ -797,7 +814,24 @@
 //!WARNING: the trainset ("train_set") must be given
 void TransformationLearner::trainBuild(){
     
-    
+    int nbOptimizations =1;
+    int defaultTransformsOffset =0;
+    int defaultBiasOffset ;
+    int defaultNoiseVarianceOffset ;
+    int defaultTransformDistributionOffset ;
+    if(withBias){
+        defaultBiasOffset = nbOptimizations ;
+        nbOptimizations ++;
+    }
+    if(learnNoiseVariance){
+        defaultNoiseVarianceOffset = nbOptimizations;
+        nbOptimizations ++;
+    }
+    if(learnTransformDistribution){
+        defaultTransformDistributionOffset = nbOptimizations;
+        nbOptimizations ++;
+    }
+
     transformsSD = sqrt(transformsVariance);
     
     //DIMENSION VARIABLES
@@ -831,27 +865,30 @@
     
     if(withBias){
         biasSet = Mat(nbTransforms,inputSpaceDim);
+        if(biasPeriod == UNDEFINED || biasOffset == UNDEFINED){
+            biasPeriod = nbOptimizations;
+            biasOffset = defaultBiasOffset;
+        }
     }
-    
+    else{
+        biasPeriod = UNDEFINED;
+        biasOffset = UNDEFINED;
+    }
+
     initTransformsParameters();
 
+  
    
     if(transformsPeriod == UNDEFINED || transformsOffset == UNDEFINED){
-        if(learnNoiseVariance){
-            transformsPeriod = 2;
-            transformsOffset = 1;
-        }
-        else{
-            transformsPeriod = 1;
-            transformsOffset = 0;
-        }
+        transformsPeriod = nbOptimizations;
+        transformsOffset = defaultTransformsOffset;
     }
 
     //training parameters for noise variance
     if(learnNoiseVariance){
         if(noiseVariancePeriod == UNDEFINED || noiseVarianceOffset == UNDEFINED){
-            noiseVariancePeriod = 2;
-            noiseVarianceOffset = 1;
+            noiseVariancePeriod = nbOptimizations;
+            noiseVarianceOffset = defaultNoiseVarianceOffset;
         }
         if(regOnNoiseVariance){
             if(noiseAlpha < 1)
@@ -865,6 +902,10 @@
             noiseBeta = NOISE_BETA_NO_REG;
         }
     }
+    else{
+        noiseVariancePeriod = UNDEFINED;
+        noiseVarianceOffset = UNDEFINED;
+    }
     
     //initialize the noise variance
      if(noiseVariance == UNDEFINED){
@@ -879,8 +920,8 @@
      //training parameters for transformation distribution
      if(learnTransformDistribution){
          if(transformDistributionPeriod == UNDEFINED || transformDistributionOffset == UNDEFINED){
-             transformDistributionPeriod = 1;
-             transformDistributionOffset = 0;
+             transformDistributionPeriod = nbOptimizations;
+             transformDistributionOffset = defaultTransformDistributionOffset;
          }
          if(regOnTransformDistribution){
              if(transformDistributionAlpha<=0){
@@ -891,6 +932,11 @@
              }
          }
      }
+     else{
+         transformDistributionPeriod = UNDEFINED;
+         transformDistributionOffset = UNDEFINED;
+         
+     }
 
 
     //transformDistribution
@@ -980,7 +1026,7 @@
     if(noiseBeta <= 0){
         noiseBeta = 1;
     }
-    if(noiseVariance_ == UNDEFINED){
+    if(noiseVariance_ < 0){
         initNoiseVariance();
     }
     else{
@@ -2002,6 +2048,8 @@
         MStepTransformDistribution();
     if(stage % transformsPeriod == transformsOffset)
         MStepTransformations();
+    if(stage % biasPeriod == biasOffset)
+        MStepBias();
     
 }
 
@@ -2047,7 +2095,7 @@
     transformDistribution << newDistribution ;
 }
 
-//!maximization step with respect to transformation parameters
+//!maximization step with respect to transformation matrices
 //!(MAP version)
 void TransformationLearner::MStepTransformations()
 {
@@ -2088,7 +2136,43 @@
     }  
 }
  
+//!maximization step with respect to transformation bias
+//!(MAP version)
+void TransformationLearner::MStepBias(){
+    Mat  newBiasSet;
+    newBiasSet.resize(nbTransforms,inputSpaceDim);
+    for(int i=0; i<nbTransforms; i++){
+        for(int j =0; j<inputSpaceDim; j++){
+            newBiasSet[i][j]= 0; 
+        }
+    }
+    int transformIdx;
+    real proba;
+    real w;
+    real sum = INIT_weight(0);
+    Vec reconstruction;
+    reconstruction.resize(inputSpaceDim);
+    for(int idx=0; idx<nbReconstructions ; idx++){
+        transformIdx = reconstructionSet[idx].transformIdx;
+        w = reconstructionSet[idx].weight;
+        proba = PROBA_weight(w);
+        sum = SUM_weights(sum, w);
+        seeNeighbor(reconstructionSet[idx].neighborIdx);
+        if(transformFamily == TRANSFORM_FAMILY_LINEAR){
+            transposeProduct(reconstruction,transforms[transformIdx],neighbor);
+        }
+        else{
+            transposeProduct(reconstruction,transforms[transformIdx],neighbor);
+            reconstruction += neighbor;
+        }
+        seeTarget(reconstructionSet[idx].targetIdx);
+        newBiasSet += proba*(target - reconstruction);
+    }
+    newBiasSet = newBiasSet/( noiseVariance/transformsVariance + PROBA_weight(sum) );
+    biasSet << newBiasSet;
+}
 
+
 //!maximization step with respect to noise variance
 void TransformationLearner::MStepNoiseVariance()
 {

Modified: trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h
===================================================================
--- trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h	2007-07-23 15:28:15 UTC (rev 7820)
+++ trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h	2007-07-23 18:34:10 UTC (rev 7821)
@@ -248,7 +248,7 @@
     //!(see MStep() for more details)
     int transformDistributionPeriod;
     int transformDistributionOffset;
-    
+
     //!This parameter have to be defined if the transformation distribution
     //!is learned using a MAP procedure. We suppose that this distribution have a a multinomial form
     //(u1,u2,...,uK) with dirichlet prior probability : 
@@ -258,9 +258,11 @@
     real transformDistributionAlpha;
 
 
-    //!tells us when to update the transformation parameters
+    //!tells us when to update the transformation matrices and bias
     int transformsPeriod;
     int transformsOffset;
+    int biasPeriod;
+    int biasOffset;
 
 
     //PARAMETERS OF THE DISTRIBUTION
@@ -825,10 +827,18 @@
     //!NOTE :  alpha =1 ->  no regularization
     void MStepTransformDistributionMAP(real alpha);
 
-    //!maximization step with respect to transformation parameters
+
+
+    //!maximization step with respect to transformation matrices
     //!(MAP version)
     void MStepTransformations();
     
+
+    //!maximization step with respect to transformation bias
+    //!(MAP version)
+    void MStepBias();
+    
+    
     //!maximization step with respect to noise variance
     void MStepNoiseVariance();
     



From tihocan at mail.berlios.de  Mon Jul 23 21:19:37 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 23 Jul 2007 21:19:37 +0200
Subject: [Plearn-commits] r7822 - trunk
Message-ID: <200707231919.l6NJJb3f022660@sheep.berlios.de>

Author: tihocan
Date: 2007-07-23 21:19:37 +0200 (Mon, 23 Jul 2007)
New Revision: 7822

Modified:
   trunk/pymake.config.model
Log:
Removed -lg2c in the link command with BLAS. It does not seem like this is necessary now (at least at lisa lab)

Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2007-07-23 18:34:10 UTC (rev 7821)
+++ trunk/pymake.config.model	2007-07-23 19:19:37 UTC (rev 7822)
@@ -651,7 +651,7 @@
 #####  Mathematical Libraries  ##############################################
 
 lapack_linkeroptions = '-llapack'
-blas_linkeroptions   = '-lblas -lg2c '
+blas_linkeroptions   = '-lblas'
 
 # Jasmin : In OS X, there is a framework with blas and lapack
 if platform=='darwin':



From nouiz at mail.berlios.de  Mon Jul 23 22:35:16 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 23 Jul 2007 22:35:16 +0200
Subject: [Plearn-commits] r7823 - trunk/plearn/io
Message-ID: <200707232035.l6NKZGHF028481@sheep.berlios.de>

Author: nouiz
Date: 2007-07-23 22:35:16 +0200 (Mon, 23 Jul 2007)
New Revision: 7823

Modified:
   trunk/plearn/io/PStream.cc
Log:
Emit an error if we expect a number but their isn't


Modified: trunk/plearn/io/PStream.cc
===================================================================
--- trunk/plearn/io/PStream.cc	2007-07-23 19:19:37 UTC (rev 7822)
+++ trunk/plearn/io/PStream.cc	2007-07-23 20:35:16 UTC (rev 7823)
@@ -794,6 +794,8 @@
         sscanf(tmpbuf,"%lf",&x);
         break;
     }
+    if(l==0)
+        PLERROR("In PStream::readAsciiNum - we read (%c) while we expected a digit",c);
 }
 
 PStream& PStream::operator=(const PStream& pios)



From yoshua at mail.berlios.de  Tue Jul 24 05:27:10 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Tue, 24 Jul 2007 05:27:10 +0200
Subject: [Plearn-commits] r7824 - in trunk/python_modules/plearn/learners:
	autolr online
Message-ID: <200707240327.l6O3RAKx003974@sheep.berlios.de>

Author: yoshua
Date: 2007-07-24 05:27:09 +0200 (Tue, 24 Jul 2007)
New Revision: 7824

Modified:
   trunk/python_modules/plearn/learners/autolr/__init__.py
   trunk/python_modules/plearn/learners/online/__init__.py
Log:
Created the testlearner function in autolr, to allow
redefining it in user's code. 
Added a use_Gaussian_inputs in online's rbm function.


Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-07-23 20:35:16 UTC (rev 7823)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-07-24 03:27:09 UTC (rev 7824)
@@ -1,3 +1,4 @@
+
 from math import *
 from numarray import *
 from plearn.bridge import *
@@ -22,7 +23,10 @@
         # we will only send our job to the server if nothing is already running
         lock.acquire()
         for method, args in tasks: # do each task sequentially
-            getattr(object, method)(*args)
+            if hasattr(object, method):
+                getattr(object, method)(*args)
+            else: # assume it is a function taking the args as arguments, not a method
+                method(*args)
         lock.release()
 
     if plearn.bridgemode.useserver and use_threads:
@@ -30,7 +34,10 @@
         return t
     else:
         for method, args in tasks:
-            getattr(object, method)(*args)
+            if hasattr(object, method):
+                getattr(object, method)(*args)
+            else: # assume it is a function taking the object as first argument, not a method
+                method(*args)
         return True
 
 def acquire_server(use_threads = False):
@@ -86,6 +93,16 @@
         lock.release()
     return o
 
+def testlearner(learner,dataset,costs=[],ts=None):
+    if not ts:
+        ts = pl.VecStatsCollector()
+        if plearn.bridgemode.useserver:
+            ts=learner.server.new(ts)
+    learner.test(dataset,ts,0,0)
+    del costs[:]
+    for k in range(len(learner.getTestCostNames())):
+        costs.append(ts.getStat("E["+str(k)+"]"))
+    return costs
 
 def merge_schedules(schedules):
     """Merge several learning rate schedules into a kind of multi-schedule
@@ -210,15 +227,13 @@
 
         # Report error on test sets
         for j in range(n_tests):
-            ts = pl.VecStatsCollector()
-            if plearn.bridgemode.useserver:
-                ts=serv.new(ts)
-            learner.test(testsets[j],ts,0,0)
+            costs = testlearner(learner,testsets[j])
             if logfile:
                 print >>logfile, "At stage ", learner.stage, " test" + str(j+1),": ",
-            for k, costname in zip(range(n_test_costs), test_costnames):
-                err = ts.getStat("E["+costname+"]")
+            for k in range(0,n_test_costs):
+                err = costs[k]
                 results[i, 1+n_schedules+n_train_costs+(j*n_test_costs)+k] = err
+                costname = costnames[cost_indices[k]]
                 if logfile:
                     print >>logfile, costname, "=", err,
                 if k==cost_to_select_best and j==0 and err < best_err:
@@ -283,11 +298,7 @@
         learner.changeOptions(options)
         learner.nstages = int(nstages+initial_stage)
         learner.train()
-        ts = pl.VecStatsCollector()
-        if plearn.bridgemode.useserver:
-            ts=serv.new(ts)
-        learner.test(testset,ts,0,0)
-        err=ts.getStat("E["+str(cost_to_select_best)+"]")
+        err=testlearner(learner,testset)[cost_to_select_best]
         if logfile:
             print >>logfile, "*trying* initial learning rate ",lr(i), \
                   " and obtained err=",err," on cost ",cost_to_select_best
@@ -484,12 +495,8 @@
             tasks = [('train', ())]
             stats = []
             for j in range(0,n_tests):
-                ts = pl.VecStatsCollector()
-                if plearn.bridgemode.useserver:
-                    # no threads are running so we don't need to lock here
-                    ts = candidate.server.new(ts)
-                stats.append(ts)
-                tasks.append(('test', (testsets[j], ts, 0, 0)))
+                stats.append([])
+                tasks.append(('testlearner', (candidate,testsets[j],stats[j])))
             active_stats.append(stats)
             threads.append(execute(candidate, tasks, use_threads))
 
@@ -513,7 +520,6 @@
             results[t,1] = all_lr[active]
             if logfile:
                 print >>logfile, "candidate ",active,":",
-
             # Report approximate training statistics
             if logfile:
                 print >>logfile, 'train :',
@@ -524,11 +530,12 @@
                     print >>logfile, costname, '=', err,
 
             # Report testing statistics
-            for j, ts in zip(range(0,n_tests), stats):
+            for j, costs in zip(range(0,n_tests), stats):
                 if logfile:
                     print >>logfile, " test" + str(j+1),": ",
-                for k, costname in zip(range(n_test_costs), test_costnames):
-                    err = ts.getStat("E["+costname+"]")
+                for k in range(0,n_test_costs):
+                    err = costs[k]
+                    costname = costnames[cost_indices[k]]
                     results[t, 2+n_train_costs+(j*n_test_costs)+k] = err
                     if logfile:
                         print >>logfile, costname, "=", err,

Modified: trunk/python_modules/plearn/learners/online/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/online/__init__.py	2007-07-23 20:35:16 UTC (rev 7823)
+++ trunk/python_modules/plearn/learners/online/__init__.py	2007-07-24 03:27:09 UTC (rev 7824)
@@ -65,7 +65,8 @@
         compute_nll=False,
         ngcd=1,
         have_reconstruction=True,
-        compute_contrastive_divergence=True):
+        compute_contrastive_divergence=True,
+        use_Gaussian_inputs=False):
     """Construct an RBMModule"""
     # Return a standard binomial RBM.
     conx = pl.RBMMatrixConnection(
@@ -78,7 +79,9 @@
             compute_contrastive_divergence = compute_contrastive_divergence,
             compute_log_likelihood = compute_nll,
             n_Gibbs_steps_CD = ngcd,
-            visible_layer = pl.RBMBinomialLayer(size = visible_size),
+            visible_layer = ifthenelse(use_Gaussian_inputs,
+                                       pl.RBMGaussianLayer(size = visible_size),
+                                       pl.RBMBinomialLayer(size = visible_size)),
             hidden_layer = pl.RBMBinomialLayer(size = hidden_size),
             connection = conx,
             reconstruction_connection = ifthenelse(have_reconstruction,



From yoshua at mail.berlios.de  Tue Jul 24 13:30:55 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Tue, 24 Jul 2007 13:30:55 +0200
Subject: [Plearn-commits] r7825 - trunk/python_modules/plearn/learners/autolr
Message-ID: <200707241130.l6OBUt8P023625@sheep.berlios.de>

Author: yoshua
Date: 2007-07-24 13:30:54 +0200 (Tue, 24 Jul 2007)
New Revision: 7825

Modified:
   trunk/python_modules/plearn/learners/autolr/__init__.py
Log:
Corrected bugs in autolr (some mine, some older!)


Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-07-24 03:27:09 UTC (rev 7824)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-07-24 11:30:54 UTC (rev 7825)
@@ -26,7 +26,7 @@
             if hasattr(object, method):
                 getattr(object, method)(*args)
             else: # assume it is a function taking the args as arguments, not a method
-                method(*args)
+                eval(method)(*args)
         lock.release()
 
     if plearn.bridgemode.useserver and use_threads:
@@ -37,7 +37,7 @@
             if hasattr(object, method):
                 getattr(object, method)(*args)
             else: # assume it is a function taking the object as first argument, not a method
-                method(*args)
+                eval(method)(*args)
         return True
 
 def acquire_server(use_threads = False):
@@ -233,7 +233,7 @@
             for k in range(0,n_test_costs):
                 err = costs[k]
                 results[i, 1+n_schedules+n_train_costs+(j*n_test_costs)+k] = err
-                costname = costnames[cost_indices[k]]
+                costname = test_costnames[k]
                 if logfile:
                     print >>logfile, costname, "=", err,
                 if k==cost_to_select_best and j==0 and err < best_err:
@@ -425,7 +425,6 @@
         test_costnames = [ name for name in test_costnames
                            if name in selected_costnames ]
 
-    #cost_indices = [costnames.index(name) for name in selected_costnames]
     n_tests = len(testsets)
     #n_costs = len(cost_indices)
     n_train_costs = len(train_costnames)
@@ -523,6 +522,7 @@
             # Report approximate training statistics
             if logfile:
                 print >>logfile, 'train :',
+            ts=candidate.getTrainStatsCollector()
             for k, costname in zip(range(n_train_costs), train_costnames):
                 err = ts.getStat('E['+costname+']')
                 results[t, 2+k] = err
@@ -535,7 +535,7 @@
                     print >>logfile, " test" + str(j+1),": ",
                 for k in range(0,n_test_costs):
                     err = costs[k]
-                    costname = costnames[cost_indices[k]]
+                    costname = test_costnames[k]
                     results[t, 2+n_train_costs+(j*n_test_costs)+k] = err
                     if logfile:
                         print >>logfile, costname, "=", err,



From yoshua at mail.berlios.de  Tue Jul 24 15:01:34 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Tue, 24 Jul 2007 15:01:34 +0200
Subject: [Plearn-commits] r7826 - trunk/plearn_learners/online
Message-ID: <200707241301.l6OD1YOm008837@sheep.berlios.de>

Author: yoshua
Date: 2007-07-24 15:01:34 +0200 (Tue, 24 Jul 2007)
New Revision: 7826

Modified:
   trunk/plearn_learners/online/OnlineLearningModule.cc
   trunk/plearn_learners/online/RBMModule.cc
Log:
added a new port visible_activation, and better error messages


Modified: trunk/plearn_learners/online/OnlineLearningModule.cc
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.cc	2007-07-24 11:30:54 UTC (rev 7825)
+++ trunk/plearn_learners/online/OnlineLearningModule.cc	2007-07-24 13:01:34 UTC (rev 7826)
@@ -381,9 +381,17 @@
     TVec<Mat*> ports_value(nPorts());
     map<string,Mat>::iterator it=inputs.begin();
     for (;it!=inputs.end();++it)
-        ports_value[getPortIndex(it->first)]= &it->second;
+    {
+        int port_index=getPortIndex(it->first);
+        PLASSERT_MSG(port_index>=0,"Unknown port name: "+it->first);
+        ports_value[port_index]= &it->second;
+    }
     for (int i=0;i<wanted_outputs.length();i++)
-        ports_value[getPortIndex(wanted_outputs[i])]= new Mat(0,0);
+    {
+        int port_index=getPortIndex(wanted_outputs[i]);
+        PLASSERT_MSG(port_index>=0,"Unknown port name: "+wanted_outputs[i]);
+        ports_value[port_index]= new Mat(0,0);
+    }
     fprop(ports_value);
     for (it=inputs.begin();it!=inputs.end();++it)
         outputs[it->first]=it->second;

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-07-24 11:30:54 UTC (rev 7825)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-07-24 13:01:34 UTC (rev 7826)
@@ -58,7 +58,8 @@
     "  - 'hidden.state' : expectations of the hidden (normally output) layer\n"
     "  - 'hidden_activations.state' : activations of hidden units (given visible)\n"
     "  - 'visible_sample' : random sample obtained on visible units (input or output port)\n"
-    "  - 'visible_expectation' : expectation of visible units (output port ONLY, for sampling)\n"
+    "  - 'visible_expectation' : expectation of visible units (output port ONLY)\n"
+    "  - 'visible_activation' : ectation of visible units (output port ONLY)\n"
     "  - 'hidden_sample' : random sample obtained on hidden units\n"
     "  - 'energy' : energy of the joint (visible,hidden) pair or free-energy\n"
     "               of the visible (if given) or of the hidden (if given).\n"
@@ -281,6 +282,7 @@
     addPortName("hidden_activations.state");
     addPortName("visible_sample");
     addPortName("visible_expectation");
+    addPortName("visible_activation");
     addPortName("hidden_sample");
     addPortName("energy");
     addPortName("hidden_bias");
@@ -306,6 +308,7 @@
         port_sizes(getPortIndex("visible"), 1) = visible_layer->size;
         port_sizes(getPortIndex("visible_sample"), 1) = visible_layer->size;
         port_sizes(getPortIndex("visible_expectation"), 1) = visible_layer->size;
+        port_sizes(getPortIndex("visible_activation"), 1) = visible_layer->size;
     }
     if (hidden_layer) {
         port_sizes(getPortIndex("hidden.state"), 1) = hidden_layer->size;
@@ -667,6 +670,7 @@
     hidden_act = ports_value[getPortIndex("hidden_activations.state")];
     Mat* visible_sample = ports_value[getPortIndex("visible_sample")];
     Mat* visible_expectation = ports_value[getPortIndex("visible_expectation")];
+    Mat* visible_activation = ports_value[getPortIndex("visible_activation")];
     Mat* hidden_sample = ports_value[getPortIndex("hidden_sample")];
     Mat* energy = ports_value[getPortIndex("energy")];
     Mat* neg_log_likelihood = ports_value[getPortIndex("neg_log_likelihood")];
@@ -747,20 +751,20 @@
     {
         if (partition_function_is_stale && !during_training)
         {
-	    computePartitionFunction();
+            computePartitionFunction();
             partition_function_is_stale=false;
         }
         if (visible && !visible->isEmpty()
             && hidden && !hidden->isEmpty())
         {
-            // neg-log-likelihood(visible,hidden) = energy(visible,visible) + log(partition_function)
+            // neg-log-likelihood(visible,hidden) = energy(visible,hidden) + log(partition_function)
             computeEnergy(*visible,*hidden,*neg_log_likelihood);
             *neg_log_likelihood += log_partition_function;
         }
         else if (visible && !visible->isEmpty())
         {
             // neg-log-likelihood(visible) = free_energy(visible) + log(partition_function)
-            computeFreeEnergyOfVisible(*visible,*neg_log_likelihood);
+            computeFreeEnergyOfVisible(*visible,*neg_log_likelihood,hidden_act);
             *neg_log_likelihood += log_partition_function;
         }
         else if (hidden && !hidden->isEmpty())
@@ -770,6 +774,7 @@
             *neg_log_likelihood += log_partition_function;
         }
         else PLERROR("RBMModule: neg_log_likelihood currently computable only of the visible as inputs");
+        found_a_valid_configuration = true;
     }
 
     // REGULAR FPROP
@@ -795,6 +800,31 @@
         found_a_valid_configuration = true;
     }
 
+    // DOWNWARD FPROP
+    // we are given hidden  and we want to compute the visible or visible_activation
+    if ( hidden && !hidden->isEmpty() && 
+         ((visible && visible->isEmpty()) ||
+          (visible_activation && visible_activation->isEmpty())) )
+    {
+        computeVisibleActivations(*hidden,true);
+        if (visible_activation)
+        {
+            PLASSERT_MSG(visible_activation->isEmpty(),"visible_activation should be an output");
+            visible_activation->resize(visible_layer->activations.length(),
+                                       visible_layer->size);
+            *visible_activation << visible_layer->activations;
+        }
+        if (visible)
+        {
+            PLASSERT_MSG(visible->isEmpty(),"visible should be an output");
+            visible_layer->computeExpectations();
+            const Mat expectations=visible_layer->getExpectations();
+            visible->resize(expectations.length(),visible_layer->size);
+            *visible_activation << visible_layer->activations;
+        }
+        found_a_valid_configuration = true;
+    }
+
     // COMPUTE AUTOASSOCIATOR RECONSTRUCTION ERROR
     if ( visible && !visible->isEmpty() &&
          ( ( visible_reconstruction && visible_reconstruction->isEmpty() ) ||



From nouiz at mail.berlios.de  Tue Jul 24 16:58:06 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 24 Jul 2007 16:58:06 +0200
Subject: [Plearn-commits] r7827 - trunk/plearn/io
Message-ID: <200707241458.l6OEw6VC018941@sheep.berlios.de>

Author: nouiz
Date: 2007-07-24 16:58:06 +0200 (Tue, 24 Jul 2007)
New Revision: 7827

Modified:
   trunk/plearn/io/PStream.cc
Log:
Corrected my bugfix so that all the test run fine.


Modified: trunk/plearn/io/PStream.cc
===================================================================
--- trunk/plearn/io/PStream.cc	2007-07-24 13:01:34 UTC (rev 7826)
+++ trunk/plearn/io/PStream.cc	2007-07-24 14:58:06 UTC (rev 7827)
@@ -793,9 +793,9 @@
         unget();
         sscanf(tmpbuf,"%lf",&x);
         break;
+        if(l==0)
+            PLERROR("In PStream::readAsciiNum - we read (%c) while we expected a digit",c);
     }
-    if(l==0)
-        PLERROR("In PStream::readAsciiNum - we read (%c) while we expected a digit",c);
 }
 
 PStream& PStream::operator=(const PStream& pios)



From nouiz at mail.berlios.de  Tue Jul 24 20:00:30 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 24 Jul 2007 20:00:30 +0200
Subject: [Plearn-commits] r7828 - trunk/python_modules/plearn/pymake
Message-ID: <200707241800.l6OI0Ue0015054@sheep.berlios.de>

Author: nouiz
Date: 2007-07-24 20:00:30 +0200 (Tue, 24 Jul 2007)
New Revision: 7828

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
some bug fix


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2007-07-24 14:58:06 UTC (rev 7827)
+++ trunk/python_modules/plearn/pymake/pymake.py	2007-07-24 18:00:30 UTC (rev 7828)
@@ -1643,7 +1643,7 @@
             elif self.hasmain:
                 self.corresponding_output = join(self.filedir, objsdir, self.filebase)
                 # We append options to the file name if they are not appended to the objsdir name
-                for opt in getOptions(options_choices,optionargs):
+                for opt in options:
                     pyopt = pymake_options_defs[opt]
                     if not pyopt.in_output_dirname:
                         self.corresponding_output = self.corresponding_output + '_' + opt
@@ -2627,7 +2627,7 @@
                 optionargs.append('tmp')
 
     # I do multiple for on optionarfs, as their is a bug that make that not all
-    # elements of optionsargs are computed
+    # elements of optionargs are computed
     for option in optionargs:
         if option[0] == 'v':
             remove_verbosity_option = True
@@ -2679,6 +2679,7 @@
     if 'dependency' in optionargs:
         if 1 <= len(otherargs) <= 2:
             optionargs.remove('dependency')
+            options = getOptions(options_choices,optionargs)
             find_dependency(otherargs)
             sys.exit()
         else:



From nouiz at mail.berlios.de  Wed Jul 25 18:42:05 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 25 Jul 2007 18:42:05 +0200
Subject: [Plearn-commits] r7829 - trunk/scripts
Message-ID: <200707251642.l6PGg5AX004728@sheep.berlios.de>

Author: nouiz
Date: 2007-07-25 18:42:03 +0200 (Wed, 25 Jul 2007)
New Revision: 7829

Modified:
   trunk/scripts/cdispatch
Log:
Changed cdispatch from perl to python


Modified: trunk/scripts/cdispatch
===================================================================
--- trunk/scripts/cdispatch	2007-07-24 18:00:30 UTC (rev 7828)
+++ trunk/scripts/cdispatch	2007-07-25 16:42:03 UTC (rev 7829)
@@ -1,55 +1,23 @@
-#!/usr/bin/perl -w
+#!/usr/bin/env python
+import sys,os,re
 
-## This file is a modification made to the file apdispatch by Frederic Bastien
-## To make it use dbi.py to launch many experience in one command line
-## condordispatch
+ScriptName="launchdbi.py"
+ShortHelp='Usage: cdispatch [--help|-h] [--log|--nolog] [--cluster|--local] [--test] [--req="CONDOR_REQUIREMENT"] [--file=FILEPATH | <command-template>]'
+LongHelp="""
+Dispatches jobs with dbi.py. dbi allow to dispatch jobs on condor, cluster, local, ssh and bqtools.
 
-## Copyright (C) 2004 ApSTAT Technologies Inc. 
-##
-## Redistribution and use in source and binary forms, with or without
-## modification, are permitted provided that the following conditions are met:
-## 
-##  1. Redistributions of source code must retain the above copyright
-##     notice, this list of conditions and the following disclaimer.
-## 
-##  2. Redistributions in binary form must reproduce the above copyright
-##     notice, this list of conditions and the following disclaimer in the
-##     documentation and/or other materials provided with the distribution.
-## 
-##  3. The name of the authors may not be used to endorse or promote
-##     products derived from this software without specific prior written
-##     permission.
-## 
-## THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-## IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-## OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-## NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-## SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-## TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-## PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-## LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-## NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-## SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-## 
-## This file is part of the PLearn library. For more information on the PLearn
-## library, go to the PLearn Web site at www.plearn.org
+%s
 
-$ScriptName="lauchdbi.py";
-$ShortHelp='Usage: cdispatch [--help|-h] [--log|--nolog] [--cluster|--local] [--test] [--file=FILEPATH] [--req="CONDOR_REQUIREMENT"]<command-template>'."\n";
-$LongHelp=<<EOUSAGE;
-Usage: cdispatch [--help|-h] [--log|--nolog] [--cluster|--local] [--test] [--file=FILEPATH] [--req="CONDOR_REQUIREMENT"]<command-template>
-Dispatches jobs on Condor with dbi.py.  
-
 where <command-template> is interpreted as follows: the first argument
 is the <command> above, and the rest are interpreted as <arguments>.
 The arguments may contain segments of the form {{a,b,c,d}}, which trigger
-parallel dispatch: a separate "cluster --execute" command is issued for
+parallel dispatch: a separate 'cluster --execute' command is issued for
 the rest of the command template, the first time with value a, the second
 time with value b, etc.  For example, the command (NOTE: THERE MUST NOT
-BE ANY SPACES WITHIN THE "numhidden={{5,10,25}}" part and the quotes are
+BE ANY SPACES WITHIN THE 'numhidden={{5,10,25}}' part and the quotes are
 important to avoid shell misinterpretation) :
 
-  dbidispatch aplearn myscript.plearn "numhidden={{5,10,25}}"
+  dbidispatch aplearn myscript.plearn 'numhidden={{5,10,25}}'
 
 is equivalent to launching three jobs in parallel on the cluster:
 
@@ -60,7 +28,7 @@
 If several arguments contain {{ }} forms, all combinations of arguments
 are taken, and the jobs are all launched in parallel.  For instance
 
-  dbidispatch aplearn myscript.plearn "numhidden={{10,25}}" "wd={{0.01,0.001}}"
+  dbidispatch aplearn myscript.plearn 'numhidden={{10,25}}' 'wd={{0.01,0.001}}'
 
 is equivalent to:
 
@@ -71,184 +39,155 @@
 
 The optional parameter '--test' make that cdispatch generate the file $ScriptName, but do not execute it. That way you can see what cdispatch generate.
 
-The optional parameter '--req="CONDOR_REQUIREMENT"' make that cdispatch send additional option to DBI that will be used to generate addtional requirement for condor. CONDOR_REQUIREMENT must follow the syntax of requirement for condor with one exception. The symbol '"' must be escaped 3 times! So the requirement (Machine == "computer.example.com") must be writed like that:
+The optional parameter '--req=\"CONDOR_REQUIREMENT\"' make that cdispatch send additional option to DBI that will be used to generate addtional requirement for condor. CONDOR_REQUIREMENT must follow the syntax of requirement for condor with one exception. The symbol '\"' must be escaped 3 times! So the requirement (Machine == \"computer.example.com\") must be writed like that:
 
-cdispatch "--req=Machine==\\\\\\"computer.example.com\\\\\\""
+cdispatch \"--req=Machine==\\\\\\\"computer.example.com\\\\\\\"\"
 or
-cdispatch '--req=Machine==\\"computer.example.com\\"'
+cdispatch '--req=Machine==\\\"computer.example.com\\\"' 
 
-It the the optinal parameter '--file=FILEPATH' is set, the script will take commands from the file instead of taking one as parameter. This file must have one command to execute by line and each of them will be expended.
+If the optinal parameter '--file=FILEPATH' is set, the script will take commands from the file instead of taking one as parameter. This file must have one command to execute by line and each of them will be expended.
 
 cdispatch --test --file=tests
 
 In the file, there must not be double quotes around the {{}} as they are for the shell and if the command is in the file, they are not interpreted by the shell.
-EOUSAGE
-    ;
+"""%ShortHelp
 
-if (scalar(@ARGV) == 0) {
-    die $ShortHelp;
-}
+if len(sys.argv) == 1:
+    print ShortHelp
+    sys.exit(1)
+optionargs = []
+otherargs = []
+FILE = ""
+REQ = ""
+for argv in sys.argv[1:]:
 
-if($ARGV[0] eq "--help" || $ARGV[0] eq "-h"){
-    print $LongHelp;
-    exit;
-}
+    if argv == "--help" or argv == "-h":
+        print LongHelp
+        sys.exit(1)
+    elif argv == "--nolog":
+        optionargs.append(argv[2:])
+    elif argv == "--log":
+        optionargs.append(argv[2:])
+    elif argv == "--cluster" and not "local" in optionargs:
+        optionargs.append(argv[2:])
+    elif argv == "--local" and not "cluster" in optionargs:
+        optionargs.append(argv[2:])
+    elif argv == "--test":
+        optionargs.append(argv[2:])
+    elif argv[0:7] == "--file=":
+        FILE = argv[7:]
+        optionargs.append(argv[2:])
+    elif argv[0:6] == "--req=":
+        REQ = argv[6:]
+    elif argv[0:1] == '-':
+	print "Unknow parameter (%s)",argv
+	print ShortHelp
+        sys.exit(1)
+    else:
+        otherargs.append(argv)
+        
+if len(otherargs) == 0 and FILE == "":
+    print ShortHelp
+    sys.exit(1)
 
-if($ARGV[0] eq "--nolog" ){
-    $LOG = 0;
-    shift;
-} elsif($ARGV[0] eq "--log" ) {
-    $LOG = 1;
-    shift;
-} else {
-    $LOG = 0;
-}
 
-if ($ARGV[0] eq "--cluster") {
-    $CLUSTER = 1;
-    $LOCAL = 0;
-    shift;
-} elsif ($ARGV[0] eq "--local") {
-    $CLUSTER = 0;
-    $LOCAL = 1;
-    shift;
-} else {
-    $CLUSTER = 0;
-    $LOCAL = 0;
-}
+if "local" in optionargs and "cluster" in optionargs:
+    print "--cluster and --local can't be used together"
+    sys.exit(1)
 
-if ($ARGV[0] eq "--test") {
-    $TEST = 1;
-    shift;
-}else {
-    $TEST = 0;
-}
 
-if (substr($ARGV[0],0,7) eq "--file=") {
-    $FILE = substr($ARGV[0],7);
-    shift;
-}else {
-    $FILE = "";
-}
+SCRIPT=open(ScriptName,'w');
+SCRIPT.write(
+"""#! /usr/bin/env python
+#%s
+from plearn.parallel.dbi import DBI
+jobs = DBI([
+"""% " ".join(sys.argv))
+def generate_combination(repl):
+    if repl == []:
+        return []
+    else:
+        res = []
+        x = repl[0]
+        res1 = generate_combination(repl[1:])
+        for y in x:
+            if res1 == []:
+                res.append(y)
+            else:
+                for r in res1:
+                    res.append(y+" "+r)
+        return res
 
-if (scalar(@ARGV) == 0) {
-    die $ShortHelp;
-}
+def expendAndPrintArgs(sp):
+### Find replacement lists in the arguments
+    repl = []
+    for arg in sp:
+        p = re.compile('\{\{\S*\}\}')
+        reg = p.search(arg)
+        if reg:
+#            print "reg:",reg.group()[2:-2]
+            curargs = reg.group()[2:-2].split(",")# if arg =~ /{{(.*)}}/
+#            print "curargs:",curargs
+            newcurargs = []
+            for curarg in curargs:
+                new = p.sub(curarg,arg)
+#                print "new:",new
+                newcurargs.append(new)
+            repl.append(newcurargs)
+        else:
+            repl.append([arg])
+#    print "repl: ",repl
+    argscombination = generate_combination(repl)
+    for arg in argscombination:
+        cmdstr = "".join(arg);
+	SCRIPT.write("   '%s',\n"%cmdstr)
+    return len(argscombination)
 
-if(scalar(@ARGV) != 0) {
-    if (substr($ARGV[0],0,6) eq "--req=") {
-	$REQ = substr($ARGV[0],6);
-	shift;
-    } elsif (substr($ARGV[0],0,1) eq '-'){
-	print "Unknow parameter ($ARGV[0]) or wrong parameter order\n";
-	die $ShortHelp
-    } else {
-	$REQ = "";
-    }
-} else {
-    $REQ = "";
-}
-#$command_name = shift;
+nbcommand=0
+#print the command
+if FILE != "":
+    FD = open(FILE,'r')#|| die "couldn't open the file $FILE!";
+    for line in FD.readlines():
+        line = line.rstrip()
+	sp = line.split(" ")
+	nbcommand+=expendAndPrintArgs(sp)
+    FD.close
+else:
+    nbcommand=expendAndPrintArgs(otherargs)
 
-#print $command_name, "\n";
-#for ($i=0; $i < scalar(@ARGV); ++$i) {
-#    print "Root = ", $ARGV[$i],
-#    "\tArglist = (", join(",",@{$repl[$i]}),")\n";
-#}
-open(SCRIPT,">$ScriptName");
-print SCRIPT "#! /usr/bin/env python\n".
-    "from plearn.parallel.dbi import DBI\n".
-    "jobs = DBI([\n";
 
-$nbcommand=0;
+if "cluster" in optionargs:
+    SCRIPT.write("   ],'Cluster'")
+elif "local" in optionargs:
+    SCRIPT.write("   ],'Local'")
+else:
+    SCRIPT.write("   ],'Condor'")
 
-#print the command
-if ($FILE ne "") {
-    open (FD, $FILE)|| die "couldn't open the file $FILE!";
-    while ($record = <FD>) {
-	my @rec = split(" ", $record, 1024);
-	$nbcommand+=printAndExpendArgs(@rec);
-	}
-    
-    close(FD);
-} else {
-    $nbcommand=printAndExpendArgs(@ARGV)
-}
 
-if($CLUSTER){
-    print SCRIPT "   ],'Cluster'";
-}elsif($LOCAL){
-    print SCRIPT "   ],'Local'";
-}else{
-    print SCRIPT "   ],'Condor'";
-}
+if REQ != "":
+    SCRIPT.write(", requirements=\"$REQ\"")
 
-if ($REQ ne "") {
-    print SCRIPT ", requirements=\"$REQ\"";
-}
+if "test" in optionargs:
+    SCRIPT.write(", test=True")
 
-if ($TEST) {
-    print SCRIPT ", test=True";
-}
+if "log" in optionargs:
+    SCRIPT.write(", dolog=True")
+else:
+    SCRIPT.write(", dolog=False")
 
-if($LOG) {
-    print SCRIPT ", dolog=True";
-} else {
-    print SCRIPT ", dolog=False";
-}
-print SCRIPT ")\n".
-    "jobs.run() \n".
-    "# There is $nbcommand command in the script\n";
+SCRIPT.write( """)
+jobs.run()
+# There is %d command in the script"""%(nbcommand))
 
-close(SCRIPT);
-system("chmod +x $ScriptName");
+SCRIPT.close()
+os.system("chmod +x %s"%(ScriptName));
 
-print "We generated $nbcommand command in the file $ScriptName\n";
+print "We generated %s command in the file"% nbcommand
 
-if ($TEST){
-    print "The script $ScriptName was not launched\n";
-} else {
-    print "Launching the script $ScriptName\n";
-    system("./$ScriptName");
-}
+if "test" in optionargs:
+    print "The script %s was not launched"% ScriptName
+else:
+    print "Launching the script %s"% ScriptName
+    os.system("./%s"%(ScriptName))
 
-sub printAndExpendArgs
-{
-### Find replacement lists in the arguments
-    my @repl = ();
-    foreach my $arg (@_) {
-	$arg =~ /{{(.*)}}/;
-	my @curargs = ("");
-	@curargs = split(",",$1) if $arg =~ /{{(.*)}}/;
-	push @repl, \@curargs;
-    }
 
-    my @idx = (0) x @_;
-    my $nbcommand=0;
-    while ($idx[$#idx] < scalar(@{$repl[$#idx]})) {
-	### Compute current argument list
-	my @curargs;
-	for (my $i=0; $i < scalar(@idx); ++$i) {
-	    my $arg = $_[$i];
-	    if ($arg =~ /{{.*}}/) {
-		my $value = ($repl[$i])->[$idx[$i]];
-		$arg =~ s/{{.*}}/$value/;
-	    }
-	    push @curargs, $arg;
-	}
-	### Two passes are necessary to properly increment
-	for (my $i=0; $i < scalar(@idx); ++$i) {
-	    $idx[$i]++;
-	    last if $idx[$i] < scalar(@{$repl[$i]}) || $i == $#idx;
-	    $idx[$i] = 0;
-	}
-	
-	
-	###There are two distinct steps: command creation and command execution
-	### Command creation depending on cluster or direct launch:
-	$cmdstr = "" . join(" ", at curargs);
-	print SCRIPT "'$cmdstr',\n";
-	$nbcommand+=1;
-    }
-    return $nbcommand
-}



From chapados at mail.berlios.de  Wed Jul 25 21:43:57 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Wed, 25 Jul 2007 21:43:57 +0200
Subject: [Plearn-commits] r7830 - trunk
Message-ID: <200707251943.l6PJhvAA011569@sheep.berlios.de>

Author: chapados
Date: 2007-07-25 21:43:56 +0200 (Wed, 25 Jul 2007)
New Revision: 7830

Modified:
   trunk/pymake.config.model
Log:
Added support for explicit python 2.3

Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2007-07-25 16:42:03 UTC (rev 7829)
+++ trunk/pymake.config.model	2007-07-25 19:43:56 UTC (rev 7830)
@@ -291,7 +291,9 @@
         numpy_site_packages = '/sw/lib/python'+python_version+'/site-packages/numarray' 
         numpy_includedirs   = [ '/sw/include/python'+python_version]
     else:
-        if 'python24' in optionargs:
+        if 'python23' in optionargs:
+            python_version = '2.3'
+        elif 'python24' in optionargs:
             python_version = '2.4'
         elif 'python25' in optionargs:
             python_version = '2.5'



From tihocan at mail.berlios.de  Thu Jul 26 17:07:16 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 26 Jul 2007 17:07:16 +0200
Subject: [Plearn-commits] r7831 - trunk/scripts
Message-ID: <200707261507.l6QF7GJt022831@sheep.berlios.de>

Author: tihocan
Date: 2007-07-26 17:07:15 +0200 (Thu, 26 Jul 2007)
New Revision: 7831

Modified:
   trunk/scripts/make_plearn_python_ext
Log:
Let the user or config define the Python version being used

Modified: trunk/scripts/make_plearn_python_ext
===================================================================
--- trunk/scripts/make_plearn_python_ext	2007-07-25 19:43:56 UTC (rev 7830)
+++ trunk/scripts/make_plearn_python_ext	2007-07-26 15:07:15 UTC (rev 7831)
@@ -32,6 +32,6 @@
 #  library, go to the PLearn Web site at www.plearn.org
 
 pushd $PLEARNDIR/python_modules/plearn/pyext
-pymake -python24 -so $* plext.cc
+pymake -so $* plext.cc
 ln -fs libplext.so plext.so
 popd



From tihocan at mail.berlios.de  Thu Jul 26 20:46:48 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 26 Jul 2007 20:46:48 +0200
Subject: [Plearn-commits] r7832 - in
	trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results:
	. expdir expdir/Split0 expdir/Split0/LearnerExpdir
Message-ID: <200707261846.l6QIkmbe017230@sheep.berlios.de>

Author: tihocan
Date: 2007-07-26 20:46:47 +0200 (Thu, 26 Jul 2007)
New Revision: 7832

Modified:
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/RUN.log
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave
Log:
Regenerated results for test PL_DBN_SimpleRBM so that it passes again when it is reactivated in the future

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/RUN.log	2007-07-26 15:07:15 UTC (rev 7831)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/RUN.log	2007-07-26 18:46:47 UTC (rev 7832)
@@ -7,7 +7,6 @@
 [DeepBeliefNet] build_() called
 [DeepBeliefNet] build_layers_and_connections() called
 [DeepBeliefNet] build_classification_cost() called
-[RBMClassificationModule] build_() called
 [DeepBeliefNet] train() called 
 [DeepBeliefNet]   training_schedule = 20 10 
 [DeepBeliefNet]   cumulative_schedule = 0 20 30 

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave	2007-07-26 15:07:15 UTC (rev 7831)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave	2007-07-26 18:46:47 UTC (rev 7832)
@@ -34,7 +34,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 2 [ -0.0041988074813960885 0.00291521845148070419 ] ;
+bias = 2 [ -0.00419880748139610759 0.0029152184514807198 ] ;
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMBinomialLayer" ;
@@ -46,7 +46,7 @@
 connections = 1 [ *4 ->RBMMatrixConnection(
 weights = 2  2  [ 
 -0.222066209664342068 	0.582925561708350082 	
-0.516883916989072545 	-0.316272804129796303 	
+0.516883916989072434 	-0.316272804129796303 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -71,8 +71,8 @@
 last_layer = *3  ;
 last_to_target = *6 ->RBMMatrixConnection(
 weights = 2  2  [ 
--0.603392333131359426 	0.370787384786479657 	
-0.09708625652197031 	-0.668099599496062679 	
+-0.603392333131359537 	0.370787384786479657 	
+0.0970862565219703239 	-0.668099599496062679 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -100,7 +100,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 2 [ -0.00487910443291659449 0.00487910443291660317 ] ;
+bias = 2 [ -0.00487910443291655199 0.00487910443291657281 ] ;
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMMultinomialLayer" ;
@@ -152,6 +152,7 @@
 gibbs_down_state = 1 [ 0  0  [ 
 ]
 ] ;
+random_gen = *2  ;
 seed = 1827 ;
 stage = 30 ;
 n_examples = 2 ;

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave	2007-07-26 15:07:15 UTC (rev 7831)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave	2007-07-26 18:46:47 UTC (rev 7832)
@@ -65,7 +65,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 2 [ -0.0041988074813960885 0.00291521845148070419 ] ;
+bias = 2 [ -0.00419880748139610759 0.0029152184514807198 ] ;
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMBinomialLayer" ;
@@ -77,7 +77,7 @@
 connections = 1 [ *9 ->RBMMatrixConnection(
 weights = 2  2  [ 
 -0.222066209664342068 	0.582925561708350082 	
-0.516883916989072545 	-0.316272804129796303 	
+0.516883916989072434 	-0.316272804129796303 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -102,8 +102,8 @@
 last_layer = *8  ;
 last_to_target = *11 ->RBMMatrixConnection(
 weights = 2  2  [ 
--0.603392333131359426 	0.370787384786479657 	
-0.09708625652197031 	-0.668099599496062679 	
+-0.603392333131359537 	0.370787384786479657 	
+0.0970862565219703239 	-0.668099599496062679 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -131,7 +131,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 2 [ -0.00487910443291659449 0.00487910443291660317 ] ;
+bias = 2 [ -0.00487910443291655199 0.00487910443291657281 ] ;
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMMultinomialLayer" ;
@@ -183,6 +183,7 @@
 gibbs_down_state = 1 [ 0  0  [ 
 ]
 ] ;
+random_gen = *7  ;
 seed = 1827 ;
 stage = 30 ;
 n_examples = 2 ;
@@ -249,6 +250,7 @@
 learner = *5  ;
 provide_learner_expdir = 0 ;
 expdir_append = "" ;
+random_gen = *0 ;
 stage = 1 ;
 n_examples = 2 ;
 inputsize = 2 ;

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/metainfos.txt	2007-07-26 15:07:15 UTC (rev 7831)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/metainfos.txt	2007-07-26 18:46:47 UTC (rev 7832)
@@ -1,2 +1,2 @@
-__REVISION__ = "PL7677"
+__REVISION__ = "PL7704"
 

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave	2007-07-26 15:07:15 UTC (rev 7831)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave	2007-07-26 18:46:47 UTC (rev 7832)
@@ -85,8 +85,8 @@
 ] ;
 connections = 1 [ *10 ->RBMMatrixConnection(
 weights = 2  2  [ 
-0.211616747512905684 	0.21566475046848535 	
-0.59211590021607885 	0.667132771633056398 	
+0.211616747512905712 	0.215664750468485322 	
+0.59211590021607885 	0.667132771633056509 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -111,8 +111,8 @@
 last_layer = *9  ;
 last_to_target = *12 ->RBMMatrixConnection(
 weights = 2  2  [ 
--0.166376995329432703 	0.522222782976989097 	
-0.449826313170107683 	-0.266489754613600693 	
+-0.166376995329432731 	0.522222782976989097 	
+0.449826313170107572 	-0.266489754613600693 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -192,6 +192,7 @@
 gibbs_down_state = 1 [ 0  0  [ 
 ]
 ] ;
+random_gen = *8  ;
 seed = 1827 ;
 stage = 0 ;
 n_examples = -1 ;
@@ -258,6 +259,7 @@
 learner = *6  ;
 provide_learner_expdir = 0 ;
 expdir_append = "" ;
+random_gen = *0 ;
 stage = 0 ;
 n_examples = -1 ;
 inputsize = -1 ;



From tihocan at mail.berlios.de  Thu Jul 26 21:03:20 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 26 Jul 2007 21:03:20 +0200
Subject: [Plearn-commits] r7833 - trunk/plearn_learners/online
Message-ID: <200707261903.l6QJ3K6T017852@sheep.berlios.de>

Author: tihocan
Date: 2007-07-26 21:03:20 +0200 (Thu, 26 Jul 2007)
New Revision: 7833

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
Log:
Fixed bug occurring when reloading object (some learned parameters were erased when using classification cost)


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-07-26 18:46:47 UTC (rev 7832)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-07-26 19:03:20 UTC (rev 7833)
@@ -489,28 +489,41 @@
             "This method has not been verified yet for minibatch "
             "compatibility");
 
-    PP<RBMMatrixConnection> last_to_target = new RBMMatrixConnection();
-    last_to_target->up_size = layers[n_layers-1]->size;
-    last_to_target->down_size = n_classes;
-    last_to_target->random_gen = random_gen;
-    last_to_target->build();
+    PP<RBMMatrixConnection> last_to_target;
+    if (classification_module)
+        last_to_target = classification_module->last_to_target;
+    if (!last_to_target ||
+         last_to_target->up_size != layers[n_layers-1]->size ||
+         last_to_target->down_size != n_classes ||
+         last_to_target->random_gen != random_gen)
+    {
+        // We need to (re-)create 'last_to_target', and thus the classification
+        // module too.
+        // This is not systematically done so that the learner can be
+        // saved and loaded without losing learned parameters.
+        last_to_target = new RBMMatrixConnection();
+        last_to_target->up_size = layers[n_layers-1]->size;
+        last_to_target->down_size = n_classes;
+        last_to_target->random_gen = random_gen;
+        last_to_target->build();
 
-    PP<RBMMultinomialLayer> target_layer = new RBMMultinomialLayer();
-    target_layer->size = n_classes;
-    target_layer->random_gen = random_gen;
-    target_layer->build();
+        PP<RBMMultinomialLayer> target_layer = new RBMMultinomialLayer();
+        target_layer->size = n_classes;
+        target_layer->random_gen = random_gen;
+        target_layer->build();
 
-    PLASSERT_MSG(n_layers >= 2, "You must specify at least two layers (the "
-            "input layer and one hidden layer)");
+        PLASSERT_MSG(n_layers >= 2, "You must specify at least two layers (the "
+                "input layer and one hidden layer)");
 
-    classification_module = new RBMClassificationModule();
-    classification_module->previous_to_last = connections[n_layers-2];
-    classification_module->last_layer =
-        (RBMBinomialLayer*) (RBMLayer*) layers[n_layers-1];
-    classification_module->last_to_target = last_to_target;
-    classification_module->target_layer = target_layer;
-    classification_module->random_gen = random_gen;
-    classification_module->build();
+        classification_module = new RBMClassificationModule();
+        classification_module->previous_to_last = connections[n_layers-2];
+        classification_module->last_layer =
+            (RBMBinomialLayer*) (RBMLayer*) layers[n_layers-1];
+        classification_module->last_to_target = last_to_target;
+        classification_module->target_layer = target_layer;
+        classification_module->random_gen = random_gen;
+        classification_module->build();
+    }
 
     classification_cost = new NLLCostModule();
     classification_cost->input_size = n_classes;
@@ -520,7 +533,7 @@
     joint_layer = new RBMMixedLayer();
     joint_layer->sub_layers.resize( 2 );
     joint_layer->sub_layers[0] = layers[ n_layers-2 ];
-    joint_layer->sub_layers[1] = target_layer;
+    joint_layer->sub_layers[1] = classification_module->target_layer;
     joint_layer->random_gen = random_gen;
     joint_layer->build();
 }



From tihocan at mail.berlios.de  Thu Jul 26 21:04:35 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 26 Jul 2007 21:04:35 +0200
Subject: [Plearn-commits] r7834 -
	trunk/plearn_learners/online/test/DeepBeliefNet
Message-ID: <200707261904.l6QJ4ZRZ017939@sheep.berlios.de>

Author: tihocan
Date: 2007-07-26 21:04:35 +0200 (Thu, 26 Jul 2007)
New Revision: 7834

Modified:
   trunk/plearn_learners/online/test/DeepBeliefNet/pytest.config
Log:
Re-enabled test PL_DBN_SimpleRBM, that should now pass

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/pytest.config
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/pytest.config	2007-07-26 19:03:20 UTC (rev 7833)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/pytest.config	2007-07-26 19:04:35 UTC (rev 7834)
@@ -119,5 +119,5 @@
     resources = [ "SimpleRBM_test.pyplearn" ],
     precision = 1e-06,
     pfileprg = "__program__",
-    disabled = True
+    disabled = False
     )



From tihocan at mail.berlios.de  Thu Jul 26 22:02:41 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 26 Jul 2007 22:02:41 +0200
Subject: [Plearn-commits] r7835 - trunk/plearn/io
Message-ID: <200707262002.l6QK2fHK021282@sheep.berlios.de>

Author: tihocan
Date: 2007-07-26 22:02:41 +0200 (Thu, 26 Jul 2007)
New Revision: 7835

Modified:
   trunk/plearn/io/PStream.cc
Log:
Moved new error check before the 'break' so that we actually get a chance to go through it

Modified: trunk/plearn/io/PStream.cc
===================================================================
--- trunk/plearn/io/PStream.cc	2007-07-26 19:04:35 UTC (rev 7834)
+++ trunk/plearn/io/PStream.cc	2007-07-26 20:02:41 UTC (rev 7835)
@@ -792,9 +792,10 @@
         tmpbuf[l] = '\0';
         unget();
         sscanf(tmpbuf,"%lf",&x);
+        if(l==0)
+            PLERROR("In PStream::readAsciiNum - we read (%c) while we "
+                    "expected a digit", c);
         break;
-        if(l==0)
-            PLERROR("In PStream::readAsciiNum - we read (%c) while we expected a digit",c);
     }
 }
 



From tihocan at mail.berlios.de  Thu Jul 26 22:21:45 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 26 Jul 2007 22:21:45 +0200
Subject: [Plearn-commits] r7836 - trunk/plearn/var/test
Message-ID: <200707262021.l6QKLjgK023365@sheep.berlios.de>

Author: tihocan
Date: 2007-07-26 22:21:45 +0200 (Thu, 26 Jul 2007)
New Revision: 7836

Modified:
   trunk/plearn/var/test/pytest.config
Log:
Disabling test PL_Var_util until I get it fixed

Modified: trunk/plearn/var/test/pytest.config
===================================================================
--- trunk/plearn/var/test/pytest.config	2007-07-26 20:02:41 UTC (rev 7835)
+++ trunk/plearn/var/test/pytest.config	2007-07-26 20:21:45 UTC (rev 7836)
@@ -104,5 +104,5 @@
     resources = [ ],
     precision = 1e-06,
     pfileprg = "__program__",
-    disabled = False
+    disabled = True
     )



From dorionc at mail.berlios.de  Thu Jul 26 23:50:34 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Thu, 26 Jul 2007 23:50:34 +0200
Subject: [Plearn-commits] r7837 - trunk/python_modules/plearn/pyplearn
Message-ID: <200707262150.l6QLoYRZ031029@sheep.berlios.de>

Author: dorionc
Date: 2007-07-26 23:50:33 +0200 (Thu, 26 Jul 2007)
New Revision: 7837

Modified:
   trunk/python_modules/plearn/pyplearn/plargs.py
Log:
Minor improvement

Modified: trunk/python_modules/plearn/pyplearn/plargs.py
===================================================================
--- trunk/python_modules/plearn/pyplearn/plargs.py	2007-07-26 20:21:45 UTC (rev 7836)
+++ trunk/python_modules/plearn/pyplearn/plargs.py	2007-07-26 21:50:33 UTC (rev 7837)
@@ -972,7 +972,8 @@
                 return attr
 
         def __setattr__(cls, key, value):
-            if key.startswith('_'):
+            if key.startswith('_') \
+               or (hasattr(cls, key) and callable(getattr(cls,key))):
                 type.__setattr__(cls,key,value)
             else:
                 try:



From lamblin at mail.berlios.de  Fri Jul 27 04:26:53 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 27 Jul 2007 04:26:53 +0200
Subject: [Plearn-commits] r7838 - trunk/python_modules/plearn/learners/autolr
Message-ID: <200707270226.l6R2QrPi032304@sheep.berlios.de>

Author: lamblin
Date: 2007-07-27 04:26:52 +0200 (Fri, 27 Jul 2007)
New Revision: 7838

Modified:
   trunk/python_modules/plearn/learners/autolr/__init__.py
Log:
Treat a performance that does not vary as an improvement.


Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-07-26 21:50:33 UTC (rev 7837)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-07-27 02:26:52 UTC (rev 7838)
@@ -566,7 +566,7 @@
                 logfile.flush()
         if save_best and t%save_best==0:
             all_candidates[best_active].save(expdir+"/"+"best_learner.psave","plearn_binary")
-        if previous_best_err > best_err:
+        if previous_best_err >= best_err:
             previous_best_err = best_err
             if logfile:
                 print >>logfile,"BEST to now is candidate ",best_active," with err=",best_err



From lamblin at mail.berlios.de  Fri Jul 27 04:30:13 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 27 Jul 2007 04:30:13 +0200
Subject: [Plearn-commits] r7839 - in trunk/plearn: base io
Message-ID: <200707270230.l6R2UDmY032512@sheep.berlios.de>

Author: lamblin
Date: 2007-07-27 04:30:10 +0200 (Fri, 27 Jul 2007)
New Revision: 7839

Modified:
   trunk/plearn/base/TypeTraits.h
   trunk/plearn/io/PStream.cc
   trunk/plearn/io/PStream.h
Log:
Hopefully better solution to the binary serialization/deserialization
problems across architectures.
It also allows "real" variables saved as "double" to be read as "float",
and vice-versa.


Modified: trunk/plearn/base/TypeTraits.h
===================================================================
--- trunk/plearn/base/TypeTraits.h	2007-07-27 02:26:52 UTC (rev 7838)
+++ trunk/plearn/base/TypeTraits.h	2007-07-27 02:30:10 UTC (rev 7839)
@@ -136,6 +136,92 @@
   { return BIG_ENDIAN_TYPECODE; }                                                       \
 }
 
+#define DECLARE_TYPE_TRAITS_FOR_INTTYPE(T)                  \
+template<>                                                  \
+class TypeTraits<T>                                         \
+{                                                           \
+public:                                                     \
+    static inline string name()                             \
+    { return #T; }                                          \
+                                                            \
+    static inline unsigned char little_endian_typecode()    \
+    {                                                       \
+        switch(sizeof(T))                                   \
+        {                                                   \
+        case sizeof(int8_t):                                \
+            return 0x01;                                    \
+        case sizeof(int16_t):                               \
+            return 0x03;                                    \
+        case sizeof(int32_t):                               \
+            return 0x07;                                    \
+        case sizeof(int64_t):                               \
+            return 0x16;                                    \
+        default:                                            \
+            return 0xFF;                                    \
+        }                                                   \
+    }                                                       \
+                                                            \
+    static inline unsigned char big_endian_typecode()       \
+    {                                                       \
+        switch(sizeof(T))                                   \
+        {                                                   \
+        case sizeof(int8_t):                                \
+            return 0x01;                                    \
+        case sizeof(int16_t):                               \
+            return 0x04;                                    \
+        case sizeof(int32_t):                               \
+            return 0x08;                                    \
+        case sizeof(int64_t):                               \
+            return 0x17;                                    \
+        default:                                            \
+            return 0xFF;                                    \
+        }                                                   \
+    }                                                       \
+}
+
+#define DECLARE_TYPE_TRAITS_FOR_UINTTYPE(T)                 \
+template<>                                                  \
+class TypeTraits<T>                                         \
+{                                                           \
+public:                                                     \
+    static inline string name()                             \
+    { return #T; }                                          \
+                                                            \
+    static inline unsigned char little_endian_typecode()    \
+    {                                                       \
+        switch(sizeof(T))                                   \
+        {                                                   \
+        case sizeof(uint8_t):                               \
+            return 0x02;                                    \
+        case sizeof(uint16_t):                              \
+            return 0x05;                                    \
+        case sizeof(uint32_t):                              \
+            return 0x0B;                                    \
+        case sizeof(uint64_t):                              \
+            return 0x18;                                    \
+        default:                                            \
+            return 0xFF;                                    \
+        }                                                   \
+    }                                                       \
+                                                            \
+    static inline unsigned char big_endian_typecode()       \
+    {                                                       \
+        switch(sizeof(T))                                   \
+        {                                                   \
+        case sizeof(uint8_t):                               \
+            return 0x02;                                    \
+        case sizeof(uint16_t):                              \
+            return 0x06;                                    \
+        case sizeof(uint32_t):                              \
+            return 0x0C;                                    \
+        case sizeof(uint64_t):                              \
+            return 0x19;                                    \
+        default:                                            \
+            return 0xFF;                                    \
+        }                                                   \
+    }                                                       \
+}
+
 #define DECLARE_TYPE_TRAITS(T)                          \
 template<>                                              \
 class TypeTraits<T>                                     \
@@ -153,21 +239,23 @@
 
 // DECLARE_TYPE_TRAITS_FOR_BASETYPE(bool, ??, ??);
 DECLARE_TYPE_TRAITS_FOR_BASETYPE(void,               0xFF, 0xFF);
-DECLARE_TYPE_TRAITS_FOR_BASETYPE(char,               0x01, 0x01);
-DECLARE_TYPE_TRAITS_FOR_BASETYPE(signed char,        0x01, 0x01);
-DECLARE_TYPE_TRAITS_FOR_BASETYPE(unsigned char,      0x02, 0x02);
-DECLARE_TYPE_TRAITS_FOR_BASETYPE(short,              0x03, 0x04);
-DECLARE_TYPE_TRAITS_FOR_BASETYPE(unsigned short,     0x05, 0x06);
-DECLARE_TYPE_TRAITS_FOR_BASETYPE(int,                0x07, 0x08);
-DECLARE_TYPE_TRAITS_FOR_BASETYPE(unsigned int,       0x0B, 0x0C);
-DECLARE_TYPE_TRAITS_FOR_BASETYPE(long,               0x07, 0x08);
-DECLARE_TYPE_TRAITS_FOR_BASETYPE(unsigned long,      0x0B, 0x0C);
-DECLARE_TYPE_TRAITS_FOR_BASETYPE(long long,          0x16, 0x17);
-DECLARE_TYPE_TRAITS_FOR_BASETYPE(unsigned long long, 0x18, 0x19);
 DECLARE_TYPE_TRAITS_FOR_BASETYPE(float,              0x0E, 0x0F);
 DECLARE_TYPE_TRAITS_FOR_BASETYPE(double,             0x10, 0x11);
 DECLARE_TYPE_TRAITS_FOR_BASETYPE(bool,               0x30, 0x30);
 
+DECLARE_TYPE_TRAITS_FOR_INTTYPE(char);
+DECLARE_TYPE_TRAITS_FOR_INTTYPE(signed char);
+DECLARE_TYPE_TRAITS_FOR_INTTYPE(short);
+DECLARE_TYPE_TRAITS_FOR_INTTYPE(int);
+DECLARE_TYPE_TRAITS_FOR_INTTYPE(long);
+DECLARE_TYPE_TRAITS_FOR_INTTYPE(long long);
+
+DECLARE_TYPE_TRAITS_FOR_UINTTYPE(unsigned char);
+DECLARE_TYPE_TRAITS_FOR_UINTTYPE(unsigned short);
+DECLARE_TYPE_TRAITS_FOR_UINTTYPE(unsigned int);
+DECLARE_TYPE_TRAITS_FOR_UINTTYPE(unsigned long);
+DECLARE_TYPE_TRAITS_FOR_UINTTYPE(unsigned long long);
+
 DECLARE_TYPE_TRAITS(string);
 
 template<class T>

Modified: trunk/plearn/io/PStream.cc
===================================================================
--- trunk/plearn/io/PStream.cc	2007-07-27 02:26:52 UTC (rev 7838)
+++ trunk/plearn/io/PStream.cc	2007-07-27 02:30:10 UTC (rev 7839)
@@ -799,6 +799,7 @@
     }
 }
 
+
 PStream& PStream::operator=(const PStream& pios)
 {
     if(this != &pios)
@@ -1088,75 +1089,52 @@
     return *this;
 }
 
-PStream& PStream::operator>>(int &x)
+PStream& PStream::operator>>(bool &x)
 {
+    int parsed = -1;
+
+    char c;
     switch(inmode)
     {
     case raw_ascii:
     case pretty_ascii:
-        skipBlanks();
-        readAsciiNum(x);
-        break;
     case raw_binary:
-        read(reinterpret_cast<char *>(&x), sizeof(int));
-        break;
     case plearn_ascii:
     case plearn_binary:
-    {
         skipBlanksAndCommentsAndSeparators();
-        int c = get();
-        if(c==0x07 || c==0x08 || c==0x0B || c==0x0C )  // plearn_binary
+        c = get();
+        if(c=='1')
+            parsed = 1;
+
+        else if(c=='0')
+            parsed = 0;
+
+        else if(c=='T')
         {
-            read(reinterpret_cast<char*>(&x),sizeof(int));
-            if( (c==0x07 && byte_order()==BIG_ENDIAN_ORDER)
-                || (c==0x08 && byte_order()==LITTLE_ENDIAN_ORDER) )
-                endianswap(&x);
+            char r = get();
+            char u = get();
+            char e = get();
+            if ( r == 'r' && u == 'u' && e == 'e' )
+                parsed = 1;
         }
-        else  // plearn_ascii
+
+        else if(c=='F')
         {
-            unget();
-            readAsciiNum(x);
+            char a = get();
+            char l = get();
+            char s = get();
+            char e = get();
+            if ( a == 'a' && l == 'l' && s == 's' && e == 'e' )
+                parsed = 0;
         }
-        break;
-    }
-    default:
-        PLERROR("In PStream::operator>>  unknown inmode!!!!!!!!!");
-        break;
-    }
-    return *this;
-}
 
-PStream& PStream::operator>>(unsigned int &x)
-{
-    switch(inmode)
-    {
-    case raw_ascii:
-    case pretty_ascii:
-        skipBlanks();
-        readAsciiNum(x);
+        if ( parsed == -1 )
+            PLERROR("In PStream::operator>>(bool &x) wrong format for bool, must be one "
+                    "of characters 0 or 1 or unquoted strings True or False" );
+        else
+            x = (parsed != 0);
         break;
-    case raw_binary:
-        read(reinterpret_cast<char *>(&x), sizeof(unsigned int));
-        break;
-    case plearn_ascii:
-    case plearn_binary:
-    {
-        skipBlanksAndCommentsAndSeparators();
-        int c = get();
-        if(c==0x0B || c==0x0C || c==0x07 || c==0x08)  // plearn_binary unsigned int or int
-        {
-            read(reinterpret_cast<char*>(&x),sizeof(unsigned int));
-            if( (c==0x0B && byte_order()==BIG_ENDIAN_ORDER)
-                || (c==0x0C && byte_order()==LITTLE_ENDIAN_ORDER) )
-                endianswap(&x);
-        }
-        else  // plearn_ascii
-        {
-            unget();
-            readAsciiNum(x);
-        }
-        break;
-    }
+
     default:
         PLERROR("In PStream::operator>>  unknown inmode!!!!!!!!!");
         break;
@@ -1164,8 +1142,7 @@
     return *this;
 }
 
-/* Commented out because "long" has not the same size on every platform
-PStream& PStream::operator>>(long &x)
+PStream& PStream::operator>>(short &x)
 {
     switch(inmode)
     {
@@ -1175,27 +1152,17 @@
         readAsciiNum(x);
         break;
     case raw_binary:
-        read(reinterpret_cast<char *>(&x), sizeof(long));
+        read(reinterpret_cast<char *>(&x), sizeof(short));
         break;
     case plearn_ascii:
     case plearn_binary:
     {
         skipBlanksAndCommentsAndSeparators();
         int c = get();
-        if(c==0x07 || c==0x08)  // plearn_binary
+        if(c>0x00 && c<0x20)  // plearn_binary
         {
-            read(reinterpret_cast<char*>(&x),sizeof(long));
-            if( (c==0x07 && byte_order()==BIG_ENDIAN_ORDER)
-                || (c==0x08 && byte_order()==LITTLE_ENDIAN_ORDER) )
-                endianswap(&x);
+            readBinaryNum(x, c);
         }
-        else if(c==0x16 || c==0x17)  // plearn_binary
-        {
-            read(reinterpret_cast<char*>(&x),sizeof(long));
-            if( (c==0x16 && byte_order()==BIG_ENDIAN_ORDER)
-                || (c==0x17 && byte_order()==LITTLE_ENDIAN_ORDER) )
-                endianswap(&x);
-        }
         else  // plearn_ascii
         {
             unget();
@@ -1209,9 +1176,8 @@
     }
     return *this;
 }
-*/
 
-PStream& PStream::operator>>(int64_t &x)
+PStream& PStream::operator>>(unsigned short &x)
 {
     switch(inmode)
     {
@@ -1221,29 +1187,17 @@
         readAsciiNum(x);
         break;
     case raw_binary:
-        read(reinterpret_cast<char *>(&x), sizeof(int64_t));
+        read(reinterpret_cast<char *>(&x), sizeof(unsigned short));
         break;
     case plearn_ascii:
     case plearn_binary:
     {
         skipBlanksAndCommentsAndSeparators();
         int c = get();
-        if(c==0x07 || c==0x08)  // plearn_binary 32 bits integer
+        if(c>0x00 && c<0x20)  // plearn_binary
         {
-            int32_t y;
-            read(reinterpret_cast<char*>(&y),sizeof(int32_t));
-            if( (c==0x07 && byte_order()==BIG_ENDIAN_ORDER)
-                || (c==0x08 && byte_order()==LITTLE_ENDIAN_ORDER) )
-                endianswap(&y);
-            x = y;
+            readBinaryNum(x, c);
         }
-        else if(c==0x16 || c==0x17)  // plearn_binary 64 bits integer
-        {
-            read(reinterpret_cast<char*>(&x),sizeof(int64_t));
-            if( (c==0x16 && byte_order()==BIG_ENDIAN_ORDER)
-                || (c==0x17 && byte_order()==LITTLE_ENDIAN_ORDER) )
-                endianswap(&x);
-        }
         else  // plearn_ascii
         {
             unget();
@@ -1258,8 +1212,7 @@
     return *this;
 }
 
-/*
-PStream& PStream::operator>>(unsigned long &x)
+PStream& PStream::operator>>(int &x)
 {
     switch(inmode)
     {
@@ -1269,19 +1222,16 @@
         readAsciiNum(x);
         break;
     case raw_binary:
-        read(reinterpret_cast<char *>(&x), sizeof(unsigned long));
+        read(reinterpret_cast<char *>(&x), sizeof(int));
         break;
     case plearn_ascii:
     case plearn_binary:
     {
         skipBlanksAndCommentsAndSeparators();
         int c = get();
-        if(c==0x0B || c==0x0C)  // plearn_binary
+        if(c>0x00 && c<0x20)  // plearn_binary
         {
-            read(reinterpret_cast<char*>(&x),sizeof(unsigned long));
-            if( (c==0x0B && byte_order()==BIG_ENDIAN_ORDER)
-                || (c==0x0C && byte_order()==LITTLE_ENDIAN_ORDER) )
-                endianswap(&x);
+            readBinaryNum(x, c);
         }
         else  // plearn_ascii
         {
@@ -1296,9 +1246,8 @@
     }
     return *this;
 }
-*/
 
-PStream& PStream::operator>>(uint64_t &x)
+PStream& PStream::operator>>(unsigned int &x)
 {
     switch(inmode)
     {
@@ -1308,29 +1257,17 @@
         readAsciiNum(x);
         break;
     case raw_binary:
-        read(reinterpret_cast<char *>(&x), sizeof(uint64_t));
+        read(reinterpret_cast<char *>(&x), sizeof(unsigned int));
         break;
     case plearn_ascii:
     case plearn_binary:
     {
         skipBlanksAndCommentsAndSeparators();
         int c = get();
-        if(c==0x0B || c==0x0C)  // plearn_binary 32 bits unsigned integer
+        if(c>0x00 && c<0x20)  // plearn_binary
         {
-            uint32_t y;
-            read(reinterpret_cast<char*>(&y),sizeof(uint32_t));
-            if( (c==0x0B && byte_order()==BIG_ENDIAN_ORDER)
-                || (c==0x0C && byte_order()==LITTLE_ENDIAN_ORDER) )
-                endianswap(&y);
-            x = y;
+            readBinaryNum(x, c);
         }
-        else if(c==0x18 || c==0x19) // plearn_binary 64 bits unsigned integer
-        {
-            read(reinterpret_cast<char*>(&x), sizeof(uint64_t));
-            if( (c==0x18 && byte_order()==BIG_ENDIAN_ORDER)
-                || (c==0x19 && byte_order()==LITTLE_ENDIAN_ORDER) )
-                endianswap(&x);
-        }
         else  // plearn_ascii
         {
             unget();
@@ -1345,8 +1282,7 @@
     return *this;
 }
 
-/*
-PStream& PStream::operator>>(long long &x)
+PStream& PStream::operator>>(long &x)
 {
     switch(inmode)
     {
@@ -1356,19 +1292,16 @@
         readAsciiNum(x);
         break;
     case raw_binary:
-        read(reinterpret_cast<char *>(&x), sizeof(long long));
+        read(reinterpret_cast<char *>(&x), sizeof(long));
         break;
     case plearn_ascii:
     case plearn_binary:
     {
         skipBlanksAndCommentsAndSeparators();
         int c = get();
-        if(c==0x16 || c==0x17)  // plearn_binary
+        if(c>0x00 && c<0x20)  // plearn_binary
         {
-            read(reinterpret_cast<char*>(&x),sizeof(long long));
-            if( (c==0x16 && byte_order()==BIG_ENDIAN_ORDER)
-                || (c==0x17 && byte_order()==LITTLE_ENDIAN_ORDER) )
-                endianswap(&x);
+            readBinaryNum(x, c);
         }
         else  // plearn_ascii
         {
@@ -1384,7 +1317,7 @@
     return *this;
 }
 
-PStream& PStream::operator>>(unsigned long long &x)
+PStream& PStream::operator>>(unsigned long &x)
 {
     switch(inmode)
     {
@@ -1394,19 +1327,16 @@
         readAsciiNum(x);
         break;
     case raw_binary:
-        read(reinterpret_cast<char *>(&x), sizeof(unsigned long long));
+        read(reinterpret_cast<char *>(&x), sizeof(unsigned long));
         break;
     case plearn_ascii:
     case plearn_binary:
     {
         skipBlanksAndCommentsAndSeparators();
         int c = get();
-        if(c==0x18 || c==0x19)  // plearn_binary
+        if(c>0x00 && c<0x20)  // plearn_binary
         {
-            read(reinterpret_cast<char*>(&x),sizeof(unsigned long long));
-            if( (c==0x18 && byte_order()==BIG_ENDIAN_ORDER)
-                || (c==0x19 && byte_order()==LITTLE_ENDIAN_ORDER) )
-                endianswap(&x);
+            readBinaryNum(x, c);
         }
         else  // plearn_ascii
         {
@@ -1421,9 +1351,8 @@
     }
     return *this;
 }
-*/
 
-PStream& PStream::operator>>(short &x)
+PStream& PStream::operator>>(long long &x)
 {
     switch(inmode)
     {
@@ -1433,19 +1362,16 @@
         readAsciiNum(x);
         break;
     case raw_binary:
-        read(reinterpret_cast<char *>(&x), sizeof(short));
+        read(reinterpret_cast<char *>(&x), sizeof(long long));
         break;
     case plearn_ascii:
     case plearn_binary:
     {
         skipBlanksAndCommentsAndSeparators();
         int c = get();
-        if(c==0x03 || c==0x04)  // plearn_binary
+        if(c>0x00 && c<0x20)  // plearn_binary
         {
-            read(reinterpret_cast<char*>(&x),sizeof(short));
-            if( (c==0x03 && byte_order()==BIG_ENDIAN_ORDER)
-                || (c==0x04 && byte_order()==LITTLE_ENDIAN_ORDER) )
-                endianswap(&x);
+            readBinaryNum(x, c);
         }
         else  // plearn_ascii
         {
@@ -1461,7 +1387,7 @@
     return *this;
 }
 
-PStream& PStream::operator>>(unsigned short &x)
+PStream& PStream::operator>>(unsigned long long &x)
 {
     switch(inmode)
     {
@@ -1471,19 +1397,16 @@
         readAsciiNum(x);
         break;
     case raw_binary:
-        read(reinterpret_cast<char *>(&x), sizeof(unsigned short));
+        read(reinterpret_cast<char *>(&x), sizeof(unsigned long long));
         break;
     case plearn_ascii:
     case plearn_binary:
     {
         skipBlanksAndCommentsAndSeparators();
         int c = get();
-        if(c==0x05 || c==0x06)  // plearn_binary
+        if(c>0x00 && c<0x20)  // plearn_binary
         {
-            read(reinterpret_cast<char*>(&x),sizeof(unsigned short));
-            if( (c==0x05 && byte_order()==BIG_ENDIAN_ORDER)
-                || (c==0x06 && byte_order()==LITTLE_ENDIAN_ORDER) )
-                endianswap(&x);
+            readBinaryNum(x, c);
         }
         else  // plearn_ascii
         {
@@ -1499,59 +1422,6 @@
     return *this;
 }
 
-PStream& PStream::operator>>(bool &x)
-{
-    int parsed = -1;
-
-    char c;
-    switch(inmode)
-    {
-    case raw_ascii:
-    case pretty_ascii:
-    case raw_binary:
-    case plearn_ascii:
-    case plearn_binary:
-        skipBlanksAndCommentsAndSeparators();
-        c = get();
-        if(c=='1')
-            parsed = 1;
-
-        else if(c=='0')
-            parsed = 0;
-
-        else if(c=='T')
-        {
-            char r = get();
-            char u = get();
-            char e = get();
-            if ( r == 'r' && u == 'u' && e == 'e' )
-                parsed = 1;
-        }
-
-        else if(c=='F')
-        {
-            char a = get();
-            char l = get();
-            char s = get();
-            char e = get();
-            if ( a == 'a' && l == 'l' && s == 's' && e == 'e' )
-                parsed = 0;
-        }
-
-        if ( parsed == -1 )
-            PLERROR("In PStream::operator>>(bool &x) wrong format for bool, must be one "
-                    "of characters 0 or 1 or unquoted strings True or False" );
-        else
-            x = (parsed != 0);
-        break;
-
-    default:
-        PLERROR("In PStream::operator>>  unknown inmode!!!!!!!!!");
-        break;
-    }
-    return *this;
-}
-
 PStream& PStream::operator>>(float &x)
 {
     switch(inmode)
@@ -1569,12 +1439,9 @@
     {
         skipBlanksAndCommentsAndSeparators();
         int c = get();
-        if(c==0x0E || c==0x0F)  // plearn_binary
+        if(c>0x00 && c<0x20)  // plearn_binary
         {
-            read(reinterpret_cast<char*>(&x),sizeof(float));
-            if( (c==0x0E && byte_order()==BIG_ENDIAN_ORDER)
-                || (c==0x0F && byte_order()==LITTLE_ENDIAN_ORDER) )
-                endianswap(&x);
+            readBinaryNum(x, c);
         }
         else  // plearn_ascii
         {
@@ -1607,12 +1474,9 @@
     {
         skipBlanksAndCommentsAndSeparators();
         int c = get();
-        if(c==0x10 || c==0x11)  // plearn_binary
+        if(c>0x00 && c<0x20)  // plearn_binary
         {
-            read(reinterpret_cast<char*>(&x),sizeof(double));
-            if( (c==0x10 && byte_order()==BIG_ENDIAN_ORDER)
-                || (c==0x11 && byte_order()==LITTLE_ENDIAN_ORDER) )
-                endianswap(&x);
+            readBinaryNum(x, c);
         }
         else  // ascii
         {
@@ -1647,8 +1511,7 @@
         put(' ');
         break;
     case plearn_binary:
-        put((char)0x01);
-        put(x);
+        writeBinaryNum(x);
         break;
     default:
         PLERROR("In PStream::operator<<  unknown outmode!!!!!!!!!");
@@ -1682,8 +1545,7 @@
         put(' ');
         break;
     case plearn_binary:
-        put((char)0x02);
-        put(x);
+        writeBinaryNum(x);
         break;
     default:
         PLERROR("In PStream::operator<<  unknown outmode!!!!!!!!!");
@@ -1785,12 +1647,12 @@
     return *this;
 }
 
-PStream& PStream::operator<<(int x)
+PStream& PStream::operator<<(short x)
 {
     switch(outmode)
     {
     case raw_binary:
-        write(reinterpret_cast<char *>(&x), sizeof(int));
+        write(reinterpret_cast<char *>(&x), sizeof(short));
         break;
     case raw_ascii:
     case pretty_ascii:
@@ -1801,12 +1663,7 @@
         put(' ');
         break;
     case plearn_binary:
-#ifdef BIGENDIAN
-        put((char)0x08);
-#else
-        put((char)0x07);
-#endif
-        write((char*)&x,sizeof(int));
+        writeBinaryNum(x);
         break;
     default:
         PLERROR("In PStream::operator<<  unknown outmode!!!!!!!!!");
@@ -1815,12 +1672,12 @@
     return *this;
 }
 
-PStream& PStream::operator<<(unsigned int x)
+PStream& PStream::operator<<(unsigned short x)
 {
     switch(outmode)
     {
     case raw_binary:
-        write(reinterpret_cast<char *>(&x), sizeof(unsigned int));
+        write(reinterpret_cast<char *>(&x), sizeof(unsigned short));
         break;
     case raw_ascii:
     case pretty_ascii:
@@ -1831,12 +1688,7 @@
         put(' ');
         break;
     case plearn_binary:
-#ifdef BIGENDIAN
-        put((char)0x0C);
-#else
-        put((char)0x0B);
-#endif
-        write((char*)&x,sizeof(unsigned int));
+        writeBinaryNum(x);
         break;
     default:
         PLERROR("In PStream::operator<<  unknown outmode!!!!!!!!!");
@@ -1845,13 +1697,12 @@
     return *this;
 }
 
-/* Commented out because "long" has not the same size on every platform
-PStream& PStream::operator<<(long x)
+PStream& PStream::operator<<(int x)
 {
     switch(outmode)
     {
     case raw_binary:
-        write(reinterpret_cast<char *>(&x), sizeof(long));
+        write(reinterpret_cast<char *>(&x), sizeof(int));
         break;
     case raw_ascii:
     case pretty_ascii:
@@ -1862,23 +1713,7 @@
         put(' ');
         break;
     case plearn_binary:
-        if(sizeof(long)==4)
-        {
-#ifdef BIGENDIAN
-        put((char)0x08);
-#else
-        put((char)0x07);
-#endif
-        }
-        else if(sizeof(long)==8)
-        {
-#ifdef BIGENDIAN
-        put((char)0x17);
-#else
-        put((char)0x16);
-#endif
-        }
-        write((char*)&x,sizeof(long));
+        writeBinaryNum(x);
         break;
     default:
         PLERROR("In PStream::operator<<  unknown outmode!!!!!!!!!");
@@ -1886,14 +1721,13 @@
     }
     return *this;
 }
-*/
 
-PStream& PStream::operator<<(int64_t x)
+PStream& PStream::operator<<(unsigned int x)
 {
     switch(outmode)
     {
     case raw_binary:
-        write(reinterpret_cast<char *>(&x), sizeof(int64_t));
+        write(reinterpret_cast<char *>(&x), sizeof(unsigned int));
         break;
     case raw_ascii:
     case pretty_ascii:
@@ -1904,12 +1738,7 @@
         put(' ');
         break;
     case plearn_binary:
-#ifdef BIGENDIAN
-        put((char)0x17);
-#else
-        put((char)0x16);
-#endif
-        write((char*)&x, sizeof(int64_t));
+        writeBinaryNum(x);
         break;
     default:
         PLERROR("In PStream::operator<<  unknown outmode!!!!!!!!!");
@@ -1918,13 +1747,12 @@
     return *this;
 }
 
-/*
-PStream& PStream::operator<<(unsigned long x)
+PStream& PStream::operator<<(long x)
 {
     switch(outmode)
     {
     case raw_binary:
-        write(reinterpret_cast<char *>(&x), sizeof(unsigned long));
+        write(reinterpret_cast<char *>(&x), sizeof(long));
         break;
     case raw_ascii:
     case pretty_ascii:
@@ -1935,12 +1763,7 @@
         put(' ');
         break;
     case plearn_binary:
-#ifdef BIGENDIAN
-        put((char)0x0C);
-#else
-        put((char)0x0B);
-#endif
-        write((char*)&x,sizeof(unsigned long));
+        writeBinaryNum(x);
         break;
     default:
         PLERROR("In PStream::operator<<  unknown outmode!!!!!!!!!");
@@ -1948,14 +1771,13 @@
     }
     return *this;
 }
-*/
 
-PStream& PStream::operator<<(uint64_t x)
+PStream& PStream::operator<<(unsigned long x)
 {
     switch(outmode)
     {
     case raw_binary:
-        write(reinterpret_cast<char *>(&x), sizeof(uint64_t));
+        write(reinterpret_cast<char *>(&x), sizeof(unsigned long));
         break;
     case raw_ascii:
     case pretty_ascii:
@@ -1966,12 +1788,7 @@
         put(' ');
         break;
     case plearn_binary:
-#ifdef BIGENDIAN
-        put((char)0x19);
-#else
-        put((char)0x18);
-#endif
-        write((char*)&x, sizeof(uint64_t));
+        writeBinaryNum(x);
         break;
     default:
         PLERROR("In PStream::operator<<  unknown outmode!!!!!!!!!");
@@ -1980,7 +1797,6 @@
     return *this;
 }
 
-/*
 PStream& PStream::operator<<(long long x)
 {
     switch(outmode)
@@ -1997,12 +1813,7 @@
         put(' ');
         break;
     case plearn_binary:
-#ifdef BIGENDIAN
-        put((char)0x17);
-#else
-        put((char)0x16);
-#endif
-        write((char*)&x,sizeof(long long));
+        writeBinaryNum(x);
         break;
     default:
         PLERROR("In PStream::operator<<  unknown outmode!!!!!!!!!");
@@ -2027,12 +1838,7 @@
         put(' ');
         break;
     case plearn_binary:
-#ifdef BIGENDIAN
-        put((char)0x19);
-#else
-        put((char)0x18);
-#endif
-        write((char*)&x,sizeof(unsigned long long));
+        writeBinaryNum(x);
         break;
     default:
         PLERROR("In PStream::operator<<  unknown outmode!!!!!!!!!");
@@ -2040,68 +1846,7 @@
     }
     return *this;
 }
-*/
 
-PStream& PStream::operator<<(short x)
-{
-    switch(outmode)
-    {
-    case raw_binary:
-        write(reinterpret_cast<char *>(&x), sizeof(short));
-        break;
-    case raw_ascii:
-    case pretty_ascii:
-        writeAsciiNum(x);
-        break;
-    case plearn_ascii:
-        writeAsciiNum(x);
-        put(' ');
-        break;
-    case plearn_binary:
-#ifdef BIGENDIAN
-        put((char)0x04);
-#else
-        put((char)0x03);
-#endif
-        write((char*)&x,sizeof(short));
-        break;
-    default:
-        PLERROR("In PStream::operator<<  unknown outmode!!!!!!!!!");
-        break;
-    }
-    return *this;
-}
-
-PStream& PStream::operator<<(unsigned short x)
-{
-    switch(outmode)
-    {
-    case raw_binary:
-        write(reinterpret_cast<char *>(&x), sizeof(unsigned short));
-        break;
-    case raw_ascii:
-    case pretty_ascii:
-        writeAsciiNum(x);
-        break;
-    case plearn_ascii:
-        writeAsciiNum(x);
-        put(' ');
-        break;
-    case plearn_binary:
-#ifdef BIGENDIAN
-        put((char)0x06);
-#else
-        put((char)0x05);
-#endif
-        write((char*)&x,sizeof(unsigned short));
-        break;
-    default:
-        PLERROR("In PStream::operator<<  unknown outmode!!!!!!!!!");
-        break;
-    }
-    return *this;
-}
-
 PStream& PStream::operator<<(float x)
 {
     switch(outmode)
@@ -2118,12 +1863,7 @@
         put(' ');
         break;
     case plearn_binary:
-#ifdef BIGENDIAN
-        put((char)0x0F);
-#else
-        put((char)0x0E);
-#endif
-        write((char*)&x,sizeof(float));
+        writeBinaryNum(x);
         break;
     default:
         PLERROR("In PStream::operator<<  unknown outmode!!!!!!!!!");
@@ -2148,12 +1888,7 @@
         put(' ');
         break;
     case plearn_binary:
-#ifdef BIGENDIAN
-        put((char)0x11);
-#else
-        put((char)0x10);
-#endif
-        write((char*)&x,sizeof(double));
+        writeBinaryNum(x);
         break;
     default:
         PLERROR("In PStream::operator<<  unknown outmode!!!!!!!!!");
@@ -2183,6 +1918,7 @@
     }
 }
 
+/*
 #define IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(BASETYPE)  \
 void binread_(PStream& in, BASETYPE* x,                \
               unsigned int n, unsigned char typecode)  \
@@ -2206,16 +1942,72 @@
 
 // IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(char);
 IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(short)
-    IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(unsigned short)
-    IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(int)
-    IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(unsigned int)
-    //IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(long)
-    //IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(unsigned long)
-    IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(int64_t)
-    IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(uint64_t)
+IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(unsigned short)
+IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(int)
+IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(unsigned int)
+IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(long)
+IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(unsigned long)
+IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(long)
+IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(unsigned long)
+//IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(int64_t)
+//IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(uint64_t)
+*/
 
-//! The binread_ for float and double are special
+#define IMPLEMENT_NUMTYPE_BINREAD_(NUMTYPE)                                 \
+void binread_(PStream& in, NUMTYPE* x,                                      \
+              unsigned int n, unsigned char typecode)                       \
+{                                                                           \
+    /* If typecode corresponds to NUMTYPE's typecode */                     \
+    if (typecode==TypeTraits<NUMTYPE>::little_endian_typecode())            \
+    {                                                                       \
+        in.read(reinterpret_cast<char*>(x), streamsize(n*sizeof(NUMTYPE))); \
+        if (byte_order()==BIG_ENDIAN_ORDER)                                 \
+            endianswap(x,n);                                                \
+    }                                                                       \
+    else if (typecode==TypeTraits<NUMTYPE>::big_endian_typecode())          \
+    {                                                                       \
+        in.read(reinterpret_cast<char*>(x), streamsize(n*sizeof(NUMTYPE))); \
+        if (byte_order()==LITTLE_ENDIAN_ORDER)                              \
+            endianswap(x,n);                                                \
+    }                                                                       \
+    /* Else, we need to try all the different types */                      \
+    else if (typecode==TypeTraits<int8_t>::little_endian_typecode())        \
+        binread_as<int8_t>(in, x, n, false);                                \
+    else if (typecode==TypeTraits<uint8_t>::little_endian_typecode())       \
+        binread_as<uint8_t>(in, x, n, false);                               \
+    else if (typecode==TypeTraits<int16_t>::little_endian_typecode())       \
+        binread_as<int16_t>(in, x, n, false);                               \
+    else if (typecode==TypeTraits<uint16_t>::little_endian_typecode())      \
+        binread_as<uint16_t>(in, x, n, false);                              \
+    else if (typecode==TypeTraits<int32_t>::little_endian_typecode())       \
+        binread_as<int32_t>(in, x, n, false);                               \
+    else if (typecode==TypeTraits<uint32_t>::little_endian_typecode())      \
+        binread_as<uint32_t>(in, x, n, false);                              \
+    else if (typecode==TypeTraits<int64_t>::little_endian_typecode())       \
+        binread_as<int64_t>(in, x, n, false);                               \
+    else if (typecode==TypeTraits<uint64_t>::little_endian_typecode())      \
+        binread_as<uint64_t>(in, x, n, false);                              \
+    else if (typecode==TypeTraits<float>::little_endian_typecode())         \
+        binread_as<float>(in, x, n, false);                                 \
+    else if (typecode==TypeTraits<double>::little_endian_typecode())        \
+        binread_as<double>(in, x, n, false);                                \
+    else                                                                    \
+        PLERROR("In binread_: Unknown typecode '%c' (%x)",                  \
+                typecode, typecode);                                        \
+}
 
+IMPLEMENT_NUMTYPE_BINREAD_(short)
+IMPLEMENT_NUMTYPE_BINREAD_(unsigned short)
+IMPLEMENT_NUMTYPE_BINREAD_(int)
+IMPLEMENT_NUMTYPE_BINREAD_(unsigned int)
+IMPLEMENT_NUMTYPE_BINREAD_(long)
+IMPLEMENT_NUMTYPE_BINREAD_(unsigned long)
+IMPLEMENT_NUMTYPE_BINREAD_(long long)
+IMPLEMENT_NUMTYPE_BINREAD_(unsigned long long)
+IMPLEMENT_NUMTYPE_BINREAD_(float)
+IMPLEMENT_NUMTYPE_BINREAD_(double)
+
+/*
 void binread_(PStream& in, double* x, unsigned int n, unsigned char typecode)
 {
     if(typecode==TypeTraits<double>::little_endian_typecode())
@@ -2305,6 +2097,7 @@
     else
         PLERROR("In binread_ incompatible typecode");
 }
+*/
 
 } //end of namespace PLearn
 

Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2007-07-27 02:26:52 UTC (rev 7838)
+++ trunk/plearn/io/PStream.h	2007-07-27 02:30:10 UTC (rev 7839)
@@ -195,7 +195,7 @@
     inline void clearInMap()  { copies_map_in.clear(); }
     inline void clearInOutMaps()  { clearInMap(); clearOutMap(); }
 
-    //! if outmode is raw_ascii or raw_binary t will be switched to
+    //! if outmode is raw_ascii or raw_binary, it will be switched to
     //! corresponding plearn_ascii, resp. plearn_binary.
     //! The old mode will be returned, so that you can call setOutMode
     //! to revert to the old mode when finished
@@ -246,6 +246,92 @@
     //! Writes the corresponding 2 hex digits (ex: 0A )
     void writeAsciiHexNum(unsigned char x);
 
+    //! Writes base types to PLearn binary format
+    //! TODO: forbid this mechanism for arbitrary classes?
+    template<class I>
+    void writeBinaryNum(I x)
+    {
+#ifdef BIGENDIAN
+        put(TypeTraits<I>::big_endian_typecode());
+#else
+        put(TypeTraits<I>::little_endian_typecode());
+#endif
+        write((char*)&x, sizeof(I));
+    }
+
+    //! Reads a J element, optionnally swaps its endianness, and returns an I
+    template<class I, class J>
+    void readBinaryNumAs(J& x, bool inverted_byte_order)
+    {
+        I y;
+        read(reinterpret_cast<char*>(&y), sizeof(I));
+        if (inverted_byte_order)
+            endianswap(&y);
+        x = y;
+    }
+
+    //! Reads base types from PLearn binary format
+    //! TODO: forbid this mechanism for arbitrary classes?
+    template<class I>
+    void readBinaryNum(I &x, unsigned char typecode)
+    {
+        // If typecode corresponds to I's typecode
+        if (typecode==TypeTraits<I>::little_endian_typecode())
+        {
+            read(reinterpret_cast<char*>(&x), sizeof(I));
+#ifdef BIGENDIAN
+            endianswap(&x);
+#endif
+        }
+        else if (typecode==TypeTraits<I>::big_endian_typecode())
+        {
+            read(reinterpret_cast<char*>(&x), sizeof(I));
+#ifdef LITTLEENDIAN
+            endianswap(&x);
+#endif
+        }
+        // Else, we need to try all the different types
+        else if (typecode==TypeTraits<int8_t>::little_endian_typecode())
+            readBinaryNumAs<int8_t>(x, false);
+        else if (typecode==TypeTraits<uint8_t>::little_endian_typecode())
+            readBinaryNumAs<uint8_t>(x, false);
+        else if (typecode==TypeTraits<int16_t>::little_endian_typecode())
+            readBinaryNumAs<int16_t>(x, byte_order()==BIG_ENDIAN_ORDER);
+        else if (typecode==TypeTraits<int16_t>::big_endian_typecode())
+            readBinaryNumAs<int16_t>(x, byte_order()==LITTLE_ENDIAN_ORDER);
+        else if (typecode==TypeTraits<uint16_t>::little_endian_typecode())
+            readBinaryNumAs<uint16_t>(x, byte_order()==BIG_ENDIAN_ORDER);
+        else if (typecode==TypeTraits<uint16_t>::big_endian_typecode())
+            readBinaryNumAs<uint16_t>(x, byte_order()==LITTLE_ENDIAN_ORDER);
+        else if (typecode==TypeTraits<int32_t>::little_endian_typecode())
+            readBinaryNumAs<int32_t>(x, byte_order()==BIG_ENDIAN_ORDER);
+        else if (typecode==TypeTraits<int32_t>::big_endian_typecode())
+            readBinaryNumAs<int32_t>(x, byte_order()==LITTLE_ENDIAN_ORDER);
+        else if (typecode==TypeTraits<uint32_t>::little_endian_typecode())
+            readBinaryNumAs<uint32_t>(x, byte_order()==BIG_ENDIAN_ORDER);
+        else if (typecode==TypeTraits<uint32_t>::big_endian_typecode())
+            readBinaryNumAs<uint32_t>(x, byte_order()==LITTLE_ENDIAN_ORDER);
+        else if (typecode==TypeTraits<int64_t>::little_endian_typecode())
+            readBinaryNumAs<int64_t>(x, byte_order()==BIG_ENDIAN_ORDER);
+        else if (typecode==TypeTraits<int64_t>::big_endian_typecode())
+            readBinaryNumAs<int64_t>(x, byte_order()==LITTLE_ENDIAN_ORDER);
+        else if (typecode==TypeTraits<uint64_t>::little_endian_typecode())
+            readBinaryNumAs<uint64_t>(x, byte_order()==BIG_ENDIAN_ORDER);
+        else if (typecode==TypeTraits<uint64_t>::big_endian_typecode())
+            readBinaryNumAs<uint64_t>(x, byte_order()==LITTLE_ENDIAN_ORDER);
+        else if (typecode==TypeTraits<float>::little_endian_typecode())
+            readBinaryNumAs<float>(x, byte_order()==BIG_ENDIAN_ORDER);
+        else if (typecode==TypeTraits<float>::big_endian_typecode())
+            readBinaryNumAs<float>(x, byte_order()==LITTLE_ENDIAN_ORDER);
+        else if (typecode==TypeTraits<double>::little_endian_typecode())
+            readBinaryNumAs<double>(x, byte_order()==BIG_ENDIAN_ORDER);
+        else if (typecode==TypeTraits<double>::big_endian_typecode())
+            readBinaryNumAs<double>(x, byte_order()==LITTLE_ENDIAN_ORDER);
+        else
+            PLERROR("In PStream readBinaryNum: Unknown typecode '%c' (%x)",
+                    typecode, typecode);
+    }
+
     inline bool eof() const
     { return ptr->eof(); }
 
@@ -459,7 +545,6 @@
     int count(char c);
 
     // operator>>'s for base types
-    PStream& operator>>(bool &x);
     PStream& operator>>(float &x);
     PStream& operator>>(double &x);
     PStream& operator>>(string &x);
@@ -467,16 +552,15 @@
     PStream& operator>>(char &x);
     PStream& operator>>(signed char &x);
     PStream& operator>>(unsigned char &x);
+    PStream& operator>>(bool &x);
+    PStream& operator>>(short &x);
+    PStream& operator>>(unsigned short &x);
     PStream& operator>>(int &x);
     PStream& operator>>(unsigned int &x);
-    //PStream& operator>>(long &x);
-    //PStream& operator>>(unsigned long &x);
-    PStream& operator>>(int64_t &x);
-    PStream& operator>>(uint64_t &x);
-    PStream& operator>>(short &x);
-    PStream& operator>>(unsigned short &x);
-    //PStream& operator>>(long long &x);
-    //PStream& operator>>(unsigned long long &x);
+    PStream& operator>>(long &x);
+    PStream& operator>>(unsigned long &x);
+    PStream& operator>>(long long &x);
+    PStream& operator>>(unsigned long long &x);
     PStream& operator>>(pl_pstream_manip func) { return (*func)(*this); }
 
     // operator<<'s for base types
@@ -502,16 +586,14 @@
     // Note: If you get mysterious mesages of problems with const bool resolutions,
     // then a workaround might be to not declare <<(bool) as a method, but as an inline function
     PStream& operator<<(bool x);
+    PStream& operator<<(short x);
+    PStream& operator<<(unsigned short x);
     PStream& operator<<(int x);
     PStream& operator<<(unsigned int x);
-    //PStream& operator<<(long x);
-    //PStream& operator<<(unsigned long x);
-    PStream& operator<<(int64_t x);
-    PStream& operator<<(uint64_t x);
-    //PStream& operator<<(long long x);
-    //PStream& operator<<(unsigned long long x);
-    PStream& operator<<(short x);
-    PStream& operator<<(unsigned short x);
+    PStream& operator<<(long x);
+    PStream& operator<<(unsigned long x);
+    PStream& operator<<(long long x);
+    PStream& operator<<(unsigned long long x);
     PStream& operator<<(pl_pstream_manip func) { return (*func)(*this); }
 
 };
@@ -895,7 +977,6 @@
 inline void binwrite_(PStream& out, unsigned int* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(unsigned int))); }
 
-/*
 inline void binwrite_(PStream& out, const long* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(long))); }
 inline void binwrite_(PStream& out, long* x, unsigned int n)
@@ -905,17 +986,16 @@
 { out.write((char*)x, streamsize(n*sizeof(unsigned long))); }
 inline void binwrite_(PStream& out, unsigned long* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(unsigned long))); }
-*/
 
-inline void binwrite_(PStream& out, const int64_t* x, unsigned int n)
-{ out.write((char*)x, streamsize(n*sizeof(int64_t))); }
-inline void binwrite_(PStream& out, int64_t* x, unsigned int n)
-{ out.write((char*)x, streamsize(n*sizeof(int64_t))); }
+inline void binwrite_(PStream& out, const long long* x, unsigned int n)
+{ out.write((char*)x, streamsize(n*sizeof(long long))); }
+inline void binwrite_(PStream& out, long long* x, unsigned int n)
+{ out.write((char*)x, streamsize(n*sizeof(long long))); }
 
-inline void binwrite_(PStream& out, const uint64_t* x, unsigned int n)
-{ out.write((char*)x, streamsize(n*sizeof(uint64_t))); }
-inline void binwrite_(PStream& out, uint64_t* x, unsigned int n)
-{ out.write((char*)x, streamsize(n*sizeof(uint64_t))); }
+inline void binwrite_(PStream& out, const unsigned long long* x, unsigned int n)
+{ out.write((char*)x, streamsize(n*sizeof(unsigned long long))); }
+inline void binwrite_(PStream& out, unsigned long long* x, unsigned int n)
+{ out.write((char*)x, streamsize(n*sizeof(unsigned long long))); }
 
 inline void binwrite_(PStream& out, const float* x, unsigned int n)
 { out.write((char*)x, streamsize(n*sizeof(float))); }
@@ -942,6 +1022,22 @@
     }
 }
 
+
+//! Auxiliary function that reads n elements of type I, optionally swaps
+//! their endianness, then converts them into J, and puts them in a J array.
+template<class I, class J>
+void binread_as(PStream& in, J* x, unsigned int n, bool inverted_byte_order)
+{
+    I y;
+    while(n--)
+    {
+        in.read(reinterpret_cast<char*>(&y), sizeof(I));
+        if (inverted_byte_order)
+            endianswap(&y);
+        *x++ = y;
+    }
+}
+
 void binread_(PStream& in, bool* x, unsigned int n, unsigned char typecode);
 
 inline void binread_(PStream& in, char* x,
@@ -967,10 +1063,10 @@
 void binread_(PStream& in, unsigned short* x, unsigned int n, unsigned char typecode);
 void binread_(PStream& in, int* x, unsigned int n, unsigned char typecode);
 void binread_(PStream& in, unsigned int* x, unsigned int n, unsigned char typecode);
-//void binread_(PStream& in, long* x, unsigned int n, unsigned char typecode);
-//void binread_(PStream& in, unsigned long* x, unsigned int n, unsigned char typecode);
-void binread_(PStream& in, int64_t* x, unsigned int n, unsigned char typecode);
-void binread_(PStream& in, uint64_t* x, unsigned int n, unsigned char typecode);
+void binread_(PStream& in, long* x, unsigned int n, unsigned char typecode);
+void binread_(PStream& in, unsigned long* x, unsigned int n, unsigned char typecode);
+void binread_(PStream& in, long long* x, unsigned int n, unsigned char typecode);
+void binread_(PStream& in, unsigned long long* x, unsigned int n, unsigned char typecode);
 void binread_(PStream& in, float* x, unsigned int n, unsigned char typecode);
 void binread_(PStream& in, double* x, unsigned int n, unsigned char typecode);
 
@@ -979,7 +1075,7 @@
 void writeSequence(PStream& out, const SequenceType& seq)
 {
     // norman: added explicit cast
-    unsigned int n = (unsigned int)seq.size();
+    uint32_t n = (uint32_t)seq.size();
     typename SequenceType::const_iterator it = seq.begin();
 
     switch(out.outmode)



From lamblin at mail.berlios.de  Fri Jul 27 05:03:00 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 27 Jul 2007 05:03:00 +0200
Subject: [Plearn-commits] r7840 - in trunk/plearn: base io
Message-ID: <200707270303.l6R330Ei001978@sheep.berlios.de>

Author: lamblin
Date: 2007-07-27 05:02:59 +0200 (Fri, 27 Jul 2007)
New Revision: 7840

Added:
   trunk/plearn/base/pl_stdint.h
Modified:
   trunk/plearn/base/TypeTraits.h
   trunk/plearn/io/PStream.h
Log:
Isolates the injection of [u]int..._t types from namespace boost to PLearn.


Modified: trunk/plearn/base/TypeTraits.h
===================================================================
--- trunk/plearn/base/TypeTraits.h	2007-07-27 02:30:10 UTC (rev 7839)
+++ trunk/plearn/base/TypeTraits.h	2007-07-27 03:02:59 UTC (rev 7840)
@@ -51,6 +51,7 @@
 #include <map>
 #include <set>
 #include <nspr/prlong.h>
+#include <plearn/base/pl_stdint.h>
 
 namespace PLearn {
 using std::string;

Added: trunk/plearn/base/pl_stdint.h
===================================================================
--- trunk/plearn/base/pl_stdint.h	2007-07-27 02:30:10 UTC (rev 7839)
+++ trunk/plearn/base/pl_stdint.h	2007-07-27 03:02:59 UTC (rev 7840)
@@ -0,0 +1,73 @@
+// -*- C++ -*-
+
+// pl_stdint.h
+//
+// Copyright (C) 2007 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file pl_stdint.h */
+
+
+#ifndef pl_stdint_INC
+#define pl_stdint_INC
+
+#include <boost/cstdint.hpp>
+
+namespace PLearn {
+
+// inject principal boost integer types into namespace PLearn
+// @TODO: use them all?
+using boost::int8_t;
+using boost::uint8_t;
+using boost::int16_t;
+using boost::uint16_t;
+using boost::int32_t;
+using boost::uint32_t;
+using boost::int64_t;
+using boost::uint64_t;
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2007-07-27 02:30:10 UTC (rev 7839)
+++ trunk/plearn/io/PStream.h	2007-07-27 03:02:59 UTC (rev 7840)
@@ -41,10 +41,10 @@
 #include <set>
 #include <sstream>
 #include <fstream>
-#include <boost/cstdint.hpp>
 #include <plearn/base/byte_order.h>
 #include <plearn/base/pl_hash_fun.h>
 #include <plearn/base/plerror.h>
+#include <plearn/base/pl_stdint.h>
 #include "PStream_util.h"
 #include "PStreamBuf.h"
 #include "StdPStreamBuf.h"
@@ -53,17 +53,6 @@
 
 using namespace std;
 
-// inject principal boost integer types into namespace PLearn
-// @TODO: use them all?
-using boost::int8_t;
-using boost::uint8_t;
-using boost::int16_t;
-using boost::uint16_t;
-using boost::int32_t;
-using boost::uint32_t;
-using boost::int64_t;
-using boost::uint64_t;
-
 /*!
  * PStream:
  *  This class defines a type of stream that should be used for all I/O within PLearn.



From lamblin at mail.berlios.de  Fri Jul 27 05:12:56 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 27 Jul 2007 05:12:56 +0200
Subject: [Plearn-commits] r7841 - trunk/plearn/io
Message-ID: <200707270312.l6R3CugG002342@sheep.berlios.de>

Author: lamblin
Date: 2007-07-27 05:12:55 +0200 (Fri, 27 Jul 2007)
New Revision: 7841

Modified:
   trunk/plearn/io/PStream.h
Log:
Removes some warnings.


Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2007-07-27 03:02:59 UTC (rev 7840)
+++ trunk/plearn/io/PStream.h	2007-07-27 03:12:55 UTC (rev 7841)
@@ -1023,7 +1023,8 @@
         in.read(reinterpret_cast<char*>(&y), sizeof(I));
         if (inverted_byte_order)
             endianswap(&y);
-        *x++ = y;
+        *x = static_cast<I>(y);
+        ++x;
     }
 }
 



From lamblin at mail.berlios.de  Fri Jul 27 05:42:06 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 27 Jul 2007 05:42:06 +0200
Subject: [Plearn-commits] r7842 - in trunk/plearn: base io
Message-ID: <200707270342.l6R3g6HD003586@sheep.berlios.de>

Author: lamblin
Date: 2007-07-27 05:42:04 +0200 (Fri, 27 Jul 2007)
New Revision: 7842

Modified:
   trunk/plearn/base/TypeTraits.h
   trunk/plearn/base/byte_order.h
   trunk/plearn/io/PStream.cc
   trunk/plearn/io/PStream.h
Log:
Cosmetic changes.


Modified: trunk/plearn/base/TypeTraits.h
===================================================================
--- trunk/plearn/base/TypeTraits.h	2007-07-27 03:12:55 UTC (rev 7841)
+++ trunk/plearn/base/TypeTraits.h	2007-07-27 03:42:04 UTC (rev 7842)
@@ -5,18 +5,18 @@
 //
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
-// 
+//
 //  1. Redistributions of source code must retain the above copyright
 //     notice, this list of conditions and the following disclaimer.
-// 
+//
 //  2. Redistributions in binary form must reproduce the above copyright
 //     notice, this list of conditions and the following disclaimer in the
 //     documentation and/or other materials provided with the distribution.
-// 
+//
 //  3. The name of the authors may not be used to endorse or promote
 //     products derived from this software without specific prior written
 //     permission.
-// 
+//
 // THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
 // IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 // OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
@@ -27,13 +27,13 @@
 // LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 // NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 // SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-// 
+//
 // This file is part of the PLearn library. For more information on the PLearn
 // library, go to the PLearn Web site at www.plearn.org
 
 
- 
-/* *******************************************************      
+
+/* *******************************************************
  * $Id$
  * AUTHORS: Pascal Vincent
  * This file is part of the PLearn library.
@@ -65,11 +65,12 @@
  *  @li Its \c name(), returned as a string
  *
  *  @li The "typecode", which is the header-byte used to indicate type of an
- *      element to follow in plearn_binary serialization. \c little_endian_typecode()
- *      and \c big_endian_typecode() respectively return the code to designate
- *      little-endian or big-endian representation.  Only the very basic C++ types
- *      have specific typecodes. For all other more complex types, these functions
- *      should always return 0xFF.
+ *      element to follow in plearn_binary serialization.
+ *      \c little_endian_typecode() and \c big_endian_typecode() respectively
+ *      return the code to designate little-endian or big-endian
+ *      representation.  Only the very basic C++ types have specific typecodes.
+ *      For all other more complex types, these functions should always
+ *      return 0xFF.
  */
 template<class T>
 class TypeTraits
@@ -98,7 +99,7 @@
 class TypeTraits<T*>
 {
 public:
-    static inline string name() 
+    static inline string name()
     { return TypeTraits<T>::name()+"*"; }
 
     static inline unsigned char little_endian_typecode()
@@ -112,7 +113,7 @@
 class TypeTraits<T const>
 {
 public:
-    static inline string name() 
+    static inline string name()
     { return TypeTraits<T>::name()+" const"; }
 
     static inline unsigned char little_endian_typecode()
@@ -264,7 +265,7 @@
 {
 public:
     static inline string name()
-    { return string("vector< ") + TypeTraits<T>::name()+" >"; }
+    { return string("vector< ") + TypeTraits<T>::name() + " >"; }
 
     static inline unsigned char little_endian_typecode()
     { return 0xFF; }
@@ -278,7 +279,7 @@
 {
 public:
     static inline string name()
-    { return string("list< ") + TypeTraits<T>::name()+" >"; }
+    { return string("list< ") + TypeTraits<T>::name() + " >"; }
 
     static inline unsigned char little_endian_typecode()
     { return 0xFF; }
@@ -292,7 +293,10 @@
 {
 public:
     static inline string name()
-    { return string("pair< ") + TypeTraits<T>::name()+", " + TypeTraits<U>::name()+" >"; }
+    {
+        return string("pair< ") + TypeTraits<T>::name()+", "
+            + TypeTraits<U>::name() + " >";
+    }
 
     static inline unsigned char little_endian_typecode()
     { return 0xFF; }
@@ -306,7 +310,10 @@
 {
 public:
     static inline string name()
-    { return string("map< ") + TypeTraits<T>::name()+", " + TypeTraits<U>::name()+" >"; }
+    {
+        return string("map< ") + TypeTraits<T>::name()+", "
+            + TypeTraits<U>::name() + " >";
+    }
 
     static inline unsigned char little_endian_typecode()
     { return 0xFF; }
@@ -320,7 +327,7 @@
 {
 public:
     static inline string name()
-    { return string("set< ") + TypeTraits<T>::name()+" >"; }
+    { return string("set< ") + TypeTraits<T>::name() + " >"; }
 
     static inline unsigned char little_endian_typecode()
     { return 0xFF; }

Modified: trunk/plearn/base/byte_order.h
===================================================================
--- trunk/plearn/base/byte_order.h	2007-07-27 03:12:55 UTC (rev 7841)
+++ trunk/plearn/base/byte_order.h	2007-07-27 03:42:04 UTC (rev 7842)
@@ -5,18 +5,18 @@
 //
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
-// 
+//
 //  1. Redistributions of source code must retain the above copyright
 //     notice, this list of conditions and the following disclaimer.
-// 
+//
 //  2. Redistributions in binary form must reproduce the above copyright
 //     notice, this list of conditions and the following disclaimer in the
 //     documentation and/or other materials provided with the distribution.
-// 
+//
 //  3. The name of the authors may not be used to endorse or promote
 //     products derived from this software without specific prior written
 //     permission.
-// 
+//
 // THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
 // IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 // OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
@@ -27,7 +27,7 @@
 // LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 // NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 // SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-// 
+//
 // This file is part of the PLearn library. For more information on the PLearn
 // library, go to the PLearn Web site at www.plearn.org
 
@@ -36,7 +36,8 @@
 #define byte_order_INC
 
 // norman:
-// If I don't add a definition of the namespace std .NET it does not compile (weird..)!
+// If I don't add a definition of the namespace std .NET it does not compile
+// (weird..)!
 #ifdef WIN32
 #include <string>
 #endif
@@ -55,7 +56,7 @@
 inline char byte_order()
 {
 #ifdef BIGENDIAN
-    return BIG_ENDIAN_ORDER; 
+    return BIG_ENDIAN_ORDER;
 #else
     return LITTLE_ENDIAN_ORDER;
 #endif
@@ -69,7 +70,8 @@
 //! swaps endians for n 8-byte elements (such as double)
 void endianswap8(void* ptr, int n);
 
-//! calls endianswap2,4, or 8 depending on elemsize (an elemsize of 1 is also valid and does nothing)
+//! calls endianswap2, 4, or 8 depending on elemsize
+//! (an elemsize of 1 is also valid and does nothing)
 inline void endianswap(void* ptr, int nelems, int elemsize)
 {
     switch(elemsize)
@@ -91,23 +93,6 @@
 }
 
 
-/*
-// Version for char and unsigned char (I know this is useless, but some code relies on it being defined)
-inline void endianswap(char* ptr, int n=1) {}
-inline void endianswap(signed char* ptr, int n=1) {}
-inline void endianswap(unsigned char* ptr, int n=1) {}
-
-// Versions for short, int, long, float and double
-inline void endianswap(short* ptr, int n=1) { endianswap(ptr,n,sizeof(short)); }
-inline void endianswap(unsigned short* ptr, int n=1) { endianswap(ptr,n,sizeof(unsigned short)); }
-inline void endianswap(int* ptr, int n=1) { endianswap(ptr,n,sizeof(int)); }
-inline void endianswap(unsigned int* ptr, int n=1) { endianswap(ptr,n,sizeof(unsigned int)); }
-inline void endianswap(long* ptr, int n=1) { endianswap(ptr,n,sizeof(long)); }
-inline void endianswap(unsigned long* ptr, int n=1) { endianswap(ptr,n,sizeof(unsigned long)); }
-inline void endianswap(float* ptr, int n=1) { endianswap(ptr,n,sizeof(float)); }
-inline void endianswap(double* ptr, int n=1) { endianswap(ptr,n,sizeof(double)); }
-*/
-
 template<class T>
 inline void endianswap(T* ptr, int n=1)
 { endianswap(ptr,n,sizeof(T)); }

Modified: trunk/plearn/io/PStream.cc
===================================================================
--- trunk/plearn/io/PStream.cc	2007-07-27 03:12:55 UTC (rev 7841)
+++ trunk/plearn/io/PStream.cc	2007-07-27 03:42:04 UTC (rev 7842)
@@ -2,7 +2,8 @@
 
 // PStream.cc
 // Copyright (C) 1998 Pascal Vincent
-// Copyright (C) 1999-2001 Pascal Vincent, Yoshua Bengio and University of Montreal
+// Copyright (C) 1999-2001 Pascal Vincent, Yoshua Bengio and
+//  University of Montreal
 // Copyright (C) 2002 Frederic Morin, Xavier Saint-Mleux, Pascal Vincent
 // Copyright (C) 2007 Xavier Saint-Mleux, ApSTAT Technologies inc.
 //
@@ -1918,41 +1919,6 @@
     }
 }
 
-/*
-#define IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(BASETYPE)  \
-void binread_(PStream& in, BASETYPE* x,                \
-              unsigned int n, unsigned char typecode)  \
-{                                                      \
-  if(typecode==TypeTraits<BASETYPE>::little_endian_typecode()) \
-    {                                                  \
-      in.read((char*)x, streamsize(n*sizeof(BASETYPE)));       \
-      if(byte_order()==BIG_ENDIAN_ORDER)               \
-        endianswap(x,n);                               \
-    }                                                  \
-  else if(typecode==TypeTraits<BASETYPE>::big_endian_typecode()) \
-    {                                                  \
-      in.read((char*)x, streamsize(n*sizeof(BASETYPE)));       \
-      if(byte_order()==LITTLE_ENDIAN_ORDER)            \
-        endianswap(x,n);                               \
-    }                                                  \
-  else                                                 \
-    PLERROR("In binread_ incompatible typecode");      \
-}
-
-
-// IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(char);
-IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(short)
-IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(unsigned short)
-IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(int)
-IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(unsigned int)
-IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(long)
-IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(unsigned long)
-IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(long)
-IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(unsigned long)
-//IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(int64_t)
-//IMPLEMENT_TYPICAL_BASETYPE_BINREAD_(uint64_t)
-*/
-
 #define IMPLEMENT_NUMTYPE_BINREAD_(NUMTYPE)                                 \
 void binread_(PStream& in, NUMTYPE* x,                                      \
               unsigned int n, unsigned char typecode)                       \
@@ -2007,98 +1973,6 @@
 IMPLEMENT_NUMTYPE_BINREAD_(float)
 IMPLEMENT_NUMTYPE_BINREAD_(double)
 
-/*
-void binread_(PStream& in, double* x, unsigned int n, unsigned char typecode)
-{
-    if(typecode==TypeTraits<double>::little_endian_typecode())
-    {
-        in.read((char*)x, streamsize(n*sizeof(double)));
-#ifdef BIGENDIAN
-        endianswap(x,n);
-#endif
-    }
-    else if(typecode==TypeTraits<double>::big_endian_typecode())
-    {
-        in.read((char*)x, streamsize(n*sizeof(double)));
-#ifdef LITTLEENDIAN
-        endianswap(x,n);
-#endif
-    }
-    else if(typecode==TypeTraits<float>::little_endian_typecode())
-    {
-        float val;
-        while(n--)
-        {
-            in.read((char*)&val, sizeof(float));
-#ifdef BIGENDIAN
-            endianswap(&val);
-#endif
-            *x++ = double(val);
-        }
-    }
-    else if(typecode==TypeTraits<float>::big_endian_typecode())
-    {
-        float val;
-        while(n--)
-        {
-            in.read((char*)&val, sizeof(float));
-#ifdef LITTLEENDIAN
-            endianswap(&val);
-#endif
-            *x++ = double(val);
-        }
-    }
-    else
-        PLERROR("In binread_ incompatible typecode");
-}
-
-
-
-void binread_(PStream& in, float* x, unsigned int n, unsigned char typecode)
-{
-    if(typecode==TypeTraits<float>::little_endian_typecode())
-    {
-        in.read((char*)x, streamsize(n*sizeof(float)));
-#ifdef BIGENDIAN
-        endianswap(x,n);
-#endif
-    }
-    else if(typecode==TypeTraits<float>::big_endian_typecode())
-    {
-        in.read((char*)x, streamsize(n*sizeof(float)));
-#ifdef LITTLEENDIAN
-        endianswap(x,n);
-#endif
-    }
-    else if(typecode==TypeTraits<double>::little_endian_typecode())
-    {
-        double val;
-        while(n--)
-        {
-            in.read((char*)&val, sizeof(double));
-#ifdef BIGENDIAN
-            endianswap(&val);
-#endif
-            *x++ = float(val);
-        }
-    }
-    else if(typecode==TypeTraits<double>::big_endian_typecode())
-    {
-        double val;
-        while(n--)
-        {
-            in.read((char*)&val, sizeof(double));
-#ifdef LITTLEENDIAN
-            endianswap(&val);
-#endif
-            *x++ = float(val);
-        }
-    }
-    else
-        PLERROR("In binread_ incompatible typecode");
-}
-*/
-
 } //end of namespace PLearn
 
 

Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2007-07-27 03:12:55 UTC (rev 7841)
+++ trunk/plearn/io/PStream.h	2007-07-27 03:42:04 UTC (rev 7842)
@@ -2,7 +2,8 @@
 
 // PStream.h
 // Copyright (C) 1998 Pascal Vincent
-// Copyright (C) 1999-2001 Pascal Vincent, Yoshua Bengio and University of Montreal
+// Copyright (C) 1999-2001 Pascal Vincent, Yoshua Bengio and
+//  University of Montreal
 // Copyright (C) 2002 Frederic Morin, Xavier Saint-Mleux, Pascal Vincent
 // Copyright (C) 2007 Xavier Saint-Mleux, ApSTAT Technologies inc.
 //
@@ -55,17 +56,19 @@
 
 /*!
  * PStream:
- *  This class defines a type of stream that should be used for all I/O within PLearn.
- *  It supports most operations available for standard c++ streams, plus:
- *   - a set of option flags that indicate which types of Object Options should be
- *     read/written (option_flags_{in|out}; has effect only for Object's);
- *   - a set of mode flags that define format used for I/O. e.g.:  "raw_ascii" for
- *     standard c++ stream behaviour, "plearn_ascii"  for human-readable
- *     serialization format, etc. (inmode and outmode);
+ *  This class defines a type of stream that should be used for all I/O within
+ *  PLearn.
+ *  It supports most operations available for standard C++ streams, plus:
+ *   - a set of option flags that indicate which types of Object Options should
+ *     be read/written (option_flags_{in|out}; has effect only for Object's);
+ *   - a set of mode flags that define format used for I/O. e.g.:
+ *      "raw_ascii" for  standard C++ stream behaviour,
+ *      "plearn_ascii"  for human-readable serialization format,
+ *      etc. (inmode and outmode);
  *   - a copies map to allow smart serialization of pointers;
- *   - a markable stream buffer which allows to 'seek back' to a previously set mark
- *     on any type of istream;
- *   - an 'attach' function to attach the stream to a POSIX file descriptor;
+ *   - a markable stream buffer which allows to 'seek back' to a previously set
+ *     mark on any type of istream;
+ *   - an 'attach' function to attach the stream to a POSIX file descriptor.
  */
 
 class PStream : public PP<PStreamBuf>
@@ -95,31 +98,54 @@
      */
     enum mode_t
     {
-        plearn_ascii,    //!< PLearn ascii serialization format (can be mixed with plearn_binary)
-        plearn_binary,   //!< PLearn binary serialization format (can be mixed with plearn_ascii)
-        raw_ascii,       //!< Raw C++ ascii output without additional separators (direct output to underlying ostream)
-        raw_binary,      //!< Simply writes the bytes as they are in memory.
-        pretty_ascii     //!< Ascii pretty print (in particular for Vec and Mat, formatted output without size info)
+        //! PLearn ascii serialization format (can be mixed with plearn_binary)
+        plearn_ascii,
+        //! PLearn binary serialization format (can be mixed with plearn_ascii)
+        plearn_binary,
+        //! Raw C++ ascii output without additional separators
+        //! (direct output to underlying ostream)
+        raw_ascii,
+        //! Simply writes the bytes as they are in memory.
+        raw_binary,
+        //! Ascii pretty print (in particular for Vec and Mat, formatted output
+        //! without size info)
+        pretty_ascii
     };
 
 
-    //! Compression mode (mostly used by binary serialization of sequences of floats or doubles, such as TMat<real>)
+    //! Compression mode (mostly used by binary serialization of sequences of
+    //! floats or doubles, such as TMat<real>)
     //! (Used on output only; autodetect on read).
     enum compr_mode_t {
-        compr_none,            //!< No compression.
-        compr_double_as_float, //!< In plearn_binary mode, store doubles as float
-        compr_sparse,          //!< PLearn
-        compr_lossy_sparse     //!< Also stores double as float
+        //! No compression.
+        compr_none,
+        //! In plearn_binary mode, store doubles as float
+        compr_double_as_float,
+        //! PLearn
+        compr_sparse,
+        //! Also stores double as float
+        compr_lossy_sparse
     };
 
 public:
-    mode_t inmode;              //!< mode for input formatting
-    // bitset<32> pl_stream_flags_in;  //!< format flags for input
-    map<unsigned int, void *> copies_map_in; //!< copies map for input
-    mode_t outmode;            //!< mode for output formatting
-    // bitset<32> pl_stream_flags_out; //!< format flags for output
-    map<void *, unsigned int> copies_map_out; //!< copies map for output
+    //! mode for input formatting
+    mode_t inmode;
 
+    //! format flags for input
+    // bitset<32> pl_stream_flags_in;
+
+    //! copies map for input
+    map<unsigned int, void *> copies_map_in;
+
+    //! mode for output formatting
+    mode_t outmode;
+
+    //! format flags for output
+    // bitset<32> pl_stream_flags_out;
+
+    //! copies map for output
+    map<void *, unsigned int> copies_map_out;
+
 private:
     //! Buffer for some formatting operations
     static char tmpbuf[100];
@@ -137,16 +163,16 @@
     static const char* format_double_default;
 
 public:
-    //! If true, then Mat and Vec will be serialized with their elements in place,
+    //! If true, then Mat and Vec will be serialized with their elements in
+    //! place,
     //! If false, they will have an explicit pointer to a storage
     bool implicit_storage;
 
     //! Determines the way data is compressed, if any.
     compr_mode_t compression_mode;
 
-    //! Should be true if this stream is used to communicate
-    //! with a remote PLearn host.  Will serialize options
-    //! accordingly.
+    //! Should be true if this stream is used to communicate with a remote
+    //! PLearn host.  Will serialize options accordingly.
     bool remote_plearn_comm;
 
 public:
@@ -166,39 +192,80 @@
     PStream(iostream* pios_, bool own_pios_=false);
 
     //! ctor. from an istream and an ostream (IO)
-    PStream(istream* pin_, ostream* pout_, bool own_pin_=false, bool own_pout_=false);
+    PStream(istream* pin_, ostream* pout_, bool own_pin_=false,
+            bool own_pout_=false);
 
     //! Default copy ctor. should be fine now.
 
     //! Destructor.
     virtual ~PStream();
 
-    inline void setBufferCapacities(streamsize inbuf_capacity, streamsize outbuf_capacity, streamsize unget_capacity)
-    { ptr->setBufferCapacities(inbuf_capacity, outbuf_capacity, unget_capacity); }
+    inline void setBufferCapacities(streamsize inbuf_capacity,
+                                    streamsize outbuf_capacity,
+                                    streamsize unget_capacity)
+    {
+        ptr->setBufferCapacities(inbuf_capacity,
+                                 outbuf_capacity,
+                                 unget_capacity);
+    }
 
-    inline mode_t setInMode(mode_t m) { mode_t oldmode = inmode; inmode = m; return oldmode; }
-    inline mode_t setOutMode(mode_t m) { mode_t oldmode = outmode; outmode = m; return oldmode; }
-    inline void setMode(mode_t m) { inmode = m; outmode = m; }
+    inline mode_t setInMode(mode_t m)
+    {
+        mode_t oldmode = inmode;
+        inmode = m;
+        return oldmode;
+    }
 
-    inline void clearOutMap() { copies_map_out.clear(); }
-    inline void clearInMap()  { copies_map_in.clear(); }
-    inline void clearInOutMaps()  { clearInMap(); clearOutMap(); }
+    inline mode_t setOutMode(mode_t m)
+    {
+        mode_t oldmode = outmode;
+        outmode = m;
+        return oldmode;
+    }
 
+    inline void setMode(mode_t m)
+    { inmode = m; outmode = m; }
+
+    inline void clearOutMap()
+    { copies_map_out.clear(); }
+
+    inline void clearInMap()
+    { copies_map_in.clear(); }
+
+    inline void clearInOutMaps()
+    {
+        clearInMap();
+        clearOutMap();
+    }
+
     //! if outmode is raw_ascii or raw_binary, it will be switched to
     //! corresponding plearn_ascii, resp. plearn_binary.
     //! The old mode will be returned, so that you can call setOutMode
     //! to revert to the old mode when finished
     mode_t switchToPLearnOutMode();
 
-    PStream& operator>>(mode_t m) { inmode = m; return *this; }
-    PStream& operator<<(mode_t m) { outmode = m; return *this; }
+    PStream& operator>>(mode_t m)
+    {
+        inmode = m;
+        return *this;
+    }
 
+    PStream& operator<<(mode_t m)
+    {
+        outmode = m;
+        return *this;
+    }
+
 public:
     //op()'s: re-init with different underlying stream(s)
 
     PStream& operator=(const PStream& pios);
+
     PStream& operator=(streambuftype* streambuf)
-    { inherited::operator=(streambuf); return *this; }
+    {
+        inherited::operator=(streambuf);
+        return *this;
+    }
 
     bool operator==(const PStream& other)
     { return PP<PStreamBuf>::operator==(other); }
@@ -490,7 +557,8 @@
         return *this;
     }
 
-    // These are convenient method for writing raw strings (whatever the outmode):
+    // These are convenient method for writing raw strings
+    // (whatever the outmode):
     inline PStream& write(const char* s)
     {
         write(s, streamsize(strlen(s)));
@@ -520,15 +588,21 @@
     //! skips all occurences of any of the given characters
     void skipAll(const char* chars_to_skip);
 
-    //! Reads characters from stream, until we meet one of the stopping symbols at the current "level".
-    //! i.e. any opening parenthesis, bracket, brace or quote will open a next level and we'll
-    //! be back to the current level only *after* we meet the corresponding closing parenthesis,
-    //! bracket, brace or quote.
-    //! All characters read, except the stoppingsymbol, will be *appended* to characters_read
-    //! The stoppingsymbol is read and returned, but not appended to characters_read.
-    //! Comments starting with # until the end of line may be skipped (as if they were not part of the stream)
-    int smartReadUntilNext(const string& stoppingsymbols, string& characters_read,
-                           bool ignore_brackets=false, bool skip_comments=true);
+    //! Reads characters from stream, until we meet one of the stopping symbols
+    //! at the current "level". i.e. any opening parenthesis, bracket, brace or
+    //! quote will open a next level and we'll be back to the current level
+    //! only *after* we meet the corresponding closing parenthesis, bracket,
+    //! brace or quote.
+    //! All characters read, except the stoppingsymbol, will be *appended* to
+    //! characters_read.
+    //! The stoppingsymbol is read and returned, but not appended to
+    //! characters_read.
+    //! Comments starting with # until the end of line may be skipped (as if
+    //! they were not part of the stream)
+    int smartReadUntilNext(const string& stoppingsymbols,
+                           string& characters_read,
+                           bool ignore_brackets=false,
+                           bool skip_comments=true);
 
     //! Count the number of occurrences of a character in the stream.
     int count(char c);
@@ -537,7 +611,8 @@
     PStream& operator>>(float &x);
     PStream& operator>>(double &x);
     PStream& operator>>(string &x);
-    PStream& operator>>(char* x); // read string in already allocated char[]
+    // read string in already allocated char[]
+    PStream& operator>>(char* x);
     PStream& operator>>(char &x);
     PStream& operator>>(signed char &x);
     PStream& operator>>(unsigned char &x);
@@ -550,7 +625,8 @@
     PStream& operator>>(unsigned long &x);
     PStream& operator>>(long long &x);
     PStream& operator>>(unsigned long long &x);
-    PStream& operator>>(pl_pstream_manip func) { return (*func)(*this); }
+    PStream& operator>>(pl_pstream_manip func)
+    { return (*func)(*this); }
 
     // operator<<'s for base types
     PStream& operator<<(float x);
@@ -572,8 +648,9 @@
     PStream& operator<<(signed char x);
     PStream& operator<<(unsigned char x);
 
-    // Note: If you get mysterious mesages of problems with const bool resolutions,
-    // then a workaround might be to not declare <<(bool) as a method, but as an inline function
+    // Note: If you get mysterious mesages of problems with const bool
+    // resolutions, then a workaround might be to not declare <<(bool) as a
+    // method, but as an inline function
     PStream& operator<<(bool x);
     PStream& operator<<(short x);
     PStream& operator<<(unsigned short x);
@@ -631,7 +708,8 @@
         in.get(); // Eat '*'
         unsigned int id;
         in >> id;
-        //don't skip blanks before we need to read something else (read might block).
+        // don't skip blanks before we need to read something else
+        // (read might block).
         //in.skipBlanksAndCommentsAndSeparators();
         if (id==0)
             x = 0;
@@ -644,31 +722,35 @@
                 char cc = in.get();
                 if(cc != '>') // Eat '>'
                     PLERROR("In PStream::operator>>(T*&)  Wrong format.  Expecting \"*%d->\" but got \"*%d-%c\".", id, id, cc);
-                //don't skip blanks before we need to read something else (read might block).
+                // don't skip blanks before we need to read something else
+                // (read might block).
                 //in.skipBlanksAndCommentsAndSeparators();
                 if(!x)
-                    x= new T();
+                    x = new T();
                 in.skipBlanksAndCommentsAndSeparators();
                 in >> *x;
-                //don't skip blanks before we need to read something else (read might block).
+                // don't skip blanks before we need to read something else
+                // (read might block).
                 //in.skipBlanksAndCommentsAndSeparators();
                 in.copies_map_in[id]= x;
             }
             else
             {
                 // Find it in map and return ptr;
-                map<unsigned int, void *>::iterator it = in.copies_map_in.find(id);
+                map<unsigned int, void *>::iterator it =
+                    in.copies_map_in.find(id);
                 if (it == in.copies_map_in.end())
                     PLERROR("In PStream::operator>>(T*&) object (ptr) to be read with id='%d' "
                             "has not been previously defined", id);
-                x= static_cast<T *>(it->second);
+                x = static_cast<T *>(it->second);
             }
         }
     }
     else
     {
         in >> *x;
-        //don't skip blanks before we need to read something else (read might block).
+        // don't skip blanks before we need to read something else
+        // (read might block).
         //in.skipBlanksAndCommentsAndSeparators();
     }
 
@@ -681,7 +763,8 @@
 {
     if(x)
     {
-        map<void *, unsigned int>::iterator it = out.copies_map_out.find(const_cast<T*&>(x));
+        map<void *, unsigned int>::iterator it =
+            out.copies_map_out.find(const_cast<T*&>(x));
         if (it == out.copies_map_out.end())
         {
             int id = (int)out.copies_map_out.size()+1;
@@ -869,42 +952,73 @@
 }
 
 template<class Key, class Value, class Compare, class Alloc>
-inline PStream& operator<<(PStream& out, const map<Key, Value, Compare, Alloc>& m)
-{ writeMap(out, m); return out; }
+inline PStream& operator<<(PStream& out,
+                           const map<Key, Value, Compare, Alloc>& m)
+{
+    writeMap(out, m);
+    return out;
+}
 
 template<class Key, class Value, class Compare, class Alloc>
 inline PStream& operator>>(PStream& in, map<Key, Value, Compare, Alloc>& m)
-{ readMap(in, m); return in; }
+{
+    readMap(in, m);
+    return in;
+}
 
 template<class Key, class Value, class Compare, class Alloc>
-inline PStream& operator<<(PStream& out, const multimap<Key, Value, Compare, Alloc>& m)
-{ writeMap(out, m); return out; }
+inline PStream& operator<<(PStream& out,
+                           const multimap<Key, Value, Compare, Alloc>& m)
+{
+    writeMap(out, m);
+    return out;
+}
 
 template<class Key, class Value, class Compare, class Alloc>
-inline PStream& operator>>(PStream& in, multimap<Key, Value, Compare, Alloc>& m)
-{ readMap(in, m); return in; }
+inline PStream& operator>>(PStream& in,
+                           multimap<Key, Value, Compare, Alloc>& m)
+{
+    readMap(in, m);
+    return in;
+}
 
 
 template<class Key, class Value, class Compare, class Alloc>
-inline PStream& operator<<(PStream& out, const hash_map<Key, Value, Compare, Alloc>& m)
-{ writeMap(out, m); return out; }
+inline PStream& operator<<(PStream& out,
+                           const hash_map<Key, Value, Compare, Alloc>& m)
+{
+    writeMap(out, m);
+    return out;
+}
 
 template<class Key, class Value, class Compare, class Alloc>
-inline PStream& operator>>(PStream& in, hash_map<Key, Value, Compare, Alloc>& m)
-{ readMap(in, m); return in; }
+inline PStream& operator>>(PStream& in,
+                           hash_map<Key, Value, Compare, Alloc>& m)
+{
+    readMap(in, m);
+    return in;
+}
 
 template<class Key, class Value, class Compare, class Alloc>
-inline PStream& operator<<(PStream& out, const hash_multimap<Key, Value, Compare, Alloc>& m)
-{ writeMap(out, m); return out; }
+inline PStream& operator<<(PStream& out,
+                           const hash_multimap<Key, Value, Compare, Alloc>& m)
+{
+    writeMap(out, m);
+    return out;
+}
 
 template<class Key, class Value, class Compare, class Alloc>
-inline PStream& operator>>(PStream& in, hash_multimap<Key, Value, Compare, Alloc>& m)
-{ readMap(in, m); return in; }
+inline PStream& operator>>(PStream& in,
+                           hash_multimap<Key, Value, Compare, Alloc>& m)
+{
+    readMap(in, m);
+    return in;
+}
 
 
 /** Serialization of sequences **/
-/* These methods are there only to simplify the writing of operator<< and operator>> and
-   should not be called by user code directly */
+/* These methods are there only to simplify the writing of operator<< and
+   operator>> and should not be called by user code directly */
 
 template<class Iterator>
 void binwrite_(PStream& out, Iterator& it, unsigned int n)
@@ -1043,20 +1157,27 @@
     in.read((char*)x, n);
 }
 
-inline void binread_(PStream& in, signed char* x, unsigned int n, unsigned char typecode)
+inline void binread_(PStream& in, signed char* x, unsigned int n,
+                     unsigned char typecode)
 { binread_(in, (char *)x, n, typecode); }
 
-inline void binread_(PStream& in, unsigned char* x, unsigned int n, unsigned char typecode)
+inline void binread_(PStream& in, unsigned char* x, unsigned int n,
+                     unsigned char typecode)
 { binread_(in, (char *)x, n, typecode); }
 
 void binread_(PStream& in, short* x, unsigned int n, unsigned char typecode);
-void binread_(PStream& in, unsigned short* x, unsigned int n, unsigned char typecode);
+void binread_(PStream& in, unsigned short* x, unsigned int n,
+              unsigned char typecode);
 void binread_(PStream& in, int* x, unsigned int n, unsigned char typecode);
-void binread_(PStream& in, unsigned int* x, unsigned int n, unsigned char typecode);
+void binread_(PStream& in, unsigned int* x, unsigned int n,
+              unsigned char typecode);
 void binread_(PStream& in, long* x, unsigned int n, unsigned char typecode);
-void binread_(PStream& in, unsigned long* x, unsigned int n, unsigned char typecode);
-void binread_(PStream& in, long long* x, unsigned int n, unsigned char typecode);
-void binread_(PStream& in, unsigned long long* x, unsigned int n, unsigned char typecode);
+void binread_(PStream& in, unsigned long* x, unsigned int n,
+              unsigned char typecode);
+void binread_(PStream& in, long long* x, unsigned int n,
+              unsigned char typecode);
+void binread_(PStream& in, unsigned long long* x, unsigned int n,
+              unsigned char typecode);
 void binread_(PStream& in, float* x, unsigned int n, unsigned char typecode);
 void binread_(PStream& in, double* x, unsigned int n, unsigned char typecode);
 
@@ -1112,12 +1233,14 @@
         if(byte_order()==LITTLE_ENDIAN_ORDER)
         {
             out.put((char)0x12); // 1D little-endian
-            typecode = TypeTraits<typename SequenceType::value_type>::little_endian_typecode();
+            typecode = TypeTraits<typename SequenceType::value_type>
+                ::little_endian_typecode();
         }
         else
         {
             out.put((char)0x13); // 1D big-endian
-            typecode = TypeTraits<typename SequenceType::value_type>::big_endian_typecode();
+            typecode = TypeTraits<typename SequenceType::value_type>
+                ::big_endian_typecode();
         }
 
         // write typecode
@@ -1139,13 +1262,15 @@
 
 
 //! Reads in a sequence type from a PStream.
-/*! For this to work with the current implementation, the SequenceType must have:
-  - typedefs defining (SequenceType::...) value_type, size_type, iterator
+/*! For this to work with the current implementation, the SequenceType must
+have:
+  - typedefs defining (SequenceType::...) value_type, size_type, iterator,
   - a begin() method that returns a proper iterator,
-  - a size_type size() method returning the size of the current container
-  - a resize(size_type n) method that allows to change the size of the container
-  (which should also work with resize(0) )
-  - a push_back(const value_type& x) method that appends the element x at the end
+  - a size_type size() method returning the size of the current container,
+  - a resize(size_type n) method that allows to change the size of the
+  container (which should also work with resize(0)),
+  - a push_back(const value_type& x) method that appends the element x at the
+  end.
 */
 template<class SequenceType>
 void readSequence(PStream& in, SequenceType& seq)
@@ -1161,7 +1286,8 @@
         {
             in.skipBlanks();
             in >> *it;
-            //don't skip blanks before we need to read something else (read might block).
+            // don't skip blanks before we need to read something else
+            // (read might block).
             //in.skipBlanks();
             ++it;
         }
@@ -1210,7 +1336,8 @@
             c = in.get();
             if(c!='[')
                 PLERROR("Error in readSequence(SequenceType& seq), expected '[', read '%c'",c);
-            //don't skip blanks before we need to read something else (read might block).
+            // don't skip blanks before we need to read something else
+            // (read might block).
             //in.skipBlanksAndCommentsAndSeparators();
             seq.resize((typename SequenceType::size_type) n);
             if (n>0)
@@ -1220,7 +1347,8 @@
                 {
                     in.skipBlanksAndCommentsAndSeparators();
                     in >> *it;
-                    //don't skip blanks before we need to read something else (read might block).
+                    // don't skip blanks before we need to read something else
+                    // (read might block).
                     //in.skipBlanksAndCommentsAndSeparators();
                     ++it;
                 }
@@ -1238,8 +1366,9 @@
             unsigned int l;
             in.read((char*)&l,sizeof(l));
 
-            bool inverted_byte_order = (    (c==0x12 && byte_order()==BIG_ENDIAN_ORDER)
-                                            || (c==0x13 && byte_order()==LITTLE_ENDIAN_ORDER) );
+            bool inverted_byte_order =
+                ((c==0x12 && byte_order()==BIG_ENDIAN_ORDER)
+                 || (c==0x13 && byte_order()==LITTLE_ENDIAN_ORDER) );
 
             if(inverted_byte_order)
                 endianswap(&l);
@@ -1359,7 +1488,8 @@
 class POFStream: public PStream
 {
 public:
-    POFStream(const string& fname, ios_base::openmode m = ios_base::out | ios_base::trunc)
+    POFStream(const string& fname,
+              ios_base::openmode m = ios_base::out | ios_base::trunc)
         :PStream(new ofstream(fname.c_str()),true)
     {
         PLDEPRECATED("POFStream is deprecated. Use the openFile function instead.");



From lamblin at mail.berlios.de  Fri Jul 27 05:57:06 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 27 Jul 2007 05:57:06 +0200
Subject: [Plearn-commits] r7843 - trunk/plearn/io
Message-ID: <200707270357.l6R3v6nT004586@sheep.berlios.de>

Author: lamblin
Date: 2007-07-27 05:57:04 +0200 (Fri, 27 Jul 2007)
New Revision: 7843

Modified:
   trunk/plearn/io/PStream.h
Log:
Fix typo.


Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2007-07-27 03:42:04 UTC (rev 7842)
+++ trunk/plearn/io/PStream.h	2007-07-27 03:57:04 UTC (rev 7843)
@@ -323,7 +323,7 @@
         read(reinterpret_cast<char*>(&y), sizeof(I));
         if (inverted_byte_order)
             endianswap(&y);
-        x = y;
+        x = static_cast<J>(y);
     }
 
     //! Reads base types from PLearn binary format
@@ -1137,7 +1137,7 @@
         in.read(reinterpret_cast<char*>(&y), sizeof(I));
         if (inverted_byte_order)
             endianswap(&y);
-        *x = static_cast<I>(y);
+        *x = static_cast<J>(y);
         ++x;
     }
 }



From lamblin at mail.berlios.de  Fri Jul 27 19:15:57 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 27 Jul 2007 19:15:57 +0200
Subject: [Plearn-commits] r7844 - in trunk/plearn: base io
Message-ID: <200707271715.l6RHFvRB006586@sheep.berlios.de>

Author: lamblin
Date: 2007-07-27 19:15:56 +0200 (Fri, 27 Jul 2007)
New Revision: 7844

Modified:
   trunk/plearn/base/TypeTraits.h
   trunk/plearn/io/PStream.h
Log:
Removes some icc warnings and remarks.


Modified: trunk/plearn/base/TypeTraits.h
===================================================================
--- trunk/plearn/base/TypeTraits.h	2007-07-27 03:57:04 UTC (rev 7843)
+++ trunk/plearn/base/TypeTraits.h	2007-07-27 17:15:56 UTC (rev 7844)
@@ -245,6 +245,10 @@
 DECLARE_TYPE_TRAITS_FOR_BASETYPE(double,             0x10, 0x11);
 DECLARE_TYPE_TRAITS_FOR_BASETYPE(bool,               0x30, 0x30);
 
+#ifdef __INTEL_COMPILER
+#pragma warning(disable:280)
+// Yes, I know that "selector expression is constant"
+#endif
 DECLARE_TYPE_TRAITS_FOR_INTTYPE(char);
 DECLARE_TYPE_TRAITS_FOR_INTTYPE(signed char);
 DECLARE_TYPE_TRAITS_FOR_INTTYPE(short);
@@ -257,6 +261,9 @@
 DECLARE_TYPE_TRAITS_FOR_UINTTYPE(unsigned int);
 DECLARE_TYPE_TRAITS_FOR_UINTTYPE(unsigned long);
 DECLARE_TYPE_TRAITS_FOR_UINTTYPE(unsigned long long);
+#ifdef __INTEL_COMPILER
+#pragma warning(default:280)
+#endif
 
 DECLARE_TYPE_TRAITS(string);
 

Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2007-07-27 03:57:04 UTC (rev 7843)
+++ trunk/plearn/io/PStream.h	2007-07-27 17:15:56 UTC (rev 7844)
@@ -323,7 +323,16 @@
         read(reinterpret_cast<char*>(&y), sizeof(I));
         if (inverted_byte_order)
             endianswap(&y);
+#ifdef __INTEL_COMPILER
+#pragma warning(disable:1682)
+// Yes, I know that "implicit conversion of a 64-bit integral type to a smaller
+// integral type (potential portability problem)", but the conversion is
+// explicit here.
+#endif
         x = static_cast<J>(y);
+#ifdef __INTEL_COMPILER
+#pragma warning(default:1682)
+#endif
     }
 
     //! Reads base types from PLearn binary format
@@ -1137,7 +1146,17 @@
         in.read(reinterpret_cast<char*>(&y), sizeof(I));
         if (inverted_byte_order)
             endianswap(&y);
+
+#ifdef __INTEL_COMPILER
+#pragma warning(disable:1682)
+// Yes, I know that "implicit conversion of a 64-bit integral type to a smaller
+// integral type (potential portability problem)", but the conversion is
+// explicit here.
+#endif
         *x = static_cast<J>(y);
+#ifdef __INTEL_COMPILER
+#pragma warning(default:1682)
+#endif
         ++x;
     }
 }



From lamblin at mail.berlios.de  Fri Jul 27 22:27:17 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 27 Jul 2007 22:27:17 +0200
Subject: [Plearn-commits] r7845 - trunk/plearn/python
Message-ID: <200707272027.l6RKRHkN010426@sheep.berlios.de>

Author: lamblin
Date: 2007-07-27 22:27:17 +0200 (Fri, 27 Jul 2007)
New Revision: 7845

Modified:
   trunk/plearn/python/PythonCodeSnippet.cc
   trunk/plearn/python/PythonCodeSnippet.h
Log:
Should fix potential problems on 64 bits machines (only the end of the pointer
was used as key).


Modified: trunk/plearn/python/PythonCodeSnippet.cc
===================================================================
--- trunk/plearn/python/PythonCodeSnippet.cc	2007-07-27 17:15:56 UTC (rev 7844)
+++ trunk/plearn/python/PythonCodeSnippet.cc	2007-07-27 20:27:17 UTC (rev 7845)
@@ -68,8 +68,8 @@
 "__injected__ = {}\n";
 
 const char* PythonCodeSnippet::SetCurrentSnippetVar =\
-// Completed by sprintf using the snippet's address casted to a long
-"_inject_import_.setCurrentSnippet(%d)\n";
+// Completed by sprintf using the snippet's address hex value
+"_inject_import_.setCurrentSnippet(%p)\n";
 
 const char* PythonCodeSnippet::ResetCurrentSnippetVar = \
 "_inject_import_.resetCurrentSnippet()\n";
@@ -114,7 +114,7 @@
       m_remap_python_exceptions(remap_python_exceptions),
       m_instance_params(),
       m_instance(),
-      m_handle(long(this)),
+      m_handle(this),
       m_compiled_code(),
       m_injected_functions(4),
       m_python_methods(4)
@@ -662,7 +662,7 @@
     Py_XDECREF(res);
 }
 
-void PythonCodeSnippet::setCurrentSnippet(const long& handle) const
+void PythonCodeSnippet::setCurrentSnippet(const void* handle) const
 {
     PythonGlobalInterpreterLock gil;         // For thread-safety
     

Modified: trunk/plearn/python/PythonCodeSnippet.h
===================================================================
--- trunk/plearn/python/PythonCodeSnippet.h	2007-07-27 17:15:56 UTC (rev 7844)
+++ trunk/plearn/python/PythonCodeSnippet.h	2007-07-27 20:27:17 UTC (rev 7845)
@@ -322,7 +322,7 @@
     //! Call PLERROR if the code contains an error.
     PythonObjectWrapper compileGlobalCode(const string& code); //const;
 
-    void setCurrentSnippet(const long& handle) const;
+    void setCurrentSnippet(const void* handle) const;
     void resetCurrentSnippet() const;
     
     //! If no Python error, do nothing.  If an error occurred, convert the
@@ -340,7 +340,7 @@
 
 protected:
     //! The Python handle for *this* instance
-    long m_handle;
+    void* m_handle;
 
 public:    
     //! Compiled Python code module and global environment



From lamblin at mail.berlios.de  Fri Jul 27 22:31:48 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 27 Jul 2007 22:31:48 +0200
Subject: [Plearn-commits] r7846 - trunk/plearn_learners/online
Message-ID: <200707272031.l6RKVmqp010655@sheep.berlios.de>

Author: lamblin
Date: 2007-07-27 22:31:47 +0200 (Fri, 27 Jul 2007)
New Revision: 7846

Modified:
   trunk/plearn_learners/online/MaxSubsampling2DModule.cc
Log:
Fix warning under icc.


Modified: trunk/plearn_learners/online/MaxSubsampling2DModule.cc
===================================================================
--- trunk/plearn_learners/online/MaxSubsampling2DModule.cc	2007-07-27 20:27:17 UTC (rev 7845)
+++ trunk/plearn_learners/online/MaxSubsampling2DModule.cc	2007-07-27 20:31:47 UTC (rev 7846)
@@ -300,8 +300,8 @@
                             .subMat(i*kernel_length, j*kernel_width,
                                     kernel_length, kernel_width);
 
-                        int argmax = (int) round(argmax_kl(i,j));
-                        input_grad_zone.data()[argmax] =
+                        int argmax_klij = (int) round(argmax_kl(i,j));
+                        input_grad_zone.data()[argmax_klij] =
                             output_grad_image_kl(i,j);
                     }
             }



From lamblin at mail.berlios.de  Fri Jul 27 22:33:23 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 27 Jul 2007 22:33:23 +0200
Subject: [Plearn-commits] r7847 - trunk/plearn/python
Message-ID: <200707272033.l6RKXNYd010720@sheep.berlios.de>

Author: lamblin
Date: 2007-07-27 22:33:22 +0200 (Fri, 27 Jul 2007)
New Revision: 7847

Modified:
   trunk/plearn/python/PythonObjectWrapper.cc
Log:
Fix warning with icc.


Modified: trunk/plearn/python/PythonObjectWrapper.cc
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.cc	2007-07-27 20:31:47 UTC (rev 7846)
+++ trunk/plearn/python/PythonObjectWrapper.cc	2007-07-27 20:33:22 UTC (rev 7847)
@@ -567,9 +567,9 @@
             + classname + "(" + wrapper_name + "):\n"
             "\tpass\n\n";
 
-        PyObject* res= PyRun_String(derivcode.c_str(), 
-                                    Py_file_input, pyenv, pyenv);
-        Py_XDECREF(res);
+        PyObject* res2= PyRun_String(derivcode.c_str(),
+                                     Py_file_input, pyenv, pyenv);
+        Py_XDECREF(res2);
         env= PythonObjectWrapper(
             pyenv, PythonObjectWrapper::transfer_ownership).as<env_t>();
         clit= env.find(classname);



From lamblin at mail.berlios.de  Fri Jul 27 22:36:36 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 27 Jul 2007 22:36:36 +0200
Subject: [Plearn-commits] r7848 - trunk/plearn/base
Message-ID: <200707272036.l6RKaaDX010915@sheep.berlios.de>

Author: lamblin
Date: 2007-07-27 22:36:36 +0200 (Fri, 27 Jul 2007)
New Revision: 7848

Modified:
   trunk/plearn/base/pl_stdint.h
Log:
Inject all types defined in <boost/cstdint.hpp> into namespace PLearn


Modified: trunk/plearn/base/pl_stdint.h
===================================================================
--- trunk/plearn/base/pl_stdint.h	2007-07-27 20:33:22 UTC (rev 7847)
+++ trunk/plearn/base/pl_stdint.h	2007-07-27 20:36:36 UTC (rev 7848)
@@ -44,17 +44,38 @@
 
 namespace PLearn {
 
-// inject principal boost integer types into namespace PLearn
-// @TODO: use them all?
+// Inject boost integer types into namespace PLearn
 using boost::int8_t;
+using boost::int_least8_t;
+using boost::int_fast8_t;
 using boost::uint8_t;
+using boost::uint_least8_t;
+using boost::uint_fast8_t;
+
 using boost::int16_t;
+using boost::int_least16_t;
+using boost::int_fast16_t;
 using boost::uint16_t;
+using boost::uint_least16_t;
+using boost::uint_fast16_t;
+
 using boost::int32_t;
+using boost::int_least32_t;
+using boost::int_fast32_t;
 using boost::uint32_t;
+using boost::uint_least32_t;
+using boost::uint_fast32_t;
+
 using boost::int64_t;
+using boost::int_least64_t;
+using boost::int_fast64_t;
 using boost::uint64_t;
+using boost::uint_least64_t;
+using boost::uint_fast64_t;
 
+using boost::intmax_t;
+using boost::uintmax_t;
+
 } // end of namespace PLearn
 
 #endif



From lamblin at mail.berlios.de  Sat Jul 28 03:07:45 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Sat, 28 Jul 2007 03:07:45 +0200
Subject: [Plearn-commits] r7849 - trunk/plearn/math
Message-ID: <200707280107.l6S17j8f009704@sheep.berlios.de>

Author: lamblin
Date: 2007-07-28 03:07:43 +0200 (Sat, 28 Jul 2007)
New Revision: 7849

Modified:
   trunk/plearn/math/pl_math.cc
Log:
Remove compiler warning


Modified: trunk/plearn/math/pl_math.cc
===================================================================
--- trunk/plearn/math/pl_math.cc	2007-07-27 20:36:36 UTC (rev 7848)
+++ trunk/plearn/math/pl_math.cc	2007-07-28 01:07:43 UTC (rev 7849)
@@ -102,7 +102,7 @@
 
 real safeexp(real a)
 {
-#if USEDOUBLE
+#ifdef USEDOUBLE
     if (a < -300) return 0;
     if (a > 300) return 1e38;
 #else



From lamblin at mail.berlios.de  Sat Jul 28 03:13:18 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Sat, 28 Jul 2007 03:13:18 +0200
Subject: [Plearn-commits] r7850 - trunk/plearn/python
Message-ID: <200707280113.l6S1DIaU010091@sheep.berlios.de>

Author: lamblin
Date: 2007-07-28 03:13:17 +0200 (Sat, 28 Jul 2007)
New Revision: 7850

Modified:
   trunk/plearn/python/PythonObjectWrapper.cc
   trunk/plearn/python/PythonObjectWrapper.h
Log:
Better conversion from python to C++ integer types


Modified: trunk/plearn/python/PythonObjectWrapper.cc
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.cc	2007-07-28 01:07:43 UTC (rev 7849)
+++ trunk/plearn/python/PythonObjectWrapper.cc	2007-07-28 01:13:17 UTC (rev 7850)
@@ -97,6 +97,8 @@
     return PyObject_IsTrue(pyobj) != 0;
 }
 
+
+#if 0
 int ConvertFromPyObject<int>::convert(PyObject* pyobj, bool print_traceback)
 {
     PLASSERT( pyobj );
@@ -146,21 +148,38 @@
         PLPythonConversionError("ConvertFromPyObject<uint64_t>", pyobj, print_traceback);
     return PyInt_AsUnsignedLongLongMask(pyobj);
 }
+#endif
 
-real ConvertFromPyObject<real>::convert(PyObject* pyobj, bool print_traceback)
+double ConvertFromPyObject<double>::convert(PyObject* pyobj,
+                                            bool print_traceback)
 {
     PLASSERT( pyobj );
     if(PyFloat_Check(pyobj))
-        return (real)PyFloat_AS_DOUBLE(pyobj);
+        return PyFloat_AS_DOUBLE(pyobj);
     if(PyLong_Check(pyobj))
-        return (real)PyLong_AsDouble(pyobj);
+        return PyLong_AsDouble(pyobj);
     if(PyInt_Check(pyobj))
-        return (real)PyInt_AS_LONG(pyobj);
-    PLPythonConversionError("ConvertFromPyObject<real>", pyobj,
+        return (double)PyInt_AS_LONG(pyobj);
+    PLPythonConversionError("ConvertFromPyObject<double>", pyobj,
                             print_traceback);
     return 0;//shut up compiler
 }
 
+float ConvertFromPyObject<float>::convert(PyObject* pyobj,
+                                          bool print_traceback)
+{
+    PLASSERT( pyobj );
+    if(PyFloat_Check(pyobj))
+        return (float)PyFloat_AS_DOUBLE(pyobj);
+    if(PyLong_Check(pyobj))
+        return (float)PyLong_AsDouble(pyobj);
+    if(PyInt_Check(pyobj))
+        return (float)PyInt_AS_LONG(pyobj);
+    PLPythonConversionError("ConvertFromPyObject<float>", pyobj,
+                            print_traceback);
+    return 0;//shut up compiler
+}
+
 string ConvertFromPyObject<string>::convert(PyObject* pyobj, bool print_traceback)
 {
     PLASSERT( pyobj );

Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2007-07-28 01:07:43 UTC (rev 7849)
+++ trunk/plearn/python/PythonObjectWrapper.h	2007-07-28 01:13:17 UTC (rev 7850)
@@ -83,7 +83,96 @@
 void PLPythonConversionError(const char* function_name, PyObject* pyobj,
                              bool print_traceback);
 
+//! Used to retrieve integer values from python if possible without precision
+//! loss, and convert them to requested type.
+//! @TODO: put I's name in error message?
+//! @TODO: call PyErr_Print() before PyErr_Clear()?
+template <class I>
+I integerFromPyObject(PyObject* pyobj, bool print_traceback)
+{
+    PLASSERT( pyobj );
 
+    I result = (I) 0;
+    if (PyInt_Check(pyobj))
+    {
+        // pyobj is represented in Python as a long,
+        // so we are sure it fits into a long, no need to check.
+        long x = PyInt_AS_LONG(pyobj);
+        result = static_cast<I>(x);
+
+        // Check if x fits into type I (overflow or sign problem)
+        if (static_cast<long>(result) != x
+            || !(numeric_limits<I>::is_signed) && x<0)
+        {
+            PLPythonConversionError("integerFromPyObject<I>", pyobj,
+                                    print_traceback);
+        }
+
+    }
+    else if (PyLong_Check(pyobj))
+    {
+        if (numeric_limits<I>::is_signed)
+        {
+            // If I is signed, we have to accept negative values, so we use a
+            // signed long long to hold the result.
+            // No signed type can hold values greater than a long long anyway.
+            long long x = PyLong_AsLongLong(pyobj);
+
+            // Check for possible overflow during conversion
+            if (!PyErr_Occurred())
+            {
+                result = static_cast<I>(x);
+
+                // Check if x fits into type I (overflow only, there
+                // cannot be any sign error because I is signed, too)
+                if (static_cast<long long>(result) != x)
+                {
+                    PLPythonConversionError("integerFromPyObject<I>", pyobj,
+                                            print_traceback);
+                }
+            }
+            else if (PyErr_ExceptionMatches(PyExc_OverflowError))
+            {
+                PyErr_Clear();
+                PLPythonConversionError("integerFromPyObject<I>", pyobj,
+                                        print_traceback);
+            }
+            // else?
+        }
+        else
+        {
+            // I is unsigned
+            unsigned long long x = PyLong_AsUnsignedLongLong(pyobj);
+
+            // Check for possible overflow during conversion
+            if (!PyErr_Occurred())
+            {
+                result = static_cast<I>(x);
+
+                // Check if x fits into type I (overflow only)
+                if (static_cast<unsigned long long>(result) != x)
+                {
+                    PLPythonConversionError("integerFromPyObject<I>", pyobj,
+                                            print_traceback);
+                }
+            }
+            else if (PyErr_ExceptionMatches(PyExc_OverflowError) // too big
+                     || PyErr_ExceptionMatches(PyExc_TypeError)) // negative
+            {
+                PyErr_Clear();
+                PLPythonConversionError("integerFromPyObject<I>", pyobj,
+                                        print_traceback);
+            }
+            // else?
+        }
+    }
+    else
+        PLPythonConversionError("integerFromPyObject<I>", pyobj,
+                                print_traceback);
+    return result;
+}
+
+
 //#####  ConvertFromPyObject  #################################################
 
 /**
@@ -101,7 +190,7 @@
  */
 template <class T>
 struct ConvertFromPyObject
-{ 
+{
     static T convert(PyObject* x, bool print_traceback);
 };
 
@@ -119,32 +208,63 @@
 };
 
 template <>
+struct ConvertFromPyObject<short>
+{
+    static int convert(PyObject* pyobj, bool print_traceback)
+    { return integerFromPyObject<short>(pyobj, print_traceback); }
+};
+
+template <>
+struct ConvertFromPyObject<unsigned short>
+{
+    static int convert(PyObject* pyobj, bool print_traceback)
+    { return integerFromPyObject<unsigned short>(pyobj, print_traceback); }
+};
+
+template <>
 struct ConvertFromPyObject<int>
 {
-    static int convert(PyObject*, bool print_traceback);
+    static int convert(PyObject* pyobj, bool print_traceback)
+    { return integerFromPyObject<int>(pyobj, print_traceback); }
 };
 
 template <>
 struct ConvertFromPyObject<unsigned int>
 {
-    static unsigned int convert(PyObject*, bool print_traceback);
+    static int convert(PyObject* pyobj, bool print_traceback)
+    { return integerFromPyObject<unsigned int>(pyobj, print_traceback); }
 };
 
-/*
 template <>
 struct ConvertFromPyObject<long>
 {
-    static long convert(PyObject*, bool print_traceback);
+    static int convert(PyObject* pyobj, bool print_traceback)
+    { return integerFromPyObject<long>(pyobj, print_traceback); }
 };
 
 template <>
 struct ConvertFromPyObject<unsigned long>
 {
-    static unsigned long convert(PyObject*, bool print_traceback);
+    static int convert(PyObject* pyobj, bool print_traceback)
+    { return integerFromPyObject<unsigned long>(pyobj, print_traceback); }
 };
-*/
 
 template <>
+struct ConvertFromPyObject<long long>
+{
+    static int convert(PyObject* pyobj, bool print_traceback)
+    { return integerFromPyObject<long long>(pyobj, print_traceback); }
+};
+
+template <>
+struct ConvertFromPyObject<unsigned long long>
+{
+    static int convert(PyObject* pyobj, bool print_traceback)
+    { return integerFromPyObject<unsigned long long>(pyobj, print_traceback); }
+};
+
+/*
+template <>
 struct ConvertFromPyObject<int64_t>
 {
     static int64_t convert(PyObject*, bool print_traceback);
@@ -155,12 +275,18 @@
 {
     static uint64_t convert(PyObject*, bool print_traceback);
 };
+*/
 
+template <>
+struct ConvertFromPyObject<double>
+{
+    static double convert(PyObject*, bool print_traceback);
+};
 
 template <>
-struct ConvertFromPyObject<real>
+struct ConvertFromPyObject<float>
 {
-    static real convert(PyObject*, bool print_traceback);
+    static float convert(PyObject*, bool print_traceback);
 };
 
 template <>



From lamblin at mail.berlios.de  Sat Jul 28 03:31:31 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Sat, 28 Jul 2007 03:31:31 +0200
Subject: [Plearn-commits] r7851 - trunk/plearn/python
Message-ID: <200707280131.l6S1VVQH010961@sheep.berlios.de>

Author: lamblin
Date: 2007-07-28 03:31:30 +0200 (Sat, 28 Jul 2007)
New Revision: 7851

Modified:
   trunk/plearn/python/PythonIncludes.h
Log:
Minor change in comments.


Modified: trunk/plearn/python/PythonIncludes.h
===================================================================
--- trunk/plearn/python/PythonIncludes.h	2007-07-28 01:13:17 UTC (rev 7850)
+++ trunk/plearn/python/PythonIncludes.h	2007-07-28 01:31:30 UTC (rev 7851)
@@ -47,6 +47,7 @@
  *
  *  - 230 = Python 2.3.x
  *  - 240 = Python 2.4.x
+ *  - 250 = Python 2.5.x
  *
  *  NOTE: This header file MUST BE INCLUDED FIRST, even before any standard
  *  library includes are carried out.  (Python.h plays hacks with some macros)



From lamblin at mail.berlios.de  Sat Jul 28 04:30:11 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Sat, 28 Jul 2007 04:30:11 +0200
Subject: [Plearn-commits] r7852 - trunk/plearn/python
Message-ID: <200707280230.l6S2UBHx013567@sheep.berlios.de>

Author: lamblin
Date: 2007-07-28 04:30:07 +0200 (Sat, 28 Jul 2007)
New Revision: 7852

Modified:
   trunk/plearn/python/PythonObjectWrapper.cc
   trunk/plearn/python/PythonObjectWrapper.h
Log:
Better conversion from C++ to python integer types


Modified: trunk/plearn/python/PythonObjectWrapper.cc
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.cc	2007-07-28 01:31:30 UTC (rev 7851)
+++ trunk/plearn/python/PythonObjectWrapper.cc	2007-07-28 02:30:07 UTC (rev 7852)
@@ -715,7 +715,7 @@
     }
 }
 
-
+#if 0
 PyObject* ConvertToPyObject<int>::newPyObject(const int& x)
 {
     return PyInt_FromLong(long(x));
@@ -747,6 +747,7 @@
 {
     return PyLong_FromUnsignedLongLong(x);
 }
+#endif
 
 PyObject* ConvertToPyObject<double>::newPyObject(const double& x)
 {

Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2007-07-28 01:31:30 UTC (rev 7851)
+++ trunk/plearn/python/PythonObjectWrapper.h	2007-07-28 02:30:07 UTC (rev 7852)
@@ -415,6 +415,22 @@
 };
 
 
+//! Used to convert integer values to python, using PyInt if possible
+template <class I>
+PyObject* integerToPyObject(const I& x)
+{
+    // Try to convert x to a long
+    long y = static_cast<long>(x);
+
+    // Check if we lost value information or sign
+    if (static_cast<I>(y) == x && (numeric_limits<I>::is_signed || y >= 0))
+        return PyInt_FromLong(y);
+    else if (numeric_limits<I>::is_signed)
+        return PyLong_FromLongLong(static_cast<long long>(x));
+    else
+        return PyLong_FromUnsignedLongLong(static_cast<unsigned long long>(x));
+}
+
 /////////////////////////////////////
 // ConvertToPyObject<>
 // Conversions from PLearn to Python
@@ -440,21 +456,59 @@
 template<> struct ConvertToPyObject<bool>
 { static PyObject* newPyObject(const bool& x); };
 
+template<> struct ConvertToPyObject<short>
+{
+    static PyObject* newPyObject(const short& x)
+    { return integerToPyObject(x); }
+};
+
+template<> struct ConvertToPyObject<unsigned short>
+{
+    static PyObject* newPyObject(const unsigned short& x)
+    { return integerToPyObject(x); }
+};
+
 template<> struct ConvertToPyObject<int>
-{ static PyObject* newPyObject(const int& x); };
+{
+    static PyObject* newPyObject(const int& x)
+    { return integerToPyObject(x); }
+};
+
 template<> struct ConvertToPyObject<unsigned int>
-{ static PyObject* newPyObject(const unsigned int& x); };
+{
+    static PyObject* newPyObject(const unsigned int& x)
+    { return integerToPyObject(x); }
+};
 
+template<> struct ConvertToPyObject<long>
+{
+    static PyObject* newPyObject(const long& x)
+    { return integerToPyObject(x); }
+};
+
+template<> struct ConvertToPyObject<unsigned long>
+{
+    static PyObject* newPyObject(const unsigned long& x)
+    { return integerToPyObject(x); }
+};
+
+template<> struct ConvertToPyObject<long long>
+{
+    static PyObject* newPyObject(const long long& x)
+    { return integerToPyObject(x); }
+};
+
+template<> struct ConvertToPyObject<unsigned long long>
+{
+    static PyObject* newPyObject(const unsigned long long& x)
+    { return integerToPyObject(x); }
+};
+
+/*
 template<> struct ConvertToPyObject<int64_t>
 { static PyObject* newPyObject(const int64_t& x); };
 template<> struct ConvertToPyObject<uint64_t>
 { static PyObject* newPyObject(const uint64_t& x); };
-
-/*
-template<> struct ConvertToPyObject<long>
-{ static PyObject* newPyObject(const long& x); };
-template<> struct ConvertToPyObject<unsigned long>
-{ static PyObject* newPyObject(const unsigned long& x); };
 */
 
 template<> struct ConvertToPyObject<double>



From lamblin at mail.berlios.de  Sat Jul 28 05:55:56 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Sat, 28 Jul 2007 05:55:56 +0200
Subject: [Plearn-commits] r7853 - in trunk/plearn/python/test: .
	.pytest/EmbeddedPython_BasicIdentityCalls/expected_results
	.pytest/EmbeddedPython_InterfunctionXchg/expected_results
Message-ID: <200707280355.l6S3tuAs017112@sheep.berlios.de>

Author: lamblin
Date: 2007-07-28 05:55:55 +0200 (Sat, 28 Jul 2007)
New Revision: 7853

Modified:
   trunk/plearn/python/test/.pytest/EmbeddedPython_BasicIdentityCalls/expected_results/RUN.log
   trunk/plearn/python/test/.pytest/EmbeddedPython_InterfunctionXchg/expected_results/RUN.log
   trunk/plearn/python/test/BasicIdentityCallsTest.cc
   trunk/plearn/python/test/InterfunctionXchgTest.cc
   trunk/plearn/python/test/MemoryStressTest.cc
Log:
Update tests and test results to reflect last changes.


Modified: trunk/plearn/python/test/.pytest/EmbeddedPython_BasicIdentityCalls/expected_results/RUN.log
===================================================================
--- trunk/plearn/python/test/.pytest/EmbeddedPython_BasicIdentityCalls/expected_results/RUN.log	2007-07-28 02:30:07 UTC (rev 7852)
+++ trunk/plearn/python/test/.pytest/EmbeddedPython_BasicIdentityCalls/expected_results/RUN.log	2007-07-28 03:55:55 UTC (rev 7853)
@@ -52,7 +52,7 @@
 isInvokable(nullary)           : 1
 Calling nullary                : Called nullary()
 Calling unary_int(42)          : 42
-Calling unary_long(int64_t(42)): 42
+Calling unary_long(2**64 - 1)  : 18446744073709551615
 Calling unary_float(42.01)     : 42.01
 Calling unary_str('Hello')     : Hello
 Calling unary_vec(v)           : 2 3 5 7 11 13 17 19 23 

Modified: trunk/plearn/python/test/.pytest/EmbeddedPython_InterfunctionXchg/expected_results/RUN.log
===================================================================
--- trunk/plearn/python/test/.pytest/EmbeddedPython_InterfunctionXchg/expected_results/RUN.log	2007-07-28 02:30:07 UTC (rev 7852)
+++ trunk/plearn/python/test/.pytest/EmbeddedPython_InterfunctionXchg/expected_results/RUN.log	2007-07-28 03:55:55 UTC (rev 7853)
@@ -43,5 +43,5 @@
 Error while calling function 'get_value' with no params.'
 Associated 'some_global_map' with: {Oui: 16, bon: 512, est: 64, et: 256, il: 32, juste: 128}
 Read back from Python environment: {Oui: 16, bon: 512, est: 64, et: 256, il: 32, juste: 128}
-Printing some_global_map within Python: {'Oui': 16L, 'est': 64L, 'juste': 128L, 'il': 32L, 'bon': 512L, 'et': 256L}
+Printing some_global_map within Python: {'Oui': 16, 'est': 64, 'juste': 128, 'il': 32, 'bon': 512, 'et': 256}
 Dump of the 'python_other' compiled environment

Modified: trunk/plearn/python/test/BasicIdentityCallsTest.cc
===================================================================
--- trunk/plearn/python/test/BasicIdentityCallsTest.cc	2007-07-28 02:30:07 UTC (rev 7852)
+++ trunk/plearn/python/test/BasicIdentityCallsTest.cc	2007-07-28 03:55:55 UTC (rev 7853)
@@ -197,8 +197,8 @@
     cout << "Calling unary_int(42)          : "
          << python->invoke("unary_int", 42).as<int>() << endl;
 
-    cout << "Calling unary_long(int64_t(42)): "
-         << python->invoke("unary_long", int64_t(42)).as<int64_t>() << endl;
+    cout << "Calling unary_long(2**64 - 1)  : "
+         << python->invoke("unary_long", 18446744073709551615ULL).as<unsigned long long>() << endl;
 
     cout << "Calling unary_float(42.01)     : "
          << python->invoke("unary_float", 42.01).as<double>() << endl;
@@ -241,13 +241,13 @@
                                    .as< vector<string> >() ))
          << endl;
 
-    map<string,int32_t> mapsd;
+    map<string,int> mapsd;
     string str_mapsd = "{ Oui:16 il:32 est:64 juste:128 et:256 bon:512 }";
     PStream is_mapsd = openString(str_mapsd, PStream::plearn_ascii);
     is_mapsd >> mapsd;
 
     cout << "Calling unary_dict(mapsd)      : "
-         << tostring( python->invoke("unary_dict", mapsd).as< map<string,int32_t> >() )
+         << tostring( python->invoke("unary_dict", mapsd).as< map<string,int> >() )
          << endl;
 }
 

Modified: trunk/plearn/python/test/InterfunctionXchgTest.cc
===================================================================
--- trunk/plearn/python/test/InterfunctionXchgTest.cc	2007-07-28 02:30:07 UTC (rev 7852)
+++ trunk/plearn/python/test/InterfunctionXchgTest.cc	2007-07-28 03:55:55 UTC (rev 7853)
@@ -198,7 +198,7 @@
 
 
     try {
-        map<string,int64_t> mapsd;
+        map<string,int> mapsd;
         string str_mapsd = "{ Oui:16 il:32 est:64 juste:128 et:256 bon:512 }";
         PStream is_mapsd = openString(str_mapsd, PStream::plearn_ascii);
         is_mapsd >> mapsd;
@@ -206,7 +206,7 @@
         python_other->setGlobalObject("some_global_map", PythonObjectWrapper(mapsd));
         cout << "Associated 'some_global_map' with: " << tostring(mapsd) << endl;
         cout << "Read back from Python environment: "
-             << tostring(python_other->getGlobalObject("some_global_map").as< map<string,int64_t> >())
+             << tostring(python_other->getGlobalObject("some_global_map").as< map<string,int> >())
              << endl;
         python_other->invoke("print_global_map");
 

Modified: trunk/plearn/python/test/MemoryStressTest.cc
===================================================================
--- trunk/plearn/python/test/MemoryStressTest.cc	2007-07-28 02:30:07 UTC (rev 7852)
+++ trunk/plearn/python/test/MemoryStressTest.cc	2007-07-28 03:55:55 UTC (rev 7853)
@@ -191,7 +191,8 @@
 void MemoryStressTest::unary(const PythonCodeSnippet* python)
 {
     int i       = python->invoke("unary_int", 42).as<int>();
-    int64_t l   = python->invoke("unary_long", int64_t(42)).as<int64_t>();
+    unsigned long long l
+                = python->invoke("unary_long", 18446744073709551615ULL).as<unsigned long long>();
     double d    = python->invoke("unary_float", 42.01).as<double>();
     string s    = python->invoke("unary_str", "Hello").as<string>();
 



From lamblin at mail.berlios.de  Sat Jul 28 06:13:58 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Sat, 28 Jul 2007 06:13:58 +0200
Subject: [Plearn-commits] r7854 - trunk/plearn/python
Message-ID: <200707280413.l6S4Dwuc017783@sheep.berlios.de>

Author: lamblin
Date: 2007-07-28 06:13:58 +0200 (Sat, 28 Jul 2007)
New Revision: 7854

Modified:
   trunk/plearn/python/PythonObjectWrapper.h
Log:
Fix some errors and disable some icc remarks


Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2007-07-28 03:55:55 UTC (rev 7853)
+++ trunk/plearn/python/PythonObjectWrapper.h	2007-07-28 04:13:58 UTC (rev 7854)
@@ -52,6 +52,7 @@
 #include <utility>                           // Pairs
 #include <vector>                            // vector<T>
 #include <map>                               // map<T,U>
+#include <limits>                            // numeric_limits<I>
 
 // From PLearn
 #include <plearn/math/TVec.h>
@@ -101,13 +102,21 @@
         result = static_cast<I>(x);
 
         // Check if x fits into type I (overflow or sign problem)
+#ifdef __INTEL_COMPILER
+#pragma warning(disable:1682)
+// Yes, I know that "implicit conversion of a 64-bit integral type to a smaller
+// integral type (potential portability problem)", but the conversion is
+// explicit here.
+#endif
         if (static_cast<long>(result) != x
             || !(numeric_limits<I>::is_signed) && x<0)
         {
             PLPythonConversionError("integerFromPyObject<I>", pyobj,
                                     print_traceback);
         }
-
+#ifdef __INTEL_COMPILER
+#pragma warning(default:1682)
+#endif
     }
     else if (PyLong_Check(pyobj))
     {
@@ -121,7 +130,16 @@
             // Check for possible overflow during conversion
             if (!PyErr_Occurred())
             {
+#ifdef __INTEL_COMPILER
+#pragma warning(disable:1682)
+// Yes, I know that "implicit conversion of a 64-bit integral type to a smaller
+// integral type (potential portability problem)", but the conversion is
+// explicit here.
+#endif
                 result = static_cast<I>(x);
+#ifdef __INTEL_COMPILER
+#pragma warning(default:1682)
+#endif
 
                 // Check if x fits into type I (overflow only, there
                 // cannot be any sign error because I is signed, too)
@@ -147,7 +165,16 @@
             // Check for possible overflow during conversion
             if (!PyErr_Occurred())
             {
+#ifdef __INTEL_COMPILER
+#pragma warning(disable:1682)
+// Yes, I know that "implicit conversion of a 64-bit integral type to a smaller
+// integral type (potential portability problem)", but the conversion is
+// explicit here.
+#endif
                 result = static_cast<I>(x);
+#ifdef __INTEL_COMPILER
+#pragma warning(default:1682)
+#endif
 
                 // Check if x fits into type I (overflow only)
                 if (static_cast<unsigned long long>(result) != x)
@@ -210,14 +237,14 @@
 template <>
 struct ConvertFromPyObject<short>
 {
-    static int convert(PyObject* pyobj, bool print_traceback)
+    static short convert(PyObject* pyobj, bool print_traceback)
     { return integerFromPyObject<short>(pyobj, print_traceback); }
 };
 
 template <>
 struct ConvertFromPyObject<unsigned short>
 {
-    static int convert(PyObject* pyobj, bool print_traceback)
+    static unsigned short convert(PyObject* pyobj, bool print_traceback)
     { return integerFromPyObject<unsigned short>(pyobj, print_traceback); }
 };
 
@@ -231,35 +258,35 @@
 template <>
 struct ConvertFromPyObject<unsigned int>
 {
-    static int convert(PyObject* pyobj, bool print_traceback)
+    static unsigned int convert(PyObject* pyobj, bool print_traceback)
     { return integerFromPyObject<unsigned int>(pyobj, print_traceback); }
 };
 
 template <>
 struct ConvertFromPyObject<long>
 {
-    static int convert(PyObject* pyobj, bool print_traceback)
+    static long convert(PyObject* pyobj, bool print_traceback)
     { return integerFromPyObject<long>(pyobj, print_traceback); }
 };
 
 template <>
 struct ConvertFromPyObject<unsigned long>
 {
-    static int convert(PyObject* pyobj, bool print_traceback)
+    static unsigned long convert(PyObject* pyobj, bool print_traceback)
     { return integerFromPyObject<unsigned long>(pyobj, print_traceback); }
 };
 
 template <>
 struct ConvertFromPyObject<long long>
 {
-    static int convert(PyObject* pyobj, bool print_traceback)
+    static long long convert(PyObject* pyobj, bool print_traceback)
     { return integerFromPyObject<long long>(pyobj, print_traceback); }
 };
 
 template <>
 struct ConvertFromPyObject<unsigned long long>
 {
-    static int convert(PyObject* pyobj, bool print_traceback)
+    static unsigned long long convert(PyObject* pyobj, bool print_traceback)
     { return integerFromPyObject<unsigned long long>(pyobj, print_traceback); }
 };
 
@@ -420,7 +447,16 @@
 PyObject* integerToPyObject(const I& x)
 {
     // Try to convert x to a long
+#ifdef __INTEL_COMPILER
+#pragma warning(disable:1682)
+// Yes, I know that "implicit conversion of a 64-bit integral type to a smaller
+// integral type (potential portability problem)", but the conversion is
+// explicit here.
+#endif
     long y = static_cast<long>(x);
+#ifdef __INTEL_COMPILER
+#pragma warning(default:1682)
+#endif
 
     // Check if we lost value information or sign
     if (static_cast<I>(y) == x && (numeric_limits<I>::is_signed || y >= 0))



From lamblin at mail.berlios.de  Sat Jul 28 06:33:34 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Sat, 28 Jul 2007 06:33:34 +0200
Subject: [Plearn-commits] r7855 - trunk/plearn/python
Message-ID: <200707280433.l6S4XYAs024881@sheep.berlios.de>

Author: lamblin
Date: 2007-07-28 06:33:24 +0200 (Sat, 28 Jul 2007)
New Revision: 7855

Modified:
   trunk/plearn/python/PythonCodeSnippet.cc
   trunk/plearn/python/PythonCodeSnippet.h
   trunk/plearn/python/PythonObjectWrapper.cc
   trunk/plearn/python/PythonObjectWrapper.h
Log:
Cosmetic changes


Modified: trunk/plearn/python/PythonCodeSnippet.cc
===================================================================
--- trunk/plearn/python/PythonCodeSnippet.cc	2007-07-28 04:13:58 UTC (rev 7854)
+++ trunk/plearn/python/PythonCodeSnippet.cc	2007-07-28 04:33:24 UTC (rev 7855)
@@ -2,22 +2,22 @@
 
 // PythonCodeSnippet.cc
 //
-// Copyright (C) 2005 Nicolas Chapados 
-// 
+// Copyright (C) 2005 Nicolas Chapados
+//
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
-// 
+//
 //  1. Redistributions of source code must retain the above copyright
 //     notice, this list of conditions and the following disclaimer.
-// 
+//
 //  2. Redistributions in binary form must reproduce the above copyright
 //     notice, this list of conditions and the following disclaimer in the
 //     documentation and/or other materials provided with the distribution.
-// 
+//
 //  3. The name of the authors may not be used to endorse or promote
 //     products derived from this software without specific prior written
 //     permission.
-// 
+//
 // THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
 // IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 // OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
@@ -28,12 +28,12 @@
 // LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 // NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 // SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-// 
+//
 // This file is part of the PLearn library. For more information on the PLearn
 // library, go to the PLearn Web site at www.plearn.org
 
-/* *******************************************************      
- * $Id: PythonCodeSnippet.cc 2771 2005-08-11 22:06:13Z chapados $ 
+/* *******************************************************
+ * $Id: PythonCodeSnippet.cc 2771 2005-08-11 22:06:13Z chapados $
  ******************************************************* */
 
 // Authors: Nicolas Chapados
@@ -105,8 +105,8 @@
     "thread-safe, i.e. the Python Global Interpreter Lock is always acquired\n"
     "before sensitive operations are carried out.\n"
     );
-  
 
+
 PythonCodeSnippet::PythonCodeSnippet(const string& code,
                                      bool remap_python_exceptions)
     : inherited(),
@@ -166,7 +166,7 @@
         // initialized into a STATIC VARIABLE of the translation unit!
         import_libnumarray();
         numarray_initialized = true;
-        
+
         PythonObjectWrapper::initializePython();
     }
 
@@ -181,7 +181,7 @@
                                             string(set_current_snippet)+m_code);
         resetCurrentSnippet();
     }
-    
+
     // Forget about injected functions
     m_injected_functions.purge_memory();
     m_python_methods.purge_memory();
@@ -263,7 +263,7 @@
             pFunc= PyObject_GetAttrString(m_instance.getPyObject(), fn);
         delete[] fn;
     }
-    if(pFunc) 
+    if(pFunc)
         instance_method= true;
     else
         pFunc= PyDict_GetItemString(m_compiled_code.getPyObject(),
@@ -291,7 +291,7 @@
             pFunc= PyObject_GetAttrString(m_instance.getPyObject(), fn);
         delete[] fn;
     }
-    if(pFunc) 
+    if(pFunc)
         instance_method= true;
     else
         pFunc= PyDict_GetItemString(m_compiled_code.getPyObject(),
@@ -306,7 +306,7 @@
         return_value = PyObject_CallObject(pFunc, NULL);
         if (! return_value)
         {
-            if(instance_method) 
+            if(instance_method)
                 Py_DECREF(pFunc);
             handlePythonErrors(string("Error while calling function '")
                                + function_name
@@ -347,7 +347,7 @@
             pFunc= PyObject_GetAttrString(m_instance.getPyObject(), fn);
         delete[] fn;
     }
-    if(pFunc) 
+    if(pFunc)
         instance_method= true;
     else
         pFunc= PyDict_GetItemString(m_compiled_code.getPyObject(),
@@ -356,9 +356,9 @@
     // pFunc: Borrowed reference if not instance_method
 
     PyObject* return_value = 0;
-    if (pFunc && PyCallable_Check(pFunc)) {        
+    if (pFunc && PyCallable_Check(pFunc)) {
         setCurrentSnippet(m_handle);
-        
+
         // Create argument tuple.  Warning: PyTuple_SetItem STEALS references.
         PyObject* pArgs = PyTuple_New(args.size());
         for (int i=0, n=args.size() ; i<n ; ++i)
@@ -366,31 +366,31 @@
             PyTuple_SetItem(pArgs, i, args[i].getPyObject());
             Py_INCREF(args[i].getPyObject());
         }
-        
+
         return_value = PyObject_CallObject(pFunc, pArgs);
 
         Py_DECREF(pArgs);
         if (! return_value)
         {
-            if(instance_method) 
+            if(instance_method)
                 Py_DECREF(pFunc);
             handlePythonErrors(string("Error while calling function '")
                                + function_name
-                               + "' with " 
+                               + "' with "
                                + tostring(args.length())
                                + " params.");
         }
-        resetCurrentSnippet();        
+        resetCurrentSnippet();
     }
     else
     {
-        if(instance_method) 
+        if(instance_method)
             Py_DECREF(pFunc);
         PLERROR("PythonCodeSnippet::invoke: cannot call function '%s'",
                 function_name);
     }
 
-    if(instance_method) 
+    if(instance_method)
         Py_DECREF(pFunc);
     //return PythonObjectWrapper(return_value);
     PythonObjectWrapper r(return_value);
@@ -415,10 +415,10 @@
         int size = PyTuple_GET_SIZE(args);
         TVec<PythonObjectWrapper> args_tvec(size);
         for (int i=0 ; i<size ; ++i) {
-            args_tvec[i]= 
+            args_tvec[i]=
                 PythonObjectWrapper(PyTuple_GET_ITEM(args,i));
         }
-        
+
         // Now get the void* stored within the PyCObject of self
         StandaloneFunction* func =
             static_cast<StandaloneFunction*>(PyCObject_AsVoidPtr(self));
@@ -429,13 +429,13 @@
     }
     // Catch PLERROR and such
     catch (const PLearnError& e) {
-        PyErr_SetString(PyExc_Exception, 
+        PyErr_SetString(PyExc_Exception,
                         (string("PLearn Error: ")+e.message()).c_str());
         return NULL;
     }
     // Catch C++ stdlib exceptions
     catch (const std::exception& e) {
-        PyErr_SetString(PyExc_Exception, 
+        PyErr_SetString(PyExc_Exception,
                         (string("C++ stdlib error: ")+e.what()).c_str());
         return NULL;
     }
@@ -457,18 +457,18 @@
 
     // Wrap the function_ptr into a PyCObject
     PyObject* self = PyCObject_FromVoidPtr(function_ptr, NULL);
-    
+
     // Create a Python Function Object
     PyMethodDef* py_method = m_python_methods.allocate();
     py_method->ml_name  = const_cast<char*>(python_name);
     py_method->ml_meth  = pythonTrampoline;
     py_method->ml_flags = METH_VARARGS;
     py_method->ml_doc   = "injected-function-from-PythonCodeSnippet";
-    
+
     PyObject* py_funcobj = PyCFunction_NewEx(py_method,
                                              self /* info for trampoline */,
                                              NULL /* module */);
-    
+
     if (py_funcobj) {
         // Inject into the running snippet.  Note that when a
         // PythonObjectWrapper is constructed from a PyObject, it steals the
@@ -478,7 +478,7 @@
         {
             char* fn= new char[strlen(python_name)+1];
             strcpy(fn, python_name);
-            PyObject_SetAttrString(m_instance.getPyObject(), 
+            PyObject_SetAttrString(m_instance.getPyObject(),
                                    fn, py_funcobj);
             delete[] fn;
         }
@@ -487,7 +487,7 @@
             // Publish the injection in the '__injected__' dictionary for imported modules
             PythonObjectWrapper inj_dict = this->getGlobalObject("__injected__");
             PyDict_SetItemString(inj_dict.getPyObject(), python_name, py_funcobj);
-            
+
             Py_XDECREF(self);
         }
     }
@@ -546,14 +546,14 @@
     //get the global env. as an stl map
     PythonObjectWrapper wrapped_globals(globals);
     Py_XDECREF(globals);
-    map<string, PyObject*> global_map= 
+    map<string, PyObject*> global_map=
         wrapped_globals.as<map<string, PyObject*> >();
 
     //inject global funcs, if not already done
     static bool global_funcs_injected= false;
     if(!global_funcs_injected)
     {
-        map<string, PyObject*>::iterator it= 
+        map<string, PyObject*>::iterator it=
             global_map.find("pl_global_funcs");
         if(it == global_map.end())
             PLERROR("in PythonCodeSnippet::compileGlobalCode : "
@@ -564,7 +564,7 @@
 
     //try to find an EmbeddedCodeSnippet to instantiate
     PyObject* snippet_found= 0;
-    map<string, PyObject*>::iterator it_id= 
+    map<string, PyObject*>::iterator it_id=
         global_map.find("pl_embedded_code_snippet_type");
 
     if(it_id != global_map.end())
@@ -574,7 +574,7 @@
         list<pair<string, PyObject*> > classes_found;
 
         //iter (find)
-        PyTypeObject* embedded_code_snippet_type= 
+        PyTypeObject* embedded_code_snippet_type=
             (PyTypeObject*)global_map["EmbeddedCodeSnippet"];
 
         //find all classes deriving from EmbeddedCodeSnippet
@@ -582,9 +582,9 @@
             it != global_map.end(); ++it)
         {
             if(PyType_Check(it->second)
-               && 0 != PyObject_Compare(it->second, 
+               && 0 != PyObject_Compare(it->second,
                                         (PyObject*)embedded_code_snippet_type)
-               && PyType_IsSubtype((PyTypeObject*)it->second, 
+               && PyType_IsSubtype((PyTypeObject*)it->second,
                                    embedded_code_snippet_type))
             {
                 classes_found.push_back(*it);
@@ -607,7 +607,7 @@
         if(nclasses == 1)
             snippet_found= jt->second;
     }
-    
+
     if(snippet_found)
     {//instantiate object of appropriate type
         PyObject* pyparams= PyDict_New();
@@ -636,8 +636,8 @@
             PLERROR("in PythonCodeSnippet::compileGlobalCode : "
                     "found subclass of EmbeddedCodeSnippet, but can't "
                     "call constructor with given params.  "
-                    "class='%s', params=%s", 
-                    ((PyTypeObject*)snippet_found)->tp_name, 
+                    "class='%s', params=%s",
+                    ((PyTypeObject*)snippet_found)->tp_name,
                     tostring(m_instance_params).c_str());
         }
         m_instance= PythonObjectWrapper(the_obj);
@@ -665,14 +665,14 @@
 void PythonCodeSnippet::setCurrentSnippet(const void* handle) const
 {
     PythonGlobalInterpreterLock gil;         // For thread-safety
-    
+
     char set_current_snippet[100];
-    sprintf(set_current_snippet, SetCurrentSnippetVar, handle);    
-    PyObject* res= PyRun_String(set_current_snippet, 
+    sprintf(set_current_snippet, SetCurrentSnippetVar, handle);
+    PyObject* res= PyRun_String(set_current_snippet,
                                 Py_file_input /* exec code block */,
-                                m_compiled_code.getPyObject(), 
+                                m_compiled_code.getPyObject(),
                                 m_compiled_code.getPyObject());
-    
+
     Py_XDECREF(res);
     if (PyErr_Occurred()) {
         Py_XDECREF(m_compiled_code.getPyObject());
@@ -685,10 +685,10 @@
 void PythonCodeSnippet::resetCurrentSnippet() const
 {
     PythonGlobalInterpreterLock gil;         // For thread-safety
-    
-    PyObject* res= PyRun_String(ResetCurrentSnippetVar, 
+
+    PyObject* res= PyRun_String(ResetCurrentSnippetVar,
                                 Py_file_input /* exec code block */,
-                                m_compiled_code.getPyObject(), 
+                                m_compiled_code.getPyObject(),
                                 m_compiled_code.getPyObject());
     Py_XDECREF(res);
     if (PyErr_Occurred()) {
@@ -719,9 +719,9 @@
                                              "exception but there is no traceback.\n")
                                       + extramsg);
             }
-            
 
-            PyObject* tbstr= 
+
+            PyObject* tbstr=
                 PyString_FromString("plearn.utilities.pltraceback");
             PyObject* tbmod= PyImport_Import(tbstr);
             Py_XDECREF(tbstr);
@@ -745,7 +745,7 @@
                                       " call to cgitb.text failed");
             string str= PyString_AsString(pystr);
             Py_XDECREF(pystr);
-            
+
             PyErr_Clear();
 
             Py_XDECREF(exception);
@@ -756,10 +756,10 @@
         else {
             PyErr_Print();
             PyErr_Clear();
-            PLERROR("PythonCodeSnippet: encountered Python exception.\n%s", 
+            PLERROR("PythonCodeSnippet: encountered Python exception.\n%s",
                     extramsg.c_str());
         }
-    }  
+    }
 }
 
 
@@ -769,7 +769,7 @@
 }
 
 
-    
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/python/PythonCodeSnippet.h
===================================================================
--- trunk/plearn/python/PythonCodeSnippet.h	2007-07-28 04:13:58 UTC (rev 7854)
+++ trunk/plearn/python/PythonCodeSnippet.h	2007-07-28 04:33:24 UTC (rev 7855)
@@ -2,22 +2,22 @@
 
 // PythonCodeSnippet.h
 //
-// Copyright (C) 2005 Nicolas Chapados 
-// 
+// Copyright (C) 2005 Nicolas Chapados
+//
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
-// 
+//
 //  1. Redistributions of source code must retain the above copyright
 //     notice, this list of conditions and the following disclaimer.
-// 
+//
 //  2. Redistributions in binary form must reproduce the above copyright
 //     notice, this list of conditions and the following disclaimer in the
 //     documentation and/or other materials provided with the distribution.
-// 
+//
 //  3. The name of the authors may not be used to endorse or promote
 //     products derived from this software without specific prior written
 //     permission.
-// 
+//
 // THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
 // IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 // OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
@@ -28,12 +28,12 @@
 // LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 // NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 // SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-// 
+//
 // This file is part of the PLearn library. For more information on the PLearn
 // library, go to the PLearn Web site at www.plearn.org
 
-/* *******************************************************      
- * $Id: PythonCodeSnippet.h 2797 2005-08-25 14:06:26Z chapados $ 
+/* *******************************************************
+ * $Id: PythonCodeSnippet.h 2797 2005-08-25 14:06:26Z chapados $
  ******************************************************* */
 
 // Authors: Nicolas Chapados
@@ -70,7 +70,7 @@
 class PythonException : public PLearnError
 {
     typedef PLearnError inherited;
-    
+
 public:
     PythonException(const string& message)
         : inherited(message)
@@ -118,14 +118,14 @@
      */
     typedef boost::function<PythonObjectWrapper (
         const TVec<PythonObjectWrapper>& args)> StandaloneFunction;
-    
+
     //! The snippet prepended to 'code' option for the injections to behave properly.
     static const char* InjectSetupSnippet;
 
     //! Used to (un)set CURRENT_SNIPPET in Python
     static const char* SetCurrentSnippetVar;
     static const char* ResetCurrentSnippetVar;
-    
+
 public:
     /**
      *  Python statement list that should be compiled at build time to provide
@@ -152,7 +152,7 @@
 
     //! the python object instance
     PythonObjectWrapper m_instance;
-    
+
 public:
     //! Default constructor.  Note that "build" IS NOT CALLED from the
     //! constructor and must be called manually after all external functions
@@ -162,7 +162,7 @@
 
     //! Default copy ctor, assignment op, dtor
 
-    
+
     //#####  Global Environment Interface  ####################################
 
     //! Return an object from the global environment.  Return None if the
@@ -180,7 +180,7 @@
     }
 
 
-    
+
     //#####  Function Call Interface  #########################################
 
     //! Checks whether the specified function name is callable
@@ -194,25 +194,25 @@
     //! 'transfer_ownership' option.
     PythonObjectWrapper invoke(const char* function_name,
                                const TVec<PythonObjectWrapper>& args) const;
-    
+
     //! Call the specified function with 1 argument.
     template <class T>
     PythonObjectWrapper invoke(const char* function_name,
                                const T& arg1) const;
-    
+
     //! Call the specified function with 2 arguments.
     template <class T, class U>
     PythonObjectWrapper invoke(const char* function_name,
                                const T& arg1,
                                const U& arg2) const;
-    
+
     //! Call the specified function with 3 arguments.
     template <class T, class U, class V>
     PythonObjectWrapper invoke(const char* function_name,
                                const T& arg1,
                                const U& arg2,
                                const V& arg3) const;
-    
+
     //! Call the specified function with 4 arguments.
     template <class T, class U, class V, class W>
     PythonObjectWrapper invoke(const char* function_name,
@@ -284,7 +284,7 @@
     template <class T>
     void inject(const char* python_name, const T* object,
                 PythonObjectWrapper (T::*)(const TVec<PythonObjectWrapper>&) const);
-    
+
     /**
      *  Inject a bound C++ member function into the Python code under the given
      *  name (non-const version).
@@ -299,14 +299,14 @@
      *  debugging purposes.
      */
     void dumpPythonEnvironment();
-    
 
+
     //#####  PLearn::Object Standard Functions  ###############################
 
     // Declares other standard object methods
     PLEARN_DECLARE_OBJECT(PythonCodeSnippet);
 
-    // simply calls inherited::build() then build_() 
+    // simply calls inherited::build() then build_()
     virtual void build();
 
     //! Transforms a shallow copy into a deep copy
@@ -314,7 +314,7 @@
 
     virtual void run();
 
-protected: 
+protected:
     //! Declares this class' options
     static void declareOptions(OptionList& ol);
 
@@ -324,7 +324,7 @@
 
     void setCurrentSnippet(const void* handle) const;
     void resetCurrentSnippet() const;
-    
+
     //! If no Python error, do nothing.  If an error occurred, convert the
     //! Python Exception into a C++ exception if required, or otherwise print a
     //! traceback and abort
@@ -342,7 +342,7 @@
     //! The Python handle for *this* instance
     void* m_handle;
 
-public:    
+public:
     //! Compiled Python code module and global environment
     PythonObjectWrapper m_compiled_code;
 
@@ -352,8 +352,8 @@
 
     //! Injected Python method definitions
     PObjectPool<PyMethodDef> m_python_methods;
-    
-private: 
+
+private:
     //! This does the actual building.  This is where the Python code
     //! is in fact compiled
     void build_();
@@ -393,16 +393,16 @@
             PLERROR("PythonCodeSnippet::invoke: error during argument conversion "
                     "from C++ to Python for function '%s'", function_name);
         }
-        
-        PyTuple_SetItem(pArgs, 0, py_arg1);        
 
+        PyTuple_SetItem(pArgs, 0, py_arg1);
+
         return_value = PyObject_CallObject(pFunc, pArgs);
 
         Py_XDECREF(pArgs);
         if (! return_value)
             handlePythonErrors();
 
-        resetCurrentSnippet();        
+        resetCurrentSnippet();
     }
     else {
         PLERROR("PythonCodeSnippet::invoke: cannot call function '%s'",
@@ -445,10 +445,10 @@
             PLERROR("PythonCodeSnippet::invoke: error during argument conversion "
                     "from C++ to Python for function '%s'", function_name);
         }
-        
+
         PyTuple_SetItem(pArgs, 0, py_arg1);
         PyTuple_SetItem(pArgs, 1, py_arg2);
-        
+
         return_value = PyObject_CallObject(pFunc, pArgs);
 
         Py_XDECREF(pArgs);
@@ -502,7 +502,7 @@
             PLERROR("PythonCodeSnippet::invoke: error during argument conversion "
                     "from C++ to Python for function '%s'", function_name);
         }
-        
+
         PyTuple_SetItem(pArgs, 0, py_arg1);
         PyTuple_SetItem(pArgs, 1, py_arg2);
         PyTuple_SetItem(pArgs, 2, py_arg3);
@@ -563,12 +563,12 @@
             PLERROR("PythonCodeSnippet::invoke: error during argument conversion "
                     "from C++ to Python for function '%s'", function_name);
         }
-        
+
         PyTuple_SetItem(pArgs, 0, py_arg1);
         PyTuple_SetItem(pArgs, 1, py_arg2);
         PyTuple_SetItem(pArgs, 2, py_arg3);
         PyTuple_SetItem(pArgs, 3, py_arg4);
-        
+
         return_value = PyObject_CallObject(pFunc, pArgs);
 
         Py_XDECREF(pArgs);
@@ -628,13 +628,13 @@
             PLERROR("PythonCodeSnippet::invoke: error during argument conversion "
                     "from C++ to Python for function '%s'", function_name);
         }
-        
+
         PyTuple_SetItem(pArgs, 0, py_arg1);
         PyTuple_SetItem(pArgs, 1, py_arg2);
         PyTuple_SetItem(pArgs, 2, py_arg3);
         PyTuple_SetItem(pArgs, 3, py_arg4);
         PyTuple_SetItem(pArgs, 4, py_arg5);
-        
+
         return_value = PyObject_CallObject(pFunc, pArgs);
 
         Py_XDECREF(pArgs);
@@ -699,14 +699,14 @@
             PLERROR("PythonCodeSnippet::invoke: error during argument conversion "
                     "from C++ to Python for function '%s'", function_name);
         }
-        
+
         PyTuple_SetItem(pArgs, 0, py_arg1);
         PyTuple_SetItem(pArgs, 1, py_arg2);
         PyTuple_SetItem(pArgs, 2, py_arg3);
         PyTuple_SetItem(pArgs, 3, py_arg4);
         PyTuple_SetItem(pArgs, 4, py_arg5);
         PyTuple_SetItem(pArgs, 5, py_arg6);
-        
+
         return_value = PyObject_CallObject(pFunc, pArgs);
 
         Py_XDECREF(pArgs);
@@ -776,7 +776,7 @@
             PLERROR("PythonCodeSnippet::invoke: error during argument conversion "
                     "from C++ to Python for function '%s'", function_name);
         }
-        
+
         PyTuple_SetItem(pArgs, 0, py_arg1);
         PyTuple_SetItem(pArgs, 1, py_arg2);
         PyTuple_SetItem(pArgs, 2, py_arg3);
@@ -784,7 +784,7 @@
         PyTuple_SetItem(pArgs, 4, py_arg5);
         PyTuple_SetItem(pArgs, 5, py_arg6);
         PyTuple_SetItem(pArgs, 6, py_arg7);
-        
+
         return_value = PyObject_CallObject(pFunc, pArgs);
 
         Py_XDECREF(pArgs);

Modified: trunk/plearn/python/PythonObjectWrapper.cc
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.cc	2007-07-28 04:13:58 UTC (rev 7854)
+++ trunk/plearn/python/PythonObjectWrapper.cc	2007-07-28 04:33:24 UTC (rev 7855)
@@ -2,23 +2,23 @@
 
 // PythonObjectWrapper.cc
 //
-// Copyright (C) 2005-2006 Nicolas Chapados 
+// Copyright (C) 2005-2006 Nicolas Chapados
 // Copyright (C) 2007 Xavier Saint-Mleux, ApSTAT Technologies inc.
-// 
+//
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
-// 
+//
 //  1. Redistributions of source code must retain the above copyright
 //     notice, this list of conditions and the following disclaimer.
-// 
+//
 //  2. Redistributions in binary form must reproduce the above copyright
 //     notice, this list of conditions and the following disclaimer in the
 //     documentation and/or other materials provided with the distribution.
-// 
+//
 //  3. The name of the authors may not be used to endorse or promote
 //     products derived from this software without specific prior written
 //     permission.
-// 
+//
 // THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
 // IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 // OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
@@ -29,12 +29,12 @@
 // LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 // NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 // SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-// 
+//
 // This file is part of the PLearn library. For more information on the PLearn
 // library, go to the PLearn Web site at www.plearn.org
 
-/* *******************************************************      
-   * $Id: .pyskeleton_header 544 2003-09-01 00:05:31Z plearner $ 
+/* *******************************************************
+   * $Id: .pyskeleton_header 544 2003-09-01 00:05:31Z plearner $
    ******************************************************* */
 
 // Authors: Nicolas Chapados
@@ -98,58 +98,6 @@
 }
 
 
-#if 0
-int ConvertFromPyObject<int>::convert(PyObject* pyobj, bool print_traceback)
-{
-    PLASSERT( pyobj );
-    if (! PyInt_Check(pyobj))
-        PLPythonConversionError("ConvertFromPyObject<int>", pyobj, print_traceback);
-    return int(PyInt_AS_LONG(pyobj));
-}
-
-unsigned int ConvertFromPyObject<unsigned int>::convert(PyObject* pyobj, bool print_traceback)
-{
-    PLASSERT( pyobj );
-    if (! PyInt_Check(pyobj))
-        PLPythonConversionError("ConvertFromPyObject<unsigned int>", pyobj, print_traceback);
-    return static_cast<unsigned int>(PyInt_AsUnsignedLongMask(pyobj));
-}
-
-/*
-long ConvertFromPyObject<long>::convert(PyObject* pyobj, bool print_traceback)
-{
-    PLASSERT( pyobj );
-    if (! PyLong_Check(pyobj))
-        PLPythonConversionError("ConvertFromPyObject<long>", pyobj, print_traceback);
-    return PyLong_AsLong(pyobj);
-}
-
-unsigned long ConvertFromPyObject<unsigned long>::convert(PyObject* pyobj, bool print_traceback)
-{
-    PLASSERT( pyobj );
-    if (! PyInt_Check(pyobj))
-        PLPythonConversionError("ConvertFromPyObject<unsigned long>", pyobj, print_traceback);
-    return PyInt_AsUnsignedLongMask(pyobj);
-}
-*/
-
-int64_t ConvertFromPyObject<int64_t>::convert(PyObject* pyobj, bool print_traceback)
-{
-    PLASSERT( pyobj );
-    if (! PyLong_Check(pyobj))
-        PLPythonConversionError("ConvertFromPyObject<int64_t>", pyobj, print_traceback);
-    return PyLong_AsLongLong(pyobj);
-}
-
-uint64_t ConvertFromPyObject<uint64_t>::convert(PyObject* pyobj, bool print_traceback)
-{
-    PLASSERT( pyobj );
-    if (! PyInt_Check(pyobj))
-        PLPythonConversionError("ConvertFromPyObject<uint64_t>", pyobj, print_traceback);
-    return PyInt_AsUnsignedLongLongMask(pyobj);
-}
-#endif
-
 double ConvertFromPyObject<double>::convert(PyObject* pyobj,
                                             bool print_traceback)
 {
@@ -180,7 +128,8 @@
     return 0;//shut up compiler
 }
 
-string ConvertFromPyObject<string>::convert(PyObject* pyobj, bool print_traceback)
+string ConvertFromPyObject<string>::convert(PyObject* pyobj,
+                                            bool print_traceback)
 {
     PLASSERT( pyobj );
     if (! PyString_Check(pyobj))
@@ -189,7 +138,8 @@
     return PyString_AsString(pyobj);
 }
 
-PPath ConvertFromPyObject<PPath>::convert(PyObject* pyobj, bool print_traceback)
+PPath ConvertFromPyObject<PPath>::convert(PyObject* pyobj,
+                                          bool print_traceback)
 {
     PLASSERT( pyobj );
     if (! PyString_Check(pyobj))
@@ -198,7 +148,8 @@
     return PPath(PyString_AsString(pyobj));
 }
 
-PPointable* ConvertFromPyObject<PPointable*>::convert(PyObject* pyobj, bool print_traceback)
+PPointable* ConvertFromPyObject<PPointable*>::convert(PyObject* pyobj,
+                                                      bool print_traceback)
 {
     PLASSERT(pyobj);
     if (! PyCObject_Check(pyobj))
@@ -207,7 +158,8 @@
     return static_cast<PPointable*>(PyCObject_AsVoidPtr(pyobj));
 }
 
-Object* ConvertFromPyObject<Object*>::convert(PyObject* pyobj, bool print_traceback)
+Object* ConvertFromPyObject<Object*>::convert(PyObject* pyobj,
+                                              bool print_traceback)
 {
     PLASSERT(pyobj);
     if(pyobj == Py_None)
@@ -225,19 +177,21 @@
         PLPythonConversionError("ConvertFromPyObject<Object*>", pyobj,
                                 print_traceback);
     Object* obj= static_cast<Object*>(PyCObject_AsVoidPtr(cptr));
-   
+
     Py_DECREF(cptr);
     return obj;
 }
 
-void ConvertFromPyObject<Vec>::convert(PyObject* pyobj, Vec& v, bool print_traceback)
+void ConvertFromPyObject<Vec>::convert(PyObject* pyobj, Vec& v,
+                                       bool print_traceback)
 {
     // NA_InputArray possibly creates a well-behaved temporary (i.e. not
     // discontinuous is memory)
     PLASSERT( pyobj );
     PyArrayObject* pyarr = NA_InputArray(pyobj, tReal, NUM_C_ARRAY);
     if (! pyarr)
-        PLPythonConversionError("ConvertFromPyObject<Vec>", pyobj, print_traceback);
+        PLPythonConversionError("ConvertFromPyObject<Vec>", pyobj,
+                                print_traceback);
     if (pyarr->nd != 1)
         PLERROR("ConvertFromPyObject<Vec>: Dimensionality of the returned array "
                 "should be 1; got %d", pyarr->nd);
@@ -254,14 +208,16 @@
     return v;
 }
 
-void ConvertFromPyObject<Mat>::convert(PyObject* pyobj, Mat& m, bool print_traceback)
+void ConvertFromPyObject<Mat>::convert(PyObject* pyobj, Mat& m,
+                                       bool print_traceback)
 {
     // NA_InputArray possibly creates a well-behaved temporary (i.e. not
     // discontinuous is memory)
     PLASSERT( pyobj );
     PyArrayObject* pyarr = NA_InputArray(pyobj, tReal, NUM_C_ARRAY);
     if (! pyarr)
-        PLPythonConversionError("ConvertFromPyObject<Mat>", pyobj, print_traceback);
+        PLPythonConversionError("ConvertFromPyObject<Mat>", pyobj,
+                                print_traceback);
     if (pyarr->nd != 2)
         PLERROR("ConvertFromPyObject<Mat>: Dimensionality of the returned array "
                 "should be 2; got %d", pyarr->nd);
@@ -279,8 +235,10 @@
     return m;
 }
 
-//VMat ConvertFromPyObject<VMat>::convert(PyObject* pyobj, bool print_traceback)
-PP<VMatrix> ConvertFromPyObject<PP<VMatrix> >::convert(PyObject* pyobj, bool print_traceback)
+//VMat ConvertFromPyObject<VMat>::convert(PyObject* pyobj,
+//                                        bool print_traceback)
+PP<VMatrix> ConvertFromPyObject<PP<VMatrix> >::convert(PyObject* pyobj,
+                                                       bool print_traceback)
 {
     PLASSERT(pyobj);
     if(PyObject_HasAttrString(pyobj, "_cptr"))
@@ -299,7 +257,8 @@
 
 //#####  Constructors+Destructors  ############################################
 PythonObjectWrapper::PythonObjectWrapper(OwnershipMode o,
-                                         bool acquire_gil /* unused in this overload */)
+                                         // unused in this overload
+                                         bool acquire_gil)
     : m_ownership(o),
       m_object(Py_None)
 {
@@ -309,10 +268,11 @@
 
 //! Constructor for pre-existing PyObject
 PythonObjectWrapper::PythonObjectWrapper(PyObject* pyobj, OwnershipMode o,
-                                         bool acquire_gil /* unused in this overload */)
+                                         // unused in this overload
+                                         bool acquire_gil)
     : m_ownership(o),
       m_object(pyobj)
-{ 
+{
     if (m_ownership == control_ownership)
         Py_XINCREF(m_object);
 }
@@ -375,8 +335,8 @@
 {
     return ! m_object || m_object == Py_None;
 }
-    
 
+
 //##### Trampoline ############################################################
 PyObject* PythonObjectWrapper::trampoline(PyObject* self, PyObject* args)
 {
@@ -387,17 +347,17 @@
 
     //perr << "refcnt self= " << self->ob_refcnt << endl;
 
-    RemoteTrampoline* tramp= 
+    RemoteTrampoline* tramp=
         dynamic_cast<RemoteTrampoline*>(s.as<PPointable*>());
-    if(!tramp) 
+    if(!tramp)
         PLERROR("in PythonObjectWrapper::trampoline : "
                 "can't unwrap RemoteTrampoline.");
 
     //wrap args
     int size = PyTuple_GET_SIZE(args);
     TVec<PythonObjectWrapper> args_tvec(size);
-    for(int i= 0; i < size; ++i) 
-        args_tvec[i]= 
+    for(int i= 0; i < size; ++i)
+        args_tvec[i]=
             PythonObjectWrapper(PyTuple_GET_ITEM(args,i));
 
     // separate self from other params.
@@ -412,17 +372,17 @@
         Py_XINCREF(to_return);
         return to_return;
     }
-    catch(const PLearnError& e) 
+    catch(const PLearnError& e)
     {
         PyErr_SetString(PyExc_Exception, e.message().c_str());
         return 0;
     }
-    catch(const std::exception& e) 
+    catch(const std::exception& e)
     {
         PyErr_SetString(PyExc_Exception, e.what());
         return 0;
     }
-    catch(...) 
+    catch(...)
     {
         PyErr_SetString(PyExc_Exception,
                         "Caught unknown C++ exception while executing injected function "
@@ -433,7 +393,7 @@
 
 PyObject* PythonObjectWrapper::python_del(PyObject* self, PyObject* args)
 {
-    TVec<PyObject*> args_tvec= 
+    TVec<PyObject*> args_tvec=
         PythonObjectWrapper(args).as<TVec<PyObject*> >();
 
     Object* obj= PythonObjectWrapper(args_tvec[0]);
@@ -460,16 +420,17 @@
 
 //#####  newPyObject  #########################################################
 
-PyObject* PythonObjectWrapper::newPyObject()           //!< Return None (increments refcount)
+//! Return None (increments refcount)
+PyObject* PythonObjectWrapper::newPyObject()
 {
     Py_XINCREF(Py_None);
     return Py_None;
 }
 
-PythonObjectWrapper::wrapped_objects_t 
+PythonObjectWrapper::wrapped_objects_t
     PythonObjectWrapper::m_wrapped_objects;//init.
 
-PythonObjectWrapper::pypl_classes_t 
+PythonObjectWrapper::pypl_classes_t
     PythonObjectWrapper::m_pypl_classes;//init.
 //init.
 bool PythonObjectWrapper::m_unref_injected= false;
@@ -479,7 +440,7 @@
 PyObject* ConvertToPyObject<Object*>::newPyObject(const Object* x)
 {
     // void ptr becomes None
-    if(!x) 
+    if(!x)
         return PythonObjectWrapper::newPyObject();
 
     PythonGlobalInterpreterLock gil;         // For thread-safety
@@ -488,7 +449,7 @@
     PythonObjectWrapper::initializePython();
 
     //see if this obj. is already wrapped
-    PythonObjectWrapper::wrapped_objects_t::iterator objit= 
+    PythonObjectWrapper::wrapped_objects_t::iterator objit=
         PythonObjectWrapper::m_wrapped_objects.find(x);
     if(objit != PythonObjectWrapper::m_wrapped_objects.end())
     {
@@ -502,9 +463,10 @@
         "import *\n";
     PyObject* pyenv= PyDict_New();
     PyDict_SetItemString(pyenv, "__builtins__", PyEval_GetBuiltins());
-    PyObject* res= PyRun_String(importcode.c_str(), Py_file_input, pyenv, pyenv);
+    PyObject* res= PyRun_String(importcode.c_str(), Py_file_input, pyenv,
+                                pyenv);
     Py_DECREF(res);
-    if (PyErr_Occurred()) 
+    if (PyErr_Occurred())
     {
         Py_DECREF(pyenv);
         PyErr_Print();
@@ -536,16 +498,16 @@
         py_method->ml_flags = METH_VARARGS;
         py_method->ml_doc   = "Injected unref function from PythonObjectWrapper; "
             "DO NOT USE THIS FUNCTION! IT MAY DEALLOCATE THE PLEARN OBJECT!";
-        
+
         PyObject* py_funcobj= PyCFunction_NewEx(py_method, NULL, NULL);
         PyObject* py_methobj= PyMethod_New(py_funcobj, NULL, wrapper);
         Py_XDECREF(py_funcobj);
-        if(!py_funcobj || !py_methobj) 
+        if(!py_funcobj || !py_methobj)
         {
             Py_DECREF(pyenv);
             Py_XDECREF(py_methobj);
             PLERROR("in PythonObjectWrapper::newPyObject : "
-                    "can't inject method '%s' (i.e. __del__)", 
+                    "can't inject method '%s' (i.e. __del__)",
                     py_method->ml_name);
         }
         PyObject_SetAttrString(wrapper, py_method->ml_name, py_methobj);
@@ -576,7 +538,7 @@
 
     // try to find existing python class
     string classname= x->classname();
-    PythonObjectWrapper::pypl_classes_t::iterator clit2= 
+    PythonObjectWrapper::pypl_classes_t::iterator clit2=
         PythonObjectWrapper::m_pypl_classes.find(classname);
     PyObject* the_pyclass= 0;
     if(clit2 == PythonObjectWrapper::m_pypl_classes.end())
@@ -595,7 +557,7 @@
         if(clit == env.end())
             PLERROR("in PythonObjectWrapper::newPyObject : "
                     "Cannot create new python class deriving from "
-                    "%s (%s).", 
+                    "%s (%s).",
                     wrapper_name.c_str(),
                     classname.c_str());
 
@@ -607,11 +569,11 @@
             optionnames[i]= options[i]->optionname();
 
         the_pyclass= clit->second;
-        if(-1==PyObject_SetAttrString(the_pyclass, "_optionnames", 
+        if(-1==PyObject_SetAttrString(the_pyclass, "_optionnames",
                                       PythonObjectWrapper(optionnames).getPyObject()))
         {
             Py_DECREF(pyenv);
-            if (PyErr_Occurred()) 
+            if (PyErr_Occurred())
                 PyErr_Print();
             PLERROR("cannot set attr _optionnames");
         }
@@ -619,12 +581,12 @@
         // inject all declared methods
         const RemoteMethodMap* methods= &x->getRemoteMethodMap();
 
-        PP<PObjectPool<PyMethodDef> > meth_def_pool= 
+        PP<PObjectPool<PyMethodDef> > meth_def_pool=
             new PObjectPool<PyMethodDef>(methods->size()+1);
 
         PythonObjectWrapper::m_pypl_classes.insert(
             make_pair(classname, PLPyClass(the_pyclass, meth_def_pool)));
-        TVec<string>& methods_help= 
+        TVec<string>& methods_help=
             PythonObjectWrapper::m_pypl_classes.find(classname)->second.methods_help;
 
         while(methods)
@@ -634,7 +596,7 @@
             {
                 //get the RemoteTrampoline
                 PyObject* tramp= PyCObject_FromVoidPtr(it->second, NULL);
-            
+
                 // Create a Python Function Object
                 PyMethodDef* py_method= meth_def_pool->allocate();
                 py_method->ml_name  = const_cast<char*>(it->first.first.c_str());
@@ -644,7 +606,7 @@
                                                                 it->first.first.c_str(),
                                                                 it->first.second));
                 py_method->ml_doc   = const_cast<char*>(methods_help.last().c_str());
-    
+
                 PyObject* py_funcobj= PyCFunction_NewEx(py_method, tramp, NULL);
 
                 // create an unbound method from the function
@@ -652,7 +614,7 @@
 
                 Py_DECREF(tramp);
                 Py_XDECREF(py_funcobj);
-                if(!py_funcobj || !py_methobj) 
+                if(!py_funcobj || !py_methobj)
                 {
                     Py_DECREF(pyenv);
                     Py_DECREF(plobj);
@@ -661,7 +623,8 @@
                             "can't inject method '%s'", py_method->ml_name);
                 }
 
-                PyObject_SetAttrString(the_pyclass, py_method->ml_name, py_methobj);
+                PyObject_SetAttrString(the_pyclass, py_method->ml_name,
+                                       py_methobj);
                 Py_DECREF(py_methobj);
             }
             methods= methods->inheritedMethods();//get parent class methods
@@ -672,12 +635,12 @@
         the_pyclass= clit2->second.pyclass;
         ++clit2->second.nref;
     }
-    
+
     //create the python object itself from the_pyclass
     PyObject* args= PyTuple_New(1);
     Py_INCREF(plobj);//keep it after it is 'stolen'
     PyTuple_SetItem(args, 0, plobj);
-    
+
     PyObject* params= PyDict_New();
     PyObject* the_obj= PyObject_Call(the_pyclass, args, params);
     Py_DECREF(args);
@@ -715,40 +678,6 @@
     }
 }
 
-#if 0
-PyObject* ConvertToPyObject<int>::newPyObject(const int& x)
-{
-    return PyInt_FromLong(long(x));
-}
-
-PyObject* ConvertToPyObject<unsigned int>::newPyObject(const unsigned int& x)
-{
-    return PyLong_FromUnsignedLong(static_cast<unsigned long>(x));
-}
-
-/*
-PyObject* ConvertToPyObject<long>::newPyObject(const long& x)
-{
-    return PyLong_FromLong(x);
-}
-
-PyObject* ConvertToPyObject<unsigned long>::newPyObject(const unsigned long& x)
-{
-    return PyLong_FromUnsignedLong(x);
-}
-*/
-
-PyObject* ConvertToPyObject<int64_t>::newPyObject(const int64_t& x)
-{
-    return PyLong_FromLongLong(x);
-}
-
-PyObject* ConvertToPyObject<uint64_t>::newPyObject(const uint64_t& x)
-{
-    return PyLong_FromUnsignedLongLong(x);
-}
-#endif
-
 PyObject* ConvertToPyObject<double>::newPyObject(const double& x)
 {
     return PyFloat_FromDouble(x);
@@ -763,7 +692,7 @@
 {
     return PyString_FromString(x);
 }
-    
+
 PyObject* ConvertToPyObject<string>::newPyObject(const string& x)
 {
     return PyString_FromString(x.c_str());
@@ -782,7 +711,7 @@
         pyarr = NA_NewArray(NULL, tReal, 1, 0);
     else
         pyarr = NA_NewArray(data.data(), tReal, 1, data.size());
-        
+
     return (PyObject*)pyarr;
 }
 
@@ -792,7 +721,8 @@
     if (data.isNull() || data.isEmpty())
         pyarr = NA_NewArray(NULL, tReal, 2, data.length(), data.width());
     else if (data.mod() == data.width())
-        pyarr = NA_NewArray(data.data(), tReal, 2, data.length(), data.width());
+        pyarr = NA_NewArray(data.data(), tReal, 2, data.length(),
+                            data.width());
     else {
         // static PyObject* NA_NewAll( int ndim, maybelong *shape, NumarrayType
         // type, void *buffer, maybelong byteoffset, maybelong bytestride, int
@@ -810,8 +740,8 @@
         // maybelong shape[2];
         // shape[0] = data.length();
         // shape[1] = data.width();
-        // pyarr = NA_NewAll(2, shape, tReal, data.data(), 0, data.mod()*sizeof(real),
-        //                   NA_ByteOrder(), 1, 1);
+        // pyarr = NA_NewAll(2, shape, tReal, data.data(), 0,
+        //                   data.mod()*sizeof(real), NA_ByteOrder(), 1, 1);
 
         // NOTE (NC) -- I could not get the above function to work; for now,
         // simply copy the matrix to new storage before converting to Python.

Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2007-07-28 04:13:58 UTC (rev 7854)
+++ trunk/plearn/python/PythonObjectWrapper.h	2007-07-28 04:33:24 UTC (rev 7855)
@@ -2,23 +2,23 @@
 
 // PythonObjectWrapper.h
 //
-// Copyright (C) 2005-2006 Nicolas Chapados 
+// Copyright (C) 2005-2006 Nicolas Chapados
 // Copyright (C) 2007 Xavier Saint-Mleux, ApSTAT Technologies inc.
-// 
+//
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
-// 
+//
 //  1. Redistributions of source code must retain the above copyright
 //     notice, this list of conditions and the following disclaimer.
-// 
+//
 //  2. Redistributions in binary form must reproduce the above copyright
 //     notice, this list of conditions and the following disclaimer in the
 //     documentation and/or other materials provided with the distribution.
-// 
+//
 //  3. The name of the authors may not be used to endorse or promote
 //     products derived from this software without specific prior written
 //     permission.
-// 
+//
 // THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
 // IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 // OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
@@ -29,12 +29,12 @@
 // LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 // NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 // SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-// 
+//
 // This file is part of the PLearn library. For more information on the PLearn
 // library, go to the PLearn Web site at www.plearn.org
 
-/* *******************************************************      
-   * $Id: .pyskeleton_header 544 2003-09-01 00:05:31Z plearner $ 
+/* *******************************************************
+   * $Id: .pyskeleton_header 544 2003-09-01 00:05:31Z plearner $
    ******************************************************* */
 
 // Authors: Nicolas Chapados
@@ -290,21 +290,8 @@
     { return integerFromPyObject<unsigned long long>(pyobj, print_traceback); }
 };
 
-/*
-template <>
-struct ConvertFromPyObject<int64_t>
-{
-    static int64_t convert(PyObject*, bool print_traceback);
-};
 
 template <>
-struct ConvertFromPyObject<uint64_t>
-{
-    static uint64_t convert(PyObject*, bool print_traceback);
-};
-*/
-
-template <>
 struct ConvertFromPyObject<double>
 {
     static double convert(PyObject*, bool print_traceback);
@@ -358,7 +345,8 @@
         //      to retrieve a pointer to something that is not an Object from
         //      python.  Only Object pointers are supported.
 
-        Object* obj = ConvertFromPyObject<Object*>::convert(pyobj, print_traceback);
+        Object* obj = ConvertFromPyObject<Object*>::convert(pyobj,
+                                                            print_traceback);
         if (T* tobj = dynamic_cast<T*>(obj))
             return tobj;
         else
@@ -580,31 +568,31 @@
 //! @TODO  Must provide a complete Python wrapper over VMatrix objects
 template<> struct ConvertToPyObject<PP<VMatrix> >
 { static PyObject* newPyObject(const PP<VMatrix>& vm); };
-    
+
 //! Generic PP: wrap pointed object
 template<class T> struct ConvertToPyObject<PP<T> >
 { static PyObject* newPyObject(const PP<T>&); };
-    
+
 //! tuples (1 to 7 elts.)
-template<class T> 
+template<class T>
 struct ConvertToPyObject<tuple<T> >
 { static PyObject* newPyObject(const tuple<T>&); };
-template <class T, class U> 
+template <class T, class U>
 struct ConvertToPyObject<tuple<T,U> >
 { static PyObject* newPyObject(const tuple<T, U>&); };
-template <class T, class U, class V> 
+template <class T, class U, class V>
 struct ConvertToPyObject<tuple<T,U,V> >
 { static PyObject* newPyObject(const tuple<T, U, V>&); };
-template <class T, class U, class V, class W> 
+template <class T, class U, class V, class W>
 struct ConvertToPyObject<tuple<T,U,V,W> >
 { static PyObject* newPyObject(const tuple<T, U, V, W>&); };
-template <class T, class U, class V, class W, class X> 
+template <class T, class U, class V, class W, class X>
 struct ConvertToPyObject<tuple<T,U,V,W,X> >
 { static PyObject* newPyObject(const tuple<T, U, V, W, X>&); };
-template <class T, class U, class V, class W, class X, class Y> 
+template <class T, class U, class V, class W, class X, class Y>
 struct ConvertToPyObject<tuple<T,U,V,W,X,Y> >
 { static PyObject* newPyObject(const tuple<T, U, V, W, X, Y>&); };
-template <class T, class U, class V, class W, class X, class Y, class Z> 
+template <class T, class U, class V, class W, class X, class Y, class Z>
 struct ConvertToPyObject<tuple<T,U,V,W,X,Y,Z> >
 { static PyObject* newPyObject(const tuple<T, U, V, W, X, Y, Z>&); };
 
@@ -627,7 +615,7 @@
 //! C++ stdlib pair<>: create a Python tuple with two elements
 template <class T, class U> struct ConvertToPyObject<std::pair<T,U> >
 { static PyObject* newPyObject(const std::pair<T,U>&); };
-    
+
 //! Pointer to vector<>: simply dereference pointer, or None if NULL
 //!
 //! (NOTE: we don't have conversion from general pointer type since it's
@@ -702,7 +690,7 @@
     PythonGlobalInterpreterLock()
         : m_gilstate(PyGILState_Ensure())
     { }
-       
+
     ~PythonGlobalInterpreterLock()
     {
         PyGILState_Release(m_gilstate);
@@ -716,8 +704,8 @@
 
 /**
  *  @class  PythonObjectWrapper
- *  @brief  Very lightweight wrapper over a Python Object that allows conversion
- *          to/from C++ types (including those of PLearn)
+ *  @brief  Very lightweight wrapper over a Python Object that allows
+ *          conversion to/from C++ types (including those of PLearn)
  *
  *  A PythonObjectWrapper provides the ability to manage a Python Object in a
  *  fairly lightweight manner.  It supports construction from a number of C++
@@ -746,7 +734,7 @@
         control_ownership,
         transfer_ownership
     };
-    
+
 public:
     //#####  Construction and Utility  ########################################
 
@@ -757,15 +745,16 @@
     //! reference-counted just like any other object.
     PythonObjectWrapper(OwnershipMode o = control_ownership,
                         bool acquire_gil = true /* unused in this overload */);
-    
+
     //! Constructor for pre-existing PyObject
     PythonObjectWrapper(PyObject* pyobj, OwnershipMode o = control_ownership,
                         bool acquire_gil = true /* unused in this overload */);
-    
+
     //! Constructor for general type (forwarded to newPyObject)
     template <class T>
-    explicit PythonObjectWrapper(const T& x, OwnershipMode o = control_ownership,
-                        bool acquire_gil = true)
+    explicit PythonObjectWrapper(const T& x,
+                                 OwnershipMode o = control_ownership,
+                                 bool acquire_gil = true)
         : m_ownership(o)
     {
         if (acquire_gil) {
@@ -778,7 +767,7 @@
 
     //! Copy constructor: increment refcount if controlling ownership.
     PythonObjectWrapper(const PythonObjectWrapper& other);
-    
+
     //! Destructor: decrement refcount if controlling ownership.
     //! Always acquire the Python Global Interpreter Lock before decrementing.
     ~PythonObjectWrapper();
@@ -791,21 +780,21 @@
 
     //! Swap *this with another instance
     void swap(PythonObjectWrapper& other);
-    
+
     //! Return the bare PyObject managed by the wrapper
     PyObject* getPyObject() const
     {
         return m_object;
     }
-    
+
     //! Return true if m_object is either NULL or stands for Python's None
     bool isNull() const;
-    
+
     //! Print out the Python object to stderr for debugging purposes
     void printDebug() const;
 
-    
 
+
     //#####  Conversion Back to C++  ##########################################
 
     //! General version that relies on ConvertFromPyOBject
@@ -826,8 +815,8 @@
     {
         return ConvertFromPyObject<T>::convert(m_object, false);
     }
-    
 
+
     //##### Trampoline for PLearn objects #####################################
 
     static PyObject* trampoline(PyObject* self, PyObject* args);
@@ -849,11 +838,12 @@
      *  initializations related to libnumarray.
      */
     static void initializePython();
-    
+
 protected:
-    OwnershipMode m_ownership;               //!< Whether we own the PyObject or not
+    //! Whether we own the PyObject or not
+    OwnershipMode m_ownership;
     PyObject* m_object;
-    
+
     //for the unique unref injected method
     static bool m_unref_injected;
     static PyMethodDef m_unref_method_def;
@@ -895,8 +885,8 @@
 
 //#####  ConvertFromPyObject Implementations  #################################
 
-template<class U, bool is_enum> 
-struct StaticConvertEnumFromPyObject 
+template<class U, bool is_enum>
+struct StaticConvertEnumFromPyObject
 {
     static U convert(PyObject* x, bool print_traceback)
     {
@@ -905,7 +895,7 @@
         return U();//to silence compiler
     }
 };
-    
+
 template<class U>
 struct StaticConvertEnumFromPyObject<U, true>
 {
@@ -924,7 +914,7 @@
     /*
     if(boost::is_enum<T>::value)
         return ConvertFromPyObject<int>::convert(x, print_traceback);
-    
+
     PLERROR("Cannot convert this object by value from python (type=%s).",
             TypeTraits<T>::name().c_str());
     return T();//to silence compiler
@@ -933,7 +923,8 @@
 
 
 template <class T>
-PP<T> ConvertFromPyObject<PP<T> >::convert(PyObject* pyobj, bool print_traceback)
+PP<T> ConvertFromPyObject<PP<T> >::convert(PyObject* pyobj,
+                                           bool print_traceback)
 {
     PLASSERT( pyobj );
     if(pyobj == Py_None)
@@ -952,10 +943,11 @@
 }
 
 template <class T>
-TVec<T> ConvertFromPyObject< TVec<T> >::convert(PyObject* pyobj, bool print_traceback)
+TVec<T> ConvertFromPyObject< TVec<T> >::convert(PyObject* pyobj,
+                                                bool print_traceback)
 {
     PLASSERT( pyobj );
-    
+
     // Here, we support both Python Tuples and Lists
     if (PyTuple_Check(pyobj)) {
         // Tuple case
@@ -985,19 +977,21 @@
 }
 
 template <class T>
-TMat<T> ConvertFromPyObject<TMat<T> >::convert(PyObject* pyobj, bool print_traceback)
+TMat<T> ConvertFromPyObject<TMat<T> >::convert(PyObject* pyobj,
+                                               bool print_traceback)
 {
     PLASSERT( pyobj );
-    
+
     // Here, we support both Python Tuples and Lists
     if (PyTuple_Check(pyobj)) {
         // Tuple case
         int len= PyTuple_GET_SIZE(pyobj);
         TMat<T> v;
-        for(int i= 0; i < len; ++i) 
+        for(int i= 0; i < len; ++i)
         {
             PyObject* row_i= PyTuple_GET_ITEM(pyobj, i);
-            TVec<T> r= ConvertFromPyObject<TVec<T> >::convert(row_i, print_traceback);
+            TVec<T> r= ConvertFromPyObject<TVec<T> >::convert(row_i,
+                                                              print_traceback);
             if(i == 0)
                 v.resize(0, r.size());
             v.appendRow(r);
@@ -1008,10 +1002,11 @@
         // List case
         int len= PyList_GET_SIZE(pyobj);
         TMat<T> v;
-        for(int i= 0; i < len; ++i) 
+        for(int i= 0; i < len; ++i)
         {
             PyObject* row_i= PyList_GET_ITEM(pyobj, i);
-            TVec<T> r= ConvertFromPyObject<TVec<T> >::convert(row_i, print_traceback);
+            TVec<T> r= ConvertFromPyObject<TVec<T> >::convert(row_i,
+                                                              print_traceback);
             if(i == 0)
                 v.resize(0, r.size());
             v.appendRow(r);
@@ -1031,7 +1026,7 @@
                                                               bool print_traceback)
 {
     PLASSERT( pyobj );
-    
+
     // Simple but inefficient implementation: create temporary TVec and copy
     // into a vector
     TVec<T> v = ConvertFromPyObject< TVec<T> >::convert(pyobj, print_traceback);
@@ -1046,7 +1041,7 @@
     if (! PyDict_Check(pyobj))
         PLPythonConversionError("ConvertFromPyObject< std::map<T,U> >", pyobj,
                                 print_traceback);
-    
+
     PyObject *key, *value;
 #if PL_PYTHON_VERSION>=250
     Py_ssize_t pos = 0;
@@ -1088,7 +1083,7 @@
 
 //#####  newPyObject Implementations  #########################################
 
-template<size_t N> 
+template<size_t N>
 PyObject*  ConvertToPyObject<char[N]>::newPyObject(const char x[N])
 {
     return ConvertToPyObject<char*>::newPyObject(x);
@@ -1123,12 +1118,12 @@
 PyObject* ConvertToPyObject<TMat<T> >::newPyObject(const TMat<T>& data)
 {
     PyObject* newlist = PyList_New(data.length());
-    for (int i=0, n=data.length() ; i<n ; ++i) 
+    for (int i=0, n=data.length() ; i<n ; ++i)
     {
         // Since PyList_SET_ITEM steals the reference to the item being set,
         // one does not need to Py_XDECREF the inserted string as was required
         // for the PyArrayObject code above...
-        PyList_SET_ITEM(newlist, i, 
+        PyList_SET_ITEM(newlist, i,
                         ConvertToPyObject<TVec<T> >::newPyObject(data(i)));
     }
     return newlist;
@@ -1146,7 +1141,7 @@
     }
     return newlist;
 }
-    
+
 template <class T, class U>
 PyObject* ConvertToPyObject<std::map<T,U> >::newPyObject(const std::map<T,U>& data)
 {



From yoshua at mail.berlios.de  Sat Jul 28 18:10:35 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Sat, 28 Jul 2007 18:10:35 +0200
Subject: [Plearn-commits] r7856 - in trunk/python_modules/plearn: .
	learners/autolr learners/online
Message-ID: <200707281610.l6SGAZoo018082@sheep.berlios.de>

Author: yoshua
Date: 2007-07-28 18:10:34 +0200 (Sat, 28 Jul 2007)
New Revision: 7856

Modified:
   trunk/python_modules/plearn/__init__.py
   trunk/python_modules/plearn/learners/autolr/__init__.py
   trunk/python_modules/plearn/learners/online/__init__.py
Log:
Transferred ifthenelse from online to plearn module.
Encapsulated getTestCostNames in a global function in autolr,
to allow redefining the set of costs measured on a learner.


Modified: trunk/python_modules/plearn/__init__.py
===================================================================
--- trunk/python_modules/plearn/__init__.py	2007-07-28 04:33:24 UTC (rev 7855)
+++ trunk/python_modules/plearn/__init__.py	2007-07-28 16:10:34 UTC (rev 7856)
@@ -3,3 +3,12 @@
 It shall englobe all and only python_modules that are relative to the
 original PLearn branch (vs LisaPLearn or apstatsoft).
 """
+
+# generically useful things...
+
+def ifthenelse(cond,elsep,condp):
+    if cond:
+        return elsep
+    else:
+        return condp
+

Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-07-28 04:33:24 UTC (rev 7855)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-07-28 16:10:34 UTC (rev 7856)
@@ -1,6 +1,6 @@
-
 from math import *
 from numarray import *
+from plearn import *
 from plearn.bridge import *
 from threading import *
 
@@ -93,14 +93,18 @@
         lock.release()
     return o
 
+def getTestCostNames(learner):
+    return learner.getTestCostNames()
+
 def testlearner(learner,dataset,costs=[],ts=None):
     if not ts:
         ts = pl.VecStatsCollector()
         if plearn.bridgemode.useserver:
             ts=learner.server.new(ts)
     learner.test(dataset,ts,0,0)
+    names = learner.getTestCostNames()
     del costs[:]
-    for k in range(len(learner.getTestCostNames())):
+    for k in range(len(names)):
         costs.append(ts.getStat("E["+str(k)+"]"))
     return costs
 
@@ -156,6 +160,7 @@
                         trainset, testsets, expdir,
                         cost_to_select_best=0,
                         selected_costnames = None,
+                        get_train_costs = True,
                         logfile=None):
     """Train a learner with one or more schedules of learning rates.
 lr_options is a list of list of option strings. Each list in lr_options
@@ -168,7 +173,7 @@
 each of the other columns (just like the result of the call to merge_schedules).
 """
     train_costnames = learner.getTrainCostNames()
-    test_costnames = learner.getTestCostNames()
+    test_costnames = getTestCostNames(learner)
 
     # Filter out unwanted costnames
     if selected_costnames is not None:
@@ -178,6 +183,8 @@
                            if name in selected_costnames ]
 
     n_train_costs = len(train_costnames)
+    if not get_train_costs:
+        n_train_costs = 0
     n_test_costs = len(test_costnames)
 
     learner.setTrainingSet(trainset,False)
@@ -204,26 +211,28 @@
         for s in range(n_schedules):
             results[i,1+s] = learning_rates[i][s]
 
-        # Report approximate training error
-        train_vsc = learner.getTrainStatsCollector()
-        if train_vsc.fieldnames == []:
-            # learner did not set train_stats fieldnames
-            train_vsc.fieldnames = learner.getTrainCostNames()
-        if logfile:
-            print >>logfile, "At stage ", learner.stage, " train :",
-        for k, costname in zip(range(n_train_costs), train_costnames):
-            err = train_vsc.getStat('E['+costname+']')
-            results[i, k+1+n_schedules] = err
+        if get_train_costs:
+            # Report approximate training error
+            train_vsc = learner.getTrainStatsCollector()
+            if train_vsc.fieldnames == []:
+                # learner did not set train_stats fieldnames
+                train_vsc.fieldnames = learner.getTrainCostNames()
+        if get_train_costs:
             if logfile:
-                print >>logfile, costname, '=', err,
-            if plearn.bridgemode.interactive:
-                plot(   results[0:i+1, 0],
-                        results[0:i+1, 1+n_schedules+k],
-                        colors[k%7]+styles[0],
-                        label='train:'+costname)
-        if logfile:
-            print >>logfile
-            logfile.flush()
+                print >>logfile, "At stage ", learner.stage, " train :",
+            for k, costname in zip(range(n_train_costs), train_costnames):
+                err = train_vsc.getStat('E['+costname+']')
+                results[i, k+1+n_schedules] = err
+                if logfile:
+                    print >>logfile, costname, '=', err,
+                if plearn.bridgemode.interactive:
+                    plot(   results[0:i+1, 0],
+                            results[0:i+1, 1+n_schedules+k],
+                            colors[k%7]+styles[0],
+                            label='train:'+costname)
+            if logfile:
+                print >>logfile
+                logfile.flush()
 
         # Report error on test sets
         for j in range(n_tests):
@@ -250,8 +259,11 @@
         if plearn.bridgemode.interactive and i==0:
             legend()
     # Return headers for the result matrix, and the results themselves
-    return (['stage', 'learning_rate']
-            + ['train.' + costname for costname in train_costnames]
+    train_names = []
+    if get_train_costs:
+        train_names = ['train.' + costname for costname in train_costnames]
+    return (['pstage', 'learning_rate']
+            + train_names
             + ['test'+str(j+1)+'.'+costname for j in range(n_tests)
                                             for costname in test_costnames],
             results)
@@ -360,7 +372,10 @@
                       min_epochs_to_delete = 2,
                       # Scaling coefficient when modifying learning rates
                       lr_steps=exp(log(10)/2),
+                      # file (or sys.stdout) where to report progress
                       logfile=None,
+                      # whether to get and report train costs
+                      get_train_costs=True,
                       # do not try to go below this learning rate
                       min_lr=1e-6,
                       # Learning rate interval for heuristic
@@ -416,8 +431,8 @@
             return False
         return True
 
-    train_costnames = learner.getTestCostNames()
-    test_costnames = learner.getTestCostNames()
+    train_costnames = learner.getTrainCostNames()
+    test_costnames = getTestCostNames(learner)
     if selected_costnames is not None:
         # Filter out unwanted costnames
         train_costnames = [ name for name in train_costnames
@@ -427,7 +442,7 @@
 
     n_tests = len(testsets)
     #n_costs = len(cost_indices)
-    n_train_costs = len(train_costnames)
+    n_train_costs = ifthenelse(get_train_costs,len(train_costnames),0)
     n_test_costs = len(test_costnames)
 
     if schedules:
@@ -441,9 +456,10 @@
         schedules = zeros([len(stages),2],Float)
         schedules[:,0]=stages
         schedules[:,1]=learning_rates[:,0]
-        if optimized_group != 0 and optimized_group != -1:
-            print "Incorrect value for 'optimized_group'"
-            raise Error
+        # YB: QUI A MIS CA? POURQUOI???
+        #if optimized_group != 0 and optimized_group != -1:
+        #    print "Incorrect value for 'optimized_group'"
+        #    raise Error
         if len(lr_options)!=1:
             lr_options=[lr_options[0]]
 
@@ -487,10 +503,11 @@
                         options[lr_option]=str(learning_rates[t,s])
             candidate.changeOptions(options)
             candidate.setTrainingSet(trainset,False)
-            train_vsc = pl.VecStatsCollector();
-            if plearn.bridgemode.useserver:
-                train_vsc = candidate.server.new(train_vsc)
-            candidate.setTrainStatsCollector(train_vsc)
+            if get_train_costs:
+                train_vsc = pl.VecStatsCollector();
+                if plearn.bridgemode.useserver:
+                    train_vsc = candidate.server.new(train_vsc)
+                candidate.setTrainStatsCollector(train_vsc)
             tasks = [('train', ())]
             stats = []
             for j in range(0,n_tests):
@@ -519,15 +536,16 @@
             results[t,1] = all_lr[active]
             if logfile:
                 print >>logfile, "candidate ",active,":",
-            # Report approximate training statistics
-            if logfile:
-                print >>logfile, 'train :',
-            ts=candidate.getTrainStatsCollector()
-            for k, costname in zip(range(n_train_costs), train_costnames):
-                err = ts.getStat('E['+costname+']')
-                results[t, 2+k] = err
+            if get_train_costs:
+                # Report approximate training statistics
                 if logfile:
-                    print >>logfile, costname, '=', err,
+                    print >>logfile, 'train :',
+                ts=candidate.getTrainStatsCollector()
+                for k, costname in zip(range(n_train_costs), train_costnames):
+                    err = ts.getStat('E['+costname+']')
+                    results[t, 2+k] = err
+                    if logfile:
+                        print >>logfile, costname, '=', err,
 
             # Report testing statistics
             for j, costs in zip(range(0,n_tests), stats):
@@ -571,9 +589,10 @@
             if logfile:
                 print >>logfile,"BEST to now is candidate ",best_active," with err=",best_err
                 print >>logfile, "stage\tl.rate\t",
-                for costname in train_costnames:
-                    print >>logfile, 'train.'+costname+"\t",
-                print >>logfile
+                if get_train_costs:
+                    for costname in train_costnames:
+                        print >>logfile, 'train.'+costname+"\t",
+                    print >>logfile
                 for costname in test_costnames:
                     print >>logfile, 'test.'+costname+'\t',
                 print >>logfile

Modified: trunk/python_modules/plearn/learners/online/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/online/__init__.py	2007-07-28 04:33:24 UTC (rev 7855)
+++ trunk/python_modules/plearn/learners/online/__init__.py	2007-07-28 16:10:34 UTC (rev 7856)
@@ -1,15 +1,9 @@
 """Code to define and manipulate online learning modules and related learners"""
 
 import plearn.bridgemode
+from plearn import *
 from plearn.bridge import *
 
-# THIS SHOULD GO SOMEWHERE ELSE!
-def ifthenelse(cond,elsep,condp):
-    if cond:
-        return elsep
-    else:
-        return condp
-
 def supervised_classification_mlp(name,input_size,n_hidden,n_classes,
                                   L1wd=0,L2wd=0,lrate=0.01):
     return pl.NetworkModule(name=name,



From yoshua at mail.berlios.de  Sun Jul 29 14:05:13 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Sun, 29 Jul 2007 14:05:13 +0200
Subject: [Plearn-commits] r7857 - in trunk/plearn_learners/online: .
	EXPERIMENTAL
Message-ID: <200707291205.l6TC5DTS007860@sheep.berlios.de>

Author: yoshua
Date: 2007-07-29 14:05:12 +0200 (Sun, 29 Jul 2007)
New Revision: 7857

Modified:
   trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
   trunk/plearn_learners/online/RBMBinomialLayer.cc
Log:
Changed commens in RBMBinomialLayer and added missing deep copies in KLp0p1RBMModule.


Modified: trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-28 16:10:34 UTC (rev 7856)
+++ trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-29 12:05:12 UTC (rev 7857)
@@ -581,6 +581,8 @@
 
     deepCopyField(hidden_layer,     copies);
     deepCopyField(visible_layer,    copies);
+    deepCopyField(conf_hidden_layer,     copies);
+    deepCopyField(conf_visible_layer,    copies);
     deepCopyField(connection,       copies);
     deepCopyField(reconstruction_connection, copies);
 

Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-07-28 16:10:34 UTC (rev 7856)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-07-29 12:05:12 UTC (rev 7857)
@@ -335,9 +335,8 @@
             target_i = target[i];
             activation_i = activation[i];
             ret += tabulated_softplus(activation_i) - target_i * activation_i;
-            // nll -= target * pl_log(expectation); 
-            // but it is numerically unstable, so use instead the following identity:
             // nll = - target*log(sigmoid(act)) -(1-target)*log(1-sigmoid(act))
+            // but it is numerically unstable, so use instead the following identity:
             //     = target*softplus(-act) +(1-target)*(act+softplus(-act))
             //     = act + softplus(-act) - target*act 
             //     = softplus(act) - target*act



From larocheh at mail.berlios.de  Mon Jul 30 18:44:59 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Mon, 30 Jul 2007 18:44:59 +0200
Subject: [Plearn-commits] r7858 - trunk/plearn_learners/online
Message-ID: <200707301644.l6UGixXO005759@sheep.berlios.de>

Author: larocheh
Date: 2007-07-30 18:44:50 +0200 (Mon, 30 Jul 2007)
New Revision: 7858

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
   trunk/plearn_learners/online/StackedAutoassociatorsNet.h
Log:
Added correlation weights, which can capture correlation between hidden neurons and potentially suppress their interaction in order to obtain sparse hidden layer representations


Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2007-07-29 12:05:12 UTC (rev 7857)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2007-07-30 16:44:50 UTC (rev 7858)
@@ -61,9 +61,7 @@
     l1_neuron_decay_center( 0 ),
     compute_all_test_costs( false ),
     n_layers( 0 ),
-    currently_trained_layer( 0 ),
-    final_module_has_learning_rate( false ),
-    final_cost_has_learning_rate( false )
+    currently_trained_layer( 0 )
 {
     // random_gen will be initialized in PLearner::build_()
     random_gen = new PRandom();
@@ -137,6 +135,13 @@
                   "The weights of the reconstruction connections between the "
                   "layers");
 
+    declareOption(ol, "correlation_connections", 
+                  &StackedAutoassociatorsNet::correlation_connections,
+                  OptionBase::buildoption,
+                  "Optional weights to capture correlation and anti-correlation\n"
+                  "in the hidden layers. They must have the same input and\n"
+                  "output sizes, compatible with their corresponding layers.");
+
     declareOption(ol, "final_module", &StackedAutoassociatorsNet::final_module,
                   OptionBase::buildoption,
                   "Module that takes as input the output of the last layer\n"
@@ -185,8 +190,15 @@
 
     declareOption(ol, "n_layers", &StackedAutoassociatorsNet::n_layers,
                   OptionBase::learntoption,
-                  "Number of layers");
+                  "Number of layers"
+        );
 
+    declareOption(ol, "correlation_layers", 
+                  &StackedAutoassociatorsNet::correlation_layers,
+                  OptionBase::learntoption,
+                  "Hidden layers for the correlation connections"
+        );
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -262,6 +274,35 @@
                 "there should be %d connections.\n",
                 n_layers-1);
 
+    if( reconstruction_connections.length() != n_layers-1 )
+        PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() - \n"
+                "there should be %d reconstruction connections.\n",
+                n_layers-1);
+    
+    if( correlation_connections.length() != 0 &&
+        correlation_connections.length() != n_layers-1 )
+        PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() - \n"
+                "there should be either %d correlation connections or none.\n",
+                n_layers-1);
+    
+    if(correlation_connections.length() != 0)
+    {
+        correlation_layers.resize( layers.length()-1 );
+        for( int i=0 ; i<n_layers-1 ; i++ )
+        {
+            if( greedy_stages[i] == 0 )
+            {
+                CopiesMap map;
+                correlation_layers[i] = 
+                    layers[i+1]->deepCopy(map);
+            }
+        }
+        correlation_activations.resize( n_layers-1 );
+        correlation_expectations.resize( n_layers-1 );
+        correlation_activation_gradients.resize( n_layers-1 );
+        correlation_expectation_gradients.resize( n_layers-1 );
+    }
+
     if(layers[0]->size != inputsize_)
         PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() - \n"
                 "layers[0] should have a size of %d.\n",
@@ -272,6 +313,7 @@
     activation_gradients.resize( n_layers );
     expectation_gradients.resize( n_layers );
 
+
     for( int i=0 ; i<n_layers-1 ; i++ )
     {
         if( layers[i]->size != connections[i]->down_size )
@@ -300,6 +342,32 @@
                     "%d.\n",
                     i, layers[i]->size);
 
+        if(correlation_connections.length() != 0)
+        {
+            if( correlation_connections[i]->up_size != layers[i+1]->size ||
+                correlation_connections[i]->down_size != layers[i+1]->size )
+                PLERROR("StackedAutoassociatorsNet::build_layers_and_connections()"
+                        " - \n"
+                        "correlation_connections[%i] should have a up_size and "
+                        "down_size of %d.\n",
+                        i, layers[i+1]->size);
+            correlation_activations[i].resize( layers[i+1]->size );
+            correlation_expectations[i].resize( layers[i+1]->size );
+            correlation_activation_gradients[i].resize( layers[i+1]->size );
+            correlation_expectation_gradients[i].resize( layers[i+1]->size );
+            if( !(correlation_connections[i]->random_gen) )
+            {
+                correlation_connections[i]->random_gen = random_gen;
+                correlation_connections[i]->forget();
+            }
+
+            if( !(correlation_layers[i]->random_gen) )
+            {
+                correlation_layers[i]->random_gen = random_gen;
+                correlation_layers[i]->forget();
+            }
+        }
+
         if( !(layers[i]->random_gen) )
         {
             layers[i]->random_gen = random_gen;
@@ -316,7 +384,7 @@
         {
             reconstruction_connections[i]->random_gen = random_gen;
             reconstruction_connections[i]->forget();
-        }
+        }        
 
         activations[i].resize( layers[i]->size );
         expectations[i].resize( layers[i]->size );
@@ -360,7 +428,7 @@
     if( final_module->output_size != final_cost->input_size )
         PLERROR("StackedAutoassociatorsNet::build_costs() - \n"
                 "final_module should have an output_size of %d.\n", 
-                final_module->input_size);
+                final_cost->input_size);
 
     final_module->setLearningRate( fine_tuning_learning_rate );
 
@@ -433,6 +501,12 @@
     deepCopyField(reconstruction_expectations, copies);
     deepCopyField(reconstruction_activation_gradients, copies);
     deepCopyField(reconstruction_expectation_gradients, copies);
+    deepCopyField(correlation_connections, copies);
+    deepCopyField(correlation_activations, copies);
+    deepCopyField(correlation_expectations, copies);
+    deepCopyField(correlation_activation_gradients, copies);
+    deepCopyField(correlation_expectation_gradients, copies);
+    deepCopyField(correlation_layers, copies);
     deepCopyField(partial_costs_positions, copies);
     deepCopyField(partial_cost_value, copies);
     deepCopyField(final_cost_input, copies);
@@ -476,6 +550,15 @@
         if( partial_costs[i] )
             partial_costs[i]->forget();
 
+    if(correlation_connections.length() != 0)
+    {        
+        for( int i=0 ; i<n_layers-1 ; i++)
+        {
+            correlation_connections[i]->forget();
+            correlation_layers[i]->forget();
+        }        
+    }
+
     stage = 0;
     greedy_stages.clear();
 }
@@ -528,12 +611,17 @@
         layers[i]->setLearningRate( lr );
         connections[i]->setLearningRate( lr );
         reconstruction_connections[i]->setLearningRate( lr );
+        if(correlation_connections.length() != 0)
+        {
+            correlation_connections[i]->setLearningRate( lr );
+            correlation_layers[i]->setLearningRate( lr );
+        }
         layers[i+1]->setLearningRate( lr );
 
-        reconstruction_activations.resize(layers[i+1]->size);
-        reconstruction_expectations.resize(layers[i+1]->size);
-        reconstruction_activation_gradients.resize(layers[i+1]->size);
-        reconstruction_expectation_gradients.resize(layers[i+1]->size);
+        reconstruction_activations.resize(layers[i]->size);
+        reconstruction_expectations.resize(layers[i]->size);
+        reconstruction_activation_gradients.resize(layers[i]->size);
+        reconstruction_expectation_gradients.resize(layers[i]->size);
 
         for( ; *this_stage<end_stage ; (*this_stage)++ )
         {
@@ -544,7 +632,12 @@
                 layers[i]->setLearningRate( lr );
                 connections[i]->setLearningRate( lr );
                 reconstruction_connections[i]->setLearningRate( lr );
-                layers[i+1]->setLearningRate( lr );                
+                layers[i+1]->setLearningRate( lr );
+                if(correlation_connections.length() != 0)
+                {
+                    correlation_connections[i]->setLearningRate( lr );
+                    correlation_layers[i]->setLearningRate( lr );
+                }
             }
 
             sample = *this_stage % nsamples;
@@ -592,7 +685,8 @@
     }
     
     train_stats->finalize();
-        
+    MODULE_LOG << "  train costs = " << train_stats->getMean() << endl;
+
     // Update currently_trained_layer
     if(stage > 0)
         currently_trained_layer = n_layers;
@@ -610,11 +704,27 @@
     PLASSERT( index < n_layers );
 
     expectations[0] << input;
-    for( int i=0 ; i<index + 1; i++ )
+    if(correlation_connections.length() != 0)
     {
-        connections[i]->fprop( expectations[i], activations[i+1] );
-        layers[i+1]->fprop(activations[i+1],expectations[i+1]);
+        for( int i=0 ; i<index + 1; i++ )
+        {
+            connections[i]->fprop( expectations[i], correlation_activations[i] );
+            layers[i+1]->fprop( correlation_activations[i],
+                                correlation_expectations[i] );
+            correlation_connections[i]->fprop( correlation_expectations[i], 
+                                               activations[i+1] );
+            correlation_layers[i]->fprop( activations[i+1], 
+                                          expectations[i+1] );
+        }
     }
+    else
+    {
+        for( int i=0 ; i<index + 1; i++ )
+        {
+            connections[i]->fprop( expectations[i], activations[i+1] );
+            layers[i+1]->fprop(activations[i+1],expectations[i+1]);
+        }
+    }
 
     if( partial_costs && partial_costs[ index ] )
     {
@@ -649,12 +759,13 @@
                                                 reconstruction_activations);
     layers[ index ]->fprop( reconstruction_activations,
                             layers[ index ]->expectation);
-    
+
+    layers[ index ]->activation << reconstruction_activations;
     layers[ index ]->expectation_is_up_to_date = true;
     train_costs[index] = layers[ index ]->fpropNLL(expectations[index]);
-
+    
     layers[ index ]->bpropNLL(expectations[index], train_costs[index],
-                                  reconstruction_activation_gradients);
+                              reconstruction_activation_gradients);
 
     layers[ index ]->update(reconstruction_activation_gradients);
 
@@ -667,7 +778,7 @@
     reconstruction_connections[ index ]->bpropUpdate( 
         expectations[ index + 1], 
         reconstruction_activations, 
-        reconstruction_expectation_gradients, //reused
+        reconstruction_expectation_gradients, 
         reconstruction_activation_gradients);
 
     if(!fast_exact_is_equal(l1_neuron_decay,0))
@@ -688,17 +799,49 @@
     }
 
     // Update hidden layer bias and weights
-    layers[ index+1 ]->bpropUpdate( activations[ index + 1 ],
-                                    expectations[ index + 1 ],
-                                    reconstruction_activation_gradients, // reused
-                                    reconstruction_expectation_gradients);    
 
-    connections[ index ]->bpropUpdate( 
-        expectations[ index ],
-        activations[ index + 1 ],
-        reconstruction_expectation_gradients, //reused
-        reconstruction_activation_gradients);
+    if(correlation_connections.length() != 0)
+    {
+        correlation_layers[ index ]->bpropUpdate(
+            activations[ index + 1 ],
+            expectations[ index + 1 ],
+            reconstruction_activation_gradients,  // reused
+            reconstruction_expectation_gradients
+            );
 
+        correlation_connections[ index ]->bpropUpdate( 
+            correlation_expectations[ index ],
+            activations[ index+1 ],
+            correlation_expectation_gradients[ index ], 
+            reconstruction_activation_gradients);
+        
+        layers[ index+1 ]->bpropUpdate( 
+            correlation_activations[ index ],
+            correlation_expectations[ index ],
+            correlation_activation_gradients [ index ],
+            correlation_expectation_gradients [ index ]);    
+        
+        connections[ index ]->bpropUpdate( 
+            expectations[ index ],
+            correlation_activations[ index ],
+            reconstruction_expectation_gradients, //reused
+            correlation_activation_gradients [ index ]);
+    }
+    else
+    {
+        layers[ index+1 ]->bpropUpdate( activations[ index + 1 ],
+                                        expectations[ index + 1 ],
+                                        // reused
+                                        reconstruction_activation_gradients, 
+                                        reconstruction_expectation_gradients);    
+        
+        connections[ index ]->bpropUpdate( 
+            expectations[ index ],
+            activations[ index + 1 ],
+            reconstruction_expectation_gradients, //reused
+            reconstruction_activation_gradients);
+    }
+
 }
 
 void StackedAutoassociatorsNet::fineTuningStep( const Vec& input, const Vec& target,
@@ -706,11 +849,28 @@
 {
     // fprop
     expectations[0] << input;
-    for( int i=0 ; i<n_layers-1; i++ )
+
+    if(correlation_connections.length() != 0)
     {
-        connections[i]->fprop( expectations[i], activations[i+1] );
-        layers[i+1]->fprop(activations[i+1],expectations[i+1]);
+        for( int i=0 ; i<n_layers-1; i++ )
+        {
+            connections[i]->fprop( expectations[i], correlation_activations[i] );
+            layers[i+1]->fprop( correlation_activations[i],
+                                correlation_expectations[i] );
+            correlation_connections[i]->fprop( correlation_expectations[i], 
+                                               activations[i+1] );
+            correlation_layers[i]->fprop( activations[i+1], 
+                                          expectations[i+1] );
+        }
     }
+    else
+    {
+        for( int i=0 ; i<n_layers-1; i++ )
+        {
+            connections[i]->fprop( expectations[i], activations[i+1] );
+            layers[i+1]->fprop(activations[i+1],expectations[i+1]);
+        }
+    }
 
     final_module->fprop( expectations[ n_layers-1 ],
                          final_cost_input );
@@ -728,18 +888,48 @@
                                expectation_gradients[ n_layers-1 ],
                                final_cost_gradient );
 
-    for( int i=n_layers-1 ; i>0 ; i-- )
+    if( correlation_connections.length() != 0 )
     {
-        layers[i]->bpropUpdate( activations[i],
-                                expectations[i],
-                                activation_gradients[i],
-                                expectation_gradients[i] );
+        for( int i=n_layers-1 ; i>0 ; i-- )
+        {
+            correlation_layers[i-1]->bpropUpdate( 
+                activations[i],
+                expectations[i],
+                activation_gradients[i],
+                expectation_gradients[i] );
 
-        connections[i-1]->bpropUpdate( expectations[i-1],
-                                       activations[i],
-                                       expectation_gradients[i-1],
-                                       activation_gradients[i] );
+            correlation_connections[i-1]->bpropUpdate( 
+                correlation_expectations[i-1],
+                activations[i],
+                correlation_expectation_gradients[i-1],
+                activation_gradients[i] );
+
+            layers[i]->bpropUpdate( correlation_activations[i-1],
+                                    correlation_expectations[i-1],
+                                    correlation_activation_gradients[i-1],
+                                    correlation_expectation_gradients[i-1] );
+            
+            connections[i-1]->bpropUpdate( expectations[i-1],
+                                           correlation_activations[i-1],
+                                           expectation_gradients[i-1],
+                                           correlation_activation_gradients[i-1] );
+        }
     }
+    else
+    {
+        for( int i=n_layers-1 ; i>0 ; i-- )
+        {
+            layers[i]->bpropUpdate( activations[i],
+                                    expectations[i],
+                                    activation_gradients[i],
+                                    expectation_gradients[i] );
+            
+            connections[i-1]->bpropUpdate( expectations[i-1],
+                                           activations[i],
+                                           expectation_gradients[i-1],
+                                           activation_gradients[i] );
+        }        
+    }
 }
 
 void StackedAutoassociatorsNet::computeOutput(const Vec& input, Vec& output) const
@@ -748,20 +938,57 @@
 
     expectations[0] << input;
 
-    for(int i=0 ; i<currently_trained_layer-1 ; i++ )
+    if(correlation_connections.length() != 0)
     {
-        connections[i]->fprop( expectations[i], activations[i+1] );
-        layers[i+1]->fprop(activations[i+1],expectations[i+1]);
+        for( int i=0 ; i<currently_trained_layer-1; i++ )
+        {
+            connections[i]->fprop( expectations[i], correlation_activations[i] );
+            layers[i+1]->fprop( correlation_activations[i],
+                                correlation_expectations[i] );
+            correlation_connections[i]->fprop( correlation_expectations[i], 
+                                               activations[i+1] );
+            correlation_layers[i]->fprop( activations[i+1], 
+                                          expectations[i+1] );
+        }
     }
+    else
+    {   
+        for(int i=0 ; i<currently_trained_layer-1 ; i++ )
+        {
+            connections[i]->fprop( expectations[i], activations[i+1] );
+            layers[i+1]->fprop(activations[i+1],expectations[i+1]);
+        }
+    }
 
     if( currently_trained_layer<n_layers )
     {
-        connections[currently_trained_layer-1]->fprop( 
-            expectations[currently_trained_layer-1], 
-            activations[currently_trained_layer] );
-        layers[currently_trained_layer]->fprop(
-            activations[currently_trained_layer],
-            output);
+        if(correlation_connections.length() != 0)
+        {
+            connections[currently_trained_layer-1]->fprop( 
+                expectations[currently_trained_layer-1], 
+                correlation_activations[currently_trained_layer-1] );
+
+            layers[currently_trained_layer]->fprop(
+                correlation_activations[currently_trained_layer-1],
+                correlation_expectations[currently_trained_layer-1] );
+
+            correlation_connections[currently_trained_layer-1]->fprop( 
+                correlation_expectations[currently_trained_layer-1], 
+                activations[currently_trained_layer] );
+
+            correlation_layers[currently_trained_layer-1]->fprop( 
+                activations[currently_trained_layer], 
+                output );
+        }
+        else
+        {
+            connections[currently_trained_layer-1]->fprop( 
+                expectations[currently_trained_layer-1], 
+                activations[currently_trained_layer] );
+            layers[currently_trained_layer]->fprop(
+                activations[currently_trained_layer],
+                output);
+        }
     }
     else        
         final_module->fprop( expectations[ currently_trained_layer - 1],
@@ -785,9 +1012,10 @@
             layers[ i ]->fprop( reconstruction_activations,
                                     layers[ i ]->expectation);
             
+            layers[ i ]->activation << reconstruction_activations;
             layers[ i ]->expectation_is_up_to_date = true;
             costs[i] = layers[ i ]->fpropNLL(expectations[ i ]);
-            
+
             if( partial_costs && partial_costs[i])
             {
                 partial_costs[ i ]->fprop( expectations[ i + 1],
@@ -808,6 +1036,8 @@
             reconstruction_activations,
             layers[ currently_trained_layer-1 ]->expectation);
         
+        layers[ currently_trained_layer-1 ]->activation << 
+            reconstruction_activations;
         layers[ currently_trained_layer-1 ]->expectation_is_up_to_date = true;
         costs[ currently_trained_layer-1 ] = 
             layers[ currently_trained_layer-1 ]->fpropNLL(
@@ -869,6 +1099,11 @@
     {
         layers[i]->setLearningRate( the_learning_rate );
         connections[i]->setLearningRate( the_learning_rate );
+        if(correlation_layers.length() != 0)
+        {
+            correlation_layers[i]->setLearningRate( the_learning_rate );
+            correlation_connections[i]->setLearningRate( the_learning_rate );
+        }
     }
     layers[n_layers-1]->setLearningRate( the_learning_rate );
 

Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.h
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2007-07-29 12:05:12 UTC (rev 7857)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2007-07-30 16:44:50 UTC (rev 7858)
@@ -105,6 +105,11 @@
     //! The weights of the reconstruction connections between the layers
     TVec< PP<RBMConnection> > reconstruction_connections;
 
+    //! Optional weights to capture correlation and anti-correlation
+    //! in the hidden layers. They must have the same input and
+    //! output sizes, compatible with their corresponding layers.
+    TVec< PP<RBMConnection> > correlation_connections;
+
     //! Module that takes as input the output of the last layer
     //! (layers[n_layers-1), and feeds its output to final_cost
     //! which defines the fine-tuning criteria.
@@ -220,12 +225,27 @@
     //! Reconstruction expectations
     mutable Vec reconstruction_expectations;
     
-    //! Reconstruction activations
+    //! Reconstruction activation gradients
     mutable Vec reconstruction_activation_gradients;
     
-    //! Reconstruction expectations
+    //! Reconstruction expectation gradients
     mutable Vec reconstruction_expectation_gradients;
 
+    //! Activations before the correlation layer
+    mutable TVec<Vec> correlation_activations;
+    
+    //! Expectations before the correlation layer
+    mutable TVec<Vec> correlation_expectations;
+    
+    //! Gradients of activations before the correlation layer
+    mutable TVec<Vec> correlation_activation_gradients;
+    
+    //! Gradients of expectations before the correlation layer
+    mutable TVec<Vec> correlation_expectation_gradients;
+
+    //! Hidden layers for the correlation connections
+    mutable TVec< PP<RBMLayer> > correlation_layers;
+
     //! Position in the total cost vector of the different partial costs
     mutable TVec<int> partial_costs_positions;
     



From larocheh at mail.berlios.de  Mon Jul 30 18:49:13 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Mon, 30 Jul 2007 18:49:13 +0200
Subject: [Plearn-commits] r7860 - trunk/plearn/ker
Message-ID: <200707301649.l6UGnDGD012463@sheep.berlios.de>

Author: larocheh
Date: 2007-07-30 18:49:12 +0200 (Mon, 30 Jul 2007)
New Revision: 7860

Added:
   trunk/plearn/ker/WeightedQuadraticPolynomialKernel.cc
   trunk/plearn/ker/WeightedQuadraticPolynomialKernel.h
Log:
Second degree polynomial kernel where you can specifiy weights for correlation and 2nd order features.


Added: trunk/plearn/ker/WeightedQuadraticPolynomialKernel.cc
===================================================================
--- trunk/plearn/ker/WeightedQuadraticPolynomialKernel.cc	2007-07-30 16:46:28 UTC (rev 7859)
+++ trunk/plearn/ker/WeightedQuadraticPolynomialKernel.cc	2007-07-30 16:49:12 UTC (rev 7860)
@@ -0,0 +1,163 @@
+// -*- C++ -*-
+
+// PLearn (A C++ Machine Learning Library)
+// Copyright (C) 1998 Pascal Vincent
+// Copyright (C) 1999-2002 Pascal Vincent, Yoshua Bengio, Rejean Ducharme and University of Montreal
+// Copyright (C) 2001-2002 Nicolas Chapados, Ichiro Takeuchi, Jean-Sebastien Senecal
+// Copyright (C) 2002 Xiangdong Wang, Christian Dorion
+
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* *******************************************************      
+ * $Id: WeightedQuadraticPolynomialKernel.cc 6802 2007-03-29 15:19:55Z tihocan $
+ * This file is part of the PLearn library.
+ ******************************************************* */
+
+#include "WeightedQuadraticPolynomialKernel.h"
+
+namespace PLearn {
+using namespace std;
+
+
+
+PLEARN_IMPLEMENT_OBJECT(
+    WeightedQuadraticPolynomialKernel,
+    "Polynomial kernel of degree two, with coefficient correction.",
+    "Computes K(x,y) = 0.5 * ( alpha (1 + <x,y>)^2 - alpha \n"
+    "                          + 2*(1 - alpha) * \\sum_i x_i y_i\n"
+    "                          + (2*beta - alpha) * \\sum_i x_i^2 y_i^2 )\n"
+    "This implies the following features:\n"
+    "  - the first degree features x_i\n"
+    "  - the correlation features x_i x_j (i \neq j) weighted by alpha\n"
+    "  - the second degree features x_i^2 weighted by beta\n"
+);
+
+//////////////////////
+// WeightedQuadraticPolynomialKernel //
+//////////////////////
+
+WeightedQuadraticPolynomialKernel::WeightedQuadraticPolynomialKernel(
+    real the_alpha, real the_beta,
+    bool call_build_):
+    inherited(true, call_build_),
+    alpha(the_alpha),
+    beta(the_beta)
+{
+    if (call_build_)
+        build_();
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void WeightedQuadraticPolynomialKernel::declareOptions(OptionList &ol)
+{
+    declareOption(ol, "alpha", &WeightedQuadraticPolynomialKernel::alpha, 
+                  OptionBase::buildoption,
+                  "Weight on correlation features.");
+
+    declareOption(ol, "beta", &WeightedQuadraticPolynomialKernel::beta, 
+                  OptionBase::buildoption,
+                  "Weight on second degree features.");
+
+    // Declare options inherited from parent class.
+    inherited::declareOptions(ol);
+}
+
+
+///////////
+// build //
+///////////
+void WeightedQuadraticPolynomialKernel::build()
+{
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void WeightedQuadraticPolynomialKernel::build_()
+{}
+
+//////////////
+// evaluate //
+//////////////
+real WeightedQuadraticPolynomialKernel::evaluate(const Vec& x1, const Vec& x2) const
+{ 
+#ifdef BOUNDCHECK
+    if(x1.length()!=x2.length())
+        PLERROR("In WeightedQuadraticPolynomialKernel::evaluate(): "
+            "x1 and x2 have different lengths.");
+#endif
+    real res = 0;
+    real corr = 0;
+    if (x1.size() > 0 && x2.size() > 0) {
+        real* v1 = x1.data();
+        real* v2 = x2.data();
+        real v1i = 0;
+        real v2i = 0;
+        for(int i=0; i<x1.length(); i++)
+        {
+            v1i = v1[i];
+            v2i = v2[i];
+            res += v1i*v2i;
+            corr += v1i*v2i*v1i*v2i;
+        }
+    }
+    
+    //Computes K(x,y) = 0.5 * ( alpha (1 + <x,y>)^2 - alpha \n"
+    //                          + 2*(1 - alpha) * \sum_i x_i y_i\n"
+    //                          + (2*beta - alpha) * \sum_i x_i^2 y_i^2 )\n"
+    return 0.5 * (alpha * ipow(res + real(1.0), 2) - alpha + 2*(1 - alpha)*res + 
+                  (2*beta - alpha)*corr);
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void WeightedQuadraticPolynomialKernel::makeDeepCopyFromShallowCopy(CopiesMap& copies) {
+    inherited::makeDeepCopyFromShallowCopy(copies);
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/ker/WeightedQuadraticPolynomialKernel.h
===================================================================
--- trunk/plearn/ker/WeightedQuadraticPolynomialKernel.h	2007-07-30 16:46:28 UTC (rev 7859)
+++ trunk/plearn/ker/WeightedQuadraticPolynomialKernel.h	2007-07-30 16:49:12 UTC (rev 7860)
@@ -0,0 +1,110 @@
+// -*- C++ -*-
+
+// PLearn (A C++ Machine Learning Library)
+// Copyright (C) 1998 Pascal Vincent
+// Copyright (C) 1999-2002 Pascal Vincent, Yoshua Bengio, Rejean Ducharme and University of Montreal
+// Copyright (C) 2001-2002 Nicolas Chapados, Ichiro Takeuchi, Jean-Sebastien Senecal
+// Copyright (C) 2002 Xiangdong Wang, Christian Dorion
+
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* *******************************************************      
+ * $Id: WeightedQuadraticPolynomialKernel.h 6859 2007-04-09 18:07:57Z nouiz $
+ * This file is part of the PLearn library.
+ ******************************************************* */
+
+#ifndef WeightedQuadraticPolynomialKernel_INC
+#define WeightedQuadraticPolynomialKernel_INC
+
+#include "Kernel.h"
+
+namespace PLearn {
+using namespace std;
+
+class WeightedQuadraticPolynomialKernel: public Kernel
+{
+    typedef Kernel inherited;
+
+public:
+    
+    //! Weight on correlation features
+    real alpha;
+    //! Weight on second degree features
+    real beta;
+
+public:
+
+    //! Convenient constructor.
+    WeightedQuadraticPolynomialKernel(real the_alph=1.0, real the_beta = 1.0, bool call_build_ = true);
+
+    PLEARN_DECLARE_OBJECT(WeightedQuadraticPolynomialKernel);
+
+    // Kernel methods.
+
+    virtual real evaluate(const Vec& x1, const Vec& x2) const; 
+
+    // Object methods.
+
+    // Simply calls inherited::build() then build_().
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy.
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+
+    //! Declares the class options.
+    static void declareOptions(OptionList &ol);  
+
+private:
+
+    //! This does the actual building.
+    void build_();
+
+};
+
+DECLARE_OBJECT_PTR(WeightedQuadraticPolynomialKernel);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From tihocan at mail.berlios.de  Mon Jul 30 19:45:20 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 30 Jul 2007 19:45:20 +0200
Subject: [Plearn-commits] r7867 - in trunk/plearn/var/test: .
	.pytest/PL_Var_util/expected_results
Message-ID: <200707301745.l6UHjKhW002571@sheep.berlios.de>

Author: tihocan
Date: 2007-07-30 19:45:20 +0200 (Mon, 30 Jul 2007)
New Revision: 7867

Modified:
   trunk/plearn/var/test/.pytest/PL_Var_util/expected_results/VarUtilsTest.psave
   trunk/plearn/var/test/pytest.config
Log:
Re-enabled test PL_Var_util after fixing it (in its old version, the data matrix was not read properly, and thus the results were flawed)

Modified: trunk/plearn/var/test/.pytest/PL_Var_util/expected_results/VarUtilsTest.psave
===================================================================
--- trunk/plearn/var/test/.pytest/PL_Var_util/expected_results/VarUtilsTest.psave	2007-07-30 17:19:09 UTC (rev 7866)
+++ trunk/plearn/var/test/.pytest/PL_Var_util/expected_results/VarUtilsTest.psave	2007-07-30 17:45:20 UTC (rev 7867)
@@ -9,7 +9,7 @@
 0.214891921170055872 	
 ]
 , "neg_log_pi_mat" : 1  5  [ 
-1.27522469607823474 	1.38932572241559882 	0.933409948437706083 	0.728803642366828708 	1.85270418478163679 	
+0.186460432996275877 	0.618667293502052185 	2.39456905411730014 	0.097875398361582544 	0.451308831247238806 	
 ]
 , "neg_log_pi_vec" : 1  1  [ 
 0.426231023016261146 	

Modified: trunk/plearn/var/test/pytest.config
===================================================================
--- trunk/plearn/var/test/pytest.config	2007-07-30 17:19:09 UTC (rev 7866)
+++ trunk/plearn/var/test/pytest.config	2007-07-30 17:45:20 UTC (rev 7867)
@@ -104,5 +104,5 @@
     resources = [ ],
     precision = 1e-06,
     pfileprg = "__program__",
-    disabled = True
+    disabled = False
     )



From tihocan at mail.berlios.de  Mon Jul 30 19:19:09 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 30 Jul 2007 19:19:09 +0200
Subject: [Plearn-commits] r7866 - trunk/plearn/math
Message-ID: <200707301719.l6UHJ95M016342@sheep.berlios.de>

Author: tihocan
Date: 2007-07-30 19:19:09 +0200 (Mon, 30 Jul 2007)
New Revision: 7866

Modified:
   trunk/plearn/math/TMat_impl.h
Log:
Fixed crash when reading a sequence like '1 2 3' into a Mat, when there is no trailing space

Modified: trunk/plearn/math/TMat_impl.h
===================================================================
--- trunk/plearn/math/TMat_impl.h	2007-07-30 17:18:38 UTC (rev 7865)
+++ trunk/plearn/math/TMat_impl.h	2007-07-30 17:19:09 UTC (rev 7866)
@@ -351,8 +351,10 @@
         T* v = rowdata(i);
         for (int j=0;j<width();j++)
         {
-            if(!(in>>v[j]))
+            if (!in)
                 PLERROR("In TMat<T>::input error encountered while reading matrix");
+            else
+                in>>v[j];
         }
     }
 }



From tihocan at mail.berlios.de  Mon Jul 30 18:56:57 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 30 Jul 2007 18:56:57 +0200
Subject: [Plearn-commits] r7863 - trunk/plearn/vmat
Message-ID: <200707301656.l6UGuvpr019556@sheep.berlios.de>

Author: tihocan
Date: 2007-07-30 18:56:56 +0200 (Mon, 30 Jul 2007)
New Revision: 7863

Modified:
   trunk/plearn/vmat/SelectColumnsVMatrix.cc
Log:
Removed useless deep copy

Modified: trunk/plearn/vmat/SelectColumnsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SelectColumnsVMatrix.cc	2007-07-30 16:55:41 UTC (rev 7862)
+++ trunk/plearn/vmat/SelectColumnsVMatrix.cc	2007-07-30 16:56:56 UTC (rev 7863)
@@ -158,7 +158,6 @@
     deepCopyField(sinput, copies);
     deepCopyField(indices, copies);
     deepCopyField(fields, copies);
-    deepCopyField(inverse_fields_selection, copies);
 }
 
 ///////////



From tihocan at mail.berlios.de  Mon Jul 30 18:57:47 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 30 Jul 2007 18:57:47 +0200
Subject: [Plearn-commits] r7864 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200707301657.l6UGvlRM020748@sheep.berlios.de>

Author: tihocan
Date: 2007-07-30 18:57:47 +0200 (Mon, 30 Jul 2007)
New Revision: 7864

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
Log:
Added an assert for safety

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2007-07-30 16:56:56 UTC (rev 7863)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2007-07-30 16:57:47 UTC (rev 7864)
@@ -811,6 +811,7 @@
                     exit(0);
                 }
             }
+            PLASSERT( iam == 0 );
             if (semctl(semaphore_id, sem_value + 1, GETVAL) == 0) {
                 // The next process is not done yet: we need to wait.
 #if 0



From tihocan at mail.berlios.de  Mon Jul 30 18:55:57 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 30 Jul 2007 18:55:57 +0200
Subject: [Plearn-commits] r7862 - trunk/plearn/db
Message-ID: <200707301655.l6UGtv8j018498@sheep.berlios.de>

Author: tihocan
Date: 2007-07-30 18:55:41 +0200 (Mon, 30 Jul 2007)
New Revision: 7862

Modified:
   trunk/plearn/db/getDataSet.cc
Log:
Can now automatically deduced inputsize from the width and other sizes

Modified: trunk/plearn/db/getDataSet.cc
===================================================================
--- trunk/plearn/db/getDataSet.cc	2007-07-30 16:55:07 UTC (rev 7861)
+++ trunk/plearn/db/getDataSet.cc	2007-07-30 16:55:41 UTC (rev 7862)
@@ -169,6 +169,17 @@
         vm->defineSizes(vm->width(), 0, 0);
     }
 
+    // Set inputsize if it can be deduced from other sizes.
+    if (vm->inputsize() < 0 && vm->width() >= 0 && vm->targetsize() >= 0 &&
+            vm->weightsize() >= 0 && vm->extrasize() >= 0)
+    {
+        int is = vm->width() - vm->targetsize() - vm->weightsize() -
+            vm->extrasize();
+        if (is >= 0)
+            vm->defineSizes(is, vm->targetsize(), vm->weightsize(),
+                    vm->extrasize());
+    }
+
     // Ensure sizes do not conflict with width.
     if (vm->inputsize() >= 0 && vm->targetsize() >= 0 && vm->weightsize() >= 0 &&
         vm->width() >= 0 &&  vm->width() < vm->inputsize() + vm->targetsize() + vm->weightsize())



From tihocan at mail.berlios.de  Mon Jul 30 19:18:39 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 30 Jul 2007 19:18:39 +0200
Subject: [Plearn-commits] r7865 - trunk/plearn/var/test
Message-ID: <200707301718.l6UHIdfv015353@sheep.berlios.de>

Author: tihocan
Date: 2007-07-30 19:18:38 +0200 (Mon, 30 Jul 2007)
New Revision: 7865

Modified:
   trunk/plearn/var/test/VarUtilsTest.cc
Log:
Attempt at fixing test PL_Var_util (not working yet)

Modified: trunk/plearn/var/test/VarUtilsTest.cc
===================================================================
--- trunk/plearn/var/test/VarUtilsTest.cc	2007-07-30 16:57:47 UTC (rev 7864)
+++ trunk/plearn/var/test/VarUtilsTest.cc	2007-07-30 17:18:38 UTC (rev 7865)
@@ -137,7 +137,7 @@
     Var prob_vec_var(var_length, 1);
     Var scalar_var(1, 1);
     Var multi_index_var(var_width, 1);
-    multi_index_var->matValue << "[ 1 3 5 2 9 ]";
+    multi_index_var->matValue << "1 3 5 2 9";
     Var single_index_var(1, 1);
     single_index_var->matValue(0, 0) = 3;
     PRandom::common(false)->fill_random_uniform(mat_var->matValue, -bound, bound);



From larocheh at mail.berlios.de  Mon Jul 30 18:46:38 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Mon, 30 Jul 2007 18:46:38 +0200
Subject: [Plearn-commits] r7859 - trunk/plearn_learners_experimental
Message-ID: <200707301646.l6UGkcMY007885@sheep.berlios.de>

Author: larocheh
Date: 2007-07-30 18:46:28 +0200 (Mon, 30 Jul 2007)
New Revision: 7859

Modified:
   trunk/plearn_learners_experimental/StackedSVDNet.cc
   trunk/plearn_learners_experimental/StackedSVDNet.h
Log:
blu bla


Modified: trunk/plearn_learners_experimental/StackedSVDNet.cc
===================================================================
--- trunk/plearn_learners_experimental/StackedSVDNet.cc	2007-07-30 16:44:50 UTC (rev 7858)
+++ trunk/plearn_learners_experimental/StackedSVDNet.cc	2007-07-30 16:46:28 UTC (rev 7859)
@@ -36,12 +36,12 @@
 
 /*! \file StackedSVDNet.cc */
 
+#include "StackedSVDNet.h"
 
 #define PL_LOG_MODULE_NAME "StackedSVDNet"
 #include <plearn/io/pl_log.h>
 #include <plearn/math/plapack.h>
 
-#include "StackedSVDNet.h"
 
 namespace PLearn {
 using namespace std;
@@ -58,7 +58,8 @@
     fine_tuning_learning_rate( 0. ),
     fine_tuning_decrease_ct( 0. ),
     batch_size(50),
-    minimum_relative_improvement(1e-3),
+    global_output_layer(false),
+    relative_min_improvement(1e-3),
     n_layers( 0 )
 {
     // random_gen will be initialized in PLearner::build_()
@@ -99,9 +100,16 @@
                   OptionBase::buildoption,
                   "Size of mini-batch for gradient descent");
 
-    declareOption(ol, "minimum_relative_improvement", 
-                  &StackedSVDNet::minimum_relative_improvement,
+    declareOption(ol, "global_output_layer", 
+                  &StackedSVDNet::global_output_layer,
                   OptionBase::buildoption,
+                  "Indication that the output layer (given by the final module)\n"
+                  "should have as input all units of the network (including the"
+                  "input units).\n");
+
+    declareOption(ol, "relative_min_improvement", 
+                  &StackedSVDNet::relative_min_improvement,
+                  OptionBase::buildoption,
                   "Minimum relative improvement convergence criteria \n"
                   "for the logistic auto-regression.");
 
@@ -159,7 +167,7 @@
                     "layers[0] should have a size of %d.\n",
                     inputsize_);
 
-        reconstruction_costs(batch_size,1);    
+        reconstruction_costs.resize(batch_size,1);    
 
         activation_gradients.resize( n_layers );
         expectation_gradients.resize( n_layers );
@@ -200,15 +208,36 @@
             PLERROR("StackedSVDNet::build_costs() - \n"
                     "final_module should be provided.\n");
     
-        if( layers[n_layers-1]->size != final_module->input_size )
-            PLERROR("StackedSVDNet::build_costs() - \n"
-                    "final_module should have an input_size of %d.\n", 
-                    layers[n_layers-1]->size);
-    
+        if(global_output_layer)
+        {
+            int sum = 0;
+            for(int i=0; i<layers.length(); i++)
+                sum += layers[i]->size;
+            if( sum != final_module->input_size )
+                PLERROR("StackedSVDNet::build_costs() - \n"
+                        "final_module should have an input_size of %d.\n", 
+                        sum);
+
+            global_output_layer_input.resize(sum);
+            global_output_layer_inputs.resize(batch_size,sum);
+            global_output_layer_input_gradients.resize(batch_size,sum);
+            expectation_gradients[n_layers-1] = 
+                global_output_layer_input_gradients.subMat(
+                    0, sum-layers[n_layers-1]->size, 
+                    batch_size, layers[n_layers-1]->size);
+        }
+        else
+        {
+            if( layers[n_layers-1]->size != final_module->input_size )
+                PLERROR("StackedSVDNet::build_costs() - \n"
+                        "final_module should have an input_size of %d.\n", 
+                        layers[n_layers-1]->size);
+        }
+
         if( final_module->output_size != final_cost->input_size )
             PLERROR("StackedSVDNet::build_costs() - \n"
                     "final_module should have an output_size of %d.\n", 
-                    final_module->input_size);
+                    final_cost->input_size);
 
         final_module->setLearningRate( fine_tuning_learning_rate );
 
@@ -243,6 +272,7 @@
     deepCopyField(final_module, copies);
     deepCopyField(final_cost, copies);
     deepCopyField(connections, copies);
+    deepCopyField(rbm_connections, copies);
     deepCopyField(activation_gradients, copies);
     deepCopyField(expectation_gradients, copies);
     deepCopyField(reconstruction_layer, copies);
@@ -251,6 +281,9 @@
     deepCopyField(reconstruction_activation_gradient, copies);
     deepCopyField(reconstruction_activation_gradients, copies);
     deepCopyField(reconstruction_input_gradients, copies);
+    deepCopyField(global_output_layer_input, copies);
+    deepCopyField(global_output_layer_inputs, copies);
+    deepCopyField(global_output_layer_input_gradients, copies);
     deepCopyField(final_cost_inputs, copies);
     deepCopyField(final_cost_value, copies);
     deepCopyField(final_cost_values, copies);
@@ -273,6 +306,9 @@
     connections.resize(0);
     rbm_connections.resize(0);
     
+    for(int i=0; i<layers.length(); i++)
+        layers[i]->forget();
+
     final_module->forget();
     final_cost->forget();
 
@@ -315,6 +351,7 @@
             connections[i] = new RBMMatrixConnection();
             connections[i]->up_size = layers[i]->size;
             connections[i]->down_size = layers[i]->size;
+            connections[i]->random_gen = random_gen;
             connections[i]->build();
             for(int j=0; j < layers[i]->size; j++)
                 connections[i]->weights(j,j) = 0;
@@ -339,10 +376,11 @@
             int nupdates = 0;
             int nepochs = 0;
             while( nepochs < 2 ||
-                   (last_cost - cost) / last_cost >= minimum_relative_improvement )
+                   (last_cost - cost) / last_cost >= relative_min_improvement )
             {
                 train_stats->forget();
-                for(int sample = 0; sample < train_set.length()/batch_size; 
+                for(int sample = 0; 
+                    sample < train_set.length()/batch_size; 
                     sample++)
                 {
                     if( !fast_exact_is_equal( greedy_decrease_ct , 0 ) )
@@ -367,29 +405,50 @@
                 train_stats->finalize();
                 nepochs++;
                 last_cost = cost;
-                cost = train_stats->getMean()[0];
+                cost = train_stats->getMean()[i];
+                if(verbosity > 2)
+                    cout << "reconstruction error at iteration " << nepochs << 
+                        ": " << 
+                        cost << " or " << cost/layers[i]->size << " (rel)" << endl;
             }
-            Mat A,U,Vt;
-            Vec S;
-            A.resize( reconstruction_layer->size, reconstruction_layer->size+1);
-            A.column( 0 ) << reconstruction_layer->bias;
-            A.subMat( 0, 1, reconstruction_layer->size, 
-                      reconstruction_layer->size ) << 
-                connections[i]->weights;
-            SVD( A, U, S, Vt );
-            connections[ i ]->up_size = layers[ i+1 ]->size;
-            connections[ i ]->down_size = layers[ i ]->size;
-            connections[ i ]->build();
-            connections[ i ]->weights << Vt.subMat( 
-                0, 0, layers[i+1]->size, Vt.width() );
-            biases[ i ].resize( layers[i+1]->size );
-            biases[ i ] << Vt.column( 0 ).toVec().subVec( 
-                0, layers[i+1]->size );
-            for(int j=0; j<connections[ i ]->up_size; j++)
+
+            // Fill in the empty diagonal
+            for(int j=0; j<layers[i]->size; j++)
             {
-                connections[ i ]->weights( j ) *= S[ j ];
-                biases[ i ][ j ] *= S[ j ];
+                connections[i]->weights(j,j) = maxabs(connections[i]->weights(j));
             }
+
+            if(layers[i]->size != layers[i+1]->size)
+            {
+                Mat A,U,Vt;
+                Vec S;
+                A.resize( reconstruction_layer->size, 
+                          reconstruction_layer->size+1);
+                A.column( 0 ) << reconstruction_layer->bias;
+                A.subMat( 0, 1, reconstruction_layer->size, 
+                          reconstruction_layer->size ) << 
+                    connections[i]->weights;
+                SVD( A, U, S, Vt );
+                connections[ i ]->up_size = layers[ i+1 ]->size;
+                connections[ i ]->down_size = layers[ i ]->size;
+                connections[ i ]->build();
+                connections[ i ]->weights << Vt.subMat( 
+                    0, 1, layers[ i+1 ]->size, Vt.width()-1 );
+                biases[ i ].resize( layers[i+1]->size );
+                for(int j=0; j<biases[ i ].length(); j++)
+                    biases[ i ][ j ] = Vt(j,0);
+
+                for(int j=0; j<connections[ i ]->up_size; j++)
+                {
+                    connections[ i ]->weights( j ) *= S[ j ];
+                    biases[ i ][ j ] *= S[ j ];
+                }
+            }
+            else
+            {
+                biases[ i ].resize( layers[ i+1 ]->size );
+                biases[ i ] << reconstruction_layer->bias;
+            }
         }
         stage++;
         for(int i=0; i<biases.length(); i++)
@@ -419,7 +478,9 @@
 
         for( ; stage<nstages ; stage++ )
         {
-            for( int sample = 0; sample<train_set->length()/batch_size; sample++)
+            for( int sample = 0; 
+                 sample<train_set->length()/batch_size; 
+                 sample++)
             {
                 if( !fast_exact_is_equal( fine_tuning_decrease_ct, 0. ) )
                     setLearningRate( fine_tuning_learning_rate
@@ -438,6 +499,10 @@
                 if( pb )
                     pb->update( stage - init_stage + 1 );
             }
+            if(verbosity > 2)
+                cout << "error at stage " << stage << ": " << 
+                    train_stats->getMean() << endl;
+
         }
     }
     
@@ -498,25 +563,62 @@
         layers[ i+1 ]->computeExpectations();
     }
 
-    final_module->fprop( layers[ n_layers-1 ]->getExpectations(),
-                         final_cost_inputs );
+    if( global_output_layer )
+    {
+        int offset = 0;
+        for(int i=0; i<layers.length(); i++)
+        {
+            global_output_layer_inputs.subMat(0, offset, 
+                                              batch_size, layers[i]->size)
+                << layers[i]->getExpectations();
+            offset += layers[i]->size;
+        }
+        final_module->fprop( global_output_layer_inputs, final_cost_inputs );
+    }
+    else
+    {
+        final_module->fprop( layers[ n_layers-1 ]->getExpectations(),
+                             final_cost_inputs );
+    }
     final_cost->fprop( final_cost_inputs, targets, final_cost_values );
 
     columnMean( final_cost_values, 
                 final_cost_value );
-    train_costs.subVec(train_costs.length()-final_cost_value.length()) << 
-        final_cost_value;
+    train_costs.subVec(train_costs.length()-final_cost_value.length(),
+                       final_cost_value.length()) << final_cost_value;
 
     final_cost->bpropUpdate( final_cost_inputs, targets,
-                             final_cost_values,
+                             final_cost_value,
                              final_cost_gradients );
-    final_module->bpropUpdate( layers[ n_layers-1 ]->getExpectations(),
-                               final_cost_inputs,
-                               expectation_gradients[ n_layers-1 ],
-                               final_cost_gradients );
+    
+    if( global_output_layer )
+    {
+        final_module->bpropUpdate( global_output_layer_inputs,
+                                   final_cost_inputs,
+                                   global_output_layer_input_gradients,
+                                   final_cost_gradients );     
+    }
+    else
+    {
+        final_module->bpropUpdate( layers[ n_layers-1 ]->getExpectations(),
+                                   final_cost_inputs,
+                                   expectation_gradients[ n_layers-1 ],
+                                   final_cost_gradients );
+    }
 
+    int sum = final_module->input_size - layers[ n_layers-1 ]->size;
     for( int i=n_layers-1 ; i>0 ; i-- )
     {
+        if( global_output_layer && i != n_layers-1 )
+        {
+            expectation_gradients[ i ] +=  
+                global_output_layer_input_gradients.subMat(
+                    0, sum - layers[i]->size,
+                    batch_size, layers[i]->size);
+            sum -= layers[i]->size;
+        }
+                
+
         layers[ i ]->bpropUpdate( layers[ i ]->activations,
                                   layers[ i ]->getExpectations(),
                                   activation_gradients[ i ],
@@ -538,12 +640,26 @@
     for( int i=0 ; i<n_layers-1 ; i++ )
     {
         connections[ i ]->setAsDownInput( layers[i]->expectation );
-        layers[ i+1 ]->getAllActivations( rbm_connections[i], 0, true );
+        layers[ i+1 ]->getAllActivations( rbm_connections[i], 0, false );
         layers[ i+1 ]->computeExpectation();
     }
 
-    final_module->fprop( layers[ n_layers-1 ]->expectation,
-                         output );
+    if(global_output_layer)
+    {
+        int offset = 0;
+        for(int i=0; i<layers.length(); i++)
+        {
+            global_output_layer_input.subVec(offset, layers[i]->size)
+                << layers[i]->expectation;
+            offset += layers[i]->size;
+        }
+        final_module->fprop( global_output_layer_input, output );
+    }
+    else
+    {
+        final_module->fprop( layers[ n_layers-1 ]->expectation,
+                             output );
+    }
 }
 
 void StackedSVDNet::computeCostsFromOutputs(const Vec& input, const Vec& output,

Modified: trunk/plearn_learners_experimental/StackedSVDNet.h
===================================================================
--- trunk/plearn_learners_experimental/StackedSVDNet.h	2007-07-30 16:44:50 UTC (rev 7858)
+++ trunk/plearn_learners_experimental/StackedSVDNet.h	2007-07-30 16:46:28 UTC (rev 7859)
@@ -77,10 +77,14 @@
 
     //! Size of mini-batch for gradient descent
     int batch_size;
+
+    //! Indication that the output layer (given by the final module)
+    //! should have as input all units of the network (including the input units)
+    bool global_output_layer;
     
     //! Minimum relative improvement convergence criteria
     //! for the logistic auto-regression.
-    real minimum_relative_improvement;
+    real relative_min_improvement;
     
     //! The layers of units in the network
     TVec< PP<RBMLayer> > layers;
@@ -198,6 +202,15 @@
     //! Reconstruction activations
     mutable Mat reconstruction_input_gradients;
 
+    //! Global output layer input
+    mutable Vec global_output_layer_input;
+
+    //! Global output layer inputs
+    mutable Mat global_output_layer_inputs;
+
+    //! Global output layer input gradients
+    mutable Mat global_output_layer_input_gradients;
+
     //! Inputs of the final_cost
     mutable Mat final_cost_inputs;
 



From tihocan at mail.berlios.de  Mon Jul 30 18:55:08 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 30 Jul 2007 18:55:08 +0200
Subject: [Plearn-commits] r7861 - trunk/plearn/vmat
Message-ID: <200707301655.l6UGt8Vw018240@sheep.berlios.de>

Author: tihocan
Date: 2007-07-30 18:55:07 +0200 (Mon, 30 Jul 2007)
New Revision: 7861

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
- Fixed potential problem where the inputsize may be too large when using setMetaInfoFrom
- Allow inputsize to be deduced from other sizes in setMetaInfoFrom, when it cannot be obtained from the source's inputsize


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2007-07-30 16:49:12 UTC (rev 7860)
+++ trunk/plearn/vmat/VMatrix.cc	2007-07-30 16:55:07 UTC (rev 7861)
@@ -1115,27 +1115,49 @@
         width_ = vm->width();
 
     // Copy sizes from vm if not set and they do not conflict with the width.
+    int current_w = max(0, inputsize_) + max(0, targetsize_) +
+                    max(0, weightsize_) + max(0, extrasize_);
     if(inputsize_<0) {
         int is = vm->inputsize();
-        if (is <= width_)
+        if (is + current_w <= width_) {
             inputsize_ = is;
+            current_w += is;
+        }
     }
     if(targetsize_<0) {
         int ts = vm->targetsize();
-        if (ts + (inputsize_ > 0 ? inputsize_ : 0) <= width_)
+        if (ts + current_w <= width_) {
             targetsize_ = ts;
+            current_w += ts;
+        }
     }
     if(weightsize_<0) {
         int ws = vm->weightsize();
-        if (ws + (inputsize_ > 0 ? inputsize_ : 0) + (targetsize_ > 0 ? targetsize_ : 0) <= width_)
+        if (ws + current_w <= width_) {
             weightsize_ = ws;
+            current_w += ws;
+        }
     }
     if(extrasize_<=0) {
         int es = vm->extrasize();
-        if (es + (inputsize_ > 0 ? inputsize_ : 0) + (targetsize_ > 0 ? targetsize_ : 0) + (weightsize_ > 0 ? weightsize_ : 0) <= width_)
+        if (es + current_w <= width_) {
             extrasize_ = es;
+            current_w += es;
+        }
     }
 
+    // Automatically find out inputsize if possible.
+    // Note that this could be done also with other sizes.
+    if (inputsize_ < 0 && width_ >= 0 && targetsize_ >= 0 &&
+            weightsize_ >= 0 && extrasize_ >= 0)
+    {
+        int new_is = width_ - targetsize_ - weightsize_ -
+            extrasize_;
+        if (new_is >= 0)
+            inputsize_ = new_is;
+    }
+
+
     // Copy fieldnames from vm if not set and they look good.
     bool same_fields_as_source =
         (!hasFieldInfos() && (width() == vm->width()) && vm->hasFieldInfos());



From lamblin at mail.berlios.de  Mon Jul 30 21:49:11 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Mon, 30 Jul 2007 21:49:11 +0200
Subject: [Plearn-commits] r7868 - trunk/plearn/io
Message-ID: <200707301949.l6UJnBwp011363@sheep.berlios.de>

Author: lamblin
Date: 2007-07-30 21:49:09 +0200 (Mon, 30 Jul 2007)
New Revision: 7868

Modified:
   trunk/plearn/io/PStream.h
Log:
Add overflow check when reading from binary stream.


Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2007-07-30 17:45:20 UTC (rev 7867)
+++ trunk/plearn/io/PStream.h	2007-07-30 19:49:09 UTC (rev 7868)
@@ -330,13 +330,24 @@
 // explicit here.
 #endif
         x = static_cast<J>(y);
+
+        // Check if there was a conversion problem (sign or overflow)
+        if (static_cast<I>(x) != y
+            || !(numeric_limits<J>::is_signed) && y<0)
+        {
+            std::stringstream error;
+            error << "In PStream::readBinaryNumAs, overflow error: " << std::endl
+                << "unable to read value \"" << y << "\""
+                << "into a " << TypeTraits<I>::name() << std::endl;
+
+            PLERROR( error.str().c_str() );
+        }
 #ifdef __INTEL_COMPILER
 #pragma warning(default:1682)
 #endif
     }
 
     //! Reads base types from PLearn binary format
-    //! TODO: forbid this mechanism for arbitrary classes?
     template<class I>
     void readBinaryNum(I &x, unsigned char typecode)
     {



From nouiz at mail.berlios.de  Mon Jul 30 22:28:03 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 30 Jul 2007 22:28:03 +0200
Subject: [Plearn-commits] r7869 - trunk/plearn_learners/meta
Message-ID: <200707302028.l6UKS3rZ013916@sheep.berlios.de>

Author: nouiz
Date: 2007-07-30 22:28:03 +0200 (Mon, 30 Jul 2007)
New Revision: 7869

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
Some minor modification to allow the use of weak learner with the outputsize >1
We use only the first output field


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2007-07-30 19:49:09 UTC (rev 7868)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2007-07-30 20:28:03 UTC (rev 7869)
@@ -270,7 +270,7 @@
     static Vec pseudo_loss;
 
     input.resize(inputsize());
-    output.resize(1);
+    output.resize(weak_learner_template->outputsize());// We use only the first one as the output from the weak learner
     target.resize(targetsize());
     examples_error.resize(n);
 
@@ -610,9 +610,9 @@
 
 void AdaBoost::computeOutput(const Vec& input, Vec& output) const
 {
-    output.resize(1);
+    output.resize(weak_learner_template->outputsize());
     real sum_out=0;
-    static Vec weak_learner_output(1);
+    static Vec weak_learner_output(output.size());
     for (int i=0;i<voting_weights.length();i++)
     {
         weak_learners[i]->computeOutput(input,weak_learner_output);
@@ -623,6 +623,7 @@
             sum_out += weak_learner_output[0]*voting_weights[i];
     }
     output[0] = sum_out/sum_voting_weights;
+    output.resize(1);
 }
 
 void AdaBoost::computeCostsFromOutputs(const Vec& input, const Vec& output, 



From nouiz at mail.berlios.de  Mon Jul 30 22:43:00 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 30 Jul 2007 22:43:00 +0200
Subject: [Plearn-commits] r7870 - trunk/plearn/vmat
Message-ID: <200707302043.l6UKh0x8015088@sheep.berlios.de>

Author: nouiz
Date: 2007-07-30 22:43:00 +0200 (Mon, 30 Jul 2007)
New Revision: 7870

Modified:
   trunk/plearn/vmat/VariableDeletionVMatrix.cc
Log:
Added some verification


Modified: trunk/plearn/vmat/VariableDeletionVMatrix.cc
===================================================================
--- trunk/plearn/vmat/VariableDeletionVMatrix.cc	2007-07-30 20:28:03 UTC (rev 7869)
+++ trunk/plearn/vmat/VariableDeletionVMatrix.cc	2007-07-30 20:43:00 UTC (rev 7870)
@@ -237,12 +237,13 @@
                 "have an inputsize defined");
 
     VMat the_train_source = train_set ? train_set : source;
+    PLCHECK( the_train_source.width() == source->width() );
+
     if (number_of_train_samples > 0 &&
         number_of_train_samples < the_train_source->length())
         the_train_source = new SubVMatrix(the_train_source, 0, 0,
                                           number_of_train_samples,
                                           the_train_source->width());
-        
     TVec<StatsCollector> stats;
     if(min_non_missing_threshold > 0 || max_constant_threshold > 0){
         stats = PLearn::computeStats(the_train_source, -1, false);



From nouiz at mail.berlios.de  Mon Jul 30 23:02:23 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 30 Jul 2007 23:02:23 +0200
Subject: [Plearn-commits] r7871 - trunk/plearn_learners/generic
Message-ID: <200707302102.l6UL2NXR016340@sheep.berlios.de>

Author: nouiz
Date: 2007-07-30 23:02:23 +0200 (Mon, 30 Jul 2007)
New Revision: 7871

Modified:
   trunk/plearn_learners/generic/PLearner.cc
Log:
Added profiling


Modified: trunk/plearn_learners/generic/PLearner.cc
===================================================================
--- trunk/plearn_learners/generic/PLearner.cc	2007-07-30 20:43:00 UTC (rev 7870)
+++ trunk/plearn_learners/generic/PLearner.cc	2007-07-30 21:02:23 UTC (rev 7871)
@@ -875,6 +875,9 @@
 void PLearner::test(VMat testset, PP<VecStatsCollector> test_stats,
                     VMat testoutputs, VMat testcosts) const
 {
+
+    Profiler::pl_profile_start("PLearner::test");
+
     int len = testset.length();
     Vec input;
     Vec target;
@@ -1083,6 +1086,9 @@
 
     if (use_a_separate_random_generator_for_testing && random_gen)
         *random_gen = *copy_random_gen;
+
+    Profiler::pl_profile_end("PLearner::test");
+
 }
 
 void PLearner::computeOutput(const Vec& input, Vec& output) const



From saintmlx at mail.berlios.de  Tue Jul 31 00:20:12 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 31 Jul 2007 00:20:12 +0200
Subject: [Plearn-commits] r7872 - in trunk: plearn/base plearn/python
	python_modules/plearn/pybridge python_modules/plearn/pyext
Message-ID: <200707302220.l6UMKCTe021046@sheep.berlios.de>

Author: saintmlx
Date: 2007-07-31 00:20:10 +0200 (Tue, 31 Jul 2007)
New Revision: 7872

Modified:
   trunk/plearn/base/Object.cc
   trunk/plearn/base/Option.h
   trunk/plearn/python/PythonExtension.cc
   trunk/plearn/python/PythonExtension.h
   trunk/plearn/python/PythonObjectWrapper.cc
   trunk/plearn/python/PythonObjectWrapper.h
   trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
   trunk/python_modules/plearn/pyext/__init__.py
Log:
- inject PLearn classes into python extension module
- fix python refcounts for getOption (was leaking)



Modified: trunk/plearn/base/Object.cc
===================================================================
--- trunk/plearn/base/Object.cc	2007-07-30 21:02:23 UTC (rev 7871)
+++ trunk/plearn/base/Object.cc	2007-07-30 22:20:10 UTC (rev 7872)
@@ -692,8 +692,7 @@
                                                  const TVec<PythonObjectWrapper>& args) const
                 {
 			checkNargs(args.size(), 0);
-                        return PythonObjectWrapper(instance,
-                                                   PythonObjectWrapper::transfer_ownership);
+                        return PythonObjectWrapper(instance);
                 }
 #endif //def PL_PYTHON_VERSION 
 
@@ -744,7 +743,11 @@
                   (BodyDoc("Returns the name of the class"),
                    RetDoc ("Class name as string")));
 
+    declareMethod(rmm, "usage", &Object::usage,
+                  (BodyDoc("Returns the refcount of this PPointable"),
+                   RetDoc ("Number of references")));
 
+
 #ifdef PL_PYTHON_VERSION 
     declareMethod(rmm, "setOptionFromPython", &Object::setOptionFromPython,
                   (BodyDoc("Change an option within the object from a PythonObjectWrapper"),

Modified: trunk/plearn/base/Option.h
===================================================================
--- trunk/plearn/base/Option.h	2007-07-30 21:02:23 UTC (rev 7871)
+++ trunk/plearn/base/Option.h	2007-07-30 22:20:10 UTC (rev 7872)
@@ -126,18 +126,12 @@
 #ifdef PL_PYTHON_VERSION 
     virtual PythonObjectWrapper getAsPythonObject(Object* o) const 
     { 
-        return PythonObjectWrapper(ConvertToPyObject<OptionType>::
-                                   newPyObject(*(OptionType*)getAsVoidPtr(o)),
-                                   PythonObjectWrapper::transfer_ownership); 
-        
+        return PythonObjectWrapper(*(OptionType*)getAsVoidPtr(o)); 
     }
 
     virtual PythonObjectWrapper getAsPythonObject(const Object* o) const 
     { 
-        return PythonObjectWrapper(ConvertToPyObject<OptionType>::
-                                   newPyObject(*(OptionType*)getAsVoidPtr(o)),
-                                   PythonObjectWrapper::transfer_ownership); 
-
+        return PythonObjectWrapper(*(OptionType*)getAsVoidPtr(o)); 
     }
 
     virtual void setFromPythonObject(Object* o, const PythonObjectWrapper& v) const

Modified: trunk/plearn/python/PythonExtension.cc
===================================================================
--- trunk/plearn/python/PythonExtension.cc	2007-07-30 21:02:23 UTC (rev 7871)
+++ trunk/plearn/python/PythonExtension.cc	2007-07-30 22:20:10 UTC (rev 7872)
@@ -32,8 +32,12 @@
 // library, go to the PLearn Web site at www.plearn.org
 
 #include <plearn/python/PythonExtension.h>
+#include <plearn/python/PythonObjectWrapper.h>
 #include <plearn/base/RemoteDeclareMethod.h>
 #include <plearn/base/HelpSystem.h>
+#include <plearn/base/TypeFactory.h>
+#include <plearn/base/PMemPool.h>
+#include <plearn/base/stringutils.h>
 
 namespace PLearn {
 
@@ -75,7 +79,6 @@
 static PObjectPool<PyMethodDef> pyfuncs(50);
 static TVec<string> funcs_help;
 
-
 void injectPLearnGlobalFunctions(PyObject* env)
 {
     const RemoteMethodMap::MethodMap& global_funcs= 
@@ -120,15 +123,272 @@
 }
 
 
+void injectPLearnClasses(PyObject* injenv)
+{
+    PythonGlobalInterpreterLock gil;         // For thread-safety
+    PythonObjectWrapper::initializePython();
+
+    PyObject* module= 0;
+    if(PyModule_Check(injenv))
+        module= injenv;
+
+    // import python class for wrapping PLearn objects
+    string importcode= "\nfrom plearn.pybridge.wrapped_plearn_object "
+        "import *\n";
+    PyObject* pyenv= PyDict_New();
+    PyDict_SetItemString(pyenv, "__builtins__", PyEval_GetBuiltins());
+    PyObject* res= PyRun_String(importcode.c_str(), Py_file_input, pyenv, pyenv);
+    if(!res)
+    {
+       if(PyErr_Occurred()) PyErr_Print();
+       PLERROR("in injectPLearnClasses : cannot import plearn.pybridge.wrapped_plearn_object.");
+    }
+
+    Py_DECREF(res);
+    if (PyErr_Occurred()) 
+    {
+        Py_DECREF(pyenv);
+        PyErr_Print();
+        PLERROR("in injectPLearnClasses : error compiling "
+                "WrappedPLearnObject python code.");
+    }
+
+    string wrapper_name= "WrappedPLearnObject";
+    // now find the class in the env.
+    typedef map<string, PyObject*> env_t;
+    env_t env= PythonObjectWrapper(
+        pyenv, PythonObjectWrapper::transfer_ownership).as<env_t>();
+    env_t::iterator clit= env.find(wrapper_name);
+    if(clit == env.end())
+        PLERROR("in injectPLearnClasses : "
+                "class %s not defined "
+                "in plearn.pybridge.wrapped_plearn_object",
+                wrapper_name.c_str());
+    PyObject* wrapper= clit->second;
+
+    //inject unref and newCPPObj methods
+    PyMethodDef* py_method= &PythonObjectWrapper::m_unref_method_def;
+    py_method->ml_name  = "_unref";
+    py_method->ml_meth  = PythonObjectWrapper::python_del;
+    py_method->ml_flags = METH_VARARGS;
+    py_method->ml_doc   = "Injected unref function from PythonObjectWrapper; "
+        "DO NOT USE THIS FUNCTION! IT MAY DEALLOCATE THE PLEARN OBJECT!";
+    PyObject* py_funcobj= PyCFunction_NewEx(py_method, NULL, NULL);
+    PyObject* py_methobj= PyMethod_New(py_funcobj, NULL, wrapper);
+    Py_XDECREF(py_funcobj);
+    if(!py_funcobj || !py_methobj) 
+    {
+        Py_DECREF(pyenv);
+        Py_XDECREF(py_methobj);
+        PLERROR("in injectPLearnClasses : "
+                "can't inject method '%s' (i.e. __del__)", 
+                py_method->ml_name);
+    }
+    PyObject_SetAttrString(wrapper, py_method->ml_name, py_methobj);
+    Py_DECREF(py_methobj);
+
+    // inject 'newCPPObj' and 'refCPPObj' class methods
+    TVec<PyMethodDef*> classmethods(2);
+    classmethods[0]= &PythonObjectWrapper::m_newCPPObj_method_def;
+    classmethods[0]->ml_name  = "_newCPPObj";
+    classmethods[0]->ml_meth  = PythonObjectWrapper::newCPPObj;
+    classmethods[0]->ml_flags = METH_VARARGS;
+    classmethods[0]->ml_doc   = "Injected new function from PythonObjectWrapper; "
+        "DO NOT USE THIS FUNCTION!";
+    classmethods[1]= &PythonObjectWrapper::m_refCPPObj_method_def;
+    classmethods[1]->ml_name  = "_refCPPObj";
+    classmethods[1]->ml_meth  = PythonObjectWrapper::refCPPObj;
+    classmethods[1]->ml_flags = METH_VARARGS;
+    classmethods[1]->ml_doc   = "Injected new function from PythonObjectWrapper; "
+        "DO NOT USE THIS FUNCTION!";
+
+    for(TVec<PyMethodDef*>::iterator mit= classmethods.begin();
+        mit != classmethods.end(); ++mit)
+    {
+        py_method= *mit;
+        py_funcobj= PyCFunction_NewEx(py_method, NULL, NULL);
+        py_methobj= PyMethod_New(py_funcobj, NULL, wrapper);
+        Py_XDECREF(py_funcobj);
+        if(!py_funcobj || !py_methobj) 
+        {
+            Py_DECREF(pyenv);
+            Py_XDECREF(py_methobj);
+            PLERROR("in injectPLearnClasses : "
+                    "can't inject method '%s' (i.e. C++'s new)", 
+                    py_method->ml_name);
+        }
+        PyObject_SetAttrString(wrapper, py_method->ml_name, py_methobj);
+        Py_DECREF(py_methobj);
+        
+        string classmethodname= wrapper_name+"."+py_method->ml_name;
+        res= PyRun_String((classmethodname
+                           +"= classmethod("+classmethodname+".im_func)").c_str(), 
+                          Py_file_input, pyenv, pyenv);
+        Py_DECREF(res);
+        if (PyErr_Occurred()) 
+        {
+            Py_DECREF(pyenv);
+            PyErr_Print();
+            PLERROR("in injectPLearnClasses : error making "
+                    "newCPPObj a classmethod.");
+        }
+    }
+
+    if(0 != PyType_Ready(reinterpret_cast<PyTypeObject*>(wrapper)))
+        PLERROR("in injectPLearnClasses : "
+                "failed PyType_Ready on wrapper class.");
+
+    if(!PyCallable_Check(wrapper))
+        PLERROR("in injectPLearnClasses : "
+                "%s is not callable [not a class?]",
+                wrapper_name.c_str());
+
+    PyObject* the_pyclass= 0;
+    const TypeMap& tp_map= TypeFactory::instance().getTypeMap();
+    for(TypeMap::const_iterator tit= tp_map.begin();
+        tit != tp_map.end(); ++tit)
+    {
+        if(!tit->second.constructor)
+            continue; //skip abstract classes
+
+        // create new python type deriving from WrappedPLearnObject
+        string classname= tit->first;
+        string pyclassname= classname;
+        search_replace(pyclassname, " ", "_");
+        search_replace(pyclassname, "<", "_");
+        search_replace(pyclassname, ">", "_");
+        string derivcode= string("\nclass ")
+            + pyclassname + "(" + wrapper_name + "):\n"
+            "  \"\"\" ... \"\"\"\n"
+            "  def __new__(cls,*args,**kwargs):\n"
+            "    #print '** "+pyclassname+".__new__',kwargs\n"
+            "    obj= object.__new__(cls,*args,**kwargs)\n"
+            "    if '_cptr' not in kwargs:\n"
+            "      obj._cptr= cls._newCPPObj('"+classname+"')\n"
+            "      cls._refCPPObj(obj)\n"
+            "    return obj\n";
+        PyObject* res= PyRun_String(derivcode.c_str(), 
+                                    Py_file_input, pyenv, pyenv);
+        Py_XDECREF(res);
+        env= PythonObjectWrapper(
+            pyenv, PythonObjectWrapper::transfer_ownership).as<env_t>();
+        clit= env.find(pyclassname);
+        if(clit == env.end())
+            PLERROR("in injectPLearnClasses : "
+                    "Cannot create new python class deriving from "
+                    "%s (%s).", 
+                    wrapper_name.c_str(),
+                    classname.c_str());
+
+        //set option names
+        OptionList& options= tit->second.getoptionlist_method();
+        unsigned int nopts= options.size();
+        TVec<string> optionnames(nopts);
+        for(unsigned int i= 0; i < nopts; ++i)
+            optionnames[i]= options[i]->optionname();
+
+        the_pyclass= clit->second;
+        if(-1==PyObject_SetAttrString(the_pyclass, "_optionnames", 
+                                      PythonObjectWrapper(optionnames).getPyObject()))
+        {
+            Py_DECREF(pyenv);
+            if (PyErr_Occurred()) PyErr_Print();
+            PLERROR("in injectPLearnClasses : "
+                    "cannot set attr _optionnames for class %s",
+                    classname.c_str());
+        }
+
+        // inject all declared methods
+        const RemoteMethodMap* methods= &tit->second.get_remote_methods();
+
+        PP<PObjectPool<PyMethodDef> > meth_def_pool= 
+            new PObjectPool<PyMethodDef>(methods->size()+1);
+
+        PythonObjectWrapper::m_pypl_classes.insert(
+            make_pair(classname, PLPyClass(the_pyclass, meth_def_pool)));
+        TVec<string>& methods_help= 
+            PythonObjectWrapper::m_pypl_classes.find(classname)->second.methods_help;
+
+        while(methods)
+        {
+            for(RemoteMethodMap::MethodMap::const_iterator it= methods->begin();
+                it != methods->end(); ++it)
+            {
+                //get the RemoteTrampoline
+                PyObject* tramp= PyCObject_FromVoidPtr(it->second, NULL);
+            
+                // Create a Python Function Object
+                PyMethodDef* py_method= meth_def_pool->allocate();
+                py_method->ml_name  = const_cast<char*>(it->first.first.c_str());
+                py_method->ml_meth  = PythonObjectWrapper::trampoline;
+                py_method->ml_flags = METH_VARARGS;
+                methods_help.push_back(HelpSystem::helpOnMethod(classname,
+                                                                it->first.first.c_str(),
+                                                                it->first.second));
+                py_method->ml_doc   = const_cast<char*>(methods_help.last().c_str());
+    
+                PyObject* py_funcobj= PyCFunction_NewEx(py_method, tramp, module);
+
+                // create an unbound method from the function
+                PyObject* py_methobj= PyMethod_New(py_funcobj, NULL, the_pyclass);
+
+                Py_DECREF(tramp);
+                Py_XDECREF(py_funcobj);
+                if(!py_funcobj || !py_methobj) 
+                {
+                    Py_DECREF(pyenv);
+                    Py_XDECREF(py_methobj);
+                    PLERROR("in injectPLearnClasses : "
+                            "can't inject method '%s'", py_method->ml_name);
+                }
+
+                if(-1==PyObject_SetAttrString(the_pyclass, py_method->ml_name, py_methobj))
+                {
+                    Py_DECREF(py_methobj);
+                    if (PyErr_Occurred()) PyErr_Print();
+                    PLERROR("in injectPLearnClasses : "
+                            "cannot set attr %s for class %s",
+                            py_method->ml_name,
+                            classname.c_str());
+                }
+                Py_DECREF(py_methobj);
+            }
+            methods= methods->inheritedMethods();//get parent class methods
+        }
+        if(0 != PyType_Ready(reinterpret_cast<PyTypeObject*>(the_pyclass)))
+            PLERROR("in injectPLearnClasses : "
+                    "failed PyType_Ready on class %s.",classname.c_str());
+        if(module)
+            if(-1==PyObject_SetAttrString(module, 
+                                          const_cast<char*>(pyclassname.c_str()), 
+                                          the_pyclass))
+            {
+                if (PyErr_Occurred()) PyErr_Print();
+                PLERROR("in injectPLearnClasses : "
+                        "cannot inject class %s.",
+                        classname.c_str());
+            }
+    }
+}
+
 // Init func for python module.
 // init module, then inject global funcs
 void initPythonExtensionModule(char* module_name)
 {
     PythonObjectWrapper::initializePython();
     PyObject* plext= Py_InitModule(module_name, NULL);
-    injectPLearnGlobalFunctions(plext);
+    setPythonModuleAndInject(plext);
 }
 
+void setPythonModuleAndInject(PyObject* module)
+{
+    injectPLearnGlobalFunctions(module);
+    injectPLearnClasses(module);
+    the_PLearn_python_module= module;
+}
+
+PyObject* the_PLearn_python_module= 0;
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/python/PythonExtension.h
===================================================================
--- trunk/plearn/python/PythonExtension.h	2007-07-30 21:02:23 UTC (rev 7871)
+++ trunk/plearn/python/PythonExtension.h	2007-07-30 22:20:10 UTC (rev 7872)
@@ -39,14 +39,22 @@
 namespace PLearn {
 
 // Trampoline for global PLearn 'remote' functions
-  PyObject* pythonGlobalFuncTramp(PyObject* self, PyObject* args);
+PyObject* pythonGlobalFuncTramp(PyObject* self, PyObject* args);
 
 // inject all PLearn global functions into a python env.
 void injectPLearnGlobalFunctions(PyObject* env);
 
+// inject all PLearn concrete classes
+void injectPLearnClasses(PyObject* env);
+
 // Init func for python module.
-  void initPythonExtensionModule(char* module_name);
+void initPythonExtensionModule(char* module_name);
 
+// inject funcs. and classes, set global module.
+void setPythonModuleAndInject(PyObject* module);
+
+extern PyObject* the_PLearn_python_module;
+
 } // end of namespace PLearn
 
 #endif

Modified: trunk/plearn/python/PythonObjectWrapper.cc
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.cc	2007-07-30 21:02:23 UTC (rev 7871)
+++ trunk/plearn/python/PythonObjectWrapper.cc	2007-07-30 22:20:10 UTC (rev 7872)
@@ -44,6 +44,7 @@
 // Must include Python first...
 #include "PythonObjectWrapper.h"
 #include "PythonEmbedder.h"
+#include "PythonExtension.h"
 
 // From C/C++ stdlib
 #include <stdio.h>
@@ -405,19 +406,43 @@
                 "deleting obj. for which no python class exists!");
     --clit->second.nref;
 
-    // TODO: avoid deleting class
-    if(0 == clit->second.nref)
-    {
-        m_pypl_classes.erase(classname);//cleanup
-    }
-
+    //perr << "delete " << (void*)obj << " : " << (void*)m_wrapped_objects[obj] << endl;
+    //perr << "bef.del o->usage()= " << obj->usage() << endl;
     obj->unref();//python no longer references this obj.
+    //perr << "aft.del o->usage()= " << obj->usage() << endl;
 
     m_wrapped_objects.erase(obj);
 
+    //printWrappedObjects();
+
     return newPyObject();//None
 }
 
+PyObject* PythonObjectWrapper::newCPPObj(PyObject* self, PyObject* args)
+{
+    TVec<PyObject*> args_tvec= 
+        PythonObjectWrapper(args).as<TVec<PyObject*> >();
+    Object* o= newObjectFromClassname(PyString_AsString(args_tvec[1]));
+    //perr << "new o->usage()= " << o->usage() << endl;
+    return PyCObject_FromVoidPtr(o, 0);
+}
+
+PyObject* PythonObjectWrapper::refCPPObj(PyObject* self, PyObject* args)
+{
+    TVec<PyObject*> args_tvec= 
+        PythonObjectWrapper(args).as<TVec<PyObject*> >();
+    PyObject* pyo= args_tvec[1];
+    Object* o= PythonObjectWrapper(pyo);
+    o->ref();
+    //perr << "ref o->usage()= " << o->usage() << endl;
+    PythonObjectWrapper::m_wrapped_objects[o]= pyo;
+    //perr << "refCPPObj: " << (void*)o << " : " << (void*)pyo << endl;
+
+    //printWrappedObjects();
+
+    return newPyObject();//None
+}
+
 //#####  newPyObject  #########################################################
 
 //! Return None (increments refcount)
@@ -435,16 +460,18 @@
 //init.
 bool PythonObjectWrapper::m_unref_injected= false;
 PyMethodDef PythonObjectWrapper::m_unref_method_def;
+PyMethodDef PythonObjectWrapper::m_newCPPObj_method_def;
+PyMethodDef PythonObjectWrapper::m_refCPPObj_method_def;
 
-
 PyObject* ConvertToPyObject<Object*>::newPyObject(const Object* x)
 {
+//     perr << "in ConvertToPyObject<Object*>::newPyObject : "
+//          << x->classname() << ' ' << (void*)x << endl;
+
     // void ptr becomes None
-    if(!x)
-        return PythonObjectWrapper::newPyObject();
+    if(!x) return PythonObjectWrapper::newPyObject();
 
     PythonGlobalInterpreterLock gil;         // For thread-safety
-
     static PythonEmbedder embedder;
     PythonObjectWrapper::initializePython();
 
@@ -454,215 +481,51 @@
     if(objit != PythonObjectWrapper::m_wrapped_objects.end())
     {
         PyObject* o= objit->second;
+
+//         perr << "NEW REF TO " << objit->first->classname() << ' ' << (void*)objit->first 
+//              << ' ' << objit->first->usage() << " : " 
+//              << (void*)objit->second << ' ' << objit->second->ob_refcnt << endl;
+
+//        printWrappedObjects();
         Py_INCREF(o);//new ref
         return o;//return ptr to already created pyobj
     }
-
-    // import python class for wrapping PLearn objects
-    string importcode= "\nfrom plearn.pybridge.wrapped_plearn_object "
-        "import *\n";
-    PyObject* pyenv= PyDict_New();
-    PyDict_SetItemString(pyenv, "__builtins__", PyEval_GetBuiltins());
-    PyObject* res= PyRun_String(importcode.c_str(), Py_file_input, pyenv,
-                                pyenv);
-    Py_DECREF(res);
-    if (PyErr_Occurred())
-    {
-        Py_DECREF(pyenv);
-        PyErr_Print();
-        PLERROR("in PythonObjectWrapper::newPyObject : error compiling "
-                "WrappedPLearnObject python code.");
-    }
-
-    string wrapper_name= "WrappedPLearnObject";
-    // now find the class in the env.
-    typedef map<string, PyObject*> env_t;
-    env_t env= PythonObjectWrapper(
-        pyenv, PythonObjectWrapper::transfer_ownership).as<env_t>();
-    env_t::iterator clit= env.find(wrapper_name);
-    if(clit == env.end())
-        PLERROR("in PythonObjectWrapper::newPyObject : "
-                "class %s not defined "
-                "in plearn.pybridge.wrapped_plearn_object",
-                wrapper_name.c_str());
-    PyObject* wrapper= clit->second;
-
-    //inject unref method if not already done
-    // N.B. always injected into WrappedPLearnObject
-    // even when using WrappedPLearnVMat
-    if(!PythonObjectWrapper::m_unref_injected)
-    {
-        PyMethodDef* py_method= &PythonObjectWrapper::m_unref_method_def;
-        py_method->ml_name  = "unref";
-        py_method->ml_meth  = PythonObjectWrapper::python_del;
-        py_method->ml_flags = METH_VARARGS;
-        py_method->ml_doc   = "Injected unref function from PythonObjectWrapper; "
-            "DO NOT USE THIS FUNCTION! IT MAY DEALLOCATE THE PLEARN OBJECT!";
-
-        PyObject* py_funcobj= PyCFunction_NewEx(py_method, NULL, NULL);
-        PyObject* py_methobj= PyMethod_New(py_funcobj, NULL, wrapper);
-        Py_XDECREF(py_funcobj);
-        if(!py_funcobj || !py_methobj)
-        {
-            Py_DECREF(pyenv);
-            Py_XDECREF(py_methobj);
-            PLERROR("in PythonObjectWrapper::newPyObject : "
-                    "can't inject method '%s' (i.e. __del__)",
-                    py_method->ml_name);
-        }
-        PyObject_SetAttrString(wrapper, py_method->ml_name, py_methobj);
-        Py_DECREF(py_methobj);
-        PythonObjectWrapper::m_unref_injected= true;
-    }
-
-    //get wrapped for VMats if needed
-    if(dynamic_cast<const VMatrix*>(x))
-    {
-        wrapper_name= "WrappedPLearnVMat";
-        clit= env.find(wrapper_name);
-        if(clit == env.end())
-            PLERROR("in PythonObjectWrapper::newPyObject : "
-                    "class %s not defined "
-                    "in plearn.pybridge.wrapped_plearn_object",
-                    wrapper_name.c_str());
-        wrapper= clit->second; // the actual wrapper class
-    }
-
-    if(!PyCallable_Check(wrapper))
-        PLERROR("in PythonObjectWrapper::newPyObject : "
-                "%s is not callable [not a class?]",
-                wrapper_name.c_str());
-
     // get ptr of object to wrap
     PyObject* plobj= PyCObject_FromVoidPtr(const_cast<Object*>(x), NULL);
 
     // try to find existing python class
     string classname= x->classname();
-    PythonObjectWrapper::pypl_classes_t::iterator clit2=
+    PythonObjectWrapper::pypl_classes_t::iterator clit= 
         PythonObjectWrapper::m_pypl_classes.find(classname);
-    PyObject* the_pyclass= 0;
-    if(clit2 == PythonObjectWrapper::m_pypl_classes.end())
-    {
-        // create new python type deriving from WrappedPLearnObject
-        string derivcode= string("\nclass ")
-            + classname + "(" + wrapper_name + "):\n"
-            "\tpass\n\n";
+    if(clit == PythonObjectWrapper::m_pypl_classes.end())
+        PLERROR("in ConvertToPyObject<Object*>::newPyObject : "
+                "cannot find python class %s",classname.c_str());
+    PyObject* the_pyclass= clit->second.pyclass;
 
-        PyObject* res2= PyRun_String(derivcode.c_str(),
-                                     Py_file_input, pyenv, pyenv);
-        Py_XDECREF(res2);
-        env= PythonObjectWrapper(
-            pyenv, PythonObjectWrapper::transfer_ownership).as<env_t>();
-        clit= env.find(classname);
-        if(clit == env.end())
-            PLERROR("in PythonObjectWrapper::newPyObject : "
-                    "Cannot create new python class deriving from "
-                    "%s (%s).",
-                    wrapper_name.c_str(),
-                    classname.c_str());
-
-        //set option names
-        OptionList& options= x->getOptionList();
-        unsigned int nopts= options.size();
-        TVec<string> optionnames(nopts);
-        for(unsigned int i= 0; i < nopts; ++i)
-            optionnames[i]= options[i]->optionname();
-
-        the_pyclass= clit->second;
-        if(-1==PyObject_SetAttrString(the_pyclass, "_optionnames",
-                                      PythonObjectWrapper(optionnames).getPyObject()))
-        {
-            Py_DECREF(pyenv);
-            if (PyErr_Occurred())
-                PyErr_Print();
-            PLERROR("cannot set attr _optionnames");
-        }
-
-        // inject all declared methods
-        const RemoteMethodMap* methods= &x->getRemoteMethodMap();
-
-        PP<PObjectPool<PyMethodDef> > meth_def_pool=
-            new PObjectPool<PyMethodDef>(methods->size()+1);
-
-        PythonObjectWrapper::m_pypl_classes.insert(
-            make_pair(classname, PLPyClass(the_pyclass, meth_def_pool)));
-        TVec<string>& methods_help=
-            PythonObjectWrapper::m_pypl_classes.find(classname)->second.methods_help;
-
-        while(methods)
-        {
-            for(RemoteMethodMap::MethodMap::const_iterator it= methods->begin();
-                it != methods->end(); ++it)
-            {
-                //get the RemoteTrampoline
-                PyObject* tramp= PyCObject_FromVoidPtr(it->second, NULL);
-
-                // Create a Python Function Object
-                PyMethodDef* py_method= meth_def_pool->allocate();
-                py_method->ml_name  = const_cast<char*>(it->first.first.c_str());
-                py_method->ml_meth  = PythonObjectWrapper::trampoline;
-                py_method->ml_flags = METH_VARARGS;
-                methods_help.push_back(HelpSystem::helpOnMethod(classname,
-                                                                it->first.first.c_str(),
-                                                                it->first.second));
-                py_method->ml_doc   = const_cast<char*>(methods_help.last().c_str());
-
-                PyObject* py_funcobj= PyCFunction_NewEx(py_method, tramp, NULL);
-
-                // create an unbound method from the function
-                PyObject* py_methobj= PyMethod_New(py_funcobj, NULL, the_pyclass);
-
-                Py_DECREF(tramp);
-                Py_XDECREF(py_funcobj);
-                if(!py_funcobj || !py_methobj)
-                {
-                    Py_DECREF(pyenv);
-                    Py_DECREF(plobj);
-                    Py_XDECREF(py_methobj);
-                    PLERROR("in PythonObjectWrapper::newPyObject : "
-                            "can't inject method '%s'", py_method->ml_name);
-                }
-
-                PyObject_SetAttrString(the_pyclass, py_method->ml_name,
-                                       py_methobj);
-                Py_DECREF(py_methobj);
-            }
-            methods= methods->inheritedMethods();//get parent class methods
-        }
-    }
-    else
-    {
-        the_pyclass= clit2->second.pyclass;
-        ++clit2->second.nref;
-    }
-
     //create the python object itself from the_pyclass
-    PyObject* args= PyTuple_New(1);
-    Py_INCREF(plobj);//keep it after it is 'stolen'
-    PyTuple_SetItem(args, 0, plobj);
-
+    PyObject* args= PyTuple_New(0);
     PyObject* params= PyDict_New();
+    PyDict_SetItemString(params, "_cptr", plobj);
+    Py_DECREF(plobj);
     PyObject* the_obj= PyObject_Call(the_pyclass, args, params);
     Py_DECREF(args);
     Py_DECREF(params);
     if(!the_obj)
     {
-        Py_DECREF(pyenv);
-        Py_DECREF(plobj);
         if (PyErr_Occurred()) PyErr_Print();
         PLERROR("in PythonObjectWrapper::newPyObject : "
                 "can't construct a WrappedPLearnObject.");
     }
 
-    //finalize
-    Py_DECREF(pyenv);
-    Py_DECREF(plobj);
-
     // augment refcount since python now 'points' to this obj.
     x->ref();
 
     PythonObjectWrapper::m_wrapped_objects[x]= the_obj;
 
+//    perr << "newPyObject: " << (void*)x << " : " << (void*)the_obj << endl;
+
+//    printWrappedObjects();
+
     return the_obj;
 }
 
@@ -775,6 +638,25 @@
     return in;
 }
 
+//! debug
+void printWrappedObjects()
+{
+    perr << "wrapped_objects= " << endl;
+    for(PythonObjectWrapper::wrapped_objects_t::iterator it= 
+            PythonObjectWrapper::m_wrapped_objects.begin();
+        it != PythonObjectWrapper::m_wrapped_objects.end(); ++it)
+        perr << '\t' << it->first->classname() << ' ' << (void*)it->first 
+             << ' ' << it->first->usage() << " : " 
+             << (void*)it->second << ' ' << it->second->ob_refcnt << endl;
+}
+
+BEGIN_DECLARE_REMOTE_FUNCTIONS
+    declareFunction("printWrappedObjects", &printWrappedObjects,
+                    (BodyDoc("Prints PLearn objects wrapped into python.\n")));
+END_DECLARE_REMOTE_FUNCTIONS
+
+
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2007-07-30 21:02:23 UTC (rev 7871)
+++ trunk/plearn/python/PythonObjectWrapper.h	2007-07-30 22:20:10 UTC (rev 7872)
@@ -823,7 +823,10 @@
 
     static PyObject* python_del(PyObject* self, PyObject* args);
 
+    static PyObject* newCPPObj(PyObject* self, PyObject* args);
 
+    static PyObject* refCPPObj(PyObject* self, PyObject* args);
+
     //#####  Low-Level PyObject Creation  #####################################
 
     /**
@@ -839,23 +842,24 @@
      */
     static void initializePython();
 
-protected:
-    //! Whether we own the PyObject or not
-    OwnershipMode m_ownership;
-    PyObject* m_object;
-
     //for the unique unref injected method
     static bool m_unref_injected;
     static PyMethodDef m_unref_method_def;
-
+    static PyMethodDef m_newCPPObj_method_def;
+    static PyMethodDef m_refCPPObj_method_def;
     typedef map<const string, PLPyClass> pypl_classes_t;
     static pypl_classes_t m_pypl_classes;
 
+protected:
+    OwnershipMode m_ownership;               //!< Whether we own the PyObject or not
+    PyObject* m_object;
+    
     typedef map<const Object*, PyObject*> wrapped_objects_t;
 
     static wrapped_objects_t m_wrapped_objects; //!< for wrapped PLearn Objects
 
     template<class T> friend class ConvertToPyObject;
+    friend void printWrappedObjects();
 };
 
 // Specialization for General T*.  Attempt to cast into Object*.  If that works
@@ -1277,6 +1281,9 @@
 PStream& operator>>(PStream& in, PythonObjectWrapper& v);
 DECLARE_TYPE_TRAITS(PythonObjectWrapper);
 
+//! for debug purposes
+void printWrappedObjects();
+
 } // end of namespace PLearn
 
 #endif

Modified: trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
===================================================================
--- trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2007-07-30 21:02:23 UTC (rev 7871)
+++ trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2007-07-30 22:20:10 UTC (rev 7872)
@@ -1,3 +1,5 @@
+## Automatically adapted for numpy.numarray Jun 13, 2007 by python_numarray_to_numpy (-xsm)
+
 #
 # Copyright (C) 2007 Xavier Saint-Mleux, ApSTAT Technologies inc.
 #
@@ -31,8 +33,26 @@
 
 class WrappedPLearnObject(object):
 
-    def __init__(self, cptr):
-        self._cptr= cptr # ptr to c++ obj
+    def __init__(self, **kwargs):
+        #print 'WrappedPLearnObject.__init__',type(self),kwargs
+        if '_cptr' in kwargs:
+            self._cptr= kwargs['_cptr'] # ptr to c++ obj
+        elif hasattr(self,'_cptr'):
+            #print 'init->SETOPTIONS2',kwargs
+            self.setOptions(kwargs)
+            
+    def setOptions(self, kwargs):
+        #print 'SETOPTIONS',kwargs
+        call_build= True
+        for k in kwargs:
+            if k=='__call_build':
+                call_build= kwargs[k]
+            elif k=='_cptr':
+                pass
+            else:
+                self.__setattr__(k, kwargs[k])
+        if call_build:
+            self.build()
     
     def __setattr__(self, attr, val):
         if attr != '_optionnames' and attr in self._optionnames:
@@ -49,11 +69,16 @@
                                    + repr(self))
         
     def __del__(self):
-        return self.unref()
+        #from plearn import pyext
+        #print 'del',type(self),self._cptr
+        #pyext.printWrappedObjects()
+        self._unref()
+        #pyext.printWrappedObjects()
 
     def __repr__(self):
         return self.asString() #PLearn repr. for now
 
+#from numpy.numarray import *
 from numarray import *
 
 class WrappedPLearnVMat(WrappedPLearnObject):

Modified: trunk/python_modules/plearn/pyext/__init__.py
===================================================================
--- trunk/python_modules/plearn/pyext/__init__.py	2007-07-30 21:02:23 UTC (rev 7871)
+++ trunk/python_modules/plearn/pyext/__init__.py	2007-07-30 22:20:10 UTC (rev 7872)
@@ -31,6 +31,7 @@
 #  library, go to the PLearn Web site at www.plearn.org
 
 from plearn.pyext.plext import *
+from plearn.pyext import plext as pl
 
 from plearn.pyplearn.plargs import *
 import cgitb
@@ -38,22 +39,6 @@
 
 print versionString()
 
-class pl:
-    class __metaclass__(type):
-        def __getattr__(cls, name):
-            def newObj(**kwargs):
-                call_build= True
-                obj= newObjectFromClassname(name)
-                for k in kwargs.keys():
-                    if k=='__call_build':
-                        call_build= kwargs[k]
-                    else:
-                        obj.__setattr__(k, kwargs[k])
-                if call_build:
-                    obj.build()
-                return obj
-            return newObj
-
 # Redefines function TMat to emulate pyplearn behaviour
 def TMat( *args ):
     """Returns a list of lists, each inner list being a row of the TMat"""



From yoshua at mail.berlios.de  Tue Jul 31 00:57:06 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Tue, 31 Jul 2007 00:57:06 +0200
Subject: [Plearn-commits] r7873 - trunk/plearn_learners/online
Message-ID: <200707302257.l6UMv6FQ024295@sheep.berlios.de>

Author: yoshua
Date: 2007-07-31 00:57:05 +0200 (Tue, 31 Jul 2007)
New Revision: 7873

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
Added new ports to RBMModule to compute P(x)=sum_h P(x|h) Q(h),
given table of Q's, values of h's, and values of x's.


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-07-30 22:20:10 UTC (rev 7872)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-07-30 22:57:05 UTC (rev 7873)
@@ -32,7 +32,7 @@
 // This file is part of the PLearn library. For more information on the PLearn
 // library, go to the PLearn Web site at www.plearn.org
 
-// Authors: Olivier Delalleau
+// Authors: Olivier Delalleau, Yoshua Bengio
 
 /*! \file RBMModule.cc */
 
@@ -288,6 +288,13 @@
     addPortName("hidden_bias");
     addPortName("weights");
     addPortName("neg_log_likelihood");
+    // a column matrix with one element -log P(h) for each row h of "hidden", 
+    // used as an input port, with neg_log_pvisible_given_phidden as output
+    addPortName("neg_log_phidden"); 
+    // compute column matrix with one entry -log P(x) = -log( sum_h P(x|h) P(h) ) for
+    // each row x of "visible", and where {P(h)}_h is provided 
+    // in "neg_log_phidden" for the set of h's in "hidden".
+    addPortName("neg_log_pvisible_given_phidden"); 
     if(reconstruction_connection)
     {
         addPortName("visible_reconstruction.state");
@@ -320,6 +327,8 @@
     }
     port_sizes(getPortIndex("energy"),1) = 1;
     port_sizes(getPortIndex("neg_log_likelihood"),1) = 1;
+    port_sizes(getPortIndex("neg_log_phidden"),1) = 1;
+    port_sizes(getPortIndex("neg_log_pvisible_given_phidden"),1) = 1;
     if(reconstruction_connection)
     {
         if (visible_layer) {
@@ -674,6 +683,8 @@
     Mat* hidden_sample = ports_value[getPortIndex("hidden_sample")];
     Mat* energy = ports_value[getPortIndex("energy")];
     Mat* neg_log_likelihood = ports_value[getPortIndex("neg_log_likelihood")];
+    Mat* neg_log_phidden = ports_value[getPortIndex("neg_log_phidden")];
+    Mat* neg_log_pvisible_given_phidden = ports_value[getPortIndex("neg_log_pvisible_given_phidden")];
     hidden_bias = ports_value[getPortIndex("hidden_bias")];
     weights = ports_value[getPortIndex("weights")];
     Mat* visible_reconstruction = 0;
@@ -746,7 +757,7 @@
         }
         found_a_valid_configuration = true;
     }
-    // COMPUTE NLL
+    // COMPUTE UNSUPERVISED NLL
     if (neg_log_likelihood && neg_log_likelihood->isEmpty() && compute_log_likelihood)
     {
         if (partition_function_is_stale && !during_training)
@@ -903,6 +914,49 @@
         found_a_valid_configuration = true;
     }
 
+    // Compute column matrix with one entry -log P(x) = -log( sum_h P(x|h) P(h) ) for
+    // each row x of "visible", and where {P(h)}_h is provided 
+    // in "neg_log_phidden" for the set of h's in "hidden".
+    // neg_log_phidden is an optional column matrix with one element -log P(h) for each 
+    // row h of "hidden", used as an input port, with neg_log_pvisible_given_phidden as output. 
+    // If neg_log_phidden is provided, it is assumed to be 1/n_h (n_h=h->length()).
+    if (neg_log_pvisible_given_phidden && neg_log_pvisible_given_phidden->isEmpty() &&
+        hidden && !hidden->isEmpty() && visible && !visible->isEmpty())
+    {
+        // estimate P(x) by sum_h P(x|h) P(h) where P(h) is either constant or provided by neg_log_phidden
+        if (neg_log_phidden)
+        {
+            PLASSERT_MSG(!neg_log_phidden->isEmpty(),"If neg_log_phidden is provided, it must be an input");
+            PLASSERT_MSG(neg_log_phidden->length()==hidden->length(),
+                     "If neg_log_phidden is provided, it must have the same length as hidden.state");
+            PLASSERT_MSG(neg_log_phidden->width()==1,"neg_log_phidden must have width 1 (single column)");
+        }
+        computeVisibleActivations(*hidden,true);
+        int n_h = hidden->length();
+        int T = visible->length();
+        real default_neg_log_ph = safelog(real(n_h)); // default P(h)=1/Nh: -log(1/Nh) = log(Nh)
+        Vec old_act = visible_layer->activation;
+        neg_log_pvisible_given_phidden->resize(T,1);
+        for (int t=0;t<T;t++)
+        {
+            Vec x_t = (*visible)(t);
+            real log_p_xt=0;
+            for (int i=0;i<n_h;i++)
+            {
+                visible_layer->activation = visible_layer->activations(i);
+                real neg_log_p_xt_given_hi = visible_layer->fpropNLL(x_t);
+                real neg_log_p_hi = neg_log_phidden?(*neg_log_phidden)(i,0):default_neg_log_ph;
+                if (i==0)
+                    log_p_xt = -(neg_log_p_xt_given_hi + neg_log_p_hi);
+                else
+                    log_p_xt = logadd(log_p_xt, -(neg_log_p_xt_given_hi + neg_log_p_hi));
+            }
+            (*neg_log_pvisible_given_phidden)(t,0) = -log_p_xt;
+        }
+        visible_layer->activation = old_act;
+        found_a_valid_configuration = true;
+    }
+
     // SAMPLING
     if ((visible_sample && visible_sample->isEmpty())               // is asked to sample visible units (discrete)
         || (visible_expectation && visible_expectation->isEmpty())  //              "                   (continous)



From manzagop at mail.berlios.de  Tue Jul 31 02:18:22 2007
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Tue, 31 Jul 2007 02:18:22 +0200
Subject: [Plearn-commits] r7874 - trunk/plearn_learners/generic
Message-ID: <200707310018.l6V0IMUC011713@sheep.berlios.de>

Author: manzagop
Date: 2007-07-31 02:18:20 +0200 (Tue, 31 Jul 2007)
New Revision: 7874

Modified:
   trunk/plearn_learners/generic/NatGradEstimator.cc
   trunk/plearn_learners/generic/NatGradEstimator.h
Log:
Added code to have a basic adaptive lambda based on the eigen decomposition.

Modified: trunk/plearn_learners/generic/NatGradEstimator.cc
===================================================================
--- trunk/plearn_learners/generic/NatGradEstimator.cc	2007-07-30 22:57:05 UTC (rev 7873)
+++ trunk/plearn_learners/generic/NatGradEstimator.cc	2007-07-31 00:18:20 UTC (rev 7874)
@@ -70,12 +70,15 @@
 NatGradEstimator::NatGradEstimator()
     /* ### Initialize all fields to their default value */
     : cov_minibatch_size(10),
-      lambda(1),
+      init_lambda(1.),
       n_eigen(10),
       gamma(0.99),
       renormalize(true),
+      amari_version(false),
+      update_lambda_from_eigen(false),
       previous_t(-1),
-      first_t(-1)
+      first_t(-1),
+      lambda(1.)
 {
     build();
 }
@@ -119,13 +122,13 @@
                   "to operator() before re-estimating the principal\n"
                   "eigenvectors/values. Note that each such re-computation will\n"
                   "cost O(n_eigen * n)");
-    declareOption(ol, "lambda", &NatGradEstimator::lambda,
+    declareOption(ol, "init_lambda", &NatGradEstimator::init_lambda,
                   OptionBase::buildoption,
                   "Initial variance. The first covariance is assumed to be\n"
-                  "lambda times the identity. Default = 1.\n");
-    declareOption(ol, "regularizer", &NatGradEstimator::lambda,
+                  "init_lambda times the identity. Default = 1.\n");
+    declareOption(ol, "regularizer", &NatGradEstimator::init_lambda,
                   OptionBase::buildoption,
-                  "Proxy for option lambda (different name to avoid python problems).\n");
+                  "Proxy for option init_lambda (different name to avoid python problems).\n");
     declareOption(ol, "n_eigen", &NatGradEstimator::n_eigen,
                   OptionBase::buildoption,
                   "Number of principal eigenvectors of the covariance matrix\n"
@@ -137,6 +140,10 @@
                   OptionBase::buildoption,
                   "Instead of our tricks, use the formula Ginv <-- (1+eps) Ginv - eps Ginv g g' Ginv\n"
                   "to estimate the inverse of the covariance matrix, and multiply it with g at each step.\n");
+    declareOption(ol, "update_lambda_from_eigen", &NatGradEstimator::update_lambda_from_eigen,
+                  OptionBase::buildoption,
+                  "Following an eigendecomposition, set lambda to the (n_eigen+1)th eigenvalue\n");
+
     declareOption(ol, "verbosity", &NatGradEstimator::verbosity,
                   OptionBase::buildoption,
                   "Verbosity level\n");
@@ -182,12 +189,15 @@
         Xt.resize(n_eigen+cov_minibatch_size, n_dim);
         Xt.clear();
         r.resize(n_eigen);
+        lambda = init_lambda;
         for (int j=0;j<n_eigen;j++)
             G(j,j) = lambda;
         first_t=-1;
         previous_t=-1;
     }
 }
+// TODO replace the calls to pow by something else. It's notoriously
+// inefficient.
 void NatGradEstimator::operator()(int t, const Vec& g, Vec v)
 {
     if (previous_t>=0)
@@ -229,21 +239,17 @@
         cout << "solution r = " << r << endl;
     // solution is in r
     transposeProduct(v, Xt, r);
-    if (renormalize)
+
+    // Multiply v by C's normalizer.
+    if (renormalize) 
         v*=(1 - pow(gamma,real(t+1)))/(1 - gamma);
-        //v/=(1 - pow(gamma,real(t+1)))/(1 - gamma);
-/*    {
-        real gnorm = dot(g,g);
-        real vnorm = dot(v,v);
-        g*=sqrt(vnorm/gnorm);
-    }
-*/
-    if (verbosity>0 && i%(cov_minibatch_size/2)==0)
+
+    if (verbosity>0 && i%(cov_minibatch_size)==0)
     {
-        real gnorm = dot(g,g);
-        real vnorm = dot(v,v);
-        real angle = acos(dot(v,g)/sqrt(gnorm*vnorm))*360/(2*3.14159);
-        cout << "angle(g,v)="<<angle<<", norm ratio="<<vnorm/gnorm<<endl;
+        real gnorm = sqrt(dot(g,g));
+        real vnorm = sqrt(dot(v,v));
+        real angle = acos(dot(v,g)/(gnorm*vnorm))*360/(2*3.14159);
+        cout << "angle(g,v)="<<angle<<", gnorm="<<gnorm<<", vnorm="<<vnorm<<", norm ratio="<<vnorm/gnorm<<endl;
     }
 
     // recompute the eigen-decomposition
@@ -253,18 +259,28 @@
         //if (save_G)
         //    saveAscii("G.amat",G);
         eigenVecOfSymmMat(G,n_eigen+1,D,Vt);
-        
-        // convert eigenvectors Vt of G into eigenvectors U of C
+        // Get all eigenvalues -> this resizes D and Vt, but it doesn't matter
+//        eigenVecOfSymmMat(G,G.width(),D,Vt);
+//        cout << "-= " << t << " =-" << endl;
+//        cout << D.length() << " eigenvalues = " << D << endl;
+
+        if( D.length() < n_eigen )
+            PLERROR("GOT LESS EIGENVECTORS THAN n_eigen.");
+
+        // convert eigenvectors Vt of G into *unnormalized* eigenvectors U of C
         product(Ut,Vkt,Xt);
         Ut *= 1.0/rn;
         D *= 1.0/rn2;
-        for (int j=0;j<n_eigen;j++) 
+        for (int j=0;j<n_eigen;j++) {
             if (D[j]<1e-10)
                 PLWARNING("NatGradEstimator: very small eigenvalue %d = %g\n",j,D[j]);
+//            if (D[j]<lambda)
+//                cout << " *** Small D[j] *** -> " << D[j] << endl;
+        }
         if (verbosity>0) // verifier Ut U = D/
         {
             static Mat Dmat;
-            cout << "eigenvalues = " << D << endl;
+            cout << D.length() << " eigenvalues = " << D << endl;
             if (verbosity>2)
             {
                 Dmat.resize(n_eigen,n_eigen);
@@ -281,6 +297,28 @@
         G.clear();
         for (int j=0;j<n_eigen;j++)
             G(j,j) = D[j];
+
+        // Update lambda in a yet to be determined smart way
+        if( update_lambda_from_eigen )    {
+//            if (D[n_eigen-1]>lambda)
+//                cout << " *** Last lambda too small? *** lambda, last eigen : " << lambda << ", " << D[n_eigen-1] << endl;
+/*            float big_eig = D[0];
+            bool cont = true;
+            for (int j=0;j<n_eigen && cont;j++) {
+                if( D[j]< (0.1*big_eig) ) {
+                    lambda = D[j];
+                    cont = false;
+                }
+            }
+            if(cont)*/
+                lambda = D[n_eigen-1];
+//          if (D[n_eigen]<1e-6)
+//              PLWARNING("NatGradEstimator: updating lambda with small value %g\n",D[n_eigen]);
+//          lambda = D[n_eigen-1];
+
+//            if(lambda<0.01)
+//                lambda = 0.01;
+        }
     }
     previous_t = t;
 }

Modified: trunk/plearn_learners/generic/NatGradEstimator.h
===================================================================
--- trunk/plearn_learners/generic/NatGradEstimator.h	2007-07-30 22:57:05 UTC (rev 7873)
+++ trunk/plearn_learners/generic/NatGradEstimator.h	2007-07-31 00:18:20 UTC (rev 7874)
@@ -66,7 +66,7 @@
     int cov_minibatch_size;
 
     //! regularization coefficient of covariance matrix (initial values on diagonal)
-    real lambda;
+    real init_lambda;
 
     //! number of eigenvectors-eigenvalues that is preserved of the covariance matrix
     int n_eigen;
@@ -80,6 +80,10 @@
     //! to estimate the inverse of the covariance matrix
     bool amari_version;
 
+    //! Following an eigendecomposition, set lambda to the (n_eigen+1)th
+    //! eigenvalue.
+    bool update_lambda_from_eigen;
+
 public:
     //#####  Public Member Functions  #########################################
 
@@ -148,7 +152,7 @@
     //#####  Private Data Members  ############################################
 
     // The rest of the private stuff goes here
-
+    real lambda;
     Vec r; //! rhs of linear system (G + gamma^{-i} lambda I) a = r
     Mat Vt;
     Mat Vkt; //! sub-matrix of Vt with first n_eigen eigen-vectors



From lysiane at mail.berlios.de  Tue Jul 31 05:48:47 2007
From: lysiane at mail.berlios.de (lysiane at BerliOS)
Date: Tue, 31 Jul 2007 05:48:47 +0200
Subject: [Plearn-commits] r7875 -
	trunk/plearn_learners/distributions/EXPERIMENTAL
Message-ID: <200707310348.l6V3mlWn022083@sheep.berlios.de>

Author: lysiane
Date: 2007-07-31 05:48:46 +0200 (Tue, 31 Jul 2007)
New Revision: 7875

Modified:
   trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc
   trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h
Log:
30 aout 2007, 
modification de log_density


Modified: trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc
===================================================================
--- trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc	2007-07-31 00:18:20 UTC (rev 7874)
+++ trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc	2007-07-31 03:48:46 UTC (rev 7875)
@@ -1,5 +1,6 @@
     // -*- C++ -*-
 
+
 // TransformationLearner.cc
 //
 // Copyright (C) 2007 Lysiane Bouchard
@@ -81,9 +82,12 @@
     nbTransforms(2),
     nbNeighbors(2)
 {
-    pout << "hello\n";
+
+    pout << "constructor called" <<endl;
+
 }
 
+
 ////////////////////
 // declareOptions //
 ////////////////////
@@ -105,7 +109,7 @@
 
 
     //buildoption
-    pout << "declare options\n";
+  
 
     declareOption(ol,
                   "behavior",
@@ -217,17 +221,17 @@
                  &TransformationLearner::transformsOffset,
                  OptionBase::buildoption,
                  "time of the first update of the transformations matrices");
+
    declareOption(ol,
                  "biasPeriod",
                  &TransformationLearner::biasPeriod,
                  OptionBase::buildoption,
-                 "time interval between 2 updates of the transformations bias (if any)");
-
+                 "time interval between two updates of the transformations bias");
    declareOption(ol,
                  "biasOffset",
                  &TransformationLearner::biasOffset,
                  OptionBase::buildoption,
-                 "time of the first update of the transformations bias (if any)");
+                 "time of the first update of the transformations bias");
 
    declareOption(ol, 
                  "noiseVariance",
@@ -257,6 +261,11 @@
    
    //learntoption
    declareOption(ol,
+                 "train_set",
+                 &TransformationLearner::train_set,
+                 OptionBase::learntoption,
+                 "We remember the training set, as this is a memory-based distribution." );
+   declareOption(ol,
                  "transformsSet",
                  &TransformationLearner::transformsSet,
                  OptionBase::learntoption,
@@ -281,60 +290,13 @@
                  &TransformationLearner::inputSpaceDim,
                  OptionBase::learntoption,
                  "dimensionality of the input space");
+   
    declareOption(ol,
-                 "nbTargetReconstructions",
-                 &TransformationLearner::nbTargetReconstructions,
+                 "reconstructionSet",
+                 &TransformationLearner::reconstructionSet,
                  OptionBase::learntoption,
-                 "number of reconstructions of the same target");
-   declareOption(ol,
-                 "nbReconstructions",
-                 &TransformationLearner::nbReconstructions,
-                 OptionBase::learntoption,
-                 "total number of reconstructions");
-   declareOption(ol,
-                 "trainingSetLength",
-                 &TransformationLearner::trainingSetLength,
-                 OptionBase::learntoption,
-                 "number of samples in the training" );
-   declareOption(ol,
-                 "transformsSD",
-                 &TransformationLearner::transformsSD,
-                 OptionBase::learntoption,
-                 "standard deviation of the transformations parameters");
-   declareOption(ol,
-                 "targetReconstructionSet",
-                 &TransformationLearner::targetReconstructionSet,
-                 OptionBase::learntoption,
-                 "will be used to store a view on the reconstructions of a same target");
-   declareOption(ol,
-                 "B_C",
-                 &TransformationLearner::B_C,
-                 OptionBase::learntoption,
-                 "storage space needed in the maximization step (to update transformations parameters)\n"
-                 " - 2mdXd matrix, m=number of transformations, d = dimensionality of the input space");
-   declareOption(ol,
-                 "B",
-                 &TransformationLearner::B,
-                 OptionBase::learntoption,
-                 "views on m first dxd submatrices of B_C \n"
-                 "(vector form)");
-   declareOption(ol,
-                 "C",
-                 &TransformationLearner::C,
-                 OptionBase::learntoption,
-                 "views on m last dxd submatrices of B_C \n"
-                 "(vector form)");
-   declareOption(ol,
-                 "target",
-                 &TransformationLearner::target,
-                 OptionBase::learntoption,
-                 "to store a view on a training sample");
-   declareOption(ol,
-                 "neighbor",
-                 &TransformationLearner::neighbor,
-                 OptionBase::learntoption,
-                 "to store a view on a training sample");
-
+                 "set of weighted reconstruction candidates");
+ 
    // Now call the parent class' declareOptions().
    inherited::declareOptions(ol);
 }
@@ -342,7 +304,7 @@
 void TransformationLearner::declareMethods(RemoteMethodMap& rmm){
 
 
-    pout << "declare methods\n";
+
     rmm.inherited(inherited::_getRemoteMethodMap_());
     
     declareMethod(rmm, 
@@ -416,6 +378,7 @@
                    ArgDoc("Vec root","data point from which all the other data points will derive (directly or indirectly)"),
                    ArgDoc("int deepness","deepness of the tree reprenting the samples created"),
                    ArgDoc("int branchingFactor","branching factor of the tree representing the samples created"),
+                   ArgDoc("int transformIdx", "index of the transformation to use (optional)"),
                    RetDoc("Mat (one row = one sample)")));
     declareMethod(rmm,
                   "returnSequenceDataSet",
@@ -424,6 +387,7 @@
                            "see 'sequenceDataSet()' implantation for more details"),
                    ArgDoc("const Vec start","data point from which all the other data points will derice (directly or indirectly)"),
                    ArgDoc("int n","number of sample data points to generate"),
+                   ArgDoc("int transformIdx","index of the transformation to use (optional)"),
                    RetDoc("nXd matrix (one row = one sample)")));
     declareMethod(rmm,
                   "returnTrainingPoint",
@@ -463,11 +427,8 @@
                   (BodyDoc("returns the parameters of each transformation"),
                    RetDoc("mdXd matrix, m = number of transformations \n"
                           "             d = dimensionality of the input space")));
+    
     declareMethod(rmm,
-                  "trainBuild",
-                  &TransformationLearner::trainBuild,
-                  (BodyDoc("training specific initialization operations")));
-    declareMethod(rmm,
                   "generatorBuild",
                   &TransformationLearner::generatorBuild,
                   (BodyDoc("generator specific initialization operations"),
@@ -538,10 +499,6 @@
                   &TransformationLearner::MStepNoiseVariance,
                   (BodyDoc("maximization step with respect to noise variance")));
     declareMethod(rmm,
-                  "stoppingCriterionReached",
-                  &TransformationLearner::stoppingCriterionReached,
-                  (BodyDoc("stages == nstages?")));
-    declareMethod(rmm,
                   "nextStage",
                   &TransformationLearner::nextStage,
                   (BodyDoc("increment 'stage' by one")));
@@ -554,7 +511,7 @@
 ///////////
 void TransformationLearner::build()
 {
-    pout << "build\n";
+
     // ### Nothing to add here, simply calls build_().
     inherited::build();
     build_();
@@ -583,34 +540,23 @@
     //                                          false);
     // TransformationLearner::setPredictor(predictor_part, false);
 
-    pout << "build_\n";
-    if(behavior == BEHAVIOR_LEARNER){
-        trainBuild();
+ 
+
+    if(behavior == BEHAVIOR_LEARNER)
+    {
+        if(train_set.isNotNull())
+        {
+            mainLearnerBuild();
+        }
+     
     }
    
     else{
         generatorBuild(); //initialization of the parameters with all the default values
     }
-    
- 
+        
 }
 
-/////////
-// cdf //
-/////////
-real TransformationLearner::cdf(const Vec& y) const
-{
-    PLERROR("cdf not implemented for TransformationLearner"); return 0;
-}
-
-/////////////////
-// expectation //
-/////////////////
-void TransformationLearner::expectation(Vec& mu) const
-{
-    PLERROR("expectation not implemented for TransformationLearner");
-}
-
 // ### Remove this method if your distribution does not implement it.
 ////////////
 // forget //
@@ -618,6 +564,7 @@
 void TransformationLearner::forget()
 {
     
+    
     /*!
       A typical forget() method should do the following:
       - initialize a random number generator with the seed option
@@ -628,7 +575,8 @@
     
     inherited::forget();
     stage = 0;
-    trainBuild();
+    build();
+   
     
 }
 
@@ -641,13 +589,15 @@
 //! - choose randomly a transformation 
 //! - apply the transformation on the choosen neighbor
 //! - add some noise 
-void TransformationLearner::generate(Vec & y) //const
+void TransformationLearner::generate(Vec & y) const
 {
     //PLERROR("generate not implemented for TransformationLearner");
     PLASSERT(y.length() == inputSpaceDim);
     int neighborIdx ;
     neighborIdx=pickNeighborIdx();
-    seeNeighbor(neighborIdx);
+    Vec neighbor;
+    neighbor.resize(inputSpaceDim);
+    seeNeighbor(neighborIdx, neighbor);
     generatePredictedFrom(neighbor, y);
 }
 
@@ -661,8 +611,10 @@
 /////////////////
 // log_density //
 /////////////////
-real TransformationLearner::log_density(const Vec& y) //const
+real TransformationLearner::log_density(const Vec& y) const
 {
+ 
+    pout << "in TransformationLearner :: log_density" << endl;
     PLASSERT(y.length() == inputSpaceDim);
     real weight;
     real totalWeight = INIT_weight(0);
@@ -681,10 +633,10 @@
     }
     totalWeight = MULT_weights(totalWeight, scalingFactor);
     return totalWeight;
-    
-    /*PLERROR("density not implemented for TransformationLearner"); return 0;*/
 }
 
+
+
 /////////////////////////////////
 // makeDeepCopyFromShallowCopy //
 /////////////////////////////////
@@ -711,42 +663,6 @@
 }
 */
 
-//////////////////
-// setPredictor //
-//////////////////
-void TransformationLearner::setPredictor(const Vec& predictor, bool call_parent) const
-{
-    if (call_parent)
-        inherited::setPredictor(predictor, true);
-    // ### Add here any specific code required by your subclass.
-}
-
-////////////////////////////////
-// setPredictorPredictedSizes //
-////////////////////////////////
-bool TransformationLearner::setPredictorPredictedSizes(int the_predictor_size,
-                                               int the_predicted_size,
-                                               bool call_parent)
-{
-    bool sizes_have_changed = false;
-    if (call_parent)
-        sizes_have_changed = inherited::setPredictorPredictedSizes(
-                the_predictor_size, the_predicted_size, true);
-
-    // ### Add here any specific code required by your subclass.
-
-    // Returned value.
-    return sizes_have_changed;
-}
-
-/////////////////
-// survival_fn //
-/////////////////
-real TransformationLearner::survival_fn(const Vec& y) const
-{
-    PLERROR("survival_fn not implemented for TransformationLearner"); return 0;
-}
-
 // ### Remove this method, if your distribution does not implement it.
 ///////////
 // train //
@@ -788,8 +704,11 @@
     }
     */
 
-    initEStep();
-    while(!stoppingCriterionReached()){
+    if(stage==0)
+        buildLearnedParameters();
+        initEStep();
+    while(stage<nstages)
+    {
         MStep();
         EStep();
         stage ++;
@@ -797,41 +716,97 @@
     
 }
 
-//////////////
-// variance //
-//////////////
-void TransformationLearner::variance(Mat& covar) const
-{
-    PLERROR("variance not implemented for TransformationLearner");
-}
 
 
+void TransformationLearner::buildLearnedParameters(){
+    
+    //LEARNED PARAMETERS
 
+
+    //set of transformations matrices
+    transformsSet = Mat(nbTransforms * inputSpaceDim, inputSpaceDim);
+    
+    //view on the set of transformations (vector)
+    //    each transformation = one matrix 
+    transforms.resize(nbTransforms);
+    for(int k = 0; k< nbTransforms; k++){
+        transforms[k] = transformsSet.subMatRows(k * inputSpaceDim, inputSpaceDim);       
+    }
+    
+    //set of transformations bias (optional)
+    if(withBias){
+        biasSet = Mat(nbTransforms,inputSpaceDim);       
+    }
+    else{
+        biasSet = Mat(0,0);   
+    }
+
+    //choose an initial value for each transformation parameter  (normal distribution) 
+    initTransformsParameters();
+
+    //initialize the noise variance
+    if(noiseVariance == UNDEFINED){
+        if(learnNoiseVariance && regOnNoiseVariance){
+            initNoiseVariance();
+        }
+        else{
+            noiseVariance = 1.0;
+        }
+    }
+
+    //transformDistribution
+    if(transformDistribution.length() == 0){
+        if(learnTransformDistribution && regOnTransformDistribution)
+            initTransformDistribution();
+        else{
+            transformDistribution.resize(nbTransforms);
+            real w = INIT_weight(1.0/nbTransforms);
+            for(int k=0; k<nbTransforms ; k++){
+                transformDistribution[k] = w;
+            }
+        }       
+    }
+    else{
+        PLASSERT(transformDistribution.length() == nbTransforms);
+        PLASSERT(isWellDefined(transformDistribution));
+    }
+
+
+     //reconstruction set 
+    reconstructionSet.resize(nbReconstructions);
+
+
+}
+
+
 //INITIALIZATION METHODS 
 
 
 //! initialization operations that have to be done before the training
 //!WARNING: the trainset ("train_set") must be given
-void TransformationLearner::trainBuild(){
+void TransformationLearner::mainLearnerBuild(){
+    int defaultPeriod = 1;
+    int defaultTransformsOffset;
+    int defaultBiasOffset;
+    int defaultNoiseVarianceOffset;
+    int defaultTransformDistributionOffset;
+
+    defaultTransformsOffset = 0;
     
-    int nbOptimizations =1;
-    int defaultTransformsOffset =0;
-    int defaultBiasOffset ;
-    int defaultNoiseVarianceOffset ;
-    int defaultTransformDistributionOffset ;
     if(withBias){
-        defaultBiasOffset = nbOptimizations ;
-        nbOptimizations ++;
+        defaultBiasOffset = defaultPeriod ;
+        defaultPeriod++;
     }
     if(learnNoiseVariance){
-        defaultNoiseVarianceOffset = nbOptimizations;
-        nbOptimizations ++;
+        defaultNoiseVarianceOffset = defaultPeriod;
+        defaultPeriod++;
     }
     if(learnTransformDistribution){
-        defaultTransformDistributionOffset = nbOptimizations;
-        nbOptimizations ++;
+        defaultTransformDistributionOffset = defaultPeriod;
+        defaultPeriod ++;
     }
-
+    
+    
     transformsSD = sqrt(transformsVariance);
     
     //DIMENSION VARIABLES
@@ -851,43 +826,32 @@
     nbReconstructions = trainingSetLength * nbTargetReconstructions;
     
     
-    //LEARNED MODEL PARAMETERS
     
-    //set of transformations (represented as a single matrix)
-    transformsSet = Mat(nbTransforms * inputSpaceDim, inputSpaceDim);
-    
-    //view on the set of transformations (vector)
-    //    each transformation = one matrix 
-    transforms.resize(nbTransforms);
-    for(int k = 0; k< nbTransforms; k++){
-        transforms[k] = transformsSet.subMatRows(k * inputSpaceDim, inputSpaceDim);       
-    }
-    
+
     if(withBias){
-        biasSet = Mat(nbTransforms,inputSpaceDim);
         if(biasPeriod == UNDEFINED || biasOffset == UNDEFINED){
-            biasPeriod = nbOptimizations;
+            biasPeriod = defaultPeriod;
             biasOffset = defaultBiasOffset;
         }
     }
+
     else{
-        biasPeriod = UNDEFINED;
+        biasPeriod = UNDEFINED ;
         biasOffset = UNDEFINED;
     }
 
-    initTransformsParameters();
+ 
 
-  
    
     if(transformsPeriod == UNDEFINED || transformsOffset == UNDEFINED){
-        transformsPeriod = nbOptimizations;
+        transformsPeriod = defaultPeriod;
         transformsOffset = defaultTransformsOffset;
     }
 
     //training parameters for noise variance
     if(learnNoiseVariance){
         if(noiseVariancePeriod == UNDEFINED || noiseVarianceOffset == UNDEFINED){
-            noiseVariancePeriod = nbOptimizations;
+            noiseVariancePeriod = defaultPeriod;
             noiseVarianceOffset = defaultNoiseVarianceOffset;
         }
         if(regOnNoiseVariance){
@@ -907,20 +871,12 @@
         noiseVarianceOffset = UNDEFINED;
     }
     
-    //initialize the noise variance
-     if(noiseVariance == UNDEFINED){
-        if(learnNoiseVariance && regOnNoiseVariance){
-            initNoiseVariance();
-        }
-        else{
-            noiseVariance = 1.0;
-        }
-     }
+ 
     
      //training parameters for transformation distribution
      if(learnTransformDistribution){
          if(transformDistributionPeriod == UNDEFINED || transformDistributionOffset == UNDEFINED){
-             transformDistributionPeriod = nbOptimizations;
+             transformDistributionPeriod = defaultPeriod;
              transformDistributionOffset = defaultTransformDistributionOffset;
          }
          if(regOnTransformDistribution){
@@ -935,39 +891,18 @@
      else{
          transformDistributionPeriod = UNDEFINED;
          transformDistributionOffset = UNDEFINED;
-         
      }
 
 
-    //transformDistribution
-    if(transformDistribution.length() == 0){
-        if(learnTransformDistribution && regOnTransformDistribution)
-            initTransformDistribution();
-        else{
-            transformDistribution.resize(nbTransforms);
-            real w = INIT_weight(1.0/nbTransforms);
-            for(int k=0; k<nbTransforms ; k++){
-                transformDistribution[k] = w;
-            }
-        }       
-    }
-    else{
-        PLASSERT(transformDistribution.length() == nbTransforms);
-        PLASSERT(isWellDefined(transformDistribution));
-    }
-
-    //reconstruction set 
-    reconstructionSet.resize(nbReconstructions);
+ 
+   
     
     
 
     //OTHER VARIABLES
     
     
-    //to store a view on the generation set 
-    //   (entries related to a specific target)
-    targetReconstructionSet.resize(nbTargetReconstructions);
-    
+     
     //Storage space used in the update of the transformation parameters
     B_C = Mat(2 * nbTransforms * inputSpaceDim , inputSpaceDim);
     
@@ -981,8 +916,6 @@
     }
     
     
-    target.resize(inputSpaceDim);
-    neighbor.resize(inputSpaceDim);
 }
 
 
@@ -1010,7 +943,9 @@
     if(withBias){
         biasSet = Mat(nbTransforms,inputSpaceDim);
     }
-
+    else{
+        biasSet = Mat(0,0);
+    }
     if(transforms_.length() == 0){
         initTransformsParameters();
     }
@@ -1026,7 +961,7 @@
     if(noiseBeta <= 0){
         noiseBeta = 1;
     }
-    if(noiseVariance_ < 0){
+    if(noiseVariance_ <= 0){
         initNoiseVariance();
     }
     else{
@@ -1049,21 +984,21 @@
 void TransformationLearner::initTransformsParameters()
 {
     
-    transformsSet.resize(nbTransforms*inputSpaceDim, inputSpaceDim);
+    transformsSet .resize(nbTransforms*inputSpaceDim, inputSpaceDim);
     transforms.resize(nbTransforms);
     for(int k = 0; k< nbTransforms; k++){
         transforms[k] = transformsSet.subMatRows(k * inputSpaceDim, inputSpaceDim);       
     }
-    int idx = 0;
     for(int t=0; t<nbTransforms ; t++){
-        transforms[t] = transformsSet.subMatRows(idx,inputSpaceDim);
-        idx += inputSpaceDim;
         random_gen->fill_random_normal(transforms[t], 0 , transformsSD);
     }
     if(withBias){
         biasSet = Mat(nbTransforms,inputSpaceDim);
         random_gen->fill_random_normal(biasSet, 0,transformsSD);
     }
+    else{
+        biasSet = Mat(0,0);
+    }
     if(transformFamily == TRANSFORM_FAMILY_LINEAR){
         for(int t=0; t<nbTransforms;t++){
             addToDiagonal(transforms[t],1.0);
@@ -1100,6 +1035,9 @@
         biasSet = Mat(nbTransforms, inputSpaceDim);
         biasSet << biasSet_;
     }
+    else{
+        biasSet = Mat(0,0);
+    }
     
 
 }
@@ -1164,7 +1102,7 @@
     int d = source.length();
     PLASSERT(d == inputSpaceDim);
     PLASSERT(sample.length() == inputSpaceDim);
-    PLASSERT(0<=transformIdx<nbTransforms);
+    PLASSERT(0<= transformIdx && transformIdx<nbTransforms);
     
     //apply the transformation
     applyTransformationOn(transformIdx,source,sample);
@@ -1178,7 +1116,7 @@
 //!generates a sample data point from a source data point and returns it
 //! (if transformIdx >= 0 , we use the corresponding transformation )
 Vec TransformationLearner::returnPredictedFrom(Vec source,
-                                               int transformIdx)
+                                               int transformIdx)const
 {
     Vec sample;
     sample.resize(inputSpaceDim);
@@ -1226,7 +1164,7 @@
 // - (*) if transformIdx>=0, we always use the corresponding transformation
 Mat TransformationLearner::returnGeneratedSamplesFrom(Vec center,
                                                       int n,
-                                                      int transformIdx)
+                                                      int transformIdx)const
 {
     Mat samples = Mat(n,inputSpaceDim);
     if(transformIdx<0)
@@ -1245,7 +1183,8 @@
     for(int i=0; i<nbTransforms; i++){
         probaTransformDistribution[i]=PROBA_weight(transformDistribution[i]);
     }
-    return random_gen->multinomial_sample(probaTransformDistribution);
+    int w= random_gen->multinomial_sample(probaTransformDistribution);
+    return w;
 }
 
 //!Select a neighbor in the training set randomly
@@ -1286,7 +1225,8 @@
 void TransformationLearner::treeDataSet(const Vec & root,
                                         int deepness,
                                         int branchingFactor,
-                                        Mat & dataPoints)
+                                        Mat & dataPoints,
+                                        int transformIdx)const
 {
 
     PLASSERT(root.length() == inputSpaceDim);
@@ -1309,14 +1249,20 @@
         
         Vec v = dataPoints(centerIdx);
         Mat m = dataPoints.subMatRows(dataIdx, branchingFactor);
-        batchGeneratePredictedFrom(v,m); 
+        if(transformIdx>=0){
+            batchGeneratePredictedFrom(v,m,transformIdx);
+        }
+        else{
+            batchGeneratePredictedFrom(v,m);
+        } 
         centerIdx ++ ;
     }  
 }
 
 Mat TransformationLearner::returnTreeDataSet(Vec root,
                                              int deepness,
-                                             int branchingFactor)
+                                             int branchingFactor,
+                                             int transformIdx)const
 {
     Mat dataPoints;
     treeDataSet(root,deepness,branchingFactor, dataPoints);
@@ -1329,25 +1275,29 @@
 //! (where "->" stands for : "generate the")
 void TransformationLearner::sequenceDataSet(const Vec & start,
                                             int n,
-                                            Mat & dataPoints)
+                                            Mat & dataPoints,
+                                            int transformIdx)const
 {
-    treeDataSet(start,n-1,1,dataPoints);
+    treeDataSet(start,n-1,1,dataPoints , transformIdx);
 }
 
 Mat TransformationLearner::returnSequenceDataSet(Vec start,
-                                                 int n)
+                                                 int n,
+                                                 int transformIdx)const
 {
     Mat dataPoints;
-    sequenceDataSet(start,n,dataPoints);
+    sequenceDataSet(start,n,dataPoints,transformIdx);
     return dataPoints;
 }
 
 
+
+
 //! COPIES OF THE STRUCTURES
 
 
 //!returns the "idx"th data point in the training set
-Vec TransformationLearner::returnTrainingPoint(int idx)
+Vec TransformationLearner::returnTrainingPoint(int idx)const
 {
     
     Vec v,temp;
@@ -1360,7 +1310,7 @@
  
 
 //!returns all the reconstructions candidates associated to a given target
-TVec<ReconstructionCandidate> TransformationLearner::returnReconstructionCandidates(int targetIdx)
+TVec<ReconstructionCandidate> TransformationLearner::returnReconstructionCandidates(int targetIdx)const
 {
    
     int startIdx = targetIdx * nbTargetReconstructions;  
@@ -1371,7 +1321,7 @@
 
 //!returns the reconstructions of the "targetIdx"th data point value in the training set
 //!(one reconstruction for each reconstruction candidate)
-Mat TransformationLearner::returnReconstructions(int targetIdx)
+Mat TransformationLearner::returnReconstructions(int targetIdx)const
 {
     Mat reconstructions = Mat(nbTargetReconstructions,inputSpaceDim);
     int candidateIdx = targetIdx*nbTargetReconstructions;
@@ -1379,7 +1329,9 @@
     for(int i=0; i<nbTargetReconstructions; i++){
         neighborIdx = reconstructionSet[candidateIdx].neighborIdx;
         transformIdx= reconstructionSet[candidateIdx].transformIdx;
-        seeNeighbor(neighborIdx);
+        Vec neighbor;
+        neighbor.resize(inputSpaceDim);
+        seeNeighbor(neighborIdx, neighbor);
         Vec v = reconstructions(i);
         applyTransformationOn(transformIdx, neighbor, v);
         candidateIdx ++;
@@ -1389,14 +1341,16 @@
 
 //!returns the neighbors choosen to reconstruct the target
 //!(one choosen neighbor for each reconstruction candidate associated to the target)
-Mat TransformationLearner::returnNeighbors(int targetIdx)
+Mat TransformationLearner::returnNeighbors(int targetIdx)const
 {
     int candidateIdx = targetIdx*nbTargetReconstructions;
     int neighborIdx;
     Mat neighbors = Mat(nbTargetReconstructions, inputSpaceDim);
     for(int i=0; i<nbTargetReconstructions; i++){
         neighborIdx = reconstructionSet[candidateIdx].neighborIdx;
-        seeNeighbor(neighborIdx);
+        Vec neighbor;
+        neighbor.resize(inputSpaceDim);
+        seeNeighbor(neighborIdx, neighbor);
         neighbors(i) << neighbor;
         candidateIdx++;
     }
@@ -1405,7 +1359,7 @@
 
 
 //!returns the parameters of the "transformIdx"th transformation
-Mat TransformationLearner::returnTransform(int transformIdx)
+Mat TransformationLearner::returnTransform(int transformIdx)const
 {
     return transforms[transformIdx].copy();    
 }
@@ -1413,7 +1367,7 @@
 //!returns the parameters of each transformation
 //!(as an KdXd matrix, K = number of transformations,
 //!                    d = dimension of input space)
-Mat TransformationLearner::returnAllTransforms()
+Mat TransformationLearner::returnAllTransforms()const
 {
     return transformsSet.copy();    
 }
@@ -1425,7 +1379,8 @@
 
 //! stores a VIEW on the reconstruction candidates related to the specified
 //! target (into the variable "targetReconstructionSet" )
-void TransformationLearner::seeTargetReconstructionSet(int targetIdx)
+void TransformationLearner::seeTargetReconstructionSet(int targetIdx, 
+                                                       TVec<ReconstructionCandidate> & targetReconstructionSet)const
 {
     int startIdx = targetIdx *nbTargetReconstructions;
     targetReconstructionSet = reconstructionSet.subVec(startIdx, 
@@ -1434,17 +1389,17 @@
 
 // stores the "targetIdx"th point in the training set into the variable
 // "target"
-void TransformationLearner::seeTarget(const int targetIdx)
+void TransformationLearner::seeTarget(const int targetIdx, Vec & storage)const
 {
     Vec v;
     real w;
-    train_set->getExample(targetIdx,target,v,w);
+    train_set->getExample(targetIdx,storage,v,w);
     
 }
 
 // stores the "neighborIdx"th input in the training set into the variable
 // "neighbor" 
-void TransformationLearner::seeNeighbor(const int neighborIdx)
+void TransformationLearner::seeNeighbor(const int neighborIdx, Vec & neighbor)const
 {
     Vec v;
     real w;
@@ -1459,14 +1414,14 @@
 
 //!returns a pseudo-random positive real number x  
 //!using the distribution p(x)=Gamma(alpha,beta)
-real TransformationLearner::gamma_sample(real alpha, real beta)
+real TransformationLearner::gamma_sample(real alpha, real beta)const
 {
   real c,x,u,d,v;
+  c = 1.0/3.0;
+  d = alpha - c ;
   do{
-      c = 1.0/3.0;
       x = random_gen->gaussian_01();
-      u = random_gen->uniform_sample();
-      d = alpha - c ;
+      u = random_gen->uniform_sample();    
       v = pow((1 + x/(pow(9*d , 0.5)))  ,3.0);
   }
   while(pl_log(u) < 0.5*pow(x,2) + d - d*v + d*pl_log(v));
@@ -1486,7 +1441,7 @@
 //!using the distribution p(x) = Dirichlet(x| all the parameters = alpha)
 //!-all the element of the vector are between 0 and 1,
 //!-the elements of the vector sum to 1
-void TransformationLearner::dirichlet_sample(real alpha, Vec & sample){
+void TransformationLearner::dirichlet_sample(real alpha, Vec & sample)const{
     int d = sample.length();
     real sum = 0;
     for(int i=0;i<d;i++){
@@ -1497,7 +1452,8 @@
         sample[i]/=sum;
     }
 }
-Vec TransformationLearner::return_dirichlet_sample(real alpha)
+
+Vec TransformationLearner::return_dirichlet_sample(real alpha)const
 {
     Vec sample ;
     sample.resize(inputSpaceDim);
@@ -1526,7 +1482,7 @@
 
 //!normalizes the reconstruction weights related to a given target.
 void TransformationLearner::normalizeTargetWeights(int targetIdx,
-                                                   real totalWeight)const
+                                                   real totalWeight)
 {
     real w;
     int startIdx = targetIdx * nbTargetReconstructions;
@@ -1608,7 +1564,7 @@
 
 //!update/compute the weight of a reconstruction candidate with
 //!the actual transformation parameters
-real TransformationLearner::updateReconstructionWeight(int candidateIdx)
+real TransformationLearner::updateReconstructionWeight(int candidateIdx) 
 {
     int targetIdx = reconstructionSet[candidateIdx].targetIdx;
     int neighborIdx = reconstructionSet[candidateIdx].neighborIdx;
@@ -1620,7 +1576,7 @@
     reconstructionSet[candidateIdx].weight = w;
     return w; 
 }
-real TransformationLearner::computeReconstructionWeight(const ReconstructionCandidate & gc)
+real TransformationLearner::computeReconstructionWeight(const ReconstructionCandidate & gc)const
 {
     return computeReconstructionWeight(gc.targetIdx,
                                        gc.neighborIdx,
@@ -1628,18 +1584,23 @@
 }
 real TransformationLearner::computeReconstructionWeight(int targetIdx,
                                                         int neighborIdx,
-                                                        int transformIdx)
+                                                        int transformIdx)const
 {
-    seeTarget(targetIdx);
+
+    Vec target;
+    target.resize(inputSpaceDim);
+    seeTarget(targetIdx,target);
     return computeReconstructionWeight(target,
                                        neighborIdx,
                                        transformIdx);
 }
 real TransformationLearner::computeReconstructionWeight(const Vec & target_,
                                                         int neighborIdx,
-                                                        int transformIdx)
+                                                        int transformIdx)const
 {
-    seeNeighbor(neighborIdx);
+    Vec neighbor;
+    neighbor.resize(inputSpaceDim);
+    seeNeighbor(neighborIdx, neighbor);
     Vec predictedTarget ;
     predictedTarget.resize(inputSpaceDim);
     applyTransformationOn(transformIdx, neighbor, predictedTarget);
@@ -1675,7 +1636,7 @@
 //! those probabilities sum to 1 . 
 //!(typical case: the distribution is represented as a set of weights, which are typically
 //! log-probabilities)
-bool  TransformationLearner::isWellDefined(Vec & distribution)
+bool  TransformationLearner::isWellDefined(Vec & distribution)const
 {  
     if(nbTransforms != distribution.length()){
         return false;
@@ -1684,7 +1645,7 @@
     real proba;
     for(int i=0; i<nbTransforms;i++){
         proba = PROBA_weight(distribution[i]);
-        if(proba < 0 || proba > 1){
+        if(proba < 0 || proba >1){
             return false;
         }
         sum += proba;
@@ -1795,14 +1756,18 @@
     PLASSERT(pq.empty()); 
   
     //capture the target from his index in the training set
-    seeTarget(targetIdx);
-     
+    Vec target;
+    target.resize(inputSpaceDim);
+    seeTarget(targetIdx, target);
+    
     //for each potential neighbor,
     real dist;    
     for(int i=0; i<trainingSetLength; i++){
         if(i != targetIdx){ //(the target cannot be his own neighbor)
             //computes the distance to the target
-            seeNeighbor(i);
+            Vec neighbor;
+            neighbor.resize(inputSpaceDim);
+            seeNeighbor(i, neighbor);
             dist = powdistance(target, neighbor); 
             //if the distance is among "nbNeighbors" smallest distances seen,
             //keep it until to see a closer neighbor. 
@@ -2046,10 +2011,10 @@
     if(transformDistributionPeriod > 0 && 
        stage % transformDistributionPeriod == transformDistributionOffset)
         MStepTransformDistribution();
+    if(biasPeriod > 0 && stage % biasPeriod == biasOffset)
+        MStepBias();
     if(stage % transformsPeriod == transformsOffset)
         MStepTransformations();
-    if(stage % biasPeriod == biasOffset)
-        MStepBias();
     
 }
 
@@ -2095,7 +2060,7 @@
     transformDistribution << newDistribution ;
 }
 
-//!maximization step with respect to transformation matrices
+//!maximization step with respect to transformation parameters
 //!(MAP version)
 void TransformationLearner::MStepTransformations()
 {
@@ -2113,8 +2078,12 @@
         real p = PROBA_weight(reconstructionSet[idx].weight);
   
         //catch the target and neighbor points from the training set
-        seeTarget(reconstructionSet[idx].targetIdx);
-        seeNeighbor(reconstructionSet[idx].neighborIdx);
+        Vec target;
+        target.resize(inputSpaceDim);
+        seeTarget(reconstructionSet[idx].targetIdx, target);
+        Vec neighbor;
+        neighbor.resize(inputSpaceDim);
+        seeNeighbor(reconstructionSet[idx].neighborIdx, neighbor);
         
         int t = reconstructionSet[idx].transformIdx;
         
@@ -2136,40 +2105,10 @@
     }  
 }
  
-//!maximization step with respect to transformation bias
-//!(MAP version)
+
+//TODO
 void TransformationLearner::MStepBias(){
-    Mat  newBiasSet;
-    newBiasSet.resize(nbTransforms,inputSpaceDim);
-    for(int i=0; i<nbTransforms; i++){
-        for(int j =0; j<inputSpaceDim; j++){
-            newBiasSet[i][j]= 0; 
-        }
-    }
-    int transformIdx;
-    real proba;
-    real w;
-    real sum = INIT_weight(0);
-    Vec reconstruction;
-    reconstruction.resize(inputSpaceDim);
-    for(int idx=0; idx<nbReconstructions ; idx++){
-        transformIdx = reconstructionSet[idx].transformIdx;
-        w = reconstructionSet[idx].weight;
-        proba = PROBA_weight(w);
-        sum = SUM_weights(sum, w);
-        seeNeighbor(reconstructionSet[idx].neighborIdx);
-        if(transformFamily == TRANSFORM_FAMILY_LINEAR){
-            transposeProduct(reconstruction,transforms[transformIdx],neighbor);
-        }
-        else{
-            transposeProduct(reconstruction,transforms[transformIdx],neighbor);
-            reconstruction += neighbor;
-        }
-        seeTarget(reconstructionSet[idx].targetIdx);
-        newBiasSet += proba*(target - reconstruction);
-    }
-    newBiasSet = newBiasSet/( noiseVariance/transformsVariance + PROBA_weight(sum) );
-    biasSet << newBiasSet;
+    
 }
 
 
@@ -2200,8 +2139,13 @@
 //!returns the distance between the reconstruction and the target
 //!for the 'candidateIdx'th reconstruction candidate
 real TransformationLearner::reconstructionEuclideanDistance(int candidateIdx){
-    seeTarget(reconstructionSet[candidateIdx].targetIdx);
-    seeNeighbor(reconstructionSet[candidateIdx].neighborIdx);
+    Vec target;
+    target.resize(inputSpaceDim);
+    seeTarget(reconstructionSet[candidateIdx].targetIdx, target);
+    Vec neighbor;
+    neighbor.resize(inputSpaceDim);
+    seeNeighbor(reconstructionSet[candidateIdx].neighborIdx,
+                neighbor);
     Vec reconstruction;
     reconstruction.resize(inputSpaceDim);
     applyTransformationOn(reconstructionSet[candidateIdx].transformIdx,
@@ -2211,16 +2155,6 @@
 }
 
 
-//!STOPPING CRITERION
-
-
-//!stages == nstages?
-bool TransformationLearner::stoppingCriterionReached()
-{
-   
-    return stage==nstages;
-}
-
 //!increments the variable 'stage' of 1 
 void TransformationLearner::nextStage(){
     stage ++;

Modified: trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h
===================================================================
--- trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h	2007-07-31 00:18:20 UTC (rev 7874)
+++ trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h	2007-07-31 03:48:46 UTC (rev 7875)
@@ -248,7 +248,7 @@
     //!(see MStep() for more details)
     int transformDistributionPeriod;
     int transformDistributionOffset;
-
+    
     //!This parameter have to be defined if the transformation distribution
     //!is learned using a MAP procedure. We suppose that this distribution have a a multinomial form
     //(u1,u2,...,uK) with dirichlet prior probability : 
@@ -258,13 +258,12 @@
     real transformDistributionAlpha;
 
 
-    //!tells us when to update the transformation matrices and bias
+    //!tells us when to update the transformation parameters
     int transformsPeriod;
     int transformsOffset;
     int biasPeriod;
     int biasOffset;
 
-
     //PARAMETERS OF THE DISTRIBUTION
 
     //! variance of the NOISE random variable. 
@@ -307,23 +306,11 @@
     //#####  PDistribution Member Functions  ##################################
 
     //! Return log of probability density log(p(y | x)).
-    virtual real log_density(const Vec& y) ;//const;
+    virtual real log_density(const Vec& y) const;
 
-    //! Return survival function: P(Y>y | x).
-    virtual real survival_fn(const Vec& y) const;
-
-    //! Return cdf: P(Y<y | x).
-    virtual real cdf(const Vec& y) const;
-
-    //! Return E[Y | x].
-    virtual void expectation(Vec& mu) const;
-
-    //! Return Var[Y | x].
-    virtual void variance(Mat& cov) const;
-
     //! Return a pseudo-random sample generated from the conditional
     //! distribution, of density p(y | x).
-    virtual void generate(Vec& y); //const ;
+    virtual void generate(Vec& y) const ;
 
     //### Override this method if you need it (and if your distribution can
     //### handle it. Default version calls PLERROR.
@@ -339,17 +326,6 @@
     //! given seed.
     // virtual void resetGenerator(long g_seed) const;
 
-    //! Set the 'predictor' and 'predicted' sizes for this distribution.
-    //### See help in PDistribution.h.
-    virtual bool setPredictorPredictedSizes(int the_predictor_size,
-                                            int the_predicted_size,
-                                            bool call_parent = true);
-
-    //! Set the value for the predictor part of a conditional probability.
-    //### See help in PDistribution.h.
-    virtual void setPredictor(const Vec& predictor, bool call_parent = true)
-                              const;
-
     // ### These methods may be overridden for efficiency purpose:
     /*
     //### Default version calls exp(log_density(y))
@@ -421,7 +397,9 @@
     //!(bias are set to 0)
     void setTransformsParameters(TVec<Mat>  transforms, Mat bias=Mat());
     
-    
+   
+
+
     //!initializes the noise variance randomly
     //!(gamma distribution)
     void initNoiseVariance();
@@ -447,7 +425,7 @@
 
     //!generates a sample data point from a source data point and returns it
     //! (if transformIdx >= 0 , we use the corresponding transformation )
-    Vec returnPredictedFrom(Vec source, int transformIdx=-1);
+    Vec returnPredictedFrom(Vec source, int transformIdx=-1)const ;
     
 
     //!fill the matrix "samples" with data points obtained from a given center data point
@@ -465,7 +443,7 @@
     //                          2) apply it on center
     //                          3) add noise)
     // - (*) if transformIdx>=0, we always use the corresponding transformation
-    Mat returnGeneratedSamplesFrom(Vec center, int n, int transformIdx=-1);
+    Mat returnGeneratedSamplesFrom(Vec center, int n, int transformIdx=-1)const;
     
     
     //!select a transformation randomly (with respect to our multinomial distribution)
@@ -504,10 +482,12 @@
     void treeDataSet(const Vec &root,
                      int deepness,
                      int branchingFactor,
-                     Mat & dataPoints);
+                     Mat & dataPoints,
+                     int transformIdx = -1)const;
     Mat returnTreeDataSet(Vec root,
                           int deepness,
-                          int branchingFactor);
+                          int branchingFactor,
+                          int transformIdx =-1)const;
     
 
     //!create a "sequential" dataset:
@@ -515,9 +495,10 @@
     //! (where "->" stands for : "generate the")
     void sequenceDataSet(const Vec & start,
                          int n,
-                         Mat & dataPoints);
+                         Mat & dataPoints,
+                         int transformIdx=-1)const;
 
-    Mat returnSequenceDataSet(Vec start,int n);
+    Mat returnSequenceDataSet(Vec start,int n, int transformIdx=-1)const;
 
   
 
@@ -528,26 +509,26 @@
     //!COPIES OF THE STRUCTURES
 
     //!returns the "idx"th data point in the training set
-    Vec returnTrainingPoint(int idx);
+    Vec returnTrainingPoint(int idx)const;
 
     //!returns all the reconstructions candidates associated to a given target
-    TVec<ReconstructionCandidate> returnReconstructionCandidates(int targetIdx);
+    TVec<ReconstructionCandidate> returnReconstructionCandidates(int targetIdx)const;
 
     //!returns the reconstructions of the "targetIdx"th data point value in the training set
     //!(one reconstruction for each reconstruction candidate)
-    Mat returnReconstructions(int targetIdx);
+    Mat returnReconstructions(int targetIdx)const;
 
     //!returns the neighbors choosen to reconstruct the target
     //!(one choosen neighbor for each reconstruction candidate associated to the target)
-    Mat returnNeighbors(int targetIdx);
+    Mat returnNeighbors(int targetIdx)const;
 
     //!returns the parameters of the "transformIdx"th transformation
-    Mat returnTransform(int transformIdx);
+    Mat returnTransform(int transformIdx)const;
 
     //!returns the parameters of each transformation
     //!(as an KdXd matrix, K = number of transformations,
     //!                    d = dimension of input space)
-    Mat returnAllTransforms();
+    Mat returnAllTransforms()const;
 
 
     //OTHER BUILDING/INITIALIZATION METHODS 
@@ -555,9 +536,12 @@
     // Simply calls inherited::build() then build_()
     virtual void build();
     
-    //! initialization operations that have to be done before the training
-    void trainBuild();
+    //! main initialization operations that have to be done before any training phase
+    void mainLearnerBuild();
     
+    void buildLearnedParameters();
+    
+
     //! initialization operations that have to be done before a generation process
     //! (all the undefined parameters will be initialized  randomly)
     void generatorBuild(int inputSpaceDim_=2,
@@ -640,10 +624,8 @@
     //!Each matrix is a view on a sub-matrix in th bigger matrix "B_C" described above.
     TVec<Mat> B,C;
     
-    //!To get easily a view on an input point from the training set
-    Vec target, neighbor;
+   
     
-    
 protected:
     //#####  Protected Member Functions  ######################################
 
@@ -666,13 +648,14 @@
     
     //! stores a VIEW on the reconstruction candidates related to the specified
     //! target (into the variable "targetReconstructionSet" )
-    void seeTargetReconstructionSet(int targetIdx) ;
+    void seeTargetReconstructionSet(int targetIdx,
+                                    TVec<ReconstructionCandidate> & targetReconstructionSet)const ;
     // stores the "targetIdx"th point in the training set into the variable
     // "target"
-    void seeTarget(const int targetIdx) ;
+    void seeTarget(const int targetIdx, Vec & target) const ;
     // stores the "neighborIdx"th input in the training set into the variable
     // "neighbor" 
-    void seeNeighbor(const int neighborIdx);
+    void seeNeighbor(const int neighborIdx, Vec & neighbor)const;
 
 
     //!GENERATE GAMMA RANDOM VARIABLES
@@ -681,7 +664,7 @@
     
     //!returns a pseudo-random positive real number x  
     //!using the distribution p(x)=Gamma(alpha,beta)
-    real gamma_sample(real alpha,real beta=1);
+    real gamma_sample(real alpha,real beta=1)const;
     
     
     //!GENERATE DIRICHLET RANDOM VARIABLES
@@ -691,8 +674,8 @@
     //!using the distribution p(x) = Dirichlet(x| all the parameters = alpha)
     //!-all the element of the vector are between 0 and 1,
     //!-the elements of the vector sum to 1
-    void dirichlet_sample(real alpha, Vec & sample);
-    Vec return_dirichlet_sample(real alpha);
+    void dirichlet_sample(real alpha, Vec & sample)const;
+    Vec return_dirichlet_sample(real alpha)const;
 
     
   
@@ -701,7 +684,7 @@
     //!OPERATIONS ON WEIGHTS 
     
      //!normalizes the reconstruction weights related to a given target. 
-    void normalizeTargetWeights(int targetIdx, real totalWeight) const;
+    void normalizeTargetWeights(int targetIdx, real totalWeight);
     
     //!returns a random weight 
     real randomWeight() const;
@@ -717,13 +700,13 @@
     //!update/compute the weight of a reconstruction candidate with
     //!the actual transformation parameters
     real updateReconstructionWeight(int candidateIdx);
-    real computeReconstructionWeight(const ReconstructionCandidate & gc);
+    real computeReconstructionWeight(const ReconstructionCandidate & gc) const;
     real computeReconstructionWeight(int targetIdx, 
                                      int neighborIdx, 
-                                     int transformIdx);
+                                     int transformIdx) const;
     real computeReconstructionWeight(const Vec & target,
                                      int neighborIdx,
-                                     int transformIdx);
+                                     int transformIdx) const;
 
     //!applies "transformIdx"th transformation on data point "src"
     void applyTransformationOn(int transformIdx, const Vec & src , Vec & dst) const ;
@@ -734,7 +717,7 @@
     //! those probabilities sum to 1 . 
     //!(the distribution is represented as a set of weights, which are typically
     //! log-probabilities)
-    bool isWellDefined(Vec & distribution);
+    bool isWellDefined(Vec & distribution)const;
 
     //!INITIAL E STEP 
     
@@ -827,18 +810,14 @@
     //!NOTE :  alpha =1 ->  no regularization
     void MStepTransformDistributionMAP(real alpha);
 
-
-
     //!maximization step with respect to transformation matrices
     //!(MAP version)
     void MStepTransformations();
-    
-
+  
     //!maximization step with respect to transformation bias
     //!(MAP version)
     void MStepBias();
-    
-    
+
     //!maximization step with respect to noise variance
     void MStepNoiseVariance();
     
@@ -851,11 +830,6 @@
     //!for the 'candidateIdx'th reconstruction candidate
     real reconstructionEuclideanDistance(int candidateIdx);
     
-    
-    //STOPPING CRITERION
-    //stage == nstages?
-    bool stoppingCriterionReached();
-    
     //increment the variable 'stage' of 1
     void nextStage();
 



From nouiz at mail.berlios.de  Tue Jul 31 16:05:12 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 31 Jul 2007 16:05:12 +0200
Subject: [Plearn-commits] r7876 -
	branches/cgi-desjardin/plearn_learners/second_iteration
Message-ID: <200707311405.l6VE5CRc000264@sheep.berlios.de>

Author: nouiz
Date: 2007-07-31 16:05:03 +0200 (Tue, 31 Jul 2007)
New Revision: 7876

Modified:
   branches/cgi-desjardin/plearn_learners/second_iteration/ComputePurenneError.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/MeanMedianModeImputationVMatrix.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.h
   branches/cgi-desjardin/plearn_learners/second_iteration/WeightedDistance.cc
Log:
Make compile with float


Modified: branches/cgi-desjardin/plearn_learners/second_iteration/ComputePurenneError.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/ComputePurenneError.cc	2007-07-31 03:48:46 UTC (rev 7875)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/ComputePurenneError.cc	2007-07-31 14:05:03 UTC (rev 7876)
@@ -141,10 +141,10 @@
 
 void ComputePurenneError::computeCostsFromOutputs(const Vec& inputv, const Vec& outputv, const Vec& targetv, Vec& costsv) const
 {
-    costsv[0] = pow((outputv[0] - targetv[0]), 2.0);
+    costsv[0] = pow((outputv[0] - targetv[0]), 2);
     costsv[1] = outputv[1] == targetv[1] ? 0 : 1;
     costsv[2] = int(round(fabs(outputv[1] - targetv[1])));
-    costsv[3] = pow((outputv[1] - targetv[1]), 2.0);
+    costsv[3] = pow((outputv[1] - targetv[1]), 2);
 }
 
 } // end of namespace PLearn

Modified: branches/cgi-desjardin/plearn_learners/second_iteration/MeanMedianModeImputationVMatrix.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/MeanMedianModeImputationVMatrix.cc	2007-07-31 03:48:46 UTC (rev 7875)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/MeanMedianModeImputationVMatrix.cc	2007-07-31 14:05:03 UTC (rev 7876)
@@ -439,7 +439,7 @@
   input_vec[index_j] = saved_value;
 }
 
-double MeanMedianModeImputationVMatrix::compare(double field1, double field2)
+real MeanMedianModeImputationVMatrix::compare(real field1, real field2)
 {
   if (is_missing(field1) && is_missing(field2)) return 0.0;
   if (is_missing(field1)) return +1.0;

Modified: branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.cc	2007-07-31 03:48:46 UTC (rev 7875)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.cc	2007-07-31 14:05:03 UTC (rev 7876)
@@ -390,9 +390,9 @@
     for (int test_row = 0; test_row < test_length; test_row++)
     {
         to_deal_with_value = test_samples_set->get(test_row, to_deal_with_next);
-        mmmf_mean_err += pow(to_deal_with_value - mmmf_mean, 2.0);
-        mmmf_median_err += pow(to_deal_with_value - mmmf_median, 2.0);
-        mmmf_mode_err += pow(to_deal_with_value - mmmf_mode, 2.0);
+        mmmf_mean_err += pow(to_deal_with_value - mmmf_mean, 2);
+        mmmf_median_err += pow(to_deal_with_value - mmmf_median, 2);
+        mmmf_mode_err += pow(to_deal_with_value - mmmf_mode, 2);
         pb->update( test_row );
     }
     delete pb;
@@ -421,7 +421,7 @@
     for (int test_row = 0; test_row < test_length; test_row++)
     {
         to_deal_with_value = test_samples_set->get(test_row, to_deal_with_next);
-        tcmf_mean_err += pow(to_deal_with_value - tcmf_file->get(indices[test_row], 0), 2.0);
+        tcmf_mean_err += pow(to_deal_with_value - tcmf_file->get(indices[test_row], 0), 2);
         pb->update( test_row );
     }
     delete pb;
@@ -459,7 +459,7 @@
     for (int test_row = 0; test_row < test_length; test_row++)
     {
         test_samples_set->getRow(test_row, train_input);
-        cvpf_mean_err += pow(to_deal_with_value - covariancePreservationValue(to_deal_with_next), 2.0);
+        cvpf_mean_err += pow(to_deal_with_value - covariancePreservationValue(to_deal_with_next), 2);
         pb->update( test_row );
     }
     delete pb;
@@ -516,7 +516,7 @@
             }
             if (knnv_value_count > 0.0)
             {
-                knnf_mean_err[knnf_row] += pow(to_deal_with_value - (knnf_sum_value / knnv_value_count), 2.0);
+                knnf_mean_err[knnf_row] += pow(to_deal_with_value - (knnf_sum_value / knnv_value_count), 2);
                 continue;
             }
             knnf_value = ref_cov((int) knnf_neighbors[knnf_row], to_deal_with_next);
@@ -524,7 +524,7 @@
                 PLERROR("In TestImputations::computeNeighborhoodStats(): missing value found in the reference with covariance preserved at: %i , %i",
                          (int) knnf_neighbors[knnf_row], to_deal_with_next);
             knnf_sum_cov += knnf_value;
-            knnf_mean_err[knnf_row] += pow(to_deal_with_value - (knnf_sum_cov / (knnf_row + 1)), 2.0);
+            knnf_mean_err[knnf_row] += pow(to_deal_with_value - (knnf_sum_cov / (knnf_row + 1)), 2);
         }
         pb->update( test_row );
     }

Modified: branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.h	2007-07-31 03:48:46 UTC (rev 7875)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.h	2007-07-31 14:05:03 UTC (rev 7876)
@@ -52,10 +52,9 @@
 #include <plearn/vmat/ExplicitSplitter.h>     //!<  For the splitter stuff.
 #include <plearn_learners/nearest_neighbors/BallTreeNearestNeighbors.h>
 #include <plearn_learners/nearest_neighbors/ExhaustiveNearestNeighbors.h>
-#include <plearn_learners/second_iteration/CovariancePreservationImputationVMatrix.h>
-#include <plearn_learners/second_iteration/NeighborhoodImputationVMatrix.h>
-#include <plearn_learners/second_iteration/WeightedDistance.h>
-#include <plearn_learners/second_iteration/Experimentation.h>
+#include <plearn_learners/cgi/CovariancePreservationImputationVMatrix.h>
+#include <plearn_learners/cgi/NeighborhoodImputationVMatrix.h>
+#include <plearn_learners/cgi/WeightedDistance.h>
 
 namespace PLearn {
 

Modified: branches/cgi-desjardin/plearn_learners/second_iteration/WeightedDistance.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/WeightedDistance.cc	2007-07-31 03:48:46 UTC (rev 7875)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/WeightedDistance.cc	2007-07-31 14:05:03 UTC (rev 7876)
@@ -87,7 +87,7 @@
     real return_value = 0.0;
     for (int i = 0; i < weights.length(); i++)
     {
-        return_value += weights[i] * pow(x1[i] - x2[i], 2.0);
+        return_value += weights[i] * pow(x1[i] - x2[i], 2);
     }
     return return_value;
 }



From nouiz at mail.berlios.de  Tue Jul 31 16:24:38 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 31 Jul 2007 16:24:38 +0200
Subject: [Plearn-commits] r7877 - trunk/plearn_learners/regressors
Message-ID: <200707311424.l6VEOcXn002059@sheep.berlios.de>

Author: nouiz
Date: 2007-07-31 16:24:37 +0200 (Tue, 31 Jul 2007)
New Revision: 7877

Modified:
   trunk/plearn_learners/regressors/BaseRegressorConfidence.cc
   trunk/plearn_learners/regressors/BaseRegressorWrapper.cc
   trunk/plearn_learners/regressors/RegressionTree.cc
   trunk/plearn_learners/regressors/RegressionTreeLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
Modified to compile with float


Modified: trunk/plearn_learners/regressors/BaseRegressorConfidence.cc
===================================================================
--- trunk/plearn_learners/regressors/BaseRegressorConfidence.cc	2007-07-31 14:05:03 UTC (rev 7876)
+++ trunk/plearn_learners/regressors/BaseRegressorConfidence.cc	2007-07-31 14:24:37 UTC (rev 7877)
@@ -106,7 +106,7 @@
         target_to_compare.resize(1);
         neighbors.resize(train_set->length(), number_of_neighbors);
         nearest_neighbbors_target_mean.resize(train_set->length());
-        two_sigma_square = 2.0 * pow(sigma, 2.0);
+        two_sigma_square = 2.0 * pow(sigma, 2);
         root_two_pi_sigma_square = sigma * 2.506628274631;
     }
 }
@@ -180,7 +180,7 @@
             nearest_neighbor = row;
         }
     }
-    outputv[1] = exp(-1.0 * pow((outputv[0] - nearest_neighbbors_target_mean[nearest_neighbor]), 2.0) / two_sigma_square); //  / root_two_pi_sigma_square?
+    outputv[1] = exp(-1.0 * pow((outputv[0] - nearest_neighbbors_target_mean[nearest_neighbor]), 2) / two_sigma_square); //  / root_two_pi_sigma_square?
     outputv[0] = nearest_neighbbors_target_mean[nearest_neighbor];
     if (outputv[1] >= raise_confidence) outputv[1] = 1.0;
     if (outputv[1] < lower_confidence) outputv[1] = 0.0;
@@ -194,7 +194,7 @@
 
 void BaseRegressorConfidence::computeCostsFromOutputs(const Vec& inputv, const Vec& outputv, const Vec& targetv, Vec& costsv) const
 {
-    costsv[0] = pow((outputv[0] - targetv[0]), 2.0);
+    costsv[0] = pow((outputv[0] - targetv[0]), 2);
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn_learners/regressors/BaseRegressorWrapper.cc
===================================================================
--- trunk/plearn_learners/regressors/BaseRegressorWrapper.cc	2007-07-31 14:05:03 UTC (rev 7876)
+++ trunk/plearn_learners/regressors/BaseRegressorWrapper.cc	2007-07-31 14:24:37 UTC (rev 7877)
@@ -259,7 +259,7 @@
 
 void BaseRegressorWrapper::computeCostsFromOutputs(const Vec& inputv, const Vec& outputv, const Vec& targetv, Vec& costsv) const
 {
-    costsv[0] = pow((outputv[0] - targetv[0]), 2.0);
+    costsv[0] = pow((outputv[0] - targetv[0]), 2);
     costsv[1] = outputv[1];
     costsv[2] = 1.0 - (2.0 * loss_function_weight * costsv[0]);
 }

Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2007-07-31 14:05:03 UTC (rev 7876)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2007-07-31 14:24:37 UTC (rev 7877)
@@ -146,7 +146,7 @@
         if (weightsize != 1 && weightsize != 0)  PLERROR("RegressionTree: expected weightsize to be 1 or 0, got %d", weightsize_);
         if (loss_function_weight != 0.0)
         {
-            l2_loss_function_factor = 2.0 / pow(loss_function_weight, 2.0);
+            l2_loss_function_factor = 2.0 / pow(loss_function_weight, 2);
             l1_loss_function_factor = 2.0 / loss_function_weight;
         }
         else
@@ -322,7 +322,7 @@
 
 void RegressionTree::computeCostsFromOutputs(const Vec& inputv, const Vec& outputv, const Vec& targetv, Vec& costsv) const
 {
-    costsv[0] = pow((outputv[0] - targetv[0]), 2.0);
+    costsv[0] = pow((outputv[0] - targetv[0]), 2);
     costsv[1] = outputv[1];
     costsv[2] = 1.0 - (l2_loss_function_factor * costsv[0]);
     costsv[3] = 1.0 - (l1_loss_function_factor * abs(outputv[0] - targetv[0]));

Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2007-07-31 14:05:03 UTC (rev 7876)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2007-07-31 14:24:37 UTC (rev 7877)
@@ -142,7 +142,7 @@
     squared_targets_sum = 0.0;
     weighted_targets_sum = 0.0;
     weighted_squared_targets_sum = 0.0; 
-    if (loss_function_weight != 0.0) loss_function_factor = 2.0 / pow(loss_function_weight, 2.0);
+    if (loss_function_weight != 0.0) loss_function_factor = 2.0 / pow(loss_function_weight, 2);
     else loss_function_factor = 1.0;
 }
 
@@ -153,7 +153,7 @@
     length += 1;
     weights_sum += weight;
     targets_sum += target;
-    squared_target = pow(target, 2.0);
+    squared_target = pow(target, 2);
     squared_targets_sum += squared_target;
     weighted_targets_sum += weight * target;
     weighted_squared_targets_sum += weight * squared_target;  
@@ -170,7 +170,7 @@
     length -= 1;
     weights_sum -= weight;
     targets_sum -= target;
-    squared_target = pow(target, 2.0);
+    squared_target = pow(target, 2);
     squared_targets_sum -= squared_target;
     weighted_targets_sum -= weight * target;
     weighted_squared_targets_sum -= weight * squared_target; 

Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2007-07-31 14:05:03 UTC (rev 7876)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2007-07-31 14:24:37 UTC (rev 7877)
@@ -112,7 +112,7 @@
     if (loss_function_weight != 0.0)
     {
         l1_loss_function_factor = 2.0 / loss_function_weight;
-        l2_loss_function_factor = 2.0 / pow(loss_function_weight, 2.0);
+        l2_loss_function_factor = 2.0 / pow(loss_function_weight, 2);
     }
     else
     {
@@ -198,7 +198,7 @@
         {
             for (multiclass_ind = 1; multiclass_ind < multiclass_outputs.length(); multiclass_ind++)
             {
-                error[0] += pow(output[0] - multiclass_outputs[multiclass_ind], 2.0) * multiclass_weights_sum[multiclass_ind];
+                error[0] += pow(output[0] - multiclass_outputs[multiclass_ind], 2) * multiclass_weights_sum[multiclass_ind];
             }
             error[0] *= l2_loss_function_factor * length / weights_sum;
             if (error[0] < 1E-10) error[0] = 0.0;

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2007-07-31 14:05:03 UTC (rev 7876)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2007-07-31 14:24:37 UTC (rev 7877)
@@ -150,17 +150,17 @@
     leave_register[row] = leave_id;
 }
 
-double RegressionTreeRegisters::getFeature(int row, int col)
+real RegressionTreeRegisters::getFeature(int row, int col)
 {
     return train_set->get(row, col);
 }
 
-double RegressionTreeRegisters::getTarget(int row)
+real RegressionTreeRegisters::getTarget(int row)
 {
     return train_set->get(row, inputsize);
 }
 
-double RegressionTreeRegisters::getWeight(int row)
+real RegressionTreeRegisters::getWeight(int row)
 {
     if (weightsize <= 0) return 1.0 / length;
     else return train_set->get(row, inputsize + 1);
@@ -344,7 +344,7 @@
     sorted_row(index_j, dim) = saved_index;
 }
 
-double RegressionTreeRegisters::compare(double field1, double field2)
+real RegressionTreeRegisters::compare(real field1, real field2)
 {
     if (is_missing(field1) && is_missing(field2)) return 0.0;
     if (is_missing(field1)) return -1.0;



From nouiz at mail.berlios.de  Tue Jul 31 16:42:02 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 31 Jul 2007 16:42:02 +0200
Subject: [Plearn-commits] r7878 - trunk/plearn_learners/meta
Message-ID: <200707311442.l6VEg2H7003318@sheep.berlios.de>

Author: nouiz
Date: 2007-07-31 16:42:01 +0200 (Tue, 31 Jul 2007)
New Revision: 7878

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
Added boundcheck


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2007-07-31 14:24:37 UTC (rev 7877)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2007-07-31 14:42:01 UTC (rev 7878)
@@ -378,6 +378,10 @@
             for (int i=0; i<n; ++i) {
                 if(report_progress) pb->update(i);
                 train_set->getExample(i, input, target, weight);
+#ifdef BOUNDCHECK
+                if(target[0]==0||target[0]==1)
+                    PLERROR("In AdaBoost::train() - target is not 0 or 1 in the training set. We implement only two class boosting.");
+#endif
                 new_weak_learner->computeOutput(input,output);
                 real y_i=target[0];
                 real f_i=output[0];



From nouiz at mail.berlios.de  Tue Jul 31 16:43:55 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 31 Jul 2007 16:43:55 +0200
Subject: [Plearn-commits] r7879 - trunk/plearn_learners/online
Message-ID: <200707311443.l6VEht1t003461@sheep.berlios.de>

Author: nouiz
Date: 2007-07-31 16:43:54 +0200 (Tue, 31 Jul 2007)
New Revision: 7879

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
Log:
Corrected the handling of the size of the output


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-07-31 14:42:01 UTC (rev 7878)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-07-31 14:43:54 UTC (rev 7879)
@@ -684,7 +684,8 @@
 
     if( final_module )
         out_size += final_module->output_size;
-    else
+    
+    if( !use_classification_cost && !final_module )
         out_size += layers[n_layers-1]->size;
 
     return out_size;
@@ -2073,6 +2074,14 @@
         }
     }
 
+    if( !use_classification_cost && !final_module)
+    {
+        connections[ n_layers-2 ]->setAsDownInput(
+            layers[ n_layers-2 ]->expectation );
+        layers[ n_layers-1 ]->getAllActivations( connections[ n_layers-2 ] );
+        layers[ n_layers-1 ]->computeExpectation();
+        output << layers[ n_layers-1 ]->expectation;
+    }
 }
 
 void DeepBeliefNet::computeCostsFromOutputs(const Vec& input, const Vec& output,



From nouiz at mail.berlios.de  Tue Jul 31 17:50:34 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 31 Jul 2007 17:50:34 +0200
Subject: [Plearn-commits] r7880 - trunk/plearn_learners/meta
Message-ID: <200707311550.l6VFoY6u012196@sheep.berlios.de>

Author: nouiz
Date: 2007-07-31 17:50:33 +0200 (Tue, 31 Jul 2007)
New Revision: 7880

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
small bugfix


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2007-07-31 14:43:54 UTC (rev 7879)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2007-07-31 15:50:33 UTC (rev 7880)
@@ -379,7 +379,7 @@
                 if(report_progress) pb->update(i);
                 train_set->getExample(i, input, target, weight);
 #ifdef BOUNDCHECK
-                if(target[0]==0||target[0]==1)
+                if(!(target[0]==0||target[0]==1))
                     PLERROR("In AdaBoost::train() - target is not 0 or 1 in the training set. We implement only two class boosting.");
 #endif
                 new_weak_learner->computeOutput(input,output);



From tihocan at mail.berlios.de  Tue Jul 31 18:05:59 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 31 Jul 2007 18:05:59 +0200
Subject: [Plearn-commits] r7881 - trunk/plearn_learners/meta
Message-ID: <200707311605.l6VG5xcs013504@sheep.berlios.de>

Author: tihocan
Date: 2007-07-31 18:05:58 +0200 (Tue, 31 Jul 2007)
New Revision: 7881

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
   trunk/plearn_learners/meta/AdaBoost.h
Log:
- Removed dangerous use of static vector with non-fixed length
- Made inherited typedef private


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2007-07-31 15:50:33 UTC (rev 7880)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2007-07-31 16:05:58 UTC (rev 7881)
@@ -213,11 +213,12 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
-    deepCopyField(learners_error, copies);
-    deepCopyField(example_weights, copies);
-    deepCopyField(voting_weights, copies);
-    deepCopyField(weak_learners, copies);
-    deepCopyField(weak_learner_template, copies);
+    deepCopyField(learners_error,           copies);
+    deepCopyField(example_weights,          copies);
+    deepCopyField(weak_learner_output,      copies);
+    deepCopyField(voting_weights,           copies);
+    deepCopyField(weak_learners,            copies);
+    deepCopyField(weak_learner_template,    copies);
 }
 
 
@@ -616,7 +617,7 @@
 {
     output.resize(weak_learner_template->outputsize());
     real sum_out=0;
-    static Vec weak_learner_output(output.size());
+    weak_learner_output.resize(output.size());
     for (int i=0;i<voting_weights.length();i++)
     {
         weak_learners[i]->computeOutput(input,weak_learner_output);

Modified: trunk/plearn_learners/meta/AdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.h	2007-07-31 15:50:33 UTC (rev 7880)
+++ trunk/plearn_learners/meta/AdaBoost.h	2007-07-31 16:05:58 UTC (rev 7881)
@@ -50,7 +50,6 @@
 
 class AdaBoost: public PLearner
 {
-public:
     typedef PLearner inherited;
   
 protected:
@@ -59,6 +58,9 @@
     // weighing scheme over examples
     Vec example_weights;
 
+    //! Used to store outputs from the weak learners.
+    mutable Vec weak_learner_output;
+
     // *********************
     // * protected options *
     // *********************



From lamblin at mail.berlios.de  Tue Jul 31 20:30:52 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 31 Jul 2007 20:30:52 +0200
Subject: [Plearn-commits] r7882 - trunk/plearn_learners/online
Message-ID: <200707311830.l6VIUq6N006254@sheep.berlios.de>

Author: lamblin
Date: 2007-07-31 20:30:51 +0200 (Tue, 31 Jul 2007)
New Revision: 7882

Modified:
   trunk/plearn_learners/online/RBMMultinomialLayer.cc
Log:
Yet another unnormalized learning rate in mini-batch.


Modified: trunk/plearn_learners/online/RBMMultinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-07-31 16:05:58 UTC (rev 7881)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-07-31 18:30:51 UTC (rev 7882)
@@ -235,6 +235,7 @@
 
     // input_gradients[k][i] =
     //   (output_gradients[k][i]-output_gradients[k].outputs[k]) outputs[k][i]
+    real mean_lr = learning_rate / mbatch_size;
     for( int k=0; k<mbatch_size; k++ )
     {
         real outg_dot_out = dot( output_gradients(k), outputs(k) );
@@ -252,14 +253,14 @@
             if( momentum == 0. )
             {
                 // update the bias: bias -= learning_rate * input_gradient
-                b[i] -= learning_rate * ing_ki;
+                b[i] -= mean_lr * ing_ki;
             }
             else
             {
                 // The update rule becomes:
                 // bias_inc = momentum*bias_inc - learning_rate*input_gradient
                 // bias += bias_inc
-                binc[i] = momentum * binc[i] - learning_rate * ing_ki;
+                binc[i] = momentum * binc[i] - mean_lr * ing_ki;
                 b[i] += binc[i];
             }
         }
@@ -440,7 +441,7 @@
 
     for ( int i = 0; i < size; ++i ) {
         output[i] = i == conf_index ? 1 : 0;
-    }    
+    }
 }
 
 



From tihocan at mail.berlios.de  Tue Jul 31 20:34:21 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 31 Jul 2007 20:34:21 +0200
Subject: [Plearn-commits] r7883 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200707311834.l6VIYL13006547@sheep.berlios.de>

Author: tihocan
Date: 2007-07-31 20:34:20 +0200 (Tue, 31 Jul 2007)
New Revision: 7883

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.h
Log:
Added an option to choose whether the final update should be synchronized or not

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2007-07-31 18:30:51 UTC (rev 7882)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2007-07-31 18:34:20 UTC (rev 7883)
@@ -68,6 +68,7 @@
 
 NatGradSMPNNet::NatGradSMPNNet():
       delayed_update(true),
+      wait_for_final_update(true),
       noutputs(-1),
       params_averaging_coeff(1.0),
       params_averaging_freq(5),
@@ -112,6 +113,13 @@
         "turn to update. This ensures no two CPUs are modifying parameters\n"
         "at the same time.");
 
+    declareOption(ol, "wait_for_final_update",
+                  &NatGradSMPNNet::wait_for_final_update,
+                  OptionBase::buildoption,
+        "If true, each CPU will wait its turn before performing its final\n"
+        "update. It should impact performance only when 'delayed_update' is\n"
+        "also true.");
+
     declareOption(ol, "noutputs", &NatGradSMPNNet::noutputs,
                   OptionBase::buildoption,
                   "Number of outputs of the neural network, which can be derived from  output_type and targetsize_\n");
@@ -789,11 +797,33 @@
         */
     }
 
+    if (!wait_for_final_update) {
+        if (nsteps >  0) {
+            //printf("CPU %d final updating (nsteps =%d)\n", iam, nsteps);
+            if (delayed_update) {
+                all_params += params_update;
+                params_update.clear();
+            }
+            nsteps = 0;
+        }
+        // Indicate this CPU is done.
+        semun_v.val = 1;
+        semctl(semaphore_id, iam + 1, SETVAL, semun_v);
+        if (iam != 0) {
+            // Exit additional processes after training.
+            //printf("CPU %d exiting\n", iam);
+            exit(0);
+        }
+    }
+
+    Profiler::reset("Synchronization");
+    Profiler::start("Synchronization");
+
     // Wait until it is our turn.
     while (true) {
         int sem_value = semctl(semaphore_id, 0, GETVAL);
         if (sem_value == iam || iam == 0) {
-            if (sem_value == iam) {
+            if (sem_value == iam && wait_for_final_update) {
                 if (nsteps >  0) {
                     //printf("CPU %d final updating (nsteps =%d)\n", iam, nsteps);
                     if (delayed_update) {
@@ -845,6 +875,12 @@
         }
     }
 
+    Profiler::end("Synchronization");
+    const Profiler::Stats& synch_stats = Profiler::getStats("Synchronization");
+    real synch_time = (synch_stats.user_duration + synch_stats.system_duration)
+        / real(Profiler::ticksPerSecond());
+    //pout << "Synch time: " << synch_time << endl;
+
     // Free semaphore's ressources.
     if (semaphore_id >= 0) {
         int success = semctl(semaphore_id, 0, IPC_RMID);

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.h	2007-07-31 18:30:51 UTC (rev 7882)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.h	2007-07-31 18:34:20 UTC (rev 7883)
@@ -58,6 +58,7 @@
     //#####  Public Build Options  ############################################
 
     bool delayed_update;
+    bool wait_for_final_update;
 
     int noutputs;
 



From lamblin at mail.berlios.de  Tue Jul 31 21:04:45 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 31 Jul 2007 21:04:45 +0200
Subject: [Plearn-commits] r7884 - trunk/plearn_learners/online
Message-ID: <200707311904.l6VJ4jS6008825@sheep.berlios.de>

Author: lamblin
Date: 2007-07-31 21:04:45 +0200 (Tue, 31 Jul 2007)
New Revision: 7884

Modified:
   trunk/plearn_learners/online/RBMMultinomialLayer.cc
Log:
Prevent use of probably incorrect code.


Modified: trunk/plearn_learners/online/RBMMultinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-07-31 18:34:20 UTC (rev 7883)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-07-31 19:04:45 UTC (rev 7884)
@@ -257,6 +257,8 @@
             }
             else
             {
+                PLCHECK_MSG(false,
+                            "Momentum not correctly implemented with batch");
                 // The update rule becomes:
                 // bias_inc = momentum*bias_inc - learning_rate*input_gradient
                 // bias += bias_inc



From yoshua at mail.berlios.de  Tue Jul 31 21:42:45 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Tue, 31 Jul 2007 21:42:45 +0200
Subject: [Plearn-commits] r7885 - trunk/plearn_learners/online
Message-ID: <200707311942.l6VJgjND012168@sheep.berlios.de>

Author: yoshua
Date: 2007-07-31 21:42:44 +0200 (Tue, 31 Jul 2007)
New Revision: 7885

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
Class documentation added on the  newly added ports for RBMmodule


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-07-31 19:04:45 UTC (rev 7884)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-07-31 19:42:44 UTC (rev 7885)
@@ -69,6 +69,16 @@
     "    of the RBM. Computing it requires re-computing the partition function (which must\n"
     "    be recomputed if the parameters have changed) and takes O(2^{min(n_hidden,n_visible)})\n"
     "    computations of the free-energy.\n"
+    "  - 'neg_log_phidden' : use as an optional input port when asking for an output on\n"
+    "    the 'neg_log_pvisible_given_phidden' port. It is a a column matrix with one element\n"
+    "    -log w_h for each row h of the input 'hidden.state'. The w_h could be interpreted as\n"
+    "    probabilities, e.g. w_h = P(h) according to some prior probability P, and sum_w w_h=1\n"
+    "    over the set of h's provided in the 'hidden.state' port.\n"
+    "  - 'neg_log_pvisible_given_phidden' : this output port is used to ask the module to compute\n"
+    "    a column matrix with entries = -log( sum_h P(x|h) w_h ) for each row x in the input\n"
+    "    'visible' port. This quantity would be a valid - log P(x) if sum_h w_h = 1, under the\n"
+    "    joint model P(x,h) = P(x|h) P(h), with P(h)=w_h.\n"
+    "    \n"
     "An RBM also has other ports that exist only if some options are set.\n"
     "If reconstruction_connection is given, then it has\n"
     "  - 'visible_reconstruction_activations.state' : the deterministic reconstruction of the\n"



From lamblin at mail.berlios.de  Tue Jul 31 21:51:54 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 31 Jul 2007 21:51:54 +0200
Subject: [Plearn-commits] r7886 - in
	trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results:
	expdir-dbn-1-0/Split0 expdir-dbn-1-1/Split0 expdir-dbn-3-0
	expdir-dbn-3-0/Split0 expdir-dbn-3-1 expdir-dbn-3-1/Split0
Message-ID: <200707311951.l6VJpskV012851@sheep.berlios.de>

Author: lamblin
Date: 2007-07-31 21:51:48 +0200 (Tue, 31 Jul 2007)
New Revision: 7886

Modified:
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-0/Split0/final_learner.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-1/Split0/final_learner.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-0/Split0/final_learner.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-0/global_stats.pmat
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-0/split_stats.pmat
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-1/Split0/final_learner.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-1/global_stats.pmat
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-1/split_stats.pmat
Log:
Update test results after bugfix


Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-0/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-0/Split0/final_learner.psave	2007-07-31 19:42:44 UTC (rev 7885)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-0/Split0/final_learner.psave	2007-07-31 19:51:48 UTC (rev 7886)
@@ -34,7 +34,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 100 [ 3.22297502090243615 3.22850343798757411 -0.0655674198722276391 -0.0620531502143017999 -0.0694639415907335606 -0.0656967869581215891 -0.065802796160687807 -0.0676665281073794145 -0.0646008478691122434 -0.0630437566075335715 -0.0675636254713856965 -0.0633361959019119736 -0.0703945608316206561 -0.0690432124347641341 -0.0669936710720746009 -0.0641822519514770595 -0.0633215475695771413 -0.0696978904345579853 -0.0655365737634271694 -0.0703843254243849137 -0.0629756492516730521 -0.0643907155410799181 -0.0672190493187708044 -0.0652833514948132076 -0.0643953690592243555 -0.0698244386567378611 -0.068417804617699432 -0.0701758716581505348 -0.0640983927904117123 -0.0665726831989223305 -0.066404014349293411 -0.0616877132603564499 -0.0681706276454905474 -0.0612647343384444751 -0.0694090319721886173 -0.0688279871752820321 -0.065124427046754807 -0.0665585108343436371 -0.0617623458616671991 -0.0658212640585509184 -0.0630170514137075588 -0.0675889446130171906 -0.0638338044249902!
 314 -0.0655297688361054925 -0.0629649661551778261 -0.0698847192903202657 -0.0654629364108216816 -0.0706810954132212621 -0.0660285024336473153 -0.0695005153829066358 -0.0693978927744081514 -0.0706556651377230061 -0.069953748785900452 -0.0684365451391091423 -0.064539881485192116 -0.0647616493444652941 -0.0601662010225776159 -0.0679644403313143103 -0.0632144607207602016 -0.0636181859919972004 -0.0652850922111074539 -0.067444192856024418 -0.0678073923733376466 -0.0634914797452603014 -0.0617561876343265051 -0.0655494061977655029 -0.0627296640677947465 -0.0664271156689422576 -0.065479249890638011 -0.0704179681617658898 -0.0626083718349837032 -0.064150644361228526 -0.0642433194221907894 -0.0641793160292923603 -0.0648969996254884218 -0.0623260735216022987 -0.0677062572193665863 -0.0672066873746975935 -0.0647069715718852079 -0.0634935571762970802 -0.0661137956042305575 -0.0668230777159821848 -0.0650723733982817298 -0.061487132750550684 -0.0678324572139960141 -0.060563054793671578 -0!
 .0648627663702872337 -0.0662330568700430083 -0.065336260342353!
 4583 -0.
0634672818945638401 -0.0648919074644516009 -0.0651717128396615553 -0.0688591344930329918 -0.0666241742120582681 -0.0675404276392551489 -0.0654246462891826669 -0.0694808269603974654 -0.0656411525931498013 -0.0653383347553044974 -0.062874898273063487 ] ;
+bias = 100 [ 3.22297502090243659 3.22850343798757411 -0.0655674198722276252 -0.062053150214301786 -0.0694639415907335606 -0.0656967869581215752 -0.065802796160687807 -0.0676665281073794145 -0.0646008478691122157 -0.0630437566075335576 -0.0675636254713856965 -0.0633361959019119736 -0.07039456083162067 -0.0690432124347641341 -0.0669936710720746148 -0.0641822519514770734 -0.0633215475695771551 -0.0696978904345579992 -0.0655365737634271694 -0.0703843254243849137 -0.0629756492516730521 -0.0643907155410799181 -0.0672190493187708321 -0.0652833514948132215 -0.0643953690592243555 -0.0698244386567378611 -0.0684178046176994181 -0.070175871658150521 -0.0640983927904116846 -0.0665726831989223305 -0.0664040143492933971 -0.0616877132603564568 -0.0681706276454905474 -0.0612647343384444543 -0.0694090319721886173 -0.0688279871752820044 -0.065124427046754807 -0.0665585108343436371 -0.06176234586166722 -0.0658212640585509184 -0.0630170514137075588 -0.0675889446130172045 -0.0638338044249902314 !
 -0.0655297688361054925 -0.0629649661551778539 -0.0698847192903202657 -0.0654629364108216816 -0.0706810954132212621 -0.0660285024336473153 -0.0695005153829066358 -0.0693978927744081375 -0.0706556651377230061 -0.0699537487859004659 -0.0684365451391091284 -0.0645398814851921021 -0.0647616493444653218 -0.0601662010225776089 -0.0679644403313142687 -0.0632144607207602016 -0.0636181859919971865 -0.0652850922111074677 -0.0674441928560244042 -0.0678073923733376188 -0.0634914797452603014 -0.0617561876343264912 -0.0655494061977655029 -0.0627296640677947603 -0.0664271156689422437 -0.065479249890638011 -0.0704179681617658759 -0.0626083718349836893 -0.064150644361228526 -0.0642433194221907755 -0.0641793160292923603 -0.0648969996254884218 -0.0623260735216023057 -0.0677062572193665863 -0.0672066873746975657 -0.0647069715718852079 -0.0634935571762970802 -0.0661137956042305575 -0.0668230777159821848 -0.065072373398281716 -0.0614871327505506909 -0.0678324572139960141 -0.060563054793671571 -0.!
 0648627663702872337 -0.0662330568700429945 -0.0653362603423534!
 583 -0.0
634672818945638401 -0.0648919074644516009 -0.0651717128396615553 -0.0688591344930329918 -0.0666241742120582819 -0.0675404276392551489 -0.0654246462891826669 -0.0694808269603974654 -0.0656411525931498013 -0.0653383347553044974 -0.0628748982730635009 ] ;
 input_size = 100 ;
 output_size = 100 ;
 name = "RBMMultinomialLayer" ;
@@ -45,112 +45,114 @@
 ] ;
 connections = 1 [ *5 ->RBMMatrixConnection(
 weights = 100  2  [ 
-1.29220320314167814 	1.23360776084591173 	
+1.29220320314167791 	1.23360776084591173 	
 1.27872961480206149 	1.26508818796771783 	
-0.0069235175860512992 	-0.0596493482467894826 	
--0.114101462969043332 	-0.0580988569046345937 	
-0.0533565400353935779 	0.0170491494543559552 	
-0.0316429427052850806 	-0.0820287849666461455 	
-0.0351032969514864304 	-0.0817381734261192205 	
-0.0634469142848316514 	-0.0505021675106046772 	
-0.0141372893669295084 	-0.100685202722473471 	
+0.0069235175860513018 	-0.0596493482467894826 	
+-0.114101462969043332 	-0.0580988569046345868 	
+0.0533565400353935848 	0.0170491494543559517 	
+0.0316429427052850737 	-0.0820287849666461455 	
+0.0351032969514864235 	-0.0817381734261192205 	
+0.0634469142848316375 	-0.0505021675106046772 	
+0.014137289366929498 	-0.100685202722473471 	
 -0.102704511553851074 	-0.0351291077576920061 	
-0.0293595458726126149 	-0.0177656145449792691 	
--0.119403330826722831 	-0.00820792272976666841 	
-0.0404489624105316206 	0.0584112377699046725 	
-0.0149953341430634525 	0.0433154538714251511 	
+0.0293595458726126149 	-0.0177656145449792656 	
+-0.119403330826722831 	-0.00820792272976666147 	
+0.0404489624105316137 	0.0584112377699046725 	
+0.0149953341430634594 	0.0433154538714251372 	
 0.0541231875169777901 	-0.0622654971409482144 	
--0.0194619452202611058 	-0.0794991574421131941 	
+-0.0194619452202611093 	-0.0794991574421131941 	
 -0.0591309423094542805 	-0.0684592801496005793 	
-0.0601840005623679591 	0.0170691892439329487 	
--0.0136891222072263643 	-0.0395756093587932289 	
-0.0626097695542838922 	0.0351989189500988703 	
+0.0601840005623679453 	0.0170691892439329349 	
+-0.0136891222072263608 	-0.039575609358793222 	
+0.0626097695542838922 	0.0351989189500988772 	
 -0.0551720135255094613 	-0.0846389760005528818 	
--0.0849388090255431272 	-0.00635662292228355869 	
-0.00653694458356686176 	-0.00490949374195461322 	
-0.0251749745065769151 	-0.0887952349207536823 	
+-0.0849388090255431411 	-0.00635662292228355869 	
+0.00653694458356685482 	-0.00490949374195461755 	
+0.025174974506576929 	-0.0887952349207536823 	
 -0.108549115284084272 	0.0158403099288679516 	
-0.0421390225675163516 	0.0392398494581973761 	
-0.0154309454120496372 	0.0236842030028510397 	
-0.0413412467827884503 	0.0504401859701714764 	
--0.0191193899502736173 	-0.0825103594157753778 	
-0.0192847607045263797 	-0.0395532286748599901 	
-0.0669408894051742309 	-0.097013099502201855 	
--0.0910475086595164268 	-0.0953504375875404292 	
-0.0213870941412512526 	0.00954489994435698576 	
+0.0421390225675163585 	0.0392398494581973761 	
+0.0154309454120496441 	0.0236842030028510397 	
+0.0413412467827884642 	0.0504401859701714833 	
+-0.0191193899502736138 	-0.08251035941577535 	
+0.0192847607045263832 	-0.0395532286748599901 	
+0.0669408894051742587 	-0.0970130995022018688 	
+-0.0910475086595164268 	-0.0953504375875404153 	
+0.0213870941412512595 	0.00954489994435697882 	
 -0.120632544614303922 	-0.0796475324973987819 	
-0.0232968722877611653 	0.0456841016756075052 	
--0.0133317017153690587 	0.0646234660009369632 	
--0.0852584387614009065 	0.0180954559630503303 	
+0.0232968722877611584 	0.0456841016756074983 	
+-0.0133317017153690605 	0.0646234660009369355 	
+-0.0852584387614009065 	0.0180954559630503165 	
 0.0528352112592731377 	-0.0761141873516854583 	
 -0.101081872518667654 	-0.0819376680973578908 	
--0.0459535875418154091 	0.00213824328063650596 	
--0.029106554106003997 	-0.110727468696116979 	
--0.00395890797306441312 	0.0175274950300004229 	
+-0.0459535875418153952 	0.00213824328063651464 	
+-0.0291065541060039762 	-0.110727468696116979 	
+-0.00395890797306439925 	0.0175274950300004298 	
 -0.0215090092317983955 	-0.0896712774928562911 	
--0.121964926858960737 	0.065767705001880894 	
--0.0401380015563763265 	-0.101276123247230621 	
-0.0284324854550349615 	0.0548329660062637989 	
--0.0470073236675853032 	-0.00839467261894549961 	
+-0.121964926858960723 	0.0657677050018809078 	
+-0.0401380015563763196 	-0.101276123247230621 	
+0.0284324854550349684 	0.0548329660062637989 	
+-0.0470073236675853032 	-0.00839467261894551002 	
 0.050587620206005407 	0.0563509932653329312 	
--0.0690597481883651748 	0.0320654114562946083 	
-0.0473208662992663856 	0.0241074559380521046 	
-0.0635123940506462092 	0.00420828933025830382 	
-0.0545511436002203351 	0.0514803846774994078 	
-0.0649765752170751026 	0.0198470196543508032 	
-0.02964339785624533 	0.00993493870292420402 	
-0.023742498414394124 	-0.114145802727042472 	
--0.00187765399950314622 	-0.0778128677930624846 	
--0.116604292498409964 	-0.124884172554877521 	
--0.0122602490947308022 	0.0374355868395198008 	
+-0.0690597481883651748 	0.0320654114562946221 	
+0.0473208662992663717 	0.0241074559380521324 	
+0.0635123940506462092 	0.00420828933025829861 	
+0.0545511436002203212 	0.0514803846774994078 	
+0.0649765752170751026 	0.0198470196543507962 	
+0.0296433978562453126 	0.00993493870292423004 	
+0.0237424984143941274 	-0.114145802727042472 	
+-0.00187765399950315642 	-0.0778128677930624846 	
+-0.11660429249840995 	-0.124884172554877521 	
+-0.0122602490947308022 	0.0374355868395197869 	
 -0.0553151493711838416 	-0.0760245423841495044 	
--0.0289047938738029066 	-0.0903658074172669773 	
--0.0538309537866579063 	-0.00676968804299703329 	
--0.0306764133596947458 	0.0392628660717239189 	
-0.0233107731838695757 	-0.00345790159285788235 	
--0.0389139668079438519 	-0.0840691835333684523 	
--0.120854759137730527 	-0.0619500968665968807 	
--0.110200228557434984 	0.0554872308335307499 	
--0.0411706677281680408 	-0.108628478414043314 	
--0.018724214606506244 	-0.00456879672499676745 	
--0.0167862916672534661 	-0.0382422711585358038 	
-0.062751256521479129 	0.0362753147015339658 	
--0.0348078718895850139 	-0.120238587017427415 	
--0.110455941578583558 	0.00991234463564253168 	
--0.0851807528351593291 	-0.0111410784080641714 	
--0.0339580751298678152 	-0.0640107816250453932 	
--0.00353598286695873962 	-0.0718191036224426066 	
--0.0677595142334457418 	-0.0964041755975349191 	
-0.0213022386562033324 	-0.00517260018543139275 	
-0.0414241226751148675 	-0.0419398819675565748 	
+-0.0289047938738029066 	-0.0903658074172669634 	
+-0.0538309537866579202 	-0.00676968804299702548 	
+-0.0306764133596947423 	0.0392628660717239467 	
+0.0233107731838695861 	-0.00345790159285788712 	
+-0.038913966807943845 	-0.0840691835333684523 	
+-0.1208547591377305 	-0.0619500968665968668 	
+-0.110200228557434984 	0.055487230833530736 	
+-0.0411706677281680478 	-0.108628478414043314 	
+-0.0187242146065062405 	-0.00456879672499675878 	
+-0.0167862916672534696 	-0.0382422711585357969 	
+0.0627512565214791151 	0.0362753147015339727 	
+-0.0348078718895850139 	-0.120238587017427401 	
+-0.110455941578583558 	0.00991234463564251607 	
+-0.0851807528351593429 	-0.0111410784080641697 	
+-0.0339580751298678082 	-0.064010781625045407 	
+-0.00353598286695875046 	-0.0718191036224426066 	
+-0.0677595142334457279 	-0.0964041755975349052 	
+0.0213022386562033324 	-0.00517260018543138581 	
+0.0414241226751148814 	-0.0419398819675565818 	
 0.0382120973673575268 	-0.123359848456372476 	
 -0.0929881295964480187 	-0.028957920723804504 	
--0.0247109665646048776 	-0.00951714433744917793 	
-0.0126175422127599128 	-0.0241006135509846675 	
--0.101730186036494891 	0.0316157246083580354 	
--0.104029401939315827 	-0.0890897101669621966 	
-0.0316467628517260596 	-0.0115046536306472325 	
--0.109160552150135107 	-0.118572727552158905 	
-0.00226257592330038554 	-0.0796744517052890427 	
--0.0428683774538702808 	0.0130289732273120747 	
--0.0164520832841672932 	-0.0433770308053599285 	
--0.10714684080366399 	-0.0166419341475811111 	
--0.0587359674711478361 	-0.0153743704526102826 	
--0.0115441064666440829 	-0.0542664942214843987 	
-0.00150219021435402241 	0.0512020860335165931 	
--0.0124047942645855273 	-0.00513744872628248399 	
--0.0492572403497689507 	0.060631313068757027 	
-0.0204587837638680638 	-0.0793406716447456889 	
-0.0289676756495262575 	0.0423600752752206211 	
--0.0829466709775956157 	0.0328361137204496237 	
+-0.0247109665646048741 	-0.0095171443374491814 	
+0.0126175422127598955 	-0.0241006135509846675 	
+-0.101730186036494905 	0.0316157246083580354 	
+-0.104029401939315813 	-0.0890897101669621966 	
+0.0316467628517260943 	-0.0115046536306472307 	
+-0.109160552150135121 	-0.118572727552158932 	
+0.0022625759233003695 	-0.0796744517052890427 	
+-0.0428683774538702808 	0.0130289732273120729 	
+-0.0164520832841672862 	-0.0433770308053599285 	
+-0.107146840803663976 	-0.0166419341475811042 	
+-0.0587359674711478499 	-0.0153743704526102878 	
+-0.0115441064666440916 	-0.0542664942214843918 	
+0.00150219021435402328 	0.0512020860335166139 	
+-0.0124047942645855256 	-0.00513744872628248746 	
+-0.0492572403497689437 	0.0606313130687570201 	
+0.0204587837638680707 	-0.0793406716447456889 	
+0.0289676756495262401 	0.0423600752752206211 	
+-0.0829466709775956157 	0.0328361137204496514 	
 0.0242568225615223537 	-0.0860029078247910583 	
--0.0401690630785093844 	-0.104937097054671441 	
+-0.0401690630785093983 	-0.104937097054671441 	
 ]
 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
+L1_penalty_factor = 0 ;
+L2_penalty_factor = 0 ;
 down_size = 2 ;
 up_size = 100 ;
 learning_rate = 0.0100000000000000002 ;
@@ -181,11 +183,11 @@
 L1_penalty_factor = 0 ;
 L2_penalty_factor = 0 ;
 weights = 2  100  [ 
-0.0571179363884330299 	0.0501099146229690012 	-0.00283641365247566321 	-0.00951556532440867887 	0.0016087601981925239 	-0.00935152806483453662 	-0.00510740503617761622 	0.00287368433171549366 	-0.00675304534641369119 	-0.00413678788857376519 	0.00850030537003726959 	-0.0111684868748137111 	-0.00780306127119203066 	-0.00814942063470409928 	-0.00954679294574281231 	-0.000614088438469789929 	-0.00731516352452465587 	-0.00282304865036278347 	0.00748388346081026255 	-0.0023990470393796281 	-0.00951902894901785444 	-0.00129605959454318141 	-0.0107927282259586976 	-0.00342207145289506265 	0.00580774854206700386 	-0.00247776382757305047 	-0.00780547079113386253 	0.0069294054970583099 	-0.00824672112506309209 	0.00713823191991569926 	0.00664356302299650738 	0.000940279142242215755 	-0.00102485514370707453 	-0.00932895795875588775 	0.00193833827130056747 	-0.000361220844477941339 	-0.00861518843955716949 	0.00815415350633161111 	-0.00131097896514859903 	0.000974099005917072251 	-0.01!
 03129990867254634 	-0.00575765436393235394 	-0.00452824694320300234 	0.00556878198151231595 	-0.00102490057129662137 	0.00455297491978201487 	0.00222249556539440719 	0.00266082274728519919 	-0.00740263579388739535 	-0.00210709456730666682 	0.00352362954929475369 	0.00654536123261181563 	-0.00959748701237313206 	-0.0101344825179493572 	0.00326867246220256639 	-0.0105244789083738365 	-0.0107385022345107932 	-0.00160905283676741353 	-0.00289771267188897131 	0.00203421491244993972 	-0.00567387788282249558 	0.00341582289983727175 	0.000530202561715857113 	0.00677671540668820481 	-0.00754837941802427061 	0.0006229809090904973 	0.0058020471328928732 	-0.0104643646725042557 	-0.00141249642388021897 	-0.00928429527767735824 	-0.00468360331918606185 	0.00499672932982297372 	0.00616881440923875823 	-0.00939918330075985298 	0.00845612416447832915 	0.00398191050011537009 	0.00574267253302497387 	-0.00666732884012975237 	-0.0109720985382393051 	-0.00153878814978031719 	0.0075306508608097!
 9482 	-0.00836043286960834464 	0.0043107997290448034 	-0.00515!
 77966711
5612208 	0.00639998094998550388 	-0.00349161079685592329 	0.00742192480911468486 	0.000247062267102248563 	-0.000498515317870227475 	0.00848067040967163954 	0.00447065128420599464 	-0.0012345618400069001 	0.000244823631494026962 	0.000862566839725849993 	-0.0106291315660229577 	0.00532573396950207587 	-0.00838831631330394685 	0.00448531943412492067 	-0.00367181364364815507 	0.00256206525739502856 	
--0.0559497281963303936 	-0.041493601745693949 	0.00816136129091281533 	0.00297350826470341905 	0.00842118677256085274 	-0.00742463991835468594 	0.00761691044965026598 	0.00836329430720171227 	0.00417769591120346982 	-0.00530274910109274512 	0.00922971816493122627 	0.00259962205615130865 	0.00648146663523604082 	-0.00107244774526877662 	0.00437946224670130412 	0.00943456334063768817 	0.00387939226877071462 	0.00214443124558895458 	0.00820215524843536173 	-0.00287122651569053601 	0.00469547062313856872 	0.00239118131162365617 	-1.02847616400715141e-05 	0.00951312098995770253 	-0.00673820581090856137 	-0.00799643028372943286 	-0.000728721498091911024 	-0.00535700685591159863 	0.00806382853802718388 	0.00545905216427026909 	-0.00284987087191196494 	-0.00296736261924756585 	-0.00694075009660812849 	0.0111507053483779275 	-0.00673948189798301376 	-0.000347497501953908719 	-0.00769506991915977923 	0.00179027700418187322 	0.00396780180303779508 	-0.00704438600071702684 	-0.00022332!
 8298618172169 	0.00666666847066049381 	0.00359073157638053336 	0.0109503938003560793 	0.00211329617895435196 	-0.00399971779435220279 	-0.00111717748196656303 	0.0037209327286009679 	0.0031364406735974815 	-0.00741163979839500921 	0.00173512793671334962 	0.00316577228843840713 	-0.00580133601046927631 	0.00300858294509262405 	-0.00455652470386023415 	0.00244895568092062636 	0.00710274627088726754 	0.00770279992638437435 	0.0109556578982081369 	-0.00799565082928353418 	0.0093635509508243446 	0.00771946867841088492 	0.00940856670895008455 	0.00301787987211131568 	0.00720031492370501198 	0.00491945327944680322 	0.0104821648715004538 	0.00501612225124228785 	0.000874085888797997254 	0.000209857217043509637 	-0.00799541294962607778 	0.00171497130334525151 	0.00806099838512624317 	0.011044704198342024 	0.0111818627413944934 	-0.00838162626922003308 	-0.00422252560522866707 	-0.00256498877889595623 	0.000453533458110704207 	0.000828846711381063701 	0.000458291316225098943 	-0.0017!
 9087939779941862 	-0.00752453697748784248 	-0.0012235517701988!
 1382 	0.
00854851613398391157 	-0.00395146654386617675 	-0.00224278948002464224 	0.00890248280081903931 	0.00208821612568750592 	-0.00191103528200058646 	0.00860363678133929839 	-0.00446889954439381749 	0.00873192332904906832 	0.000271146672955575863 	0.00661216754954073015 	0.00592254381482310802 	-0.00808308707686518357 	0.0112175035891868662 	0.00685471069916975938 	-0.00286792829416912792 	
+0.0571179363884330715 	0.0501099146229689527 	-0.00283641365247566234 	-0.00951556532440867887 	0.00160876019819252369 	-0.00935152806483453836 	-0.00510740503617761622 	0.00287368433171549366 	-0.00675304534641368946 	-0.00413678788857376519 	0.00850030537003727132 	-0.0111684868748137111 	-0.00780306127119202719 	-0.00814942063470409928 	-0.00954679294574280884 	-0.000614088438469789061 	-0.00731516352452465501 	-0.0028230486503627826 	0.00748388346081026168 	-0.00239904703937962724 	-0.00951902894901785444 	-0.00129605959454318293 	-0.0107927282259586959 	-0.00342207145289506265 	0.00580774854206700473 	-0.0024777638275730496 	-0.00780547079113386166 	0.0069294054970583099 	-0.00824672112506308688 	0.00713823191991569926 	0.00664356302299650738 	0.000940279142242216406 	-0.0010248551437070741 	-0.00932895795875588948 	0.0019383382713005692 	-0.000361220844477940796 	-0.00861518843955717123 	0.00815415350633160937 	-0.00131097896514859947 	0.000974099005917070733 	-0.0103!
 129990867254599 	-0.00575765436393235394 	-0.00452824694320300147 	0.00556878198151231681 	-0.0010249005712966218 	0.004552974919782014 	0.00222249556539440762 	0.00266082274728519832 	-0.00740263579388739362 	-0.00210709456730666638 	0.00352362954929475369 	0.00654536123261181737 	-0.0095974870123731338 	-0.0101344825179493607 	0.00326867246220256596 	-0.0105244789083738383 	-0.0107385022345107915 	-0.00160905283676741439 	-0.00289771267188897131 	0.00203421491244993972 	-0.00567387788282249471 	0.00341582289983727132 	0.000530202561715857222 	0.00677671540668820307 	-0.00754837941802427234 	0.000622980909090498601 	0.00580204713289287147 	-0.010464364672504254 	-0.00141249642388021853 	-0.00928429527767735997 	-0.00468360331918606185 	0.00499672932982297285 	0.00616881440923875996 	-0.00939918330075985124 	0.00845612416447832915 	0.00398191050011536835 	0.00574267253302497213 	-0.0066673288401297515 	-0.0109720985382393086 	-0.00153878814978031697 	0.00753065086080979308 !
 	-0.00836043286960834464 	0.00431079972904480253 	-0.005157796!
 67115612
295 	0.00639998094998550388 	-0.00349161079685592372 	0.00742192480911468139 	0.000247062267102250406 	-0.00049851531787022845 	0.00848067040967163607 	0.00447065128420599551 	-0.00123456184000690075 	0.000244823631494027938 	0.000862566839725850102 	-0.0106291315660229611 	0.00532573396950207847 	-0.00838831631330394685 	0.0044853194341249198 	-0.00367181364364815377 	0.00256206525739502682 	
+-0.0559497281963304352 	-0.0414936017456938935 	0.00816136129091281359 	0.00297350826470341861 	0.00842118677256085274 	-0.00742463991835468594 	0.00761691044965026598 	0.008363294307201714 	0.00417769591120347328 	-0.00530274910109274251 	0.00922971816493122627 	0.00259962205615130865 	0.00648146663523603908 	-0.00107244774526877771 	0.00437946224670130238 	0.00943456334063768817 	0.00387939226877071506 	0.00214443124558895588 	0.00820215524843536173 	-0.00287122651569053644 	0.00469547062313856872 	0.00239118131162365357 	-1.02847616400707399e-05 	0.00951312098995769732 	-0.0067382058109085579 	-0.00799643028372943286 	-0.000728721498091911349 	-0.0053570068559115969 	0.00806382853802718735 	0.00545905216427026996 	-0.00284987087191196581 	-0.00296736261924756715 	-0.00694075009660812675 	0.0111507053483779327 	-0.00673948189798301463 	-0.000347497501953908123 	-0.00769506991915977923 	0.00179027700418187365 	0.00396780180303779595 	-0.00704438600071702684 	-0.00022332829!
 8618172413 	0.00666666847066049294 	0.0035907315763805325 	0.0109503938003560793 	0.00211329617895435196 	-0.00399971779435220366 	-0.00111717748196656238 	0.0037209327286009679 	0.00313644067359748323 	-0.00741163979839501007 	0.00173512793671334853 	0.00316577228843840713 	-0.00580133601046927804 	0.00300858294509262448 	-0.00455652470386023242 	0.0024489556809206268 	0.00710274627088726841 	0.00770279992638437608 	0.0109556578982081369 	-0.00799565082928353418 	0.0093635509508243446 	0.00771946867841088492 	0.00940856670895008629 	0.00301787987211131481 	0.00720031492370501285 	0.00491945327944680409 	0.0104821648715004538 	0.00501612225124228959 	0.000874085888797998122 	0.000209857217043509555 	-0.00799541294962607604 	0.00171497130334524977 	0.0080609983851262397 	0.0110447041983420223 	0.0111818627413944934 	-0.00838162626922003308 	-0.00422252560522866794 	-0.0025649887788959558 	0.000453533458110704857 	0.000828846711381064243 	0.000458291316225099702 	-0.001790879!
 39779941905 	-0.00752453697748784421 	-0.0012235517701988136 	!
 0.008548
51613398391157 	-0.00395146654386617675 	-0.0022427894800246418 	0.00890248280081903758 	0.00208821612568750635 	-0.00191103528200058472 	0.00860363678133929839 	-0.00446889954439381835 	0.00873192332904906658 	0.000271146672955575483 	0.00661216754954073189 	0.00592254381482310889 	-0.00808308707686518184 	0.0112175035891868679 	0.00685471069916975765 	-0.00286792829416912835 	
 ]
 ;
-bias = 2 [ -0.0322402029767411905 0.0322402029767412598 ] ;
+bias = 2 [ -0.0322402029767411766 0.0322402029767412668 ] ;
 input_size = 100 ;
 output_size = 2 ;
 name = "GradNNetLayerModule" ;
@@ -261,6 +263,7 @@
 gibbs_down_state = 1 [ 0  0  [ 
 ]
 ] ;
+random_gen = *3  ;
 seed = 1827 ;
 stage = 2000 ;
 n_examples = 4 ;

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-1/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-1/Split0/final_learner.psave	2007-07-31 19:42:44 UTC (rev 7885)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-1/Split0/final_learner.psave	2007-07-31 19:51:48 UTC (rev 7886)
@@ -34,7 +34,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 100 [ 4.33273938062738928 4.34213993410700283 -0.088204571348629579 -0.0839684393536935664 -0.0928823561132764292 -0.0883649721278776668 -0.0884903050550344544 -0.0907308059625350216 -0.0870444767617748333 -0.0851670367578043069 -0.0906043378833479851 -0.0855157280731373287 -0.093994825582694605 -0.0923762769413334628 -0.0899213889269599026 -0.0865376360683695195 -0.0854988138212786908 -0.0931633295456289895 -0.0881682224466178444 -0.0939859157720970839 -0.0850815970387604481 -0.0867878324914827526 -0.0901873951672764601 -0.0878652643364732922 -0.0867982308213180176 -0.0933152233563941808 -0.0916264905154507009 -0.0937369638746636535 -0.0864351187425157208 -0.0894153298628880683 -0.0892212011575993919 -0.0835338549363071475 -0.0913327598696803272 -0.0830141340166982988 -0.0928174262541913153 -0.0921193265512855047 -0.0876716699574961733 -0.089403450459274475 -0.0836202697787554045 -0.0885111489819176561 -0.0851349794678951111 -0.0906305634336227184 -0.086118935539891!
 0561 -0.0881613358275717129 -0.085073325097997185 -0.0933873403560817328 -0.0880793734547384716 -0.0943401228795518326 -0.0887566532073704034 -0.0929276378631985339 -0.0928055814139878005 -0.0943105206420898068 -0.0934701923530768114 -0.0916486244331431166 -0.0869773547786528434 -0.0872351452710243142 -0.0816886634694239666 -0.0910816766531487854 -0.0853692782416057144 -0.0858641028921361266 -0.0878611086369744665 -0.0904581686504181276 -0.0908946202910138473 -0.0857090460576450386 -0.0836093889250728167 -0.0881840540347646057 -0.0847895277075860904 -0.0892344597301107101 -0.0880987817164715359 -0.0940247149622682188 -0.0846467093966444262 -0.0865013358775244268 -0.0866108410192989037 -0.086530102032007214 -0.0874001930942246985 -0.0843070574656997057 -0.0907767781573203814 -0.0901769535117404458 -0.0871757644291058509 -0.0857078607918897312 -0.088862835014407851 -0.0897134775637553616 -0.0876126284089064483 -0.0832887442018750007 -0.0909265836630956253 -0.08217540835519779!
 32 -0.0873627859717287791 -0.0890024063612101651 -0.0879269268!
 68388072
3 -0.0856800091525585228 -0.087390743888033473 -0.0877310087418665951 -0.0921551835833112065 -0.0894747740662434343 -0.0905720902783772042 -0.0880371305006222432 -0.0929021937092538164 -0.088292093490187698 -0.0879317082348496093 -0.0849675822040164286 ] ;
+bias = 100 [ 4.33273938062738928 4.34213993410700283 -0.088204571348629579 -0.0839684393536935664 -0.0928823561132764153 -0.0883649721278776668 -0.0884903050550344406 -0.0907308059625350216 -0.0870444767617748333 -0.0851670367578043069 -0.0906043378833479851 -0.0855157280731373287 -0.0939948255826946327 -0.0923762769413334905 -0.0899213889269599026 -0.0865376360683695056 -0.0854988138212787047 -0.0931633295456290034 -0.0881682224466178305 -0.0939859157720970839 -0.0850815970387604203 -0.0867878324914827387 -0.090187395167276474 -0.087865264336473306 -0.0867982308213179898 -0.0933152233563941946 -0.0916264905154507009 -0.0937369638746636119 -0.0864351187425157208 -0.0894153298628880822 -0.0892212011575993919 -0.0835338549363071337 -0.0913327598696803411 -0.0830141340166982711 -0.0928174262541913292 -0.0921193265512855047 -0.0876716699574961456 -0.0894034504592744472 -0.0836202697787553906 -0.08851114898191767 -0.0851349794678951249 -0.0906305634336227184 -0.08611893553989107!
  -0.0881613358275717129 -0.0850733250979972266 -0.0933873403560817467 -0.0880793734547384993 -0.0943401228795518326 -0.0887566532073703895 -0.0929276378631985339 -0.0928055814139877866 -0.0943105206420898068 -0.0934701923530768253 -0.0916486244331431166 -0.0869773547786528711 -0.0872351452710243142 -0.0816886634694239527 -0.0910816766531487854 -0.0853692782416057006 -0.0858641028921361266 -0.0878611086369744665 -0.0904581686504181415 -0.0908946202910138334 -0.0857090460576450247 -0.0836093889250728445 -0.0881840540347645918 -0.0847895277075860487 -0.0892344597301107101 -0.088098781716471522 -0.0940247149622682327 -0.0846467093966444262 -0.0865013358775244268 -0.0866108410192989314 -0.0865301020320072001 -0.0874001930942247263 -0.0843070574656997196 -0.0907767781573203675 -0.0901769535117404458 -0.0871757644291058231 -0.085707860791889745 -0.0888628350144078649 -0.0897134775637553339 -0.0876126284089064483 -0.0832887442018749868 -0.0909265836630956392 -0.0821754083551977932 !
 -0.0873627859717287653 -0.089002406361210179 -0.08792692686838!
 80584 -0
.0856800091525585228 -0.0873907438880334869 -0.0877310087418665951 -0.0921551835833112065 -0.0894747740662434204 -0.0905720902783771903 -0.0880371305006222571 -0.0929021937092538025 -0.0882920934901876842 -0.0879317082348495954 -0.0849675822040164286 ] ;
 input_size = 100 ;
 output_size = 100 ;
 name = "RBMMultinomialLayer" ;
@@ -45,105 +45,105 @@
 ] ;
 connections = 1 [ *5 ->RBMMatrixConnection(
 weights = 100  2  [ 
-1.29857318855224602 	1.36455047376715077 	
+1.29857318855224557 	1.36455047376715011 	
 1.29503014985172449 	1.37722523831473675 	
--0.000252780665954720218 	-0.0663565453561717616 	
--0.120492482610137133 	-0.0645854320225070361 	
-0.0456893983539013032 	0.00972867070355152262 	
-0.0243550790860833297 	-0.0886339901078360276 	
-0.0277726720773790944 	-0.0883624560091030575 	
-0.0558589857291091341 	-0.0573831943664615332 	
-0.0069934384091225937 	-0.107146230578152063 	
--0.109192150211851011 	-0.0417584880666270267 	
-0.0219514948606382432 	-0.024786226207574643 	
--0.125854973288939742 	-0.0150065264583970626 	
-0.0327746851195726691 	0.0508174379487866673 	
-0.00754614672980853662 	0.0358965596485881916 	
-0.0466149485486741788 	-0.0690559363149078204 	
--0.0264179334804802504 	-0.0860260390623684701 	
--0.0658485990481896821 	-0.0749808175081218276 	
-0.052468480768551172 	0.00973441317483856629 	
--0.0207510229337725359 	-0.0463586678886904191 	
-0.0548390937624918945 	0.0277343286268516252 	
--0.0618894708438075605 	-0.0910711651982611115 	
+-0.000252780665954722061 	-0.0663565453561717755 	
+-0.120492482610137133 	-0.0645854320225070222 	
+0.0456893983539012963 	0.00972867070355151742 	
+0.0243550790860833262 	-0.0886339901078360137 	
+0.0277726720773790632 	-0.0883624560091030437 	
+0.0558589857291091271 	-0.057383194366461554 	
+0.00699343840912258936 	-0.107146230578152035 	
+-0.109192150211851025 	-0.0417584880666270267 	
+0.0219514948606382397 	-0.024786226207574643 	
+-0.125854973288939714 	-0.0150065264583970643 	
+0.0327746851195726344 	0.0508174379487866604 	
+0.00754614672980854009 	0.0358965596485881777 	
+0.0466149485486741788 	-0.0690559363149078342 	
+-0.0264179334804802469 	-0.0860260390623684701 	
+-0.065848599048189696 	-0.0749808175081218276 	
+0.0524684807685511581 	0.00973441317483855588 	
+-0.0207510229337725359 	-0.0463586678886903983 	
+0.0548390937624919014 	0.0277343286268516183 	
+-0.0618894708438075675 	-0.0910711651982611253 	
 -0.0916013181680967875 	-0.0132164241331492651 	
--0.000750215746710206619 	-0.0119785452684900116 	
-0.0179277584065802437 	-0.0953548503092788852 	
--0.115090264040634674 	0.00891037791005744838 	
-0.0345144494722095727 	0.0318002140111754983 	
-0.00802274509847889991 	0.0164028539955009892 	
-0.0337013262646112879 	0.0429289556612899095 	
--0.0260816807507645611 	-0.0890266673801552166 	
-0.0119986734108657508 	-0.0464038488597266693 	
+-0.000750215746710216811 	-0.0119785452684900151 	
+0.0179277584065802541 	-0.0953548503092788713 	
+-0.115090264040634674 	0.00891037791005745705 	
+0.0345144494722095727 	0.0318002140111754913 	
+0.00802274509847889818 	0.0164028539955009961 	
+0.0337013262646112879 	0.0429289556612898957 	
+-0.0260816807507645472 	-0.0890266673801552166 	
+0.0119986734108657508 	-0.0464038488597266416 	
 0.0594543527503941377 	-0.103589035001864102 	
--0.0974894342862974717 	-0.101622646678464976 	
-0.013982098258917228 	0.00236285580240335217 	
--0.126956301839784597 	-0.0860030077220637484 	
-0.0157963719524664842 	0.0382470126871552046 	
--0.0206234009737232341 	0.0571259148886519713 	
--0.0919656417300176582 	0.0110777468755206502 	
-0.0453985648840396633 	-0.0827951972378504208 	
--0.107500047927003572 	-0.0882879545551681216 	
--0.0528762439290722738 	-0.00484062810201232787 	
+-0.0974894342862974578 	-0.101622646678464976 	
+0.0139820982589172419 	0.0023628558024033396 	
+-0.126956301839784597 	-0.0860030077220637346 	
+0.0157963719524664634 	0.0382470126871552046 	
+-0.0206234009737232445 	0.0571259148886519644 	
+-0.0919656417300176443 	0.0110777468755206398 	
+0.0453985648840396563 	-0.0827951972378503931 	
+-0.107500047927003572 	-0.0882879545551681355 	
+-0.0528762439290722669 	-0.00484062810201233134 	
 -0.0359387819954649496 	-0.117042364171918434 	
--0.0112224488366410788 	0.0103250371647676305 	
--0.0284284782490021072 	-0.0961280713635520173 	
--0.128538514008866955 	0.0584897876081584833 	
--0.0469042015390775807 	-0.107618930689975731 	
-0.0208727968646322259 	0.0473177452652355918 	
--0.0539056974226136409 	-0.0153078294554692268 	
-0.0428540411435972268 	0.0487613042436001592 	
--0.0759102158970440949 	0.024902608440362517 	
-0.039694277685536572 	0.0167625801937488807 	
-0.0558099007941625438 	-0.00303854988471284138 	
-0.0468036811197067512 	0.0439206048260738849 	
-0.0572203209597936524 	0.0124824390985034117 	
-0.0221576094536766431 	0.00271125886158923206 	
-0.0165930449959407963 	-0.120524172810695052 	
--0.00896032649170553155 	-0.0843883802869024591 	
--0.122871642717333862 	-0.130970016326819622 	
--0.0195060329534726962 	0.0301163330459929372 	
--0.0620471976943272469 	-0.0825096519951731416 	
--0.0357446869289266209 	-0.0967808757306874573 	
--0.0607079842258980429 	-0.0137072352750048469 	
--0.0377946640426320563 	0.0319781379436551647 	
+-0.0112224488366410597 	0.0103250371647676323 	
+-0.0284284782490021141 	-0.0961280713635520173 	
+-0.128538514008866955 	0.0584897876081584764 	
+-0.046904201539077553 	-0.107618930689975759 	
+0.0208727968646322398 	0.0473177452652355918 	
+-0.0539056974226136479 	-0.0153078294554692407 	
+0.042854041143597206 	0.0487613042436001662 	
+-0.075910215897044081 	0.024902608440362517 	
+0.039694277685536572 	0.0167625801937488876 	
+0.05580990079416253 	-0.00303854988471285309 	
+0.0468036811197067373 	0.0439206048260738988 	
+0.0572203209597936385 	0.01248243909850341 	
+0.0221576094536766396 	0.00271125886158923553 	
+0.0165930449959407963 	-0.120524172810695065 	
+-0.0089603264917055437 	-0.0843883802869024591 	
+-0.122871642717333848 	-0.130970016326819622 	
+-0.0195060329534726996 	0.0301163330459929234 	
+-0.0620471976943272399 	-0.0825096519951731416 	
+-0.035744686928926607 	-0.0967808757306874712 	
+-0.0607079842258980082 	-0.01370723527500484 	
+-0.0377946640426320493 	0.0319781379436551716 	
 0.0159043582945675409 	-0.0105709436057683012 	
 -0.0457106238823807784 	-0.0905114283836155609 	
--0.127200309513792131 	-0.068404463933075213 	
--0.116826845402778517 	0.0482588080762488963 	
--0.047918728355812186 	-0.114926464365377595 	
--0.0258426034880490547 	-0.0115929898431668954 	
--0.023832047400995135 	-0.0450295642345298688 	
-0.0549673432422954633 	0.0287941570930679257 	
--0.0415614860833058153 	-0.126468135852143448 	
--0.116981703220581795 	0.00300855899836534671 	
--0.091830196492139235 	-0.0179658301158359505 	
--0.0408627699492909238 	-0.0706212091823788801 	
--0.0106033827865716045 	-0.0784176146443308214 	
--0.0743325188467987286 	-0.102701653534542497 	
-0.0139361477518215471 	-0.0122491885067015586 	
-0.0339784394229312309 	-0.0488284680760181014 	
-0.0309452094450517849 	-0.129728345547099222 	
--0.0995532082202472968 	-0.0356519795916181367 	
--0.0317522386420212993 	-0.0164652758150018452 	
-0.00533389366990226748 	-0.0310484595693043437 	
--0.108348407801560062 	0.0245639666817451807 	
--0.110414329531041536 	-0.095388678428209514 	
-0.0242067557913066972 	-0.0185742036017027476 	
--0.115454110673715221 	-0.124678425465277937 	
--0.00480860338684518072 	-0.0862211112911418082 	
--0.0498520813047680783 	0.00594508090814193941 	
--0.023489889666694401 	-0.0501319023232620967 	
--0.113632484613219023 	-0.0233651478295003982 	
--0.0655510114657870402 	-0.0222282438482392167 	
--0.0185844365573900501 	-0.0609538189214826748 	
--0.00586998784931073558 	0.0437557537188896992 	
--0.0195469593322523429 	-0.0121502364841579152 	
--0.0563049888373474486 	0.053216137318378133 	
-0.0132454397771677983 	-0.0859383916298488704 	
-0.0214261997383835695 	0.0349233757161144104 	
--0.0897044020026717542 	0.0257031979551521177 	
-0.0170144225586621074 	-0.0925760031804471611 	
+-0.127200309513792131 	-0.0684044639330751991 	
+-0.116826845402778517 	0.0482588080762488825 	
+-0.047918728355812179 	-0.114926464365377581 	
+-0.0258426034880490478 	-0.0115929898431668989 	
+-0.0238320474009951384 	-0.0450295642345298688 	
+0.0549673432422954425 	0.0287941570930679187 	
+-0.0415614860833058222 	-0.126468135852143448 	
+-0.116981703220581781 	0.00300855899836532676 	
+-0.0918301964921392211 	-0.0179658301158359367 	
+-0.0408627699492909099 	-0.0706212091823788662 	
+-0.0106033827865716079 	-0.0784176146443308075 	
+-0.0743325188467987286 	-0.102701653534542511 	
+0.013936147751821561 	-0.0122491885067015586 	
+0.0339784394229312517 	-0.0488284680760181222 	
+0.0309452094450517849 	-0.12972834554709925 	
+-0.0995532082202473106 	-0.0356519795916181298 	
+-0.0317522386420212924 	-0.016465275815001866 	
+0.00533389366990224666 	-0.0310484595693043437 	
+-0.108348407801560062 	0.0245639666817451738 	
+-0.110414329531041508 	-0.0953886784282095418 	
+0.0242067557913067076 	-0.0185742036017027407 	
+-0.115454110673715221 	-0.124678425465277951 	
+-0.00480860338684519373 	-0.0862211112911417943 	
+-0.0498520813047680644 	0.00594508090814194288 	
+-0.0234898896666944045 	-0.0501319023232620967 	
+-0.113632484613219023 	-0.0233651478295003948 	
+-0.0655510114657870402 	-0.0222282438482392201 	
+-0.018584436557390057 	-0.0609538189214826678 	
+-0.00586998784931075119 	0.04375575371888972 	
+-0.0195469593322523429 	-0.0121502364841579186 	
+-0.0563049888373474278 	0.0532161373183781469 	
+0.013245439777167814 	-0.0859383916298488565 	
+0.0214261997383835626 	0.0349233757161143965 	
+-0.0897044020026717265 	0.0257031979551521489 	
+0.0170144225586621109 	-0.0925760031804471611 	
 -0.0469138127321606577 	-0.111246197882682057 	
 ]
 ;
@@ -151,6 +151,8 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
+L1_penalty_factor = 0 ;
+L2_penalty_factor = 0 ;
 down_size = 2 ;
 up_size = 100 ;
 learning_rate = 0.0100000000000000002 ;
@@ -181,11 +183,11 @@
 L1_penalty_factor = 0 ;
 L2_penalty_factor = 0 ;
 weights = 2  100  [ 
-0.0536258697428237258 	0.079771188059310244 	-0.00326805927482090933 	-0.0101229918485100604 	0.00132124513011801247 	-0.00976680035729327739 	-0.00551641246591087937 	0.00254003340756739533 	-0.00721877830476366777 	-0.00468686665758342308 	0.00814809711888342392 	-0.0116876545229839049 	-0.00806173142044939832 	-0.00844777528480624394 	-0.00990555371904450546 	-0.00111239955721024347 	-0.00786032371784966405 	-0.00310281135291619456 	0.0070465220500588446 	-0.00265996475887015802 	-0.0100824644275515612 	-0.00177291933557880885 	-0.0111593304689046741 	-0.00385551710435362398 	0.00534703358449456403 	-0.00275507471051412252 	-0.00812720392968241563 	0.00666305567954580437 	-0.00874838287369643364 	0.00674817119844186696 	0.00627860122963375122 	0.000302175516742306347 	-0.00135664417948326088 	-0.00998550006068867763 	0.00165042782930788939 	-0.000655044087697747527 	-0.00904853174342502495 	0.00778106626316841469 	-0.00194207063149650955 	0.000555605035051326781 	-0.0108!
 686767428995933 	-0.00610774713115255167 	-0.00504316984626982951 	0.00519809660452281823 	-0.00158718442579143288 	0.00428019214136634001 	0.00178641056789957849 	0.00240861182377035982 	-0.00779553954556565058 	-0.00239444393683160275 	0.00323645674539891791 	0.006291858113373139 	-0.00986938945263454714 	-0.0104563344000978657 	0.00280662907268667931 	-0.0109917825537314533 	-0.0114696134856420804 	-0.00194047294777082747 	-0.00344848702059746088 	0.00150594576613708158 	-0.00611549231782085052 	0.00307000904075582694 	0.000185932299171378868 	0.00624048575727767507 	-0.0081723764797312394 	0.000239532188827330924 	0.00522778090410386601 	-0.010861109485718886 	-0.00185245300057644861 	-0.00954410501496315289 	-0.00526218925171827341 	0.00452255102975858587 	0.00568321458944887666 	-0.00989952837609308842 	0.00799337046359746735 	0.00338016544604234854 	0.00539398668242293839 	-0.00702719021479430734 	-0.0114150502039247568 	-0.00206543733759186532 	0.0071197648460417095!
 6 	-0.00874220908263338183 	0.00388714478005785851 	-0.0058058!
 68728593
84026 	0.00605745616534606943 	-0.00419900638732950346 	0.00695931687446142786 	-0.000151112416788917817 	-0.000944819039427680259 	0.00795937629094451218 	0.00400941810416922256 	-0.00168785423660831619 	-5.54994787460903836e-05 	0.000472649845764057538 	-0.0109565853518955626 	0.00489409718203449948 	-0.00867480616208503134 	0.00408101082119373838 	-0.00410396684372572526 	0.00199467788050524493 	
--0.0524576615507209854 	-0.0711548751820349351 	0.00859300691325806318 	0.00358093478880480492 	0.00870870184063538347 	-0.00700936762589595818 	0.00802591787938353868 	0.00869694523134979976 	0.00464342886955343599 	-0.00475267033208308289 	0.00958192641608508061 	0.00311878970432149377 	0.00674013678449342842 	-0.000774093095166621657 	0.00473822302000300333 	0.0099328744593782034 	0.00442455246209573494 	0.0024241939481423661 	0.00863951665918676059 	-0.00261030879619999091 	0.00525890610167225464 	0.00286804105265928881 	0.000356317481305886948 	0.00994656664141626082 	-0.00627749085333609898 	-0.00771911940078836688 	-0.000406988359543328975 	-0.0050906570383990879 	0.00856549028666053237 	0.00584911288574413782 	-0.00248490907854921313 	-0.00232925899374765866 	-0.00660896106083192804 	0.0118072474503107122 	-0.00645157145599034414 	-5.36742587341041639e-05 	-0.00726172661529194633 	0.002163364247345082 	0.00459889346938568803 	-0.00662589202985131021 	0.0003323493575!
 55962119 	0.0070167612378806898 	0.00410565447944734752 	0.0113210791773455596 	0.00267558003344916129 	-0.00372693501593652967 	-0.000681092484471729014 	0.00397314365211581897 	0.00352934442527573933 	-0.00712429042887002296 	0.00202230074060919798 	0.00341927540767706512 	-0.00552943357020784301 	0.00333043482724112603 	-0.00409448131434433926 	0.00291625932627824144 	0.00783385752201855128 	0.00803422003738779936 	0.0115064322469165814 	-0.00746738168297064828 	0.00980516538582273771 	0.00806528253749231108 	0.00975283697149454537 	0.0035541095215218645 	0.00782431198541197644 	0.00530290199970997762 	0.0110564311002894437 	0.00541286706445689471 	0.00131404246549422983 	0.000469666954329339388 	-0.00741682701709382806 	0.00218914960340962396 	0.0085465982049161178 	0.0115450492736752612 	0.011644616442275357 	-0.00777988121514700849 	-0.00387383975462663507 	-0.00220512740423139822 	0.000896485123796164759 	0.00135549589919261009 	0.000869177330993192872 	-0.0014091031!
 8477438469 	-0.00710088202850090496 	-0.000575479712761079159 !
 	0.00889
104091862338505 	-0.00324407095339257403 	-0.00178018154537139283 	0.00930065748471015433 	0.00253451984724496803 	-0.00138974116327348512 	0.00906486996137605312 	-0.00401560714779240682 	0.00903224643928919081 	0.000661063666917365662 	0.00693962133541331429 	0.00635418060229068181 	-0.00779659722808411382 	0.0116218122021180016 	0.00728686389924735212 	-0.00230054091727934385 	
+0.0536258697428236564 	0.0797711880593102302 	-0.00326805927482090716 	-0.0101229918485100621 	0.00132124513011801052 	-0.00976680035729327739 	-0.00551641246591088024 	0.00254003340756739837 	-0.00721877830476366257 	-0.00468686665758342134 	0.00814809711888342565 	-0.0116876545229839014 	-0.00806173142044940005 	-0.00844777528480624568 	-0.00990555371904450373 	-0.00111239955721024173 	-0.00786032371784966231 	-0.00310281135291619542 	0.00704652205005884373 	-0.00265996475887015759 	-0.0100824644275515594 	-0.00177291933557880972 	-0.0111593304689046741 	-0.00385551710435362398 	0.00534703358449457097 	-0.00275507471051412252 	-0.00812720392968241563 	0.0066630556795458009 	-0.00874838287369643364 	0.00674817119844187043 	0.00627860122963375209 	0.000302175516742305805 	-0.00135664417948326023 	-0.00998550006068867763 	0.00165042782930789156 	-0.000655044087697746443 	-0.00904853174342502668 	0.00778106626316841122 	-0.0019420706314965102 	0.000555605035051326347 	-0.0108!
 686767428995846 	-0.00610774713115254733 	-0.00504316984626982951 	0.0051980966045228191 	-0.00158718442579143071 	0.00428019214136633915 	0.00178641056789958065 	0.00240861182377036459 	-0.00779553954556565058 	-0.00239444393683160102 	0.003236456745398914 	0.00629185811337314421 	-0.00986938945263454714 	-0.010456334400097864 	0.00280662907268667931 	-0.0109917825537314499 	-0.0114696134856420769 	-0.00194047294777082661 	-0.00344848702059746002 	0.00150594576613708527 	-0.00611549231782085399 	0.00307000904075582304 	0.000185932299171381144 	0.00624048575727767681 	-0.00817237647973124114 	0.000239532188827333581 	0.00522778090410387208 	-0.0108611094857188825 	-0.00185245300057644601 	-0.00954410501496315809 	-0.00526218925171827428 	0.00452255102975857894 	0.00568321458944887926 	-0.00989952837609308668 	0.00799337046359746388 	0.0033801654460423494 	0.00539398668242293666 	-0.0070271902147943056 	-0.0114150502039247603 	-0.00206543733759186488 	0.0071197648460417113 	!
 -0.00874220908263338009 	0.00388714478005785765 	-0.0058058687!
 28593842
86 	0.0060574561653460729 	-0.00419900638732950172 	0.00695931687446142699 	-0.000151112416788917357 	-0.0009448190394276795 	0.00795937629094451739 	0.0040094181041692217 	-0.00168785423660831597 	-5.54994787460898957e-05 	0.000472649845764052333 	-0.0109565853518955609 	0.00489409718203450209 	-0.00867480616208502961 	0.00408101082119374445 	-0.00410396684372572786 	0.00199467788050524579 	
+-0.0524576615507209229 	-0.0711548751820350184 	0.00859300691325805971 	0.00358093478880480579 	0.0087087018406353852 	-0.00700936762589595645 	0.00802591787938354041 	0.00869694523134979976 	0.00464342886955343599 	-0.00475267033208308549 	0.00958192641608507888 	0.00311878970432149117 	0.00674013678449342842 	-0.00077409309516662556 	0.00473822302000300507 	0.00993287445937820514 	0.00442455246209573581 	0.0024241939481423674 	0.00863951665918675712 	-0.00261030879619999611 	0.00525890610167224944 	0.00286804105265928881 	0.000356317481305886189 	0.00994656664141625561 	-0.00627749085333610072 	-0.00771911940078835995 	-0.000406988359543328812 	-0.00509065703839908963 	0.00856549028666053237 	0.00584911288574413175 	-0.00248490907854921443 	-0.00232925899374766083 	-0.00660896106083193151 	0.0118072474503107053 	-0.00645157145599035021 	-5.36742587341045772e-05 	-0.0072617266152919446 	0.0021633642473450807 	0.00459889346938568717 	-0.00662589202985131021 	0.0003323493575!
 55963311 	0.00701676123788068807 	0.00410565447944734926 	0.0113210791773455614 	0.00267558003344915869 	-0.00372693501593652967 	-0.000681092484471730315 	0.00397314365211581463 	0.00352934442527574064 	-0.0071242904288700273 	0.00202230074060919755 	0.00341927540767706468 	-0.00552943357020784388 	0.0033304348272411243 	-0.00409448131434433839 	0.00291625932627824318 	0.00783385752201855301 	0.00803422003738779936 	0.0115064322469165797 	-0.00746738168297064568 	0.00980516538582273771 	0.00806528253749231282 	0.00975283697149455057 	0.00355410952152186406 	0.00782431198541197991 	0.00530290199970997935 	0.0110564311002894385 	0.00541286706445689384 	0.00131404246549422917 	0.000469666954329340256 	-0.00741682701709382632 	0.00218914960340962396 	0.00854659820491611606 	0.0115450492736752577 	0.0116446164422753587 	-0.00777988121514700676 	-0.00387383975462663507 	-0.00220512740423139909 	0.000896485123796165627 	0.00135549589919260966 	0.000869177330993193523 	-0.00140910!
 318477438642 	-0.0071008820285009067 	-0.000575479712761081653!
  	0.0088
9104091862338679 	-0.00324407095339257706 	-0.00178018154537139739 	0.0093006574847101578 	0.00253451984724496803 	-0.0013897411632734849 	0.00906486996137605312 	-0.00401560714779240682 	0.00903224643928918734 	0.000661063666917364578 	0.00693962133541331343 	0.00635418060229068094 	-0.00779659722808411209 	0.0116218122021180016 	0.00728686389924735212 	-0.00230054091727934862 	
 ]
 ;
-bias = 2 [ -0.0479920603803564855 0.0479920603803568463 ] ;
+bias = 2 [ -0.0479920603803564438 0.0479920603803568324 ] ;
 input_size = 100 ;
 output_size = 2 ;
 name = "GradNNetLayerModule" ;
@@ -261,6 +263,7 @@
 gibbs_down_state = 1 [ 0  0  [ 
 ]
 ] ;
+random_gen = *3  ;
 seed = 1827 ;
 stage = 2000 ;
 n_examples = 4 ;

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-0/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-0/Split0/final_learner.psave	2007-07-31 19:42:44 UTC (rev 7885)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-0/Split0/final_learner.psave	2007-07-31 19:51:48 UTC (rev 7886)
@@ -15,7 +15,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 2 [ -0.083333333333333301 -0.0600000000000000047 ] ;
+bias = 2 [ -0.0366666666666666669 0.00666666666666666102 ] ;
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMBinomialLayer" ;
@@ -34,7 +34,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 100 [ 3.68880251319452146 3.67455572697473087 -0.0748056477610814785 -0.0707067281518160839 -0.0794316222066286659 -0.0749255137498329588 -0.0750635319871154894 -0.0772618806308057321 -0.073641443280301172 -0.0718607350161090258 -0.0771651258335693119 -0.0722440742688146703 -0.0805696794069616362 -0.0789613112572297854 -0.0764742098994685704 -0.0731698766452147792 -0.0721761964128051908 -0.0797042166243662892 -0.0747733902094199904 -0.0805211583755357507 -0.0717639495149448581 -0.073464206098538723 -0.076784839179855971 -0.0744510158274711425 -0.0734646795027404376 -0.0798614449615559774 -0.0782103179133733784 -0.0802772948589520613 -0.073078045351197371 -0.0759826086790340721 -0.0757239946960654442 -0.0702277123773308726 -0.07789291394744563 -0.0697834112429474712 -0.0793754900503946537 -0.0787186866454939738 -0.0743382352194213447 -0.0759276280251790225 -0.070338746160929097 -0.0751299449899901495 -0.0717849484592146675 -0.0772394307811129083 -0.0727521073155275727!
  -0.0748553547437217776 -0.0717221705041391627 -0.079942813452833078 -0.0747078261285098327 -0.0808907990333310734 -0.0754205994699822385 -0.0794675734812663737 -0.0793329145737262237 -0.0808518798972240882 -0.0800058752450361899 -0.0782272636798783116 -0.073533293451067322 -0.0738515406762122323 -0.0684666617143456502 -0.0776928711958970869 -0.0720488995279365274 -0.072474735612465771 -0.0745251009662933339 -0.0770781602286758905 -0.0774730651132638504 -0.0723408065348730839 -0.0703609060057496655 -0.0748692514306317225 -0.0714440308086841808 -0.0758636992657139575 -0.0747095125039830971 -0.0805721793124508645 -0.0712816233477524819 -0.0731863774792445715 -0.0732856325934941738 -0.0731935352174640369 -0.0740010586698171685 -0.0709587708179019033 -0.0773305918377132778 -0.0767297062261277779 -0.0737460077516916973 -0.0723959745071232341 -0.0754641989578205136 -0.0762994147979306481 -0.0742707440510437106 -0.0700104862165812913 -0.0774873633583698374 -0.0689049443094244818 -!
 0.0739378324707463813 -0.0756424682784822416 -0.07453836879051!
 58132 -0
.0723568603509939928 -0.0740438450018933436 -0.0743291358145748288 -0.0787535633411283881 -0.0760738545048729037 -0.0772240880043779665 -0.0746081796641250433 -0.0794668618979842839 -0.0749625415005695794 -0.0745145065393848505 -0.0716019257964348038 ] ;
+bias = 100 [ 1.53902645376136871 1.53287975428339629 -0.0312147953074692446 -0.0294089837995962014 -0.0332254359754929479 -0.0312729910165803898 -0.031337297801657657 -0.0322931732664276522 -0.0307168476645442133 -0.0299032370506452719 -0.0322362597564401221 -0.0300658087861667284 -0.0337132966902711079 -0.0330063519633913213 -0.0319526809075127843 -0.0305010601914717933 -0.0300595751302068989 -0.0333453981731375165 -0.031189436649062794 -0.0336975121433344196 -0.0298852413171584765 -0.0305994805190471578 -0.0320663224992080997 -0.0310699420524318121 -0.030584652837473407 -0.0334023993667867669 -0.0326828630412137472 -0.0335795035960490401 -0.0304638120370469452 -0.0317220858809491094 -0.0316265756182914151 -0.0292071758059737133 -0.0325445124058321641 -0.029013888976878191 -0.033183672118466935 -0.0328873465337548229 -0.0309734938745898115 -0.0317100129632481575 -0.0292539910023831874 -0.0313268334355716674 -0.0299027173724071768 -0.0322582052551700776 -0.03032018860359004!
 74 -0.0311813892235213268 -0.0298693303330919979 -0.0334306704042711608 -0.0311465322109097838 -0.0338524031445776655 -0.0314481349918532271 -0.0332346913193916696 -0.0331844403669249235 -0.0338359999766364022 -0.0334772946143155892 -0.0326980273258471177 -0.0306676619914250277 -0.0308017325675320995 -0.0284511445660079014 -0.0324489420355161678 -0.0300073084557735716 -0.0301922490262648557 -0.0310712384174130704 -0.0321749457907859679 -0.0323690439605787972 -0.0301330257416131397 -0.0292590804159707125 -0.0311920075101714996 -0.0297501091442765597 -0.0316610623392811 -0.0311612496890834763 -0.0337225971049988568 -0.0296801177648348541 -0.0304676212840513418 -0.0305221824006458066 -0.0305090406317516799 -0.0308619966007405155 -0.0295263822097292898 -0.0323008750762722244 -0.0320545738378344852 -0.0307710263448516402 -0.030137673139787536 -0.0314795380785417023 -0.0318584641026296855 -0.0309334555960045672 -0.0291118716229199805 -0.0323765044863651361 -0.0286353177034468567 !
 -0.0308329579852668588 -0.031553786794131268 -0.03108790025637!
 37676 -0
.0301110316176923624 -0.0308590144816007807 -0.0309979789761982427 -0.03291144265784475 -0.0317489009060902619 -0.0322338900012070564 -0.0311316805018305547 -0.033228271245262099 -0.0312443584642127445 -0.0310961107000570017 -0.0298148705216259614 ] ;
 input_size = 100 ;
 output_size = 100 ;
 name = "RBMMultinomialLayer" ;
@@ -45,112 +45,114 @@
 ] ;
 connections = 1 [ *5 ->RBMMatrixConnection(
 weights = 100  2  [ 
-0.60324184452909746 	0.52610861717432944 	
-0.573889414252517605 	0.556757554437076529 	
-0.0261172339319165163 	-0.0417266797118322 	
--0.0970764807226444371 	-0.0410491724357002485 	
-0.0741131894083618126 	0.0367940492146358877 	
-0.0511143452564637932 	-0.0643270428244155645 	
-0.0546696974742041514 	-0.0639812835063832963 	
-0.0838175544274623319 	-0.0319622112866029748 	
-0.0331478951397085053 	-0.0834267803158253995 	
--0.085346975633093361 	-0.0176323575944970674 	
-0.0493112301169950987 	0.00108652499183434985 	
--0.102107014510997435 	0.0096758636761334945 	
-0.0613318419464063264 	0.0788860997150088589 	
-0.0352036300223434601 	0.063239167992612727 	
-0.074227168976902777 	-0.0440016598760048436 	
--0.00092084284171127951 	-0.0621321696535562032 	
--0.0412257856745673959 	-0.0511947772496824521 	
-0.0810782785497820846 	0.0368716425058080652 	
-0.00524895074668816318 	-0.0214776774928671747 	
-0.0837077558817662798 	0.0553725060799138274 	
--0.0373082375788181958 	-0.0676159155161298459 	
--0.0670440376649158037 	0.0117951035043272791 	
-0.0261694113569656273 	0.0140156385848142825 	
-0.044493486467578329 	-0.0712406715889533082 	
--0.0909357541752950777 	0.0341783580594380298 	
-0.0628356885182562996 	0.0592957337386767619 	
-0.0354753811298013089 	0.0432252770180205914 	
-0.0621082037461518147 	0.0706981017678859486 	
--0.000580808082183228145 	-0.065179720660335741 	
-0.038850672569852876 	-0.0211946431624295793 	
-0.0869594178043191363 	-0.0793162883123661039 	
--0.0739242031243203951 	-0.0787995735400271458 	
-0.0413984147681554018 	0.028833693246709851 	
--0.103851286532993733 	-0.0629881050587312308 	
-0.0436587846659366521 	0.0656916133076659303 	
-0.00648415962861266255 	0.0847055847217833835 	
--0.0671858402426120105 	0.0366864364727969849 	
-0.0727509233151331647 	-0.0581538430599003314 	
--0.0840172959618063486 	-0.0652142892658817697 	
--0.027306329445590017 	0.0207231266998691432 	
--0.0109737322845146375 	-0.093963630514310531 	
-0.015661041083923171 	0.0367955952452021021 	
--0.00308659411959401374 	-0.0724990326043745276 	
--0.10416475850439072 	0.0849801137766668657 	
--0.022150463268884802 	-0.084448224751915768 	
-0.0489822036368685182 	0.0750707485878217423 	
--0.0284536317518466891 	0.00999823954382460071 	
-0.0716328620142713535 	0.0768460562442043332 	
--0.0505608144640681545 	0.0510695206284420555 	
-0.0679880308741660638 	0.043909685427124992 	
-0.0843441106583406597 	0.0237727328559663408 	
-0.075623920417366583 	0.0719023355961756483 	
-0.0859934587953604807 	0.0397452099532621464 	
-0.0498672742800235422 	0.0293461945737641255 	
-0.0427886162805164172 	-0.0970826872115481271 	
-0.0170111273480947796 	-0.0602761089824957325 	
--0.10006493915181143 	-0.108939149963149823 	
-0.0073603412153819929 	0.0570115030349874485 	
--0.0373917485400131339 	-0.0588563328154444176 	
--0.0106630295782426266 	-0.0732984366739459492 	
--0.0353513704442752486 	0.0116412035123935557 	
--0.0114082774919415722 	0.058706100029514062 	
-0.0432824628029821259 	0.0156319939448138767 	
--0.0207931958773757281 	-0.0669553416286962771 	
--0.103966467156310513 	-0.0450079640470971015 	
--0.0922739886998298897 	0.0745934058759385316 	
--0.0232514676085771553 	-0.0919278547650759642 	
-0.000432885488560150384 	0.0141648235669390096 	
-0.0021074033303418011 	-0.0201413191849692079 	
-0.083885644304006815 	0.0564950500408555253 	
--0.0168759761212089951 	-0.103701250339013146 	
--0.0929026382573996112 	0.0281495766550947588 	
--0.0673304729666608787 	0.0069182639265360571 	
--0.0155437347969988628 	-0.046461453752145547 	
-0.0153448363653640348 	-0.054211707033286094 	
--0.0502592553615646143 	-0.0797252286538448762 	
-0.0411763690141464175 	0.0138251725859302194 	
-0.0614203820515334328 	-0.0234296605012813929 	
-0.057509547962485405 	-0.106304249680942875 	
--0.0754086609777919192 	-0.011273652281573298 	
--0.00575725713588796581 	0.0090247286111742233 	
-0.0321995055439566608 	-0.00549372924335959478 	
--0.083872824153877798 	0.0502993195370103341 	
--0.0870672370416587377 	-0.0725084402829943298 	
-0.0517006246513320791 	0.00748826658549397051 	
--0.0924935534593258474 	-0.102515504356175402 	
-0.0211636098592593302 	-0.0621858826813804083 	
--0.0240399595060106254 	0.0318806266452920511 	
-0.00240796449009358622 	-0.0253653007642216863 	
--0.0897490519257006375 	0.00112661385718494525 	
--0.040436786628099429 	0.00281631858396141476 	
-0.00730964533356637594 	-0.0364237165347537015 	
-0.0215128174012390073 	0.0711670024008028562 	
-0.00683328492840286957 	0.0136007448724301452 	
--0.0301366831378556618 	0.0803666391553434917 	
-0.0397323374070795987 	-0.0616827204556885042 	
-0.0494355409814625363 	0.0623732547212860269 	
--0.0647038813372060456 	0.0517347343407355276 	
-0.0435739884637324215 	-0.0684115705809415775 	
--0.0222274194688612953 	-0.0881885023254915384 	
+0.79462416258962465 	0.708714243527043597 	
+0.761998366676349015 	0.740775020620458968 	
+0.0226943280880259717 	-0.0447216182192758244 	
+-0.10009407462972382 	-0.0438497816063247309 	
+0.0703707575926479229 	0.0334548677323289537 	
+0.0476428910453497237 	-0.0672954006544962124 	
+0.0511830682459991013 	-0.0669563184281088825 	
+0.0801683150285066315 	-0.0350901897856310818 	
+0.0297700754459803221 	-0.0863071571688051042 	
+-0.0884358798295634407 	-0.0205168498180810371 	
+0.0457300090726118977 	-0.00208590229100311846 	
+-0.105186540949654017 	0.006732431702763466 	
+0.0575563274048417409 	0.0754245728078506705 	
+0.0315579437060435203 	0.0598837553157687899 	
+0.0706343225711335149 	-0.0470730124064806105 	
+-0.00421645301082355407 	-0.0650203148334948522 	
+-0.0444050850121608179 	-0.0540511413881740455 	
+0.0773087287795562877 	0.0335183768639642499 	
+0.00186554106834445734 	-0.0244995991630909751 	
+0.0798908511929773829 	0.0519485452565484548 	
+-0.0404751825032169527 	-0.0704307226449379931 	
+-0.0702394663586567713 	0.00879089047462784309 	
+0.0226487394230743266 	0.0108431732277497316 	
+0.0410553313229231956 	-0.0741763522916748241 	
+-0.0940903046347904976 	0.0311470759934990814 	
+0.0590918674143132289 	0.0558994235960910898 	
+0.0318667714006530753 	0.0399381174535612532 	
+0.0583437399332065801 	0.0672624822911873227 	
+-0.00387367108475061709 	-0.0680596541373065939 	
+0.0353481012032452399 	-0.0242762383155908402 	
+0.083386517156818496 	-0.082302162135573731 	
+-0.0769547663495500217 	-0.081524334781875582 	
+0.0377948276579106596 	0.0255837394320384687 	
+-0.106816981343190745 	-0.0657179439567690954 	
+0.0399753496513197276 	0.0623108236320968623 	
+0.00290332606056676679 	0.08133074804665473 	
+-0.0704222816215416686 	0.0336047028930634359 	
+0.0691918342425827532 	-0.0611797083510401407 	
+-0.0870383227439321255 	-0.0679638727055128233 	
+-0.0306499782137995141 	0.0176249314951256002 	
+-0.0141853244497741303 	-0.0967429104019290886 	
+0.0121371252759188818 	0.0335667972566529998 	
+-0.00635787894805693154 	-0.0753529140981873957 	
+-0.107360916545427676 	0.0818040613513960208 	
+-0.0253387487790428627 	-0.0872369130621838129 	
+0.0452598136045025967 	0.0716463224903839818 	
+-0.0317748574273547177 	0.00693568372810476983 	
+0.0678199164401574323 	0.0733700685586171908 	
+-0.0538775004412728534 	0.0479114135103793429 	
+0.0642549430671540273 	0.0405568825393273005 	
+0.0805876143191117528 	0.0204575116057137754 	
+0.0718049811619611705 	0.0684355624851287758 	
+0.0821990644715877711 	0.0363721051404711296 	
+0.0462311843960937913 	0.0260791998330627967 	
+0.0394038995096594249 	-0.0999389675785798404 	
+0.0136507259485087098 	-0.0631992131473531477 	
+-0.102976822434378459 	-0.111547534488010386 	
+0.00382928876327718075 	0.0537300023582303332 	
+-0.0405719525474573933 	-0.0616964972800696718 	
+-0.0139044957155045681 	-0.0761367303277837026 	
+-0.0386525129667407846 	0.00858319571692932326 	
+-0.0148752188040222021 	0.0554537101395898707 	
+0.0396963613959412723 	0.0124229888629342044 	
+-0.0240123528480145863 	-0.069796346852707955 	
+-0.106957235807517578 	-0.0477874390598453103 	
+-0.0954916388431744922 	0.0714313522539851836 	
+-0.0264248652925035306 	-0.0946935663866832755 	
+-0.00299625944636697705 	0.0110374205950532541 	
+-0.00126753154630362638 	-0.023162089752242758 	
+0.0800663163332292899 	0.0530678501692746982 	
+-0.0200502363615609122 	-0.106442809755268306 	
+-0.0960408266959102835 	0.0251383747863423365 	
+-0.0705169052477177538 	0.00392991003699512485 	
+-0.0188168034216517552 	-0.0493735234870806283 	
+0.0119816962908368373 	-0.0571501697112642218 	
+-0.0533613095498321269 	-0.0824821516515972797 	
+0.037601890176097448 	0.0106255616686625007 	
+0.0578390563376181485 	-0.026544430348332905 	
+0.0540839976398827249 	-0.109157006150016195 	
+-0.0785395659553341924 	-0.0141935715313082876 	
+-0.00915496268241485639 	0.00592599973018796081 	
+0.028692236865362649 	-0.00861361276408664084 	
+-0.0870776017849616557 	0.047204941361393235 	
+-0.0900678405487130573 	-0.0752326945865099961 	
+0.0480986783680973085 	0.00429041514857957888 	
+-0.0954333095600831266 	-0.105148324559747111 	
+0.0177957040907608458 	-0.0651098337754503209 	
+-0.027414448415076783 	0.0287391027379765053 	
+-0.000959124330880182954 	-0.0283703541737347111 	
+-0.0928532700604430689 	-0.00180532451434606864 	
+-0.0437055843389625165 	-0.000204179264151569566 	
+0.00394504798016846129 	-0.0394027362192218775 	
+0.0179031403969577736 	0.0678102918079838274 	
+0.00338295098680184946 	0.0104645816629280099 	
+-0.0335755941582558393 	0.0770737654427485486 	
+0.0362969814104082827 	-0.0646396942668429481 	
+0.0457373159154405157 	0.0589930110876363406 	
+-0.0679740552805807308 	0.0485994264692320571 	
+0.0401347630464296556 	-0.0713543483124261801 	
+-0.0254079337733598767 	-0.0909656505381771646 	
 ]
 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
+L1_penalty_factor = 0 ;
+L2_penalty_factor = 0 ;
 down_size = 2 ;
 up_size = 100 ;
 learning_rate = 0.0100000000000000002 ;
@@ -181,11 +183,11 @@
 L1_penalty_factor = 0 ;
 L2_penalty_factor = 0 ;
 weights = 2  100  [ 
-0.0112930086105568449 	0.00616351561707523341 	-0.00160124426969843322 	-0.00802923364774296537 	0.00257365332377883053 	-0.00812585784087365738 	-0.00389030633538853428 	0.00396073978712296968 	-0.00544979422319446811 	-0.00272314781273958958 	0.00959605413022264313 	-0.00978206878944272787 	-0.00690313915092731056 	-0.0071583777452128329 	-0.00841361536316625677 	0.000720548967869462094 	-0.00591881034530785961 	-0.00187381153038211645 	0.00872183759475835506 	-0.00149595484940319304 	-0.00809712575578339783 	1.78988179957268726e-05 	-0.00967442763147958844 	-0.00216791085383057818 	0.00711801911543108196 	-0.00153741792568698357 	-0.00677056645157830383 	0.00784590579796891861 	-0.00690663089690580177 	0.00830341046778483062 	0.00781717632155564529 	0.00245902778824725905 	2.87068481194732033e-05 	-0.00778361873442472499 	0.00290579750983127908 	0.000640988181111136176 	-0.00735628493145134513 	0.00931883557940061009 	0.000200120989175870258 	0.00218908285970437081 	-0.0!
 0889358298502484178 	-0.00466660676048613349 	-0.00316817797404978733 	0.006786580520917373 	0.000399249430999985605 	0.00548796099863497165 	0.0034632416153528708 	0.00354287535829364911 	-0.00620872539959907606 	-0.00114420335390142563 	0.00449362450527556086 	0.00742971657202637793 	-0.00866544049941442296 	-0.00910025631504250525 	0.00457702039075809884 	-0.00923199573811228288 	-0.00910743854459208566 	-0.000545625040137308077 	-0.00149347604161448018 	0.0034118547128509924 	-0.00442262911536558251 	0.00451397641136160651 	0.00160821261276277556 	0.00816291825515298546 	-0.00604038605311225853 	0.00184300334127493535 	0.00724334381932651827 	-0.00929205025401912653 	-0.000170697534500130878 	-0.00838400966523670556 	-0.00323266907268441716 	0.00632436577670770703 	0.00749401810403165728 	-0.00806559711156345713 	0.00973990699691918739 	0.00545469430216973059 	0.00682901262664863465 	-0.00554724677438882523 	-0.00967905977762816526 	-0.000158221712962183759 	0.008726836!
 61315692104 	-0.00721359382370552471 	0.00557098704223077303 	!
 -0.00362
573588239716374 	0.00747697002782156915 	-0.0018883244797203299 	0.00870913922116942768 	0.00143092288873057183 	0.000753574583016262782 	0.00986211025418400229 	0.00575127316528340708 	3.00429243871667279e-05 	0.00124685295869076751 	0.00202262793202890537 	-0.00954277131961513321 	0.00657137608518045761 	-0.00742577634385300882 	0.00570537745406464892 	-0.00242122632293401228 	0.00399375881836177768 	
--0.0101248004184541999 	0.00245279726019983699 	0.0069261919081355771 	0.001487176588037696 	0.00745629364697455478 	-0.00865031014231556865 	0.0063998117488611771 	0.00727623885179423842 	0.0028744447879842489 	-0.00671638917692694761 	0.00813396940474585099 	0.00121320397078031263 	0.00558154451497132244 	-0.00206349063476004646 	0.00324628466412478457 	0.008099925934298478 	0.00248303908955393397 	0.00119519412560828496 	0.00696420111448728483 	-0.00377431870566696803 	0.00327356742990412815 	0.00107722289908474683 	-0.0011285853561191848 	0.00825896039089322499 	-0.00804847638427265075 	-0.0089367761856154887 	-0.00176362583764744707 	-0.00627350715682216311 	0.00672373830986990745 	0.00429387361640116462 	-0.00402348417047111804 	-0.00448611126525261944 	-0.00799431208843467966 	0.00960536612404676825 	-0.00770694113651373362 	-0.00134970652754298629 	-0.00895397342726560359 	0.000625594931112897655 	0.00245670184871332265 	-0.00825936985450433959 	-0.00164274440031878!
 595 	0.00557562086721426989 	0.00223066260722730664 	0.00973259526095102481 	0.000689146176657742002 	-0.00493470387320516218 	-0.00235792353192502361 	0.00283888011759251581 	0.00194253027930914985 	-0.00837453101180023543 	0.000765132980732538765 	0.00228141694902383356 	-0.00673338252342801317 	0.00197435674218579592 	-0.00586487263241577267 	0.001156472510659066 	0.00547168258096855827 	0.00663937212975426391 	0.00955142126793357901 	-0.00937329062968458512 	0.00811230218336748271 	0.00662131516688653672 	0.00833055665790316709 	0.00163167702364654175 	0.00569232155879299991 	0.00369943084726236408 	0.00904086818506683911 	0.00384380783275716604 	-0.000367713000582090021 	-0.000690428395397136674 	-0.00944634719612769992 	0.000387334856460506711 	0.00673579469033333197 	0.00971111800914562819 	0.00989807990895363865 	-0.00985441007127441093 	-0.00530886569885233046 	-0.00368507084463687642 	-0.000839505302500462552 	-0.000551719725437070218 	-0.000737894436122015461 	-0!
 .00293771844370225915 	-0.0087847242906738416 	-0.002755612558!
 95776609
 	0.0074715270561478463 	-0.00555475286100175387 	-0.00353000389207932651 	0.00771862217919071832 	0.000836126224801017831 	-0.00329247512651293078 	0.0073230149002618556 	-0.00573350430878788626 	0.00772989400185233877 	-0.000888914419347477188 	0.00552580730313291003 	0.00467690169914472542 	-0.00904562704631614416 	0.00999744556924713271 	0.00560412337845562093 	-0.00429962185513587704 	
+-0.00194486746516770023 	-0.00690309077309282301 	-0.00129917141308795643 	-0.00774035838581879892 	0.00287387832640887114 	-0.00781964883705902301 	-0.00358354802922155723 	0.00426866842059896081 	-0.00514602812731984691 	-0.00242892329329646127 	0.00989872933524002518 	-0.00948315263269114764 	-0.00660574718922594256 	-0.00685777843444654051 	-0.00810556413843992185 	0.00101876209609721427 	-0.00562525674839697235 	-0.0015737748237753587 	0.00902208120517115908 	-0.0011981899890972517 	-0.00780467806644808207 	0.00031782479332407421 	-0.00937279690427458324 	-0.00186247670944258785 	0.00742200220466920219 	-0.00123865006690160578 	-0.00646949922289207207 	0.00814384698228735882 	-0.00660847791658081014 	0.00860634267371974959 	0.00813096270192557695 	0.00274439892910540795 	0.0003300633112460526 	-0.00749985120846198994 	0.00320564686823331417 	0.000944462847820864808 	-0.0070524086922477882 	0.00962798872911302395 	0.000486159004431979094 	0.00249048994039886449 	-0.0085!
 9850276346362831 	-0.00436486723219704515 	-0.00287069829213080661 	0.00710249870539886915 	0.000692800347361197458 	0.00578689503731380853 	0.00376367335259855879 	0.00383933303037925066 	-0.00590335896931601465 	-0.000844442549739712455 	0.00479513619971176901 	0.00772626865924771165 	-0.0083658845931873331 	-0.00879901725232397 	0.00488312872858213259 	-0.0089311071118296035 	-0.00883186741233565366 	-0.000243088710357947948 	-0.00120019664017580316 	0.00370805705714521419 	-0.00412217491304890486 	0.00481781229905189207 	0.00191006771939728859 	0.00845790860981385402 	-0.00575300216332364032 	0.00215541415757722647 	0.00753616516304099308 	-0.00899082282992851728 	0.000129345537143261161 	-0.00808639127158941595 	-0.00293933319580840955 	0.00662716406584913306 	0.00779317700209307667 	-0.00776854909378277281 	0.0100406096748009467 	0.00574381738844291761 	0.0071309018832318976 	-0.00524228595476171084 	-0.00936920073022265605 	0.000137690697849392278 	0.0090277131046626!
 9142 	-0.00691149812494487852 	0.0058778936514806172 	-0.00334!
 13826336
3737646 	0.0077793939497520035 	-0.00161011058928650403 	0.00901063999725531389 	0.00173327713431316007 	0.00105346535294773523 	0.0101596134780142025 	0.00605066170878855707 	0.000330079813410994751 	0.00154852204530678077 	0.00232393712931801104 	-0.00923552789717206092 	0.00687569179944930078 	-0.00712623014494468722 	0.00601157034614300569 	-0.00211605889135666218 	0.00428703523455266702 	
+0.00311307565727032484 	0.0155194036503678995 	0.0066241190515251118 	0.0011983013261135276 	0.00715606864434450204 	-0.00895651914613020822 	0.00609305344269421047 	0.00696831021831824816 	0.00257067869210961643 	-0.00701061369637004644 	0.0078312941997284724 	0.00091428781402874801 	0.00528415255326995272 	-0.00236408994552634233 	0.00293823343939841929 	0.00780171280607068734 	0.00218948549264305235 	0.000895157419001529485 	0.00666395750407443917 	-0.00407208356597290894 	0.00298111974056882237 	0.000777296923756397892 	-0.00143021608332421428 	0.0079535262465052943 	-0.00835245947351073541 	-0.00923554404440090314 	-0.00206469306633367189 	-0.0065714483411406276 	0.00642558532954491668 	0.00399094141046618753 	-0.0043372705508410098 	-0.00477148240611077745 	-0.00829566855156123328 	0.00932159859808398723 	-0.0080067904949157713 	-0.00165318119425271297 	-0.00925784966646914752 	0.000316441781400469762 	0.0021706638334572188 	-0.00856077693519882894 	-0.001937824621879!
 9914 	0.00527388133892518155 	0.00193318292530832419 	0.00941667707646952346 	0.000395595260296533618 	-0.00523363791188400339 	-0.00265835526917071333 	0.00254242244550691686 	0.00163716384902611598 	-0.00867429181596196357 	0.000463621286296348335 	0.00198486486180250504 	-0.00703293842965505706 	0.00167311767946725417 	-0.00617098097023980469 	0.000855583884376374146 	0.00519611144871211586 	0.00633683579997489522 	0.00925814186649493755 	-0.00966949297397879 	0.00781184798105079899 	0.00631747927919625203 	0.00802870155126864972 	0.00133668666898569552 	0.00540493766900438256 	0.00338702003096007708 	0.00874804684135233566 	0.003542580408666526 	-0.000667756072225480189 	-0.000988046789044427255 	-0.00973968307300368367 	8.45365673190943224e-05 	0.00643663579227191345 	0.00941406999136493519 	0.00959737723107187586 	-0.0101435331575475546 	-0.0056107549554355934 	-0.00399003166426399082 	-0.00114936434990594444 	-0.00084763213624864742 	-0.00103877092762778172 	-0.00323!
 981414246291488 	-0.00909163089992367016 	-0.00303996580771755!
 554 	0.0
0716910313421741281 	-0.00583296675143557802 	-0.0038315046681652561 	0.00741626793360812119 	0.000536235454869546141 	-0.0035899783503431466 	0.00702362635675670387 	-0.00603354119781173651 	0.00742822491523631445 	-0.00119022361663658361 	0.00521856388068984642 	0.00437258598487588138 	-0.00934517324522442586 	0.00969125267716877334 	0.00529895594687826692 	-0.00459289827132676378 	
 ]
 ;
-bias = 2 [ -0.00270410090954439257 0.00270410090954443074 ] ;
+bias = 2 [ 0.000349142846218535117 -0.000349142846218485027 ] ;
 input_size = 100 ;
 output_size = 2 ;
 name = "GradNNetLayerModule" ;
@@ -261,6 +263,7 @@
 gibbs_down_state = 1 [ 0  0  [ 
 ]
 ] ;
+random_gen = *3  ;
 seed = 1827 ;
 stage = 2000 ;
 n_examples = 4 ;

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-0/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-0/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-1/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-1/Split0/final_learner.psave	2007-07-31 19:42:44 UTC (rev 7885)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-1/Split0/final_learner.psave	2007-07-31 19:51:48 UTC (rev 7886)
@@ -15,7 +15,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 2 [ -0.306666666666666698 -0.310000000000000053 ] ;
+bias = 2 [ -0.196666666666666434 -0.199999999999999761 ] ;
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMBinomialLayer" ;
@@ -34,7 +34,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 100 [ 4.89451941270932966 4.89197942438928912 -0.0994420803264053077 -0.0942778846704483142 -0.105270493455753844 -0.0995908208512013143 -0.0997637257232806968 -0.102534368163212236 -0.0979702072386830908 -0.0957394442620273095 -0.102417956428504736 -0.0962278870500655609 -0.106706045188527068 -0.104685782833708454 -0.101541489311648997 -0.0973788381890702687 -0.0961283172113815004 -0.105612846178692946 -0.0994058639730708155 -0.106641928652250625 -0.0956053153143950618 -0.0977652900667601854 -0.101941362116462569 -0.0989914507667482013 -0.0977733527136772362 -0.10581571029155 -0.103738546189915584 -0.106340627335140117 -0.0972619991501929154 -0.100927576553259776 -0.100595521994940873 -0.0936679707242785986 -0.10333825612701987 -0.0931072375720367329 -0.105207747035523921 -0.104387571792632389 -0.0988717876544415591 -0.100853778649247536 -0.0938094528930358412 -0.0998641169201297529 -0.0956282507427345801 -0.10251724938560812 -0.0968509892805764633 -0.09953679041182!
 32144 -0.095551223865302265 -0.105921653701933766 -0.0993302563383529485 -0.107109760683654992 -0.100235821197798655 -0.105318301624457877 -0.105145242632932581 -0.107060422627812998 -0.105992055984730604 -0.103756330112340728 -0.0978340861345907836 -0.0982376561275740412 -0.091435906267223685 -0.103091868949142934 -0.0959661474433256001 -0.0965032227289652211 -0.0990990157708882963 -0.102320868045302965 -0.102806740567493088 -0.0963347271999627497 -0.0938405954230747269 -0.0995507244656916312 -0.0951991253302886808 -0.100782999283239624 -0.0993255626263235825 -0.106704954191659707 -0.0949935369946697944 -0.097420725822433607 -0.0975394671206146202 -0.0974100970752363599 -0.0984278880063963885 -0.0945910963829521001 -0.102629268292377993 -0.101866355992808869 -0.0980998123378614001 -0.0964147972248048241 -0.100281744085462435 -0.101327864225343761 -0.0987917945610762155 -0.0933940369808724247 -0.102823910036620558 -0.0919935419983451408 -0.0983483085868706447 -0.10050967981!
 8633318 -0.0991091765591899099 -0.0963699248024051658 -0.09849!
 24766700
019938 -0.0988444425335484739 -0.104426907930808888 -0.101048666937016404 -0.10250921927481367 -0.0991916579055246855 -0.105320663815402643 -0.0996605948828681726 -0.0990719624774636515 -0.0954000170520914598 ] ;
+bias = 100 [ 2.58927831149933851 2.5864667316340344 -0.0525863262800879971 -0.049701151626684828 -0.0558370842730609995 -0.0526670928117814255 -0.0527692186319411172 -0.0543118730603495137 -0.051768235510647978 -0.0505061149237686635 -0.0542405712863612624 -0.0507805330764750582 -0.0566392105228117129 -0.0555045107889033038 -0.0537618939982477187 -0.0514353897847103364 -0.0507359503158334446 -0.0560281091690213937 -0.0525580721079815308 -0.0565997775566389333 -0.0504478872711732126 -0.0516353231331728169 -0.0539768619227540963 -0.0523385100540057491 -0.0516260747638155604 -0.0561328277131424094 -0.0549770541571955734 -0.0564218316989937574 -0.0513737031409291708 -0.0534080704055221109 -0.0532234323359034173 -0.049359119719903366 -0.054748481420695147 -0.0490558456261177833 -0.0557887904963138559 -0.0553286325680148808 -0.0522475457064476914 -0.0533680706771226254 -0.0494404869299046659 -0.0528025070759326615 -0.0504618351492271203 -0.0542962960222423893 -0.05114087127192587!
 51 -0.0526087129614231097 -0.0504142520847571107 -0.0561876891955786079 -0.0525081678439186378 -0.0568594543724587967 -0.0530132070352526477 -0.0558564985986491139 -0.0557642293607729447 -0.0568303570889149151 -0.0562397677437568716 -0.0549925767764838785 -0.0516836592606707695 -0.0519163950418511655 -0.0481302008387591782 -0.0546127241916318507 -0.0506479050117313312 -0.0509375463294486838 -0.0523877537339214536 -0.0541773091437517187 -0.0544604044458761108 -0.0508458809510060825 -0.0494588585776306322 -0.0526192640571188849 -0.0502194625640121073 -0.0533304472390503104 -0.0525137553499726156 -0.0566401566636174011 -0.0501019892190422353 -0.0514348801423082508 -0.0515091279013158571 -0.051456071021884861 -0.0520167360434775836 -0.0498700121024035353 -0.0543515442838604784 -0.0539365710036960253 -0.051842325623519897 -0.0508841749245111316 -0.053039148832892978 -0.0536341949650127672 -0.0521926542063309018 -0.0492092279049861775 -0.0544677590541281093 -0.0484307543177743688!
  -0.051966819879535206 -0.0531702701625530044 -0.0523936731622!
 55551 -0
.0508498324117470785 -0.0520444895988692244 -0.0522442434809628348 -0.0553582585228156662 -0.0534710938099692348 -0.0542841486703997886 -0.0524431647977401383 -0.0558574615367430638 -0.0526886433404681817 -0.0523820034795238615 -0.0503259572848782852 ] ;
 input_size = 100 ;
 output_size = 100 ;
 name = "RBMMultinomialLayer" ;
@@ -45,112 +45,114 @@
 ] ;
 connections = 1 [ *5 ->RBMMatrixConnection(
 weights = 100  2  [ 
-0.662158354424907891 	0.583232854342350615 	
-0.640064466062109427 	0.608798193940926713 	
-0.022527412437393389 	-0.0453103422133988507 	
--0.100266891588086798 	-0.0444642975036807497 	
-0.0702295556677405702 	0.0328532174664254439 	
-0.047473830353256169 	-0.0678698446488236462 	
-0.0510132237044769107 	-0.0675309405493934312 	
-0.0800098875205152577 	-0.035667553088809055 	
-0.0295945439730681409 	-0.086879820017033399 	
--0.0886030416950577687 	-0.0211361569633355464 	
-0.0455763521836290778 	-0.00268248005582537178 	
--0.105350063086555548 	0.00609662927904994385 	
-0.0574254211913950985 	0.0748055798832980706 	
-0.0314187402169011434 	0.0592625199905627703 	
-0.0704709948580114071 	-0.0476495496382620606 	
--0.0043894888250582036 	-0.0656076840431512459 	
--0.0445779806701181239 	-0.0546509115189693084 	
-0.0771686825448611585 	0.032918881442087565 	
-0.00170242077002343115 	-0.025099071089440296 	
-0.0797568907209052769 	0.0513446672167540627 	
--0.0406513951961165743 	-0.0710239575492945113 	
--0.0704003900367427132 	0.00816312518888440662 	
-0.0224951788845600513 	0.0102353844886529984 	
-0.0408830077684917623 	-0.0747508817475029813 	
--0.0942471517414461973 	0.030508494763001829 	
-0.0589559210333447867 	0.0552885530688721066 	
-0.0317220950414108041 	0.0393234591481275916 	
-0.0582111484660004647 	0.0666484882870143208 	
--0.00404796326721662634 	-0.0686465542027541453 	
-0.0351881294664364297 	-0.0248677733188647741 	
-0.0832186421140416327 	-0.082862637938772743 	
--0.0771320771131728811 	-0.0821174329393306868 	
-0.0376477065629339941 	0.0249766399520880943 	
--0.106994475045222595 	-0.0663262563819450451 	
-0.0398384755177023236 	0.0616927636838966939 	
-0.00276645173593164078 	0.0806951989051894192 	
--0.0705778277040368246 	0.0329679151144941368 	
-0.0690270234418436518 	-0.0617505667080445464 	
--0.0872144610485454802 	-0.0685653858942102123 	
--0.030805730619030882 	0.0170041856586852745 	
--0.0143651193546521731 	-0.0973207514133055535 	
-0.0119881909269199168 	0.0329483775113013772 	
--0.00653292824618773634 	-0.0759367741124321177 	
--0.107508406524880057 	0.0811397774181080583 	
--0.0255163862771277013 	-0.0878195693130585237 	
-0.0451263495710342855 	0.0710268329550134275 	
--0.0319333119034645035 	0.00631793124270262855 	
-0.0676903729911221552 	0.0727559957419445791 	
--0.0540290149289171032 	0.0472715615216171142 	
-0.0641154119265864852 	0.0399522123719096706 	
-0.0804446851382299571 	0.0198634165623277487 	
-0.071674718730542139 	0.0678245326884632377 	
-0.0820606732950758394 	0.0357732820017380437 	
-0.046084256474110824 	0.0254724581819018511 	
-0.0392284482108255853 	-0.100503223493228919 	
-0.0134788660749131355 	-0.0637837024876457132 	
--0.103161602985934894 	-0.11213644058335516 	
-0.00368484984043175286 	0.0531028729535656385 	
--0.0407464407245594559 	-0.0622929861333136395 	
--0.0140781234809960482 	-0.0767197188767816118 	
--0.0388121926402475448 	0.0079607752389461682 	
--0.0150210981163975688 	0.054821689198744257 	
-0.0395452117824861976 	0.0118193186934059564 	
--0.0241859232165774728 	-0.0703843417427873208 	
--0.107131075815018045 	-0.0484020032333970576 	
--0.0956406337548928454 	0.0707739746173131762 	
--0.0266040933847700861 	-0.0952738525063773767 	
--0.00315255194014438939 	0.0104225674220301233 	
--0.00143072069558646312 	-0.023762917767274655 	
-0.0799323620048853906 	0.052462994890691339 	
--0.0202304107648025172 	-0.107016633036747189 	
--0.0961993961416405946 	0.024500135955506732 	
--0.0706787356854953058 	0.00330422045808240845 	
--0.0189883818151368421 	-0.0499709111451519597 	
-0.0118120532825840736 	-0.0577361019533632544 	
--0.0535373303811612772 	-0.0830696516560714415 	
-0.0374511855232045401 	0.0100237064219110533 	
-0.0576803633973364677 	-0.0271301803890959849 	
-0.0539051641178651397 	-0.109716565071205918 	
--0.0787054032790544394 	-0.0148139951541322278 	
--0.00931162442447889052 	0.0053141875865541895 	
-0.0285347315353834674 	-0.00921285620513620507 	
--0.0872308162461840758 	0.0465615086994649804 	
--0.0902451275813747067 	-0.0758316671018854221 	
-0.0479468263188592703 	0.00369223713080175106 	
--0.0956149663757486501 	-0.10573571181435916 	
-0.0176260572271366329 	-0.065690471987151125 	
--0.0275682464441893357 	0.0281128525154292763 	
--0.00112346216923607392 	-0.0289692992359120334 	
--0.093016498452142049 	-0.00243123932537142986 	
--0.043866864213928547 	-0.000822958151842882279 	
-0.00377917811659888702 	-0.0399960413215662142 	
-0.0177642960633843106 	0.0671824370848984054 	
-0.00322806621841479919 	0.00985336178814748102 	
--0.0337181670178254358 	0.076427473507669319 	
-0.0361277275274375434 	-0.0652174804828195148 	
-0.0456000680094742414 	0.0583767919401788626 	
--0.0681263411873757468 	0.0479566088286199135 	
-0.039963263199487245 	-0.0719298457490680915 	
--0.0255852854871204613 	-0.0915459023913980374 	
+1.19323444237248788 	1.10365134020205291 	
+1.16455791575720546 	1.1355021575841302 	
+0.0128036549823034515 	-0.0546697883032441628 	
+-0.10887284932728393 	-0.0532803989953366738 	
+0.0596259802471913736 	0.0224833702714180815 	
+0.0376200401292133071 	-0.0771353164643296951 	
+0.0411159107714298908 	-0.0768203736787752001 	
+0.0696660041607025937 	-0.0454008667458865797 	
+0.0199980688049050878 	-0.0958909979026360687 	
+-0.0974044323932856393 	-0.0302019903373169675 	
+0.0354184220286879112 	-0.0125611304358038151 	
+-0.114131411831397175 	-0.00315477081028009594 	
+0.0467246048507069453 	0.0640609898122714017 	
+0.02107826230735068 	0.0488270569950124342 	
+0.0602803704386650277 	-0.0572218016409999641 	
+-0.0137612550592881907 	-0.0746545996212691354 	
+-0.0536300362218993174 	-0.0636165735846278557 	
+0.0664918670416289187 	0.0225112606484414446 	
+-0.00791140581492407076 	-0.034544195625692567 	
+0.0689526832755080338 	0.0407311741636741248 	
+-0.0496685562073276785 	-0.0798628675343147354 	
+-0.0794987836830611388 	-0.00125843451143519552 	
+0.0124988341600040086 	0.000342211347447629946 	
+0.0311194053018360381 	-0.0839256295554418191 	
+-0.103227227400035676 	0.0209965948909844775 	
+0.048353899526284265 	0.0447505077732809342 	
+0.0214843045020424311 	0.0290921577393402769 	
+0.0475564005076015983 	0.0559969794535006143 	
+-0.0134129247355141319 	-0.0776705022077120583 	
+0.0252476268825518289 	-0.0344772540609961931 	
+0.0730890435823567663 	-0.0921593328411282192 	
+-0.0857692743681313513 	-0.0906934290512722008 	
+0.0274301692817681998 	0.0148655859337688719 	
+-0.115456235687370512 	-0.0749293684606054528 	
+0.0294031487722755738 	0.0511933596600224125 	
+-0.00739325615111159195 	0.070191992696761199 	
+-0.0797893824866778567 	0.0233125767443296064 	
+0.0589340775210044909 	-0.0711754717550912713 	
+-0.0958271005588528235 	-0.0772213849384036505 	
+-0.0403086301253670481 	0.00731804959302331803 	
+-0.0235029000370315343 	-0.10604170048416367 	
+0.00198064346701789119 	0.0228813107886171943 	
+-0.0158366318496782205 	-0.0848802522225989148 	
+-0.1166127679251618 	0.0711846307355772645 	
+-0.0345898678194880294 	-0.0965713110481548204 	
+0.0345843667358964046 	0.0603999271313219108 	
+-0.0413759633305404023 	-0.0032625508914142726 	
+0.0568953331524017086 	0.0619810130960365968 	
+-0.0634666909662302547 	0.0373912094084370844 	
+0.0535429400853260976 	0.0295462051864257905 	
+0.069807218280953362 	0.00957505293860321481 	
+0.0608660950276555812 	0.0570816700015629314 	
+0.0713164087175934686 	0.0253094968696553242 	
+0.0357693168722422461 	0.0153044737325466959 	
+0.0296156270992086676 	-0.109429806788760242 	
+0.00392809093975230589 	-0.0729306566471017853 	
+-0.111469754933580592 	-0.120367891684431461 	
+-0.00634213058906721479 	0.042875693347806651 	
+-0.0498008712594534206 	-0.0712087344132402572 	
+-0.0232971321093652477 	-0.0856136580186096507 	
+-0.0482061418463571797 	-0.00161313750454946205 	
+-0.0248696022935214096 	0.0446756257026981324 	
+0.0293700253150780063 	0.0018244315069270683 	
+-0.0333445289699803721 	-0.0792923446265925008 	
+-0.115662878954422135 	-0.0571561953855523192 	
+-0.104803349227026971 	0.0608658684308862954 	
+-0.0356361888053585985 	-0.103956689053216353 	
+-0.0128992478109994715 	0.000652900826941616623 	
+-0.0110217411046003386 	-0.0332064052489344577 	
+0.0691161167596030662 	0.0418333998069146951 	
+-0.0292636866747932115 	-0.115621781103275034 	
+-0.105137795267041312 	0.0150476339745554688 	
+-0.0797514666755506779 	-0.00606928545664890077 	
+-0.028301327684715983 	-0.0590984409778385969 	
+0.00225519042321181262 	-0.0669262344826936639 	
+-0.0623712109063180378 	-0.0917329013746771005 	
+0.0273153410904841271 	6.52094837636866534e-05 	
+0.0475223423845173715 	-0.0368325126935498334 	
+0.044179395011721391 	-0.118635570703876134 	
+-0.0876241142877172974 	-0.0239836229616016625 	
+-0.0189633207792590484 	-0.00436423113984147487 	
+0.0185779726058129917 	-0.0189438734958861758 	
+-0.096349841489912616 	0.0368621669978151448 	
+-0.0988006306957863178 	-0.0844115922057575852 	
+0.0377312835201180641 	-0.00626227110423731477 	
+-0.103999980656173399 	-0.114038694659248499 	
+0.00805864621039445103 	-0.0748297159189370148 	
+-0.0371631232440120945 	0.0182933059578666152 	
+-0.0106927141702562994 	-0.0383654004456464096 	
+-0.101858045600465311 	-0.0116421159769202719 	
+-0.0531673510373415759 	-0.0102832582626856718 	
+-0.00578125687040188449 	-0.0493096297365558933 	
+0.00752127585122116995 	0.0567365302401570543 	
+-0.00657077892673409159 	6.56886074690213865e-05 	
+-0.0434977398831923739 	0.066148600248103373 	
+0.0263731492134056701 	-0.0744527041884158192 	
+0.0351194715380596031 	0.0478770450733050287 	
+-0.0774341019405877379 	0.0381399610289265487 	
+0.0301968535147759225 	-0.0811252394624102369 	
+-0.0346362874759416836 	-0.100260574098479868 	
 ]
 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
+L1_penalty_factor = 0 ;
+L2_penalty_factor = 0 ;
 down_size = 2 ;
 up_size = 100 ;
 learning_rate = 0.0100000000000000002 ;
@@ -181,11 +183,11 @@
 L1_penalty_factor = 0 ;
 L2_penalty_factor = 0 ;
 weights = 2  100  [ 
-0.0123095540038060513 	0.00883269083283308443 	-0.00164942141170705265 	-0.0081036761942630698 	0.00253566988886377636 	-0.00816932558833289788 	-0.00393274090447697823 	0.00392566856556591116 	-0.00550002870429359188 	-0.00278679617049868052 	0.00955537399950993904 	-0.00983725066694298919 	-0.0069407138091524042 	-0.00719533120339206113 	-0.00845060279724597213 	0.000662886268420245892 	-0.00598429177189082825 	-0.00191155793018608477 	0.00867211703386516763 	-0.00153435583843726888 	-0.00816574163814774712 	-3.32695231458855485e-05 	-0.00971658259858894369 	-0.00221369033864166802 	0.00707341489929369776 	-0.00157542538431900011 	-0.00680930893147756004 	0.00780812098798183262 	-0.00696470478833612853 	0.00825970132911229941 	0.00778474804689965709 	0.0023770416377033492 	-1.09780547725701351e-05 	-0.00786798391658959963 	0.00286871108339486602 	0.000608588343548391634 	-0.00739910533196916527 	0.00928158490075437111 	0.00011991527491460068 	0.00214372758739949081 	-0.00!
 895984715449167982 	-0.00470659240588922664 	-0.00322818379872686061 	0.00676347736322614225 	0.000331232525777035017 	0.00545115252760670216 	0.00341519558746549313 	0.00350449224011963045 	-0.00624663198141399856 	-0.00118238027851743223 	0.00445652370724215503 	0.00739113019304100775 	-0.0087031505534373426 	-0.0091394482304814912 	0.00452906553797543035 	-0.00928457507165773681 	-0.00920788489313654514 	-0.000582642413974866508 	-0.00155995661041069349 	0.0033495029178421923 	-0.00447092224843721915 	0.0044776295812166949 	0.00156774506916903785 	0.00809895877987856436 	-0.00611777613146196383 	0.00181467320458168009 	0.00717340736278910401 	-0.00933651903630436375 	-0.000220749351164449797 	-0.0084224589884120387 	-0.0033028101519856929 	0.00627729257006310031 	0.00744120610164107892 	-0.00812402397033160778 	0.00968766529268026352 	0.00537929524572177895 	0.0067882260588817871 	-0.00558689771081887377 	-0.00972195660418946096 	-0.000218153053328702412 	0.0086808447539!
 9950699 	-0.00725703444283621029 	0.00553278802625390212 	-0.0!
 03709269
62823098184 	0.00743688132580626673 	-0.00198395713324295373 	0.00865745503577009662 	0.00138842646266003973 	0.000702719534444898038 	0.00980483240321440054 	0.00570012771018503332 	-2.15772583666940978e-05 	0.00121137547075392195 	0.00197864458720439351 	-0.00957312438910756965 	0.00652485640508057834 	-0.00746327917414440264 	0.00566770032383096065 	-0.00246710406198358961 	0.0039249367347760104 	
--0.011141345811703415 	-0.000216377955558022159 	0.00697436905014421453 	0.0015616191345578412 	0.00749427708188957122 	-0.00860684239485632815 	0.00644224631794959503 	0.00731131007335129347 	0.00292467926908334752 	-0.00665274081916784193 	0.00817464953545855855 	0.00126838584828057157 	0.00561911917319641609 	-0.00202653717658083038 	0.00328327209820448301 	0.00815758863374769182 	0.00254852051613689134 	0.0012329405254122535 	0.00701392167538051302 	-0.00373591771663289241 	0.0033421833122684614 	0.00112839124022635754 	-0.00108643038900984191 	0.00830473987570431657 	-0.00800387216813527175 	-0.00889876872698347997 	-0.00172488335774820516 	-0.00623572234683515692 	0.00678181220130023507 	0.00433758275507368802 	-0.00399105589581511769 	-0.00440412511470870266 	-0.00795462718554264606 	0.00968973130621162554 	-0.00766985471007732792 	-0.0013173066899802412 	-0.00891115302674781555 	0.000662845609759145859 	0.00253690756297459416 	-0.00821401458219949213 	-0.00157648023!
 085192601 	0.0056156065126173639 	0.00229066843190437993 	0.00975569841864227898 	0.000757163081880695083 	-0.00489789540217689182 	-0.0023098775040376455 	0.00287726323576653707 	0.00198043686112406303 	-0.0083363540871842258 	0.000802233778765951426 	0.00232000332800923236 	-0.00669567246940506663 	0.00201354865762473851 	-0.00581691777963310244 	0.00120905184420450019 	0.00557212892951302035 	0.00667638950359183709 	0.00961790183672981552 	-0.0093109388346757499 	0.00816059531643908119 	0.0066576619970314466 	0.00837102420149688463 	0.00169563649892095851 	0.00576971163714270433 	0.003727760983955615 	0.009110804641604223 	0.00388827661504239069 	-0.000317661183917771319 	-0.000651979072221808625 	-0.00937620611682643025 	0.000434408063105112883 	0.00678860669272391033 	0.00976954486791378057 	0.00995032161319256252 	-0.00977901101482642546 	-0.00526807913108548118 	-0.0036454199082068422 	-0.000796608475939119902 	-0.000491788385070552323 	-0.000691902576964578534 	-0.0!
 0289427782457156359 	-0.00874652527469698543 	-0.0026720788131!
 2394886 
	0.00751161575816314785 	-0.00545912020747913156 	-0.0034783197066800167 	0.00776111860526124131 	0.00088698127337238301 	-0.00323519727554336415 	0.00737416035536022849 	-0.00568188412603403293 	0.0077653714897891607 	-0.000844931074522962391 	0.00555616037262536555 	0.00472342137924460642 	-0.00900812421602477376 	0.010035122699480847 	0.0056500011175052 	-0.00423079977155010716 	
+-0.0051126666632509473 	-0.00999088620274462061 	-0.00121466696870137789 	-0.00773300611596802571 	0.00302148579126679815 	-0.00772809316375816426 	-0.00348919697825370977 	0.00439610512651089462 	-0.00507683601503103668 	-0.00239567932057347874 	0.0100182593535812822 	-0.00943524090449528868 	-0.00644519681370213631 	-0.00671400083506097558 	-0.0079890873505423475 	0.00107369341016355386 	-0.0055907976476629118 	-0.00142290618098027529 	0.00910434177622430202 	-0.0010392033704020255 	-0.00777859363462465783 	0.000384013757316790168 	-0.00925920662414934231 	-0.00177893841800526287 	0.00749624275386964051 	-0.00108635924430357592 	-0.00633613061998029165 	0.00830095846111596203 	-0.00655504416372737313 	0.00870920586928189584 	0.00824477338811939985 	0.00273712874764631652 	0.000458789082551274005 	-0.0075148041761148518 	0.00335378021721601102 	0.0010912099579492387 	-0.00696637853990363532 	0.00973813252118751506 	0.000482345190178480783 	0.00258204293059011636 	-0.008569!
 34253533120534 	-0.00424362373088101562 	-0.00282323665100731817 	0.00721844963936686072 	0.000719107626785250951 	0.00594156483052304864 	0.00384729880794616296 	0.00400239112586509276 	-0.00579942313792349701 	-0.000696671002405571454 	0.00494273657121188214 	0.00788865022188791135 	-0.00821168176060774711 	-0.00866602948767879015 	0.00495387805501592681 	-0.00886246284277298485 	-0.00888108140933724713 	-0.000113231638400241905 	-0.00116835906847599039 	0.00374954500000187346 	-0.00404081722252320249 	0.00494178035821720655 	0.00203324327329691169 	0.00849597907405493825 	-0.00575305964926590193 	0.00226508928132380287 	0.0075570620107589595 	-0.00889013002654167814 	0.000210504781530930523 	-0.0079269121812115019 	-0.00292050995324969875 	0.00669533612776303504 	0.00785538235414176902 	-0.00771425299131676175 	0.0101112894185951922 	0.0057526315788456418 	0.00725211495606297319 	-0.0051262732613356907 	-0.00928988224725322241 	0.000181386200525739257 	0.0091221305725039!
 4269 	-0.00680481503025695718 	0.00596885101690989954 	-0.0033!
 52900415
68023786 	0.00790317672346741232 	-0.0016486825469074636 	0.00908127617458759188 	0.00183398261006754103 	0.00113176041998486925 	0.0102061139389016571 	0.00612321962325854726 	0.000405121500552078396 	0.00169185580472786088 	0.00242750165585267413 	-0.00910257860152900548 	0.00696002928635123738 	-0.00697761715645561168 	0.00611077258251763004 	-0.00203194781463226799 	0.0043110020621627285 	
+0.0062808748553535871 	0.018607199080019711 	0.00653961460713854172 	0.0011909490562627763 	0.00700846117948656158 	-0.00904807481943104702 	0.00599870239172634739 	0.00684087351240631782 	0.00250148657982082138 	-0.00704385766909305412 	0.00771176418138723452 	0.000866376085832874093 	0.00512360217774615167 	-0.00250786754491189642 	0.00282175665150085318 	0.00774678149200439415 	0.00215502639190898963 	0.000744288776206447384 	0.00658169693302130751 	-0.0042310701846681462 	0.0029550353087453708 	0.000711107959763680199 	-0.00154380636344943829 	0.00786998795506793289 	-0.00842670002271118067 	-0.00938783486699894427 	-0.00219806166924547703 	-0.00672855981996926377 	0.00637215157669147447 	0.00388807821490403391 	-0.00445108123703488474 	-0.0047642122246516665 	-0.00842439432286647485 	0.00933655156573692628 	-0.00815492384389846121 	-0.00179992830438108859 	-0.00934387981881335331 	0.000206297989325970979 	0.00217447764771071424 	-0.00865232992539006172 	-0.001966984850!
 01238575 	0.00515263783760915549 	0.00188572128418484203 	0.00930072614250153189 	0.000369287980872480505 	-0.0053883077050932383 	-0.00274198072451831707 	0.00237936435002106912 	0.00153322801763357145 	-0.00882206336329610132 	0.000316020914796217645 	0.00182248329916229559 	-0.00718714126223458406 	0.00154012991482207085 	-0.00624173029667359977 	0.000786939615319756365 	0.00524532544571377091 	0.00620697872801721263 	0.00922630429479513042 	-0.00971098091683546272 	0.00773049029052503504 	0.00619351122003093755 	0.00790552599736898086 	0.00129861620474462387 	0.0054049951549466433 	0.00327734490721350242 	0.00872714999363435363 	0.00344188760527968729 	-0.000748915316613148522 	-0.0011475258794223181 	-0.00975850631556246691 	1.63645054051856736e-05 	0.0063744304402232263 	0.00935977388889892847 	0.00952669748727763208 	-0.0101523473479503308 	-0.00573196802826666553 	-0.00410604435769002744 	-0.00122868283287535813 	-0.000891327638924992258 	-0.00113318839546903429 	-0!
 .00334649723715081844 	-0.00918258826535293948 	-0.00302844802!
 56746919
6 	0.00704532036050202135 	-0.00579439479381463948 	-0.00390214084549752325 	0.00731556245785374652 	0.000457940387832414942 	-0.00363647881123063199 	0.00695106844228671455 	-0.00610858288495282412 	0.00728489115581521657 	-0.00129378814317124887 	0.00508561458504681006 	0.00428824849797394738 	-0.00949378623371353696 	0.00959205044079419408 	0.00521484487015389398 	-0.00461686509893682786 	
 ]
 ;
-bias = 2 [ -0.00386380145527327095 0.00386380145527336506 ] ;
+bias = 2 [ 0.00257529472867298187 -0.00257529472867288429 ] ;
 input_size = 100 ;
 output_size = 2 ;
 name = "GradNNetLayerModule" ;
@@ -261,6 +263,7 @@
 gibbs_down_state = 1 [ 0  0  [ 
 ]
 ] ;
+random_gen = *3  ;
 seed = 1827 ;
 stage = 2000 ;
 n_examples = 4 ;

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-1/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-1/split_stats.pmat
===================================================================
(Binary files differ)



