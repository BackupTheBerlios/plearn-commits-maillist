From nouiz at mail.berlios.de  Tue Apr  1 17:23:32 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 1 Apr 2008 17:23:32 +0200
Subject: [Plearn-commits] r8743 - trunk/plearn/misc
Message-ID: <200804011523.m31FNWXf011141@sheep.berlios.de>

Author: nouiz
Date: 2008-04-01 17:23:31 +0200 (Tue, 01 Apr 2008)
New Revision: 8743

Modified:
   trunk/plearn/misc/vmatmain.cc
Log:
Added an option save_vmat in plearn vmat convert <source> <destination> ... [save_vmat]. This option save the original vmat is the destination metadatadir. Usefull if you want to remember it.


Modified: trunk/plearn/misc/vmatmain.cc
===================================================================
--- trunk/plearn/misc/vmatmain.cc	2008-03-31 20:30:01 UTC (rev 8742)
+++ trunk/plearn/misc/vmatmain.cc	2008-04-01 15:23:31 UTC (rev 8743)
@@ -53,6 +53,7 @@
 #include <plearn/db/getDataSet.h>
 #include <plearn/display/Gnuplot.h>
 #include <plearn/io/openFile.h>
+#include <plearn/io/load_and_save.h>
 #include <plearn/base/HelpSystem.h>
 #include <plearn/base/PDate.h>
 #include <algorithm>                         // for max
@@ -491,7 +492,7 @@
         bool mat_to_mem=false;
         if(argc<4)
             PLERROR("Usage: vmat convert <source> <destination> "
-                    "[--mat_to_mem] [--cols=col1,col2,col3,...]");
+                    "[--mat_to_mem] [--cols=col1,col2,col3,...] [save_vmat]");
 
         /**
          * Interpret the following options:
@@ -519,6 +520,7 @@
         int precision = 12;
         string delimiter = ",";
         bool convert_date = false;
+        bool save_vmat = false;
         for (int i=4 ; i < argc && argv[i] ; ++i) {
             string curopt = removeblanks(argv[i]);
             if (curopt == "")
@@ -539,6 +541,8 @@
                 convert_date = true;
             else if (curopt =="--mat_to_mem")
                 mat_to_mem = true;
+            else if (curopt == "save_vmat")
+                save_vmat = true;
             else
                 PLWARNING("VMat convert: unrecognized option '%s'; ignoring it...",
                           curopt.c_str());
@@ -580,6 +584,10 @@
             cerr << "ERROR: can only convert to .amat .pmat .dmat or .csv" << endl
                  << "Please specify a destination name with a valid extension " << endl;
         }
+        if(save_vmat && extract_extension(source)==".vmat")
+            PLearn::save(destination+".metadata/orig.vmat",vm);
+        else if(save_vmat)
+            PLWARNING("We haven't saved the original file as it is not a vmat");
     }
     else if(command=="info")
     {



From nouiz at mail.berlios.de  Tue Apr  1 17:25:31 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 1 Apr 2008 17:25:31 +0200
Subject: [Plearn-commits] r8744 - trunk/plearn/vmat
Message-ID: <200804011525.m31FPV7l011382@sheep.berlios.de>

Author: nouiz
Date: 2008-04-01 17:25:30 +0200 (Tue, 01 Apr 2008)
New Revision: 8744

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
-in VMatrix::setMetaInfoFrom, only set the info if the sourve VMatrix have the info.
-better error message


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-04-01 15:23:31 UTC (rev 8743)
+++ trunk/plearn/vmat/VMatrix.cc	2008-04-01 15:25:30 UTC (rev 8744)
@@ -1184,22 +1184,22 @@
     // Copy sizes from vm if not set and they do not conflict with the width.
     int current_w = max(0, inputsize_) + max(0, targetsize_) +
                     max(0, weightsize_) + max(0, extrasize_);
-    if(inputsize_<0) {
-        int is = vm->inputsize();
+    int is = vm->inputsize();
+    if(inputsize_<0 && is>=0) {
         if (is + current_w <= width_) {
             inputsize_ = is;
             current_w += is;
         }
     }
-    if(targetsize_<0) {
-        int ts = vm->targetsize();
+    int ts = vm->targetsize();
+    if(targetsize_<0 && ts>=0) {
         if (ts + current_w <= width_) {
             targetsize_ = ts;
             current_w += ts;
         }
     }
-    if(weightsize_<0) {
-        int ws = vm->weightsize();
+    int ws = vm->weightsize();
+    if(weightsize_<0 && ws>=0) {
         if (ws + current_w <= width_) {
             // We must also ensure the total sum of sizes (if available)
             // will match the width. Otherwise we may end up with sizes
@@ -1212,8 +1212,8 @@
             }
         }
     }
-    if(extrasize_<=0) {
-        int es = vm->extrasize();
+    int es = vm->extrasize();
+    if(extrasize_<=0 && es>=0) {
         if (es + current_w <= width_) {
             // Same as above.
             if (inputsize_ < 0 || targetsize_ < 0 || weightsize_ < 0 ||
@@ -1309,9 +1309,12 @@
 void VMatrix::lockMetaDataDir(time_t max_lock_age, bool verbose) const
 {
     if(!hasMetaDataDir())
-        PLERROR("In VMatrix::lockMetaDataDir(): metadatadir was not set");
+        PLERROR("In VMatrix::lockMetaDataDir() subclass %s -"
+                " metadatadir was not set", classname().c_str());
     if(lockf_) // Already locked by this object!
-        PLERROR("VMatrix::lockMetaDataDir() called while already locked by this object.");
+        PLERROR("VMatrix::lockMetaDataDir() subclass %s -"
+                " called while already locked by this object.",
+                classname().c_str());
     if(!pathexists(metadatadir))
         force_mkdir(metadatadir);
 



From nouiz at mail.berlios.de  Tue Apr  1 17:31:31 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 1 Apr 2008 17:31:31 +0200
Subject: [Plearn-commits] r8745 - trunk/plearn/vmat
Message-ID: <200804011531.m31FVVYZ011967@sheep.berlios.de>

Author: nouiz
Date: 2008-04-01 17:31:30 +0200 (Tue, 01 Apr 2008)
New Revision: 8745

Modified:
   trunk/plearn/vmat/PrecomputedVMatrix.cc
Log:
In PrecomputedVMatrix::build_, we do setMetaInfoFromSource, when the source vmat have it. Not just if it have a metadatadir



Modified: trunk/plearn/vmat/PrecomputedVMatrix.cc
===================================================================
--- trunk/plearn/vmat/PrecomputedVMatrix.cc	2008-04-01 15:25:30 UTC (rev 8744)
+++ trunk/plearn/vmat/PrecomputedVMatrix.cc	2008-04-01 15:31:30 UTC (rev 8745)
@@ -176,14 +176,15 @@
     }
 
     else
-        PLERROR("Invalid precomp_type=%s. Must be one of: dmat, pmat.",precomp_type.c_str());
+        PLERROR("Invalid precomp_type=%s. Must be one of: dmat, pmat.",
+                precomp_type.c_str());
 }
 
 void PrecomputedVMatrix::build_()
 {
-    //We only call it their, as some matrix(FilteredVMatrix) are only completly
-    //set if they have a metadatadir.
-    if(hasMetaDataDir())
+    //We don't always call it as some matrix(FilteredVMatrix) are only
+    //completly set if they have a metadatadir.
+    if(hasMetaDataDir() ||(source->width()>0 && source->length()>0))
         setMetaInfoFromSource();
 }
 



From nouiz at mail.berlios.de  Tue Apr  1 18:18:04 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 1 Apr 2008 18:18:04 +0200
Subject: [Plearn-commits] r8746 - trunk/plearn/vmat
Message-ID: <200804011618.m31GI49v016631@sheep.berlios.de>

Author: nouiz
Date: 2008-04-01 18:18:04 +0200 (Tue, 01 Apr 2008)
New Revision: 8746

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
bugfix. if their was a medatadir that is was existing, we were keeping it.


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-04-01 15:31:30 UTC (rev 8745)
+++ trunk/plearn/vmat/VMatrix.cc	2008-04-01 16:18:04 UTC (rev 8746)
@@ -1900,6 +1900,8 @@
     m.saveAllStringMappings();
     }// to ensure that m is deleted?
     
+    rm(pmatfile);
+    force_rmdir(pmatfile+".metadata");
     mv(pmatfiletmp,pmatfile);
     mv(pmatfiletmp+".metadata",pmatfile+".metadata");
 }



From nouiz at mail.berlios.de  Tue Apr  1 20:06:57 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 1 Apr 2008 20:06:57 +0200
Subject: [Plearn-commits] r8747 - trunk/python_modules/plearn/parallel
Message-ID: <200804011806.m31I6vST018381@sheep.berlios.de>

Author: nouiz
Date: 2008-04-01 20:06:56 +0200 (Tue, 01 Apr 2008)
New Revision: 8747

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
check that condor_submit worked. Otherwise print a error message


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-04-01 16:18:04 UTC (rev 8746)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-04-01 18:06:56 UTC (rev 8747)
@@ -798,7 +798,7 @@
                 error          = %s/condor.%s.$(Process).error
                 log            = %s/condor.log
                 getenv         = True
-                ''' % (self.tmp_dir,req,
+                ''' % (self.log_dir,req,
                        self.log_dir,self.unique_id,
                        self.log_dir,self.unique_id,
                        self.log_dir)))
@@ -868,7 +868,11 @@
             print "[DBI] Executing: condor_submit " + condor_file
             for task in self.tasks:
                 task.set_scheduled_time()
-            self.p = Popen( 'condor_submit '+ condor_file, shell=True , stdout=output, stderr=error)
+            self.p = Popen( 'condor_submit '+ condor_file, shell=True)
+#            self.p = Popen( 'condor_submit '+ condor_file, shell=True , stdout=output, stderr=error)
+            self.p.wait()
+            if self.p.returncode != 0:
+                print "[DBI] condor_submit failed! We can't stard the jobs"
         else:
             print "[DBI] Created condor file: " + condor_file
             if self.dolog:



From nouiz at mail.berlios.de  Tue Apr  1 21:41:50 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 1 Apr 2008 21:41:50 +0200
Subject: [Plearn-commits] r8748 - trunk/scripts
Message-ID: <200804011941.m31JfotW026610@sheep.berlios.de>

Author: nouiz
Date: 2008-04-01 21:41:49 +0200 (Tue, 01 Apr 2008)
New Revision: 8748

Modified:
   trunk/scripts/dbidispatch
Log:
-check if the parameter is valid with the back-end used.
-corredted help



Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-04-01 18:06:56 UTC (rev 8747)
+++ trunk/scripts/dbidispatch	2008-04-01 19:41:49 UTC (rev 8748)
@@ -58,11 +58,12 @@
 condor only options:
   The '--req=\"CONDOR_REQUIREMENT\"' option makes dbidispatch send additional option to DBI that will be used to generate additional requirement for condor. CONDOR_REQUIREMENT must follow the syntax of requirement for condor with one exception. The symbol '\"' must be escaped 3 times! So the requirement (Machine == \"computer.example.com\") must be writen in the following way:
 
-  dbidispatch \"--req=Machine==\\\\\\\"computer.example.com\\\\\\\"\"
+  dbidispatch \"--req=Machine==\\\"computer.example.com\\\"\"
      or
-  dbidispatch '--req=Machine==\\\"computer.example.com\\\"' 
+  dbidispatch '--req=Machine=="computer.example.com"' 
+     or
+  dbidispatch '--req=regexp("mona0*", target.Machine)'
 
-
 where <command-template> is interpreted as follows: the first argument
 is the <command> above, and the rest are interpreted as <arguments>.
 The arguments may contain segments of the form {{a,b,c,d}}, which trigger
@@ -148,7 +149,7 @@
     elif argv == "--nowait":
         dbi_param["cluster_wait"]=False
     elif argv[0:6] == "--req=":
-        dbi_param["requirements"]="\"%s\""%argv[6:]
+        dbi_param["requirements"]="%s"%argv[6:]
     elif argv.startswith("--micro"):
         dbi_param["micro"]=20
         if len(argv)>7:
@@ -172,8 +173,23 @@
     print ShortHelp
     sys.exit(1)
 
+valid_dbi_param=["clean_up","test","arch","dolog"]
+if launch_cmd=="Ssh":
+    valid_dbi_param+=["nb_proc"]
+elif launch_cmd=="Cluster":
+    valid_dbi_param +=["cluster_wait","force","nb_proc",'--duree','--3264', '--32', '--64',"--interruptible","--duree","--cpu","--mem","--os"]
+elif launch_cmd=="Condor":
+    valid_dbi_param +=["requirements", "--3264", "--32", "--64"]
+elif launch_cmd=="Bqtools":
+    valid_dbi_param +=["--micro", "--long","nb_proc",'--duree','--micro']
+elif launch_cmd=="Local":
+    valid_dbi_param +=["nb_proc"]
+
 print "\n\nThe jobs will be launched on the system:", launch_cmd
 print "With options: ",dbi_param
+for i in dbi_param:
+    if i not in valid_dbi_param:
+        print "WARNING: The parameter",i,"is not valid for the",launch_cmd,"back-end"
 print "With the command to be expended:"," ".join(command_argv),"\n\n"
 
 def generate_combination(repl):



From nouiz at mail.berlios.de  Tue Apr  1 22:02:52 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 1 Apr 2008 22:02:52 +0200
Subject: [Plearn-commits] r8749 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200804012002.m31K2qZ3028356@sheep.berlios.de>

Author: nouiz
Date: 2008-04-01 22:02:52 +0200 (Tue, 01 Apr 2008)
New Revision: 8749

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
added option --getenv for condor


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-04-01 19:41:49 UTC (rev 8748)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-04-01 20:02:52 UTC (rev 8749)
@@ -527,7 +527,7 @@
         print "[DBI] Their was %d jobs where the back-end failled"%(self.backend_failed)
         print "[DBI] Their was %d jobs that returned a failure status."%(self.jobs_failed)
 
-class DBIbqtools(DBIBase):
+class DBIBqtools(DBIBase):
 
     def __init__( self, commands, **args ):
         self.nb_proc = 1
@@ -664,6 +664,8 @@
         from socket import gethostname
         if (not os.path.abspath(os.path.curdir).startswith("/home/fringant2/")) and gethostname().endswith(".iro.umontreal.ca"):
             raise Exception("You must be in a subfolder of /home/fringant2/")
+
+        self.getenv = True
         DBIBase.__init__(self, commands, **args)
         if not os.path.exists(self.log_dir):
             os.mkdir(self.log_dir) # condor log are always generated
@@ -797,11 +799,11 @@
                 output         = %s/condor.%s.$(Process).out
                 error          = %s/condor.%s.$(Process).error
                 log            = %s/condor.log
-                getenv         = True
+                getenv         = %s
                 ''' % (self.log_dir,req,
                        self.log_dir,self.unique_id,
                        self.log_dir,self.unique_id,
-                       self.log_dir)))
+                       self.log_dir,str(self.getenv))))
 
         if len(condor_datas)!=0:
             for i in condor_datas:
@@ -834,7 +836,8 @@
             if os.getenv("CONDOR_LOCAL_SOURCE"):
                 launch_dat.write('source ' + os.getenv("CONDOR_LOCAL_SOURCE") + '\n')
             launch_dat.write(dedent('''\
-                    echo "Executing on ${HOSTNAME}" 1>&2
+                    echo "Executing on " `hostname` 1>&2
+                    echo "HOSTNAME: ${HOSTNAME}" 1>&2
                     echo "PATH: $PATH" 1>&2
                     echo "PYTHONPATH: $PYTHONPATH" 1>&2
                     echo "LD_LIBRARY_PATH: $LD_LIBRARY_PATH" 1>&2
@@ -869,7 +872,6 @@
             for task in self.tasks:
                 task.set_scheduled_time()
             self.p = Popen( 'condor_submit '+ condor_file, shell=True)
-#            self.p = Popen( 'condor_submit '+ condor_file, shell=True , stdout=output, stderr=error)
             self.p.wait()
             if self.p.returncode != 0:
                 print "[DBI] condor_submit failed! We can't stard the jobs"
@@ -1254,7 +1256,7 @@
 
 def main():
     if len(sys.argv)!=2:
-        print "Usage:%s {Condor|Cluster|Ssh|Local|bqtools} < joblist"%(sys.argv[0])
+        print "Usage:%s {Condor|Cluster|Ssh|Local|Bqtools} < joblist"%(sys.argv[0])
         print "Where joblist is a file containing one exeperiement by line"
         sys.exit(0)
     DBI([ s[0:-1] for s in sys.stdin.readlines() ], sys.argv[1]).run()

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-04-01 19:41:49 UTC (rev 8748)
+++ trunk/scripts/dbidispatch	2008-04-01 20:02:52 UTC (rev 8749)
@@ -3,7 +3,7 @@
 from plearn.utilities.toolkit import search_file
 
 ScriptName="launchdbi.py"
-ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|*--nodbilog] [--condor|--bqtools[=nb_process]|--cluster[=nb_process]|--local[=nb_process]|--ssh[=nb_process]] [--nb_proc=nb_process] [--mem=X] [--os=X] [--test|--no_test] [--long] [--micro[=nb_batch]] [--duree=X] [--wait|--nowait] [--req="CONDOR_REQUIREMENT"] [--force] [--interruptible] [--cpu=nb_cpu_per_node] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- signals the default option value.'
+ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|*--nodbilog] [--condor|--bqtools[=nb_process]|--cluster[=nb_process]|--local[=nb_process]|--ssh[=nb_process]] [--nb_proc=nb_process] [--mem=X] [--os=X] [--test|*--no_test] [--long|*--no_long] [--micro[=nb_batch]] [--duree=X] [--wait|--nowait] [--req="CONDOR_REQUIREMENT"] [--force|*--no_force] [--interruptible|*--no_interruptible] [--cpu=nb_cpu_per_node] [*--getenv|--no_getenv] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- signals the default option value.'
 LongHelp="""Dispatches jobs with dbi.py. dbi can dispatch jobs on condor, bqtools, cluster, local and ssh. If no system is selected on the command line, we try them in the previous order. ssh is never automaticaly selected.
 
 %(ShortHelp)s
@@ -56,6 +56,7 @@
   The '--cpu=nb_cpu_per_node' option is passed to cluster
 
 condor only options:
+  The '--getenv'('--no_getenv') option is forwarded to condor. If True, the current environnement variable will be forwarded to the execution node.
   The '--req=\"CONDOR_REQUIREMENT\"' option makes dbidispatch send additional option to DBI that will be used to generate additional requirement for condor. CONDOR_REQUIREMENT must follow the syntax of requirement for condor with one exception. The symbol '\"' must be escaped 3 times! So the requirement (Machine == \"computer.example.com\") must be writen in the following way:
 
   dbidispatch \"--req=Machine==\\\"computer.example.com\\\"\"
@@ -95,6 +96,10 @@
 
 In the file of the option --file=FILEPATH, there must not be double quotes around the {{}} as they are for the shell and if the command is in the file, they are not interpreted by the shell.
 
+
+Not defined but existing parameter: --no_clean_up.
+
+
 The environnement variable DBIDISPATCH_DEFAULT_OPTION can contain default option that you always want to pass to dbidispatch. You can override them on the command line.
 """%{'ShortHelp':ShortHelp,'ScriptName':ScriptName}
 
@@ -155,9 +160,10 @@
         if len(argv)>7:
             assert(argv[7]=="=")
             dbi_param["micro"]=argv[8:]
-    elif argv in  ["--force", "--interruptible", "--long", "--test"]:
+    elif argv in  ["--force", "--interruptible", "--long", "--test", "--getenv"]:
         dbi_param[argv[2:]]=True
-    elif argv in ["--no_test","--no_clean_up"]:
+    elif argv in ["--no_force", "--no_interruptible", "--no_long", "--no_test",\
+                      "--no_getenv", "--no_clean_up" ]:
         dbi_param[argv[5:]]=False
     elif argv.split('=')[0] in ["--duree","--cpu","--mem","--os","--nb_proc"]:
         dbi_param[argv.split('=')[0][2:]]=argv.split('=')[1]
@@ -173,13 +179,13 @@
     print ShortHelp
     sys.exit(1)
 
-valid_dbi_param=["clean_up","test","arch","dolog"]
+valid_dbi_param=["clean_up", "test", "arch", "dolog"]
 if launch_cmd=="Ssh":
     valid_dbi_param+=["nb_proc"]
 elif launch_cmd=="Cluster":
     valid_dbi_param +=["cluster_wait","force","nb_proc",'--duree','--3264', '--32', '--64',"--interruptible","--duree","--cpu","--mem","--os"]
 elif launch_cmd=="Condor":
-    valid_dbi_param +=["requirements", "--3264", "--32", "--64"]
+    valid_dbi_param +=["requirements", "--3264", "--32", "--64", "getenv"]
 elif launch_cmd=="Bqtools":
     valid_dbi_param +=["--micro", "--long","nb_proc",'--duree','--micro']
 elif launch_cmd=="Local":



From nouiz at mail.berlios.de  Tue Apr  1 22:22:53 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 1 Apr 2008 22:22:53 +0200
Subject: [Plearn-commits] r8750 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200804012022.m31KMrlH029970@sheep.berlios.de>

Author: nouiz
Date: 2008-04-01 22:22:52 +0200 (Tue, 01 Apr 2008)
New Revision: 8750

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
better handling of parameter


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-04-01 20:02:52 UTC (rev 8749)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-04-01 20:22:52 UTC (rev 8750)
@@ -171,7 +171,6 @@
         self.redirect_stderr_to_stdout = False
 
         # Initialize the namespace
-        self.requirements = ''
         self.test = False
         self.dolog = False
         self.temp_files = []
@@ -388,7 +387,7 @@
     def __init__(self, commands, **args ):
         self.duree=None
         self.arch=None
-        self.cluster_wait=True
+        self.cwait=True
         self.force=False
         self.interruptible=False
         self.cpu=1
@@ -446,7 +445,7 @@
             command += " --typecpu all"
         if self.duree:
             command += " --duree "+self.duree
-        if self.cluster_wait:
+        if self.cwait:
             command += " --wait"
         if self.mem:
             command += " --memoire "+self.mem
@@ -666,6 +665,7 @@
             raise Exception("You must be in a subfolder of /home/fringant2/")
 
         self.getenv = True
+        self.req = ''
         DBIBase.__init__(self, commands, **args)
         if not os.path.exists(self.log_dir):
             os.mkdir(self.log_dir) # condor log are always generated
@@ -789,8 +789,8 @@
         else :
             req="(Arch == \"%s\")"%(self.targetcondorplatform)
 
-        if self.requirements != "":
-            req = req+'&&('+self.requirements+')'
+        if self.req != "":
+            req = req+'&&('+self.req+')'
 
         condor_dat.write( dedent('''\
                 executable     = %s/launch.sh

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-04-01 20:02:52 UTC (rev 8749)
+++ trunk/scripts/dbidispatch	2008-04-01 20:22:52 UTC (rev 8750)
@@ -3,7 +3,7 @@
 from plearn.utilities.toolkit import search_file
 
 ScriptName="launchdbi.py"
-ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|*--nodbilog] [--condor|--bqtools[=nb_process]|--cluster[=nb_process]|--local[=nb_process]|--ssh[=nb_process]] [--nb_proc=nb_process] [--mem=X] [--os=X] [--test|*--no_test] [--long|*--no_long] [--micro[=nb_batch]] [--duree=X] [--wait|--nowait] [--req="CONDOR_REQUIREMENT"] [--force|*--no_force] [--interruptible|*--no_interruptible] [--cpu=nb_cpu_per_node] [*--getenv|--no_getenv] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- signals the default option value.'
+ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|**--nodbilog] [--condor|--bqtools[=nb_proc]|--cluster[=nb_proc]|--local[=nb_proc]|--ssh[=nb_proc]] [--nb_proc=nb_proc] [--mem=X] [--os=X] [--test|*--no_test] [--long|**--no_long] [--micro[=nb_batch]] [--duree=X] [**--cwait|--no_cwait] [--req="CONDOR_REQUIREMENT"] [--force|**--no_force] [--interruptible|**--no_interruptible] [--cpu=nb_cpu_per_node] [**--getenv|--no_getenv] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- signals the default option value in dbidispatch. An ** before -- signal a default option value in dbi'
 LongHelp="""Dispatches jobs with dbi.py. dbi can dispatch jobs on condor, bqtools, cluster, local and ssh. If no system is selected on the command line, we try them in the previous order. ssh is never automaticaly selected.
 
 %(ShortHelp)s
@@ -20,7 +20,7 @@
 
 
 bqtools, cluster, local and ssh options:
-  --nb_proc=nb_process, specifies the maximum number of concurrent jobs running. The value -1 will try to execute all jobs concurently. Use with care as some back-end or configuration do not handle this correctly.
+  --nb_proc=nb_proc, specifies the maximum number of concurrent jobs running. The value -1 will try to execute all jobs concurently. Use with care as some back-end or configuration do not handle this correctly.
     --local=X is the same as --local --nb_proc=X
     --cluster=X is the same as --cluster --nb_proc=X
     --bqtools=X is the same as --bqtools --nb_proc=X
@@ -47,8 +47,8 @@
   The '--3264', '--32' or '--64' specify which type of cpu the node must have to execute the commands.
 
 cluster only options:
-  The '--wait' is transfered to cluster. This must be enabled if there is not nb_process available nodes. Otherwise when there are no nodes available, the launch of that command fails.
-  The '--nowait' means the --wait option is not given to the cluster command, as in the default.
+  The '--cwait' is transfered to cluster. This must be enabled if there is not nb_proc available nodes. Otherwise when there are no nodes available, the launch of that command fails.
+  The '--no_cwait' means the --cwait option is not given to the cluster command, as in the default.
   The '--mem=X' speficify the number of meg the program need to execute.
   The '--os=X' speficify the os of the server: fc4 or fc7. Default: fc4
   The '--force' option is passed to cluster
@@ -138,34 +138,26 @@
         launch_cmd = argv[2].upper()+argv.split('=')[0][3:]
         if len(argv.split('='))>1:
             dbi_param["nb_proc"]=argv.split('=')[1]
-    elif argv.startswith("--ssh"):
-        launch_cmd = "Ssh"
-        if len(argv)>5:
-            assert(argv[5]=="=")
-            dbi_param["nb_proc"]=argv[6:]
-        dbi_param["file_redirect_stdout"]=False
-        dbi_param["file_redirect_stderr"]=False
+        if argv.startswith("--ssh"):
+            dbi_param["file_redirect_stdout"]=False
+            dbi_param["file_redirect_stderr"]=False
     elif argv.startswith("--file="):
         FILE = argv[7:]
     elif argv in ["--32","--64","--3264"]:
         dbi_param["arch"]=argv[2:]
-    elif argv == "--wait":
-        dbi_param["cluster_wait"]=True
-    elif argv == "--nowait":
-        dbi_param["cluster_wait"]=False
-    elif argv[0:6] == "--req=":
-        dbi_param["requirements"]="%s"%argv[6:]
     elif argv.startswith("--micro"):
         dbi_param["micro"]=20
         if len(argv)>7:
             assert(argv[7]=="=")
             dbi_param["micro"]=argv[8:]
-    elif argv in  ["--force", "--interruptible", "--long", "--test", "--getenv"]:
+    elif argv in  ["--force", "--interruptible", "--long", "--test",
+                   "--getenv", "--cwait"]:
         dbi_param[argv[2:]]=True
-    elif argv in ["--no_force", "--no_interruptible", "--no_long", "--no_test",\
-                      "--no_getenv", "--no_clean_up" ]:
+    elif argv in ["--no_force", "--no_interruptible", "--no_long", "--no_test",
+                  "--no_getenv", "--no_cwait", "--no_clean_up" ]:
         dbi_param[argv[5:]]=False
-    elif argv.split('=')[0] in ["--duree","--cpu","--mem","--os","--nb_proc"]:
+    elif argv.split('=')[0] in ["--duree","--cpu","--mem","--os",
+                                "--nb_proc","--req"]:
         dbi_param[argv.split('=')[0][2:]]=argv.split('=')[1]
     elif argv[0:1] == '-':
 	print "Unknow option (%s)",argv
@@ -179,15 +171,16 @@
     print ShortHelp
     sys.exit(1)
 
-valid_dbi_param=["clean_up", "test", "arch", "dolog"]
+valid_dbi_param=["clean_up", "test", "dolog"]
 if launch_cmd=="Ssh":
     valid_dbi_param+=["nb_proc"]
 elif launch_cmd=="Cluster":
-    valid_dbi_param +=["cluster_wait","force","nb_proc",'--duree','--3264', '--32', '--64',"--interruptible","--duree","--cpu","--mem","--os"]
+    valid_dbi_param +=["cwait","force","nb_proc","arch","interruptible",
+                       "duree","cpu","mem","os"]
 elif launch_cmd=="Condor":
-    valid_dbi_param +=["requirements", "--3264", "--32", "--64", "getenv"]
+    valid_dbi_param +=["req", "arch", "getenv"]
 elif launch_cmd=="Bqtools":
-    valid_dbi_param +=["--micro", "--long","nb_proc",'--duree','--micro']
+    valid_dbi_param +=["micro", "long","nb_proc",'duree','micro']
 elif launch_cmd=="Local":
     valid_dbi_param +=["nb_proc"]
 



From nouiz at mail.berlios.de  Thu Apr  3 15:41:22 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 3 Apr 2008 15:41:22 +0200
Subject: [Plearn-commits] r8751 - trunk/scripts
Message-ID: <200804031341.m33DfMke027879@sheep.berlios.de>

Author: nouiz
Date: 2008-04-03 15:41:22 +0200 (Thu, 03 Apr 2008)
New Revision: 8751

Modified:
   trunk/scripts/dbidispatch
Log:
removed duplicate item


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-04-01 20:22:52 UTC (rev 8750)
+++ trunk/scripts/dbidispatch	2008-04-03 13:41:22 UTC (rev 8751)
@@ -180,7 +180,7 @@
 elif launch_cmd=="Condor":
     valid_dbi_param +=["req", "arch", "getenv"]
 elif launch_cmd=="Bqtools":
-    valid_dbi_param +=["micro", "long","nb_proc",'duree','micro']
+    valid_dbi_param +=["micro", "long","nb_proc","duree"]
 elif launch_cmd=="Local":
     valid_dbi_param +=["nb_proc"]
 



From nouiz at mail.berlios.de  Thu Apr  3 16:00:21 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 3 Apr 2008 16:00:21 +0200
Subject: [Plearn-commits] r8752 - trunk/plearn/db
Message-ID: <200804031400.m33E0LLi029590@sheep.berlios.de>

Author: nouiz
Date: 2008-04-03 16:00:18 +0200 (Thu, 03 Apr 2008)
New Revision: 8752

Modified:
   trunk/plearn/db/getDataSet.cc
Log:
now set the mtime for amat, abmat, py et pymat file.


Modified: trunk/plearn/db/getDataSet.cc
===================================================================
--- trunk/plearn/db/getDataSet.cc	2008-04-03 13:41:22 UTC (rev 8751)
+++ trunk/plearn/db/getDataSet.cc	2008-04-03 14:00:18 UTC (rev 8752)
@@ -53,6 +53,7 @@
 #include <plearn/vmat/FileVMatrix.h>
 #include <plearn/vmat/VMat.h>
 #include <plearn/vmat/VVMatrix.h>
+#include <nspr/prtime.h>
 
 namespace PLearn {
 using namespace std;
@@ -97,6 +98,7 @@
             Mat tempMat;
             loadAsciiSingleBinaryDescriptor(dataset, tempMat);
             vm = VMat(tempMat);
+            vm->updateMtime(dataset);
         } else if (ext == "pmat") {
             vm = new FileVMatrix(dataset);
         } 
@@ -136,11 +138,31 @@
             if (vm.isNull())
                 PLERROR("In getDataSet - Object described in %s is not a VMatrix subclass",
                         dataset.absolute().c_str());
+            //Their is two case:
+            //1) params.size()>0, The mtime should be now
+            //2) params.size()==0 the mtime should be the file mtime
+            //     But as we can't trust the file mtime as it can
+            //     have dependency in it that we don't look,
+            //     we set it to 0 to be safe.
+            if(params.size()==0)
+                vm->updateMtime(0);
+            else{
+    // The NSPR PRTime is number of microseconds since the epoch, while
+    // time_t is the number of seconds since the (same) epoch.
+    // Translate from the former to the later by dividing by 1e6, using
+    // NSPR long long manipulation macros to be extra safe.
+                PRInt64 time_t_compatible_value;
+                PRInt64 one_million = LL_INIT(0, 1000000);
+                LL_DIV(time_t_compatible_value, PR_Now(), one_million);
+                vm->updateMtime((time_t)time_t_compatible_value);
+
+            }
         } else if (VMatrixExtensionRegistrar::VMatrixInstantiator inst =
                    VMatrixExtensionRegistrar::getInstantiator(ext))
         {
             // Support user-added extensions
             vm = inst(dataset);
+            vm->updateMtime(0);
         }
         else 
             PLERROR("In getDataSet - Unknown extension for VMat file: %s", ext.c_str());



From chapados at mail.berlios.de  Thu Apr  3 16:19:38 2008
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Thu, 3 Apr 2008 16:19:38 +0200
Subject: [Plearn-commits] r8753 - trunk/python_modules/plearn/utilities
Message-ID: <200804031419.m33EJcNB031190@sheep.berlios.de>

Author: chapados
Date: 2008-04-03 16:19:37 +0200 (Thu, 03 Apr 2008)
New Revision: 8753

Modified:
   trunk/python_modules/plearn/utilities/pldatetime.py
Log:
Added pljulian_to_date

Modified: trunk/python_modules/plearn/utilities/pldatetime.py
===================================================================
--- trunk/python_modules/plearn/utilities/pldatetime.py	2008-04-03 14:00:18 UTC (rev 8752)
+++ trunk/python_modules/plearn/utilities/pldatetime.py	2008-04-03 14:19:37 UTC (rev 8753)
@@ -19,6 +19,11 @@
 
     return date(yyyy,mm,dd)
 
+def pljulian_to_date(x):
+    """Convert a PLearn Julian Number to a Python date.
+    """
+    assert not isNaN(x)
+    return date.fromordinal(int(x - 1721425))
   
 def date_to_cyymmdd(d):
     return (d.year-1900)*10000 + d.month*100 +d.day



From chapados at mail.berlios.de  Thu Apr  3 16:20:20 2008
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Thu, 3 Apr 2008 16:20:20 +0200
Subject: [Plearn-commits] r8754 - trunk/plearn/opt
Message-ID: <200804031420.m33EKKN7031406@sheep.berlios.de>

Author: chapados
Date: 2008-04-03 16:20:19 +0200 (Thu, 03 Apr 2008)
New Revision: 8754

Modified:
   trunk/plearn/opt/ConjGradientOptimizer.cc
Log:
Some debugging code -- ifdefed out

Modified: trunk/plearn/opt/ConjGradientOptimizer.cc
===================================================================
--- trunk/plearn/opt/ConjGradientOptimizer.cc	2008-04-03 14:19:37 UTC (rev 8753)
+++ trunk/plearn/opt/ConjGradientOptimizer.cc	2008-04-03 14:20:19 UTC (rev 8754)
@@ -222,6 +222,13 @@
         this->params.update(alpha, this->search_direction);
         computeGradient(this->delta);
         cost = this->cost->value[0];
+
+#if 0
+        Vec tmpparams(this->params.nelems());
+        this->params >> tmpparams;
+        perr << "Params: " << tmpparams << "   Cost: " << cost << endl;
+#endif
+        
         derivative = dot(this->search_direction, this->delta);
         this->params.copyFrom(this->tmp_storage);
     }
@@ -238,6 +245,13 @@
     this->params.update(alpha, this->search_direction);
     this->proppath.fprop();
     real c = this->cost->value[0];
+
+#if 0
+    Vec tmpparams(this->params.nelems());
+    this->params >> tmpparams;
+    perr << "Params: " << tmpparams << "   Cost: " << c << endl;
+#endif
+
     this->params.copyFrom(this->tmp_storage);
     return c;
 }
@@ -252,6 +266,13 @@
     this->params.copyTo(this->tmp_storage);
     this->params.update(alpha, this->search_direction);
     computeGradient(this->delta);
+
+#if 0
+    Vec tmpparams(this->params.nelems());
+    this->params >> tmpparams;
+    perr << "Params: " << tmpparams << "   Cost: " << this->cost->value[0] << endl;
+#endif
+
     this->params.copyFrom(this->tmp_storage);
     return dot(this->search_direction, this->delta);
 }



From nouiz at mail.berlios.de  Thu Apr  3 17:12:36 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 3 Apr 2008 17:12:36 +0200
Subject: [Plearn-commits] r8755 - trunk/plearn_learners/meta
Message-ID: <200804031512.m33FCagV005652@sheep.berlios.de>

Author: nouiz
Date: 2008-04-03 17:12:36 +0200 (Thu, 03 Apr 2008)
New Revision: 8755

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
optimisation


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2008-04-03 14:20:19 UTC (rev 8754)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2008-04-03 15:12:36 UTC (rev 8755)
@@ -335,13 +335,10 @@
         if (train_set->weightsize()>0)
         {
             PP<ProgressBar> pb;
-            if(report_progress) pb = new ProgressBar(
-                "AdaBoost round " + tostring(stage) +
-                ": extracting initial weights", n);
             initial_sum_weights=0;
+            int weight_col = train_set->inputsize()+train_set->targetsize();
             for (int i=0; i<n; ++i) {
-                if(report_progress) pb->update(i);
-                train_set->getExample(i, input, target, weight);
+                weight=train_set->get(i,weight_col);
                 example_weights[i]=weight;
                 initial_sum_weights += weight;
             }



From chapados at mail.berlios.de  Thu Apr  3 18:12:17 2008
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Thu, 3 Apr 2008 18:12:17 +0200
Subject: [Plearn-commits] r8756 - trunk/plearn/math
Message-ID: <200804031612.m33GCHFX012056@sheep.berlios.de>

Author: chapados
Date: 2008-04-03 18:12:16 +0200 (Thu, 03 Apr 2008)
New Revision: 8756

Modified:
   trunk/plearn/math/TMat_decl.h
Log:
Bugfix when resizing a mod-zero matrix

Modified: trunk/plearn/math/TMat_decl.h
===================================================================
--- trunk/plearn/math/TMat_decl.h	2008-04-03 15:12:36 UTC (rev 8755)
+++ trunk/plearn/math/TMat_decl.h	2008-04-03 16:12:16 UTC (rev 8756)
@@ -215,7 +215,7 @@
         else
         {
             int usage = storage->usage();
-            if (usage > 1 && new_width > mod()-offset_%mod())
+            if (usage > 1 && mod() != 0 && new_width > mod()-offset_%mod())
                 resizeModError();
             else if (preserve_content && size() > 0)
                 resizePreserve(new_length, new_width, extra);



From louradou at mail.berlios.de  Thu Apr  3 21:54:23 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Thu, 3 Apr 2008 21:54:23 +0200
Subject: [Plearn-commits] r8757 - trunk/python_modules/plearn/learners
Message-ID: <200804031954.m33JsNa5020383@sheep.berlios.de>

Author: louradou
Date: 2008-04-03 21:54:22 +0200 (Thu, 03 Apr 2008)
New Revision: 8757

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:


Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2008-04-03 16:12:16 UTC (rev 8756)
+++ trunk/python_modules/plearn/learners/SVM.py	2008-04-03 19:54:22 UTC (rev 8757)
@@ -1,6 +1,5 @@
 import os
 
-#import plearn.utilities
 from libsvm import *
 
 from plearn.pyext import *
@@ -8,6 +7,7 @@
 from math import *
 import random
 
+
 class SVMHyperParamOracle__kernel(object):
     """ An oracle that gives values of hyperparameters      
         to train a SVM on vectors, given a popular kernel.
@@ -534,6 +534,8 @@
     """ Return <bool>: whether or not we should try other hyperparameter values.
     """
     def should_be_tuned_again(self):
+        if len(self.trials_param_list) > 50:
+            return False
         best_degree = self.best_param['degree']
         tried_degrees = self.get_trials_oneparam_list('degree')
         if( best_degree in tried_degrees
@@ -732,8 +734,7 @@
         self.class_priors = None
         self.weight = None
         self.labels = None
-        
-        
+                
         self.results_filename      = None
         self.preproc_optionnames  = []
         self.preproc_optionvalues = []
@@ -749,6 +750,8 @@
         self.validset_key = 'validset'
         self.testset_key  = 'testset'
 
+        self.compute_outputs_from_probabilities = None
+
     def forget(self):
         for expert in self.all_experts:
              expert.forget()
@@ -762,14 +765,18 @@
         #       a good candidate
 
     def train_inputspec(self, dataspec):
+        assert type(dataspec) == dict
         if self.trainset_key not in dataspec:
-            return None
+            raise KeyError, "Key %s not in dataspec (keys %s)" % \
+                            ( self.trainset_key, dataspec.keys() )
         return dataspec[ self.trainset_key ]
     def valid_inputspec(self, dataspec):
+        assert type(dataspec) == dict
         if self.validset_key not in dataspec:
             return None
         return dataspec[ self.validset_key ]
     def test_inputspec(self, dataspec):
+        assert type(dataspec) == dict
         if self.testset_key not in dataspec:
             return None
         return dataspec[ self.testset_key ]
@@ -913,8 +920,13 @@
                       for pn in param )
         if len(s)>0:s=', '+s
 
-        # Note: 'svm_type = C_SVC' stands for classification
-        return eval('svm_parameter( svm_type = C_SVC '+s+')' )
+        
+        if self.compute_outputs_from_probabilities:
+            # if this function is defined (see 
+            return eval('svm_parameter( svm_type = C_SVC, probability = 1 '+s+')' )
+        else:
+            # Note: 'svm_type = C_SVC' stands for classification
+            return eval('svm_parameter( svm_type = C_SVC '+s+')' )
     
 
     """ Write given results with corresponding parameters
@@ -1146,12 +1158,24 @@
             teststats= VecStatsCollector()        
             teststats.setFieldNames( costnames )
 
-        predictions=[]
-        for x in samples:
-            ## specific to libsvm
-            predictions.append( int(model.predict(x)) )
-        predictions, targets = self.update_predictions_targets( predictions, targets, testset)
+        if self.compute_outputs_from_probabilities:
+            predictions=[]
+            probas=[]
+            for x in samples:
+                ## specific to libsvm
+                prd, prb = model.predict_probability(x)
+                probas.append(prb)
+                predictions.append( int(prd) )
+            predictions, targets = self.compute_outputs_from_probabilities( probas, targets, testset)
 
+        else:
+            predictions=[]
+            for x in samples:
+                ## specific to libsvm
+                predictions.append( int(model.predict(x)) )
+            predictions, targets = self.update_predictions_targets( predictions, targets, testset)
+
+
         # Computing misclassification costs for the default normalized
         # classification error (= class error weighted w.r.t class priors)
         
@@ -1265,8 +1289,11 @@
         return validstats
 
 
-    """ THE interesting function of the class
-# TODO: document the function
+    """ THE interesting function of the class.
+        See __main__ below for usage.
+        dataspec is a dictionary which specifies train, valid, test sets.
+        The train set is mandatory, but valid and/or test sets can be missing.
+        cf. train_inputspec(), valid_inputspec(), and test_inputspec().
     """
     def train_and_tune(self, dataspec):
 
@@ -1293,13 +1320,17 @@
 
             valid_stats = self.valid(dataspec, param)
 
-            # Better valid cost is obtained!
+            # No improvement measured
             if not self.update_trials( param, valid_stats ):
                 if self.verbosity > 0:
                     print " -- valid costs: ", self.get_all_costs( valid_stats )
 
                 # We reject the model (to avoid testing on it)
                 self.model = None
+                
+                self.write_results( param, valid_stats )
+
+            # Better valid cost is obtained!
             else:
 
                 # Cross Validation
@@ -1308,27 +1339,27 @@
 
                 # Simple Validation
                 else:
+                    self.validtype = 'simple'
                     self.best_model = self.model
                     if self.retrain_on_valid:
 
-                        # TODO: remove this part
+                        """ Uncomment following lines if you want to check that
+                            retraining on {train + valid} sets does not degrade.
+                        """
+                        #train_stats = None
+                        #test_stats = None
+                        #if self.test_on_train:
+                        #    if self.verbosity > 2:
+                        #        print "\n** (testing simple valid on "+self.trainset_key+")"
+                        #    train_stats = self.test( trainset )
+                        #if testset <> None:
+                        #    if self.verbosity > 2:
+                        #        print "\n** (testing simple valid on "+self.testset_key+")"
+                        #test_stats = self.test( testset  )
+                        #
+                        #self.write_results( self.best_param,
+                        #                    valid_stats, test_stats, train_stats )
 
-                        self.validtype = 'simple'
-
-                        train_stats = None
-                        test_stats = None
-                        if self.test_on_train:
-                            if self.verbosity > 2:
-                                print "\n** (testing simple valid on "+self.trainset_key+")"
-                            train_stats = self.test( trainset )
-                        if testset <> None:
-                            if self.verbosity > 2:
-                                print "\n** (testing simple valid on "+self.testset_key+")"
-                        test_stats = self.test( testset  )
-
-                        self.write_results( self.best_param,
-                                            valid_stats, test_stats, train_stats )
-
                         self.validtype = 'simple+retrain'
 
                         tv_set = ConcatRowsVMatrix(
@@ -1361,7 +1392,27 @@
 
         return dataspec
 
+""" Some AUXILIARY FUNCTIONS, to have access or modify 
+    the input statistics.
+"""
+def get_std_cmp(data,i):
+    values=[float(vec[i]) for vec in data]
+    tot = sum(values)
+    avg = tot*1.0/len(values)
+    sdsq = sum([(i-avg)**2 for i in values])
+    return (sdsq*1.0/(len(values)-1 or 1))**.5
 
+def mean_std(data):
+    stds=[get_std_cmp(data,i) for i in range(len(data[0]))]
+    while 0 in stds:
+        stds.remove(0)
+    stds=array(stds)
+    return stds.mean(), stds.std()
+
+def get_mean_cmp(data,i):
+    values=[float(vec[i]) for vec in data]
+    return  sum(values)/len(values)
+
 def normalize_data(data, mean=None, std=None):
     if mean == None:
         mean=[]
@@ -1380,23 +1431,9 @@
             data[j][i]=(data[j][i]-mean[i])/std[i]
     return mean, std
 
-def mean_std(data):
-    stds=[get_std_cmp(data,i) for i in range(len(data[0]))]
-    while 0 in stds:
-        stds.remove(0)
-    stds=array(stds)
-    return stds.mean(), stds.std()
-
-def get_std_cmp(data,i):
-    values=[float(vec[i]) for vec in data]
-    tot = sum(values)
-    avg = tot*1.0/len(values)
-    sdsq = sum([(i-avg)**2 for i in values])
-    return (sdsq*1.0/(len(values)-1 or 1))**.5
-def get_mean_cmp(data,i):
-    values=[float(vec[i]) for vec in data]
-    return  sum(values)/len(values)
-
+""" Geometric mean (useful to deal with multiplicative hyperparameters
+                    such as 'C', 'gamma', ...)
+"""
 def geom_mean(data):
     if type(data[0]) == list:
         res=[]
@@ -1409,7 +1446,80 @@
             prod *= value
         return prod**(1./len(data))
 
+""""""
 
+""" In the following:
+    Some (API) Functions that can be assigned
+    to a SVM object, to deal with bags of data
+    to classify.
+"""
+"""
+
+# Usage:
+#-------
+    
+svm = SVM()
+svm.get_datalist = ToBagClassifier_get_datalist
+
+vote_with_proba = 1 # whether or not to use probabiblity
+                    # instead of hard vote [0,1] for each element of a bag.
+if vote_with_proba:
+    svm.compute_outputs_from_probabilities = ToBagClassifier_compute_outputs_from_probabilities
+else:
+    svm.update_predictions_targets = ToBagClassifier_update_predictions_targets
+"""         
+def get_baginfo( input_vmat ):
+    data_array = input_vmat.getMat()
+    inputsize = input_vmat.inputsize
+    targetsize = input_vmat.targetsize
+    assert targetsize == 2
+    baginfo = [ int(t) for t in data_array[:,inputsize+1] ]
+    return baginfo
+
+def ToBagClassifier_get_datalist( input_vmat ):
+    data_array = input_vmat.getMat()
+    inputsize = input_vmat.inputsize
+    targetsize = input_vmat.targetsize
+    nsamples = input_vmat.length
+    assert shape(data_array)[0] == nsamples
+    assert targetsize==2
+    samples   = [ [ float(x_t_i)    for x_t_i in x_t ]
+                                    for x_t in data_array[:,:inputsize] ]
+    targets   = [ float(t) for t in data_array[:,inputsize] ]
+    return samples, targets
+
+def ToBagClassifier_update_predictions_targets(predictions, targets, vmat):
+    baginfo = get_baginfo(vmat)
+    assert len(predictions) == len(targets) == len(baginfo)
+    nclasses = max(predictions)+1
+    bag_predictions=[]
+    bag_targets=[]
+    for p, t, b in zip(predictions, targets, baginfo):
+        if b in [1,3]: # beginning of a bag
+            votes = zeros(nclasses)
+        votes[p] +=1
+        if b in [2,3]: # end of a bag
+            bag_predictions.append( votes.argmax() )
+            bag_targets.append( t )
+    return bag_predictions, bag_targets
+
+def ToBagClassifier_compute_outputs_from_probabilities(probas, targets, vmat):
+    baginfo = get_baginfo(vmat)
+    assert len(probas) == len(targets) == len(baginfo)
+    nclasses = len(probas[0])
+    bag_predictions=[]
+    bag_targets=[]
+    for p, t, b in zip(probas, targets, baginfo):
+        if b in [1,3]: # beginning of a bag
+            votes = zeros(nclasses)
+        for c in p:
+            votes[c] += p[c]
+        if b in [2,3]: # end of a bag
+            bag_predictions.append( votes.argmax() )
+            bag_targets.append( t )
+    return bag_predictions, bag_targets
+
+
 if __name__ == '__main__':
 
     import os.path



From nouiz at mail.berlios.de  Fri Apr  4 17:07:55 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 4 Apr 2008 17:07:55 +0200
Subject: [Plearn-commits] r8758 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200804041507.m34F7tef004797@sheep.berlios.de>

Author: nouiz
Date: 2008-04-04 17:07:55 +0200 (Fri, 04 Apr 2008)
New Revision: 8758

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
--added option --nice, --machine=, --machines=, --server in condor mode
--in dbi.py, in condor mode getenv is false by default. This is not safe to set it at true, as HOSTNAME is not set correctly otherwise


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-04-03 19:54:22 UTC (rev 8757)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-04-04 15:07:55 UTC (rev 8758)
@@ -664,7 +664,8 @@
         if (not os.path.abspath(os.path.curdir).startswith("/home/fringant2/")) and gethostname().endswith(".iro.umontreal.ca"):
             raise Exception("You must be in a subfolder of /home/fringant2/")
 
-        self.getenv = True
+        self.getenv = False
+        self.nice = False
         self.req = ''
         DBIBase.__init__(self, commands, **args)
         if not os.path.exists(self.log_dir):
@@ -800,10 +801,11 @@
                 error          = %s/condor.%s.$(Process).error
                 log            = %s/condor.log
                 getenv         = %s
+                nice_user      = %s
                 ''' % (self.log_dir,req,
                        self.log_dir,self.unique_id,
                        self.log_dir,self.unique_id,
-                       self.log_dir,str(self.getenv))))
+                       self.log_dir,str(self.getenv),str(self.nice))))
 
         if len(condor_datas)!=0:
             for i in condor_datas:

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-04-03 19:54:22 UTC (rev 8757)
+++ trunk/scripts/dbidispatch	2008-04-04 15:07:55 UTC (rev 8758)
@@ -3,7 +3,7 @@
 from plearn.utilities.toolkit import search_file
 
 ScriptName="launchdbi.py"
-ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|**--nodbilog] [--condor|--bqtools[=nb_proc]|--cluster[=nb_proc]|--local[=nb_proc]|--ssh[=nb_proc]] [--nb_proc=nb_proc] [--mem=X] [--os=X] [--test|*--no_test] [--long|**--no_long] [--micro[=nb_batch]] [--duree=X] [**--cwait|--no_cwait] [--req="CONDOR_REQUIREMENT"] [--force|**--no_force] [--interruptible|**--no_interruptible] [--cpu=nb_cpu_per_node] [**--getenv|--no_getenv] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- signals the default option value in dbidispatch. An ** before -- signal a default option value in dbi'
+ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|**--nodbilog] [--condor|--bqtools[=nb_proc]|--cluster[=nb_proc]|--local[=nb_proc]|--ssh[=nb_proc]] [--nb_proc=nb_proc] [--mem=X] [--os=X] [--test|*--no_test] [--long|**--no_long] [--micro[=nb_batch]] [--duree=X] [**--cwait|--no_cwait] [--req="CONDOR_REQUIREMENT"] [--force|**--no_force] [--nice|**--no_nice] [--interruptible|**--no_interruptible] [--cpu=nb_cpu_per_node] [**--getenv|--no_getenv] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- signals the default option value in dbidispatch. An ** before -- signal a default option value in dbi'
 LongHelp="""Dispatches jobs with dbi.py. dbi can dispatch jobs on condor, bqtools, cluster, local and ssh. If no system is selected on the command line, we try them in the previous order. ssh is never automaticaly selected.
 
 %(ShortHelp)s
@@ -62,9 +62,18 @@
   dbidispatch \"--req=Machine==\\\"computer.example.com\\\"\"
      or
   dbidispatch '--req=Machine=="computer.example.com"' 
-     or
-  dbidispatch '--req=regexp("mona0*", target.Machine)'
 
+  The '--server'(--no_server) option add the requirement that the executing host must be a server dedicated to computing. This is equivalent to: dbidispatch '--req=SERVER==True'(SERVER==False)
+  The '--machine=full_host_name' option add the requirement that the executing host is full_host_name
+     dbidispatch --machine=computer.example.com
+        witch is equivalent to
+     dbidispatch '--req=Machine=="computer.example.com"' 
+  The '--machines=regexp' option add the requirement that the executing host name must be match the regexp
+     dbidispatch '--machines=computer00*'
+        witch is equivalent to
+     dbidispatch '--req=regexp("computer0*", target.Machine)'
+  The '--nice'('--no_nice') option set the nice_user option to condor. If nice, the job(s) will have the lowest possible priority.
+
 where <command-template> is interpreted as follows: the first argument
 is the <command> above, and the rest are interpreted as <arguments>.
 The arguments may contain segments of the form {{a,b,c,d}}, which trigger
@@ -151,16 +160,34 @@
             assert(argv[7]=="=")
             dbi_param["micro"]=argv[8:]
     elif argv in  ["--force", "--interruptible", "--long", "--test",
-                   "--getenv", "--cwait"]:
+                   "--getenv", "--cwait", "--nice"]:
         dbi_param[argv[2:]]=True
     elif argv in ["--no_force", "--no_interruptible", "--no_long", "--no_test",
-                  "--no_getenv", "--no_cwait", "--no_clean_up" ]:
+                  "--no_getenv", "--no_cwait", "--no_clean_up" , "--no_nice"]:
         dbi_param[argv[5:]]=False
     elif argv.split('=')[0] in ["--duree","--cpu","--mem","--os",
                                 "--nb_proc","--req"]:
         dbi_param[argv.split('=')[0][2:]]=argv.split('=')[1]
+    elif argv.startswith('--machine=') or argv.startswith('--machines='):
+        if argv.split('=')[0] == "--machine":
+            new='Machine=="'+argv.split('=')[1]+'"'
+        elif argv.split('=')[0] == "--machines":
+            new=dbi_param["req"]='regexp("'+argv.split('=')[1]+'", target.Machine)'
+        s=dbi_param.get('req','')
+        if s:
+            s+=' && '
+        dbi_param["req"]=s+new
+    elif argv=="--server" or argv=="--no_server":
+        if 'req' in dbi_param:
+            dbi_param["req"]+=' && '
+        else:
+            dbi_param['req']=''
+        if argv=="--server":
+            dbi_param["req"]+='SERVER==True'
+        else:
+            dbi_param["req"]+='SERVER==False'
     elif argv[0:1] == '-':
-	print "Unknow option (%s)",argv
+	print "Unknow option (%s)"%argv
 	print ShortHelp
         sys.exit(1)
     else:
@@ -178,7 +205,7 @@
     valid_dbi_param +=["cwait","force","nb_proc","arch","interruptible",
                        "duree","cpu","mem","os"]
 elif launch_cmd=="Condor":
-    valid_dbi_param +=["req", "arch", "getenv"]
+    valid_dbi_param +=["req", "arch", "getenv", "nice"]
 elif launch_cmd=="Bqtools":
     valid_dbi_param +=["micro", "long","nb_proc","duree"]
 elif launch_cmd=="Local":



From saintmlx at mail.berlios.de  Fri Apr  4 19:57:01 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Fri, 4 Apr 2008 19:57:01 +0200
Subject: [Plearn-commits] r8759 - trunk/plearn/python
Message-ID: <200804041757.m34Hv10d008739@sheep.berlios.de>

Author: saintmlx
Date: 2008-04-04 19:57:00 +0200 (Fri, 04 Apr 2008)
New Revision: 8759

Modified:
   trunk/plearn/python/PythonExtension.cc
Log:
- corrected error msgs



Modified: trunk/plearn/python/PythonExtension.cc
===================================================================
--- trunk/plearn/python/PythonExtension.cc	2008-04-04 15:07:55 UTC (rev 8758)
+++ trunk/plearn/python/PythonExtension.cc	2008-04-04 17:57:00 UTC (rev 8759)
@@ -413,7 +413,7 @@
 {
     PLASSERT(the_PLearn_python_module);
     if(-1 == PyObject_SetAttrString(the_PLearn_python_module, "_tmp_wrapped_instance", o))
-        PLERROR("in addToWrappedObjectsSet : cannot add wrapped object to module.");
+        PLERROR("in removeFromWrappedObjectsSet : cannot add wrapped object to module.");
     PyObject* res= PyRun_String("\nwrapped_PLearn_instances.remove(_tmp_wrapped_instance)"
                                 "\ndel _tmp_wrapped_instance\n", 
                                 Py_file_input, 
@@ -422,7 +422,7 @@
     if(!res)
     {
         if(PyErr_Occurred()) PyErr_Print();
-        PLERROR("in addToWrappedObjectsSet : cannot add wrapped object to set.");
+        PLERROR("in removeFromWrappedObjectsSet : cannot remove wrapped object from set.");
     }
     Py_DECREF(res);
 }



From saintmlx at mail.berlios.de  Fri Apr  4 19:58:06 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Fri, 4 Apr 2008 19:58:06 +0200
Subject: [Plearn-commits] r8760 - trunk/plearn/math
Message-ID: <200804041758.m34Hw6cO008845@sheep.berlios.de>

Author: saintmlx
Date: 2008-04-04 19:58:06 +0200 (Fri, 04 Apr 2008)
New Revision: 8760

Modified:
   trunk/plearn/math/stats_utils.cc
Log:
- added remote version of KS_test



Modified: trunk/plearn/math/stats_utils.cc
===================================================================
--- trunk/plearn/math/stats_utils.cc	2008-04-04 17:57:00 UTC (rev 8759)
+++ trunk/plearn/math/stats_utils.cc	2008-04-04 17:58:06 UTC (rev 8760)
@@ -44,6 +44,7 @@
 #include "TMat_maths.h"
 #include "pl_erf.h"
 #include "random.h"
+#include <plearn/base/RemoteDeclareMethod.h>
 
 namespace PLearn {
 using namespace std;
@@ -339,6 +340,20 @@
     return ks_stat;
 }
 
+tuple<real,real> remote_KS_test(Vec& v1, Vec& v2, int conv)
+{
+    real D, pvalue;
+    KS_test(v1,v2,conv,D, pvalue);
+    return make_tuple(D, pvalue);
+}
+
+tuple<Vec,Vec> remote_KS_tests(VMat& m1, VMat& m2, int conv)
+{
+    Vec Ds, pvalues;
+    KS_test(m1, m2, conv, Ds, pvalues);
+    return make_tuple(Ds, pvalues);
+}
+
 real paired_t_test(Vec u, Vec v)
 {
     int n = u.length();
@@ -407,6 +422,28 @@
 }
 
 
+
+
+BEGIN_DECLARE_REMOTE_FUNCTIONS
+
+    declareFunction("KS_test", &remote_KS_test,
+                    (BodyDoc("Returns result of Kolmogorov-Smirnov test between 2 samples.\n"),
+                     ArgDoc ("v1","Vec1: first distr."),
+                     ArgDoc ("v2","Vec2: second distr."),
+                     ArgDoc ("conv","precision"),
+                     RetDoc ("tuple of (D, p-value)")));
+
+    declareFunction("KS_tests", &remote_KS_tests,
+                    (BodyDoc("Returns result of Kolmogorov-Smirnov test between 2 VMats, for each column.\n"),
+                     ArgDoc ("m1","VMat1: first distr."),
+                     ArgDoc ("m2","VMat2: second distr."),
+                     ArgDoc ("conv","precision"),
+                     RetDoc ("tuple of (Ds, p-values)")));
+
+END_DECLARE_REMOTE_FUNCTIONS
+
+
+
 } // end of namespace PLearn
 
 /* 
@@ -464,6 +501,7 @@
 
 */
 
+
 
 /*
   Local Variables:



From saintmlx at mail.berlios.de  Fri Apr  4 19:59:42 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Fri, 4 Apr 2008 19:59:42 +0200
Subject: [Plearn-commits] r8761 - trunk/plearn/math
Message-ID: <200804041759.m34HxgRI008950@sheep.berlios.de>

Author: saintmlx
Date: 2008-04-04 19:59:42 +0200 (Fri, 04 Apr 2008)
New Revision: 8761

Modified:
   trunk/plearn/math/stats_utils.h
Log:
- added remote version of KS_test



Modified: trunk/plearn/math/stats_utils.h
===================================================================
--- trunk/plearn/math/stats_utils.h	2008-04-04 17:58:06 UTC (rev 8760)
+++ trunk/plearn/math/stats_utils.h	2008-04-04 17:59:42 UTC (rev 8761)
@@ -142,6 +142,12 @@
  */
 real KS_test(Vec& v1, Vec& v2, int conv=10);
 
+/** 
+ * Returns result of Kolmogorov-Smirnov test between 2 samples
+ * (D and p-value)
+ * The call sorts v1 and v2.
+ */
+tuple<real,real> remote_KS_test(Vec& v1, Vec& v2, int conv);
 
 /** 
  * Returns result of Kolmogorov-Smirnov test for each pair of variable
@@ -149,6 +155,14 @@
  */
 void KS_test(const VMat& m1, const VMat& m2, const int conv, Vec& Ds, Vec& p_values, const bool report_progress = false);
 
+/** 
+ * Returns result of Kolmogorov-Smirnov test for each pair of variable
+ * between the two VMat
+ * (Ds and p-values)
+ * The call sorts v1 and v2.
+ */
+tuple<Vec,Vec> remote_KS_tests(VMat& m1, VMat& m2, int conv);
+
 /**
  * Given two paired sets u and v of n measured values, the paired t-test 
  * determines whether they differ from each other in a significant way under 



From saintmlx at mail.berlios.de  Fri Apr  4 20:00:39 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Fri, 4 Apr 2008 20:00:39 +0200
Subject: [Plearn-commits] r8762 - trunk/plearn/vmat
Message-ID: <200804041800.m34I0dcw009111@sheep.berlios.de>

Author: saintmlx
Date: 2008-04-04 20:00:39 +0200 (Fri, 04 Apr 2008)
New Revision: 8762

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
- suppress warning on sizes when saving to PMat



Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-04-04 17:59:42 UTC (rev 8761)
+++ trunk/plearn/vmat/VMatrix.cc	2008-04-04 18:00:39 UTC (rev 8762)
@@ -1227,7 +1227,7 @@
 
     // Fill missing size if possible, also display warning when sizes are not
     // compatible with the width.
-    computeMissingSizeValue();
+    computeMissingSizeValue(false);
 
     // Copy fieldnames from vm if not set and they look good.
     bool same_fields_as_source =



From nouiz at mail.berlios.de  Fri Apr  4 20:28:08 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 4 Apr 2008 20:28:08 +0200
Subject: [Plearn-commits] r8763 - trunk/python_modules/plearn/pymake
Message-ID: <200804041828.m34IS8hu011281@sheep.berlios.de>

Author: nouiz
Date: 2008-04-04 20:28:08 +0200 (Fri, 04 Apr 2008)
New Revision: 8763

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
tell when the waiting for nfs is finished


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-04-04 18:00:39 UTC (rev 8762)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-04-04 18:28:08 UTC (rev 8763)
@@ -1053,8 +1053,9 @@
                 link_exit_code = 1
         else:
             if not local_compilation:
-                print 'Waiting for NFS to catch up...'
+                print 'Waiting for NFS to catch up...',
                 ccfile.nfs_wait_for_all_linkable_ofiles()
+                print 'done'
             if local_ofiles:
                 copy_ofiles_locally(executables_to_link)
             if verbose>=2:



From nouiz at mail.berlios.de  Mon Apr  7 22:49:21 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 7 Apr 2008 22:49:21 +0200
Subject: [Plearn-commits] r8764 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200804072049.m37KnLV3017597@sheep.berlios.de>

Author: nouiz
Date: 2008-04-07 22:49:20 +0200 (Mon, 07 Apr 2008)
New Revision: 8764

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
all stuff that is for lisa are done in dbidispatch only.
added an option for DBICondor.copy_local_source_file that default to false


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-04-04 18:28:08 UTC (rev 8763)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-04-07 20:49:20 UTC (rev 8764)
@@ -660,13 +660,10 @@
 class DBICondor(DBIBase):
 
     def __init__( self, commands, **args ):
-        from socket import gethostname
-        if (not os.path.abspath(os.path.curdir).startswith("/home/fringant2/")) and gethostname().endswith(".iro.umontreal.ca"):
-            raise Exception("You must be in a subfolder of /home/fringant2/")
-
         self.getenv = False
         self.nice = False
         self.req = ''
+        self.copy_local_source_file = False
         DBIBase.__init__(self, commands, **args)
         if not os.path.exists(self.log_dir):
             os.mkdir(self.log_dir) # condor log are always generated
@@ -827,6 +824,15 @@
                 if mtimed>mtimel:
                     print '[DBI] WARNING: We overwrite the file "'+launch_file+'" with a new version. Update it to your need!'
                     overwrite_launch_file=True
+        source_file=os.getenv("CONDOR_LOCAL_SOURCE")
+        
+        if self.copy_local_source_file:
+            source_file_dest = os.path.join(self.log_dir,
+                                            os.path.basename(source_file))
+            shutil.copy( source_file, source_file_dest)
+            self.temp_files.append(source_file_dest)
+            os.chmod(source_file_dest, 0755)   
+            source_file=source_file_dest
 
         if not os.path.exists(launch_file) or overwrite_launch_file:
             self.temp_files.append(launch_file)
@@ -835,8 +841,8 @@
                 #!/bin/sh
                 PROGRAM=$1
                 shift\n'''))
-            if os.getenv("CONDOR_LOCAL_SOURCE"):
-                launch_dat.write('source ' + os.getenv("CONDOR_LOCAL_SOURCE") + '\n')
+            if source_file:
+                launch_dat.write('source ' + source_file_dest + '\n')
             launch_dat.write(dedent('''\
                     echo "Executing on " `hostname` 1>&2
                     echo "HOSTNAME: ${HOSTNAME}" 1>&2

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-04-04 18:28:08 UTC (rev 8763)
+++ trunk/scripts/dbidispatch	2008-04-07 20:49:20 UTC (rev 8764)
@@ -105,10 +105,8 @@
 
 In the file of the option --file=FILEPATH, there must not be double quotes around the {{}} as they are for the shell and if the command is in the file, they are not interpreted by the shell.
 
+The '--no_clean_up' set the DBI option clean_up to false
 
-Not defined but existing parameter: --no_clean_up.
-
-
 The environnement variable DBIDISPATCH_DEFAULT_OPTION can contain default option that you always want to pass to dbidispatch. You can override them on the command line.
 """%{'ShortHelp':ShortHelp,'ScriptName':ScriptName}
 
@@ -218,6 +216,13 @@
         print "WARNING: The parameter",i,"is not valid for the",launch_cmd,"back-end"
 print "With the command to be expended:"," ".join(command_argv),"\n\n"
 
+from socket import gethostname
+if gethostname().endswith(".iro.umontreal.ca"):
+    if not os.path.abspath(os.path.curdir).startswith("/home/fringant2/"):
+        raise Exception("You must be in a subfolder of /home/fringant2/")
+    if not os.getenv("CONDOR_LOCAL_SOURCE").startswith("/home/fringant2/"):
+        dbi_param['copy_local_source_file']=True
+
 def generate_combination(repl):
     if repl == []:
         return []



From nouiz at mail.berlios.de  Mon Apr  7 23:47:26 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 7 Apr 2008 23:47:26 +0200
Subject: [Plearn-commits] r8765 - trunk/scripts
Message-ID: <200804072147.m37LlQpD023041@sheep.berlios.de>

Author: nouiz
Date: 2008-04-07 23:47:26 +0200 (Mon, 07 Apr 2008)
New Revision: 8765

Modified:
   trunk/scripts/dbidispatch
Log:
modified to print all option passed to dbi


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-04-07 20:49:20 UTC (rev 8764)
+++ trunk/scripts/dbidispatch	2008-04-07 21:47:26 UTC (rev 8765)
@@ -209,6 +209,14 @@
 elif launch_cmd=="Local":
     valid_dbi_param +=["nb_proc"]
 
+from socket import gethostname
+if gethostname().endswith(".iro.umontreal.ca"):
+    if not os.path.abspath(os.path.curdir).startswith("/home/fringant2/"):
+        raise Exception("You must be in a subfolder of /home/fringant2/")
+    f=os.getenv("CONDOR_LOCAL_SOURCE")
+    if f and not f.startswith("/home/fringant2/"):
+        dbi_param['copy_local_source_file']=True
+
 print "\n\nThe jobs will be launched on the system:", launch_cmd
 print "With options: ",dbi_param
 for i in dbi_param:
@@ -216,13 +224,6 @@
         print "WARNING: The parameter",i,"is not valid for the",launch_cmd,"back-end"
 print "With the command to be expended:"," ".join(command_argv),"\n\n"
 
-from socket import gethostname
-if gethostname().endswith(".iro.umontreal.ca"):
-    if not os.path.abspath(os.path.curdir).startswith("/home/fringant2/"):
-        raise Exception("You must be in a subfolder of /home/fringant2/")
-    if not os.getenv("CONDOR_LOCAL_SOURCE").startswith("/home/fringant2/"):
-        dbi_param['copy_local_source_file']=True
-
 def generate_combination(repl):
     if repl == []:
         return []



From nouiz at mail.berlios.de  Mon Apr  7 23:48:33 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 7 Apr 2008 23:48:33 +0200
Subject: [Plearn-commits] r8766 - trunk/python_modules/plearn/parallel
Message-ID: <200804072148.m37LmXn6023080@sheep.berlios.de>

Author: nouiz
Date: 2008-04-07 23:48:32 +0200 (Mon, 07 Apr 2008)
New Revision: 8766

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
bugfix and implemented a script in DBICondor to accept file in csh format in CONDOR_LOCAL_SOURCE environment variable


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-04-07 21:47:26 UTC (rev 8765)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-04-07 21:48:32 UTC (rev 8766)
@@ -790,8 +790,14 @@
         if self.req != "":
             req = req+'&&('+self.req+')'
 
+        source_file=os.getenv("CONDOR_LOCAL_SOURCE")
+        if source_file and source_file.endswith(".cshrc"):
+            launch_file = os.path.join(self.log_dir, 'launch.csh')
+        else:
+            launch_file = os.path.join(self.log_dir, 'launch.sh')
+
         condor_dat.write( dedent('''\
-                executable     = %s/launch.sh
+                executable     = %s
                 universe       = vanilla
                 requirements   = %s
                 output         = %s/condor.%s.$(Process).out
@@ -799,7 +805,7 @@
                 log            = %s/condor.log
                 getenv         = %s
                 nice_user      = %s
-                ''' % (self.log_dir,req,
+                ''' % (launch_file,req,
                        self.log_dir,self.unique_id,
                        self.log_dir,self.unique_id,
                        self.log_dir,str(self.getenv),str(self.nice))))
@@ -812,7 +818,6 @@
                 condor_dat.write("arguments      = %s \nqueue\n" %(' ; '.join(task.commands)))
         condor_dat.close()
 
-        launch_file = os.path.join(self.log_dir, 'launch.sh')
         dbi_file=get_plearndir()+'/python_modules/plearn/parallel/dbi.py'
         overwrite_launch_file=False
         if not os.path.exists(dbi_file):
@@ -824,7 +829,6 @@
                 if mtimed>mtimel:
                     print '[DBI] WARNING: We overwrite the file "'+launch_file+'" with a new version. Update it to your need!'
                     overwrite_launch_file=True
-        source_file=os.getenv("CONDOR_LOCAL_SOURCE")
         
         if self.copy_local_source_file:
             source_file_dest = os.path.join(self.log_dir,
@@ -837,13 +841,15 @@
         if not os.path.exists(launch_file) or overwrite_launch_file:
             self.temp_files.append(launch_file)
             launch_dat = open(launch_file,'w')
-            launch_dat.write(dedent('''\
+            if source_file and not source_file.endswith(".cshrc"):
+
+                launch_dat.write(dedent('''\
                 #!/bin/sh
                 PROGRAM=$1
                 shift\n'''))
-            if source_file:
-                launch_dat.write('source ' + source_file_dest + '\n')
-            launch_dat.write(dedent('''\
+                if source_file:
+                    launch_dat.write('source ' + source_file + '\n')
+                launch_dat.write(dedent('''\
                     echo "Executing on " `hostname` 1>&2
                     echo "HOSTNAME: ${HOSTNAME}" 1>&2
                     echo "PATH: $PATH" 1>&2
@@ -856,6 +862,26 @@
                     #/usr/bin/python -V 1>&2
                     echo ${PROGRAM} $@ 1>&2
                     ${PROGRAM} "$@"'''))
+            else:
+                launch_dat.write(dedent('''\
+                    #!/bin/csh
+                    \n'''))
+                if source_file:
+                    launch_dat.write('source ' + source_file + '\n')
+                launch_dat.write(dedent('''\
+                    echo "Executing on " `hostname`
+                    echo "HOSTNAME: ${HOSTNAME}"
+                    echo "PATH: $PATH"
+                    echo "PYTHONPATH: $PYTHONPATH"
+                    echo "LD_LIBRARY_PATH: $LD_LIBRARY_PATH"
+                    #which python
+                    #echo -n python version:
+                    #python -V
+                    #echo -n /usr/bin/python version:
+                    #/usr/bin/python -V
+                    #echo ${PROGRAM} $@
+                    #${PROGRAM} "$@"
+                    $argv'''))
             launch_dat.close()
             os.chmod(launch_file, 0755)
 



From chapados at mail.berlios.de  Tue Apr  8 14:10:09 2008
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Tue, 8 Apr 2008 14:10:09 +0200
Subject: [Plearn-commits] r8767 - trunk/plearn_learners/generic
Message-ID: <200804081210.m38CA92E029801@sheep.berlios.de>

Author: chapados
Date: 2008-04-08 14:10:08 +0200 (Tue, 08 Apr 2008)
New Revision: 8767

Modified:
   trunk/plearn_learners/generic/NNet.cc
Log:
Added introspective options (nosave) to easily access trained weights from Python

Modified: trunk/plearn_learners/generic/NNet.cc
===================================================================
--- trunk/plearn_learners/generic/NNet.cc	2008-04-07 21:48:32 UTC (rev 8766)
+++ trunk/plearn_learners/generic/NNet.cc	2008-04-08 12:10:08 UTC (rev 8767)
@@ -380,8 +380,41 @@
         ol, "paramsvalues", &NNet::paramsvalues, OptionBase::learntoption, 
         "The learned parameter vector\n");
 
+    // Introspective options.  The following are direct views on the individual
+    // parameters of the NNet.  They are marked 'nosave' since they overlap
+    // with paramsvalues, but are useful for inspecting the NNet structure from
+    // a Python program.
+    declareOption(
+        ol, "w1", &NNet::w1,
+        OptionBase::learntoption | OptionBase::nosave,
+        "(Introspection option)  bias and weights of first hidden layer");
+    
+    declareOption(
+        ol, "w2", &NNet::w2,
+        OptionBase::learntoption | OptionBase::nosave,
+        "(Introspection option)  bias and weights of second hidden layer");
+    
+    declareOption(
+        ol, "wout", &NNet::wout,
+        OptionBase::learntoption | OptionBase::nosave,
+        "(Introspection option)  bias and weights of output layer");
+
+    declareOption(
+        ol, "outbias", &NNet::outbias,
+        OptionBase::learntoption | OptionBase::nosave,
+        "(Introspection option)  bias used only if fixed_output_weights");
+    
+    declareOption(
+        ol, "wdirect", &NNet::wdirect,
+        OptionBase::learntoption | OptionBase::nosave,
+        "(Introspection option)  bias and weights for direct in-to-out connection");
+
+    declareOption(
+        ol, "wrec", &NNet::wrec,
+        OptionBase::learntoption | OptionBase::nosave,
+        "(Introspection option)  input reconstruction weights (optional), from hidden layer to predicted input");
+    
     inherited::declareOptions(ol);
-
 }
 
 ///////////



From nouiz at mail.berlios.de  Tue Apr  8 15:18:40 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 8 Apr 2008 15:18:40 +0200
Subject: [Plearn-commits] r8768 - trunk/scripts
Message-ID: <200804081318.m38DIeXa004044@sheep.berlios.de>

Author: nouiz
Date: 2008-04-08 15:18:39 +0200 (Tue, 08 Apr 2008)
New Revision: 8768

Modified:
   trunk/scripts/dbidispatch
Log:
bugfix


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-04-08 12:10:08 UTC (rev 8767)
+++ trunk/scripts/dbidispatch	2008-04-08 13:18:39 UTC (rev 8768)
@@ -210,7 +210,7 @@
     valid_dbi_param +=["nb_proc"]
 
 from socket import gethostname
-if gethostname().endswith(".iro.umontreal.ca"):
+if  launch_cmd == 'Condor' and gethostname().endswith(".iro.umontreal.ca"):
     if not os.path.abspath(os.path.curdir).startswith("/home/fringant2/"):
         raise Exception("You must be in a subfolder of /home/fringant2/")
     f=os.getenv("CONDOR_LOCAL_SOURCE")



From tihocan at mail.berlios.de  Tue Apr  8 17:06:38 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 8 Apr 2008 17:06:38 +0200
Subject: [Plearn-commits] r8769 - trunk/plearn/base
Message-ID: <200804081506.m38F6csa014691@sheep.berlios.de>

Author: tihocan
Date: 2008-04-08 17:06:38 +0200 (Tue, 08 Apr 2008)
New Revision: 8769

Modified:
   trunk/plearn/base/diff.h
Log:
Added diff for Vars. This should fix the neural network tests

Modified: trunk/plearn/base/diff.h
===================================================================
--- trunk/plearn/base/diff.h	2008-04-08 13:18:39 UTC (rev 8768)
+++ trunk/plearn/base/diff.h	2008-04-08 15:06:38 UTC (rev 8769)
@@ -62,6 +62,7 @@
 class Object;
 template<class ObjectType, class OptionType> class Option;
 //template <class T> class TVec; TODO Use this if possible.
+class Var;
 class VMat;
 class VMatrix;
 class PLearnDiff;
@@ -346,7 +347,6 @@
     return n_diffs;
 }
 
-
 /*
 //! diff for Object.
 template<class ObjectType, class OptionType>
@@ -394,7 +394,15 @@
 }
 #endif
 
+//! diff for Var.
+template<class ObjectType>
+int diff(const string& refer, const string& other, const Option<ObjectType, Var >* opt, PLearnDiff* diffs)
+{
+    return diff(refer, other,
+                (Option<ObjectType, PP<Variable> >*) opt, diffs);
+}
 
+
 //! Add 'prefix' in front of the last 'n' difference names in 'diffs'.
  void addDiffPrefix(const string& prefix, PLearnDiff* diffs, int n);
 



From tihocan at mail.berlios.de  Tue Apr  8 19:20:15 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 8 Apr 2008 19:20:15 +0200
Subject: [Plearn-commits] r8770 - trunk/plearn/var
Message-ID: <200804081720.m38HKFqW022703@sheep.berlios.de>

Author: tihocan
Date: 2008-04-08 19:20:13 +0200 (Tue, 08 Apr 2008)
New Revision: 8770

Modified:
   trunk/plearn/var/AffineTransformVariable.cc
   trunk/plearn/var/AffineTransformVariable.h
Log:
- Added an option to force the resulting vector to be a row vector (this is meant in particular to remove the ambiguity between row and vector when the input is scalar)
- build_ is now private instead of being protected
- Fixed inconsistencies in recomputeSize()


Modified: trunk/plearn/var/AffineTransformVariable.cc
===================================================================
--- trunk/plearn/var/AffineTransformVariable.cc	2008-04-08 15:06:38 UTC (rev 8769)
+++ trunk/plearn/var/AffineTransformVariable.cc	2008-04-08 17:20:13 UTC (rev 8770)
@@ -51,7 +51,13 @@
         "Affine transformation of a vector variable.",
         "The first input is the vector variable.\n"
         "The second input is the matrix of biases (on the first row) and\n"
-        "weights (in other rows).");
+        "weights (in other rows).\n"
+        "If the first input vector is a row vector, then the result is a\n"
+        "row vector as well. If it is a column vector, then the result is\n"
+        "also a column vector. If it is a scalar, then the result is a\n"
+        "column vector as well, unless the option 'force_row_vec' is set\n"
+        "to true."
+);
 
 /////////////////////////////
 // AffineTransformVariable //
@@ -60,13 +66,12 @@
                                                  Variable* transformation,
                                                  bool call_build_):
     inherited(vec, transformation, 
-            (vec->size() == 1) ? transformation->width()
-                               : (vec->isRowVec() ? 1
-                                                  : transformation->width()),
-            (vec->size() == 1) ? 1
-                               : (vec->isRowVec() ? transformation->width()
-                                                  : 1),
-            call_build_)
+            vec->isScalar() || vec->isColumnVec() ? transformation->width()
+                                                  : 1,
+            vec->isScalar() || vec->isColumnVec() ? 1
+                                                  : transformation->width(),
+            call_build_),
+    force_row_vec(false)
 {
     if (call_build_)
         build_();
@@ -91,19 +96,41 @@
         PLERROR("In AffineTransformVariable: expecting a vector Var (row or column) as first argument");
 }
 
+////////////////////
+// declareOptions //
+////////////////////
+void AffineTransformVariable::declareOptions(OptionList& ol)
+{
+     declareOption(ol, "force_row_vec", &AffineTransformVariable::force_row_vec,
+                   OptionBase::buildoption,
+        "If set to true, then the resulting vector will always be a row\n"
+        "vector, even when the input is a column or a scalar.");
+
+    inherited::declareOptions(ol);
+}
+
 ///////////////////
 // recomputeSize //
 ///////////////////
 void AffineTransformVariable::recomputeSize(int& l, int& w) const
 { 
     if (input1 && input2) {
-        l = input1->isRowVec() ? 1 : input2->width();
-        w = input1->isColumnVec() ? 1 : input2->width(); 
+        if (force_row_vec || (!input1->isScalar() && input1->isRowVec())) {
+            // Result is a row vector.
+            l = 1;
+            w = input2->width();
+        } else {
+            // Result is a column vector.
+            l = input2->width();
+            w = 1;
+        }
     } else
         l = w = 0;
 }
 
-
+///////////
+// fprop //
+///////////
 void AffineTransformVariable::fprop()
 {
     value << input2->matValue.firstRow();
@@ -111,7 +138,9 @@
     transposeProductAcc(value, lintransform, input1->value);
 }
 
-
+///////////
+// bprop //
+///////////
 void AffineTransformVariable::bprop()
 {
     Mat&  afftr = input2->matValue;

Modified: trunk/plearn/var/AffineTransformVariable.h
===================================================================
--- trunk/plearn/var/AffineTransformVariable.h	2008-04-08 15:06:38 UTC (rev 8769)
+++ trunk/plearn/var/AffineTransformVariable.h	2008-04-08 17:20:13 UTC (rev 8770)
@@ -51,21 +51,22 @@
 using namespace std;
 
 
-//! Affine transformation of a vector variable.
-//! Should work for both column and row vectors: result vector will be of same kind (row or col)
-//! First row of transformation matrix contains bias b, following rows contain linear-transformation T
-//! Will compute b + x.T (if you consider b and x to be row vectors)
-//! which is equivalent to b + 
 class AffineTransformVariable: public BinaryVariable
 {
     typedef BinaryVariable inherited;
 
 public:
+
+    // Public options.
+
+    bool force_row_vec;
+
     //!  Default constructor for persistence
     AffineTransformVariable() {}
     AffineTransformVariable(Variable* vec, Variable* transformation,
                             bool call_build_ = true);
 
+
     PLEARN_DECLARE_OBJECT(AffineTransformVariable);
 
     virtual void build();
@@ -75,17 +76,33 @@
     virtual void bprop();
     virtual void symbolicBprop();
 
+private:
+
+    void build_();
+
 protected:
-    void build_();
+
+    static void declareOptions(OptionList& ol);
+
 };
 
 DECLARE_OBJECT_PTR(AffineTransformVariable);
 
-//! first row of transformation is the bias.
-inline Var affine_transform(Var vec, Var transformation)
+//! First row of transformation is the bias.
+//! The boolean 'force_row_vec' may be used when the first input is a
+//! a vector, to force the resulting variable to be a row vector, even
+//! when the input is a column vector or a scalar.
+//! If the first input is a matrix, this parameter is ignored.
+inline Var affine_transform(Var vec, Var transformation,
+                            bool force_row_vec = false)
 { 
-    if (vec->isVec())
-        return new AffineTransformVariable(vec, transformation); 
+    if (vec->isVec()) {
+        PP<AffineTransformVariable> res =
+            new AffineTransformVariable(vec, transformation, false);
+        res->force_row_vec = force_row_vec;
+        res->build();
+        return get_pointer(res);
+    }
     else return new MatrixAffineTransformVariable(vec, transformation);
 }
 



From tihocan at mail.berlios.de  Tue Apr  8 19:21:10 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 8 Apr 2008 19:21:10 +0200
Subject: [Plearn-commits] r8771 - trunk/plearn_learners/generic
Message-ID: <200804081721.m38HLAcH024230@sheep.berlios.de>

Author: tihocan
Date: 2008-04-08 19:21:09 +0200 (Tue, 08 Apr 2008)
New Revision: 8771

Modified:
   trunk/plearn_learners/generic/NNet.cc
Log:
Forcing some computations to result in row vectors, to prevent size mismatchs when using scalar variables

Modified: trunk/plearn_learners/generic/NNet.cc
===================================================================
--- trunk/plearn_learners/generic/NNet.cc	2008-04-08 17:20:13 UTC (rev 8770)
+++ trunk/plearn_learners/generic/NNet.cc	2008-04-08 17:21:09 UTC (rev 8771)
@@ -800,7 +800,7 @@
     // Output layer before transfer function.
     if (!first_hidden_layer_is_output) {
         wout = Var(1 + output->width(), outputsize(), "wout");
-        output = affine_transform(output, wout);
+        output = affine_transform(output, wout, true);
         output->setName("output_activations");
         if (!fixed_output_weights)
             params.append(wout);
@@ -897,7 +897,7 @@
     if (input_reconstruction_penalty>0)
     {
         wrec = Var(1 + hidden_layer->width(),input->width(),"wrec");
-        predicted_input = affine_transform(hidden_layer, wrec);
+        predicted_input = affine_transform(hidden_layer, wrec, true);
         params.append(wrec);
         penalties.append(input_reconstruction_penalty*sumsquare(predicted_input - input));
     }
@@ -1040,7 +1040,7 @@
 // hiddenLayer //
 /////////////////
 Var NNet::hiddenLayer(const Var& input, const Var& weights, string transfer_func) {
-    Var hidden = affine_transform(input, weights); 
+    Var hidden = affine_transform(input, weights, true);
     hidden->setName("hidden_layer_activations");
     Var result;
     if (transfer_func == "default")



From lamblin at mail.berlios.de  Tue Apr  8 19:59:44 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 8 Apr 2008 19:59:44 +0200
Subject: [Plearn-commits] r8772 - trunk/python_modules/plearn/parallel
Message-ID: <200804081759.m38HxiIe020779@sheep.berlios.de>

Author: lamblin
Date: 2008-04-08 19:59:43 +0200 (Tue, 08 Apr 2008)
New Revision: 8772

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
More legible error messages


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-04-08 17:21:09 UTC (rev 8771)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-04-08 17:59:43 UTC (rev 8772)
@@ -102,7 +102,7 @@
         if maxThreads==-1:
             nb_thread=len(argsVector)
         elif maxThreads<=0:
-            print "[DBI] you set %d concurent jobs. Must be higher then 0!!"%(maxThreads)
+            print "[DBI] you set %d concurrent jobs. Must be higher then 0!!"%(maxThreads)
             sys.exit(1)
         else:
             nb_thread=maxThreads
@@ -202,8 +202,8 @@
     def add_commands(self,commands): raise NotImplementedError, "DBIBase.add_commands()"
 
     def get_redirection(self,stdout_file,stderr_file):
-        """Calcule the needed redirection based of the objects attribute
-        return a tuple (stdout,stderr) that can be used with popen
+        """Compute the needed redirection based of the objects attribute.
+        Return a tuple (stdout,stderr) that can be used with popen.
         """
         output = PIPE
         error = PIPE
@@ -461,7 +461,7 @@
 
         self.started+=1
         started=self.started# not thread safe!!!
-        print "[DBI,%d/%d,%s] %s"%(started,len(self.tasks),time.ctime(),command)
+        print "[DBI, %d/%d, %s] %s"%(started,len(self.tasks),time.ctime(),command)
         if self.test:
             task.status=STATUS_FINISHED
             return
@@ -482,7 +482,12 @@
                 task.dbi_return_status=int(last.split()[-1])
 #        print "[DBI,%d/%d,%s] Job ended, popen returncode:%d, popen.wait.return:%d, dbi echo return code:%s"%(started,len(self.tasks),time.ctime(),task.p.returncode,task.p_wait_ret,task.dbi_return_status)
         if task.dbi_return_status==None:
-            print "[DBI,%d/%d,%s] Trouble with launching/executing '%s'. Its execution did not finished. Probable cause is the back-end itself. Meaby you want to rerun it. popen returncode:%d, popen.wait.return:%d, dbi echo return code:%s"%(started,len(self.tasks),time.ctime(),command,task.p.returncode,task.p_wait_ret,task.dbi_return_status)
+            print "[DBI, %d/%d, %s] Trouble with launching/executing '%s'." % (started,len(self.tasks),time.ctime(),command)
+            print "    Its execution did not finished. Probable cause is the back-end itself."
+            print "    You may want to run the task again."
+            print "    popen returncode: %d"     % task.p.returncode
+            print "    popen.wait.return: %d"    % task.p_wait_ret
+            print "    dbi echo return code: %s" % task.dbi_return_status
             self.backend_failed+=1
         elif task.dbi_return_status!=0:
             self.jobs_failed+=1
@@ -523,8 +528,8 @@
         else:
             print "[DBI] WARNING jobs not started!"
         self.print_jobs_status()
-        print "[DBI] Their was %d jobs where the back-end failled"%(self.backend_failed)
-        print "[DBI] Their was %d jobs that returned a failure status."%(self.jobs_failed)
+        print "[DBI] %d jobs where the back-end failed." % (self.backend_failed)
+        print "[DBI] %d jobs returned a failure status." % (self.jobs_failed)
 
 class DBIBqtools(DBIBase):
 
@@ -727,7 +732,7 @@
                 newcommand+='; else '
                 newcommand+=c+".32"+c2+'; fi'
                 if not os.access(c+".64", os.X_OK):
-                    raise Exception("The command '"+c+".64' do not have execution permission!")
+                    raise Exception("The command '"+c+".64' does not have execution permission!")
 #                newcommand=command
                 c+=".32"
             elif self.cplat=="INTEL" and os.path.exists(c+".32"):
@@ -745,9 +750,9 @@
             if shellcommand:
                 pass
             elif not os.path.exists(c):
-                raise Exception("The command '"+c+"' do not exist! You must provide the full path to the executable")
+                raise Exception("The command '"+c+"' does not exist! You must provide the full path to the executable")
             elif not os.access(c, os.X_OK):
-                raise Exception("The command '"+c+"' do not have execution permission!")
+                raise Exception("The command '"+c+"' does not have execution permission!")
 
             self.tasks.append(Task(newcommand, self.tmp_dir, self.log_dir,
                                    self.time_format, self.pre_tasks,
@@ -821,13 +826,13 @@
         dbi_file=get_plearndir()+'/python_modules/plearn/parallel/dbi.py'
         overwrite_launch_file=False
         if not os.path.exists(dbi_file):
-            print '[DBI] WARNING: Can\' locate dbi.py file. Meaby the file "'+launch_file+'" is not up to date!'
+            print '[DBI] WARNING: Can\'t locate file "dbi.py". Maybe the file "'+launch_file+'" is not up to date!'
         else:
             if os.path.exists(launch_file):
                 mtimed=os.stat(dbi_file)[8]
                 mtimel=os.stat(launch_file)[8]
                 if mtimed>mtimel:
-                    print '[DBI] WARNING: We overwrite the file "'+launch_file+'" with a new version. Update it to your need!'
+                    print '[DBI] WARNING: We overwrite the file "'+launch_file+'" with a new version. Update it to your needs!'
                     overwrite_launch_file=True
         
         if self.copy_local_source_file:
@@ -985,7 +990,7 @@
             # same architecture as the architecture of the launch computer
 
             if not os.access(c, os.X_OK):
-                raise Exception("The command '"+c+"' do not exist or have execution permission!")
+                raise Exception("The command '"+c+"' does not exist or does not have execution permission!")
             self.tasks.append(Task(command, tmp_dir, log_dir,
                                    time_format, pre_tasks,
                                    post_tasks,dolog,id,False,self.args))
@@ -1278,7 +1283,7 @@
 def DBI(commands, launch_system, **args):
     """The Distributed Batch Interface is a collection of python classes
     that make it easy to execute commands in parallel using different
-    systems like condor, bqtools on Mammoth, the cluster command or localy.
+    systems like condor, bqtools on Mammouth, the cluster command or localy.
     """
     try:
         jobs = eval('DBI'+launch_system+'(commands,**args)')
@@ -1290,8 +1295,8 @@
 
 def main():
     if len(sys.argv)!=2:
-        print "Usage:%s {Condor|Cluster|Ssh|Local|Bqtools} < joblist"%(sys.argv[0])
-        print "Where joblist is a file containing one exeperiement by line"
+        print "Usage: %s {Condor|Cluster|Ssh|Local|Bqtools} < joblist"%(sys.argv[0])
+        print "Where joblist is a file containing one experiment on each line"
         sys.exit(0)
     DBI([ s[0:-1] for s in sys.stdin.readlines() ], sys.argv[1]).run()
 #    jobs.clean()



From saintmlx at mail.berlios.de  Tue Apr  8 21:36:09 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 8 Apr 2008 21:36:09 +0200
Subject: [Plearn-commits] r8773 - in trunk: plearn/python plearn/var
	plearn_learners/distributions plearn_learners/sequential
Message-ID: <200804081936.m38Ja9ah002091@sheep.berlios.de>

Author: saintmlx
Date: 2008-04-08 21:36:07 +0200 (Tue, 08 Apr 2008)
New Revision: 8773

Modified:
   trunk/plearn/python/PythonObjectWrapper.cc
   trunk/plearn/python/PythonObjectWrapper.h
   trunk/plearn/var/ProjectionErrorVariable.cc
   trunk/plearn_learners/distributions/HistogramDistribution.cc
   trunk/plearn_learners/distributions/HistogramDistribution.h
   trunk/plearn_learners/sequential/SequentialValidation.cc
Log:
- enable real=float (32bits) instead of double



Modified: trunk/plearn/python/PythonObjectWrapper.cc
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.cc	2008-04-08 17:59:43 UTC (rev 8772)
+++ trunk/plearn/python/PythonObjectWrapper.cc	2008-04-08 19:36:07 UTC (rev 8773)
@@ -67,6 +67,7 @@
         fprintf(stderr,"For python object: ");
         PyObject_Print(pyobj, stderr, Py_PRINT_RAW);
     }
+    if (PyErr_Occurred()) PyErr_Print();
     PLERROR("Cannot convert Python object using %s", function_name);
 }
 
@@ -78,6 +79,7 @@
         // must be in each translation unit that makes use of libnumarray;
         // weird stuff related to table of function pointers that's being
         // initialized into a STATIC VARIABLE of the translation unit!
+        import_array();//needed for PyArray_DescFromType (will segfault otherwise)
         import_libnumarray();
         numarray_initialized = true;
     }
@@ -109,6 +111,12 @@
         return PyLong_AsDouble(pyobj);
     if(PyInt_Check(pyobj))
         return (double)PyInt_AS_LONG(pyobj);
+    if(PyArray_CheckScalar(pyobj))
+    {
+        double ret= 0.;
+        PyArray_CastScalarToCtype(pyobj, &ret, PyArray_DescrFromType(NPY_DOUBLE));
+        return ret;
+    }
     PLPythonConversionError("ConvertFromPyObject<double>", pyobj,
                             print_traceback);
     return 0;//shut up compiler
@@ -124,6 +132,12 @@
         return (float)PyLong_AsDouble(pyobj);
     if(PyInt_Check(pyobj))
         return (float)PyInt_AS_LONG(pyobj);
+    if(PyArray_CheckScalar(pyobj))
+    {
+        float ret= 0.;
+        PyArray_CastScalarToCtype(pyobj, &ret, PyArray_DescrFromType(NPY_FLOAT));
+        return ret;
+    }
     PLPythonConversionError("ConvertFromPyObject<float>", pyobj,
                             print_traceback);
     return 0;//shut up compiler
@@ -187,19 +201,17 @@
 void ConvertFromPyObject<Vec>::convert(PyObject* pyobj, Vec& v,
                                        bool print_traceback)
 {
-    // NA_InputArray possibly creates a well-behaved temporary (i.e. not
-    // discontinuous is memory)
     PLASSERT( pyobj );
-    PyArrayObject* pyarr = NA_InputArray(pyobj, tReal, NUM_C_ARRAY);
+    PyObject* pyarr0= PyArray_CheckFromAny(pyobj, NULL,
+                                           1, 1, NPY_CARRAY_RO, Py_None);
+    PyObject* pyarr= 
+        PyArray_CastToType(reinterpret_cast<PyArrayObject*>(pyarr0),
+                           PyArray_DescrFromType(PL_NPY_REAL), 0);
     if (! pyarr)
         PLPythonConversionError("ConvertFromPyObject<Vec>", pyobj,
                                 print_traceback);
-    if (pyarr->nd != 1)
-        PLERROR("ConvertFromPyObject<Vec>: Dimensionality of the returned array "
-                "should be 1; got %d", pyarr->nd);
-
-    v.resize(pyarr->dimensions[0]);
-    v.copyFrom((real*)(NA_OFFSETDATA(pyarr)), pyarr->dimensions[0]);
+    v.resize(PyArray_DIM(pyarr,0));
+    v.copyFrom((real*)(PyArray_DATA(pyarr)), PyArray_DIM(pyarr,0));
     Py_XDECREF(pyarr);
 }
 
@@ -213,20 +225,18 @@
 void ConvertFromPyObject<Mat>::convert(PyObject* pyobj, Mat& m,
                                        bool print_traceback)
 {
-    // NA_InputArray possibly creates a well-behaved temporary (i.e. not
-    // discontinuous is memory)
     PLASSERT( pyobj );
-    PyArrayObject* pyarr = NA_InputArray(pyobj, tReal, NUM_C_ARRAY);
+    PyObject* pyarr0= PyArray_CheckFromAny(pyobj, NULL,
+                                           2, 2, NPY_CARRAY_RO, Py_None);
+    PyObject* pyarr= 
+        PyArray_CastToType(reinterpret_cast<PyArrayObject*>(pyarr0),
+                           PyArray_DescrFromType(PL_NPY_REAL), 0);
     if (! pyarr)
         PLPythonConversionError("ConvertFromPyObject<Mat>", pyobj,
                                 print_traceback);
-    if (pyarr->nd != 2)
-        PLERROR("ConvertFromPyObject<Mat>: Dimensionality of the returned array "
-                "should be 2; got %d", pyarr->nd);
-
-    m.resize(pyarr->dimensions[0], pyarr->dimensions[1]);
-    m.toVec().copyFrom((real*)(NA_OFFSETDATA(pyarr)),
-                       pyarr->dimensions[0] * pyarr->dimensions[1]);
+    m.resize(PyArray_DIM(pyarr,0), PyArray_DIM(pyarr,1));
+    m.toVec().copyFrom((real*)(PyArray_DATA(pyarr)),
+                       PyArray_DIM(pyarr,0) * PyArray_DIM(pyarr,1));
     Py_XDECREF(pyarr);
 }
 

Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2008-04-08 17:59:43 UTC (rev 8772)
+++ trunk/plearn/python/PythonObjectWrapper.h	2008-04-08 19:36:07 UTC (rev 8773)
@@ -70,8 +70,10 @@
 
 
 #ifdef USEFLOAT
+#define PL_NPY_REAL NPY_FLOAT
 #define tReal tFloat32
 #else
+#define PL_NPY_REAL NPY_DOUBLE
 #define tReal tFloat64
 #endif
 

Modified: trunk/plearn/var/ProjectionErrorVariable.cc
===================================================================
--- trunk/plearn/var/ProjectionErrorVariable.cc	2008-04-08 17:59:43 UTC (rev 8772)
+++ trunk/plearn/var/ProjectionErrorVariable.cc	2008-04-08 19:36:07 UTC (rev 8773)
@@ -227,7 +227,7 @@
     {
         // use SVD of (F' -T')
         FT1 << F;
-        multiply(FT2,TT,-1.0);
+        multiply(FT2,TT,static_cast<real>(-1.0));
         lapackSVD(FT, Ut, S, V);
         wwuu.clear();//
         for (int k=0;k<S.length();k++)
@@ -254,7 +254,7 @@
             Vec res(ww.length());
             product(res,A11,ww);
             productAcc(res,A12,uu);
-            res -= 1.0;
+            res -= static_cast<real>(1.0);
             cout << "norm of error in w equations: " << norm(res) << endl;
             Vec res2(uu.length());
             transposeProduct(res2,A12,ww);

Modified: trunk/plearn_learners/distributions/HistogramDistribution.cc
===================================================================
--- trunk/plearn_learners/distributions/HistogramDistribution.cc	2008-04-08 17:59:43 UTC (rev 8772)
+++ trunk/plearn_learners/distributions/HistogramDistribution.cc	2008-04-08 19:36:07 UTC (rev 8773)
@@ -196,13 +196,13 @@
     deepCopyField(smoother, copies);
 }
 
-double HistogramDistribution::log_density(const Vec& x) const
+real HistogramDistribution::log_density(const Vec& x) const
 {
     return pl_log(density(x));
 }
 
 
-double HistogramDistribution::density(const Vec& x) const
+real HistogramDistribution::density(const Vec& x) const
 {
     if(x.size() != 1)
         PLERROR("HistogramDistribution::density implemented only for univariate data (vec size == 1).");
@@ -210,7 +210,7 @@
 }
 
 
-double HistogramDistribution::survival_fn(const Vec& x) const
+real HistogramDistribution::survival_fn(const Vec& x) const
 {
     if(x.size() != 1)
         PLERROR("HistogramDistribution::survival_fn implemented only for univariate data (vec size == 1).");
@@ -228,7 +228,7 @@
     return survival_values[bin];
 }
 
-double HistogramDistribution::cdf(const Vec& x) const
+real HistogramDistribution::cdf(const Vec& x) const
 {
     return 1.0-survival_fn(x);
 }
@@ -259,7 +259,7 @@
     cov(0,0) = abs(sumsq-(sum*sum)/n)/n;
 }
 
-double HistogramDistribution::prob_in_range(const Vec& x0, const Vec& x1) const
+real HistogramDistribution::prob_in_range(const Vec& x0, const Vec& x1) const
 {
     return survival_fn(x0) - survival_fn(x1);
 }

Modified: trunk/plearn_learners/distributions/HistogramDistribution.h
===================================================================
--- trunk/plearn_learners/distributions/HistogramDistribution.h	2008-04-08 17:59:43 UTC (rev 8772)
+++ trunk/plearn_learners/distributions/HistogramDistribution.h	2008-04-08 19:36:07 UTC (rev 8773)
@@ -134,17 +134,17 @@
     // ************************
 
     //! return log of probability density log(p(x))
-    virtual double log_density(const Vec& x) const;
+    virtual real log_density(const Vec& x) const;
 
     //! return probability density p(x)
     //! [ default version returns exp(log_density(x)) ]
-    virtual double density(const Vec& x) const;
+    virtual real density(const Vec& x) const;
 
     //! return survival fn = P(X>x)
-    virtual double survival_fn(const Vec& x) const;
+    virtual real survival_fn(const Vec& x) const;
 
     //! return survival fn = P(X<x)
-    virtual double cdf(const Vec& x) const;
+    virtual real cdf(const Vec& x) const;
 
     //! return E[X]
     virtual void expectation(Vec& mu) const;
@@ -153,7 +153,7 @@
     virtual void variance(Mat& cov) const;
 
     //! return P(x0 < X < x1)
-    virtual double prob_in_range(const Vec& x0, const Vec& x1) const;
+    virtual real prob_in_range(const Vec& x0, const Vec& x1) const;
 
     //protected:
     //the following methods are used internally by HistogramDistribution

Modified: trunk/plearn_learners/sequential/SequentialValidation.cc
===================================================================
--- trunk/plearn_learners/sequential/SequentialValidation.cc	2008-04-08 17:59:43 UTC (rev 8772)
+++ trunk/plearn_learners/sequential/SequentialValidation.cc	2008-04-08 19:36:07 UTC (rev 8773)
@@ -621,7 +621,7 @@
 
 void SequentialValidation::testLearners(VMat test_set)
 {
-    double weight;
+    real weight;
     test_set.getExample(test_set.length()-1, input, target, weight);
     for (int a=0, n=accessory_learners.length() ; a<n ; ++a )
     {



From tihocan at mail.berlios.de  Tue Apr  8 22:16:07 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 8 Apr 2008 22:16:07 +0200
Subject: [Plearn-commits] r8774 - trunk/plearn/math
Message-ID: <200804082016.m38KG74Z006782@sheep.berlios.de>

Author: tihocan
Date: 2008-04-08 22:16:06 +0200 (Tue, 08 Apr 2008)
New Revision: 8774

Modified:
   trunk/plearn/math/pl_erf.cc
Log:
Fixed infinite loop when compiled in optimized mode on the mammouth cluster

Modified: trunk/plearn/math/pl_erf.cc
===================================================================
--- trunk/plearn/math/pl_erf.cc	2008-04-08 19:36:07 UTC (rev 8773)
+++ trunk/plearn/math/pl_erf.cc	2008-04-08 20:16:06 UTC (rev 8774)
@@ -179,6 +179,12 @@
     PLASSERT(!is_missing(q));
 #endif
 
+    // Handle special cases that can lead to infinite loops below.
+    if (fast_exact_is_equal(q, real(0)))
+        return -INFINITY;
+    else if (fast_exact_is_equal(q, real(1)))
+        return INFINITY;
+
     // first find a reasonable interval (a,b) s.t. cum(a)<q<cum(b)
     real a=-2;
     real b=2;
@@ -245,8 +251,9 @@
 {
     //! Fill the table
     real scaling = 1./(GAUSSQUANTILETABLESIZE-1);
-    for(int i=0; i<GAUSSQUANTILETABLESIZE; i++)
+    for(int i=0; i<GAUSSQUANTILETABLESIZE; i++) {
         gaussQuantiletable[i] = (float) gauss_01_quantile(i*scaling);
+    }
 }
 
 PLGaussQuantileInitializer::~PLGaussQuantileInitializer() {}



From louradou at mail.berlios.de  Tue Apr  8 22:39:32 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Tue, 8 Apr 2008 22:39:32 +0200
Subject: [Plearn-commits] r8775 - trunk/python_modules/plearn/learners
Message-ID: <200804082039.m38KdW8U008940@sheep.berlios.de>

Author: louradou
Date: 2008-04-08 22:39:32 +0200 (Tue, 08 Apr 2008)
New Revision: 8775

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:


Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2008-04-08 20:16:06 UTC (rev 8774)
+++ trunk/python_modules/plearn/learners/SVM.py	2008-04-08 20:39:32 UTC (rev 8775)
@@ -456,7 +456,7 @@
                    coef0= None):
         if not coef0:
             coef0 = self.coef0_initvalue
-        return [  coef0, coef0/100., coef0*100. ]
+        return [  coef0, coef0/10., coef0*10. ]
 
     """ Return <list> of <dict>: which hyperparameter values to try FIRST.
         - 'samples': <array>(n_samples,dim) of (train/valid) samples.
@@ -522,7 +522,7 @@
             elif( tried_coef0[0] == min(tried_coef0)
                or tried_coef0[0] == max(tried_coef0) ):
                 new_param_list += self.choose_first_C_param( \
-                                       [ {'degree':d, 'coef0':c} for c in self.choose_new_param_geom( tried_coef0 ) ], \
+                                       [ {'degree':d, 'coef0':c} for c in self.choose_new_param_geom( tried_coef0 )[:2] ], \
                                        tried_C[0] )
                                        
             # we try other 'C'
@@ -751,6 +751,7 @@
         self.testset_key  = 'testset'
 
         self.compute_outputs_from_probabilities = None
+        self.use_proba = False
 
     def forget(self):
         for expert in self.all_experts:
@@ -921,7 +922,7 @@
         if len(s)>0:s=', '+s
 
         
-        if self.compute_outputs_from_probabilities:
+        if self.use_proba or self.compute_outputs_from_probabilities:
             # if this function is defined (see 
             return eval('svm_parameter( svm_type = C_SVC, probability = 1 '+s+')' )
         else:
@@ -1590,7 +1591,7 @@
     """
 
     svm.results_filename = os.path.join( outputPath,
-                                         'results_%s_svm' % ( os.path.basename(train_file) )
+                                         'RES_%s_svm' % ( os.path.basename(train_file) )
                            )
 
     """ ................................................................. """
@@ -1605,11 +1606,21 @@
 
     """ =============================================================== """
     """ 3. RUN EXPERIMENTS. """
+
+
+    normalize_inputs = 0
+    use_proba = 0
     
-    svm.kernel_type = 'rbf'
+    svm.normalize_inputs = normalize_inputs
+    svm.use_proba = use_proba
+    
+    svm.preproc_optionnames = [ 'renormalize_inputs', 'use_proba' ]
+    svm.preproc_optionvalues = [ normalize_inputs ,  use_proba ]
+    
+    svm.kernel_type = 'poly'
     svm.train_and_tune(dataspec)
 
-    svm.kernel_type = 'poly'
+    svm.kernel_type = 'rbf'
     svm.train_and_tune(dataspec)
 
     svm.kernel_type = 'linear'



From louradou at mail.berlios.de  Tue Apr  8 23:25:11 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Tue, 8 Apr 2008 23:25:11 +0200
Subject: [Plearn-commits] r8776 - in trunk: plearn/vmat
	plearn_learners/classifiers
Message-ID: <200804082125.m38LPB98013868@sheep.berlios.de>

Author: louradou
Date: 2008-04-08 23:25:11 +0200 (Tue, 08 Apr 2008)
New Revision: 8776

Modified:
   trunk/plearn/vmat/ReplicateSamplesVMatrix.cc
   trunk/plearn_learners/classifiers/ToBagClassifier.cc
Log:
makeDeepCopyFromShallowCopy implemented



Modified: trunk/plearn/vmat/ReplicateSamplesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ReplicateSamplesVMatrix.cc	2008-04-08 20:39:32 UTC (rev 8775)
+++ trunk/plearn/vmat/ReplicateSamplesVMatrix.cc	2008-04-08 21:25:11 UTC (rev 8776)
@@ -186,15 +186,7 @@
 void ReplicateSamplesVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
-
-    // ### Call deepCopyField on all "pointer-like" fields
-    // ### that you wish to be deepCopied rather than
-    // ### shallow-copied.
-    // ### ex:
-    // deepCopyField(trainvec, copies);
-
-    // ### Remove this line when you have fully implemented this method.
-    PLERROR("ReplicateSamplesVMatrix::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
+    deepCopyField(random_gen, copies);
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn_learners/classifiers/ToBagClassifier.cc
===================================================================
--- trunk/plearn_learners/classifiers/ToBagClassifier.cc	2008-04-08 20:39:32 UTC (rev 8775)
+++ trunk/plearn_learners/classifiers/ToBagClassifier.cc	2008-04-08 21:25:11 UTC (rev 8776)
@@ -152,15 +152,9 @@
 void ToBagClassifier::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
-
-    // ### Call deepCopyField on all "pointer-like" fields
-    // ### that you wish to be deepCopied rather than
-    // ### shallow-copied.
-    // ### ex:
-    // deepCopyField(trainvec, copies);
-
-    // ### Remove this line when you have fully implemented this method.
-    PLERROR("ToBagClassifier::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
+    deepCopyField(sub_target, copies);
+    deepCopyField(bag_output, copies);
+    deepCopyField(votes, copies);
 }
 
 ////////////////////



From nouiz at mail.berlios.de  Wed Apr  9 17:09:21 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 9 Apr 2008 17:09:21 +0200
Subject: [Plearn-commits] r8777 - in trunk: commands/PLearnCommands
	plearn/misc
Message-ID: <200804091509.m39F9L5v022055@sheep.berlios.de>

Author: nouiz
Date: 2008-04-09 17:09:20 +0200 (Wed, 09 Apr 2008)
New Revision: 8777

Modified:
   trunk/commands/PLearnCommands/VMatCommand.cc
   trunk/plearn/misc/vmatmain.cc
Log:
-implemented plearn vmat convert file.vmat file2.vmat
-added the documentation for it
-- changed option plearn vmat convert ... [save_vmat] to [--save_vmat] to keep sonsistency with other option of vmat convert


Modified: trunk/commands/PLearnCommands/VMatCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/VMatCommand.cc	2008-04-08 21:25:11 UTC (rev 8776)
+++ trunk/commands/PLearnCommands/VMatCommand.cc	2008-04-09 15:09:20 UTC (rev 8777)
@@ -72,7 +72,7 @@
         "   or: vmat stats <dataset> \n"
         "       Will display basic statistics for each field \n"
         "   or: vmat convert <source> <destination> [--cols=col1,col2,col3,...]\n"
-        "       To convert any dataset into a .amat, .pmat, .dmat or .csv format. \n"
+        "       To convert any dataset into a .amat, .pmat, .dmat, .vmat or .csv format. \n"
         "       The extension of the destination is used to determine the format you want. \n"
         "       If the option --cols is specified, it requests to keep only the given columns\n"
         "       (no space between the commas and the columns); columns can be given either as a\n"
@@ -86,6 +86,8 @@
         "         --delimiter=C:   use character C as the field delimiter (default = ',')\n"
         "         --convert-date:  first column is assumed to be in CYYMMDD format; it is\n"
         "                          exported as YYYYMMDD in the .csv file (19000000 is added)\n"
+        "         --mat_to_mem:    Load the original matrice to memory\n"
+        "         --save_vmat:     Save the source vmat in the destinatino metadatadir\n"
         "   or: vmat gendef <source> [binnum1 binnum2 ...] \n"
         "       Generate stats for dataset (will put them in its associated metadatadir). \n"
         "   or: vmat genvmat <source_dataset> <dest_vmat> [binned{num} | onehot{num} | normalized]\n"

Modified: trunk/plearn/misc/vmatmain.cc
===================================================================
--- trunk/plearn/misc/vmatmain.cc	2008-04-08 21:25:11 UTC (rev 8776)
+++ trunk/plearn/misc/vmatmain.cc	2008-04-09 15:09:20 UTC (rev 8777)
@@ -492,7 +492,7 @@
         bool mat_to_mem=false;
         if(argc<4)
             PLERROR("Usage: vmat convert <source> <destination> "
-                    "[--mat_to_mem] [--cols=col1,col2,col3,...] [save_vmat]");
+                    "[--mat_to_mem] [--cols=col1,col2,col3,...] [--save_vmat] [--skip-missings] [--precision=N] [--delimiter=CHAR]");
 
         /**
          * Interpret the following options:
@@ -514,6 +514,9 @@
          *           :: conversion to CSV uses specified character as field delimiter
          *     --mat_to_mem
          *           :: load the source vmat in memory before saving
+         *     --save_vmat 
+         *           :: if the source is a vmat, we serialize the constructed
+         *           ::object in the metadatadir of the destination
          */
         TVec<string> columns;
         bool skip_missings = false;
@@ -541,7 +544,7 @@
                 convert_date = true;
             else if (curopt =="--mat_to_mem")
                 mat_to_mem = true;
-            else if (curopt == "save_vmat")
+            else if (curopt == "--save_vmat")
                 save_vmat = true;
             else
                 PLWARNING("VMat convert: unrecognized option '%s'; ignoring it...",
@@ -578,10 +581,11 @@
                 save_vmat_as_csv(vm, out, skip_missings, precision, delimiter, true /*verbose*/,
                                  convert_date);
             }
-        }
+        }else if(ext == ".vmat")
+            PLearn::save(destination,vm);
         else
         {
-            cerr << "ERROR: can only convert to .amat .pmat .dmat or .csv" << endl
+            cerr << "ERROR: can only convert to .amat .pmat .dmat, .vmat or .csv" << endl
                  << "Please specify a destination name with a valid extension " << endl;
         }
         if(save_vmat && extract_extension(source)==".vmat")



From nouiz at mail.berlios.de  Wed Apr  9 18:56:08 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 9 Apr 2008 18:56:08 +0200
Subject: [Plearn-commits] r8778 - trunk/plearn/math
Message-ID: <200804091656.m39Gu860005344@sheep.berlios.de>

Author: nouiz
Date: 2008-04-09 18:56:07 +0200 (Wed, 09 Apr 2008)
New Revision: 8778

Modified:
   trunk/plearn/math/TVec_decl.h
Log:
Added bound check


Modified: trunk/plearn/math/TVec_decl.h
===================================================================
--- trunk/plearn/math/TVec_decl.h	2008-04-09 15:09:20 UTC (rev 8777)
+++ trunk/plearn/math/TVec_decl.h	2008-04-09 16:56:07 UTC (rev 8778)
@@ -651,10 +651,24 @@
     }
 
     inline T& lastElement() const
-    { return storage->data[offset_+length()-1]; }
+    { 
+#ifdef BOUNDCHECK
+        if(length()==0)
+            PLERROR("TVec::lastElement() - can't access last"
+                    " element of TVec as there is 0 element!");
+#endif
+        return storage->data[offset_+length()-1];
+    }
 
     inline T& firstElement() const
-    { return storage->data[offset_]; }
+    { 
+#ifdef BOUNDCHECK
+        if(length()==0)
+            PLERROR("TVec::firstElement() - can't access first"
+                    " element of TVec as there is 0 element!");
+#endif 
+        return storage->data[offset_]; 
+    }
 
     inline T& front() const { return firstElement(); }
     inline T& back() const { return lastElement(); }



From tihocan at mail.berlios.de  Wed Apr  9 19:07:12 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 9 Apr 2008 19:07:12 +0200
Subject: [Plearn-commits] r8779 - trunk/plearn/var
Message-ID: <200804091707.m39H7Cce018343@sheep.berlios.de>

Author: tihocan
Date: 2008-04-09 19:07:12 +0200 (Wed, 09 Apr 2008)
New Revision: 8779

Modified:
   trunk/plearn/var/BinaryVariable.cc
Log:
Minor code simplification

Modified: trunk/plearn/var/BinaryVariable.cc
===================================================================
--- trunk/plearn/var/BinaryVariable.cc	2008-04-09 16:56:07 UTC (rev 8778)
+++ trunk/plearn/var/BinaryVariable.cc	2008-04-09 17:07:12 UTC (rev 8779)
@@ -171,8 +171,7 @@
         return input2->sources();
     if (!input2)
         return input1->sources();
-    return (input1 ? input1->sources() : VarArray(0, 0)) &
-           (input2 ? input2->sources() : VarArray(0, 0));
+    return input1->sources() & input2->sources();
 }
 
 ////////////////////



From nouiz at mail.berlios.de  Wed Apr  9 19:45:39 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 9 Apr 2008 19:45:39 +0200
Subject: [Plearn-commits] r8780 - trunk/python_modules/plearn/pymake
Message-ID: <200804091745.m39HjdoD027121@sheep.berlios.de>

Author: nouiz
Date: 2008-04-09 19:45:39 +0200 (Wed, 09 Apr 2008)
New Revision: 8780

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
remove host from list more often.


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-04-09 17:07:12 UTC (rev 8779)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-04-09 17:45:39 UTC (rev 8780)
@@ -2071,7 +2071,7 @@
         # single error message for a syntax error, etc.
         if not hasattr(self,"compilation_status"):
             if warningmsgs:
-                if warningmsgs[0].startswith('ssh: '):
+                if warningmsgs[0].startswith('ssh: ') or warningmsgs[0].startswith('Connection closed by'):
                     # The hostname has a problem, so we remove it from the list
                     # and retry on another machine
                     self.remove_hostname = True



From nouiz at mail.berlios.de  Wed Apr  9 19:50:43 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 9 Apr 2008 19:50:43 +0200
Subject: [Plearn-commits] r8781 - trunk/plearn/vmat
Message-ID: <200804091750.m39HohK8027521@sheep.berlios.de>

Author: nouiz
Date: 2008-04-09 19:50:42 +0200 (Wed, 09 Apr 2008)
New Revision: 8781

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
Log:
bugfix for a corner case where the file end with \n


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2008-04-09 17:45:39 UTC (rev 8780)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2008-04-09 17:50:42 UTC (rev 8781)
@@ -523,7 +523,7 @@
 
 TVec<string> TextFilesVMatrix::splitIntoFields(const string& raw_row) const
 {
-    return split_quoted_delimiter(raw_row, delimiter[0],quote_delimiter);
+    return split_quoted_delimiter(removeblanks(raw_row), delimiter[0],quote_delimiter);
 }
 
 TVec<string> TextFilesVMatrix::getTextFields(int i) const



From tihocan at mail.berlios.de  Wed Apr  9 19:53:28 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 9 Apr 2008 19:53:28 +0200
Subject: [Plearn-commits] r8782 - in
	trunk/plearn/vmat/test/.pytest/PL_AutoVMatrix/expected_results:
	. data_train_amat__to__data_train.dmat.metadata
	data_train_amat__to__data_train.pmat.metadata
	linear_4x_2y_amat__to__linear_4x_2y.dmat.metadata
	linear_4x_2y_amat__to__linear_4x_2y.pmat.metadata
	linear_4x_2y_pmat__to__linear_4x_2y.dmat.metadata
	linear_4x_2y_pmat__to__linear_4x_2y.pmat.metadata
Message-ID: <200804091753.m39HrSV8027784@sheep.berlios.de>

Author: tihocan
Date: 2008-04-09 19:53:27 +0200 (Wed, 09 Apr 2008)
New Revision: 8782

Removed:
   trunk/plearn/vmat/test/.pytest/PL_AutoVMatrix/expected_results/data_train_amat__to__data_train.dmat.metadata/FieldInfo/
   trunk/plearn/vmat/test/.pytest/PL_AutoVMatrix/expected_results/data_train_amat__to__data_train.pmat.metadata/FieldInfo/
   trunk/plearn/vmat/test/.pytest/PL_AutoVMatrix/expected_results/linear_4x_2y_amat__to__linear_4x_2y.dmat.metadata/FieldInfo/
   trunk/plearn/vmat/test/.pytest/PL_AutoVMatrix/expected_results/linear_4x_2y_amat__to__linear_4x_2y.pmat.metadata/FieldInfo/
   trunk/plearn/vmat/test/.pytest/PL_AutoVMatrix/expected_results/linear_4x_2y_pmat__to__linear_4x_2y.dmat.metadata/FieldInfo/
   trunk/plearn/vmat/test/.pytest/PL_AutoVMatrix/expected_results/linear_4x_2y_pmat__to__linear_4x_2y.pmat.metadata/FieldInfo/
Modified:
   trunk/plearn/vmat/test/.pytest/PL_AutoVMatrix/expected_results/RUN.log
   trunk/plearn/vmat/test/.pytest/PL_AutoVMatrix/expected_results/linear_4x_2y_pmat__to__linear_4x_2y.amat
   trunk/plearn/vmat/test/.pytest/PL_AutoVMatrix/expected_results/linear_4x_2y_pmat__to__linear_4x_2y.dmat.metadata/sizes
   trunk/plearn/vmat/test/.pytest/PL_AutoVMatrix/expected_results/linear_4x_2y_pmat__to__linear_4x_2y.pmat.metadata/sizes
Log:
Fixed test after recent change to a test data sizes

Modified: trunk/plearn/vmat/test/.pytest/PL_AutoVMatrix/expected_results/RUN.log
===================================================================
--- trunk/plearn/vmat/test/.pytest/PL_AutoVMatrix/expected_results/RUN.log	2008-04-09 17:50:42 UTC (rev 8781)
+++ trunk/plearn/vmat/test/.pytest/PL_AutoVMatrix/expected_results/RUN.log	2008-04-09 17:53:27 UTC (rev 8782)
@@ -232,8 +232,8 @@
 writable = 0 ;
 length = 200 ;
 width = 6 ;
-inputsize = 6 ;
-targetsize = 0 ;
+inputsize = 4 ;
+targetsize = 2 ;
 weightsize = 0 ;
 extrasize = 0 ;
 metadatadir = "PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat.metadata/"  )

Modified: trunk/plearn/vmat/test/.pytest/PL_AutoVMatrix/expected_results/linear_4x_2y_pmat__to__linear_4x_2y.amat
===================================================================
--- trunk/plearn/vmat/test/.pytest/PL_AutoVMatrix/expected_results/linear_4x_2y_pmat__to__linear_4x_2y.amat	2008-04-09 17:50:42 UTC (rev 8781)
+++ trunk/plearn/vmat/test/.pytest/PL_AutoVMatrix/expected_results/linear_4x_2y_pmat__to__linear_4x_2y.amat	2008-04-09 17:53:27 UTC (rev 8782)
@@ -1,6 +1,6 @@
 #size: 200 6
 #: 0 1 2 3 4 5 
-#sizes: 6 0 0 0
+#sizes: 4 2 0 0
 -0.676509999999999945 -0.354930000000000023 -6.93189999999999973 -1.70795000000000008 -35.9233100000000007 -7.30156999999999989 
 1.64172000000000007 0.531970000000000054 13.5284200000000006 3.81586999999999987 71.6844300000000061 89.1621899999999954 
 -0.236990000000000006 0.917590000000000017 7.99099999999999966 0.442350000000000021 96.8868700000000018 47.2869799999999998 

Modified: trunk/plearn/vmat/test/.pytest/PL_AutoVMatrix/expected_results/linear_4x_2y_pmat__to__linear_4x_2y.dmat.metadata/sizes
===================================================================
--- trunk/plearn/vmat/test/.pytest/PL_AutoVMatrix/expected_results/linear_4x_2y_pmat__to__linear_4x_2y.dmat.metadata/sizes	2008-04-09 17:50:42 UTC (rev 8781)
+++ trunk/plearn/vmat/test/.pytest/PL_AutoVMatrix/expected_results/linear_4x_2y_pmat__to__linear_4x_2y.dmat.metadata/sizes	2008-04-09 17:53:27 UTC (rev 8782)
@@ -1 +1 @@
-6 0 0 0 
+4 2 0 0 

Modified: trunk/plearn/vmat/test/.pytest/PL_AutoVMatrix/expected_results/linear_4x_2y_pmat__to__linear_4x_2y.pmat.metadata/sizes
===================================================================
--- trunk/plearn/vmat/test/.pytest/PL_AutoVMatrix/expected_results/linear_4x_2y_pmat__to__linear_4x_2y.pmat.metadata/sizes	2008-04-09 17:50:42 UTC (rev 8781)
+++ trunk/plearn/vmat/test/.pytest/PL_AutoVMatrix/expected_results/linear_4x_2y_pmat__to__linear_4x_2y.pmat.metadata/sizes	2008-04-09 17:53:27 UTC (rev 8782)
@@ -1 +1 @@
-6 0 0 0 
+4 2 0 0 



From nouiz at mail.berlios.de  Wed Apr  9 19:54:30 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 9 Apr 2008 19:54:30 +0200
Subject: [Plearn-commits] r8783 - trunk/plearn/vmat
Message-ID: <200804091754.m39HsUbK027853@sheep.berlios.de>

Author: nouiz
Date: 2008-04-09 19:54:30 +0200 (Wed, 09 Apr 2008)
New Revision: 8783

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
Log:
In TextFilesVMatrix added the type auto-num. This type automatically remove the symbol $ at the beginning or the end of the value. Also, it remove any comma that separate thousant.


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2008-04-09 17:53:27 UTC (rev 8782)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2008-04-09 17:54:30 UTC (rev 8783)
@@ -185,7 +185,7 @@
     return (ftype=="auto" || ftype=="num" || ftype=="date" || ftype=="jdate" ||
             ftype=="postal" || ftype=="dollar" || ftype=="dollar-comma" ||
             ftype=="YYYYMM" || ftype=="sas_date" || ftype == "bell_range" ||
-            ftype == "char" || ftype=="num-comma" );
+            ftype == "char" || ftype=="num-comma" || ftype=="auto-num");
 }
 
 void TextFilesVMatrix::setColumnNamesAndWidth()
@@ -626,6 +626,24 @@
         else
             dest[0] = getMapping(k, strval);
     }
+    else if(fieldtype=="auto-num")
+    {//We suppose the decimal point is '.'
+        string s=strval;
+        if(strval[0]=='$')
+            s.erase(0,1);
+        if(strval[strval.size()-1]=='$')
+            s.erase(s.end());
+        
+        for(unsigned int pos=0; pos<strval.size(); pos++)
+            if(s[pos]==',')
+                s.erase(pos--,1);
+        if(pl_isnumber(s,&val))
+            dest[0] = real(val);
+        else
+            PLERROR("In TextFilesVMatrix::transformStringToValue -"
+                    " expedted [$]number[$] as the value for field %d(%s)."
+                    " Got %s", k, fieldname.c_str(), strval.c_str());
+    }
     else if(fieldtype=="char")
     {
         dest[0] = getMapping(k, strval);



From nouiz at mail.berlios.de  Wed Apr  9 20:37:33 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 9 Apr 2008 20:37:33 +0200
Subject: [Plearn-commits] r8784 - trunk/plearn/db
Message-ID: <200804091837.m39IbXR3030777@sheep.berlios.de>

Author: nouiz
Date: 2008-04-09 20:37:32 +0200 (Wed, 09 Apr 2008)
New Revision: 8784

Modified:
   trunk/plearn/db/getDataSet.h
Log:
removed dependency by forward declaration


Modified: trunk/plearn/db/getDataSet.h
===================================================================
--- trunk/plearn/db/getDataSet.h	2008-04-09 17:54:30 UTC (rev 8783)
+++ trunk/plearn/db/getDataSet.h	2008-04-09 18:37:32 UTC (rev 8784)
@@ -44,15 +44,12 @@
 #ifndef getDataSet_INC
 #define getDataSet_INC
 
-//#include <map>
-//#include <string>
-//#include <plearn/vmat/VMat.h>
-#include <plearn/io/PPath.h>
 
 namespace PLearn {
 using namespace std;
 
 class VMat;
+class PPath;
 
 //! Return help on the dataset syntax.
 string getDataSetHelp();



From tihocan at mail.berlios.de  Wed Apr  9 20:51:23 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 9 Apr 2008 20:51:23 +0200
Subject: [Plearn-commits] r8785 - trunk/plearn/vmat
Message-ID: <200804091851.m39IpNZD000329@sheep.berlios.de>

Author: tihocan
Date: 2008-04-09 20:51:23 +0200 (Wed, 09 Apr 2008)
New Revision: 8785

Modified:
   trunk/plearn/vmat/VMatrix.h
Log:
Removed updateMtime method that did not seem to be used

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-04-09 18:37:32 UTC (rev 8784)
+++ trunk/plearn/vmat/VMatrix.h	2008-04-09 18:51:23 UTC (rev 8785)
@@ -385,8 +385,6 @@
 
     void updateMtime(const PPath& p);
 
-    void updateMtime(const VMatrix& v){updateMtime(v.getMtime());}
-
     void updateMtime(VMat v);
 
     /**



From tihocan at mail.berlios.de  Wed Apr  9 20:51:53 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 9 Apr 2008 20:51:53 +0200
Subject: [Plearn-commits] r8786 - trunk/scripts/Skeletons
Message-ID: <200804091851.m39IprT0000368@sheep.berlios.de>

Author: tihocan
Date: 2008-04-09 20:51:53 +0200 (Wed, 09 Apr 2008)
New Revision: 8786

Modified:
   trunk/scripts/Skeletons/RowBufferedVMatrix.cc
   trunk/scripts/Skeletons/SourceVMatrix.cc
   trunk/scripts/Skeletons/VMatrix.cc
Log:
Updated skeletons after removal of unused updateMtime method

Modified: trunk/scripts/Skeletons/RowBufferedVMatrix.cc
===================================================================
--- trunk/scripts/Skeletons/RowBufferedVMatrix.cc	2008-04-09 18:51:23 UTC (rev 8785)
+++ trunk/scripts/Skeletons/RowBufferedVMatrix.cc	2008-04-09 18:51:53 UTC (rev 8786)
@@ -63,7 +63,6 @@
     updateMtime(0);
     //updateMtime(filename);
     //updateMtime(VMat);
-    //updateMtime(VMatrix);
 }
 
 // ### Nothing to add here, simply calls build_

Modified: trunk/scripts/Skeletons/SourceVMatrix.cc
===================================================================
--- trunk/scripts/Skeletons/SourceVMatrix.cc	2008-04-09 18:51:23 UTC (rev 8785)
+++ trunk/scripts/Skeletons/SourceVMatrix.cc	2008-04-09 18:51:53 UTC (rev 8786)
@@ -73,13 +73,14 @@
     // ### In a SourceVMatrix, you will typically end build_() with:
     // setMetaInfoFromSource();
 
-    // ### You should keep the line 'updateMtime(0);' if you don't implement the 
-    // ### update of the mtime. Otherwise you can have an mtime != 0 that is not valid.
-    // ### setMetaInfoFromSource() update the mtime to the same as the source.
+    // ### You should keep the line 'updateMtime(0);' if you do not implement
+    // ### the update of the mtime. Otherwise you can have an mtime != 0 that
+    // ### is not valid.
+    // ### Note that setMetaInfoFromSource() updates the mtime to the same as
+    // ### the source, but this value will be erased with 'updateMtime(0)'.
     updateMtime(0);
     //updateMtime(filename);
     //updateMtime(VMat);
-    //updateMtime(VMatrix);
 }
 
 ///////////////

Modified: trunk/scripts/Skeletons/VMatrix.cc
===================================================================
--- trunk/scripts/Skeletons/VMatrix.cc	2008-04-09 18:51:23 UTC (rev 8785)
+++ trunk/scripts/Skeletons/VMatrix.cc	2008-04-09 18:51:53 UTC (rev 8786)
@@ -75,7 +75,6 @@
     updateMtime(0);
     //updateMtime(filename);
     //updateMtime(VMat);
-    //updateMtime(VMatrix);
 }
 
 /////////



From tihocan at mail.berlios.de  Wed Apr  9 20:52:31 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 9 Apr 2008 20:52:31 +0200
Subject: [Plearn-commits] r8787 - trunk/plearn_learners/distributions
Message-ID: <200804091852.m39IqVOo000430@sheep.berlios.de>

Author: tihocan
Date: 2008-04-09 20:52:31 +0200 (Wed, 09 Apr 2008)
New Revision: 8787

Modified:
   trunk/plearn_learners/distributions/GaussMix.cc
Log:
Fixed bug when computing the likelihood of a sample with all missing values

Modified: trunk/plearn_learners/distributions/GaussMix.cc
===================================================================
--- trunk/plearn_learners/distributions/GaussMix.cc	2008-04-09 18:51:53 UTC (rev 8786)
+++ trunk/plearn_learners/distributions/GaussMix.cc	2008-04-09 18:52:31 UTC (rev 8787)
@@ -1523,7 +1523,9 @@
                     // real squared_norm_y_centered = pownorm(y_centered);
                     int n_eig = n_non_missing;
 
-                    real lambda0 = max(var_min, eigenvals.lastElement());
+                    real lambda0 = var_min;
+                    if (!eigenvals.isEmpty() && eigenvals.lastElement() > lambda0)
+                        lambda0 = eigenvals.lastElement();
                     PLASSERT( lambda0 > 0 );
                     real one_over_lambda0 = 1.0 / lambda0;
 



From tihocan at mail.berlios.de  Wed Apr  9 20:55:11 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 9 Apr 2008 20:55:11 +0200
Subject: [Plearn-commits] r8788 - trunk/plearn_learners/distributions/test
Message-ID: <200804091855.m39ItBDH000818@sheep.berlios.de>

Author: tihocan
Date: 2008-04-09 20:55:10 +0200 (Wed, 09 Apr 2008)
New Revision: 8788

Modified:
   trunk/plearn_learners/distributions/test/pytest.config
Log:
Enabled test PL_GaussMix_General_Missing: I think the bug that made it crash on some platforms is now fixed (thanks Fred!)

Modified: trunk/plearn_learners/distributions/test/pytest.config
===================================================================
--- trunk/plearn_learners/distributions/test/pytest.config	2008-04-09 18:52:31 UTC (rev 8787)
+++ trunk/plearn_learners/distributions/test/pytest.config	2008-04-09 18:55:10 UTC (rev 8788)
@@ -209,7 +209,7 @@
     resources = [ "gaussmix_general_missing.pyplearn" ],
     precision = 1e-06,
     pfileprg = "__program__",
-    disabled = True
+    disabled = False
     )
 
 Test(



From nouiz at mail.berlios.de  Wed Apr  9 21:44:18 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 9 Apr 2008 21:44:18 +0200
Subject: [Plearn-commits] r8789 - trunk/plearn/sys
Message-ID: <200804091944.m39JiI4T006401@sheep.berlios.de>

Author: nouiz
Date: 2008-04-09 21:44:18 +0200 (Wed, 09 Apr 2008)
New Revision: 8789

Modified:
   trunk/plearn/sys/PLMPI.cc
   trunk/plearn/sys/PLMPI.h
Log:
broke dependency


Modified: trunk/plearn/sys/PLMPI.cc
===================================================================
--- trunk/plearn/sys/PLMPI.cc	2008-04-09 18:55:10 UTC (rev 8788)
+++ trunk/plearn/sys/PLMPI.cc	2008-04-09 19:44:18 UTC (rev 8789)
@@ -38,11 +38,13 @@
  ******************************************************* */
 
 #include "PLMPI.h"
-#include "string.h"
 #include <plearn/base/plerror.h>
-#include <stdio.h>
 #include <plearn/io/FdPStreamBuf.h>
+#include <plearn/io/PStream.h>
+#include <plearn/math/Mat.h>
 
+#include "string.h"
+#include <stdio.h>
 namespace PLearn {
 using namespace std;
 

Modified: trunk/plearn/sys/PLMPI.h
===================================================================
--- trunk/plearn/sys/PLMPI.h	2008-04-09 18:55:10 UTC (rev 8788)
+++ trunk/plearn/sys/PLMPI.h	2008-04-09 19:44:18 UTC (rev 8789)
@@ -51,17 +51,16 @@
 #include <mpi.h>
 #endif
 
-#include <plearn/math/Mat.h>
-#include <plearn/base/plerror.h>
 // norman: changed to standard calls
 #include <iostream>
 #include <fstream>
-#include <plearn/io/PStream.h>
 
 namespace PLearn {
 using namespace std;
 
-
+//! Forward declarations.
+class Mat;
+class PStream;
 /*! Example of code using the PLMPI facility:
 
 In your main function, make sure to call



From tihocan at mail.berlios.de  Thu Apr 10 16:34:06 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 10 Apr 2008 16:34:06 +0200
Subject: [Plearn-commits] r8790 - in trunk/examples/data/test_suite: .
	linear_4x_2y.pmat.metadata
Message-ID: <200804101434.m3AEY6RI002019@sheep.berlios.de>

Author: tihocan
Date: 2008-04-10 16:34:05 +0200 (Thu, 10 Apr 2008)
New Revision: 8790

Added:
   trunk/examples/data/test_suite/linear_4x_2y.pmat.metadata/
   trunk/examples/data/test_suite/linear_4x_2y.pmat.metadata/FieldInfo/
   trunk/examples/data/test_suite/linear_4x_2y.pmat.metadata/sizes
Modified:
   trunk/examples/data/test_suite/
Log:
Added metadata directory to the repository, so that the sizes are correct


Property changes on: trunk/examples/data/test_suite
___________________________________________________________________
Name: svn:ignore
   - gauss_1D_100pt.amat.metadata
linear_2x_2y.amat.metadata
linear_4x_2y.amat.metadata
sin_signcos_1x_2y.amat.metadata
multi_gaussian_data.amat.metadata
linear_4x_2y.pmat.metadata
data_with_strings.amat.metadata

   + gauss_1D_100pt.amat.metadata
linear_2x_2y.amat.metadata
linear_4x_2y.amat.metadata
sin_signcos_1x_2y.amat.metadata
multi_gaussian_data.amat.metadata
data_with_strings.amat.metadata


Added: trunk/examples/data/test_suite/linear_4x_2y.pmat.metadata/sizes
===================================================================
--- trunk/examples/data/test_suite/linear_4x_2y.pmat.metadata/sizes	2008-04-09 19:44:18 UTC (rev 8789)
+++ trunk/examples/data/test_suite/linear_4x_2y.pmat.metadata/sizes	2008-04-10 14:34:05 UTC (rev 8790)
@@ -0,0 +1 @@
+4 2 0 0



From tihocan at mail.berlios.de  Thu Apr 10 17:02:34 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 10 Apr 2008 17:02:34 +0200
Subject: [Plearn-commits] r8791 - trunk/plearn_learners/distributions/test
Message-ID: <200804101502.m3AF2YYL003657@sheep.berlios.de>

Author: tihocan
Date: 2008-04-10 17:02:34 +0200 (Thu, 10 Apr 2008)
New Revision: 8791

Modified:
   trunk/plearn_learners/distributions/test/pytest.config
Log:
Re-disabled test PL_GaussMix_General_Missing: it looks like it still does not work everywhere yet ;)

Modified: trunk/plearn_learners/distributions/test/pytest.config
===================================================================
--- trunk/plearn_learners/distributions/test/pytest.config	2008-04-10 14:34:05 UTC (rev 8790)
+++ trunk/plearn_learners/distributions/test/pytest.config	2008-04-10 15:02:34 UTC (rev 8791)
@@ -209,7 +209,7 @@
     resources = [ "gaussmix_general_missing.pyplearn" ],
     precision = 1e-06,
     pfileprg = "__program__",
-    disabled = False
+    disabled = True
     )
 
 Test(



From louradou at mail.berlios.de  Thu Apr 10 17:40:05 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Thu, 10 Apr 2008 17:40:05 +0200
Subject: [Plearn-commits] r8792 - trunk/python_modules/plearn/learners
Message-ID: <200804101540.m3AFe5lb007913@sheep.berlios.de>

Author: louradou
Date: 2008-04-10 17:40:05 +0200 (Thu, 10 Apr 2008)
New Revision: 8792

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:


Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2008-04-10 15:02:34 UTC (rev 8791)
+++ trunk/python_modules/plearn/learners/SVM.py	2008-04-10 15:40:05 UTC (rev 8792)
@@ -192,7 +192,7 @@
                              (paramname, self.param_names)
 
         if not len(self.trials_param_list):
-            return None
+            return []
 
         param_list=[]
         cost_list=[]
@@ -349,7 +349,7 @@
             if cost <= self.trials_cost_list[i]:
                 self.trials_cost_list[i] = cost
 
-        if( self.best_cost == None or cost <= self.best_cost):
+        if( self.best_cost == None or cost < self.best_cost):
         # TODO: what if cost == bestcost and param <> self.best_param?
         #       -> best param should be a list...
             self.best_param = param
@@ -498,7 +498,7 @@
 
             if not tried_C:
                 new_param_list += self.choose_first_C_param( \
-                                        [ {'degree':d, 'coef0':self.coef0_initvalue} ], \
+                                        [ {'degree':d, 'coef0':self.best_param['coef0']} ], \
                                         self.best_param['C'] )
             
             elif( tried_C[0] == min(tried_C)
@@ -534,8 +534,6 @@
     """ Return <bool>: whether or not we should try other hyperparameter values.
     """
     def should_be_tuned_again(self):
-        if len(self.trials_param_list) > 50:
-            return False
         best_degree = self.best_param['degree']
         tried_degrees = self.get_trials_oneparam_list('degree')
         if( best_degree in tried_degrees
@@ -590,38 +588,49 @@
                          By default (if set to None), the first cost of costnames
                          is taken. 
 
-        'validtype': <str> type of validation: 'simple' or 'cross'.
-        
         'n_fold': <int> Number of folds in the cross-validation
 
+        'normalize_inputs': <bool> Whether to normalize inputs so as to have
+                            mean=0, std=1 for each input component on the train set.
+        
         'balanceC': <bool> Whether to use a different 'C' for each class,
-                         class SVM(    based on the frequency of each class in the training
-                             set. Useful when: classes are unbalanced and the cost
-                             of interest is normalized w.r.t class prior probas.
+                           based on the frequency of each class in the training
+                           set. Useful when: classes are unbalanced and the cost
+                           of interest is normalized w.r.t class prior probas.
+
+        'use_proba': <int> in [-1,0,1] specifies the kinds of SVM outputs (of which
+                     will depend the predictions).
+                     0 is a simple onehot of the prediction.
+                     -1 is the vote counts (one-against-one strategy for multi-class).
+                     1 is the posterior emperical probability (using sigmoids of SVM standard outputs).
                                      
-        'results_filename'
-        'preproc_optionnames'        
-        'preproc_optionvalues'
-
-
-
-        - 'retrain_on_valid': <bool> Use the best hyperparameters (found on valid)
+        'retrain_on_valid': <bool> Use the best hyperparameters (found on valid)
                              to re-train a bigger model on {train, valid}.
                              Note: this option is obsolete for cross-validation
                              (valid_samples = None), were the model is re-trained
                              in any case.
 
-        - 'retrain_until_local_optimum_is_found': <bool> Should train_and_tune()
-                         class SVM(continue to tune hyperparameters until a local optimum
-                         is found. If False, you can re-run train_and_tune()
-                         several times. If True, you can also re-run to possibly
-                         find better performance.
+        'retrain_until_local_optimum_is_found': <bool> when calling run(), whether or
+                         not to continue to tune hyperparameters until a local optimum
+                         is found. If False, you can re-run run() several times. If True,
+                         you can also re-run to possibly find better performance.
 
-        - 'test_on_train': <bool> Should we test best models on {test, train} (1)
+        'max_ntries': <int> when calling run(), maximum number of hyperparameters to try
+                      since the last forget().
 
+        'test_on_train': <bool> Should we test best models on {test, train} (1)
 
-        - 'verbosity': <int> Level of verbosity.
+        'results_filename': <string> Path to an output file for results
+        
+        'preproc_optionnames': <string> or <list of strings> indicating the names of the
+                               hyperparameters of the pre-processing that should appear
+                               in the results (results_filename)
 
+        'preproc_optionvalues': <string> or <list> indicating the values of the hyperparameters
+                                corresponding to preproc_optionnames.
+
+        'verbosity': <int> Level of verbosity.
+
     public: learntoption
 
         'best_model':
@@ -650,6 +659,8 @@
 
         'input_avgstd': <float> input std
 
+        'validtype': <str> type of validation: 'simple' or 'cross'.
+        
        
     protected:
 
@@ -668,14 +679,20 @@
                         'costnames',
                         'errorcosts',
                         'maincost_name',
-                        'validtype',
                         'n_fold',
+                        'normalize_inputs',
                         'balanceC',
-                        'normalize_inputs',
+                        'use_proba',
+                        'retrain_until_local_optimum_is_found',
+                        'max_ntries',
+                        'retrain_on_valid',
+                        'test_on_train',
+                        'verbosity',
                         'results_filename',
                         'preproc_optionnames',
                         'preproc_optionvalues',
-                        'verbosity',
+                        \
+                        'validtype',
                         'param',
                         'best_param',
                         'model',
@@ -683,13 +700,13 @@
                         'valid_stats',
                         'test_stats',
                         'train_stats',
-                        'param_names',
                         'nclasses',
                         'class_priors',
                         'inputsize',
                         'input_avgstd',
-                        'stats_are_uptodate',
-                        'get_datalist'
+                        \
+                        'param_names',
+                        'stats_are_uptodate'
                      ]
      
     def __init__(self):
@@ -741,6 +758,7 @@
 
         self.retrain_on_valid = True
         self.retrain_until_local_optimum_is_found = True
+        self.max_ntries = 50
         self.test_on_train = True
 
         
@@ -750,7 +768,6 @@
         self.validset_key = 'validset'
         self.testset_key  = 'testset'
 
-        self.compute_outputs_from_probabilities = None
         self.use_proba = False
 
     def forget(self):
@@ -842,6 +859,7 @@
 
         samples, targets = self.get_svminputlist( vmat, True )
 
+        self.all_experts[0].verbosity = self.verbosity
         self.inputsize, self.input_avgstd = self.all_experts[0].get_input_stats(samples)
         for expert in self.all_experts[1:]:
             expert.set_input_stats( self.inputsize, self.input_avgstd )
@@ -922,7 +940,7 @@
         if len(s)>0:s=', '+s
 
         
-        if self.use_proba or self.compute_outputs_from_probabilities:
+        if self.use_proba:
             # if this function is defined (see 
             return eval('svm_parameter( svm_type = C_SVC, probability = 1 '+s+')' )
         else:
@@ -1070,7 +1088,7 @@
             kernel_param[pn] = param[pn]
         expert.update_trials(kernel_param, maincost)
 
-        if( bestcost == None or maincost <= bestcost):
+        if( bestcost == None or maincost < bestcost):
             self.best_param = param
             self.valid_stats = valid_stats
             self.test_stats  = test_stats
@@ -1091,7 +1109,7 @@
         
         if param == None:
             if not self.best_param:
-                return self.train_and_tune(dataspec)
+                return self.run(dataspec)
             param = self.best_param.copy()
         elif 'kernel_type' not in param:
             param['kernel_type'] = self.kernel_type
@@ -1121,16 +1139,15 @@
         decision than the standard ones, for instance when
         classifying bags of data (where each bag correspond to a target)
     """
-    def update_predictions_targets(self, predictions, targets, vmat):
+    def predict_from_outputs(self, outputs, targets, vmat):
+        assert self.nclasses == len(outputs[0])
+        predictions = []
+        for o in outputs:
+            predictions.append( array(o).argmax() )
         return predictions, targets
 
-    """ Return the costs obtained by a libSVM model
-        on a given dataset
-    """
-    def test( self,
-              testset,
-              teststats = None
-             ):
+    def get_outputs_targets( self,
+                                 testset ):
         assert testset <> None
         # By default take the LAST model trained
         if self.model <> None:
@@ -1144,6 +1161,48 @@
         nclasses = self.nclasses
         nsamples = len(samples)
 
+        ## specific to libsvm
+        ##
+        # Note: model.predict_values(x)
+        #           gives a dictionary with the SVM scores for one-against-one
+        #       model.predict(x)
+        #           gives the prediction by "max win" strategy (each SVM has a vote 1 for a class)
+        #       model.predict_probability(x)
+        #           gives a dictionary with proabilities corresponding to each class
+        outputs = []
+        if self.use_proba == 1:
+            for x in samples:
+                prd, prb = model.predict_probability(x)
+                output = [0]*nclasses
+                for c in prb:
+                    output[int(c)] = prb[c]
+                outputs.append(output)
+        elif self.use_proba == 0:
+            for x in samples:
+                output = [0]*nclasses
+                output[ int(model.predict(x)) ] += 1
+                outputs.append( output )                
+        elif self.use_proba == -1:
+            for x in samples:
+                output = [0]*nclasses
+                oneagainstone_dict = model.predict_values(x)
+                for cl1cl2 in oneagainstone_dict:
+                    if oneagainstone_dict[cl1cl2] > 0:
+                        output[cl1cl2[0]] += 1
+                    else:
+                        output[cl1cl2[1]] += 1
+                outputs.append( output )
+
+        return outputs, targets
+
+    """ Return the costs obtained by a libSVM model
+        on a given dataset
+    """
+    def test( self,
+              testset,
+              teststats = None
+             ):
+        nclasses = self.nclasses
         costnames = self.costnames
         # Translation of the cost 'confusion_matrix'
         # to several costs (one per matrix component)
@@ -1159,24 +1218,9 @@
             teststats= VecStatsCollector()        
             teststats.setFieldNames( costnames )
 
-        if self.compute_outputs_from_probabilities:
-            predictions=[]
-            probas=[]
-            for x in samples:
-                ## specific to libsvm
-                prd, prb = model.predict_probability(x)
-                probas.append(prb)
-                predictions.append( int(prd) )
-            predictions, targets = self.compute_outputs_from_probabilities( probas, targets, testset)
+        outputs, targets = self.get_outputs_targets( testset )
+        predictions, targets = self.predict_from_outputs( outputs, targets, testset)
 
-        else:
-            predictions=[]
-            for x in samples:
-                ## specific to libsvm
-                predictions.append( int(model.predict(x)) )
-            predictions, targets = self.update_predictions_targets( predictions, targets, testset)
-
-
         # Computing misclassification costs for the default normalized
         # classification error (= class error weighted w.r.t class priors)
         
@@ -1229,7 +1273,6 @@
         if self.validtype[:6]=='simple' and self.validset_key in dataspec:
             return self.simplevalid(dataspec, param)
         else:
-            assert self.validset_key in dataspec
             return self.crossvalid(dataspec, param)
 
     """ Return the costs obtained by (simple) validation
@@ -1238,11 +1281,11 @@
     def simplevalid( self,
                      dataspec,
                      param= None,
-                     validstats = None ):
-        self.validtype = 'simple'
+                     validstats = None,
+                     verbosity = True ):
         if not param:
             param = self.best_param
-        if self.verbosity > 0:
+        if self.verbosity > 0 and verbosity:
             print "\n** Simple Validation"
             print "   with param %s" % param
         self.train(dataspec, param)
@@ -1267,25 +1310,45 @@
             print "   with param %s" % param
         
         trainset = self.train_inputspec(dataspec)
-        N=trainset.length
+        trainset_class = [None]*self.nclasses
+        N=[0]*self.nclasses
+        Nfold=[0]*self.nclasses
+        Nlastfold=[0]*self.nclasses
+        for c in range(self.nclasses):
+            trainset_class[c] = ClassSubsetVMatrix( source = trainset,
+                                                    classes = [c],)
+            N[c] = trainset_class[c].length
+            Nfold[c] = int(N[c] / n_fold)
+            if Nfold[c] == 0:
+                raise ValueError, "%d-fold cross-validation is not possible as class %d has only %d samples" % \
+                                 (n_fold, c, N[c])
+            Nlastfold[c] = N[c] - Nfold[c] * (n_fold-1)
         
         validstats = None
+        sub_trainset_class = [None]*self.nclasses
+        sub_testset_class = [None]*self.nclasses
         for i in range(n_fold):
-            sub_testset = SelectRowsVMatrix(
-                                source = trainset,
-                                indices = range(i,N,n_fold)
-                            )
-            indices = []
-            for j in range(0,i)+range(i+1,n_fold):
-                indices += range(j,N,n_fold)
-            sub_trainset = SelectRowsVMatrix(
-                                source = trainset,
-                                indices = indices
-                            )
+            for c in range(self.nclasses):
+                if i < n_fold-1:
+                    test_indices = range( i*Nfold[c], (i+1)*Nfold[c] )
+                    train_indices = range( 0,i*Nfold[c])+range((i+1)*Nfold[c], N[c] )
+                else:
+                    test_indices = range( i*Nfold[c], N[c] )
+                    train_indices = range( 0, N[c]-Nlastfold[c] )
+                sub_testset_class[c] = SelectRowsVMatrix(
+                                    source = trainset_class[c],
+                                    indices = test_indices,)
+                sub_trainset_class[c] = SelectRowsVMatrix(
+                                    source = trainset_class[c],
+                                    indices = train_indices,)
+            sub_testset = ConcatRowsVMatrix( sources = sub_testset_class,
+                                             fully_check_mappings = False,)
+            sub_trainset = ConcatRowsVMatrix( sources = sub_trainset_class,
+                                             fully_check_mappings = False,)
             validstats = self.simplevalid({self.trainset_key:sub_trainset,
                                            self.validset_key:sub_testset},
                                             param,
-                                            validstats)
+                                            validstats, False)
 
         return validstats
 
@@ -1296,10 +1359,10 @@
         The train set is mandatory, but valid and/or test sets can be missing.
         cf. train_inputspec(), valid_inputspec(), and test_inputspec().
     """
-    def train_and_tune(self, dataspec):
+    def run(self, dataspec):
 
         if self.verbosity > 3:
-            print "SVM::train_and_tune() called ", dataspec.keys()
+            print "SVM::run() called ", dataspec.keys()
 
         trainset = self.train_inputspec(dataspec)
         validset = self.valid_inputspec(dataspec)
@@ -1388,8 +1451,10 @@
                 self.write_results( self.best_param,
                                     self.valid_stats, self.test_stats, self.train_stats )
 
-        if self.retrain_until_local_optimum_is_found and expert.should_be_tuned_again():
-           self.train_and_tune( dataspec )
+        if( self.retrain_until_local_optimum_is_found
+        and expert.should_be_tuned_again()
+        and len(expert.trials_param_list) < self.max_ntries):
+           self.run( dataspec )
 
         return dataspec
 
@@ -1449,25 +1514,18 @@
 
 """"""
 
-""" In the following:
+""" Below:
     Some (API) Functions that can be assigned
     to a SVM object, to deal with bags of data
     to classify.
-"""
-"""
 
 # Usage:
 #-------
     
 svm = SVM()
 svm.get_datalist = ToBagClassifier_get_datalist
+svm.predict_from_outputs = ToBagClassifier_predict_from_outputs
 
-vote_with_proba = 1 # whether or not to use probabiblity
-                    # instead of hard vote [0,1] for each element of a bag.
-if vote_with_proba:
-    svm.compute_outputs_from_probabilities = ToBagClassifier_compute_outputs_from_probabilities
-else:
-    svm.update_predictions_targets = ToBagClassifier_update_predictions_targets
 """         
 def get_baginfo( input_vmat ):
     data_array = input_vmat.getMat()
@@ -1489,38 +1547,24 @@
     targets   = [ float(t) for t in data_array[:,inputsize] ]
     return samples, targets
 
-def ToBagClassifier_update_predictions_targets(predictions, targets, vmat):
+def ToBagClassifier_predict_from_outputs(outputs, targets, vmat):
     baginfo = get_baginfo(vmat)
-    assert len(predictions) == len(targets) == len(baginfo)
-    nclasses = max(predictions)+1
+    assert len(outputs) == len(targets) == len(baginfo)
+    nclasses = len(outputs[0])
     bag_predictions=[]
     bag_targets=[]
-    for p, t, b in zip(predictions, targets, baginfo):
+    votes = zeros(nclasses)
+    for output, t, b in zip(outputs, targets, baginfo):
         if b in [1,3]: # beginning of a bag
             votes = zeros(nclasses)
-        votes[p] +=1
+        for o, c in zip(output, range(nclasses)):
+            votes[c] += o
         if b in [2,3]: # end of a bag
             bag_predictions.append( votes.argmax() )
             bag_targets.append( t )
     return bag_predictions, bag_targets
 
-def ToBagClassifier_compute_outputs_from_probabilities(probas, targets, vmat):
-    baginfo = get_baginfo(vmat)
-    assert len(probas) == len(targets) == len(baginfo)
-    nclasses = len(probas[0])
-    bag_predictions=[]
-    bag_targets=[]
-    for p, t, b in zip(probas, targets, baginfo):
-        if b in [1,3]: # beginning of a bag
-            votes = zeros(nclasses)
-        for c in p:
-            votes[c] += p[c]
-        if b in [2,3]: # end of a bag
-            bag_predictions.append( votes.argmax() )
-            bag_targets.append( t )
-    return bag_predictions, bag_targets
 
-
 if __name__ == '__main__':
 
     import os.path
@@ -1618,12 +1662,12 @@
     svm.preproc_optionvalues = [ normalize_inputs ,  use_proba ]
     
     svm.kernel_type = 'poly'
-    svm.train_and_tune(dataspec)
+    svm.run(dataspec)
 
     svm.kernel_type = 'rbf'
-    svm.train_and_tune(dataspec)
+    svm.run(dataspec)
 
     svm.kernel_type = 'linear'
-    svm.train_and_tune(dataspec)
+    svm.run(dataspec)
 
     """ =============================================================== """



From louradou at mail.berlios.de  Thu Apr 10 17:55:49 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Thu, 10 Apr 2008 17:55:49 +0200
Subject: [Plearn-commits] r8793 - in trunk: commands plearn_learners/online
Message-ID: <200804101555.m3AFtnw9009275@sheep.berlios.de>

Author: louradou
Date: 2008-04-10 17:55:48 +0200 (Thu, 10 Apr 2008)
New Revision: 8793

Added:
   trunk/plearn_learners/online/LogaddOnBagsModule.cc
   trunk/plearn_learners/online/LogaddOnBagsModule.h
   trunk/plearn_learners/online/OnBagsModule.cc
   trunk/plearn_learners/online/OnBagsModule.h
Modified:
   trunk/commands/plearn_noblas_inc.h
   trunk/plearn_learners/online/ModuleLearner.cc
   trunk/plearn_learners/online/ModuleLearner.h
Log:
Added some stuffs to process bags of data with modules.



Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2008-04-10 15:40:05 UTC (rev 8792)
+++ trunk/commands/plearn_noblas_inc.h	2008-04-10 15:55:48 UTC (rev 8793)
@@ -214,6 +214,7 @@
 #include <plearn_learners/online/LayerCostModule.h>
 #include <plearn_learners/online/LinearCombinationModule.h>
 #include <plearn_learners/online/LinearFilterModule.h>
+#include <plearn_learners/online/LogaddOnBagsModule.h>
 #include <plearn_learners/online/MatrixModule.h>
 #include <plearn_learners/online/MaxSubsampling2DModule.h>
 #include <plearn_learners/online/ModuleLearner.h>

Added: trunk/plearn_learners/online/LogaddOnBagsModule.cc
===================================================================
--- trunk/plearn_learners/online/LogaddOnBagsModule.cc	2008-04-10 15:40:05 UTC (rev 8792)
+++ trunk/plearn_learners/online/LogaddOnBagsModule.cc	2008-04-10 15:55:48 UTC (rev 8793)
@@ -0,0 +1,142 @@
+// -*- C++ -*-
+
+// LogaddOnBagsModule.cc
+//
+// Copyright (C) 2008 Jerome Louradour
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Author: Jerome Louradour
+
+/*! \file LogaddOnBagsModule.cc */
+
+
+
+#include "LogaddOnBagsModule.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    LogaddOnBagsModule,
+    "For each input i, on a bag of size N, outputs:"
+       " Logadd([ input_i(1), ..., input_i(N) ]).",
+    "The 'input' port is typically connected to the output\n"
+    "port of class activities (input_size = number of classes).\n"
+    "see OnBagsModule for details on bags.\n");
+
+LogaddOnBagsModule::LogaddOnBagsModule()
+{
+    output_size = -1;
+}
+
+void LogaddOnBagsModule::declareOptions(OptionList& ol)
+{
+    inherited::declareOptions(ol);
+
+    redeclareOption(ol, "output_size", &LogaddOnBagsModule::output_size,
+                  OptionBase::learntoption,
+                  "Size of the 'output' port (same as 'input').");
+}
+
+void LogaddOnBagsModule::build_()
+{
+    PLASSERT( input_size > 0 );
+    output_size = input_size;
+    accumulated_output.resize(output_size);
+    inherited::build();
+}
+
+void LogaddOnBagsModule::build()
+{
+    inherited::build();
+    build_();
+}
+
+void LogaddOnBagsModule::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+    deepCopyField(accumulated_output, copies);
+}
+
+///////////
+// fprop //
+///////////
+
+void LogaddOnBagsModule::fpropInit(const Vec& input)
+{
+    accumulated_output << input;
+}
+void LogaddOnBagsModule::fpropAcc(const Vec& input)
+{
+    for( int i = 0; i < input_size; i++ )
+        accumulated_output[i] = logadd(accumulated_output[i],
+                                       input[i]); 
+}
+void LogaddOnBagsModule::fpropOutput(Vec& output)
+{
+    output.resize( output_size );
+    output << accumulated_output;
+}
+
+/////////////////
+// bpropUpdate //
+/////////////////
+
+void LogaddOnBagsModule::bprop( const Mat& baginputs,
+                                const Vec& bagoutput_gradient,
+                                Mat& baginputs_gradients)
+{
+    int nsamples = baginputs.length();
+    baginputs_gradients.resize( nsamples, input_size);
+    for( int i = 0; i < input_size; i++ )
+    {
+        Vec tmp_input_gradient;
+        tmp_input_gradient.resize( nsamples );
+        softmax( baginputs.column(i).toVecCopy() ,
+                 tmp_input_gradient );
+        tmp_input_gradient *= bagoutput_gradient[i];
+        baginputs_gradients.column(i) << tmp_input_gradient;
+    }
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


Property changes on: trunk/plearn_learners/online/LogaddOnBagsModule.cc
___________________________________________________________________
Name: svn:executable
   + *

Added: trunk/plearn_learners/online/LogaddOnBagsModule.h
===================================================================
--- trunk/plearn_learners/online/LogaddOnBagsModule.h	2008-04-10 15:40:05 UTC (rev 8792)
+++ trunk/plearn_learners/online/LogaddOnBagsModule.h	2008-04-10 15:55:48 UTC (rev 8793)
@@ -0,0 +1,125 @@
+// -*- C++ -*-
+
+// LogaddOnBagsModule.h
+//
+// Copyright (C) 2007 Jerome Louradour
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Jerome Louradour
+
+/*! \file LogaddOnBagsModule.h */
+
+#ifndef LogaddOnBagsModule_INC
+#define LogaddOnBagsModule_INC
+
+#include <plearn_learners/online/OnBagsModule.h>
+
+#include <map>
+
+namespace PLearn {
+
+class LogaddOnBagsModule : public OnBagsModule
+{
+    typedef OnBagsModule inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //#####  Public Learnt Options  ###########################################
+
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    LogaddOnBagsModule();
+
+    //! update internal statistics needed to compute the output of bag
+    virtual void fpropAcc(const Vec& input);
+    virtual void fpropInit(const Vec& input);
+    virtual void fpropOutput(Vec& output);
+
+    //! compute the gradient w.r.t bag inputs
+    virtual void bprop( const Mat& baginputs,
+                        const Vec& bagoutput_gradient,
+                        Mat& baginputs_gradients);
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(LogaddOnBagsModule);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+
+    Vec accumulated_output;
+    
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(LogaddOnBagsModule);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


Property changes on: trunk/plearn_learners/online/LogaddOnBagsModule.h
___________________________________________________________________
Name: svn:executable
   + *

Modified: trunk/plearn_learners/online/ModuleLearner.cc
===================================================================
--- trunk/plearn_learners/online/ModuleLearner.cc	2008-04-10 15:40:05 UTC (rev 8792)
+++ trunk/plearn_learners/online/ModuleLearner.cc	2008-04-10 15:55:48 UTC (rev 8793)
@@ -42,6 +42,7 @@
 #include "ModuleLearner.h"
 #include <plearn_learners/online/NullModule.h>
 #include <plearn/io/pl_log.h>
+#include <plearn/var/SumOverBagsVariable.h>
 
 namespace PLearn {
 using namespace std;
@@ -79,6 +80,7 @@
     target_ports(TVec<string>(1, "target")),
     // Note: many learners do not use weights, thus the default behavior is not
     // to have a 'weight' port in 'weight_ports'.
+    operate_on_bags(false),
     reset_seed_upon_train(0),
     mbatch_size(-1)
 {
@@ -123,6 +125,11 @@
                   OptionBase::buildoption,
        "List of ports that take the weight part of a sample as input.");
 
+    declareOption(ol, "operate_on_bags", &ModuleLearner::operate_on_bags,
+                  OptionBase::buildoption,
+       "If true, then each training step will be done on batch_size *bags*\n"
+       "of samples (instead of batch_size samples).");
+
     declareOption(ol, "mbatch_size", &ModuleLearner::mbatch_size,
                   OptionBase::learntoption,
        "Effective 'batch_size': it takes the same value as 'batch_size'\n"
@@ -316,22 +323,61 @@
     if (report_progress)
         pb = new ProgressBar( "Training " + classname(), nstages - stage);
 
-    while (stage + mbatch_size <= nstages) {
-        // Obtain training samples.
-        int sample_start = stage % train_set->length();
-        train_set->getExamples(sample_start, mbatch_size, inputs, targets,
-                weights, NULL, true);
-        // Perform a training step.
-        trainingStep(inputs, targets, weights);
-        // Handle training progress.
-        stage += mbatch_size;
-        if (report_progress)
-            pb->update(stage - stage_init);
-    }
+    if( operate_on_bags && batch_size>0 )
+        while ( stage < nstages ) {
+            // Obtain training samples.
+            int sample_start = stage % train_set->length();
+            int isample = sample_start;
+            inputs.resize(0,0);
+            targets.resize(0,0);
+            weights.resize(0);
+            for( int nbags = 0; nbags < mbatch_size; nbags++ ) {
+                int bag_info = 0;
+                while( !(bag_info & SumOverBagsVariable::TARGET_COLUMN_LAST) ) {
+                    PLASSERT( isample < train_set->length() );
+                    Vec input, target; real weight;
+                    train_set->getExample(isample, input, target, weight);
+                    inputs.appendRow(input);
+                    targets.appendRow(target);
+                    weights.append( weight );
+                    bag_info = int(round(target.lastElement()));
+                    isample ++;
+                }
+                isample = isample % train_set->length();                 
+            }
+            if( stage + inputs.length() > nstages )
+                break;
+            // Perform a training step.
+            trainingStep(inputs, targets, weights);              
+            // Handle training progress.
+            stage += inputs.length();
+            if (report_progress)
+                pb->update(stage - stage_init);
+        }    
+    else
+        while (stage + mbatch_size <= nstages) {
+            // Obtain training samples.
+            int sample_start = stage % train_set->length();
+            train_set->getExamples(sample_start, mbatch_size, inputs, targets,
+                    weights, NULL, true);
+            // Perform a training step.
+            trainingStep(inputs, targets, weights);
+            // Handle training progress.
+            stage += mbatch_size;
+            if (report_progress)
+                pb->update(stage - stage_init);
+        }
     if (stage != nstages)
+    {
+        if( operate_on_bags && batch_size>0 )
         PLWARNING("In ModuleLearner::train - The network was trained for "
+                "only %d stages (instead of nstages = %d, which could not "
+                "be fulfilled with batch_size of %d bags)", stage, nstages, batch_size);
+        else
+        PLWARNING("In ModuleLearner::train - The network was trained for "
                 "only %d stages (instead of nstages = %d, which is not a "
-                "multiple of batch_size = %d", stage, nstages, batch_size);
+                "multiple of batch_size = %d)", stage, nstages, batch_size);
+    }
     OnlineLearningModule::during_training=false;
 
     // finalize statistics for this call

Modified: trunk/plearn_learners/online/ModuleLearner.h
===================================================================
--- trunk/plearn_learners/online/ModuleLearner.h	2008-04-10 15:40:05 UTC (rev 8792)
+++ trunk/plearn_learners/online/ModuleLearner.h	2008-04-10 15:55:48 UTC (rev 8793)
@@ -63,6 +63,8 @@
     TVec<string> target_ports;
     TVec<string> weight_ports;
 
+    bool operate_on_bags;
+
     int reset_seed_upon_train;
 
 public:

Added: trunk/plearn_learners/online/OnBagsModule.cc
===================================================================
--- trunk/plearn_learners/online/OnBagsModule.cc	2008-04-10 15:40:05 UTC (rev 8792)
+++ trunk/plearn_learners/online/OnBagsModule.cc	2008-04-10 15:55:48 UTC (rev 8793)
@@ -0,0 +1,354 @@
+// -*- C++ -*-
+
+// OnBagsModule.cc
+//
+// Copyright (C) 2008 Jerome Louradour
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Author: Jerome Louradour
+
+/*! \file OnBagsModule.cc */
+
+
+
+#include "OnBagsModule.h"
+#include <plearn/var/SumOverBagsVariable.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    OnBagsModule,
+    "Module that process (variable-length) bags of vectors.\n",
+    "One output vector is propagated at the end of the bag. Other outputs are MISSING_VALUE.\n"
+    "The 'bagtarget' contains the bag information at the last position, has follows\n"
+    "     bag info = 1. (01) beginning of bag\n"
+    "              = 0. (00) middle of bag\n"
+    "              = 2. (10) end of bag\n"
+    "              = 3. (11) single-row bag (both beginning and end)\n");
+
+OnBagsModule::OnBagsModule():
+    bagtarget_size(1)
+{
+}
+
+void OnBagsModule::declareOptions(OptionList& ol)
+{
+    inherited::declareOptions(ol);
+
+    declareOption(ol, "bagtarget_size", &OnBagsModule::bagtarget_size,
+                  OptionBase::buildoption,
+                  "Size of the 'bagtarget' port (bag info is the last element).");
+}
+
+void OnBagsModule::build_()
+{
+    PLASSERT( bagtarget_size > 0 );
+    PLASSERT( input_size > 0 );
+    
+    // The port story...
+    ports.resize(0);
+    portname_to_index.clear();
+    addPortName("input");
+    addPortName("output");
+    addPortName("bagtarget");
+
+    port_sizes.resize(nPorts(), 3);
+    port_sizes.fill(-1);
+    port_sizes(getPortIndex("input"), 1) = input_size;
+    port_sizes(getPortIndex("output"), 1) = output_size;
+    port_sizes(getPortIndex("bagtarget"), 1) = bagtarget_size;
+}
+
+void OnBagsModule::build()
+{
+    inherited::build();
+    build_();
+}
+
+void OnBagsModule::forget()
+{
+    wait_new_bag = true;
+}
+
+void OnBagsModule::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+    deepCopyField(ports, copies);
+}
+
+
+///////////
+// fprop //
+///////////
+
+
+void OnBagsModule::fprop(const TVec<Mat*>& ports_value)
+{
+    PLASSERT( ports_value.length() == nPorts() );
+
+    Mat* p_inputs = ports_value[getPortIndex("input")];
+    Mat* p_bagtargets = ports_value[getPortIndex("bagtarget")];
+    Mat* p_outputs = ports_value[getPortIndex("output")];
+
+    if ( p_outputs && p_outputs->isEmpty() )
+    {
+        PLASSERT( p_inputs && !p_inputs->isEmpty() );
+        PLASSERT( p_bagtargets && !p_bagtargets->isEmpty() );
+        fprop(*p_inputs, *p_bagtargets, *p_outputs);
+        checkProp(ports_value);
+    }
+    else
+        PLERROR("In OnBagsModule::fprop - Port configuration not implemented.");
+}
+
+void OnBagsModule::fprop(const Mat& inputs, const Mat& bagtargets, Mat& outputs)
+{
+    int n_samples = inputs.length();
+    outputs.resize( n_samples, output_size );
+    //outputs.fill( MISSING_VALUE );
+    for (int isample = 0; isample < n_samples; isample++)
+    {
+        Vec output;
+        fprop( inputs(isample), bagtargets(isample), output );
+        outputs.row(isample) << output;
+    }
+}
+
+void OnBagsModule::fprop(const Vec& input, const Vec& bagtarget, Vec& output)
+{
+    PLASSERT( input.length() == input_size );
+    PLASSERT( bagtarget.length() == bagtarget_size );
+    output.resize( output_size );
+    output.fill( MISSING_VALUE );
+    int bag_info = int(round(bagtarget.lastElement()));
+    if (bag_info & SumOverBagsVariable::TARGET_COLUMN_FIRST)
+    {
+        PLASSERT( wait_new_bag );
+        fpropInit( input );
+        wait_new_bag = false;
+    }
+    else
+    {   
+        PLASSERT( !wait_new_bag );
+        fpropAcc( input );
+    }
+    if (bag_info & SumOverBagsVariable::TARGET_COLUMN_LAST)
+    {
+        fpropOutput( output );
+        wait_new_bag = true;
+    }
+}
+
+// This function initializes internal statistics
+// for the first sample of a bag
+void OnBagsModule::fpropInit(const Vec& input)
+{
+    PLERROR("In OnBagsModule::fpropInit(input,output) - not implemented for class %s",classname().c_str());
+}
+
+// This function updates internal statistics given a new bag sample
+void OnBagsModule::fpropAcc(const Vec& input)
+{
+    PLERROR("In OnBagsModule::fpropAcc(input,output) - not implemented for class %s",classname().c_str());
+}
+
+// This function fills the output from the internal statistics
+// (once all the bag has been seen)
+void OnBagsModule::fpropOutput(Vec& output)
+{
+    PLERROR("In OnBagsModule::fpropOutput(input,output) - not implemented for class %s",classname().c_str());
+}
+
+/////////////////
+// bpropUpdate //
+/////////////////
+
+void OnBagsModule::bpropAccUpdate(const TVec<Mat*>& ports_value,
+                                  const TVec<Mat*>& ports_gradient)
+{
+    PLASSERT( input_size > 1 );
+    PLASSERT( ports_value.length() == nPorts() );
+    PLASSERT( ports_gradient.length() == nPorts() );
+
+    const Mat* p_inputs = ports_value[getPortIndex("input")];
+    const Mat* p_bagtargets = ports_value[getPortIndex("bagtarget")];
+    const Mat* p_output_grad = ports_gradient[getPortIndex("output")];
+    Mat* p_inputs_grad = ports_gradient[getPortIndex("input")];
+
+    if( p_inputs_grad
+        && p_output_grad && !p_output_grad->isEmpty() )
+    {
+	    PLASSERT( p_inputs && !p_inputs->isEmpty());
+	    PLASSERT( p_bagtargets && !p_bagtargets->isEmpty());
+        int n_samples = p_inputs->length();
+    	PLASSERT( p_output_grad->length() == n_samples );
+    	PLASSERT( p_output_grad->width() == output_size );
+        if( p_inputs_grad->isEmpty() )
+        {
+            p_inputs_grad->resize( n_samples, input_size);
+            p_inputs_grad->clear();
+        }
+        bpropUpdate( *p_inputs, *p_bagtargets, 
+                     *p_inputs_grad, *p_output_grad, true );
+    	checkProp(ports_gradient);
+    }
+    else if( !p_inputs_grad && !p_output_grad )
+        return;
+    else
+        PLERROR("In OnBagsModule::bpropAccUpdate - Port configuration not implemented.");
+
+}
+
+void OnBagsModule::bpropUpdate( const Mat& inputs,
+                                const Mat& bagtargets,
+                                Mat& input_gradients,
+                                const Mat& output_gradients,
+                                bool accumulate)
+{
+    PLASSERT( inputs.width() == input_size );
+    PLASSERT( bagtargets.width() == bagtarget_size );
+    PLASSERT( output_gradients.width() == output_size );
+    int n_samples = inputs.length();
+    PLASSERT( n_samples == bagtargets.length() );
+    PLASSERT( n_samples == output_gradients.length() );
+    if( accumulate )
+        PLASSERT_MSG( input_gradients.width() == input_size &&
+                input_gradients.length() == n_samples,
+                "Cannot resize input_gradients and accumulate into it." );
+    else
+    {
+        input_gradients.resize(inputs.length(), input_size);
+        input_gradients.fill(0);
+    }
+    bool bprop_wait_new_bag = true;
+    int bag_start;
+    for (int j = 0; j < inputs.length(); j++) {
+        int bag_info = int(round(bagtargets(j).lastElement()));
+        if (bag_info & SumOverBagsVariable::TARGET_COLUMN_FIRST)
+        {
+            PLASSERT( bprop_wait_new_bag );
+            bprop_wait_new_bag = false;
+            bag_start = j;
+        }
+        else
+            PLASSERT( !bprop_wait_new_bag );
+        if (bag_info & SumOverBagsVariable::TARGET_COLUMN_LAST)
+        {
+            PLASSERT( !is_missing(output_gradients(j,0)) );
+            bprop_wait_new_bag = true;
+            int bag_length = j - bag_start + 1;
+            Mat baginputs_gradients;
+            bprop( inputs.subMatRows(bag_start, bag_length),
+                   output_gradients(j),
+                   baginputs_gradients );
+            input_gradients.subMatRows(bag_start, bag_length) << baginputs_gradients;
+        }
+        else
+            PLASSERT( is_missing(output_gradients(j,0)) );
+    }
+
+    if (!bprop_wait_new_bag)
+        PLERROR("In OnBagsModule::bpropUpdate - entire bags must be given."
+                "If you use ModuleLearner, you should use the option operate_on_bags = True");
+}
+
+// This function computes the inputs gradients,
+// given all inputs from a same bag,
+//   and the gradient on the (single) output of the bag.
+void OnBagsModule::bprop( const Mat& baginputs,
+                          const Vec& bagoutput_gradient,
+                          Mat& baginputs_gradients)
+{
+    PLERROR("In OnBagsModule::bprop() - not implemented for class %s",classname().c_str() );
+}
+
+
+//////////
+// name //
+//////////
+TVec<string> OnBagsModule::name()
+{
+    return TVec<string>(1, OnlineLearningModule::name);
+}
+
+/////////////////
+// addPortName //
+/////////////////
+void OnBagsModule::addPortName(const string& name)
+{
+    PLASSERT( portname_to_index.find(name) == portname_to_index.end() );
+    portname_to_index[name] = ports.length();
+    ports.append(name);
+}
+
+//////////////
+// getPorts //
+//////////////
+const TVec<string>& OnBagsModule::getPorts()
+{
+    return ports;
+}
+
+///////////////////
+// getPortsSizes //
+///////////////////
+const TMat<int>& OnBagsModule::getPortSizes()
+{
+    return port_sizes;
+}
+
+//////////////////
+// getPortIndex //
+//////////////////
+int OnBagsModule::getPortIndex(const string& port)
+{
+    map<string, int>::const_iterator it = portname_to_index.find(port);
+    if (it == portname_to_index.end())
+        return -1;
+    else
+        return it->second;
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


Property changes on: trunk/plearn_learners/online/OnBagsModule.cc
___________________________________________________________________
Name: svn:executable
   + *

Added: trunk/plearn_learners/online/OnBagsModule.h
===================================================================
--- trunk/plearn_learners/online/OnBagsModule.h	2008-04-10 15:40:05 UTC (rev 8792)
+++ trunk/plearn_learners/online/OnBagsModule.h	2008-04-10 15:55:48 UTC (rev 8793)
@@ -0,0 +1,172 @@
+// -*- C++ -*-
+
+// OnBagsModule.h
+//
+// Copyright (C) 2007 Jerome Louradour
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Jerome Louradour
+
+/*! \file OnBagsModule.h */
+
+#ifndef OnBagsModule_INC
+#define OnBagsModule_INC
+
+#include <plearn_learners/online/OnlineLearningModule.h>
+
+#include <map>
+
+namespace PLearn {
+
+class OnBagsModule : public OnlineLearningModule
+{
+    typedef OnlineLearningModule inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    int bagtarget_size;
+
+    //#####  Public Learnt Options  ###########################################
+
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    OnBagsModule();
+
+    //!### Main functions to be coded in subclasses ###
+    
+    //! update internal statistics needed to compute the output of bag
+    virtual void fpropAcc(const Vec& input);
+    virtual void fpropInit(const Vec& input);
+    virtual void fpropOutput(Vec& output);
+
+    //! compute the gradient w.r.t bag inputs
+    virtual void bprop( const Mat& baginputs,
+                          const Vec& bagoutput_gradient,
+                          Mat& baginputs_gradients);
+                          
+   //!#################################################
+
+    //! given the input and target, compute the cost
+    virtual void fprop(const TVec<Mat*>& ports_value);
+    virtual void fprop(const Mat& inputs, const Mat& targets, Mat& outputs);
+    virtual void fprop(const Vec& input, const Vec& target, Vec& output);
+
+    //! backpropagate the derivative w.r.t. activation
+    virtual void bpropUpdate(const Mat& inputs, const Mat& targets,
+                             Mat& input_gradients, const Mat& output_gradients,
+                             bool accumulate=false);
+    virtual void bpropAccUpdate(const TVec<Mat*>& ports_value,
+                                const TVec<Mat*>& ports_gradient);
+
+    //! Returns all ports in the module.
+    virtual const TVec<string>& getPorts();
+
+    //! Return all ports sizes.
+    virtual const TMat<int>& getPortSizes();
+
+    //! Return the index (as in the list of ports returned by getPorts()) of
+    //! a given port.
+    //! If 'port' does not exist, -1 is returned.
+    virtual int getPortIndex(const string& port);
+
+    //! Overridden to do nothing (in particular, no warning).
+    virtual void setLearningRate(real dynamic_learning_rate) {}
+
+    //! Indicates the name of the computed costs
+    virtual TVec<string> name();
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(OnBagsModule);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+    virtual void forget();
+
+protected:
+    
+    bool wait_new_bag;
+
+    //! Map from a port name to its index in the 'ports' vector.
+    map<string, int> portname_to_index;
+
+    //! List of port names.
+    TVec<string> ports;
+
+    //#####  Protected Member Functions  ######################################
+
+    //! Add a new port to the 'portname_to_index' map and 'ports' vector.
+    void addPortName(const string& name);
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(OnBagsModule);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


Property changes on: trunk/plearn_learners/online/OnBagsModule.h
___________________________________________________________________
Name: svn:executable
   + *



From saintmlx at mail.berlios.de  Thu Apr 10 19:17:52 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 10 Apr 2008 19:17:52 +0200
Subject: [Plearn-commits] r8794 - trunk/python_modules/plearn/pybridge
Message-ID: <200804101717.m3AHHqnL012087@sheep.berlios.de>

Author: saintmlx
Date: 2008-04-10 19:17:51 +0200 (Thu, 10 Apr 2008)
New Revision: 8794

Modified:
   trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
Log:
- warn when setting a non-PLearn option on a wrapped PLearn object



Modified: trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
===================================================================
--- trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2008-04-10 15:55:48 UTC (rev 8793)
+++ trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2008-04-10 17:17:51 UTC (rev 8794)
@@ -31,11 +31,17 @@
 #  This file is part of the PLearn library. For more information on the PLearn
 #  library, go to the PLearn Web site at www.plearn.org
 
+
+import warnings
+
 global plearn_module
 plearn_module= None
 
 class WrappedPLearnObject(object):
 
+    allowed_non_PLearn_options= ['_cptr']
+    warn_non_PLearn_options= True
+
     def __init__(self, **kwargs):
         #print 'WrappedPLearnObject.__init__',type(self),kwargs
         if '_cptr' in kwargs:
@@ -61,6 +67,10 @@
         if attr != '_optionnames' and attr in self._optionnames:
             self.setOptionFromPython(attr, val)
         else:
+            if self.warn_non_PLearn_options \
+                    and attr not in self.allowed_non_PLearn_options:
+                warnings.warn("This is not a PLearn option: '"+attr
+                              +"' (for class "+self.__class__.__name__+")")
             object.__setattr__(self, attr, val)
 
     def __getattr__(self, attr):



From tihocan at mail.berlios.de  Fri Apr 11 15:41:51 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 11 Apr 2008 15:41:51 +0200
Subject: [Plearn-commits] r8795 - trunk/plearn/vmat
Message-ID: <200804111341.m3BDfpiB014952@sheep.berlios.de>

Author: tihocan
Date: 2008-04-11 15:41:50 +0200 (Fri, 11 Apr 2008)
New Revision: 8795

Modified:
   trunk/plearn/vmat/FractionSplitter.cc
   trunk/plearn/vmat/FractionSplitter.h
Log:
Added an advanced option to allow the use of '1' as an absolute position rather than a fraction

Modified: trunk/plearn/vmat/FractionSplitter.cc
===================================================================
--- trunk/plearn/vmat/FractionSplitter.cc	2008-04-10 17:17:51 UTC (rev 8794)
+++ trunk/plearn/vmat/FractionSplitter.cc	2008-04-11 13:41:50 UTC (rev 8795)
@@ -42,17 +42,22 @@
 namespace PLearn {
 using namespace std;
 
-FractionSplitter::FractionSplitter()
-    : Splitter(),
-      round_to_closest(0)
+//////////////////////
+// FractionSplitter //
+//////////////////////
+FractionSplitter::FractionSplitter():
+    one_is_absolute(false),
+    round_to_closest(0)
 {}
 
-
 PLEARN_IMPLEMENT_OBJECT(FractionSplitter,
                         "A Splitter that can extract several subparts of a dataset in each split.",
                         "Ranges of the dataset are specified explicitly as start:end positions,\n"
                         "that can be absolute or relative to the number of samples in the training set.");
 
+////////////////////
+// declareOptions //
+////////////////////
 void FractionSplitter::declareOptions(OptionList& ol)
 {
 
@@ -72,15 +77,27 @@
                   "Ex: 1 2 [ 0:0.80, 0.80:1 ]  yields a single split with the first part being the first 80% \n"
                   "of the data, and the second the next 20% \n");
 
+    declareOption(ol, "one_is_absolute",
+                  &FractionSplitter::one_is_absolute, OptionBase::buildoption,
+        "If true, then 1 is always considered as an absolute index, not as a\n"
+        "fraction giving the end of the dataset. This can be useful if you\n"
+        "actually want a split with a single element in it.",
+        OptionBase::advanced_level);
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
 
+////////////
+// build_ //
+////////////
 void FractionSplitter::build_()
 {
 }
 
-// ### Nothing to add here, simply calls build_
+///////////
+// build //
+///////////
 void FractionSplitter::build()
 {
     inherited::build();
@@ -117,8 +134,8 @@
         real fstart = frac_k[i].first;
         real fend = frac_k[i].second;
 
-        if(fstart>1) // absolute position
-            start = int(fstart);
+        if(fstart>1 || (one_is_absolute && is_equal(fstart, 1))) // absolute position
+            start = int(round(fstart));
         else {// relative position
             if (round_to_closest) {
                 start = int(fstart*l + 0.5);
@@ -127,8 +144,8 @@
             }
         }
 
-        if(fend>1) // absolute end position
-            end = int(fend);
+        if(fend>1 || (one_is_absolute && is_equal(fend, 1))) // absolute end position
+            end = int(round(fend));
         else if(is_equal(fend,1)) // until last element inclusive
             end = l;
         else {// relative end position

Modified: trunk/plearn/vmat/FractionSplitter.h
===================================================================
--- trunk/plearn/vmat/FractionSplitter.h	2008-04-10 17:17:51 UTC (rev 8794)
+++ trunk/plearn/vmat/FractionSplitter.h	2008-04-11 13:41:50 UTC (rev 8795)
@@ -67,6 +67,7 @@
     // * public build options *
     // ************************
 
+    bool one_is_absolute;
     bool round_to_closest;
     TMat< pair<real, real> > splits;
 



From tihocan at mail.berlios.de  Fri Apr 11 15:43:10 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 11 Apr 2008 15:43:10 +0200
Subject: [Plearn-commits] r8796 - trunk/plearn/vmat
Message-ID: <200804111343.m3BDhACK015031@sheep.berlios.de>

Author: tihocan
Date: 2008-04-11 15:43:09 +0200 (Fri, 11 Apr 2008)
New Revision: 8796

Modified:
   trunk/plearn/vmat/ToBagSplitter.cc
   trunk/plearn/vmat/ToBagSplitter.h
Log:
Added an option to remove the bag column

Modified: trunk/plearn/vmat/ToBagSplitter.cc
===================================================================
--- trunk/plearn/vmat/ToBagSplitter.cc	2008-04-11 13:41:50 UTC (rev 8795)
+++ trunk/plearn/vmat/ToBagSplitter.cc	2008-04-11 13:43:09 UTC (rev 8796)
@@ -41,9 +41,10 @@
 /*! \file ToBagSplitter.cc */
 
 
-#include "SelectRowsVMatrix.h"
+#include "ToBagSplitter.h"
 #include <plearn/var/SumOverBagsVariable.h>  //!< For SumOverBagsVariable::TARGET_COLUMN_LAST.
-#include "ToBagSplitter.h"
+#include <plearn/vmat/SelectColumnsVMatrix.h>
+#include <plearn/vmat/SelectRowsVMatrix.h>
 
 namespace PLearn {
 using namespace std;
@@ -51,27 +52,35 @@
 ///////////////////
 // ToBagSplitter //
 ///////////////////
-ToBagSplitter::ToBagSplitter()
-    : Splitter(),
-      expected_size_of_bag(10)
+ToBagSplitter::ToBagSplitter():
+    expected_size_of_bag(10),
+    remove_bag(false)
 {}
 
 PLEARN_IMPLEMENT_OBJECT(ToBagSplitter,
-                        "A Splitter that makes any existing splitter operate on bags only.",
-                        "The dataset provided must contain bag information, as described in\n"
-                        "SumOverBagsVariable");
+        "A Splitter that makes any existing splitter operate on bags only.",
+        "The dataset provided must contain bag information, as described in\n"
+        "SumOverBagsVariable."
+);
 
 ////////////////////
 // declareOptions //
 ////////////////////
 void ToBagSplitter::declareOptions(OptionList& ol)
 {
-    declareOption(ol, "expected_size_of_bag", &ToBagSplitter::expected_size_of_bag, OptionBase::buildoption,
-                  "The expected size of each bag. It is not compulsory to change this option.");
+    declareOption(ol, "expected_size_of_bag",
+                  &ToBagSplitter::expected_size_of_bag,
+                  OptionBase::buildoption,
+        "The expected size of each bag (optional).");
 
-    declareOption(ol, "sub_splitter", &ToBagSplitter::sub_splitter, OptionBase::buildoption,
-                  "The underlying splitter we want to make operate on bags.");
+    declareOption(ol, "sub_splitter",
+                  &ToBagSplitter::sub_splitter, OptionBase::buildoption,
+        "The underlying splitter we want to make operate on bags.");
 
+    declareOption(ol, "remove_bag",
+                  &ToBagSplitter::remove_bag, OptionBase::buildoption,
+        "If true, then the bag column will be removed from the data splits.");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -127,7 +136,7 @@
         // Resize to the minimum size needed.
         bags_store.resize(num_bag, max_ninstances + 1, 0, true);
         bags_index = VMat(bags_store);
-        bags_index->savePMAT("HOME:tmp/bid.pmat");
+        //bags_index->savePMAT("HOME:tmp/bid.pmat");
         // Provide this index to the sub_splitter.
         sub_splitter->setDataSet(bags_index);
     }
@@ -138,7 +147,6 @@
 //////////////
 TVec<VMat> ToBagSplitter::getSplit(int k)
 {
-    // ### Build and return the kth split
     TVec<VMat> sub_splits = sub_splitter->getSplit(k);
     TVec<VMat> result;
     for (int i = 0; i < sub_splits.length(); i++) {
@@ -152,7 +160,23 @@
                 indices_int.append(indice);
             }
         }
-        result.append(new SelectRowsVMatrix(dataset, indices_int));
+        VMat selected_rows = new SelectRowsVMatrix(dataset, indices_int);
+        if (remove_bag) {
+            // Remove the bag column.
+            int bag_column = selected_rows->inputsize() +
+                             selected_rows->targetsize() - 1;
+            TVec<int> removed_col(1, bag_column);
+            PP<SelectColumnsVMatrix> new_sel =
+                new SelectColumnsVMatrix(selected_rows, removed_col, false);
+            new_sel->inverse_fields_selection = true;
+            new_sel->defineSizes(selected_rows->inputsize(),
+                                 selected_rows->targetsize() - 1,
+                                 selected_rows->weightsize(),
+                                 selected_rows->extrasize());
+            new_sel->build();
+            selected_rows = get_pointer(new_sel);
+        }
+        result.append(selected_rows);
     }
     return result;
 }

Modified: trunk/plearn/vmat/ToBagSplitter.h
===================================================================
--- trunk/plearn/vmat/ToBagSplitter.h	2008-04-11 13:41:50 UTC (rev 8795)
+++ trunk/plearn/vmat/ToBagSplitter.h	2008-04-11 13:43:09 UTC (rev 8796)
@@ -52,7 +52,7 @@
 class ToBagSplitter: public Splitter
 {
 
-public:
+private:
 
     typedef Splitter inherited;
 
@@ -78,6 +78,7 @@
     // ************************
 
     int expected_size_of_bag;
+    bool remove_bag;
     PP<Splitter> sub_splitter;
 
     // ****************



From tihocan at mail.berlios.de  Fri Apr 11 15:44:58 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 11 Apr 2008 15:44:58 +0200
Subject: [Plearn-commits] r8797 - trunk/plearn/vmat
Message-ID: <200804111344.m3BDiwxt015161@sheep.berlios.de>

Author: tihocan
Date: 2008-04-11 15:44:58 +0200 (Fri, 11 Apr 2008)
New Revision: 8797

Modified:
   trunk/plearn/vmat/SelectColumnsVMatrix.cc
   trunk/plearn/vmat/SelectColumnsVMatrix.h
Log:
Fixed bug when reloading a saved SelectColumnsVMatrix with the inverse_fields_selection set to true

Modified: trunk/plearn/vmat/SelectColumnsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-04-11 13:43:09 UTC (rev 8796)
+++ trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-04-11 13:44:58 UTC (rev 8797)
@@ -112,7 +112,7 @@
                 i,j,width_,length_);
 #endif
     static int col;
-    col = indices[j];
+    col = sel_indices[j];
     if (col == -1)
         return MISSING_VALUE;
     return source->get(i, col);
@@ -131,7 +131,7 @@
 #endif
     static int col;
     for(int jj=0; jj<v.length(); jj++) {
-        col = indices[j+jj];
+        col = sel_indices[j+jj];
         if (col == -1)
             v[jj] = MISSING_VALUE;
         else
@@ -157,10 +157,12 @@
     declareOption(ol, "extend_with_missing", &SelectColumnsVMatrix::extend_with_missing, OptionBase::buildoption,
                   "If set to 1, then fields specified in the 'fields' option that do not exist\n"
                   "in the source VMatrix will be filled with missing values.");
-    declareOption(ol, "inverse_fields_selection", &SelectColumnsVMatrix::inverse_fields_selection,
+
+    declareOption(ol, "inverse_fields_selection",
+                  &SelectColumnsVMatrix::inverse_fields_selection,
                   OptionBase::buildoption,
-                  "If set to true, after all previous fields selection, we inverse the selection.\n"
-                  "This way we can specify the indicies we don't want.");
+        "If set to true, the selection is reversed. This provides an easy\n"
+        "way to specify columns we do not want.");
 
     inherited::declareOptions(ol);
 }
@@ -174,6 +176,7 @@
     deepCopyField(sinput, copies);
     deepCopyField(indices, copies);
     deepCopyField(fields, copies);
+    deepCopyField(sel_indices, copies);
 }
 
 ///////////
@@ -261,10 +264,12 @@
                 if(!found)
                     newindices.append(i);
             }
-            indices = newindices;
-        }
+            sel_indices = newindices;
+        } else
+            sel_indices = indices;
+
         // Copy matrix dimensions
-        width_ = indices.length();
+        width_ = sel_indices.length();
         length_ = source->length();
 
         if(!extend_with_missing)
@@ -299,7 +304,7 @@
         fieldinfos.resize(width());
         if (source->getFieldInfos().size() > 0) {
             for (int i=0; i<width(); ++i) {
-                int col = indices[i];
+                int col = sel_indices[i];
                 if (col == -1) {
                     if (extend_with_missing)
                         // This must be because it is a field that did not exist in
@@ -324,7 +329,7 @@
 const map<string,real>& SelectColumnsVMatrix::getStringToRealMapping(int col) const {
     static int the_col;
     static map<string, real> empty_mapping;
-    the_col = indices[col];
+    the_col = sel_indices[col];
     if (the_col == -1)
         return empty_mapping;
     return source->getStringToRealMapping(the_col);
@@ -335,7 +340,7 @@
 //////////////////
 real SelectColumnsVMatrix::getStringVal(int col, const string & str) const {
     static int the_col;
-    the_col = indices[col];
+    the_col = sel_indices[col];
     if (the_col == -1)
         return MISSING_VALUE;
     return source->getStringVal(the_col, str);
@@ -347,7 +352,7 @@
 const map<real,string>& SelectColumnsVMatrix::getRealToStringMapping(int col) const {
     static int the_col;
     static map<real, string> empty_mapping;
-    the_col = indices[col];
+    the_col = sel_indices[col];
     if (the_col == -1)
         return empty_mapping;
     return source->getRealToStringMapping(the_col);
@@ -358,7 +363,7 @@
 //////////////////
 string SelectColumnsVMatrix::getValString(int col, real val) const {
     static int the_col;
-    the_col = indices[col];
+    the_col = sel_indices[col];
     if (the_col == -1)
         return "";
     return source->getValString(the_col, val);
@@ -369,7 +374,7 @@
     if(col>=width_)
         PLERROR("access out of bound. Width=%i accessed col=%i",width_,col);
     static int the_col;
-    the_col = indices[col];
+    the_col = sel_indices[col];
     if (the_col == -1)
         return 0;
     return source->getDictionary(the_col);
@@ -381,7 +386,7 @@
     if(col>=width_)
         PLERROR("access out of bound. Width=%i accessed col=%i",width_,col);
     static int the_col;
-    the_col = indices[col];
+    the_col = sel_indices[col];
     if (the_col == -1)
         values.resize(0);
     else
@@ -393,13 +398,13 @@
     if(col>=width_)
         PLERROR("access out of bound. Width=%i accessed col=%i",width_,col);
     static int the_col;
-    the_col = indices[col];
+    the_col = sel_indices[col];
     if (the_col == -1)
         values.resize(0);
     else
     {
-        for(int i=0; i<indices.length(); i++)
-            sinput[indices[i]] = input[i];
+        for(int i=0; i<sel_indices.length(); i++)
+            sinput[sel_indices[i]] = input[i];
         source->getValues(sinput, the_col, values);
     }
 }

Modified: trunk/plearn/vmat/SelectColumnsVMatrix.h
===================================================================
--- trunk/plearn/vmat/SelectColumnsVMatrix.h	2008-04-11 13:43:09 UTC (rev 8796)
+++ trunk/plearn/vmat/SelectColumnsVMatrix.h	2008-04-11 13:44:58 UTC (rev 8797)
@@ -99,6 +99,9 @@
 
 protected:
 
+    //! Final column indices to be selected.
+    TVec<int> sel_indices;
+
     static void declareOptions(OptionList &ol);
     void getNewRow(int i, const Vec& v) const { getSubRow(i, 0, v); }
 



From tihocan at mail.berlios.de  Fri Apr 11 17:17:26 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 11 Apr 2008 17:17:26 +0200
Subject: [Plearn-commits] r8798 - trunk/plearn/python
Message-ID: <200804111517.m3BFHQsM026028@sheep.berlios.de>

Author: tihocan
Date: 2008-04-11 17:17:26 +0200 (Fri, 11 Apr 2008)
New Revision: 8798

Modified:
   trunk/plearn/python/PythonProcessedVMatrix.cc
   trunk/plearn/python/PythonProcessedVMatrix.h
Log:
Can now give the code in a file rather than a string

Modified: trunk/plearn/python/PythonProcessedVMatrix.cc
===================================================================
--- trunk/plearn/python/PythonProcessedVMatrix.cc	2008-04-11 13:44:58 UTC (rev 8797)
+++ trunk/plearn/python/PythonProcessedVMatrix.cc	2008-04-11 15:17:26 UTC (rev 8798)
@@ -42,6 +42,7 @@
 
 
 #include "PythonProcessedVMatrix.h"
+#include <plearn/io/fileutils.h>    // For loadFileAsString().
 
 namespace PLearn {
 using namespace std;
@@ -125,6 +126,11 @@
                   "after an initial build(), changing this string calling build() again\n"
                   "DOES NOT result in the recompilation of the code.\n");
 
+    declareOption(ol, "code_path", &PythonProcessedVMatrix::code_path,
+                  OptionBase::buildoption,
+        "If set, the 'code' option will be ignored (it should be left empty)\n"
+        "and the code will be loaded from the given file.");
+
     declareOption(ol, "params", &PythonProcessedVMatrix::m_params,
                   OptionBase::buildoption,
                   "General-purpose parameters that are injected into the Python code\n"
@@ -289,7 +295,15 @@
 void PythonProcessedVMatrix::compileAndInject()
 {
     if (! python) {
-        python = new PythonCodeSnippet(m_code);
+        string the_code = m_code;
+        if (!code_path.isEmpty()) {
+            // Load code from file.
+            the_code = loadFileAsString(code_path);
+            PLCHECK_MSG( m_code.empty(),
+                         "When using 'code_path' the 'code' option should "
+                         "not be used" );
+        }
+        python = new PythonCodeSnippet(the_code);
         PLASSERT( python );
         python->build();
         python->inject("getSourceRow", this, &PythonProcessedVMatrix::getSourceRow);

Modified: trunk/plearn/python/PythonProcessedVMatrix.h
===================================================================
--- trunk/plearn/python/PythonProcessedVMatrix.h	2008-04-11 13:44:58 UTC (rev 8797)
+++ trunk/plearn/python/PythonProcessedVMatrix.h	2008-04-11 15:17:26 UTC (rev 8798)
@@ -128,6 +128,8 @@
      */
     string m_code;
 
+    PPath code_path;
+
     /**
      *  General-purpose parameters that are injected into the Python code
      *  snippet and accessible via the getParam/setParam functions.  Can be



From tihocan at mail.berlios.de  Fri Apr 11 17:35:52 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 11 Apr 2008 17:35:52 +0200
Subject: [Plearn-commits] r8799 - trunk/plearn/python
Message-ID: <200804111535.m3BFZqoo027773@sheep.berlios.de>

Author: tihocan
Date: 2008-04-11 17:35:52 +0200 (Fri, 11 Apr 2008)
New Revision: 8799

Modified:
   trunk/plearn/python/PythonProcessedVMatrix.cc
   trunk/plearn/python/PythonProcessedVMatrix.h
Log:
Reverted to previous version, one can just use a code of the form 'from myfile import *' to load code from a file

Modified: trunk/plearn/python/PythonProcessedVMatrix.cc
===================================================================
--- trunk/plearn/python/PythonProcessedVMatrix.cc	2008-04-11 15:17:26 UTC (rev 8798)
+++ trunk/plearn/python/PythonProcessedVMatrix.cc	2008-04-11 15:35:52 UTC (rev 8799)
@@ -42,7 +42,6 @@
 
 
 #include "PythonProcessedVMatrix.h"
-#include <plearn/io/fileutils.h>    // For loadFileAsString().
 
 namespace PLearn {
 using namespace std;
@@ -126,11 +125,6 @@
                   "after an initial build(), changing this string calling build() again\n"
                   "DOES NOT result in the recompilation of the code.\n");
 
-    declareOption(ol, "code_path", &PythonProcessedVMatrix::code_path,
-                  OptionBase::buildoption,
-        "If set, the 'code' option will be ignored (it should be left empty)\n"
-        "and the code will be loaded from the given file.");
-
     declareOption(ol, "params", &PythonProcessedVMatrix::m_params,
                   OptionBase::buildoption,
                   "General-purpose parameters that are injected into the Python code\n"
@@ -295,15 +289,7 @@
 void PythonProcessedVMatrix::compileAndInject()
 {
     if (! python) {
-        string the_code = m_code;
-        if (!code_path.isEmpty()) {
-            // Load code from file.
-            the_code = loadFileAsString(code_path);
-            PLCHECK_MSG( m_code.empty(),
-                         "When using 'code_path' the 'code' option should "
-                         "not be used" );
-        }
-        python = new PythonCodeSnippet(the_code);
+        python = new PythonCodeSnippet(m_code);
         PLASSERT( python );
         python->build();
         python->inject("getSourceRow", this, &PythonProcessedVMatrix::getSourceRow);

Modified: trunk/plearn/python/PythonProcessedVMatrix.h
===================================================================
--- trunk/plearn/python/PythonProcessedVMatrix.h	2008-04-11 15:17:26 UTC (rev 8798)
+++ trunk/plearn/python/PythonProcessedVMatrix.h	2008-04-11 15:35:52 UTC (rev 8799)
@@ -128,8 +128,6 @@
      */
     string m_code;
 
-    PPath code_path;
-
     /**
      *  General-purpose parameters that are injected into the Python code
      *  snippet and accessible via the getParam/setParam functions.  Can be



From tihocan at mail.berlios.de  Fri Apr 11 21:13:13 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 11 Apr 2008 21:13:13 +0200
Subject: [Plearn-commits] r8800 - in trunk: commands plearn/vmat
Message-ID: <200804111913.m3BJDD69007268@sheep.berlios.de>

Author: tihocan
Date: 2008-04-11 21:13:12 +0200 (Fri, 11 Apr 2008)
New Revision: 8800

Added:
   trunk/plearn/vmat/SelectSetsSplitter.cc
   trunk/plearn/vmat/SelectSetsSplitter.h
Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
New splitter that can be used to switch around the sets of another splitter

Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2008-04-11 15:35:52 UTC (rev 8799)
+++ trunk/commands/plearn_noblas_inc.h	2008-04-11 19:13:12 UTC (rev 8800)
@@ -265,6 +265,7 @@
 #include <plearn/vmat/NoSplitSplitter.h>
 #include <plearn/vmat/MultiTaskSeparationSplitter.h>
 #include <plearn/vmat/RepeatSplitter.h>
+#include <plearn/vmat/SelectSetsSplitter.h>
 #include <plearn/vmat/SourceVMatrixSplitter.h>
 #include <plearn/vmat/StackedSplitter.h>
 #include <plearn/vmat/TestInTrainSplitter.h>

Added: trunk/plearn/vmat/SelectSetsSplitter.cc
===================================================================
--- trunk/plearn/vmat/SelectSetsSplitter.cc	2008-04-11 15:35:52 UTC (rev 8799)
+++ trunk/plearn/vmat/SelectSetsSplitter.cc	2008-04-11 19:13:12 UTC (rev 8800)
@@ -0,0 +1,143 @@
+// -*- C++ -*-
+
+// SelectSetsSplitter.cc
+//
+// Copyright (C) 2008 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file SelectSetsSplitter.cc */
+
+
+#include "SelectSetsSplitter.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    SelectSetsSplitter,
+    "Selects a subset of sets from an underlying splitter.",
+    ""
+);
+
+SelectSetsSplitter::SelectSetsSplitter()
+{}
+
+////////////////////
+// declareOptions //
+////////////////////
+void SelectSetsSplitter::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "splitter", &SelectSetsSplitter::splitter,
+                  OptionBase::buildoption,
+        "The underlying splitter.");
+
+    declareOption(ol, "indices", &SelectSetsSplitter::indices,
+                  OptionBase::buildoption,
+        "Indices of sets being selected.");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+////////////
+// build_ //
+////////////
+void SelectSetsSplitter::build_()
+{}
+
+void SelectSetsSplitter::build()
+{
+    inherited::build();
+    build_();
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void SelectSetsSplitter::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(indices,  copies);
+    deepCopyField(splitter, copies);
+}
+
+/////////////
+// nsplits //
+/////////////
+int SelectSetsSplitter::nsplits() const
+{
+    return splitter->nsplits();
+}
+
+///////////////////
+// nSetsPerSplit //
+///////////////////
+int SelectSetsSplitter::nSetsPerSplit() const
+{
+    return indices.length();
+}
+
+//////////////
+// getSplit //
+//////////////
+TVec<VMat> SelectSetsSplitter::getSplit(int k)
+{
+    TVec<VMat> sub = splitter->getSplit(k);
+    TVec<VMat> result;
+    for (int i = 0; i < indices.length(); i++)
+        result.append(sub[indices[i]]);
+    return result;
+}
+
+////////////////
+// setDataSet //
+////////////////
+void SelectSetsSplitter::setDataSet(VMat the_dataset)
+{
+    splitter->setDataSet(the_dataset);
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/vmat/SelectSetsSplitter.h
===================================================================
--- trunk/plearn/vmat/SelectSetsSplitter.h	2008-04-11 15:35:52 UTC (rev 8799)
+++ trunk/plearn/vmat/SelectSetsSplitter.h	2008-04-11 19:13:12 UTC (rev 8800)
@@ -0,0 +1,144 @@
+// -*- C++ -*-
+
+// SelectSetsSplitter.h
+//
+// Copyright (C) 2008 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file SelectSetsSplitter.h */
+
+
+#ifndef SelectSetsSplitter_INC
+#define SelectSetsSplitter_INC
+
+#include <plearn/vmat/Splitter.h>
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class SelectSetsSplitter : public Splitter
+{
+    typedef Splitter inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+    
+    TVec<int> indices;
+    PP<Splitter> splitter;
+
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    SelectSetsSplitter();
+
+    //#####  Splitter Member Functions  #######################################
+
+    //! Returns the number of available different "splits"
+    virtual int nsplits() const;
+
+    //! Returns the number of sets per split
+    virtual int nSetsPerSplit() const;
+
+    //! Returns split number i
+    virtual TVec<VMat> getSplit(int i=0);
+
+    //! Overridden to forward to underlying splitter.
+    virtual void setDataSet(VMat the_dataset);
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(SelectSetsSplitter);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(SelectSetsSplitter);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From tihocan at mail.berlios.de  Fri Apr 11 21:13:48 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 11 Apr 2008 21:13:48 +0200
Subject: [Plearn-commits] r8801 - trunk/plearn/vmat
Message-ID: <200804111913.m3BJDmcl007332@sheep.berlios.de>

Author: tihocan
Date: 2008-04-11 21:13:48 +0200 (Fri, 11 Apr 2008)
New Revision: 8801

Modified:
   trunk/plearn/vmat/SortRowsVMatrix.cc
   trunk/plearn/vmat/SortRowsVMatrix.h
Log:
Added an option to avoid crashing when sorted fields are missing

Modified: trunk/plearn/vmat/SortRowsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SortRowsVMatrix.cc	2008-04-11 19:13:12 UTC (rev 8800)
+++ trunk/plearn/vmat/SortRowsVMatrix.cc	2008-04-11 19:13:48 UTC (rev 8801)
@@ -58,6 +58,7 @@
 // SortRowsVMatrix //
 /////////////////////
 SortRowsVMatrix::SortRowsVMatrix():
+    ignore_missing_fields(false),
     increasing_order(true)
 {}
 
@@ -78,6 +79,12 @@
         "first criterion). This option is optional and, if provided, the\n"
         "'sort_columns' option will be ignored.");
 
+    declareOption(ol, "ignore_missing_fields",
+                  &SortRowsVMatrix::ignore_missing_fields,
+                  OptionBase::buildoption,
+        "If true, then no error will be thrown when a column given in\n"
+        "sort_columns or sort_columns_by_name is missing.");
+
     declareOption(ol, "increasing_order", &SortRowsVMatrix::increasing_order, OptionBase::buildoption,
                   "    if set to 1, the data will be sorted in increasing order");
 
@@ -117,13 +124,23 @@
 {
     if (sort_columns_by_name.isNotEmpty() && source) {
         // Convert column names into column indices.
-        sort_columns.resize(sort_columns_by_name.length());
-        for (int i = 0; i < sort_columns_by_name.length(); i++)
-            sort_columns[i] = source->getFieldIndex(sort_columns_by_name[i]);
+        sort_columns.resize(0);
+        for (int i = 0; i < sort_columns_by_name.length(); i++) {
+            int idx = source->getFieldIndex(sort_columns_by_name[i],
+                                            !ignore_missing_fields);
+            if (idx >= 0)
+                sort_columns.append(i);
+        }
     }
     // Check we don't try to sort twice by the same column (this can be confusing).
+    // Also verify fields do exist, just in case.
     if (sort_columns.isNotEmpty()) {
         for (int i = 0; i < sort_columns.length(); i++) {
+            if (!ignore_missing_fields &&
+                    (sort_columns[i] < 0 || sort_columns[i] >= source->width()))
+                PLERROR("In SortRowsVMatrix::build_ - Field with index %d "
+                        "cannot exist in the source VMatrix, whose width is "
+                        "%d", sort_columns[i], source->width());
             for (int j = i + 1; j < sort_columns.length(); j++) {
                 if (sort_columns[j] == sort_columns[i]) {
                     PLERROR("In SortRowsVMatrix::build_ - You have a duplicated index in the 'sort_columns' vector");

Modified: trunk/plearn/vmat/SortRowsVMatrix.h
===================================================================
--- trunk/plearn/vmat/SortRowsVMatrix.h	2008-04-11 19:13:12 UTC (rev 8800)
+++ trunk/plearn/vmat/SortRowsVMatrix.h	2008-04-11 19:13:48 UTC (rev 8801)
@@ -59,6 +59,7 @@
 public:
 
     //! Public build options.
+    bool ignore_missing_fields;
     bool increasing_order;
     TVec<int> sort_columns;
     TVec<string> sort_columns_by_name;



From tihocan at mail.berlios.de  Fri Apr 11 21:14:03 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 11 Apr 2008 21:14:03 +0200
Subject: [Plearn-commits] r8802 - trunk/scripts/Skeletons
Message-ID: <200804111914.m3BJE3JR007362@sheep.berlios.de>

Author: tihocan
Date: 2008-04-11 21:14:03 +0200 (Fri, 11 Apr 2008)
New Revision: 8802

Modified:
   trunk/scripts/Skeletons/Splitter.cc
Log:
Removed useless line in skeleton

Modified: trunk/scripts/Skeletons/Splitter.cc
===================================================================
--- trunk/scripts/Skeletons/Splitter.cc	2008-04-11 19:13:48 UTC (rev 8801)
+++ trunk/scripts/Skeletons/Splitter.cc	2008-04-11 19:14:03 UTC (rev 8802)
@@ -10,7 +10,6 @@
     );
 
 DERIVEDCLASS::DERIVEDCLASS()
-    :Splitter()
     /* ### Initialize all fields to their default value */
 {
     // ...



From tihocan at mail.berlios.de  Fri Apr 11 23:01:42 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 11 Apr 2008 23:01:42 +0200
Subject: [Plearn-commits] r8803 - trunk/plearn/vmat
Message-ID: <200804112101.m3BL1g0a020140@sheep.berlios.de>

Author: tihocan
Date: 2008-04-11 23:01:42 +0200 (Fri, 11 Apr 2008)
New Revision: 8803

Modified:
   trunk/plearn/vmat/ToBagSplitter.cc
Log:
Do not crash anymore if the expected bag size was not appropriate

Modified: trunk/plearn/vmat/ToBagSplitter.cc
===================================================================
--- trunk/plearn/vmat/ToBagSplitter.cc	2008-04-11 19:14:03 UTC (rev 8802)
+++ trunk/plearn/vmat/ToBagSplitter.cc	2008-04-11 21:01:42 UTC (rev 8803)
@@ -110,10 +110,7 @@
         int num_instance = 0;
         int bag_signal_column = dataset->inputsize() + dataset->targetsize() - 1; // Bag signal in the last target column.
         for (int i = 0; i < dataset->length(); i++) {
-            if (num_instance + 1 >= bags_store.width()) {
-                if (num_instance > 10*(expected_size_of_bag+1))
-                    PLERROR("ToBagSplitter: found bag size (%d) more than 10 times bigger than expected_size_of_bag (%d)!\n",
-                            num_instance,expected_size_of_bag);
+            while (num_instance + 1 >= bags_store.width()) {
                 // Need to resize bags_store.
                 bags_store.resize(bags_store.length(), bags_store.width() * 2, 0, true);
             }



From tihocan at mail.berlios.de  Sun Apr 13 11:38:53 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Sun, 13 Apr 2008 11:38:53 +0200
Subject: [Plearn-commits] r8804 - trunk/plearn/vmat
Message-ID: <200804130938.m3D9crcn003956@sheep.berlios.de>

Author: tihocan
Date: 2008-04-13 11:38:53 +0200 (Sun, 13 Apr 2008)
New Revision: 8804

Modified:
   trunk/plearn/vmat/SortRowsVMatrix.cc
Log:
Fixed stupid bug introduced in previous commit

Modified: trunk/plearn/vmat/SortRowsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SortRowsVMatrix.cc	2008-04-11 21:01:42 UTC (rev 8803)
+++ trunk/plearn/vmat/SortRowsVMatrix.cc	2008-04-13 09:38:53 UTC (rev 8804)
@@ -129,7 +129,7 @@
             int idx = source->getFieldIndex(sort_columns_by_name[i],
                                             !ignore_missing_fields);
             if (idx >= 0)
-                sort_columns.append(i);
+                sort_columns.append(idx);
         }
     }
     // Check we don't try to sort twice by the same column (this can be confusing).



From tihocan at mail.berlios.de  Sun Apr 13 14:17:21 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Sun, 13 Apr 2008 14:17:21 +0200
Subject: [Plearn-commits] r8805 - in trunk/plearn/vmat/test: . .pytest
	.pytest/PL_SortRowsVMatrix
	.pytest/PL_SortRowsVMatrix/expected_results
Message-ID: <200804131217.m3DCHLwo023660@sheep.berlios.de>

Author: tihocan
Date: 2008-04-13 14:17:20 +0200 (Sun, 13 Apr 2008)
New Revision: 8805

Added:
   trunk/plearn/vmat/test/.pytest/PL_SortRowsVMatrix/
   trunk/plearn/vmat/test/.pytest/PL_SortRowsVMatrix/expected_results/
   trunk/plearn/vmat/test/.pytest/PL_SortRowsVMatrix/expected_results/RUN.log
   trunk/plearn/vmat/test/sortrowsvmatrix.pymat
Modified:
   trunk/plearn/vmat/test/pytest.config
Log:
New test PL_SortRowsVMatrix


Property changes on: trunk/plearn/vmat/test/.pytest/PL_SortRowsVMatrix
___________________________________________________________________
Name: svn:ignore
   + .plearn
run_results
PSAVEDIFF


Added: trunk/plearn/vmat/test/.pytest/PL_SortRowsVMatrix/expected_results/RUN.log
===================================================================
--- trunk/plearn/vmat/test/.pytest/PL_SortRowsVMatrix/expected_results/RUN.log	2008-04-13 09:38:53 UTC (rev 8804)
+++ trunk/plearn/vmat/test/.pytest/PL_SortRowsVMatrix/expected_results/RUN.log	2008-04-13 12:17:20 UTC (rev 8805)
@@ -0,0 +1,10 @@
+-0.851439999999999975 -0.574479999999999991 -10.0020199999999999 -2.27916000000000007 -55.9754700000000014 -60.5778600000000012 
+-0.676509999999999945 -0.354930000000000023 -6.93189999999999973 -1.70795000000000008 -35.9233100000000007 -7.30156999999999989 
+1.51631999999999989 -0.513920000000000043 2.4423499999999998 2.51865000000000006 -29.3890199999999986 26.4317400000000013 
+-1.48755999999999999 -0.104679999999999995 -8.48466999999999949 -3.07901999999999987 -14.2792999999999992 -42.9353499999999997 
+0.957949999999999968 0.136770000000000003 6.15754000000000001 2.05163999999999991 24.4552699999999987 51.1811999999999969 
+0.897809999999999997 0.159090000000000009 6.08004000000000033 1.95443999999999996 27.7973999999999997 58.394919999999999 
+-1.11019000000000001 0.360729999999999995 -1.94358999999999993 -1.86051000000000011 34.1033899999999974 23.8113300000000017 
+-0.359319999999999973 0.575289999999999968 3.9563299999999999 -0.143020000000000008 56.4558600000000013 46.144599999999997 
+1.64172000000000007 0.531970000000000054 13.5284200000000006 3.81586999999999987 71.6844300000000061 89.1621899999999954 
+-0.236990000000000006 0.917590000000000017 7.99099999999999966 0.442350000000000021 96.8868700000000018 47.2869799999999998 

Modified: trunk/plearn/vmat/test/pytest.config
===================================================================
--- trunk/plearn/vmat/test/pytest.config	2008-04-13 09:38:53 UTC (rev 8804)
+++ trunk/plearn/vmat/test/pytest.config	2008-04-13 12:17:20 UTC (rev 8805)
@@ -5,39 +5,31 @@
     For each Test instance you declare in a config file, a test will be ran
     by PyTest.
     
-      @ivar(name):
-    The name of the Test must uniquely determine the
+    @ivar name: The name of the Test must uniquely determine the
     test. Among others, it will be used to identify the test's results
-    (.PyTest/name/*_results/) and to report test informations.
-      @type(name):
-    String
+    (.PyTest/I{name}/*_results/) and to report test informations.
+    @type name: String
     
-      @ivar(description):
-    The description must provide other users an
+    @ivar description: The description must provide other users an
     insight of what exactly is the Test testing. You are encouraged
     to used triple quoted strings for indented multi-lines
     descriptions.
-      @type(description):
-    String
+    @type description: String
     
-      @ivar(category):
-    The category to which this test belongs. By default, a
+    @ivar category: The category to which this test belongs. By default, a
     test is considered a 'General' test.
     
     It is not desirable to let an extensive and lengthy test as 'General',
     while one shall refrain abusive use of categories since it is likely
     that only 'General' tests will be ran before most commits...
     
-      @type(category):
-    string
+    @type category: string
     
-      @ivar(program):
-    The program to be run by the Test. The program's name
-    PRGNAME is used to lookup for the program in the following manner:
+    @ivar program: The program to be run by the Test. The program's name
+    PRGNAME is used to lookup for the program in the following manner::
     
     1) Look for a local program named PRGNAME
-    2) Look for a plearn-like command (plearn, plearn_tests, ...) named 
-PRGNAME
+    2) Look for a plearn-like command (plearn, plearn_tests, ...) named PRGNAME
     3) Call 'which PRGNAME'
     4) Fail
     
@@ -46,53 +38,59 @@
     "compiler = 'pymake'"). If no compiler is provided while the program is
     believed to be compilable, 'pymake' will be assigned by
     default. Arguments to be forwarded to the compiler can be provided as a
-    string through the 'compile_options' keyword argument. @type program:
-    Program
+    string through the 'compile_options' keyword argument.  @type program:
+    L{Program}
     
-      @ivar(arguments):
-    The command line arguments to be passed to the program
+    @ivar arguments: The command line arguments to be passed to the program
     for the test to proceed.
-      @type(arguments):
-    String
+    @type arguments: String
     
-      @ivar(resources):
-    A list of resources that are used by your program
+    @ivar resources: A list of resources that are used by your program
     either in the command line or directly in the code (plearn or pyplearn
-    files, databases, ...). The elements of the list must be string
+    files, databases, ...).  The elements of the list must be string
     representations of the path, absolute or relative, to the resource.
-      @type(resources):
-    List of Strings
+    @type resources: List of Strings
     
-      @ivar(precision):
-    The precision (absolute and relative) used when comparing
+    @ivar precision: The precision (absolute and relative) used when comparing
     floating numbers in the test output (default = 1e-6)
-      @type(precision):
-    float
+    @type precision: float
     
-      @ivar(pfileprg):
-    The program to be used for comparing files of psave &
-    vmat formats. It can be either:
-      - "__program__": maps to this test's program if its compilable;
+    @ivar pfileprg: The program to be used for comparing files of psave &
+    vmat formats. It can be either::
+    - "__program__": maps to this test's program if its compilable;
     maps to 'plearn_tests' otherwise (default);
-      - "__plearn__": always maps to 'plearn_tests' (for when the program
+    - "__plearn__": always maps to 'plearn_tests' (for when the program
     under test is not a version of PLearn);
-      - A Program (see 'program' option) instance
-      - None: if you are sure no files are to be compared.
+    - A Program (see 'program' option) instance
+    - None: if you are sure no files are to be compared.
     
-      @ivar(ignored_files_re):
-    Default behaviour of a test is to compare all
+    @ivar ignored_files_re: Default behaviour of a test is to compare all
     files created by running the test. In some case, one may prefer some of
     these files to be ignored.
-      @type(ignored_files_re):
-    list of regular expressions
+    @type ignored_files_re: list of regular expressions
     
-      @ivar(disabled):
-    If true, the test will not be ran.
-      @type(disabled):
-    bool
+    @ivar disabled: If true, the test will not be ran.
+    @type disabled: bool
     
 """
 Test(
+    name = "PL_SortRowsVMatrix",
+    description = "Test the sorting of rows through a SortRowsVMatrix",
+    category = "General",
+    program = Program(
+        name = "plearn_tests",
+        compiler = "pymake"
+        ),
+    arguments = "vmat cat sortrowsvmatrix.pymat",
+    resources = [ "sortrowsvmatrix.pymat" ],
+    precision = 1e-06,
+    pfileprg = "__program__",
+    disabled = False,
+    runtime = None,
+    difftime = None
+    )
+
+Test(
     name = "PL_SelectRowsVMatrix",
     description = "Test sizes of SelectRowsVMatrix",
     category = "General",
@@ -104,7 +102,9 @@
     resources = [ "selectrowsvmat.vmat" ],
     precision = 1e-06,
     pfileprg = "__program__",
-    disabled = False
+    disabled = False,
+    runtime = None,
+    difftime = None
     )
 
 Test(
@@ -119,7 +119,9 @@
     resources = [ "test_processingvmatrix_sizes.vmat" ],
     precision = 0,
     pfileprg = "__program__",
-    disabled = False
+    disabled = False,
+    runtime = None,
+    difftime = None
     )
 
 Test(
@@ -134,7 +136,9 @@
     resources = [ "test_processing_vmatrix.pymat" ],
     precision = 1e-06,
     pfileprg = "__program__",
-    disabled = False
+    disabled = False,
+    runtime = None,
+    difftime = None
     )
 
 Test(
@@ -149,7 +153,9 @@
     resources = [ "rowbufferedvmatrix_test.plearn" ],
     precision = 1e-06,
     pfileprg = "__program__",
-    disabled = False
+    disabled = False,
+    runtime = None,
+    difftime = None
     )
 
 Test(
@@ -164,7 +170,9 @@
     resources = [ ],
     precision = 1e-06,
     pfileprg = "__program__",
-    disabled = False
+    disabled = False,
+    runtime = None,
+    difftime = None
     )
 
 Test(
@@ -179,7 +187,9 @@
     resources = [ "filevmatrix_test.plearn" ],
     precision = 1e-06,
     pfileprg = "__program__",
-    disabled = False
+    disabled = False,
+    runtime = None,
+    difftime = None
     )
 
 Test(
@@ -194,7 +204,9 @@
     resources = [ "PL_indexed_vmatrix.plearn" ],
     precision = 1e-06,
     pfileprg = "__program__",
-    disabled = False
+    disabled = False,
+    runtime = None,
+    difftime = None
     )
 
 Test(
@@ -212,5 +224,7 @@
         ],
     precision = 1e-06,
     pfileprg = "__program__",
-    disabled = False
+    disabled = False,
+    runtime = None,
+    difftime = None
     )

Added: trunk/plearn/vmat/test/sortrowsvmatrix.pymat
===================================================================
--- trunk/plearn/vmat/test/sortrowsvmatrix.pymat	2008-04-13 09:38:53 UTC (rev 8804)
+++ trunk/plearn/vmat/test/sortrowsvmatrix.pymat	2008-04-13 12:17:20 UTC (rev 8805)
@@ -0,0 +1,13 @@
+from plearn.pyplearn import pl
+
+def main():
+    return pl.SortRowsVMatrix(
+            source = pl.SubVMatrix(
+                length = 10,
+                source = pl.AutoVMatrix(
+                    filename = 'PLEARNDIR:examples/data/test_suite/linear_4x_2y.amat',
+                    ),
+                ),
+            sort_columns_by_name = [ 'y1', 'x2', ],
+            )
+



From tihocan at mail.berlios.de  Sun Apr 13 14:34:38 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Sun, 13 Apr 2008 14:34:38 +0200
Subject: [Plearn-commits] r8806 - trunk/plearn/io
Message-ID: <200804131234.m3DCYcuC025054@sheep.berlios.de>

Author: tihocan
Date: 2008-04-13 14:34:38 +0200 (Sun, 13 Apr 2008)
New Revision: 8806

Modified:
   trunk/plearn/io/PPath.h
Log:
Indentation change only

Modified: trunk/plearn/io/PPath.h
===================================================================
--- trunk/plearn/io/PPath.h	2008-04-13 12:17:20 UTC (rev 8805)
+++ trunk/plearn/io/PPath.h	2008-04-13 12:34:38 UTC (rev 8806)
@@ -173,296 +173,296 @@
 PPath::home, PPath::getenv (with a default value!) and PPath::getcwd.
 */
 
-             class PPath: public string
-             {
-             public:
+class PPath: public string
+{
+public:
 
-                 static PPath home  ();
-                 static PPath getcwd();
-                 static PPath getenv(const string& var, const PPath& default_="")  ;
-                 
-                 /*!
-                  *  Add a new metaprotocol-to-metapath binding.
-                  *  Return 'true' iff the given metaprotocol was not already
-                  *  binded.
-                  *  If 'force' is set to true, the binding will be made even
-                  *  if the given metaprotocol was already binded. Otherwise,
-                  *  the existing metaprotocol will be preserved.
-                  */
-                 static bool addMetaprotocolBinding(const string& metaprotocol,
-                                                    const PPath& metapath,
-                                                    bool  force = true);
+    static PPath home  ();
+    static PPath getcwd();
+    static PPath getenv(const string& var, const PPath& default_="")  ;
+    
+    /*!
+     *  Add a new metaprotocol-to-metapath binding.
+     *  Return 'true' iff the given metaprotocol was not already
+     *  binded.
+     *  If 'force' is set to true, the binding will be made even
+     *  if the given metaprotocol was already binded. Otherwise,
+     *  the existing metaprotocol will be preserved.
+     */
+    static bool addMetaprotocolBinding(const string& metaprotocol,
+                                       const PPath& metapath,
+                                       bool  force = true);
 
-                 /*!
-                  *  Decide whether or not to display canonical paths in error
-                  *  messages. The default behavior is to display absolute
-                  *  paths, but canonical paths might sometimes be preferred
-                  *  (e.g. in testing, for cross-platform compatibility, or for
-                  *  debug purpose).
-                  */
-                 static void setCanonicalInErrors(bool canonical);
+    /*!
+     *  Decide whether or not to display canonical paths in error
+     *  messages. The default behavior is to display absolute
+     *  paths, but canonical paths might sometimes be preferred
+     *  (e.g. in testing, for cross-platform compatibility, or for
+     *  debug purpose).
+     */
+    static void setCanonicalInErrors(bool canonical);
 
-             protected:
+protected:
 
-                 /***********************
-                  *  protected methods  *
-                  **********************/
+    /***********************
+     *  protected methods  *
+     **********************/
 
-                 /*! OS dependent list of forbidden chars.
-                  *
-                  * POSIX SETTINGS: "\\"
-                  *  Even if the posix standard allows backslashes in file paths,
-                  *  PLearn users should never use those since PLearn aims at full and
-                  *  easy portability.
-                  *
-                  *  Other chars may be forbidden soon.
-                  *
-                  * DOS SETTINGS: ""
-                  *  None for now; coming soon.
-                  */
-                 static string forbidden_chars();
+    /*! OS dependent list of forbidden chars.
+     *
+     * POSIX SETTINGS: "\\"
+     *  Even if the posix standard allows backslashes in file paths,
+     *  PLearn users should never use those since PLearn aims at full and
+     *  easy portability.
+     *
+     *  Other chars may be forbidden soon.
+     *
+     * DOS SETTINGS: ""
+     *  None for now; coming soon.
+     */
+    static string forbidden_chars();
 
-                 /*!
-                   Builds a static metaprotocol-to-metapath map once and returns it at
-                   each call.
-                 */
-                 static  const map<string, PPath>&  metaprotocolToMetapath();
+    /*!
+      Builds a static metaprotocol-to-metapath map once and returns it at
+      each call.
+    */
+    static  const map<string, PPath>&  metaprotocolToMetapath();
 
-                 /*!
-                   Within PPath(const string& path_) ctor, the path_ argument may
-                   contain environment variables that must be expanded prior to any other
-                   processing. This method MUST NOT return a path since it would lead to
-                   an infinite loop of ctors.
-                 */
-                 static string expandEnvVariables(const string& path);
+    /*!
+      Within PPath(const string& path_) ctor, the path_ argument may
+      contain environment variables that must be expanded prior to any other
+      processing. This method MUST NOT return a path since it would lead to
+      an infinite loop of ctors.
+    */
+    static string expandEnvVariables(const string& path);
 
-                 /*!
-                   Replace any slash character (canonical '/' or '_slash_char') by the
-                   '_slash_char' character, removing duplicated slashes.
-                 */
-                 void resolveSlashChars   ( );
-  
-                 //! Remove extra dots from the path.
-                 void resolveDots         ( );
+    /*!
+      Replace any slash character (canonical '/' or '_slash_char') by the
+      '_slash_char' character, removing duplicated slashes.
+    */
+    void resolveSlashChars   ( );
 
-                 //! Remove only extra single dots from the path. Assume there is no explicit
-                 //! protocol in the path.
-                 void resolveSingleDots   ( );
+    //! Remove extra dots from the path.
+    void resolveDots         ( );
 
-                 //! Remove only extra double dots from the path (assume there are no extra
-                 //! single dots, i.e. in general that resolveSingleDots() was called first).
-                 //! Also assume there is no explicit protocol in the path.
-                 void resolveDoubleDots   ( );
-  
-                 /*!
-                   Used in operator= (and therefore in the ctor) to transform
-                   recognized metaprotocol contained in the path, if any.
-                 */
-                 void expandMetaprotocols ( );
+    //! Remove only extra single dots from the path. Assume there is no explicit
+    //! protocol in the path.
+    void resolveSingleDots   ( );
 
-                 /*!
-                   Used in operator= (and therefore in the ctor) to assign the _protocol
-                   member.
-                 */
-                 void parseProtocol       ( );
+    //! Remove only extra double dots from the path (assume there are no extra
+    //! single dots, i.e. in general that resolveSingleDots() was called first).
+    //! Also assume there is no explicit protocol in the path.
+    void resolveDoubleDots   ( );
 
-                 /***********************
-                  *  protected members  *
-                  **********************/
+    /*!
+      Used in operator= (and therefore in the ctor) to transform
+      recognized metaprotocol contained in the path, if any.
+    */
+    void expandMetaprotocols ( );
 
-                 /*!
-                   Even if the default protocol is considered to be the file protocol,
-                   the _protocol member keeps the exact protocol value in the
-                   string. The protocol() method, however, returns FILE_PROTOCOL if the
-                   protocol was not specified.
-                 */
-                 string _protocol;
+    /*!
+      Used in operator= (and therefore in the ctor) to assign the _protocol
+      member.
+    */
+    void parseProtocol       ( );
 
-             public:
+    /***********************
+     *  protected members  *
+     **********************/
 
-                 //! Constructs a PPath from a string (which can be a serialized PPath).
-                 PPath(const string &path_="");
+    /*!
+      Even if the default protocol is considered to be the file protocol,
+      the _protocol member keeps the exact protocol value in the
+      string. The protocol() method, however, returns FILE_PROTOCOL if the
+      protocol was not specified.
+    */
+    string _protocol;
 
-                 // Shorthand.
-                 PPath(const char* path);
+public:
 
-                 //! Returns an absolute path in the form appropriate for the OS.
-                 //! The returned path never ends with a slash unless it is a root directory.
-                 //!  - if 'add_protocol' is false, then only the FILE_PROTOCOL is allowed
-                 //!    and the protocol will be systematically removed,
-                 //!  - if 'add_protocol' is true, then all protocols are allowed and the
-                 //!    protocol will be systematically added.
-                 PPath absolute(bool add_protocol = false) const;
+    //! Constructs a PPath from a string (which can be a serialized PPath).
+    PPath(const string &path_="");
 
-                 //! Returns a PPath in the canonical (serialized form).
-                 //! It is a string because it needs to be converted to a system-dependent
-                 //! version before it can be used directly as a PPath.
-                 string canonical() const;
+    // Shorthand.
+    PPath(const char* path);
 
-                 //! Return the string that should be displayed in an error
-                 //! message. It will be either the absolute or canonical
-                 //! string, depending on the value of the global PPath
-                 //! boolean 'canonical_in_errors' (the default being to
-                 //! display the absolute path).
-                 string errorDisplay() const;
-  
-                 /*!
-                   Even if the default protocol is considered to be the file protocol,
-                   the _protocol member keeps the exact protocol value in the
-                   string. This is to be able to quickly know whether there is a protocol
-                   in the actual string or not.
-                   The protocol() method, however, returns FILE_PROTOCOL if the
-                   protocol was not specified.
-                 */
-                 string protocol      ()  const { return _protocol.empty() ? FILE_PROTOCOL : _protocol;   }
-                 //! Return either a copy of this PPath if it has an explicit protocol,
-                 //! or a copy with an explicit FILE_PROTOCOL (the default) otherwise.
-                 PPath  addProtocol   ()  const ; 
-                 //! Return a copy of this PPath without its explicit protocol.
-                 PPath  removeProtocol()  const ; 
-                 //! Remove the current trailing slash if there is one and it is not a
-                 //! root directory.
-                 void removeTrailingSlash();
+    //! Returns an absolute path in the form appropriate for the OS.
+    //! The returned path never ends with a slash unless it is a root directory.
+    //!  - if 'add_protocol' is false, then only the FILE_PROTOCOL is allowed
+    //!    and the protocol will be systematically removed,
+    //!  - if 'add_protocol' is true, then all protocols are allowed and the
+    //!    protocol will be systematically added.
+    PPath absolute(bool add_protocol = false) const;
 
-                 //! Parse a PPath url-like parameters. For instance, if the PPath is
-                 //! protocol:/path?param1=value1&param2=value2&param3=value3
-                 //! then after calling this method, 'base_path' will be equal to
-                 //! protocol:/path, and the 'parameters' map will be such that
-                 //! parameters["paramX"] = "valueX".
-                 //! Note that existing mappings in 'parameters' are not removed (unless
-                 //! they are overwritten by the PPath parameters).
-                 void parseUrlParameters(PPath& base_path, map<string, string>& parameters) const;
+    //! Returns a PPath in the canonical (serialized form).
+    //! It is a string because it needs to be converted to a system-dependent
+    //! version before it can be used directly as a PPath.
+    string canonical() const;
 
-                 //! Return true iff this is an absolute path.
-                 bool   isAbsPath     ()  const { return isabs();   } 
-                 bool   isFilePath    ()  const { return  protocol() == FILE_PROTOCOL; }
-                 bool   isHttpPath    ()  const { return _protocol   == HTTP_PROTOCOL; }
-                 bool   isFtpPath     ()  const { return _protocol   ==  FTP_PROTOCOL; }  
+    //! Return the string that should be displayed in an error
+    //! message. It will be either the absolute or canonical
+    //! string, depending on the value of the global PPath
+    //! boolean 'canonical_in_errors' (the default being to
+    //! display the absolute path).
+    string errorDisplay() const;
 
-                 //! Return true iff this is an empty path, i.e. it is equal to "" after its
-                 //! protocol has been removed.
-                 bool   isEmpty       ()  const { return removeProtocol().empty(); }
-                 //! Return true iff this is an (absolute) root directory.
-                 bool   isRoot        ()  const;
+    /*!
+      Even if the default protocol is considered to be the file protocol,
+      the _protocol member keeps the exact protocol value in the
+      string. This is to be able to quickly know whether there is a protocol
+      in the actual string or not.
+      The protocol() method, however, returns FILE_PROTOCOL if the
+      protocol was not specified.
+    */
+    string protocol      ()  const { return _protocol.empty() ? FILE_PROTOCOL : _protocol;   }
+    //! Return either a copy of this PPath if it has an explicit protocol,
+    //! or a copy with an explicit FILE_PROTOCOL (the default) otherwise.
+    PPath  addProtocol   ()  const ; 
+    //! Return a copy of this PPath without its explicit protocol.
+    PPath  removeProtocol()  const ; 
+    //! Remove the current trailing slash if there is one and it is not a
+    //! root directory.
+    void removeTrailingSlash();
 
-                 static const string& _slash();       //!< System-dependent slash string.
-                 static       char   _slash_char();  //!< System-dependent slash character.
+    //! Parse a PPath url-like parameters. For instance, if the PPath is
+    //! protocol:/path?param1=value1&param2=value2&param3=value3
+    //! then after calling this method, 'base_path' will be equal to
+    //! protocol:/path, and the 'parameters' map will be such that
+    //! parameters["paramX"] = "valueX".
+    //! Note that existing mappings in 'parameters' are not removed (unless
+    //! they are overwritten by the PPath parameters).
+    void parseUrlParameters(PPath& base_path, map<string, string>& parameters) const;
 
-                 //! Path concatenation. Note there is no need for an
-                 //! operator/(const string& other)
-                 //! because a string will be automatically converted to a PPath.
-                 PPath  operator/  (const char*    other) const { return operator/ (PPath(other)); }
-                 PPath& operator/= (const char*    other)       { return operator/=(PPath(other)); }
-                 PPath  operator/  (const PPath&   other) const;
-                 PPath& operator/= (const PPath&   other);
+    //! Return true iff this is an absolute path.
+    bool   isAbsPath     ()  const { return isabs();   } 
+    bool   isFilePath    ()  const { return  protocol() == FILE_PROTOCOL; }
+    bool   isHttpPath    ()  const { return _protocol   == HTTP_PROTOCOL; }
+    bool   isFtpPath     ()  const { return _protocol   ==  FTP_PROTOCOL; }  
 
-                 //! The operator '==' returns true iff the two paths represent the same file or
-                 //! directory. The final slash is systematically ignored. For instance:
-                 //!   "/foo/bar"    == "/foo/bar/"
-                 //!   "/foo/bar/.." == "/foo"
-                 //!   "bar"         == "/foo/bar" if the current working directory is "/foo"
-                 //! This is done by comparing this->absolute() to other.absolute().
-                 bool   operator== (const char*   other) const  { return operator==(string(other)); }
-                 bool   operator== (const string& other) const;
-                 bool   operator== (const PPath&  other) const;
+    //! Return true iff this is an empty path, i.e. it is equal to "" after its
+    //! protocol has been removed.
+    bool   isEmpty       ()  const { return removeProtocol().empty(); }
+    //! Return true iff this is an (absolute) root directory.
+    bool   isRoot        ()  const;
 
-                 //! The operator '!=' is defined as the contrary of '=='.
-                 inline bool operator!= (const char*    other) const { return !(operator==(string(other))); }
-                 inline bool operator!= (const string&  other) const { return !(operator==(other));         }
-                 inline bool operator!= (const PPath&   other) const { return !(operator==(other));         }
+    static const string& _slash();       //!< System-dependent slash string.
+    static       char   _slash_char();  //!< System-dependent slash character.
 
-                 /*!
-                   Return a PPath pointing to the parent directory of the PPath, assuming the
-                   PPath represents a directory: you should never call this method on a PPath
-                   pointing to a file, but instead use file_path.dirname().up().
-                   The final '/' in the PPath is ignored.
+    //! Path concatenation. Note there is no need for an
+    //! operator/(const string& other)
+    //! because a string will be automatically converted to a PPath.
+    PPath  operator/  (const char*    other) const { return operator/ (PPath(other)); }
+    PPath& operator/= (const char*    other)       { return operator/=(PPath(other)); }
+    PPath  operator/  (const PPath&   other) const;
+    PPath& operator/= (const PPath&   other);
 
-                   If there is no parent directory, throws a PLERROR.
+    //! The operator '==' returns true iff the two paths represent the same file or
+    //! directory. The final slash is systematically ignored. For instance:
+    //!   "/foo/bar"    == "/foo/bar/"
+    //!   "/foo/bar/.." == "/foo"
+    //!   "bar"         == "/foo/bar" if the current working directory is "/foo"
+    //! This is done by comparing this->absolute() to other.absolute().
+    bool   operator== (const char*   other) const  { return operator==(string(other)); }
+    bool   operator== (const string& other) const;
+    bool   operator== (const PPath&  other) const;
 
-                   PPath::up examples:
+    //! The operator '!=' is defined as the contrary of '=='.
+    inline bool operator!= (const char*    other) const { return !(operator==(string(other))); }
+    inline bool operator!= (const string&  other) const { return !(operator==(other));         }
+    inline bool operator!= (const PPath&   other) const { return !(operator==(other));         }
 
-                   PPath("/").up()                      // PLERROR
-                   PPath("").up()                       // PLERROR
-                   PPath("/foo").up()                   // "/"
-                   PPath("foo").up()                    // "."
-                   PPath(".").up()                      // ".."
-                   PPath("foo/bar").up()                // "foo"
-                   PPath("foo/bar/").up()               // "foo"
-                   PPath("foo/bar/hi.cc").up()          // "foo/bar", but never do this
-                 */
-                 PPath up() const;
+    /*!
+      Return a PPath pointing to the parent directory of the PPath, assuming the
+      PPath represents a directory: you should never call this method on a PPath
+      pointing to a file, but instead use file_path.dirname().up().
+      The final '/' in the PPath is ignored.
 
-                 /*!
-                   Returns a PPath that points to the directory component of the PPath.
-                   Contrary to the up() method, the final slash is important, and the PPath
-                   may point either to a file or to a directory ressource.
-                   The returned PPath will never end with a slash, unless it is a root directory.
+      If there is no parent directory, throws a PLERROR.
 
-                   It is always true that
+      PPath::up examples:
 
-                   path.dirname() / path.basename() == path
-    
-                   PPath::dirname examples:
+      PPath("/").up()                      // PLERROR
+      PPath("").up()                       // PLERROR
+      PPath("/foo").up()                   // "/"
+      PPath("foo").up()                    // "."
+      PPath(".").up()                      // ".."
+      PPath("foo/bar").up()                // "foo"
+      PPath("foo/bar/").up()               // "foo"
+      PPath("foo/bar/hi.cc").up()          // "foo/bar", but never do this
+    */
+    PPath up() const;
 
-                   PPath("").dirname()                  // ""
-                   PPath("foo.cc").dirname()            // "."
-                   PPath(".").dirname()                 // "."
-                   PPath("./").dirname()                // "."
-                   PPath("/").dirname()                 // "/"
-                   PPath("/foo.cc").dirname()           // "/"
-                   PPath("foo/bar").dirname()           // "foo"
-                   PPath("foo/bar/").dirname()          // "foo/bar"
-                   PPath("foo/bar/hi.cc").dirname()     // "foo/bar"
-                 */
-                 PPath dirname() const;
+    /*!
+      Returns a PPath that points to the directory component of the PPath.
+      Contrary to the up() method, the final slash is important, and the PPath
+      may point either to a file or to a directory ressource.
+      The returned PPath will never end with a slash, unless it is a root directory.
 
-                 /*!
-                   Returns the final component of a pathname (which will be "" if it ends
-                   with a slash). The basename never contains a slash.
+      It is always true that
 
-                   It is always true that
+      path.dirname() / path.basename() == path
 
-                   path.dirname() / path.basename() == path
+      PPath::dirname examples:
 
-                   PPath::basename examples:
+      PPath("").dirname()                  // ""
+      PPath("foo.cc").dirname()            // "."
+      PPath(".").dirname()                 // "."
+      PPath("./").dirname()                // "."
+      PPath("/").dirname()                 // "/"
+      PPath("/foo.cc").dirname()           // "/"
+      PPath("foo/bar").dirname()           // "foo"
+      PPath("foo/bar/").dirname()          // "foo/bar"
+      PPath("foo/bar/hi.cc").dirname()     // "foo/bar"
+    */
+    PPath dirname() const;
 
-                   PPath("").basename()                 // ""
-                   PPath("foo.cc").basename()           // "foo.cc"
-                   PPath("/").basename()                // ""
-                   PPath(".").basename()                // "."
-                   PPath("./").basename()               // ""
-                   PPath("foo/bar").basename()          // "bar"
-                   PPath("foo/bar/").basename()         // ""
-                   PPath("foo/bar/hi.cc").basename()    // "hi.cc"
-                 */
-                 PPath basename  () const;
+    /*!
+      Returns the final component of a pathname (which will be "" if it ends
+      with a slash). The basename never contains a slash.
 
-                 /*!
-                   Return the hostname of a PPath representing an url.
-                   Although this method is meant to be called on a PPath with
-                   a HTTP or FTP protocol, it may also be used with other
-                   protocols.
+      It is always true that
 
-                   PPath::hostname example:
+      path.dirname() / path.basename() == path
 
-                   PPath("http://foo.com/bar/hi").hostname() // "foo.com"
-                 */
-                 string hostname() const;
+      PPath::basename examples:
 
-                 /*!
-                   Return the extension of basename() (or an empty string if it has no
-                   extension). The dot may or may not be included, depending on the value
-                   of the 'with_dot' parameter.
-                 */
-                 string extension (bool with_dot = false) const;
+      PPath("").basename()                 // ""
+      PPath("foo.cc").basename()           // "foo.cc"
+      PPath("/").basename()                // ""
+      PPath(".").basename()                // "."
+      PPath("./").basename()               // ""
+      PPath("foo/bar").basename()          // "bar"
+      PPath("foo/bar/").basename()         // ""
+      PPath("foo/bar/hi.cc").basename()    // "hi.cc"
+    */
+    PPath basename  () const;
 
-                 /*!
-                   Return a copy of the path, but without its extension (as computed by the
-                   extension() method).
-                 */
-                 PPath no_extension () const;
+    /*!
+      Return the hostname of a PPath representing an url.
+      Although this method is meant to be called on a PPath with
+      a HTTP or FTP protocol, it may also be used with other
+      protocols.
 
+      PPath::hostname example:
 
+      PPath("http://foo.com/bar/hi").hostname() // "foo.com"
+    */
+    string hostname() const;
+
+    /*!
+      Return the extension of basename() (or an empty string if it has no
+      extension). The dot may or may not be included, depending on the value
+      of the 'with_dot' parameter.
+    */
+    string extension (bool with_dot = false) const;
+
+    /*!
+      Return a copy of the path, but without its extension (as computed by the
+      extension() method).
+    */
+    PPath no_extension () const;
+
+
 //   /*! Migrated from fileutils.{h,cc}
 // 
 //   Returns a ppath shorter than 256 character and exempt of any of the
@@ -477,41 +477,40 @@
 //!<   bool endsWith  (const string& s) const;
 
 
-                 /*************************************************************************
-                  * Dos/Posix dependent methods
-                  ************************************************************************/
-  
-  
-                 /**
-                    Returns the drive specification of a path (a drive letter followed by a
-                    colon) under dos. Under posix, always returns "".
-                 */
-                 PPath drive() const;
+    /*************************************************************************
+     * Dos/Posix dependent methods
+     ************************************************************************/
 
 
-             protected:
+    /**
+       Returns the drive specification of a path (a drive letter followed by a
+       colon) under dos. Under posix, always returns "".
+    */
+    PPath drive() const;
 
-                 /**
-                    Returns whether a path is absolute.
+protected:
 
-                    Trivial in Posix, harder on the Mac or MS-DOS. For DOS it is absolute
-                    if it starts with a slash or backslash (current volume), or if a
-                    pathname after the volume letter and colon starts with a slash or
-                    backslash.
+    /**
+       Returns whether a path is absolute.
 
-                    It is protected because one should use isAbsPath().
-                 */
-                 bool isabs() const;  
+       Trivial in Posix, harder on the Mac or MS-DOS. For DOS it is absolute
+       if it starts with a slash or backslash (current volume), or if a
+       pathname after the volume letter and colon starts with a slash or
+       backslash.
 
-             private:
+       It is protected because one should use isAbsPath().
+    */
+    bool isabs() const;  
 
-                 //! Whether or not to display the canonical path in
-                 //! errorDisplay(). Default value is 'false', and it can be
-                 //! modified through the setCanonicalInErrors(..) function.
-                 static bool canonical_in_errors;
+private:
 
-             };
+    //! Whether or not to display the canonical path in
+    //! errorDisplay(). Default value is 'false', and it can be
+    //! modified through the setCanonicalInErrors(..) function.
+    static bool canonical_in_errors;
 
+};
+
 DECLARE_TYPE_TRAITS(PPath);
   
 //! Serialization and output of a PPath.



From tihocan at mail.berlios.de  Sun Apr 13 15:19:50 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Sun, 13 Apr 2008 15:19:50 +0200
Subject: [Plearn-commits] r8807 - trunk/plearn/io
Message-ID: <200804131319.m3DDJoAx029152@sheep.berlios.de>

Author: tihocan
Date: 2008-04-13 15:19:48 +0200 (Sun, 13 Apr 2008)
New Revision: 8807

Added:
   trunk/plearn/io/RPPath.cc
   trunk/plearn/io/RPPath.h
Log:
Remote interface to PPaths

Added: trunk/plearn/io/RPPath.cc
===================================================================
--- trunk/plearn/io/RPPath.cc	2008-04-13 12:34:38 UTC (rev 8806)
+++ trunk/plearn/io/RPPath.cc	2008-04-13 13:19:48 UTC (rev 8807)
@@ -0,0 +1,142 @@
+// -*- C++ -*-
+
+// RPPath.cc
+//
+// Copyright (C) 2008 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file RPPath.cc */
+
+
+#include "RPPath.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    RPPath,
+    "Remote interface for a PPath",
+    "This class is intended to manipulate file and directory paths from\n"
+    "outside PLearn.\n"
+    "Currently it can only be used to convert a PPath to its absolute or\n"
+    "canonical representation, but more forwarded methods may be added in\n"
+    "the future."
+
+);
+
+////////////
+// RPPath //
+////////////
+RPPath::RPPath()
+{}
+
+////////////////////
+// declareOptions //
+////////////////////
+void RPPath::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "path", &RPPath::path,
+            OptionBase::buildoption,
+        "Path (to file or directory) that is being manipulated.");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+////////////////////
+// declareMethods //
+////////////////////
+void RPPath::declareMethods(RemoteMethodMap& rmm)
+{
+    rmm.inherited(inherited::_getRemoteMethodMap_());
+
+    declareMethod(rmm, "absolute", &RPPath::absolute,
+            (BodyDoc("Return the absolute path.")));
+
+    declareMethod(rmm, "canonical", &RPPath::canonical,
+            (BodyDoc("Return the canonic path.")));
+}
+
+///////////
+// build //
+///////////
+void RPPath::build()
+{
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void RPPath::build_()
+{}
+
+//////////////
+// absolute //
+//////////////
+PPath RPPath::absolute()
+{
+    return path.absolute();
+}
+
+///////////////
+// canonical //
+///////////////
+string RPPath::canonical()
+{
+    return path.canonical();
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void RPPath::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/io/RPPath.h
===================================================================
--- trunk/plearn/io/RPPath.h	2008-04-13 12:34:38 UTC (rev 8806)
+++ trunk/plearn/io/RPPath.h	2008-04-13 13:19:48 UTC (rev 8807)
@@ -0,0 +1,120 @@
+// -*- C++ -*-
+
+// RPPath.h
+//
+// Copyright (C) 2008 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file RPPath.h */
+
+
+#ifndef RPPath_INC
+#define RPPath_INC
+
+#include <plearn/base/Object.h>
+#include <plearn/io/PPath.h>
+
+namespace PLearn {
+
+class RPPath : public Object
+{
+    typedef Object inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    PPath path;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    RPPath();
+
+    // The methods below are forwarded to the underlying PPath.
+
+    PPath absolute();
+    string canonical();
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(RPPath);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+    static void declareMethods(RemoteMethodMap& rmm);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(RPPath);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From tihocan at mail.berlios.de  Sun Apr 13 15:33:04 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Sun, 13 Apr 2008 15:33:04 +0200
Subject: [Plearn-commits] r8808 - trunk/commands
Message-ID: <200804131333.m3DDX4IM030461@sheep.berlios.de>

Author: tihocan
Date: 2008-04-13 15:33:04 +0200 (Sun, 13 Apr 2008)
New Revision: 8808

Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
Added include of RPPath

Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2008-04-13 13:19:48 UTC (rev 8807)
+++ trunk/commands/plearn_noblas_inc.h	2008-04-13 13:33:04 UTC (rev 8808)
@@ -52,6 +52,7 @@
  *****************/
 #include <plearn/db/UCISpecification.h>
 #include <plearn/io/openUrl.h>
+#include <plearn/io/RPPath.h>
 #include <plearn/math/ManualBinner.h>
 #include <plearn/math/SoftHistogramBinner.h>
 #include <plearn/misc/ShellScript.h>



From nouiz at mail.berlios.de  Sun Apr 13 16:11:08 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Sun, 13 Apr 2008 16:11:08 +0200
Subject: [Plearn-commits] r8809 - trunk
Message-ID: <200804131411.m3DEB8id000423@sheep.berlios.de>

Author: nouiz
Date: 2008-04-13 16:11:04 +0200 (Sun, 13 Apr 2008)
New Revision: 8809

Modified:
   trunk/pymake.config.model
Log:
don't add the -fPIC parameter in cygwin.


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2008-04-13 13:33:04 UTC (rev 8808)
+++ trunk/pymake.config.model	2008-04-13 14:11:04 UTC (rev 8809)
@@ -85,7 +85,7 @@
 # We add -fPIC, as it is requested to make a shared '.so' file, it does not
 # have any speed impact, and is ignored by the compiler if it cannot use it.
 # We do not add it under Windows since it generates a useless compiler warning.
-if platform != 'win32':
+if platform != 'win32' and platform != 'cygwin':
     compileflags += ' -fPIC'
 
 # The platform variable contains the type of platform from which pymake has been invoked. 



From nouiz at mail.berlios.de  Sun Apr 13 17:14:31 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Sun, 13 Apr 2008 17:14:31 +0200
Subject: [Plearn-commits] r8810 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200804131514.m3DFEVt0005738@sheep.berlios.de>

Author: nouiz
Date: 2008-04-13 17:14:30 +0200 (Sun, 13 Apr 2008)
New Revision: 8810

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
Added the --files option. This option use the file transfert mechanis of condor, so we won't need NFS. 
WARNING: condor don't handle directory...


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-04-13 14:11:04 UTC (rev 8809)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-04-13 15:14:30 UTC (rev 8810)
@@ -669,6 +669,7 @@
         self.nice = False
         self.req = ''
         self.copy_local_source_file = False
+        self.files = ''
         DBIBase.__init__(self, commands, **args)
         if not os.path.exists(self.log_dir):
             os.mkdir(self.log_dir) # condor log are always generated
@@ -698,7 +699,7 @@
             autorized_shell_command=[ "touch", "echo"]
             if c in autorized_shell_command:
                 shellcommand=True
-            else:
+            elif not self.files:
                 c = os.path.normpath(os.path.join(os.getcwd(), c))
             command = "".join([c,c2])
 
@@ -800,21 +801,26 @@
             launch_file = os.path.join(self.log_dir, 'launch.csh')
         else:
             launch_file = os.path.join(self.log_dir, 'launch.sh')
-
+            
         condor_dat.write( dedent('''\
                 executable     = %s
                 universe       = vanilla
                 requirements   = %s
-                output         = %s/condor.%s.$(Process).out
-                error          = %s/condor.%s.$(Process).error
+                output         = %s/condor.$(Process).out
+                error          = %s/condor.$(Process).error
                 log            = %s/condor.log
                 getenv         = %s
                 nice_user      = %s
                 ''' % (launch_file,req,
-                       self.log_dir,self.unique_id,
-                       self.log_dir,self.unique_id,
+                       self.log_dir,
+                       self.log_dir,
                        self.log_dir,str(self.getenv),str(self.nice))))
-
+        if self.files: #ON_EXIT_OR_EVICT
+            condor_dat.write( dedent('''\
+                when_to_transfer_output = ON_EXIT
+                should_transfer_files = Yes
+                transfer_input_files = %s
+                '''%(self.files+','+launch_file+','+self.tasks[0].commands[0].split()[0]))) # no directory
         if len(condor_datas)!=0:
             for i in condor_datas:
                 condor_dat.write("arguments      = sh "+i+" $$(Arch) \nqueue\n")

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-04-13 14:11:04 UTC (rev 8809)
+++ trunk/scripts/dbidispatch	2008-04-13 15:14:30 UTC (rev 8810)
@@ -164,7 +164,7 @@
                   "--no_getenv", "--no_cwait", "--no_clean_up" , "--no_nice"]:
         dbi_param[argv[5:]]=False
     elif argv.split('=')[0] in ["--duree","--cpu","--mem","--os",
-                                "--nb_proc","--req"]:
+                                "--nb_proc","--req", "--files"]:
         dbi_param[argv.split('=')[0][2:]]=argv.split('=')[1]
     elif argv.startswith('--machine=') or argv.startswith('--machines='):
         if argv.split('=')[0] == "--machine":
@@ -203,7 +203,7 @@
     valid_dbi_param +=["cwait","force","nb_proc","arch","interruptible",
                        "duree","cpu","mem","os"]
 elif launch_cmd=="Condor":
-    valid_dbi_param +=["req", "arch", "getenv", "nice"]
+    valid_dbi_param +=["req", "arch", "getenv", "nice", "files"]
 elif launch_cmd=="Bqtools":
     valid_dbi_param +=["micro", "long","nb_proc","duree"]
 elif launch_cmd=="Local":
@@ -211,7 +211,7 @@
 
 from socket import gethostname
 if  launch_cmd == 'Condor' and gethostname().endswith(".iro.umontreal.ca"):
-    if not os.path.abspath(os.path.curdir).startswith("/home/fringant2/"):
+    if not os.path.abspath(os.path.curdir).startswith("/home/fringant2/") and not dbi_param.get('files'):
         raise Exception("You must be in a subfolder of /home/fringant2/")
     f=os.getenv("CONDOR_LOCAL_SOURCE")
     if f and not f.startswith("/home/fringant2/"):



From nouiz at mail.berlios.de  Sun Apr 13 17:26:30 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Sun, 13 Apr 2008 17:26:30 +0200
Subject: [Plearn-commits] r8811 - trunk/python_modules/plearn/pymake
Message-ID: <200804131526.m3DFQUhZ006868@sheep.berlios.de>

Author: nouiz
Date: 2008-04-13 17:26:29 +0200 (Sun, 13 Apr 2008)
New Revision: 8811

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
print the name of host that is troublesome. This help to know which host is bugged


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-04-13 15:14:30 UTC (rev 8810)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-04-13 15:26:29 UTC (rev 8811)
@@ -960,6 +960,8 @@
         # print error messages, warnings, and get failure/success status
         info.finished_compilation()
         if info.remove_hostname:
+            if verbose>=3:
+                print "Removing host from list:", info.hostname
             list_of_hosts.remove(info.hostname)
             info.hostname = ''
 



From nouiz at mail.berlios.de  Sun Apr 13 18:15:02 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Sun, 13 Apr 2008 18:15:02 +0200
Subject: [Plearn-commits] r8812 - trunk/python_modules/plearn/pymake
Message-ID: <200804131615.m3DGF2d3012711@sheep.berlios.de>

Author: nouiz
Date: 2008-04-13 18:15:02 +0200 (Sun, 13 Apr 2008)
New Revision: 8812

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
check if file is present


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-04-13 15:26:29 UTC (rev 8811)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-04-13 16:15:02 UTC (rev 8812)
@@ -1171,6 +1171,12 @@
         cctarget = target
     else:
         cctarget = get_ccpath_from_noncc_path(target)
+
+    if not cctarget:
+        raise IOError("File not found: "+target)
+    elif not os.path.exists(cctarget):
+        raise IOError("File not found: "+cctarget)
+
     info = file_info(cctarget)
 
     if len(args) == 1: # only the target was specified: generate the full graph



From nouiz at mail.berlios.de  Sun Apr 13 18:18:16 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Sun, 13 Apr 2008 18:18:16 +0200
Subject: [Plearn-commits] r8813 - trunk/python_modules/plearn/pymake
Message-ID: <200804131618.m3DGIGDf012855@sheep.berlios.de>

Author: nouiz
Date: 2008-04-13 18:18:16 +0200 (Sun, 13 Apr 2008)
New Revision: 8813

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
..


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-04-13 16:15:02 UTC (rev 8812)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-04-13 16:18:16 UTC (rev 8813)
@@ -2804,8 +2804,10 @@
 
         print '*** Running pymake on '+os.path.basename(target)+' using configuration file: ' + configpath
         print '*** Running pymake on '+os.path.basename(target)+' using options: ' + string.join(map(lambda o: '-'+o, options))
-        print '++++ Computing dependencies of '+target+' ...'
+        print '++++ Computing dependencies of '+target+' ...',
         get_ccfiles_to_compile_and_link(target, ccfiles_to_compile, executables_to_link, linkname)
+        print ' done'
+        get_ccfiles_to_compile_and_link(target, ccfiles_to_compile, executables_to_link, linkname)
 
         if distribute:
             # We dont want to compile. We will extract the necessary file to compile



From louradou at mail.berlios.de  Tue Apr 15 17:07:06 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Tue, 15 Apr 2008 17:07:06 +0200
Subject: [Plearn-commits] r8814 - trunk/python_modules/plearn/learners
Message-ID: <200804151507.m3FF76g5028670@sheep.berlios.de>

Author: louradou
Date: 2008-04-15 17:07:06 +0200 (Tue, 15 Apr 2008)
New Revision: 8814

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:
Added one-vs-all strategy for multi-class in SVM.
Also added different kinfs of outputs (some are being tested, and temporary).



Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2008-04-13 16:18:16 UTC (rev 8813)
+++ trunk/python_modules/plearn/learners/SVM.py	2008-04-15 15:07:06 UTC (rev 8814)
@@ -417,10 +417,12 @@
         tried_gamma = self.get_trials_oneparam_list('gamma')
         if best_gamma in tried_gamma:
             if best_gamma <> tried_gamma[0]:
-                raise ValueError,"in SVMHyperParamOracle__rbf::choose_new_param() " + \
+                print "WARNING in SVMHyperParamOracle__rbf::choose_new_param() " + \
                                  "best gamma %s is not the 1st of tried_gamma %s." % \
                                  (best_gamma, tried_gamma ) + \
                                  "\nThis list should be ordered w.r.t. costs."
+                tried_gamma.remove(best_gamma)
+                tried_gamma = [best_gamma]+tried_gamma
             gamma_list = self.choose_new_param_geom( tried_gamma )
         else:
             gamma_list = self.init_gamma(best_gamma)
@@ -586,7 +588,7 @@
         'maincost_name': <str> Cost that leads the choice of hyperparameters.
                          The main cost is the cost to minimize it by validation.
                          By default (if set to None), the first cost of costnames
-                         is taken. 
+                         is considered. 
 
         'n_fold': <int> Number of folds in the cross-validation
 
@@ -598,11 +600,17 @@
                            set. Useful when: classes are unbalanced and the cost
                            of interest is normalized w.r.t class prior probas.
 
-        'use_proba': <int> in [-1,0,1] specifies the kinds of SVM outputs (of which
-                     will depend the predictions).
-                     0 is a simple onehot of the prediction.
-                     -1 is the vote counts (one-against-one strategy for multi-class).
-                     1 is the posterior emperical probability (using sigmoids of SVM standard outputs).
+        'multiclass_strategy': <string> Kind of strategy to compute outputs.
+                           - 'onevsone': One-against-one
+                           - 'onevsall': One-against-all
+
+        'outputs_type': <string> that specifies the kinds of SVM outputs (of which
+                        may depend the predictions).
+                        - 'onehot' is a simple onehot of the prediction.
+                        - 'votes' is the vote counts (one-against-one strategy for multi-class).
+                        - 'dist' is the usual output for a SVM (distance to the boundary). For one-vs-one, it is the sum of distances (not recommended).
+                        - 'proba' is the posterior emperical probability (using sigmoids of SVM standard outputs).
+                        Note: with the one-vs-all strategy, 'votes' and 'onehot' give the same outputs.
                                      
         'retrain_on_valid': <bool> Use the best hyperparameters (found on valid)
                              to re-train a bigger model on {train, valid}.
@@ -615,7 +623,7 @@
                          is found. If False, you can re-run run() several times. If True,
                          you can also re-run to possibly find better performance.
 
-        'max_ntries': <int> when calling run(), maximum number of hyperparameters to try
+        'max_ntrials': <int> when calling run(), maximum number of hyperparameters to try
                       since the last forget().
 
         'test_on_train': <bool> Should we test best models on {test, train} (1)
@@ -682,9 +690,10 @@
                         'n_fold',
                         'normalize_inputs',
                         'balanceC',
-                        'use_proba',
+                        'multiclass_strategy',
+                        'outputs_type',
                         'retrain_until_local_optimum_is_found',
-                        'max_ntries',
+                        'max_ntrials',
                         'retrain_on_valid',
                         'test_on_train',
                         'verbosity',
@@ -758,7 +767,7 @@
 
         self.retrain_on_valid = True
         self.retrain_until_local_optimum_is_found = True
-        self.max_ntries = 50
+        self.max_ntrials = 50
         self.test_on_train = True
 
         
@@ -768,7 +777,7 @@
         self.validset_key = 'validset'
         self.testset_key  = 'testset'
 
-        self.use_proba = False
+        self.outputs_type = 'votes'
 
     def forget(self):
         for expert in self.all_experts:
@@ -940,7 +949,7 @@
         if len(s)>0:s=', '+s
 
         
-        if self.use_proba:
+        if 'proba' in self.outputs_type:
             # if this function is defined (see 
             return eval('svm_parameter( svm_type = C_SVC, probability = 1 '+s+')' )
         else:
@@ -1106,6 +1115,8 @@
              ):
         if self.verbosity > 3:
             print "SVM::train() called ", dataspec.keys()
+        if self.verbosity > 1:
+            print "launching libsvm with param %s " % param    
         
         if param == None:
             if not self.best_param:
@@ -1115,18 +1126,33 @@
             param['kernel_type'] = self.kernel_type
         
         trainset = self.train_inputspec(dataspec)
-        if self.balanceC:
-            self.get_data_stats( trainset )
-            param.update({'weight':self.weight,
-                          'nr_weight':len(self.weight),
-                          'weight_label':self.labels})
+        self.get_data_stats( trainset )
         
         train_samples, train_targets = self.get_svminputlist( trainset, True )
-        train_problem = svm_problem( train_targets ,
-                                     train_samples )
-        if self.verbosity > 1:
-            print "launching libsvm with param %s " % param    
-        model = svm_model(train_problem, self.get_libsvm_param( param ) )
+        # one-against-one strategy is the only one implemented in libsvm
+        if self.multiclass_strategy == 'onevsone':
+            if self.balanceC:
+                param.update({'weight':self.weight,
+                              'nr_weight':len(self.weight),
+                              'weight_label':self.labels})
+            train_problem = svm_problem( train_targets ,
+                                         train_samples )
+            model = svm_model(train_problem, self.get_libsvm_param( param ) )
+        elif self.multiclass_strategy == 'onevsall':
+            model = []
+            for c in range(self.nclasses):
+                if self.balanceC:
+                    p = self.class_priors[c]
+                    cst = 2*p*(1-p) # constant to have average 1 on weights
+                    param.update({'weight':[ cst/p,cst/(1-p) ],
+                                  'nr_weight':2,
+                                  'weight_label':[1,-1]})
+                onevsall_targets = [ int(t)==c and 1 or -1 for t in train_targets ]
+                train_problem = svm_problem( onevsall_targets ,
+                                             train_samples )
+                model.append( svm_model(train_problem, self.get_libsvm_param( param ) ) )
+        else:
+            raise ValueError, "Unknown value %s for option 'multiclass_strategy'" % self.multiclass_strategy
         
         if param == self.best_param:
             self.best_model = model
@@ -1141,13 +1167,14 @@
     """
     def predict_from_outputs(self, outputs, targets, vmat):
         assert self.nclasses == len(outputs[0])
+        assert type(outputs[0]) == list
         predictions = []
         for o in outputs:
             predictions.append( array(o).argmax() )
         return predictions, targets
 
     def get_outputs_targets( self,
-                                 testset ):
+                             testset ):
         assert testset <> None
         # By default take the LAST model trained
         if self.model <> None:
@@ -1161,7 +1188,7 @@
         nclasses = self.nclasses
         nsamples = len(samples)
 
-        ## specific to libsvm
+        ## The following is specific to libsvm
         ##
         # Note: model.predict_values(x)
         #           gives a dictionary with the SVM scores for one-against-one
@@ -1169,30 +1196,84 @@
         #           gives the prediction by "max win" strategy (each SVM has a vote 1 for a class)
         #       model.predict_probability(x)
         #           gives a dictionary with proabilities corresponding to each class
-        outputs = []
-        if self.use_proba == 1:
-            for x in samples:
-                prd, prb = model.predict_probability(x)
-                output = [0]*nclasses
-                for c in prb:
-                    output[int(c)] = prb[c]
-                outputs.append(output)
-        elif self.use_proba == 0:
-            for x in samples:
-                output = [0]*nclasses
-                output[ int(model.predict(x)) ] += 1
-                outputs.append( output )                
-        elif self.use_proba == -1:
-            for x in samples:
-                output = [0]*nclasses
-                oneagainstone_dict = model.predict_values(x)
-                for cl1cl2 in oneagainstone_dict:
-                    if oneagainstone_dict[cl1cl2] > 0:
-                        output[cl1cl2[0]] += 1
-                    else:
-                        output[cl1cl2[1]] += 1
-                outputs.append( output )
+        if self.multiclass_strategy == 'onevsone':
+            outputs = []
+            if self.outputs_type == 'proba':
+                for x in samples:
+                    prd, prb = model.predict_probability(x)
+                    output = [0]*nclasses
+                    for c in prb:
+                        output[int(c)] = prb[c]
+                    outputs.append(output)
+            elif self.outputs_type == 'onehot':
+                for x in samples:
+                    output = [0]*nclasses
+                    output[ int(model.predict(x)) ] += 1
+                    outputs.append( output )                
+            elif self.outputs_type == 'votes':
+                for x in samples:
+                    output = [0]*nclasses
+                    onevsone_dict = model.predict_values(x)
+                    for cl1cl2 in onevsone_dict:
+                        if onevsone_dict[cl1cl2] > 0:
+                            output[cl1cl2[0]] += 1
+                    outputs.append( output )
+            elif self.outputs_type == 'dist':
+                for x in samples:
+                    output = [0]*nclasses
+                    onevsone_dict = model.predict_values(x)
+                    for cl1cl2 in onevsone_dict:
+                        if onevsone_dict[cl1cl2] > 0:
+                            output[cl1cl2[0]] += onevsone_dict[cl1cl2]
+                    outputs.append( output )
+            else:
+                raise ValueError, "Unknown value %s for option 'outputs_type'" % self.outputs_type
+        elif self.multiclass_strategy == 'onevsall':
+            assert type(model) == list
+            outputs = []
+            if self.outputs_type == 'proba':
+                for x in samples:
+                    output = [0]*nclasses
+                    for c in range(nclasses):
+                        prd, prb = model[c].predict_probability(x)
+                        output[c] = prb[1]
+                    outputs.append(output)
+            elif self.outputs_type == 'proba2':
+                for x in samples:
+                    output = [0]*nclasses
+                    for c in range(nclasses):
+                        prd, prb = model[c].predict_probability(x)
+                        output[c] = prb[1]
+                    outputs.append( softmax(output) )
+            elif self.outputs_type == 'onehot' or self.outputs_type == 'votes':
+                for x in samples:
+                    output = [0]*nclasses
+                    svm_outputs = zeros(nclasses)
+                    for c in range(nclasses):
+                        onevsone_dict = model[c].predict_values(x)
+                        svm_outputs[c] = onevsone_dict[(1,-1)]
+                    output[ svm_outputs.argmax() ] += 1
+                    outputs.append( output )                
+            elif self.outputs_type == 'dist':
+                for x in samples:
+                    output = [0]*nclasses
+                    for c in range(nclasses):
+                        onevsone_dict = model[c].predict_values(x)
+                        output[c] = onevsone_dict[(1,-1)]
+                    outputs.append( output )
+            elif self.outputs_type == 'dist2':
+                for x in samples:
+                    output = [0]*nclasses
+                    for c in range(nclasses):
+                        onevsone_dict = model[c].predict_values(x)
+                        output[c] = onevsone_dict[(1,-1)]
+                    outputs.append( softmax(output) )
+            else:
+                raise ValueError, "Unknown value %s for option 'outputs_type'" % self.outputs_type
+        else:
+            raise ValueError, "Unknown value %s for option 'multiclass_strategy'" % self.multiclass_strategy
 
+        assert len(outputs) == len(targets)
         return outputs, targets
 
     """ Return the costs obtained by a libSVM model
@@ -1360,7 +1441,6 @@
         cf. train_inputspec(), valid_inputspec(), and test_inputspec().
     """
     def run(self, dataspec):
-
         if self.verbosity > 3:
             print "SVM::run() called ", dataspec.keys()
 
@@ -1368,7 +1448,7 @@
         validset = self.valid_inputspec(dataspec)
         testset  = self.test_inputspec(dataspec)
 
-        
+        # Input statistics are needed to choose the first values of some hyperparameters
         self.get_data_stats( trainset )
 
         expert = self.get_expert( self.kernel_type )
@@ -1451,9 +1531,11 @@
                 self.write_results( self.best_param,
                                     self.valid_stats, self.test_stats, self.train_stats )
 
+            if len(expert.trials_param_list) >= self.max_ntrials:
+                return dataspec
+
         if( self.retrain_until_local_optimum_is_found
-        and expert.should_be_tuned_again()
-        and len(expert.trials_param_list) < self.max_ntries):
+        and expert.should_be_tuned_again() ):
            self.run( dataspec )
 
         return dataspec
@@ -1512,6 +1594,13 @@
             prod *= value
         return prod**(1./len(data))
 
+""" Softmax function
+"""
+def softmax(output):
+    expterms = [ exp(o) for o in output ]
+    S = sum(expterms)
+    return [ e/S for e in expterms ]
+
 """"""
 
 """ Below:
@@ -1523,8 +1612,8 @@
 #-------
     
 svm = SVM()
-svm.get_datalist = ToBagClassifier_get_datalist
-svm.predict_from_outputs = ToBagClassifier_predict_from_outputs
+svm.get_datalist = get_datalist_onBags
+svm.predict_from_outputs = predict_from_outputs_onBags
 
 """         
 def get_baginfo( input_vmat ):
@@ -1535,22 +1624,19 @@
     baginfo = [ int(t) for t in data_array[:,inputsize+1] ]
     return baginfo
 
-def ToBagClassifier_get_datalist( input_vmat ):
+def get_datalist_onBags( input_vmat ):
     data_array = input_vmat.getMat()
     inputsize = input_vmat.inputsize
-    targetsize = input_vmat.targetsize
-    nsamples = input_vmat.length
-    assert shape(data_array)[0] == nsamples
-    assert targetsize==2
     samples   = [ [ float(x_t_i)    for x_t_i in x_t ]
                                     for x_t in data_array[:,:inputsize] ]
     targets   = [ float(t) for t in data_array[:,inputsize] ]
     return samples, targets
 
-def ToBagClassifier_predict_from_outputs(outputs, targets, vmat):
+def predict_from_outputs_onBags(outputs, targets, vmat):
     baginfo = get_baginfo(vmat)
     assert len(outputs) == len(targets) == len(baginfo)
     nclasses = len(outputs[0])
+    assert type(outputs[0]) == list
     bag_predictions=[]
     bag_targets=[]
     votes = zeros(nclasses)
@@ -1653,13 +1739,13 @@
 
 
     normalize_inputs = 0
-    use_proba = 0
+    outputs_type = 'votes'
     
     svm.normalize_inputs = normalize_inputs
-    svm.use_proba = use_proba
+    svm.outputs_type = outputs_type
     
-    svm.preproc_optionnames = [ 'renormalize_inputs', 'use_proba' ]
-    svm.preproc_optionvalues = [ normalize_inputs ,  use_proba ]
+    svm.preproc_optionnames = [ 'renormalize_inputs', 'outputs_type' ]
+    svm.preproc_optionvalues = [ normalize_inputs ,  outputs_type ]
     
     svm.kernel_type = 'poly'
     svm.run(dataspec)



From louradou at mail.berlios.de  Tue Apr 15 18:40:06 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Tue, 15 Apr 2008 18:40:06 +0200
Subject: [Plearn-commits] r8815 - trunk/python_modules/plearn/learners
Message-ID: <200804151640.m3FGe52g003750@sheep.berlios.de>

Author: louradou
Date: 2008-04-15 18:40:04 +0200 (Tue, 15 Apr 2008)
New Revision: 8815

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:


Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2008-04-15 15:07:06 UTC (rev 8814)
+++ trunk/python_modules/plearn/learners/SVM.py	2008-04-15 16:40:04 UTC (rev 8815)
@@ -741,7 +741,7 @@
                                    self.HyperParamOracle__rbf,
                                    self.HyperParamOracle__poly,
                                   ]
-        self.param_names =['validtype','normalize_inputs','kernel_type','balanceC']
+        self.param_names =['outputs_type','multiclass_strategy','validtype','normalize_inputs','kernel_type','balanceC']
         for expert in self.all_experts:
             for pn in expert.param_names:
                 if pn not in self.param_names:
@@ -1009,7 +1009,6 @@
         else:
             raise TypeError, "preproc_optionvalues must be of type str or list"
         
-        
         param_names=self.param_names
         param_values=[]
         for pn in param_names:



From tihocan at mail.berlios.de  Tue Apr 15 19:07:14 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 15 Apr 2008 19:07:14 +0200
Subject: [Plearn-commits] r8816 - trunk/plearn/vmat
Message-ID: <200804151707.m3FH7E0K008527@sheep.berlios.de>

Author: tihocan
Date: 2008-04-15 19:07:13 +0200 (Tue, 15 Apr 2008)
New Revision: 8816

Modified:
   trunk/plearn/vmat/AddBagInformationVMatrix.cc
   trunk/plearn/vmat/AddBagInformationVMatrix.h
Log:
Added an option to remove the bag information column from the source VMat

Modified: trunk/plearn/vmat/AddBagInformationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/AddBagInformationVMatrix.cc	2008-04-15 16:40:04 UTC (rev 8815)
+++ trunk/plearn/vmat/AddBagInformationVMatrix.cc	2008-04-15 17:07:13 UTC (rev 8816)
@@ -60,6 +60,7 @@
 // AddBagInformationVMatrix //
 //////////////////////////////
 AddBagInformationVMatrix::AddBagInformationVMatrix():
+    remove_bag_info_column(false),
     bag_info_idx(-1)
 {
 }
@@ -75,6 +76,12 @@
         "The source's column that is used to find bags in the data. It can\n"
         "be either a number or a column's name.");
 
+    declareOption(ol, "remove_bag_info_column",
+                  &AddBagInformationVMatrix::remove_bag_info_column,
+                  OptionBase::buildoption,
+        "If true, then the source's column given by bag_info_column is\n"
+        "removed from the resulting data.");
+ 
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -84,7 +91,6 @@
 ///////////
 void AddBagInformationVMatrix::build()
 {
-    // ### Nothing to add here, simply calls build_
     inherited::build();
     build_();
 }
@@ -98,7 +104,6 @@
         updateMtime(source);
         bag_info_idx = source->getFieldIndex(bag_info_column);
         sourcerow.resize(source->width());
-        width_ = source->width() + 1;
         int st = source->targetsize();
         if (st >= 0)
             targetsize_ = st + 1;
@@ -106,8 +111,22 @@
         // Set field infos.
         Array<VMField> fields = source->getFieldInfos().copy();
         fields.append(VMField("bag_info"));
+
+        width_ = source->width() + 1;
+        inputsize_ = source->inputsize();
+
+        if (remove_bag_info_column) {
+            // We need to remove a column from the source VMat.
+            width_--;
+            inputsize_--;
+            Array<VMField> new_fields;
+            for (int i = 0; i < fields.length(); i++)
+                if (i != bag_info_idx)
+                    new_fields.append(fields[i]);
+            fields = new_fields;
+        }
+
         setFieldInfos(fields);
-
         setMetaInfoFromSource();
     }
 }
@@ -142,7 +161,13 @@
                                     : 1
                            : is_end ? 2
                                     : 0;
-    v.subVec(0, sourcerow.length()) << sourcerow;
+    if (remove_bag_info_column) {
+        v.subVec(0, bag_info_idx) << sourcerow.subVec(0, bag_info_idx);
+        int n_left = sourcerow.length() - bag_info_idx - 1;
+        v.subVec(bag_info_idx, n_left) <<
+            sourcerow.subVec(bag_info_idx + 1, n_left);
+    } else
+        v.subVec(0, sourcerow.length()) << sourcerow;
     v.lastElement() = bag_info;
 }
 

Modified: trunk/plearn/vmat/AddBagInformationVMatrix.h
===================================================================
--- trunk/plearn/vmat/AddBagInformationVMatrix.h	2008-04-15 16:40:04 UTC (rev 8815)
+++ trunk/plearn/vmat/AddBagInformationVMatrix.h	2008-04-15 17:07:13 UTC (rev 8816)
@@ -62,6 +62,7 @@
     //#####  Public Build Options  ############################################
 
     string bag_info_column;
+    bool remove_bag_info_column;
 
 public:
     //#####  Public Member Functions  #########################################



From louradou at mail.berlios.de  Tue Apr 15 20:52:59 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Tue, 15 Apr 2008 20:52:59 +0200
Subject: [Plearn-commits] r8817 - trunk/python_modules/plearn/learners
Message-ID: <200804151852.m3FIqxeW024943@sheep.berlios.de>

Author: louradou
Date: 2008-04-15 20:52:59 +0200 (Tue, 15 Apr 2008)
New Revision: 8817

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:


Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2008-04-15 17:07:13 UTC (rev 8816)
+++ trunk/python_modules/plearn/learners/SVM.py	2008-04-15 18:52:59 UTC (rev 8817)
@@ -1380,8 +1380,8 @@
     def crossvalid( self,
                     dataspec,
                     param = None ):
-        self.validtype = 'cross'
         n_fold = self.n_fold
+        self.validtype = '%s-fold' % n_fold
         if not param:
             param = self.best_param
 
@@ -1452,6 +1452,8 @@
 
         expert = self.get_expert( self.kernel_type )
         expert.verbosity = self.verbosity
+        
+        L0=len(expert.trials_param_list)
 
         # HyperParamOracle__kernel.best_param is None just at the __init__
         if expert.best_param  == None:
@@ -1477,7 +1479,7 @@
             else:
 
                 # Cross Validation
-                if self.validtype == 'cross':
+                if 'fold' in self.validtype:
                     self.train( dataspec )
 
                 # Simple Validation
@@ -1530,7 +1532,7 @@
                 self.write_results( self.best_param,
                                     self.valid_stats, self.test_stats, self.train_stats )
 
-            if len(expert.trials_param_list) >= self.max_ntrials:
+            if len(expert.trials_param_list)-L0 >= self.max_ntrials:
                 return dataspec
 
         if( self.retrain_until_local_optimum_is_found



From louradou at mail.berlios.de  Wed Apr 16 00:23:05 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Wed, 16 Apr 2008 00:23:05 +0200
Subject: [Plearn-commits] r8818 - trunk/python_modules/plearn/learners
Message-ID: <200804152223.m3FMN5id016673@sheep.berlios.de>

Author: louradou
Date: 2008-04-16 00:23:03 +0200 (Wed, 16 Apr 2008)
New Revision: 8818

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:


Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2008-04-15 18:52:59 UTC (rev 8817)
+++ trunk/python_modules/plearn/learners/SVM.py	2008-04-15 22:23:03 UTC (rev 8818)
@@ -122,7 +122,7 @@
                 print "WARNING: get_data_stats() takes default value for input stats."
             return ( 2, 1. )
 
-        if self.verbosity > 0:
+        if self.verbosity > 1:
             print "  (computing input stats)"
         
         self.inputsize = len(samples[0])
@@ -131,7 +131,7 @@
             if( self.input_avgstd < 0.5
             or  self.input_avgstd > 10.
             or  std__std_per_component/self.input_avgstd > 0.1 ):
-                print "Warning in SVMHyperParamOracle__kernel::get_input_stats() " + \
+                print "WARNING in SVMHyperParamOracle__kernel::get_input_stats() " + \
                     "\n\tYour data does not seem to be normalized: " + \
                     "\n\t(E[std_comp] = %.2f, std[std_comp] = %.2f)" % \
                     ( self.input_avgstd, std__std_per_component )
@@ -731,7 +731,8 @@
         self.validtype       = 'simple'
 
         self.n_fold   = 5
-        self.balanceC = True
+        self.balanceC = False
+        self.balance_classes = False
         self.normalize_inputs = False
 
         self.HyperParamOracle__linear  = SVMHyperParamOracle__linear()
@@ -756,6 +757,8 @@
         self.stats_are_uptodate = False
         self.inputsize = None
         self.input_avgstd = None
+        self.input_means = None
+        self.input_stds = None
         self.nclasses = None
         self.class_priors = None
         self.weight = None
@@ -786,6 +789,8 @@
         self.test_stats      = None
         self.train_stats     = None
         self.stats_are_uptodate = False
+        self.input_means = None
+        self.input_stds = None
         # Note: when we forget, we keep the value of
         #       'self.best_param'. This allow to initialize
         #       a new search (when data changed a bit) to
@@ -808,6 +813,11 @@
             return None
         return dataspec[ self.testset_key ]
 
+    def additional_preproc(self, input_vmat, isTrain=False):
+        if self.balance_classes and isTrain:
+            return ReplicateSamplesVMatrix(source = input_vmat, operate_on_bags = True)
+        return input_vmat
+
     ## specific to libsvm
     """ Return samples and targets in the format required
         by libsvm, i.e. lists of float.
@@ -834,9 +844,10 @@
         return samples, targets
 
     def get_svminputlist(self, input_vmat, isTrain=False):
+        input_vmat = self.additional_preproc( input_vmat, isTrain )
         samples, targets = self.get_datalist( input_vmat )
         if self.normalize_inputs:
-            if isTrain:
+            if self.input_means == None:
                 self.input_means, self.input_stds = normalize_data(samples)
             else:
                 assert self.input_means
@@ -866,7 +877,7 @@
                      self.inputsize, self.input_avgstd )
         assert vmat <> None
 
-        samples, targets = self.get_svminputlist( vmat, True )
+        samples, targets = self.get_svminputlist( vmat )
 
         self.all_experts[0].verbosity = self.verbosity
         self.inputsize, self.input_avgstd = self.all_experts[0].get_input_stats(samples)
@@ -928,8 +939,13 @@
         return self.get_cost( stats, maincost_name )
     def get_all_costs(self, stats):
         allcosts={}
+        allNones = True
         for cost_name in self.costnames:
             allcosts[cost_name] = self.get_cost(stats, cost_name)
+            if allNones and allcosts[cost_name] <> None:
+                allNones = False
+        if allNones:
+            return None
         return allcosts
 
     """ Return a svm_parameter in the format for libsvm.
@@ -1042,7 +1058,7 @@
                     continue
 
                 costnames_string += "E[%s.E[%s]] " % (dataset, cn)
-                if cn in costs:
+                if costs <> None and cn in costs:
                     costvalues_string += "%s " % costs[cn]
                 else:
                     costvalues_string += "None "
@@ -1380,21 +1396,21 @@
     def crossvalid( self,
                     dataspec,
                     param = None ):
+        if not param:
+            param = self.best_param
+        nclasses = self.nclasses
         n_fold = self.n_fold
         self.validtype = '%s-fold' % n_fold
-        if not param:
-            param = self.best_param
-
         if self.verbosity > 0:
             print "\n** %d-fold Cross Validation" % n_fold
             print "   with param %s" % param
         
         trainset = self.train_inputspec(dataspec)
-        trainset_class = [None]*self.nclasses
-        N=[0]*self.nclasses
-        Nfold=[0]*self.nclasses
-        Nlastfold=[0]*self.nclasses
-        for c in range(self.nclasses):
+        trainset_class = [None]*nclasses
+        N=[0]*nclasses
+        Nfold=[0]*nclasses
+        Nlastfold=[0]*nclasses
+        for c in range(nclasses):
             trainset_class[c] = ClassSubsetVMatrix( source = trainset,
                                                     classes = [c],)
             N[c] = trainset_class[c].length
@@ -1405,10 +1421,10 @@
             Nlastfold[c] = N[c] - Nfold[c] * (n_fold-1)
         
         validstats = None
-        sub_trainset_class = [None]*self.nclasses
-        sub_testset_class = [None]*self.nclasses
+        sub_trainset_class = [None]*nclasses
+        sub_testset_class = [None]*nclasses
         for i in range(n_fold):
-            for c in range(self.nclasses):
+            for c in range(nclasses):
                 if i < n_fold-1:
                     test_indices = range( i*Nfold[c], (i+1)*Nfold[c] )
                     train_indices = range( 0,i*Nfold[c])+range((i+1)*Nfold[c], N[c] )
@@ -1467,8 +1483,6 @@
 
             # No improvement measured
             if not self.update_trials( param, valid_stats ):
-                if self.verbosity > 0:
-                    print " -- valid costs: ", self.get_all_costs( valid_stats )
 
                 # We reject the model (to avoid testing on it)
                 self.model = None



From chapados at mail.berlios.de  Wed Apr 16 21:14:53 2008
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Wed, 16 Apr 2008 21:14:53 +0200
Subject: [Plearn-commits] r8819 - trunk/plearn/var
Message-ID: <200804161914.m3GJErEv013860@sheep.berlios.de>

Author: chapados
Date: 2008-04-16 21:14:53 +0200 (Wed, 16 Apr 2008)
New Revision: 8819

Modified:
   trunk/plearn/var/Func.cc
Log:
Call recomputeParents() upon build to ensure that the func computes the right function.

Modified: trunk/plearn/var/Func.cc
===================================================================
--- trunk/plearn/var/Func.cc	2008-04-15 22:23:03 UTC (rev 8818)
+++ trunk/plearn/var/Func.cc	2008-04-16 19:14:53 UTC (rev 8819)
@@ -167,6 +167,7 @@
         bproppath = propagationPath(inputs, outputs);
 
     parentspath = propagationPathToParentsOfPath(inputs, outputs);
+    recomputeParents();
 
     //parameters_to_optimize.printNames();
     //cout<<"**************Func::printInfo(inputs, outputs);"<<endl;



From chapados at mail.berlios.de  Wed Apr 16 21:15:31 2008
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Wed, 16 Apr 2008 21:15:31 +0200
Subject: [Plearn-commits] r8820 - trunk/plearn/db
Message-ID: <200804161915.m3GJFVJm013990@sheep.berlios.de>

Author: chapados
Date: 2008-04-16 21:15:30 +0200 (Wed, 16 Apr 2008)
New Revision: 8820

Modified:
   trunk/plearn/db/getDataSet.h
Log:
Added a few missing includes to getDataSet.h

Modified: trunk/plearn/db/getDataSet.h
===================================================================
--- trunk/plearn/db/getDataSet.h	2008-04-16 19:14:53 UTC (rev 8819)
+++ trunk/plearn/db/getDataSet.h	2008-04-16 19:15:30 UTC (rev 8820)
@@ -44,6 +44,9 @@
 #ifndef getDataSet_INC
 #define getDataSet_INC
 
+#include <string>
+#include <time.h>
+#include <plearn/vmat/VMat.h>
 
 namespace PLearn {
 using namespace std;



From nouiz at mail.berlios.de  Wed Apr 16 22:57:30 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 16 Apr 2008 22:57:30 +0200
Subject: [Plearn-commits] r8821 - trunk
Message-ID: <200804162057.m3GKvUAx023477@sheep.berlios.de>

Author: nouiz
Date: 2008-04-16 22:57:30 +0200 (Wed, 16 Apr 2008)
New Revision: 8821

Modified:
   trunk/pymake.config.model
Log:
put lapack as an optionalLibrary for goto and defblas options


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2008-04-16 19:15:30 UTC (rev 8820)
+++ trunk/pymake.config.model	2008-04-16 20:57:30 UTC (rev 8821)
@@ -780,7 +780,7 @@
 
 pymakeLinkOption( name = 'defblas',
               description = 'linking with default BLAS',
-              linkeroptions = lapack_linkeroptions + ' ' + blas_linkeroptions
+              linkeroptions = blas_linkeroptions
               )
 
 pymakeLinkOption( name = 'nolibblas',
@@ -823,9 +823,19 @@
 
 pymakeLinkOption( name = 'goto',
               description = 'linking using GOTO lib for BLAS',
-              linkeroptions = '-L' + libdir +'goto -llapack -lgoto -lgfortran'
+              linkeroptions = '-L' + libdir +'goto -lgoto -lgfortran'
               )
 
+tmp = getOptions(options_choices, optionargs[:])
+if 'goto' in tmp:
+    optionalLibrary( name = 'gotolapack',
+                 triggers = 'lapack_proto.h',
+                 linkeroptions   = '-llapack')
+elif 'defblas' in tmp:
+    optionalLibrary( name = 'deflapack',
+                     triggers = 'lapack_proto.h',
+                     linkeroptions   = lapack_linkeroptions)
+
 ## We must link again the static version of lapack as the dynamic version is linked again the default version of blas
 ## and we don't want to link again it. Also, we must remove fonction from lapack as some of them are also in GOTO
 pymakeLinkOption( name = 'lisa',



From chapados at mail.berlios.de  Wed Apr 16 23:22:52 2008
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Wed, 16 Apr 2008 23:22:52 +0200
Subject: [Plearn-commits] r8822 - trunk/plearn/db
Message-ID: <200804162122.m3GLMqMc026053@sheep.berlios.de>

Author: chapados
Date: 2008-04-16 23:22:52 +0200 (Wed, 16 Apr 2008)
New Revision: 8822

Modified:
   trunk/plearn/db/getDataSet.h
Log:
Took out include of VMat.h -- however, a user must include VMat.h by hand before calling getDataSet

Modified: trunk/plearn/db/getDataSet.h
===================================================================
--- trunk/plearn/db/getDataSet.h	2008-04-16 20:57:30 UTC (rev 8821)
+++ trunk/plearn/db/getDataSet.h	2008-04-16 21:22:52 UTC (rev 8822)
@@ -45,8 +45,8 @@
 #define getDataSet_INC
 
 #include <string>
+#include <map>
 #include <time.h>
-#include <plearn/vmat/VMat.h>
 
 namespace PLearn {
 using namespace std;



From saintmlx at mail.berlios.de  Thu Apr 17 00:38:13 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 17 Apr 2008 00:38:13 +0200
Subject: [Plearn-commits] r8823 - trunk/plearn/python
Message-ID: <200804162238.m3GMcD1v011408@sheep.berlios.de>

Author: saintmlx
Date: 2008-04-17 00:38:12 +0200 (Thu, 17 Apr 2008)
New Revision: 8823

Modified:
   trunk/plearn/python/PythonExtension.cc
Log:
- new remote function to suppress pout output



Modified: trunk/plearn/python/PythonExtension.cc
===================================================================
--- trunk/plearn/python/PythonExtension.cc	2008-04-16 21:22:52 UTC (rev 8822)
+++ trunk/plearn/python/PythonExtension.cc	2008-04-16 22:38:12 UTC (rev 8823)
@@ -447,6 +447,24 @@
 
 PyObject* the_PLearn_python_module= 0;
 
+
+
+
+void setNullPout()
+{
+    pout= pnull;
+}
+
+BEGIN_DECLARE_REMOTE_FUNCTIONS
+
+    declareFunction("setNullPout", &setNullPout,
+                    (BodyDoc("Sets the pout output stream to be null.\n")));
+
+END_DECLARE_REMOTE_FUNCTIONS
+        
+
+
+
 } // end of namespace PLearn
 
 



From saintmlx at mail.berlios.de  Thu Apr 17 00:40:53 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 17 Apr 2008 00:40:53 +0200
Subject: [Plearn-commits] r8824 - in trunk: plearn/base
	python_modules/plearn/pybridge
Message-ID: <200804162240.m3GMeq2c016049@sheep.berlios.de>

Author: saintmlx
Date: 2008-04-17 00:40:51 +0200 (Thu, 17 Apr 2008)
New Revision: 8824

Modified:
   trunk/plearn/base/Object.cc
   trunk/plearn/base/Object.h
   trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
Log:
- new asStringRemoteTransmit function to serialize w/ remotetransmit options
- changed default for non-PLearn options warning: no warning (can be changed at run-time)



Modified: trunk/plearn/base/Object.cc
===================================================================
--- trunk/plearn/base/Object.cc	2008-04-16 22:38:12 UTC (rev 8823)
+++ trunk/plearn/base/Object.cc	2008-04-16 22:40:51 UTC (rev 8824)
@@ -145,6 +145,16 @@
     return removeblanks(s);
 }
 
+string Object::asStringRemoteTransmit() const
+{
+    string s;
+    PStream out= openString(s, PStream::plearn_ascii, "w");
+    out.remote_plearn_comm= true;
+    out << this;
+    out.flush();
+    return removeblanks(s);
+}
+
 //#####  Option-Manipulation Functions  #######################################
 
 void Object::setOption(const string& optionname, const string& value)
@@ -707,6 +717,11 @@
                   (BodyDoc("Returns a string representation of this object."),
                    RetDoc ("string representation of the object")));
 
+    declareMethod(rmm, "asStringRemoteTransmit", &Object::asStringRemoteTransmit,
+                  (BodyDoc("Returns a string representation of this object,"
+                           " including remotetransmit options."),
+                   RetDoc ("string representation of the object (w/remotetransmit)")));
+
     declareMethod(rmm, "run", &Object::run,
                   (BodyDoc("Run the given object, if it is runnable; "
                            "raise an exception if it is not.")));

Modified: trunk/plearn/base/Object.h
===================================================================
--- trunk/plearn/base/Object.h	2008-04-16 22:38:12 UTC (rev 8823)
+++ trunk/plearn/base/Object.h	2008-04-16 22:40:51 UTC (rev 8824)
@@ -692,6 +692,11 @@
      */
     virtual string asString() const; 
 
+    /**
+     * Returns a string representation of the object, including "remotetransmit" options
+     */
+    virtual string asStringRemoteTransmit() const; 
+
     //#####  Options-Related Functions  #######################################
     
     /**

Modified: trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
===================================================================
--- trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2008-04-16 22:38:12 UTC (rev 8823)
+++ trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2008-04-16 22:40:51 UTC (rev 8824)
@@ -36,11 +36,30 @@
 
 global plearn_module
 plearn_module= None
+def get_plearn_module():
+    global plearn_module
+    return plearn_module
 
+# remote pickle: when true, __getstate__ includes remotetransmit options;
+#                    used to transmit objects to remote processes
+#                when false, __getstate__ does not include nosave options;
+#                    used to save objects to disk
+global remote_pickle
+remote_pickle= False
+def get_remote_pickle():
+    global remote_pickle
+    return remote_pickle
+def set_remote_pickle(rp):
+    global remote_pickle
+    prev= remote_pickle
+    remote_pickle= rp
+    return prev
+
+
 class WrappedPLearnObject(object):
 
     allowed_non_PLearn_options= ['_cptr']
-    warn_non_PLearn_options= True
+    warn_non_PLearn_options= False # you can turn this on to help debugging
 
     def __init__(self, **kwargs):
         #print 'WrappedPLearnObject.__init__',type(self),kwargs
@@ -106,7 +125,10 @@
 
     def __getstate__(self):
         d= self.__dict__.copy()
-        d['_cptr']= self.asString()
+        if remote_pickle:
+            d['_cptr']= self.asStringRemoteTransmit()
+        else:
+            d['_cptr']= self.asString()
         return d
     
     def __setstate__(self, dict):



From louradou at mail.berlios.de  Thu Apr 17 02:05:02 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Thu, 17 Apr 2008 02:05:02 +0200
Subject: [Plearn-commits] r8825 - trunk/python_modules/plearn/learners
Message-ID: <200804170005.m3H052fD028958@sheep.berlios.de>

Author: louradou
Date: 2008-04-17 02:05:01 +0200 (Thu, 17 Apr 2008)
New Revision: 8825

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:
outputs_type for one-vs-all multiclass_strategy:
limited to best ones ('proba' that was
'proba2' in the previous version, 'dist' and 'votes'='onehot')



Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2008-04-16 22:40:51 UTC (rev 8824)
+++ trunk/python_modules/plearn/learners/SVM.py	2008-04-17 00:05:01 UTC (rev 8825)
@@ -1252,37 +1252,23 @@
                     for c in range(nclasses):
                         prd, prb = model[c].predict_probability(x)
                         output[c] = prb[1]
-                    outputs.append(output)
-            elif self.outputs_type == 'proba2':
-                for x in samples:
-                    output = [0]*nclasses
-                    for c in range(nclasses):
-                        prd, prb = model[c].predict_probability(x)
-                        output[c] = prb[1]
                     outputs.append( softmax(output) )
             elif self.outputs_type == 'onehot' or self.outputs_type == 'votes':
                 for x in samples:
                     output = [0]*nclasses
                     svm_outputs = zeros(nclasses)
                     for c in range(nclasses):
-                        onevsone_dict = model[c].predict_values(x)
-                        svm_outputs[c] = onevsone_dict[(1,-1)]
+                        onevsall_dict = model[c].predict_values(x)
+                        svm_outputs[c] = onevsall_dict[(1,-1)]
                     output[ svm_outputs.argmax() ] += 1
                     outputs.append( output )                
             elif self.outputs_type == 'dist':
                 for x in samples:
                     output = [0]*nclasses
                     for c in range(nclasses):
-                        onevsone_dict = model[c].predict_values(x)
-                        output[c] = onevsone_dict[(1,-1)]
+                        onevsall_dict = model[c].predict_values(x)
+                        output[c] = onevsall_dict[(1,-1)]
                     outputs.append( output )
-            elif self.outputs_type == 'dist2':
-                for x in samples:
-                    output = [0]*nclasses
-                    for c in range(nclasses):
-                        onevsone_dict = model[c].predict_values(x)
-                        output[c] = onevsone_dict[(1,-1)]
-                    outputs.append( softmax(output) )
             else:
                 raise ValueError, "Unknown value %s for option 'outputs_type'" % self.outputs_type
         else:



From nouiz at mail.berlios.de  Thu Apr 17 15:39:26 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 17 Apr 2008 15:39:26 +0200
Subject: [Plearn-commits] r8826 - in trunk/plearn_learners/regressors: .
	test/RegressionTree/.pytest/PL_RegressionTree/expected_results
	test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir
	test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results
	test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir
Message-ID: <200804171339.m3HDdQaK005809@sheep.berlios.de>

Author: nouiz
Date: 2008-04-17 15:39:25 +0200 (Thu, 17 Apr 2008)
New Revision: 8826

Modified:
   trunk/plearn_learners/regressors/RegressionTree.cc
   trunk/plearn_learners/regressors/RegressionTree.h
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/RUN.log
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/RUN.log
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave
Log:
-added logging of split value.
-Modified test in consequence


Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2008-04-17 00:05:01 UTC (rev 8825)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2008-04-17 13:39:25 UTC (rev 8826)
@@ -106,10 +106,14 @@
                   "The heap to store potential nodes to expand\n");
     declareOption(ol, "first_leave", &RegressionTree::first_leave, OptionBase::learntoption,
                   "The first leave built with the root containing all train set rows at the beginning\n");
-    declareOption(ol, "split_cols", &RegressionTree::split_cols, OptionBase::learntoption,
-                  "contain in order of first to last the columns used to split the tree.\n");
+    declareOption(ol, "split_cols", &RegressionTree::split_cols,
+                  OptionBase::learntoption,
+                  "Contain in order of addition of node the columns used to"
+                  " split the tree.\n");
+    declareOption(ol, "split_values", &RegressionTree::split_values,
+                  OptionBase::learntoption,
+                  "Contain in order of addition of node the split value.\n");
 
-
     declareOption(ol, "first_leave_output", &RegressionTree::tmp_vec,
                   OptionBase::learntoption | OptionBase::nosave,
                   "DEPRECATED\n");
@@ -136,6 +140,7 @@
     deepCopyField(priority_queue, copies);
     deepCopyField(first_leave, copies);
     deepCopyField(split_cols, copies);
+    deepCopyField(split_values, copies);
     deepCopyField(tmp_vec, copies);
     
 }
@@ -203,9 +208,10 @@
     {    
         if (stage > 0)
         {
-            int split_col = expandTree();
-            if (split_col < 0) break;
-            split_cols.append(split_col);
+            PP<RegressionTreeNode> node= expandTree();
+            if (node == NULL) break;
+            split_cols.append(node->getSplitCol());
+            split_values.append(node->getSplitValue());
         }
         if (report_progress) pb->update(stage);
     }
@@ -233,6 +239,7 @@
     }
     train_stats->finalize();
     verbose("split_cols: "+tostring(split_cols),2);
+    verbose("split_values: "+tostring(split_values),2);
 }
 
 void RegressionTree::verbose(string the_msg, int the_level)
@@ -295,12 +302,12 @@
     priority_queue->addHeap(root);
 }
 
-int RegressionTree::expandTree()
+PP<RegressionTreeNode> RegressionTree::expandTree()
 {
     if (priority_queue->isEmpty() <= 0)
     {
         verbose("RegressionTree: priority queue empty, stage: " + tostring(stage), 3);
-        return -1;
+        return NULL;
     }
     PP<RegressionTreeNode> node = priority_queue->popHeap();
     if (node->getErrorImprovment() < complexity_penalty_factor * sqrt((real)stage))
@@ -308,19 +315,19 @@
         verbose("RegressionTree: early stopping at stage: " + tostring(stage)
                 + ", error improvement: " + tostring(node->getErrorImprovment())
                 + ", penalty: " + tostring(complexity_penalty_factor * sqrt((real)stage)), 3);
-        return -1;
+        return NULL;
     }
     int split_col = node->expandNode();
     if (split_col < 0)
     {
         verbose("RegressionTree: expand is negative?", 3);
-        return -1;
+        return NULL;
     }
     TVec< PP<RegressionTreeNode> > subnode = node->getNodes();
     priority_queue->addHeap(subnode[0]); 
     priority_queue->addHeap(subnode[1]);
     if (missing_is_valid) priority_queue->addHeap(subnode[2]);
-    return split_col; 
+    return node; 
 }
 
 int RegressionTree::outputsize() const

Modified: trunk/plearn_learners/regressors/RegressionTree.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.h	2008-04-17 00:05:01 UTC (rev 8825)
+++ trunk/plearn_learners/regressors/RegressionTree.h	2008-04-17 13:39:25 UTC (rev 8826)
@@ -88,6 +88,7 @@
     real l2_loss_function_factor;
     real l1_loss_function_factor;
     TVec<int> split_cols;
+    Vec       split_values;
     TVec<PP<RegressionTreeNode> > *nodes;
 
     Vec tmp_vec;
@@ -114,10 +115,10 @@
     virtual void         computeCostsFromOutputs(const Vec& input, const Vec& output, const Vec& target, Vec& costs) const;
   
 private:
-    void         build_();
-    void         initialiseTree();
-    int          expandTree();
-    void         verbose(string msg, int level);
+    void                   build_();
+    void                   initialiseTree();
+    PP<RegressionTreeNode> expandTree();
+    void                   verbose(string msg, int level);
 };
 
 DECLARE_OBJECT_PTR(RegressionTree);

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/RUN.log	2008-04-17 00:05:01 UTC (rev 8825)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/RUN.log	2008-04-17 13:39:25 UTC (rev 8826)
@@ -1,5 +1,9 @@
 HyperLearner: starting the optimization
 split_cols: []
 
+split_values: []
+
 split_cols: 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1 3 2 3 3 
+split_values: 0.036194999999999998 0.937115000000000031 -0.796699999999999964 1.44567500000000004 -1.31519999999999992 0.432620000000000005 -0.34517500000000001 24.753779999999999 -1.16121499999999989 0.756380000000000052 1.2774350000000001 -1.76971500000000015 -7.35407999999999973 -0.0821300000000000086 1.77604000000000006 0.26894499999999999 -2.19003499999999995 -6.68825000000000003 -4.74906000000000006 3.03715500000000027 
 split_cols: 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1 3 2 3 3 3 3 3 1 3 3 1 2 3 3 3 3 1 3 1 3 3 1 3 3 
+split_values: 0.036194999999999998 0.937115000000000031 -0.796699999999999964 1.44567500000000004 -1.31519999999999992 0.432620000000000005 -0.34517500000000001 24.753779999999999 -1.16121499999999989 0.756380000000000052 1.2774350000000001 -1.76971500000000015 -7.35407999999999973 -0.0821300000000000086 1.77604000000000006 0.26894499999999999 -2.19003499999999995 -6.68825000000000003 -4.74906000000000006 3.03715500000000027 -0.0792649999999999744 -0.349470000000000003 0.852400000000000047 -0.581860000000000044 -1.15891499999999992 2.82453999999999983 -0.617779999999999996 5.85068500000000036 -0.972770000000000024 0.287649999999999961 3.08020000000000005 0.591400000000000037 -1.43682500000000002 1.28273500000000018 1.58981499999999998 -2.12482499999999996 1.45652000000000004 -1.04829499999999998 -1.65359500000000015 -1.10694000000000004 

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt	2008-04-17 00:05:01 UTC (rev 8825)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt	2008-04-17 13:39:25 UTC (rev 8826)
@@ -1,2 +1,2 @@
-__REVISION__ = "PL8677"
+__REVISION__ = "PL8793"
 data                                          = PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave	2008-04-17 00:05:01 UTC (rev 8825)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave	2008-04-17 13:39:25 UTC (rev 8826)
@@ -18,6 +18,7 @@
 fistart = -1 ;
 flength = -1 ;
 source = *2  ;
+deep_copy_source = 1 ;
 writable = 0 ;
 length = 200 ;
 width = 5 ;
@@ -82,6 +83,8 @@
 first_leave = *0 ;
 split_cols = []
 ;
+split_values = []
+;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 0 ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/RUN.log	2008-04-17 00:05:01 UTC (rev 8825)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/RUN.log	2008-04-17 13:39:25 UTC (rev 8826)
@@ -1,6 +1,11 @@
 HyperLearner: starting the optimization
 split_cols: []
 
+split_values: []
+
 split_cols: 1 
+split_values: 0.14412705026016423 
 split_cols: 1 0 
+split_values: 0.14412705026016423 2.81501972474946704 
 split_cols: 1 0 0 
+split_values: 0.14412705026016423 2.81501972474946704 3.90430461537466744 

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt	2008-04-17 00:05:01 UTC (rev 8825)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt	2008-04-17 13:39:25 UTC (rev 8826)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL8649"
+__REVISION__ = "PL8793"
 datatest                                      = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat
 datatrain                                     = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave	2008-04-17 00:05:01 UTC (rev 8825)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave	2008-04-17 13:39:25 UTC (rev 8826)
@@ -97,6 +97,8 @@
 first_leave = *0 ;
 split_cols = []
 ;
+split_values = []
+;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 0 ;



From tihocan at mail.berlios.de  Thu Apr 17 16:04:42 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 17 Apr 2008 16:04:42 +0200
Subject: [Plearn-commits] r8827 - trunk/plearn_learners/regressors
Message-ID: <200804171404.m3HE4gNN007955@sheep.berlios.de>

Author: tihocan
Date: 2008-04-17 16:04:42 +0200 (Thu, 17 Apr 2008)
New Revision: 8827

Modified:
   trunk/plearn_learners/regressors/RegressionTree.cc
Log:
Fixed compilation issue

Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2008-04-17 13:39:25 UTC (rev 8826)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2008-04-17 14:04:42 UTC (rev 8827)
@@ -211,7 +211,8 @@
             PP<RegressionTreeNode> node= expandTree();
             if (node == NULL) break;
             split_cols.append(node->getSplitCol());
-            split_values.append(node->getSplitValue());
+            //split_values.append(node->getSplitValue());
+            PLCHECK_MSG( false, "Fred, please check this!" );
         }
         if (report_progress) pb->update(stage);
     }



From nouiz at mail.berlios.de  Thu Apr 17 16:29:48 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 17 Apr 2008 16:29:48 +0200
Subject: [Plearn-commits] r8828 - trunk/plearn_learners/regressors
Message-ID: <200804171429.m3HETm5I010476@sheep.berlios.de>

Author: nouiz
Date: 2008-04-17 16:29:47 +0200 (Thu, 17 Apr 2008)
New Revision: 8828

Modified:
   trunk/plearn_learners/regressors/RegressionTree.cc
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
   trunk/plearn_learners/regressors/RegressionTreeNode.h
Log:
commited modification that was forgoted in the last commit


Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2008-04-17 14:04:42 UTC (rev 8827)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2008-04-17 14:29:47 UTC (rev 8828)
@@ -211,8 +211,7 @@
             PP<RegressionTreeNode> node= expandTree();
             if (node == NULL) break;
             split_cols.append(node->getSplitCol());
-            //split_values.append(node->getSplitValue());
-            PLCHECK_MSG( false, "Fred, please check this!" );
+            split_values.append(node->getSplitValue());
         }
         if (report_progress) pb->update(stage);
     }

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-04-17 14:04:42 UTC (rev 8827)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-04-17 14:29:47 UTC (rev 8828)
@@ -217,8 +217,7 @@
 {
     if(leave->length<=1)
         return;
-    TVec<int> candidate(train_set->length());//list of candidate row to split
-    candidate.resize(0);
+    TVec<int> candidate(0,train_set->length());//list of candidate row to split
     TVec<int> registered_row;
     tmp_vec.resize(2);
     Vec left_error(3);
@@ -365,6 +364,11 @@
     return split_col;
 }
 
+real RegressionTreeNode::getSplitValue() const
+{
+    return split_feature_value;
+}
+
 TVec< PP<RegressionTreeNode> > RegressionTreeNode::getNodes()
 {
     TVec< PP<RegressionTreeNode> > return_value;

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.h	2008-04-17 14:04:42 UTC (rev 8827)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.h	2008-04-17 14:29:47 UTC (rev 8828)
@@ -105,6 +105,7 @@
     int          getSplitBalance()const;
     real         getErrorImprovment()const;
     int          getSplitCol() const;
+    real         getSplitValue() const;
     TVec< PP<RegressionTreeNode> >  getNodes();
     void         computeOutputAndNodes(const Vec& inputv, Vec& outputv, TVec<PP<RegressionTreeNode> >* nodes=0);
     void         computeOutput(const Vec& inputv, Vec& outputv)
@@ -115,7 +116,7 @@
     
 private:
     void         build_();
-    void         verbose(string msg, int level);    
+    void         verbose(string msg, int level); 
 };
 
 DECLARE_OBJECT_PTR(RegressionTreeNode);



From tihocan at mail.berlios.de  Thu Apr 17 17:34:38 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 17 Apr 2008 17:34:38 +0200
Subject: [Plearn-commits] r8829 - trunk/plearn/vmat
Message-ID: <200804171534.m3HFYcM5014855@sheep.berlios.de>

Author: tihocan
Date: 2008-04-17 17:34:37 +0200 (Thu, 17 Apr 2008)
New Revision: 8829

Modified:
   trunk/plearn/vmat/FilteredVMatrix.cc
   trunk/plearn/vmat/FilteredVMatrix.h
Log:
- Can now work even when no metadatadir is available
- Minor fixes to the code to be more coherent with the rest of the PLearn code


Modified: trunk/plearn/vmat/FilteredVMatrix.cc
===================================================================
--- trunk/plearn/vmat/FilteredVMatrix.cc	2008-04-17 14:29:47 UTC (rev 8828)
+++ trunk/plearn/vmat/FilteredVMatrix.cc	2008-04-17 15:34:37 UTC (rev 8829)
@@ -46,91 +46,117 @@
 namespace PLearn {
 using namespace std;
 
+PLEARN_IMPLEMENT_OBJECT(
+    FilteredVMatrix,
+    "A filtered view of its source VMatrix.",
+    "The filter is an expression in VPL language.\n"
+    "If a metadata directory is provided, the filtered indexes are saved in\n"
+    "this directory. Otherwise the filtered indexes are re-computed at each\n"
+    "call to build()."
+);
 
-FilteredVMatrix::FilteredVMatrix()
-    : inherited(),
-      build_complete(false),
-      allow_repeat_rows(false),
-      repeat_id_field_name(""),
-      repeat_count_field_name(""),
-      report_progress(true)
-{
-}
+/////////////////////
+// FilteredVMatrix //
+/////////////////////
+FilteredVMatrix::FilteredVMatrix():
+    build_complete(false),
+    allow_repeat_rows(false),
+    repeat_id_field_name(""),
+    repeat_count_field_name(""),
+    report_progress(true)
+{}
 
 FilteredVMatrix::FilteredVMatrix( VMat the_source, const string& program_string,
                                   const PPath& the_metadatadir, bool the_report_progress,
                                   bool allow_repeat_rows_, 
                                   const string& repeat_id_field_name_,
-                                  const string& repeat_count_field_name_)
+                                  const string& repeat_count_field_name_,
+                                  bool call_build_):
+    inherited(the_source, call_build_),
+    allow_repeat_rows(allow_repeat_rows_),
+    repeat_id_field_name(repeat_id_field_name_),
+    repeat_count_field_name(repeat_count_field_name_),
+    report_progress(the_report_progress),
+    prg(program_string)
+{
+    // Note that although VMatrix::build_ would be tempted to call
+    // setMetaDataDir when inherited(the_source, true) is called above (if
+    // call_build_ is true), the metadatadir is only set below, so it will not
+    // happen.
+    metadatadir = the_metadatadir;
 
-    : SourceVMatrix(the_source),
-      allow_repeat_rows(allow_repeat_rows_),
-      repeat_id_field_name(repeat_id_field_name_),
-      repeat_count_field_name(repeat_count_field_name_),
-      report_progress(the_report_progress),
-      prg(program_string)
+    if (call_build_)
+        build_();
+}
 
+////////////////////////////
+// computeFilteredIndices //
+////////////////////////////
+void FilteredVMatrix::computeFilteredIndices()
 {
-    metadatadir = the_metadatadir;
-    build_();
+    int l = source.length();
+    Vec result(1);
+    PP<ProgressBar> pb;
+    if (report_progress)
+        pb = new ProgressBar("Filtering source vmat", l);
+    mem_indices.resize(0);
+    for(int i=0; i<l; i++)
+    {
+        if (report_progress)
+            pb->update(i);
+        program.run(i,result);
+        if(!allow_repeat_rows)
+        {
+            if(!fast_exact_is_equal(result[0], 0))
+                mem_indices.append(i);
+        }
+        else
+            for(int x = int(round(result[0])); x > 0; --x)
+                mem_indices.append(i);
+    }
+    length_ = mem_indices.length();
 }
 
-PLEARN_IMPLEMENT_OBJECT(FilteredVMatrix, "A filtered view of its source vmatrix",
-                        "The filter is an exression in VPL language.\n"
-                        "The filtered indexes are saved in the metadata directory, that NEEDS to\n"
-                        "be provided.\n" );
-
-
-
+///////////////
+// openIndex //
+///////////////
 void FilteredVMatrix::openIndex()
 {
     PLASSERT(hasMetaDataDir());
-    string idxfname = getMetaDataDir() / "filtered.idx";
+
+    PPath idxfname = getMetaDataDir() / "filtered.idx";
     if(!force_mkdir(getMetaDataDir()))
-        PLERROR("In FilteredVMatrix::openIndex could not create directory %s",getMetaDataDir().absolute().c_str());
+        PLERROR("In FilteredVMatrix::openIndex - Could not create "
+                "directory %s", getMetaDataDir().absolute().c_str());
 
-
     lockMetaDataDir();
     if(isUpToDate(idxfname))
-        indexes.open(idxfname);
+        indexes.open(idxfname.absolute());
     else  // let's (re)create the index
     {
+        computeFilteredIndices();
         rm(idxfname);       // force remove it
-        int l = source.length();
-        Vec result(1);
-        indexes.open(idxfname,true);
-        PP<ProgressBar> pb;
-        if (report_progress)
-            pb = new ProgressBar("Filtering source vmat", l);
-        for(int i=0; i<l; i++)
-        {
-            if (report_progress)
-                pb->update(i);
-            program.run(i,result);
-            if(!allow_repeat_rows)
-            {
-                if(!fast_exact_is_equal(result[0], 0))
-                    indexes.append(i);
-            }
-            else
-                for(int x = int(round(result[0])); x > 0; --x)
-                    indexes.append(i);
-
-        }
+        indexes.open(idxfname.absolute(), true);
+        for (int i = 0; i < mem_indices.length(); i++)
+            indexes.append(i);
         indexes.close();
-        indexes.open(idxfname);
+        indexes.open(idxfname.absolute());
+        mem_indices = TVec<int>();  // Free memory.
     }
     unlockMetaDataDir();
 
     length_ = indexes.length();
 }
 
+////////////////////
+// setMetaDataDir //
+////////////////////
 void FilteredVMatrix::setMetaDataDir(const PPath& the_metadatadir)
 {
     inherited::setMetaDataDir(the_metadatadir);
     if (build_complete) {
         // Only call openIndex() if the build has been completed,
-        // otherwise the filtering program won't be ready yet.
+        // otherwise the filtering program will not be ready yet.
         openIndex();
         // We call 'setMetaInfoFromSource' only after the index file has been
         // correctly read.
@@ -138,37 +164,51 @@
     }
 }
 
+///////////////
+// getNewRow //
+///////////////
 void FilteredVMatrix::getNewRow(int i, const Vec& v) const
 {
-    if (indexes.length() == -1)
-        PLERROR("In FilteredVMatrix::getNewRow - The filtered indexes are not set, make sure you provided a metadatadir");
+    if (mem_indices.isEmpty() && indexes.length() == -1)
+        PLERROR("In FilteredVMatrix::getNewRow - Filtered indices are not\n"
+                "set yet.");
 
     int j= source->width();
+    int idx = mem_indices.isEmpty() ? indexes[i] : mem_indices[i];
 
-    source->getRow(indexes[i],v.subVec(0, j));
+    source->getRow(idx, v.subVec(0, j));
 
-    if("" != repeat_id_field_name)
+    if(!repeat_id_field_name.empty())
     {
         int k= 1;
-        while(k <= i && indexes[i]==indexes[i-k])
+        while(k <= i && (mem_indices.isEmpty()
+                                ? indexes[i]==indexes[i-k]
+                                : mem_indices[i] == mem_indices[i-k]))
             ++k;
         v[j++]= static_cast<real>(k-1);
     }
 
-    if("" != repeat_count_field_name)
+    if(!repeat_count_field_name.empty())
     {
         int k0= 1;
-        while(k0 <= i && indexes[i]==indexes[i-k0])
+        while(k0 <= i && (mem_indices.isEmpty()
+                                ? indexes[i]==indexes[i-k0]
+                                : mem_indices[i] == mem_indices[i-k0]))
             ++k0;
         --k0;
         int k1= 1;
-        while(k1+i < length() && indexes[i]==indexes[i+k1])
+        while(k1+i < length() && (mem_indices.isEmpty()
+                                        ? indexes[i]==indexes[i+k1]
+                                        : mem_indices[i] == mem_indices[i+k1]))
             ++k1;
         v[j++]= static_cast<real>(k0+k1);
     }
 
 }
 
+////////////////////
+// declareOptions //
+////////////////////
 void FilteredVMatrix::declareOptions(OptionList& ol)
 {
     declareOption(ol, "prg", &FilteredVMatrix::prg, OptionBase::buildoption,
@@ -192,6 +232,9 @@
     inherited::declareOptions(ol);
 }
 
+////////////
+// build_ //
+////////////
 void FilteredVMatrix::build_()
 {
     if (source) {
@@ -201,21 +244,24 @@
         program.compileString(prg,fieldnames);
         build_complete = true;
         if (hasMetaDataDir())
+            // Calling setMetaDataDir() will compute indices and save them
+            // in the give metadata directory.
             setMetaDataDir(getMetaDataDir());
-        else
-            // Ensure we do not retain a previous value for length and width.
-            length_ = width_ = -1;
+        else {
+            // Compute selected indices in memory only.
+            computeFilteredIndices();
+        }
 
         Array<VMField> finfos= source->getFieldInfos().copy();
             
-        if("" != repeat_id_field_name)
+        if(!repeat_id_field_name.empty())
         {
             finfos.append(VMField(repeat_id_field_name));
             if(0 < width_)
                 ++width_;
         }
 
-        if("" != repeat_count_field_name)
+        if(!repeat_count_field_name.empty())
         {
             finfos.append(VMField(repeat_count_field_name));
             if(0 < width_)
@@ -223,14 +269,14 @@
         }
 
         setFieldInfos(finfos);
-
+        setMetaInfoFromSource();
     } else
         length_ = width_ = 0;
-
-
 }
 
-// ### Nothing to add here, simply calls build_
+///////////
+// build //
+///////////
 void FilteredVMatrix::build()
 {
     build_complete = false;
@@ -238,9 +284,13 @@
     build_();
 }
 
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
 void FilteredVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
+    deepCopyField(mem_indices, copies);
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn/vmat/FilteredVMatrix.h
===================================================================
--- trunk/plearn/vmat/FilteredVMatrix.h	2008-04-17 14:29:47 UTC (rev 8828)
+++ trunk/plearn/vmat/FilteredVMatrix.h	2008-04-17 15:34:37 UTC (rev 8829)
@@ -61,12 +61,17 @@
     //! the build is complete.
     bool build_complete;
 
-
 protected:
 
     VMatLanguage program;
-    IntVecFile indexes;  // indexes[i] is the
 
+    //! The i-th element is the index of the i-th selected item in the source
+    //! VMatrix.
+    IntVecFile indexes;
+
+    //! Indices stored in memory when no metadata directory is available.
+    TVec<int> mem_indices;
+
     bool allow_repeat_rows;
     string repeat_id_field_name; // 0, 1, ..., n-1; "" means no field is added
     string repeat_count_field_name; // n; "" means no field is added
@@ -75,6 +80,9 @@
     //! If it exists and is up to date, simply opens it.
     void openIndex();
 
+    //! Compute the filtered indices.
+    void computeFilteredIndices();
+
 public:
 
     // ************************
@@ -96,18 +104,17 @@
                     const PPath& the_metadatadir = "", bool the_report_progress = true,
                     bool allow_repeat_rows_= false, 
                     const string& repeat_id_field_name_= "",
-                    const string& repeat_count_field_name_= "");
+                    const string& repeat_count_field_name_= "",
+                    bool call_build_ = true);
 
     virtual void setMetaDataDir(const PPath& the_metadatadir);
 
 private:
     //! This does the actual building.
-    // (Please implement in .cc)
     void build_();
 
 protected:
     //! Declares this class' options
-    // (Please implement in .cc)
     static void declareOptions(OptionList& ol);
 
     //!  This is the only method requiring implementation



From saintmlx at mail.berlios.de  Thu Apr 17 18:04:01 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 17 Apr 2008 18:04:01 +0200
Subject: [Plearn-commits] r8830 - trunk/plearn_learners/regressors
Message-ID: <200804171604.m3HG41O4018285@sheep.berlios.de>

Author: saintmlx
Date: 2008-04-17 18:04:01 +0200 (Thu, 17 Apr 2008)
New Revision: 8830

Modified:
   trunk/plearn_learners/regressors/WPLS.cc
Log:
- use pout instead of cout



Modified: trunk/plearn_learners/regressors/WPLS.cc
===================================================================
--- trunk/plearn_learners/regressors/WPLS.cc	2008-04-17 15:34:37 UTC (rev 8829)
+++ trunk/plearn_learners/regressors/WPLS.cc	2008-04-17 16:04:01 UTC (rev 8830)
@@ -447,11 +447,11 @@
     if (stage == nstages) {
         // Already trained.
         if (verbosity >= 1)
-            cout << "Skipping WPLS training" << endl;
+            pout << "Skipping WPLS training" << endl;
         return;
     }
     if (verbosity >= 1)
-        cout << "WPLS training started" << endl;
+        pout << "WPLS training started" << endl;
 
     int n    = train_set->length();
     int wlen = train_set->weightsize();
@@ -460,8 +460,8 @@
     Vec means, stddev;
     computeWeightedInputOutputMeansAndStddev(d, means, stddev);
     if (verbosity >= 2) {
-        cout << "means = " << means << endl;
-        cout << "stddev = " << stddev << endl;
+        pout << "means = " << means << endl;
+        pout << "stddev = " << stddev << endl;
     }
     normalize(d, means, stddev);
     mean_input  = means.subVec(0, p);
@@ -584,7 +584,7 @@
         }
         while (stage < nstages) {
             if (verbosity >= 1)
-                cout << "stage=" << stage << endl;
+                pout << "stage=" << stage << endl;
             s << Y;
             normalize(s, 2.0);  
             finished = false;
@@ -602,9 +602,9 @@
                     finished = true;
                 else {
                     if (verbosity >= 2)
-                        cout << "dold = " << dold << endl;
+                        pout << "dold = " << dold << endl;
                     if (count%100==0 && verbosity>=1)
-                        cout << "loop counts = " << count << endl;
+                        pout << "loop counts = " << count << endl;
                 }
             }
             transposeProduct(lx, X, s);
@@ -622,21 +622,21 @@
         productTranspose(tmp_np, T, P);
         
         if (verbosity >= 2) {
-            cout << "T = " << endl << T << endl;
-            cout << "P = " << endl << P << endl;
-            cout << "Q = " << endl << Q << endl;
-            cout << "tmp_np = " << endl << tmp_np << endl;
-            cout << endl;
+            pout << "T = " << endl << T << endl;
+            pout << "P = " << endl << P << endl;
+            pout << "Q = " << endl << Q << endl;
+            pout << "tmp_np = " << endl << tmp_np << endl;
+            pout << endl;
         }
         Mat U, Vt;
         Vec D;
         real safeguard = 1.1;
         SVD(tmp_np, U, D, Vt, 'S', safeguard);
         if (verbosity >= 2) {
-            cout << "U = " << endl << U << endl;  
-            cout << "D = " << endl << D << endl;
-            cout << "Vt = " << endl << Vt << endl;
-            cout << endl;
+            pout << "U = " << endl << U << endl;  
+            pout << "D = " << endl << D << endl;
+            pout << "Vt = " << endl << Vt << endl;
+            pout << endl;
         }
         
         Mat invDmat(p,p);
@@ -654,11 +654,11 @@
         B.resize(p,1);
         productTranspose(B, W, Q);
         if (verbosity >= 2) {
-            cout << "W = " << W << endl;
-            cout << "B = " << B << endl;
+            pout << "W = " << W << endl;
+            pout << "B = " << B << endl;
         }
         if (verbosity >= 1)
-            cout << "WPLS training ended" << endl;
+            pout << "WPLS training ended" << endl;
     }
 }
 



From chrish at mail.berlios.de  Thu Apr 17 22:36:28 2008
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Thu, 17 Apr 2008 22:36:28 +0200
Subject: [Plearn-commits] r8831 - trunk/python_modules/plearn/pyext
Message-ID: <200804172036.m3HKaSeh000897@sheep.berlios.de>

Author: chrish
Date: 2008-04-17 22:36:27 +0200 (Thu, 17 Apr 2008)
New Revision: 8831

Modified:
   trunk/python_modules/plearn/pyext/__init__.py
Log:
Don't enable cgitb by default in pyext: too general a place for this! Also move all imports out of functions and to the beginning of the code.

Modified: trunk/python_modules/plearn/pyext/__init__.py
===================================================================
--- trunk/python_modules/plearn/pyext/__init__.py	2008-04-17 16:04:01 UTC (rev 8830)
+++ trunk/python_modules/plearn/pyext/__init__.py	2008-04-17 20:36:27 UTC (rev 8831)
@@ -30,20 +30,17 @@
 #  This file is part of the PLearn library. For more information on the PLearn
 #  library, go to the PLearn Web site at www.plearn.org
 
+# Import correct PLearn python extension module as set by plearn.getLib()
 from plearn import getLib
-
 pl_lib_dir, pl_lib_name = getLib()
 exec 'from %s.%s import *' % (pl_lib_dir, pl_lib_name)
 exec 'from %s import %s as pl' % (pl_lib_dir, pl_lib_name)
 
+import gc, atexit
 from plearn.pyplearn.plargs import *
-import os
-import cgitb
-cgitb.enable(format='PLearn')
+from plearn.utilities.options_dialog import *
 
-import atexit
 def cleanupWrappedObjects():
-    import gc
     gc.collect()
     if pl: #if plext still loaded
         ramassePoubelles()
@@ -75,8 +72,7 @@
         return [ content[i*ncols:(i+1)*ncols] for i in range(nrows) ]
 
 
-from plearn.utilities.options_dialog import *
-verb, logs, namespaces, use_gui= getGuiInfo(sys.argv)
+verb, logs, namespaces, use_gui = getGuiInfo(sys.argv)
 
 # Enact the use of plargs: the current behavior is to consider as a plargs
 # any command-line argument that contains a '=' char and to neglect all



From louradou at mail.berlios.de  Fri Apr 18 00:32:25 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Fri, 18 Apr 2008 00:32:25 +0200
Subject: [Plearn-commits] r8832 - trunk/plearn_learners/online
Message-ID: <200804172232.m3HMWPgc014215@sheep.berlios.de>

Author: louradou
Date: 2008-04-18 00:32:20 +0200 (Fri, 18 Apr 2008)
New Revision: 8832

Modified:
   trunk/plearn_learners/online/RBMTruncExpLayer.cc
   trunk/plearn_learners/online/RBMTruncExpLayer.h
Log:
batch size version of RBMTruncExpLayer added



Modified: trunk/plearn_learners/online/RBMTruncExpLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMTruncExpLayer.cc	2008-04-17 20:36:27 UTC (rev 8831)
+++ trunk/plearn_learners/online/RBMTruncExpLayer.cc	2008-04-17 22:32:20 UTC (rev 8832)
@@ -95,6 +95,29 @@
     }
 }
 
+void RBMTruncExpLayer::generateSamples()
+{
+    PLASSERT_MSG(random_gen,
+                 "random_gen should be initialized before generating samples");
+
+    PLCHECK_MSG(expectations_are_up_to_date, "Expectations should be computed "
+            "before calling generateSamples()");
+
+    PLASSERT( samples.width() == size && samples.length() == batch_size );
+
+    for (int k = 0; k < batch_size; k++)
+        for (int i=0 ; i<size ; i++)
+        {
+            real s = random_gen->uniform_sample();
+            real a_i = activations(k,i);
+            if( fabs( a_i ) <= 1e-5 )
+                samples(k, i) = s + a_i*( s*(1 - s)/2 );
+            else
+                samples(k, i) = logadd( pl_log( 1-s ), pl_log(s) + a_i ) / a_i;
+        }
+
+}
+
 void RBMTruncExpLayer::computeExpectation()
 {
     if( expectation_is_up_to_date )
@@ -120,6 +143,35 @@
 }
 
 
+void RBMTruncExpLayer::computeExpectations()
+{
+    if( expectations_are_up_to_date )
+        return;
+
+    /* Conditional expectation:
+     * E[u|x] = 1/(1-exp(-a)) - 1/a
+     */
+
+    PLASSERT( expectations.width() == size
+              && expectations.length() == batch_size );
+
+    for (int k = 0; k < batch_size; k++)
+        for( int i=0 ; i<size ; i++ )
+        {
+            real a_i = activations(k,i);
+
+            // Polynomial approximation to avoid numerical instability
+            // f(a) = 1/2 + a/12 - a^3/720 + O(a^5)
+            if( fabs( a_i ) <= 0.01 )
+                expectations(k,i) = 0.5 + a_i*(1./12. - a_i*a_i/720.);
+            else
+                expectations(k,i) = 1/(1-exp(-a_i)) - 1/a_i;
+        }
+
+    expectations_are_up_to_date = true;
+}
+
+
 void RBMTruncExpLayer::fprop( const Vec& input, Vec& output ) const
 {
     PLASSERT( input.size() == input_size );

Modified: trunk/plearn_learners/online/RBMTruncExpLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMTruncExpLayer.h	2008-04-17 20:36:27 UTC (rev 8831)
+++ trunk/plearn_learners/online/RBMTruncExpLayer.h	2008-04-17 22:32:20 UTC (rev 8832)
@@ -72,15 +72,13 @@
     //! generate a sample, and update the sample field
     virtual void generateSample() ;
 
-    virtual void generateSamples() {
-        PLASSERT( false ); // Not implemented.
-    }
+    virtual void generateSamples();
 
     //! compute the expectation
     virtual void computeExpectation() ;
 
     //! Not implemented.
-    virtual void computeExpectations() { PLASSERT( false ); }
+    virtual void computeExpectations();
 
     //! forward propagation
     virtual void fprop( const Vec& input, Vec& output ) const;



From louradou at mail.berlios.de  Fri Apr 18 00:39:26 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Fri, 18 Apr 2008 00:39:26 +0200
Subject: [Plearn-commits] r8833 - trunk/plearn_learners/online
Message-ID: <200804172239.m3HMdQ9b026606@sheep.berlios.de>

Author: louradou
Date: 2008-04-18 00:39:16 +0200 (Fri, 18 Apr 2008)
New Revision: 8833

Modified:
   trunk/plearn_learners/online/RBMLayer.cc
   trunk/plearn_learners/online/RBMLayer.h
Log:
added a default batch version to RBMLayer



Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2008-04-17 22:32:20 UTC (rev 8832)
+++ trunk/plearn_learners/online/RBMLayer.cc	2008-04-17 22:39:16 UTC (rev 8833)
@@ -154,6 +154,17 @@
                     "output_size = size");
 }
 
+void RBMLayer::declareMethods(RemoteMethodMap& rmm)
+{
+    // Make sure that inherited methods are declared
+    rmm.inherited(inherited::_getRemoteMethodMap_());
+
+    declareMethod(rmm, "setAllBias", &RBMLayer::setAllBias,
+                  (BodyDoc("Set the biases values"),
+                   ArgDoc ("bias", "the vector of biases")
+                  ));
+}
+
 ////////////
 // build_ //
 ////////////
@@ -311,7 +322,8 @@
 void RBMLayer::fprop( const Vec& input, const Vec& rbm_bias,
                       Vec& output ) const
 {
-    PLERROR("In RBMLayer::fprop(): not implemented");
+    PLERROR("In RBMLayer::fprop(): not implemented in subclass %s",
+            this->classname().c_str());
 }
 
 void RBMLayer::bpropUpdate(const Vec& input, const Vec& rbm_bias,
@@ -319,29 +331,56 @@
                            Vec& input_gradient, Vec& rbm_bias_gradient,
                            const Vec& output_gradient)
 {
-    PLERROR("In RBMLayer::bpropUpdate(): not implemented");
+    PLERROR("In RBMLayer::bpropUpdate(): not implemented in subclass %s",
+            this->classname().c_str());
 }
 
 real RBMLayer::fpropNLL(const Vec& target)
 {
-    PLERROR("In RBMLayer::fpropNLL(): not implemented");
+    PLERROR("In RBMLayer::fpropNLL(): not implemented in subclass %s",
+            this->classname().c_str());
     return REAL_MAX;
 }
 
 void RBMLayer::fpropNLL(const Mat& targets, const Mat& costs_column)
 {
-    PLERROR("In RBMLayer::fpropNLL(): not implemented");
+    PLWARNING("batch version of RBMLayer::fpropNLL may not be optimized in subclass %s",
+              this->classname().c_str());
+    PLASSERT( targets.width() == input_size );
+    PLASSERT( targets.length() == batch_size );
+    PLASSERT( costs_column.width() == 1 );
+    PLASSERT( costs_column.length() == batch_size );
+
+    Mat tmp;
+    tmp.resize(1,input_size);
+    Vec target;
+    target.resize(input_size);
+
+    computeExpectations();
+    expectation_is_up_to_date = false;
+    for (int k=0;k<batch_size;k++) // loop over minibatch
+    {
+        selectRows(expectations, TVec<int>(1, k), tmp );
+        expectation << tmp;
+        selectRows( activations, TVec<int>(1, k), tmp );
+        activation << tmp;
+        selectRows( targets, TVec<int>(1, k), tmp );
+	target << tmp;
+        costs_column(k,0) = fpropNLL( target );
+    }
 }
 
 void RBMLayer::bpropNLL(const Vec& target, real nll, Vec& bias_gradient)
 {
-    PLERROR("In RBMLayer::bpropNLL(): not implemented");
+    PLERROR("In RBMLayer::bpropNLL(): not implemented in subclass %s",
+            this->classname().c_str());
 }
 
 void RBMLayer::bpropNLL(const Mat& targets,  const Mat& costs_column, 
                         Mat& bias_gradients)
 {
-    PLERROR("In RBMLayer::bpropNLL(): not implemented");
+    PLERROR("In RBMLayer::bpropNLL(): not implemented in subclass %s",
+            this->classname().c_str());
 }
 
 ////////////////////////
@@ -388,7 +427,7 @@
     real* bps = bias_pos_stats.data();
     real* bns = bias_neg_stats.data();
 
-    if( momentum == 0. )
+    if( fast_is_equal( momentum, 0.) )
     {
         // no need to use bias_inc
         for( int i=0 ; i<size ; i++ )
@@ -425,7 +464,7 @@
 
     for( int i=0 ; i<size ; i++ )
     {
-        if( momentum == 0. )
+        if( fast_is_equal( momentum, 0.) )
         {
             // update the bias: bias -= learning_rate * input_gradient
             b[i] -= learning_rate * gb[i];
@@ -443,6 +482,33 @@
     applyBiasDecay();
 }
 
+void RBMLayer::update( const Mat& grad )
+{
+    int batch_size = grad.length();
+    real* b = bias.data();
+    real* binc = momentum==0?0:bias_inc.data();
+    real avg_lr = learning_rate / (real)batch_size;
+
+    for( int isample=0; isample<batch_size; isample++)
+        for( int i=0 ; i<size ; i++ )
+        {
+            if( fast_is_equal( momentum, 0.) )
+            {
+                // update the bias: bias -= learning_rate * input_gradient
+                b[i] -= avg_lr * grad(isample,i);
+            }
+            else
+            {
+                // The update rule becomes:
+                // bias_inc = momentum * bias_inc - learning_rate * input_gradient
+                // bias += bias_inc
+                binc[i] = momentum * binc[i] - avg_lr * grad(isample,i);
+                b[i] += binc[i];
+            }
+        }
+}
+
+
 void RBMLayer::update( const Vec& pos_values, const Vec& neg_values)
 {
     // bias += learning_rate * (pos_values - neg_values)
@@ -450,7 +516,7 @@
     real* pv = pos_values.data();
     real* nv = neg_values.data();
 
-    if( momentum == 0. )
+    if( fast_is_equal( momentum, 0.) )
     {
         for( int i=0 ; i<size ; i++ )
             b[i] += learning_rate * ( pv[i] - nv[i] );
@@ -487,7 +553,7 @@
     // We take the average gradient over the mini-batch.
     real avg_lr = learning_rate / n;
 
-    if( momentum == 0. )
+    if( fast_is_equal( momentum, 0.) )
     {
         transposeProductScaleAcc(bias, pos_values, ones,  avg_lr, real(1));
         transposeProductScaleAcc(bias, neg_values, ones, -avg_lr, real(1));
@@ -706,7 +772,6 @@
     PLERROR("RBMLayer::getConfiguration(int, Vec) not implemented in subclass %s\n",classname().c_str());
 }
 
-
 void RBMLayer::addBiasDecay(Vec& bias_gradient)
 {
     PLASSERT(bias_gradient.size()==size);
@@ -775,6 +840,7 @@
                 " the list, in subclass %s\n",bias_decay_type.c_str(),classname().c_str());
 
 }   
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2008-04-17 22:32:20 UTC (rev 8832)
+++ trunk/plearn_learners/online/RBMLayer.h	2008-04-17 22:39:16 UTC (rev 8833)
@@ -232,6 +232,7 @@
 
     //! Updates parameters according to the given gradient
     virtual void update( const Vec& grad );
+    virtual void update( const Mat& grad );
 
     //! Update parameters according to one pair of vectors
     virtual void update( const Vec& pos_values, const Vec& neg_values );
@@ -346,6 +347,9 @@
     //! Declares the class options.
     static void declareOptions(OptionList& ol);
 
+    //! Declares the class Methods.
+    static void declareMethods(RemoteMethodMap& rmm);
+
 private:
     //#####  Private Member Functions  ########################################
 



From nouiz at mail.berlios.de  Fri Apr 18 17:41:29 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 18 Apr 2008 17:41:29 +0200
Subject: [Plearn-commits] r8834 - trunk/plearn/vmat
Message-ID: <200804181541.m3IFfTnF015250@sheep.berlios.de>

Author: nouiz
Date: 2008-04-18 17:41:28 +0200 (Fri, 18 Apr 2008)
New Revision: 8834

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
Log:
better error msg


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2008-04-17 22:39:16 UTC (rev 8833)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2008-04-18 15:41:28 UTC (rev 8834)
@@ -332,7 +332,7 @@
         buildIdx(); // (re)build it first!
     idxfile = fopen(idxfname.c_str(),"rb");
     if(fgetc(idxfile) != byte_order())
-        PLERROR("Wrong endianness. Remove the index file for it to be automatically rebuilt");
+        PLERROR("In TextFilesVMatrix::build_ - Wrong endianness. Remove the index file for it to be automatically rebuilt");
     fread(&length_, 4, 1, idxfile);
 
     // Initialize some sizes
@@ -352,7 +352,7 @@
 
     // Sanity checking
     if (delimiter.size() != 1)
-        PLERROR("TextFilesVMatrix: the 'delimiter' option '%s' must contain exactly one character",
+        PLERROR("In TextFilesVMatrix::build_ - the 'delimiter' option '%s' must contain exactly one character",
                 delimiter.c_str());
 }
 
@@ -365,7 +365,7 @@
     FILE* f = txtfiles[(int)fileno];
     fseek(f,pos,SEEK_SET);
     if(!fgets(buf, sizeof(buf), f))
-        PLERROR("Problem in TextFilesVMatrix::getTextRow fgets for row %d returned NULL",i);
+        PLERROR("In TextFilesVMatrix::getTextRow - fgets for row %d returned NULL",i);
     return removenewline(buf);
 }
 
@@ -502,7 +502,7 @@
 
     // strval not found
     if(!auto_extend_map)
-        PLERROR("No mapping found for field %d (%s) string-value \"%s\" ", fieldnum, fieldspec[fieldnum].first.c_str(), strval.c_str());
+        PLERROR("In TextFilesVMatrix::getMapping - No mapping found for field %d (%s) string-value \"%s\" ", fieldnum, fieldspec[fieldnum].first.c_str(), strval.c_str());
 
     // OK, let's extend the mapping...
     real val = real(-1000 - int(m.size()));
@@ -514,7 +514,7 @@
         force_mkdir_for_file(fname);
         mapfiles[fieldnum] = fopen(fname.c_str(),"a");
         if(!mapfiles[fieldnum])
-            PLERROR("Could not open map file %s\n for appending\n",fname.c_str());
+            PLERROR("In TextFilesVMatrix::getMapping - Could not open map file %s\n for appending\n",fname.c_str());
     }
 
     fprintf(mapfiles[fieldnum],"\n\"%s\" %f", strval.c_str(), val);
@@ -531,7 +531,7 @@
     string rowi = getTextRow(i);
     TVec<string> fields =  splitIntoFields(rowi);
     if(fields.size() != fieldspec.size())
-        PLERROR("In getting fields of row %d, wrong number of fields: %d (should be %d):\n%s\n",i,fields.size(),fieldspec.size(),rowi.c_str());
+        PLERROR("In TextFilesVMatrix::getMapping - In getting fields of row %d, wrong number of fields: %d (should be %d):\n%s\n",i,fields.size(),fieldspec.size(),rowi.c_str());
     for(int k=0; k<fields.size(); k++)
         fields[k] = removeblanks(fields[k]);
     return fields;
@@ -671,11 +671,13 @@
         else if(pl_isnumber(strval,&val)) {
             dest[0] = val;
             if (val <= 0) {
-                PLERROR("I didn't know a sas_date could be negative");
+                PLERROR("In TextFilesVMatrix::transformStringToValue - "
+                        "I didn't know a sas_date could be negative");
             }
         }
         else
-            PLERROR("Error while parsing a sas_date");
+            PLERROR("In TextFilesVMatrix::transformStringToValue - "
+                    "Error while parsing a sas_date");
     }
     else if(fieldtype=="YYYYMM")
     {
@@ -851,7 +853,8 @@
     in.skipBlanksAndComments();
     char eq = in.get();
     if(option!=optionname || eq!='=')
-        PLERROR("Bad syntax in .txtmat file.\n"
+        PLERROR("In TextFilesVMatrix::readAndCheckOptionName - "
+                "Bad syntax in .txtmat file.\n"
                 "Expected option %s = ...\n"
                 "Read %s %c\n", optionname.c_str(), option.c_str(), eq);
 }



From lamblin at mail.berlios.de  Fri Apr 18 23:13:57 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 18 Apr 2008 23:13:57 +0200
Subject: [Plearn-commits] r8835 - trunk/plearn_learners/online
Message-ID: <200804182113.m3ILDvsR026980@sheep.berlios.de>

Author: lamblin
Date: 2008-04-18 23:13:57 +0200 (Fri, 18 Apr 2008)
New Revision: 8835

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
Log:
Cosmetic changes.


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2008-04-18 15:41:28 UTC (rev 8834)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2008-04-18 21:13:57 UTC (rev 8835)
@@ -2584,7 +2584,7 @@
     {
         Vec in_i = inputs(isample);
         Vec out_i = outputs(isample); 
-	computeOutput(in_i, out_i);
+        computeOutput(in_i, out_i);
         if( !partial_costs.isEmpty() )
         {
             Vec pcosts;
@@ -2599,7 +2599,7 @@
                         << pcosts;
                 }
         }
-	if (reconstruct_layerwise)
+        if (reconstruct_layerwise)
            costs(isample).subVec(reconstruction_cost_index, reconstruction_costs.length())
                 << reconstruction_costs;
     }



From lamblin at mail.berlios.de  Fri Apr 18 23:58:43 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 18 Apr 2008 23:58:43 +0200
Subject: [Plearn-commits] r8836 - trunk/plearn_learners/online/EXPERIMENTAL
Message-ID: <200804182158.m3ILwh4o030300@sheep.berlios.de>

Author: lamblin
Date: 2008-04-18 23:58:43 +0200 (Fri, 18 Apr 2008)
New Revision: 8836

Added:
   trunk/plearn_learners/online/EXPERIMENTAL/SemiSupervisedDBN.cc
   trunk/plearn_learners/online/EXPERIMENTAL/SemiSupervisedDBN.h
Log:
Experimental DBN with semi-supervised learning at each layer.


Added: trunk/plearn_learners/online/EXPERIMENTAL/SemiSupervisedDBN.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/SemiSupervisedDBN.cc	2008-04-18 21:13:57 UTC (rev 8835)
+++ trunk/plearn_learners/online/EXPERIMENTAL/SemiSupervisedDBN.cc	2008-04-18 21:58:43 UTC (rev 8836)
@@ -0,0 +1,265 @@
+// -*- C++ -*-
+
+// SemiSupervisedDBN.cc
+//
+// Copyright (C) 2008 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file SemiSupervisedDBN.cc */
+
+
+#include "SemiSupervisedDBN.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    SemiSupervisedDBN,
+    "Deep Belief Net, possibly supervised, trained only with CD",
+    "");
+
+SemiSupervisedDBN::SemiSupervisedDBN():
+    learning_rate(0),
+    n_classes(0),
+    share_layers(false),
+    n_layers(0)
+{
+    random_gen = new PRandom();
+}
+
+void SemiSupervisedDBN::declareOptions(OptionList& ol)
+{
+    // declareOption(ol, "myoption", &SemiSupervisedDBN::myoption,
+    //               OptionBase::buildoption,
+    //               "Help text describing this option");
+
+    declareOption(ol, "", &SemiSupervisedDBN::,
+                  OptionBase::buildoption,
+                  "");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void SemiSupervisedDBN::build_()
+{
+    bool rbms_need_build = false;
+    if (layer_sizes.length() != rbms.size()+1)
+        rbms_need_build = true;
+    else
+    {
+        n_rbms = rbms.size();
+        for (int i=0; i<n_rbms; i++)
+        {
+            if (layer_sizes[i] != rbms[i]->input_size)
+            {
+                rbms_need_build = true;
+                break;
+            }
+            if (layer_is_supervised[i] && (rbms[i]->target_size != n_target))
+            {
+                rbms_need_build = true;
+                break;
+            }
+            if (!layer_is_supervised[i] && (rbms[i]->target_size != 0))
+            {
+                rbms_need_build = true;
+                break;
+            }
+            if (layer_sizes[i+1] != rbms[i]->hidden_size)
+            {
+                rbms_need_build = true;
+                break;
+            }
+        }
+    }
+
+    if (rbms_need_build)
+        build_rbms();
+}
+
+void SemiSupervisedDBN::build_rbms()
+{
+    n_layers = layer_sizes.length();
+    n_rbms = n_layers - 1;
+    rbms.resize(n_rbms);
+    for (int i=0; i<n_rbms; i++)
+    {
+        if (rbms[i].isNull())
+            rbms[i] = new InferenceRBM;
+
+        if (i==0 && first_layer_type == "gaussian")
+            rbms[i]->input_layer = new RBMGaussianLayer(layer_sizes[0]);
+        else if (i>0 && share_layers)
+            rbms[i]->input_layer = rbms[i-1]->hidden_layer;
+        else
+            rbms[i]->input_layer = new RBMGaussianLayer(layer_sizes[i]);
+
+
+        if (layer_is_supervised[i])
+            rbms[i]->target = new RBMMultinomialLayer(n_classes);
+
+        rbms[i]->hidden = new RBMBinomialLayer(layer_sizes[i+1]);
+
+        rbms[i]->random_gen = random_gen;
+        rbms[i]->build();
+        rbms[i]->setLearningRate(learning_rate);
+    }
+}
+
+
+void SemiSupervisedDBN::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void SemiSupervisedDBN::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### Call deepCopyField on all "pointer-like" fields
+    // ### that you wish to be deepCopied rather than
+    // ### shallow-copied.
+    // ### ex:
+    // deepCopyField(trainvec, copies);
+
+    // ### Remove this line when you have fully implemented this method.
+    PLERROR("SemiSupervisedDBN::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
+}
+
+
+int SemiSupervisedDBN::outputsize() const
+{
+    // Compute and return the size of this learner's output (which typically
+    // may depend on its inputsize(), targetsize() and set options).
+}
+
+void SemiSupervisedDBN::forget()
+{
+    //! (Re-)initialize the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!)
+    /*!
+      A typical forget() method should do the following:
+      - call inherited::forget() to initialize its random number generator
+        with the 'seed' option
+      - initialize the learner's parameters, using this random generator
+      - stage = 0
+    */
+    inherited::forget();
+}
+
+void SemiSupervisedDBN::train()
+{
+    // The role of the train method is to bring the learner up to
+    // stage==nstages, updating train_stats with training costs measured
+    // on-line in the process.
+
+    /* TYPICAL CODE:
+
+    static Vec input;  // static so we don't reallocate memory each time...
+    static Vec target; // (but be careful that static means shared!)
+    input.resize(inputsize());    // the train_set's inputsize()
+    target.resize(targetsize());  // the train_set's targetsize()
+    real weight;
+
+    // This generic PLearner method does a number of standard stuff useful for
+    // (almost) any learner, and return 'false' if no training should take
+    // place. See PLearner.h for more details.
+    if (!initTrain())
+        return;
+
+    while(stage<nstages)
+    {
+        // clear statistics of previous epoch
+        train_stats->forget();
+
+        //... train for 1 stage, and update train_stats,
+        // using train_set->getExample(input, target, weight)
+        // and train_stats->update(train_costs)
+
+        ++stage;
+        train_stats->finalize(); // finalize statistics for this epoch
+    }
+    */
+}
+
+
+void SemiSupervisedDBN::computeOutput(const Vec& input, Vec& output) const
+{
+    // Compute the output from the input.
+    // int nout = outputsize();
+    // output.resize(nout);
+    // ...
+}
+
+void SemiSupervisedDBN::computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                           const Vec& target, Vec& costs) const
+{
+// Compute the costs from *already* computed output.
+// ...
+}
+
+TVec<string> SemiSupervisedDBN::getTestCostNames() const
+{
+    // Return the names of the costs computed by computeCostsFromOutputs
+    // (these may or may not be exactly the same as what's returned by
+    // getTrainCostNames).
+    // ...
+}
+
+TVec<string> SemiSupervisedDBN::getTrainCostNames() const
+{
+    // Return the names of the objective costs that the train method computes
+    // and for which it updates the VecStatsCollector train_stats
+    // (these may or may not be exactly the same as what's returned by
+    // getTestCostNames).
+    // ...
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/EXPERIMENTAL/SemiSupervisedDBN.h
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/SemiSupervisedDBN.h	2008-04-18 21:13:57 UTC (rev 8835)
+++ trunk/plearn_learners/online/EXPERIMENTAL/SemiSupervisedDBN.h	2008-04-18 21:58:43 UTC (rev 8836)
@@ -0,0 +1,187 @@
+// -*- C++ -*-
+
+// SemiSupervisedDBN.h
+//
+// Copyright (C) 2008 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file SemiSupervisedDBN.h */
+
+
+#ifndef SemiSupervisedDBN_INC
+#define SemiSupervisedDBN_INC
+
+#include <plearn_learners/generic/PLearner.h>
+
+namespace PLearn {
+
+/**
+ * Deep Belief Net, possibly supervised, trained only with CD.
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class SemiSupervisedDBN : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    SemiSupervisedDBN();
+
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Returns the size of this learner's output, (which typically
+    //! may depend on its inputsize(), targetsize() and set options).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual int outputsize() const;
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training costs
+    //! measured on-line in the process.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void train();
+
+    //! Computes the output from the input.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeOutput(const Vec& input, Vec& output) const;
+
+    //! Computes the costs from already computed output.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                         const Vec& target, Vec& costs) const;
+
+    //! Returns the names of the costs computed by computeCostsFromOutpus (and
+    //! thus the test method).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec<std::string> getTestCostNames() const;
+
+    //! Returns the names of the objective costs that the train method computes
+    //! and for which it updates the VecStatsCollector train_stats.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec<std::string> getTrainCostNames() const;
+
+
+    // *** SUBCLASS WRITING: ***
+    // While in general not necessary, in case of particular needs
+    // (efficiency concerns for ex) you may also want to overload
+    // some of the following methods:
+    // virtual void computeOutputAndCosts(const Vec& input, const Vec& target,
+    //                                    Vec& output, Vec& costs) const;
+    // virtual void computeCostsOnly(const Vec& input, const Vec& target,
+    //                               Vec& costs) const;
+    // virtual void test(VMat testset, PP<VecStatsCollector> test_stats,
+    //                   VMat testoutputs=0, VMat testcosts=0) const;
+    // virtual int nTestCosts() const;
+    // virtual int nTrainCosts() const;
+    // virtual void resetInternalState();
+    // virtual bool isStatefulLearner() const;
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(SemiSupervisedDBN);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(SemiSupervisedDBN);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From lamblin at mail.berlios.de  Sat Apr 19 03:25:42 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Sat, 19 Apr 2008 03:25:42 +0200
Subject: [Plearn-commits] r8837 - trunk/plearn/io
Message-ID: <200804190125.m3J1Pgcn032625@sheep.berlios.de>

Author: lamblin
Date: 2008-04-19 03:25:41 +0200 (Sat, 19 Apr 2008)
New Revision: 8837

Modified:
   trunk/plearn/io/IntVecFile.cc
   trunk/plearn/io/Poll.cc
   trunk/plearn/io/Poll.h
Log:
Should fix some compilation warnings


Modified: trunk/plearn/io/IntVecFile.cc
===================================================================
--- trunk/plearn/io/IntVecFile.cc	2008-04-18 21:58:43 UTC (rev 8836)
+++ trunk/plearn/io/IntVecFile.cc	2008-04-19 01:25:41 UTC (rev 8837)
@@ -50,7 +50,7 @@
 using namespace std;
 
 const char IntVecFile::signature[] = {
-    0xDE, 0xAD, 0xBE, 0xEF, 0x00               //!< deadbeef, really...
+    '\xDE', '\xAD', '\xBE', '\xEF', '\x00'     //!< deadbeef, really...
 };
 
 const int IntVecFile::header_size[] = {      //!< in number of ints

Modified: trunk/plearn/io/Poll.cc
===================================================================
--- trunk/plearn/io/Poll.cc	2008-04-18 21:58:43 UTC (rev 8836)
+++ trunk/plearn/io/Poll.cc	2008-04-19 01:25:41 UTC (rev 8837)
@@ -71,7 +71,7 @@
 
 }
 
-int Poll::waitForEvents(int timeout, bool shuffle_events_) 
+int Poll::waitForEvents(uint32_t timeout, bool shuffle_events_) 
 {
     if (m_poll_descriptors.size() == 0)
         PLERROR("Poll::waitforEvents: called with no streams to watch.");

Modified: trunk/plearn/io/Poll.h
===================================================================
--- trunk/plearn/io/Poll.h	2008-04-18 21:58:43 UTC (rev 8836)
+++ trunk/plearn/io/Poll.h	2008-04-19 01:25:41 UTC (rev 8837)
@@ -62,7 +62,7 @@
 public:
     void setStreamsToWatch(const vector<PStream>& streams);
 
-    int waitForEvents(int timeout = 0, bool shuffle_events_= false);
+    int waitForEvents(uint32_t timeout = 0, bool shuffle_events_= false);
 
     PStream getNextPendingEvent();
 



From lamblin at mail.berlios.de  Sat Apr 19 03:26:38 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Sat, 19 Apr 2008 03:26:38 +0200
Subject: [Plearn-commits] r8838 - trunk/plearn_learners/online
Message-ID: <200804190126.m3J1QcTZ032721@sheep.berlios.de>

Author: lamblin
Date: 2008-04-19 03:26:38 +0200 (Sat, 19 Apr 2008)
New Revision: 8838

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
Log:
Fix another warning


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2008-04-19 01:25:41 UTC (rev 8837)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2008-04-19 01:26:38 UTC (rev 8838)
@@ -1943,7 +1943,7 @@
     }
 
     // Updates
-    real nll;
+    real nll = 0.; // Actually unused
     for( int i=0 ; i<n_layers-2 ; i++ )
     {
         // Update recognition weights



From lamblin at mail.berlios.de  Sat Apr 19 03:27:57 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Sat, 19 Apr 2008 03:27:57 +0200
Subject: [Plearn-commits] r8839 - trunk/python_modules/plearn/pymake
Message-ID: <200804190127.m3J1RvHF000024@sheep.berlios.de>

Author: lamblin
Date: 2008-04-19 03:27:56 +0200 (Sat, 19 Apr 2008)
New Revision: 8839

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
Also remove from compilation lists hosts that are a security threat.


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-04-19 01:26:38 UTC (rev 8838)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-04-19 01:27:56 UTC (rev 8839)
@@ -2079,7 +2079,9 @@
         # single error message for a syntax error, etc.
         if not hasattr(self,"compilation_status"):
             if warningmsgs:
-                if warningmsgs[0].startswith('ssh: ') or warningmsgs[0].startswith('Connection closed by'):
+                if warningmsgs[0].startswith('ssh: ') \
+                        or warningmsgs[0].startswith('Connection closed by') \
+                        or warningmsgs[0].startswith('@@@@@@@'):
                     # The hostname has a problem, so we remove it from the list
                     # and retry on another machine
                     self.remove_hostname = True



From nouiz at mail.berlios.de  Mon Apr 21 15:49:43 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 21 Apr 2008 15:49:43 +0200
Subject: [Plearn-commits] r8840 - trunk/plearn_learners/regressors
Message-ID: <200804211349.m3LDnhEh013219@sheep.berlios.de>

Author: nouiz
Date: 2008-04-21 15:49:42 +0200 (Mon, 21 Apr 2008)
New Revision: 8840

Modified:
   trunk/plearn_learners/regressors/RegressionTree.cc
   trunk/plearn_learners/regressors/RegressionTree.h
Log:
the output of RegressionTree is now by default of size 1 and contain only the predicted value. If you also want the confidence, you must set to true the option output_confidence_target.


Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2008-04-19 01:27:56 UTC (rev 8839)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2008-04-21 13:49:42 UTC (rev 8840)
@@ -67,7 +67,9 @@
       loss_function_weight(1.0),
       maximum_number_of_nodes(400),
       compute_train_stats(1),
-      complexity_penalty_factor(0.0)
+      complexity_penalty_factor(0.0),
+      output_confidence_target(false)
+
 {
 }
 
@@ -91,6 +93,15 @@
                   "A factor that is multiplied with the square root of the number of leaves.\n"
                   "If the error inprovement for the next split is less than the result, the algorithm proceed to an early stop."
                   "(When set to 0.0, the default value, it has no impact).");
+
+    declareOption(ol, "output_confidence_target",
+                  &RegressionTree::output_confidence_target,
+                  OptionBase::buildoption,
+                  "If false the output size is 1 and contain only the predicted"
+                  " target. Else output size is 2 and contain also the"
+                  " confidence\n");
+
+
     declareOption(ol, "multiclass_outputs", &RegressionTree::multiclass_outputs, OptionBase::buildoption,
                   "A vector of possible output values when solving a multiclass problem.\n"
                   "When making a prediction, the tree will adjust the output value of each leave to the closest value provided in this vector.");
@@ -182,6 +193,8 @@
             PLERROR("RegressionTree: expected weightsize to be 1 or 0, got %d",
                     weightsize);
         nodes = new TVec<PP<RegressionTreeNode> >();
+        if(!output_confidence_target)
+            tmp_vec.resize(2);
     }
 
     if (loss_function_weight != 0.0)
@@ -332,7 +345,10 @@
 
 int RegressionTree::outputsize() const
 {
-    return 2;
+    if(output_confidence_target)
+        return 2;
+    else
+        return 1;
 }
 
 TVec<string> RegressionTree::getTrainCostNames() const
@@ -364,13 +380,25 @@
 
 void RegressionTree::computeOutput(const Vec& inputv, Vec& outputv) const
 {
-    computeOutputAndNodes(inputv,outputv);
+    if(!output_confidence_target){
+        computeOutputAndNodes(inputv, tmp_vec);
+        outputv[0]=tmp_vec[0];
+    }
+    else
+        computeOutputAndNodes(inputv, outputv);
+        
 }
 
 void RegressionTree::computeOutputAndNodes(const Vec& inputv, Vec& outputv,
                                            TVec<PP<RegressionTreeNode> >* nodes) const
 {
-    root->computeOutputAndNodes(inputv, outputv, nodes);
+    if(!output_confidence_target){
+        root->computeOutputAndNodes(inputv, tmp_vec, nodes);
+        outputv[0]=tmp_vec[0];
+    }
+    else
+        root->computeOutputAndNodes(inputv, outputv, nodes);
+
     if (multiclass_outputs.length() <= 0) return;
     real closest_value=multiclass_outputs[0];
     real margin_to_closest_value=abs(outputv[0] - multiclass_outputs[0]);
@@ -393,10 +421,17 @@
     PLASSERT(costsv.size()==nTestCosts());
     PLASSERT(nodes);
     nodes->resize(0);
-    computeOutputAndNodes(inputv, outputv, nodes);
+
+    computeOutputAndNodes(inputv, tmp_vec, nodes);
+    if(!output_confidence_target)
+        outputv[0]=tmp_vec[0];
+    else
+        outputv<<tmp_vec;
+
     costsv.clear();
     costsv[0] = pow((outputv[0] - targetv[0]), 2);
-    costsv[1] = outputv[1];
+    
+    costsv[1] = tmp_vec[1];
     costsv[2] = 1.0 - (l2_loss_function_factor * costsv[0]);
     costsv[3] = 1.0 - (l1_loss_function_factor * abs(outputv[0] - targetv[0]));
     costsv[4] = !fast_is_equal(targetv[0],outputv[0]);

Modified: trunk/plearn_learners/regressors/RegressionTree.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.h	2008-04-19 01:27:56 UTC (rev 8839)
+++ trunk/plearn_learners/regressors/RegressionTree.h	2008-04-21 13:49:42 UTC (rev 8840)
@@ -68,6 +68,7 @@
     int maximum_number_of_nodes;
     int compute_train_stats;   
     real complexity_penalty_factor;
+    bool output_confidence_target;
     Vec multiclass_outputs;
     PP<RegressionTreeLeave> leave_template;    
     PP<RegressionTreeRegisters> sorted_train_set;
@@ -91,7 +92,7 @@
     Vec       split_values;
     TVec<PP<RegressionTreeNode> > *nodes;
 
-    Vec tmp_vec;
+    mutable Vec tmp_vec;
 public:
     RegressionTree();
     virtual              ~RegressionTree();



From nouiz at mail.berlios.de  Mon Apr 21 15:54:17 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 21 Apr 2008 15:54:17 +0200
Subject: [Plearn-commits] r8841 - trunk/plearn/vmat
Message-ID: <200804211354.m3LDsHtJ013626@sheep.berlios.de>

Author: nouiz
Date: 2008-04-21 15:54:16 +0200 (Mon, 21 Apr 2008)
New Revision: 8841

Modified:
   trunk/plearn/vmat/FileVMatrix.cc
   trunk/plearn/vmat/FileVMatrix.h
Log:
Added some check


Modified: trunk/plearn/vmat/FileVMatrix.cc
===================================================================
--- trunk/plearn/vmat/FileVMatrix.cc	2008-04-21 13:49:42 UTC (rev 8840)
+++ trunk/plearn/vmat/FileVMatrix.cc	2008-04-21 13:54:16 UTC (rev 8841)
@@ -272,6 +272,20 @@
             map_sr = TVec<map<string,real> >(width_);
             map_rs = TVec<map<real,string> >(width_);
         }
+#ifdef USE_NSPR_FILE
+        //check if the file have all data
+        PRFileInfo64 info;
+        if(PR_FAILURE==PR_GetFileInfo64(filename_.absolute().c_str(), &info))
+            PLERROR("In FileVMatrix::build_() - can't get file info for file '%s'",
+                    filename_.c_str());
+        PRInt64 elemsize = file_is_float ? sizeof(float) : sizeof(double);
+        PRInt64 expectedsize=DATAFILE_HEADERLENGTH+length_*width_*elemsize;
+        if(info.size!=expectedsize)
+            PLWARNING("In FileVMatrix::build_() - The file '%s' have a size"
+                      " of %d, expected %d",
+                      filename_.c_str(), info.size, expectedsize);
+#endif
+        
     }
 
     setMetaDataDir(filename_ + ".metadata");

Modified: trunk/plearn/vmat/FileVMatrix.h
===================================================================
--- trunk/plearn/vmat/FileVMatrix.h	2008-04-21 13:49:42 UTC (rev 8840)
+++ trunk/plearn/vmat/FileVMatrix.h	2008-04-21 13:54:16 UTC (rev 8841)
@@ -45,7 +45,7 @@
 #define FileVMatrix_INC
 
 #include "RowBufferedVMatrix.h"
-
+#include <nspr/prlong.h>
 // While under development, we use this define to control
 // whether to use the NSPR 64 bit file access or the old std C FILE*
 #define USE_NSPR_FILE



From nouiz at mail.berlios.de  Mon Apr 21 17:52:59 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 21 Apr 2008 17:52:59 +0200
Subject: [Plearn-commits] r8842 - trunk/plearn/io
Message-ID: <200804211552.m3LFqxav024635@sheep.berlios.de>

Author: nouiz
Date: 2008-04-21 17:52:57 +0200 (Mon, 21 Apr 2008)
New Revision: 8842

Modified:
   trunk/plearn/io/fileutils.h
Log:
Added include for the type PRUint64


Modified: trunk/plearn/io/fileutils.h
===================================================================
--- trunk/plearn/io/fileutils.h	2008-04-21 13:54:16 UTC (rev 8841)
+++ trunk/plearn/io/fileutils.h	2008-04-21 15:52:57 UTC (rev 8842)
@@ -58,6 +58,7 @@
 
 #include "PPath.h"
 #include "PStream.h"
+#include <nspr/prlong.h> //< for PRUint64
 
 namespace PLearn {
 using namespace std;



From saintmlx at mail.berlios.de  Mon Apr 21 19:54:47 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Mon, 21 Apr 2008 19:54:47 +0200
Subject: [Plearn-commits] r8843 - trunk/plearn/python
Message-ID: <200804211754.m3LHslbM005184@sheep.berlios.de>

Author: saintmlx
Date: 2008-04-21 19:54:45 +0200 (Mon, 21 Apr 2008)
New Revision: 8843

Modified:
   trunk/plearn/python/PythonObjectWrapper.cc
   trunk/plearn/python/PythonObjectWrapper.h
Log:
- support for VarArray in PLearn-python bridge



Modified: trunk/plearn/python/PythonObjectWrapper.cc
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.cc	2008-04-21 15:52:57 UTC (rev 8842)
+++ trunk/plearn/python/PythonObjectWrapper.cc	2008-04-21 17:54:45 UTC (rev 8843)
@@ -55,6 +55,7 @@
 #include <plearn/vmat/VMat.h>
 #include <plearn/base/RemoteTrampoline.h>
 #include <plearn/base/HelpSystem.h>
+#include <plearn/var/VarArray.h>
 
 namespace PLearn {
 using namespace std;
@@ -297,6 +298,12 @@
     return copies;
 }
 
+VarArray ConvertFromPyObject<VarArray>::convert(PyObject* pyobj,
+                                                bool print_traceback)
+{
+    return static_cast<VarArray>(ConvertFromPyObject<TVec<Var> >::convert(pyobj, print_traceback));
+}
+
 //#####  Constructors+Destructors  ############################################
 
 PythonObjectWrapper::PythonObjectWrapper(OwnershipMode o,
@@ -721,6 +728,11 @@
     return pyobj;
 }
 
+PyObject* ConvertToPyObject<VarArray>::newPyObject(const VarArray& var)
+{
+    return ConvertToPyObject<TVec<Var> >::newPyObject(var);
+}
+
 PStream& operator>>(PStream& in, PythonObjectWrapper& v)
 {
     PLERROR("operator>>(PStream&, PythonObjectWrapper&) : "

Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2008-04-21 15:52:57 UTC (rev 8842)
+++ trunk/plearn/python/PythonObjectWrapper.h	2008-04-21 17:54:45 UTC (rev 8843)
@@ -82,6 +82,7 @@
 class PythonObjectWrapper;                   // Forward-declare
 class Object;
 class VMatrix;
+class VarArray;
 
 //! Used for error reporting.  If 'print_traceback' is true, a full
 //! Python traceback is printed to stderr.  Otherwise, raise PLERROR.
@@ -444,7 +445,14 @@
     static CopiesMap convert(PyObject*, bool print_traceback);
 };
 
+template <>
+struct ConvertFromPyObject<VarArray>
+{
+    static VarArray convert(PyObject*, bool print_traceback);
+};
 
+
+
 //! Used to convert integer values to python, using PyInt if possible
 template <class I>
 PyObject* integerToPyObject(const I& x)
@@ -654,6 +662,8 @@
 template<> struct ConvertToPyObject<CopiesMap>
 { static PyObject* newPyObject(const CopiesMap& copies); };
 
+template <> struct ConvertToPyObject<VarArray>
+{ static PyObject* newPyObject(const VarArray&); };
 
 struct PLPyClass
 {



From nouiz at mail.berlios.de  Mon Apr 21 20:13:01 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 21 Apr 2008 20:13:01 +0200
Subject: [Plearn-commits] r8844 - in trunk/plearn_learners/regressors: .
	test/RegressionTree
	test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir
	test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir
Message-ID: <200804211813.m3LID1pw009058@sheep.berlios.de>

Author: nouiz
Date: 2008-04-21 20:13:01 +0200 (Mon, 21 Apr 2008)
New Revision: 8844

Modified:
   trunk/plearn_learners/regressors/RegressionTree.cc
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/experiment.plearn
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/experiment.plearn
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave
   trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn
   trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass.pyplearn
Log:
bug fix for last modif in RegressionTree
updated the test


Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2008-04-21 17:54:45 UTC (rev 8843)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2008-04-21 18:13:01 UTC (rev 8844)
@@ -193,8 +193,7 @@
             PLERROR("RegressionTree: expected weightsize to be 1 or 0, got %d",
                     weightsize);
         nodes = new TVec<PP<RegressionTreeNode> >();
-        if(!output_confidence_target)
-            tmp_vec.resize(2);
+        tmp_vec.resize(2);
     }
 
     if (loss_function_weight != 0.0)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/experiment.plearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/experiment.plearn	2008-04-21 17:54:45 UTC (rev 8843)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/experiment.plearn	2008-04-21 18:13:01 UTC (rev 8844)
@@ -23,6 +23,7 @@
             loss_function_weight = 1,
             maximum_number_of_nodes = 50,
             nstages = 10,
+            output_confidence_target = 1,
             report_progress = 1,
             verbosity = 2
             ),

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt	2008-04-21 17:54:45 UTC (rev 8843)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt	2008-04-21 18:13:01 UTC (rev 8844)
@@ -1,2 +1,2 @@
-__REVISION__ = "PL8793"
+__REVISION__ = "PL8839"
 data                                          = PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave	2008-04-21 17:54:45 UTC (rev 8843)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave	2008-04-21 18:13:01 UTC (rev 8844)
@@ -33,8 +33,9 @@
 splits = 1  3  [ 
 (0 , 1 )	(0 , 0.75 )	(0.75 , 1 )	
 ]
- )
 ;
+one_is_absolute = 0  )
+;
 statnames = 8 [ "E[test1.E[mse]]" "E[test1.E[base_confidence]]" "E[test1.E[base_reward_l2]]" "E[test1.E[base_reward_l1]]" "E[test2.E[mse]]" "E[test2.E[base_confidence]]" "E[test2.E[base_reward_l2]]" "E[test2.E[base_reward_l1]]" ] ;
 statmask = []
 ;
@@ -47,8 +48,9 @@
 splits = 1  3  [ 
 (0 , 0.75 )	(0 , 0.75 )	(0.75 , 1 )	
 ]
- )
 ;
+one_is_absolute = 0  )
+;
 statnames = 8 [ "E[test1.E[mse]]" "E[test1.E[base_confidence]]" "E[test1.E[base_reward_l2]]" "E[test1.E[base_reward_l1]]" "E[test2.E[mse]]" "E[test2.E[base_confidence]]" "E[test2.E[base_reward_l2]]" "E[test2.E[base_reward_l1]]" ] ;
 statmask = []
 ;
@@ -58,6 +60,7 @@
 maximum_number_of_nodes = 50 ;
 compute_train_stats = 1 ;
 complexity_penalty_factor = 0 ;
+output_confidence_target = 1 ;
 multiclass_outputs = []
 ;
 leave_template = *8 ->RegressionTreeLeave(
@@ -147,7 +150,8 @@
 rerun_after_sub = 0 ;
 provide_sub_expdir = 1 ;
 save_best_learner = 0 ;
-splitter = *0  )
+splitter = *0 ;
+verbosity = 0  )
 ] ;
 provide_strategy_expdir = 1 ;
 save_final_learner = 0 ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/experiment.plearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/experiment.plearn	2008-04-21 17:54:45 UTC (rev 8843)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/experiment.plearn	2008-04-21 18:13:01 UTC (rev 8844)
@@ -33,6 +33,7 @@
             loss_function_weight = 1,
             maximum_number_of_nodes = 50,
             nstages = 10,
+            output_confidence_target = 1,
             report_progress = 1,
             verbosity = 2
             ),

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt	2008-04-21 17:54:45 UTC (rev 8843)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt	2008-04-21 18:13:01 UTC (rev 8844)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL8793"
+__REVISION__ = "PL8839"
 datatest                                      = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat
 datatrain                                     = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave	2008-04-21 17:54:45 UTC (rev 8843)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave	2008-04-21 18:13:01 UTC (rev 8844)
@@ -41,8 +41,9 @@
 splits = 1  3  [ 
 (0 , 1 )	(0 , 200 )	(200 , 1 )	
 ]
- )
 ;
+one_is_absolute = 0  )
+;
 statnames = 8 [ "E[test1.E[class_error]]" "E[test1.E[base_confidence]]" "E[test1.E[base_reward_l2]]" "E[test1.E[base_reward_l1]]" "E[test2.E[class_error]]" "E[test2.E[base_confidence]]" "E[test2.E[base_reward_l2]]" "E[test2.E[base_reward_l1]]" ] ;
 statmask = []
 ;
@@ -55,8 +56,9 @@
 splits = 1  3  [ 
 (0 , 200 )	(0 , 200 )	(200 , 1 )	
 ]
- )
 ;
+one_is_absolute = 0  )
+;
 statnames = 8 [ "E[test1.E[class_error]]" "E[test1.E[base_confidence]]" "E[test1.E[base_reward_l2]]" "E[test1.E[base_reward_l1]]" "E[test2.E[class_error]]" "E[test2.E[base_confidence]]" "E[test2.E[base_reward_l2]]" "E[test2.E[base_reward_l1]]" ] ;
 statmask = []
 ;
@@ -66,6 +68,7 @@
 maximum_number_of_nodes = 50 ;
 compute_train_stats = 1 ;
 complexity_penalty_factor = 0 ;
+output_confidence_target = 1 ;
 multiclass_outputs = []
 ;
 leave_template = *9 ->RegressionTreeMulticlassLeave(
@@ -161,7 +164,8 @@
 rerun_after_sub = 0 ;
 provide_sub_expdir = 1 ;
 save_best_learner = 0 ;
-splitter = *0  )
+splitter = *0 ;
+verbosity = 0  )
 ] ;
 provide_strategy_expdir = 1 ;
 save_final_learner = 0 ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn	2008-04-21 17:54:45 UTC (rev 8843)
+++ trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn	2008-04-21 18:13:01 UTC (rev 8844)
@@ -34,6 +34,7 @@
         ,maximum_number_of_nodes = 50
         ,compute_train_stats = 1
         ,complexity_penalty_factor = 0.0
+	,output_confidence_target = 1
         ,verbosity = 2
         ,report_progress = 1
         ,forget_when_training_set_changes = 1

Modified: trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass.pyplearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass.pyplearn	2008-04-21 17:54:45 UTC (rev 8843)
+++ trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass.pyplearn	2008-04-21 18:13:01 UTC (rev 8844)
@@ -30,6 +30,7 @@
         ,maximum_number_of_nodes = 50
         ,compute_train_stats = 1
         ,complexity_penalty_factor = 0.0
+        ,output_confidence_target = 1
         ,verbosity = 2
         ,report_progress = 1
         ,forget_when_training_set_changes = 1



From tihocan at mail.berlios.de  Mon Apr 21 22:46:36 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 21 Apr 2008 22:46:36 +0200
Subject: [Plearn-commits] r8845 - trunk/plearn/var
Message-ID: <200804212046.m3LKka5c015727@sheep.berlios.de>

Author: tihocan
Date: 2008-04-21 22:46:35 +0200 (Mon, 21 Apr 2008)
New Revision: 8845

Modified:
   trunk/plearn/var/Variable.cc
   trunk/plearn/var/Variable.h
Log:
Added new method fillValue, and declared fillValue and fprop as remote methods

Modified: trunk/plearn/var/Variable.cc
===================================================================
--- trunk/plearn/var/Variable.cc	2008-04-21 18:13:01 UTC (rev 8844)
+++ trunk/plearn/var/Variable.cc	2008-04-21 20:46:35 UTC (rev 8845)
@@ -245,6 +245,9 @@
 {}
 
 
+////////////////////
+// declareOptions //
+////////////////////
 void Variable::declareOptions(OptionList& ol)
 {
     declareOption(ol, "varname", &Variable::varname, OptionBase::buildoption, 
@@ -267,6 +270,26 @@
     inherited::declareOptions(ol);
 }
 
+////////////////////
+// declareMethods //
+////////////////////
+void Variable::declareMethods(RemoteMethodMap& rmm)
+{
+    // Insert a backpointer to remote methods; note that this
+    // different than for declareOptions()
+    rmm.inherited(inherited::_getRemoteMethodMap_());
+
+    declareMethod(
+            rmm, "fillValue", &Variable::fillValue,
+            (BodyDoc("Fill value with the given constant"),
+             ArgDoc ("val", "Value to fill with")));
+
+    declareMethod(
+            rmm, "fprop", &Variable::fprop,
+            (BodyDoc("Update value of this Var")));
+
+}
+
 ////////////
 // build_ //
 ////////////

Modified: trunk/plearn/var/Variable.h
===================================================================
--- trunk/plearn/var/Variable.h	2008-04-21 18:13:01 UTC (rev 8844)
+++ trunk/plearn/var/Variable.h	2008-04-21 20:46:35 UTC (rev 8845)
@@ -116,6 +116,9 @@
 
 protected:
     static void declareOptions(OptionList & ol);
+
+    //! Declare the methods that are remote-callable.
+    static void declareMethods(RemoteMethodMap& rmm);
   
     friend class Var;
     friend class RandomVariable;
@@ -273,6 +276,7 @@
     bool isMarked() { return marked; }
 
     void fillGradient(real value) { gradient.fill(value); }
+    void fillValue(real val) { value.fill(val); }
     void clearRowsToUpdate()
     {
         rows_to_update.resize(0);



From tihocan at mail.berlios.de  Mon Apr 21 22:47:44 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 21 Apr 2008 22:47:44 +0200
Subject: [Plearn-commits] r8846 - trunk/plearn/var
Message-ID: <200804212047.m3LKliZT015855@sheep.berlios.de>

Author: tihocan
Date: 2008-04-21 22:47:43 +0200 (Mon, 21 Apr 2008)
New Revision: 8846

Modified:
   trunk/plearn/var/UnaryVariable.cc
   trunk/plearn/var/UnaryVariable.h
Log:
Implemented the proper Object building mechanism


Modified: trunk/plearn/var/UnaryVariable.cc
===================================================================
--- trunk/plearn/var/UnaryVariable.cc	2008-04-21 20:46:35 UTC (rev 8845)
+++ trunk/plearn/var/UnaryVariable.cc	2008-04-21 20:47:43 UTC (rev 8846)
@@ -45,20 +45,41 @@
 namespace PLearn {
 using namespace std;
 
-
-/** UnaryVariable **/
-
-UnaryVariable::UnaryVariable(Variable* v, int thelength, int thewidth)
-    : Variable(thelength,thewidth), input(v) 
-{}
-
-
 PLEARN_IMPLEMENT_OBJECT(
     UnaryVariable,
     "Variable that has a single parent.",
     ""
 );
 
+///////////////////
+// UnaryVariable //
+///////////////////
+UnaryVariable::UnaryVariable(Variable* v, int thelength, int thewidth,
+                             bool call_build_):
+    Variable(thelength, thewidth, call_build_),
+    input(v) 
+{
+    if (call_build_)
+        build_();
+}
+
+///////////
+// build //
+///////////
+void UnaryVariable::build()
+{
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void UnaryVariable::build_()
+{
+    // Nothing to do here.
+}
+
 void UnaryVariable::declareOptions(OptionList& ol)
 {
     declareOption(ol, "input", &UnaryVariable::input, OptionBase::buildoption, 

Modified: trunk/plearn/var/UnaryVariable.h
===================================================================
--- trunk/plearn/var/UnaryVariable.h	2008-04-21 20:46:35 UTC (rev 8845)
+++ trunk/plearn/var/UnaryVariable.h	2008-04-21 20:47:43 UTC (rev 8846)
@@ -51,24 +51,24 @@
 
 class UnaryVariable: public Variable
 {
-public:
     typedef Variable inherited;
 
-public:
-
-    //!  Default constructor for persistence
-    UnaryVariable() {}
-
 protected:
-    static void declareOptions(OptionList & ol);
 
-protected:
     Var input;
 
 public:
-    UnaryVariable(Variable* v, int thelength, int thewidth);
 
+    //! Default constructor for persistence
+    UnaryVariable() {}
+
+    //! Convenience constructor.
+    UnaryVariable(Variable* v, int thelength, int thewidth,
+                  bool call_build_ = true);
+
     PLEARN_DECLARE_OBJECT(UnaryVariable);
+
+    virtual void build();
   
     //! Set this Variable's input (simply call build after setting the new
     //! input).
@@ -97,6 +97,13 @@
     //! will issue a PLERROR if any of the input or current value or gradient matrices are not contiguous.
     void checkContiguity() const;
 
+protected:
+
+    static void declareOptions(OptionList & ol);
+
+private:
+
+    void build_();
 };
 
 // Declares a few other classes and functions related to this class.



From tihocan at mail.berlios.de  Mon Apr 21 22:49:49 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 21 Apr 2008 22:49:49 +0200
Subject: [Plearn-commits] r8847 - trunk/plearn/var
Message-ID: <200804212049.m3LKnn23016272@sheep.berlios.de>

Author: tihocan
Date: 2008-04-21 22:49:49 +0200 (Mon, 21 Apr 2008)
New Revision: 8847

Modified:
   trunk/plearn/var/LogSoftmaxVariable.cc
   trunk/plearn/var/LogSoftmaxVariable.h
Log:
Implemented proper Object building mechanism

Modified: trunk/plearn/var/LogSoftmaxVariable.cc
===================================================================
--- trunk/plearn/var/LogSoftmaxVariable.cc	2008-04-21 20:47:43 UTC (rev 8846)
+++ trunk/plearn/var/LogSoftmaxVariable.cc	2008-04-21 20:49:49 UTC (rev 8847)
@@ -48,15 +48,39 @@
 
 /** LogSoftmaxVariable **/
 
-PLEARN_IMPLEMENT_OBJECT(LogSoftmaxVariable,
-                        "ONE LINE DESCR",
-                        "NO HELP");
+PLEARN_IMPLEMENT_OBJECT(
+        LogSoftmaxVariable,
+        "Compute the logarithm of the softmax of its input.",
+        "This is more stable than combinining a softmax and a logarithm."
+);
 
-LogSoftmaxVariable::LogSoftmaxVariable(Variable* input) 
-    : inherited(input, input->length(), input->width())
+////////////////////////
+// LogSoftmaxVariable //
+////////////////////////
+LogSoftmaxVariable::LogSoftmaxVariable(Variable* input, bool call_build_):
+    inherited(input, input->length(), input->width(), call_build_)
 {
+    if (call_build_)
+        build_();
 }
 
+///////////
+// build //
+///////////
+void LogSoftmaxVariable::build()
+{
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void LogSoftmaxVariable::build_()
+{
+    // Nothing to do here.
+}
+
 void LogSoftmaxVariable::recomputeSize(int& l, int& w) const
 {
     if (input) {

Modified: trunk/plearn/var/LogSoftmaxVariable.h
===================================================================
--- trunk/plearn/var/LogSoftmaxVariable.h	2008-04-21 20:47:43 UTC (rev 8846)
+++ trunk/plearn/var/LogSoftmaxVariable.h	2008-04-21 20:49:49 UTC (rev 8847)
@@ -54,10 +54,13 @@
     typedef UnaryVariable inherited;
 
 public:
-    //! Default constructor for pestilence
+
+    //! Default constructor.
     LogSoftmaxVariable() {};
-    LogSoftmaxVariable(Variable *input);
 
+    //! Convenience constructor.
+    LogSoftmaxVariable(Variable* input, bool call_build_ = true);
+
     PLEARN_DECLARE_OBJECT(LogSoftmaxVariable);
 
     virtual void recomputeSize(int& l, int& w) const;
@@ -65,6 +68,13 @@
     virtual void bprop();
     virtual void bbprop();
     virtual void symbolicBprop();
+
+    virtual void build();
+
+private:
+
+    void build_();
+
 }; // class LogSoftmaxVariable
 
 DECLARE_OBJECT_PTR(LogSoftmaxVariable);



From tihocan at mail.berlios.de  Mon Apr 21 22:51:05 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 21 Apr 2008 22:51:05 +0200
Subject: [Plearn-commits] r8848 - trunk/plearn/var
Message-ID: <200804212051.m3LKp5xI016621@sheep.berlios.de>

Author: tihocan
Date: 2008-04-21 22:51:04 +0200 (Mon, 21 Apr 2008)
New Revision: 8848

Modified:
   trunk/plearn/var/SoftmaxVariable.cc
Log:
Added basic class help

Modified: trunk/plearn/var/SoftmaxVariable.cc
===================================================================
--- trunk/plearn/var/SoftmaxVariable.cc	2008-04-21 20:49:49 UTC (rev 8847)
+++ trunk/plearn/var/SoftmaxVariable.cc	2008-04-21 20:51:04 UTC (rev 8848)
@@ -48,9 +48,11 @@
 
 /** SoftmaxVariable **/
 
-PLEARN_IMPLEMENT_OBJECT(SoftmaxVariable,
-                        "ONE LINE DESCR",
-                        "NO HELP");
+PLEARN_IMPLEMENT_OBJECT(
+        SoftmaxVariable,
+       "Softmax of its input variable.",
+       ""
+);
 
 SoftmaxVariable::SoftmaxVariable(Variable* input) 
     : inherited(input, input->length(), input->width())



From tihocan at mail.berlios.de  Mon Apr 21 22:51:46 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 21 Apr 2008 22:51:46 +0200
Subject: [Plearn-commits] r8849 - trunk/plearn/vmat
Message-ID: <200804212051.m3LKpkwO016729@sheep.berlios.de>

Author: tihocan
Date: 2008-04-21 22:51:46 +0200 (Mon, 21 Apr 2008)
New Revision: 8849

Modified:
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
Added getExample as remote method

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-04-21 20:51:04 UTC (rev 8848)
+++ trunk/plearn/vmat/VMatrix.cc	2008-04-21 20:51:46 UTC (rev 8849)
@@ -167,6 +167,12 @@
          RetDoc ("row i vector")));
 
     declareMethod(
+        rmm, "getExample", &VMatrix::remote_getExample,
+        (BodyDoc("Returns the input, target and weight parts of a row.\n"),
+         ArgDoc ("i", "Position of the row to get.\n"),
+         RetDoc ("An (input, target, weight) tuple.")));
+
+    declareMethod(
         rmm, "getColumn", &VMatrix::remote_getColumn,
         (BodyDoc("Returns a row of a matrix \n"),
          ArgDoc ("i", "Position of the row to get.\n"),
@@ -589,6 +595,17 @@
         weight = get(i,inputsize_+targetsize_);
 }
 
+///////////////////////
+// remote_getExample //
+///////////////////////
+boost::tuple<Vec, Vec, real> VMatrix::remote_getExample(int i)
+{
+    Vec input, target;
+    real weight;
+    getExample(i, input, target, weight);
+    return boost::tuple<Vec, Vec, real>(input, target, weight);
+}
+
 /////////////////
 // getExamples //
 /////////////////

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-04-21 20:51:04 UTC (rev 8848)
+++ trunk/plearn/vmat/VMatrix.h	2008-04-21 20:51:46 UTC (rev 8849)
@@ -484,6 +484,9 @@
      */
     virtual void getExample(int i, Vec& input, Vec& target, real& weight);
 
+    //! Remote version of getExample.
+    boost::tuple<Vec, Vec, real> remote_getExample(int i);
+
     //! Obtain a subset of 'length' examples, starting from 'i_start'.
     //! The 'extra' matrix is provided as a pointer so that it can be omitted
     //! without significant overhead.



From tihocan at mail.berlios.de  Mon Apr 21 22:52:34 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 21 Apr 2008 22:52:34 +0200
Subject: [Plearn-commits] r8850 - trunk/plearn/math
Message-ID: <200804212052.m3LKqYZN016838@sheep.berlios.de>

Author: tihocan
Date: 2008-04-21 22:52:33 +0200 (Mon, 21 Apr 2008)
New Revision: 8850

Modified:
   trunk/plearn/math/VecStatsCollector.cc
   trunk/plearn/math/VecStatsCollector.h
Log:
Added getMean() as remote method

Modified: trunk/plearn/math/VecStatsCollector.cc
===================================================================
--- trunk/plearn/math/VecStatsCollector.cc	2008-04-21 20:51:46 UTC (rev 8849)
+++ trunk/plearn/math/VecStatsCollector.cc	2008-04-21 20:52:33 UTC (rev 8850)
@@ -211,6 +211,11 @@
          RetDoc ("Requested statistic (a real number).")));
 
     declareMethod(
+        rmm, "getMean", &VecStatsCollector::remote_getMean,
+        (BodyDoc("Return the mean of each field..\n"),
+         RetDoc ("The vector of means for each field.")));
+
+    declareMethod(
         rmm, "setFieldNames", &VecStatsCollector::setFieldNames,
         (BodyDoc("Set field names.\n"),
          ArgDoc ("fieldnames", 

Modified: trunk/plearn/math/VecStatsCollector.h
===================================================================
--- trunk/plearn/math/VecStatsCollector.h	2008-04-21 20:51:46 UTC (rev 8849)
+++ trunk/plearn/math/VecStatsCollector.h	2008-04-21 20:52:33 UTC (rev 8850)
@@ -224,6 +224,11 @@
         getMean(mean);
         return mean;
     }
+
+    //! Remote version of getMean.
+    Vec remote_getMean() {
+        return getMean();
+    }
   
     //! Store the empirical mean in the given vec (which is resized)
     void getMean(Vec& mean) const;



From tihocan at mail.berlios.de  Mon Apr 21 22:53:25 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 21 Apr 2008 22:53:25 +0200
Subject: [Plearn-commits] r8851 - trunk/plearn/var
Message-ID: <200804212053.m3LKrPZl016995@sheep.berlios.de>

Author: tihocan
Date: 2008-04-21 22:53:25 +0200 (Mon, 21 Apr 2008)
New Revision: 8851

Modified:
   trunk/plearn/var/VarElementVariable.cc
   trunk/plearn/var/VarElementVariable.h
Log:
Implemented proper Object building mechanism

Modified: trunk/plearn/var/VarElementVariable.cc
===================================================================
--- trunk/plearn/var/VarElementVariable.cc	2008-04-21 20:52:33 UTC (rev 8850)
+++ trunk/plearn/var/VarElementVariable.cc	2008-04-21 20:53:25 UTC (rev 8851)
@@ -49,25 +49,39 @@
 
 /** VarElementVariable **/
 
-PLEARN_IMPLEMENT_OBJECT(VarElementVariable,
-                        "ONE LINE DESCR",
-                        "NO HELP");
+PLEARN_IMPLEMENT_OBJECT(
+    VarElementVariable,
+    "Variable that is the element of input1 indexed by input2.",
+    "If input2 has 2 elements, it is interpreted as a (i,j) pair of indices\n"
+    "in the matrix view of input1.\n"
+    "If input2 is scalar, it is interpreted as the index in the vector view\n"
+    "of input1."
+);
 
-VarElementVariable::VarElementVariable(Variable* input1, Variable* input2)
-    : inherited(input1, input2, 1, 1)
+////////////////////////
+// VarElementVariable //
+////////////////////////
+VarElementVariable::VarElementVariable(Variable* input1, Variable* input2,
+                                       bool call_build_):
+    inherited(input1, input2, 1, 1, call_build_)
 {
-    build_();
+    if (call_build_)
+        build_();
 }
 
-void
-VarElementVariable::build()
+///////////
+// build //
+///////////
+void VarElementVariable::build()
 {
     inherited::build();
     build_();
 }
 
-void
-VarElementVariable::build_()
+////////////
+// build_ //
+////////////
+void VarElementVariable::build_()
 {
     if (input2 && input2->nelems() > 2)
         PLERROR("IN VarElementVariable(Variable* input1, Variable* input2) input2 must have 1 (a k position index) or 2 elements (an i,j position index)");

Modified: trunk/plearn/var/VarElementVariable.h
===================================================================
--- trunk/plearn/var/VarElementVariable.h	2008-04-21 20:52:33 UTC (rev 8850)
+++ trunk/plearn/var/VarElementVariable.h	2008-04-21 20:53:25 UTC (rev 8851)
@@ -49,20 +49,19 @@
 using namespace std;
 
 
-/*!   Variable that is the element of the input1 variable indexed 
-  by the input2 variable.
-  If input2 has 2 elements, they are interpreted as the (i,j) indexes
-  If input2 is a scalar, it is interpreted as the k index of a vec view of input1
-*/
 class VarElementVariable: public BinaryVariable
 {
     typedef BinaryVariable inherited;
 
 public:
-    //!  Default constructor for persistence
+
+    //! Default constructor.
     VarElementVariable() {}
-    VarElementVariable(Variable* input1, Variable* input2);
 
+    //! Convenience constructor.
+    VarElementVariable(Variable* input1, Variable* input2,
+                       bool call_build_ = true);
+
     PLEARN_DECLARE_OBJECT(VarElementVariable);
 
     virtual void build();
@@ -74,8 +73,10 @@
     virtual void symbolicBprop();
     virtual void rfprop();
 
-protected:
+private:
+
     void build_();
+
 };
 
 DECLARE_OBJECT_PTR(VarElementVariable);



From tihocan at mail.berlios.de  Mon Apr 21 22:54:19 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 21 Apr 2008 22:54:19 +0200
Subject: [Plearn-commits] r8852 - trunk/plearn/var
Message-ID: <200804212054.m3LKsJDO017118@sheep.berlios.de>

Author: tihocan
Date: 2008-04-21 22:54:19 +0200 (Mon, 21 Apr 2008)
New Revision: 8852

Modified:
   trunk/plearn/var/LogAddVariable.cc
   trunk/plearn/var/LogAddVariable.h
Log:
Fixed bug when using PLearn standard object building process instead of calling the convenience constructor

Modified: trunk/plearn/var/LogAddVariable.cc
===================================================================
--- trunk/plearn/var/LogAddVariable.cc	2008-04-21 20:53:25 UTC (rev 8851)
+++ trunk/plearn/var/LogAddVariable.cc	2008-04-21 20:54:19 UTC (rev 8852)
@@ -70,6 +70,11 @@
 ////////////////////
 // LogAddVariable //
 ////////////////////
+LogAddVariable::LogAddVariable():
+    vector_logadd("none"),
+    vector_logadd_id(0)
+{}
+    
 LogAddVariable::LogAddVariable(Variable* input1, Variable* input2,
                                const string& vl,
                                bool call_build_):
@@ -135,6 +140,9 @@
             PLERROR("In LogAddVariable::build_ - input1 and input2 must "
                     "have the same size");
     }
+
+    // Need to rebuild since correct sizes depend on 'vector_logadd_id'.
+    inherited::build();
 }
 
 /////////////////////////////////

Modified: trunk/plearn/var/LogAddVariable.h
===================================================================
--- trunk/plearn/var/LogAddVariable.h	2008-04-21 20:53:25 UTC (rev 8851)
+++ trunk/plearn/var/LogAddVariable.h	2008-04-21 20:54:19 UTC (rev 8852)
@@ -66,7 +66,7 @@
     string vector_logadd;
 
     //! Default constructor.
-    LogAddVariable() {}
+    LogAddVariable();
 
     //! Convenience constructor.
     LogAddVariable(Variable* input1, Variable* input2,



From tihocan at mail.berlios.de  Mon Apr 21 22:55:06 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 21 Apr 2008 22:55:06 +0200
Subject: [Plearn-commits] r8853 - trunk/plearn/var
Message-ID: <200804212055.m3LKt6cr017230@sheep.berlios.de>

Author: tihocan
Date: 2008-04-21 22:55:06 +0200 (Mon, 21 Apr 2008)
New Revision: 8853

Modified:
   trunk/plearn/var/SumOfVariable.cc
   trunk/plearn/var/SumOfVariable.h
Log:
Fixed bug when using PLearn standard object building process instead of calling the convenience constructor

Modified: trunk/plearn/var/SumOfVariable.cc
===================================================================
--- trunk/plearn/var/SumOfVariable.cc	2008-04-21 20:54:19 UTC (rev 8852)
+++ trunk/plearn/var/SumOfVariable.cc	2008-04-21 20:55:06 UTC (rev 8853)
@@ -65,38 +65,63 @@
     "the VMatrix we are summing over).\n");
     
 
-SumOfVariable::SumOfVariable(VMat the_distr, Func the_f, int the_nsamples,bool the_do_sizeprop)
-    : inherited(nonInputParentsOfPath(the_f->inputs,the_f->outputs), 
-                the_f->outputs[0]->length(), 
-                the_f->outputs[0]->width()),
-      distr(the_distr), f(the_f), nsamples(the_nsamples), curpos(0), loop(false),
-      //input_value(the_distr->inputsize()+the_distr->targetsize()+the_distr->weightsize()), 
-      //input_gradient(the_distr->inputsize()+the_distr->targetsize()+the_distr->weightsize()), 
-      input_value(the_distr->width()),
-      input_gradient(the_distr->width()),
-      output_value(the_f->outputs[0]->size()),
-      do_sizeprop(the_do_sizeprop)
+///////////////////
+// SumOfVariable //
+///////////////////
+SumOfVariable::SumOfVariable():
+    nsamples(0),
+    curpos(0),
+    loop(false),
+    do_sizeprop(false)
+{}
+
+SumOfVariable::SumOfVariable(VMat the_distr, Func the_f, int the_nsamples,
+                             bool the_do_sizeprop, bool call_build_):
+    inherited(nonInputParentsOfPath(the_f->inputs, the_f->outputs), 
+            the_f->outputs[0]->length(), 
+            the_f->outputs[0]->width(),
+            call_build_),
+    distr(the_distr),
+    f(the_f),
+    nsamples(the_nsamples),
+    curpos(0),
+    loop(false),
+    input_value(the_distr->width()),
+    input_gradient(the_distr->width()),
+    output_value(the_f->outputs[0]->size()),
+    do_sizeprop(the_do_sizeprop)
 {
-    build_();
+    if (call_build_)
+        build_();
 }
 
-void
-SumOfVariable::build()
+///////////
+// build //
+///////////
+void SumOfVariable::build()
 {
     inherited::build();
     build_();
 }
 
-void
-SumOfVariable::build_()
+////////////
+// build_ //
+////////////
+void SumOfVariable::build_()
 {
     if (f && distr) 
     {
+        varray = nonInputParentsOfPath(f->inputs, f->outputs);
+        // We need to rebuild the parent class since a build option changed.
+        inherited::build();
+
         input_value.resize(distr->inputsize() + distr->targetsize() + distr->weightsize());
         input_gradient.resize(distr->inputsize() + distr->targetsize() + distr->weightsize());
         if(f->outputs.size() != 1)
-            PLERROR("In SumOfVariable: function must have a single variable output (maybe you can vconcat the vars into a single one prior to calling sumOf, if this is really what you want)");
-
+            PLERROR("In SumOfVariable::build_: function must have a single "
+                    "variable output (maybe you can vconcat the vars into a "
+                    "single one prior to calling sumOf, if this is really "
+                    "what you want)");
         if(nsamples == -1)
             nsamples = distr->length();
         f->inputs.setDontBpropHere(true);

Modified: trunk/plearn/var/SumOfVariable.h
===================================================================
--- trunk/plearn/var/SumOfVariable.h	2008-04-21 20:54:19 UTC (rev 8852)
+++ trunk/plearn/var/SumOfVariable.h	2008-04-21 20:55:06 UTC (rev 8853)
@@ -88,11 +88,14 @@
     int endpos;
     
 public:
-    //!  protected default constructor for persistence
-    SumOfVariable() : distr(), f(), nsamples(0), curpos(0), loop(false) {}
 
-    //!  Sum_{inputs \in distr} f(inputs)
-    SumOfVariable(VMat the_distr, Func the_f, int the_nsamples=-1, bool the_do_resizeprop=false);
+    //! Default constructor.
+    SumOfVariable();
+
+    //! Convenience constructor.
+    SumOfVariable(VMat the_distr, Func the_f, int the_nsamples=-1,
+                  bool the_do_resizeprop = false,
+                  bool call_build_ = true);
     
     PLEARN_DECLARE_OBJECT(SumOfVariable);
     static void declareOptions(OptionList &ol);



From tihocan at mail.berlios.de  Mon Apr 21 22:56:26 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 21 Apr 2008 22:56:26 +0200
Subject: [Plearn-commits] r8854 - trunk/plearn/var
Message-ID: <200804212056.m3LKuQfd018049@sheep.berlios.de>

Author: tihocan
Date: 2008-04-21 22:56:25 +0200 (Mon, 21 Apr 2008)
New Revision: 8854

Modified:
   trunk/plearn/var/SumOverBagsVariable.cc
   trunk/plearn/var/SumOverBagsVariable.h
Log:
Fixed bug when using PLearn standard object building process instead of calling the convenience constructor

Modified: trunk/plearn/var/SumOverBagsVariable.cc
===================================================================
--- trunk/plearn/var/SumOverBagsVariable.cc	2008-04-21 20:55:06 UTC (rev 8853)
+++ trunk/plearn/var/SumOverBagsVariable.cc	2008-04-21 20:56:25 UTC (rev 8854)
@@ -75,25 +75,29 @@
 /////////////////////////
 // SumOverBagsVariable //
 /////////////////////////
-SumOverBagsVariable::SumOverBagsVariable()
-    : vmat(), f(),
-      average(0),
-      max_bag_size(-1), n_samples(1),
-      transpose(0),
-      curpos()
+SumOverBagsVariable::SumOverBagsVariable():
+    average(false),
+    max_bag_size(-1),
+    n_samples(1),
+    transpose(false),
+    curpos()
 {}
 
-SumOverBagsVariable::SumOverBagsVariable(VMat the_vmat, Func the_f, int max_bagsize, int nsamples, bool the_average, bool the_transpose)
-    : inherited(nonInputParentsOfPath(the_f->inputs,the_f->outputs), 
-                the_f->outputs[0]->length(), 
-                the_f->outputs[0]->width()),
-      vmat(the_vmat), f(the_f),
-      average(the_average),
-      max_bag_size(max_bagsize), n_samples(nsamples),
-      transpose(the_transpose),
-      curpos(0), bag_size(0)
+SumOverBagsVariable::SumOverBagsVariable(
+        VMat the_vmat, Func the_f, int max_bagsize, int nsamples,
+        bool the_average, bool the_transpose, bool call_build_):
+    inherited(nonInputParentsOfPath(the_f->inputs,the_f->outputs), 
+            the_f->outputs[0]->length(), 
+            the_f->outputs[0]->width(),
+            call_build_),
+    vmat(the_vmat), f(the_f),
+    average(the_average),
+    max_bag_size(max_bagsize), n_samples(nsamples),
+    transpose(the_transpose),
+    curpos(0), bag_size(0)
 {
-    build();
+    if (call_build_)
+        build_();
 }
 
 ///////////
@@ -112,6 +116,12 @@
 {
     if (vmat)
     {
+        PLASSERT( f );
+
+        varray = nonInputParentsOfPath(f->inputs, f->outputs);
+        // We need to rebuild the parent class since a build option changed.
+        inherited::build();
+
         if (f->outputs.size()!=1)
             PLERROR("SumOverBagsVariable: expected a func with a single output variable (you may use concat to form a single output Var)");
         if (vmat->weightsize()!=0 && vmat->weightsize()!=1)

Modified: trunk/plearn/var/SumOverBagsVariable.h
===================================================================
--- trunk/plearn/var/SumOverBagsVariable.h	2008-04-21 20:55:06 UTC (rev 8853)
+++ trunk/plearn/var/SumOverBagsVariable.h	2008-04-21 20:56:25 UTC (rev 8854)
@@ -89,7 +89,9 @@
     //!   last_column_of_target == 1+2==3 ==> single-row bag (both first and last)
     //! the last column of the target is not given in the call to f, but a bag_size input is provided instead.
     //! The inputs to f are: (matrix of bag inputs, the bag size, the bag target, the bag weight).
-    SumOverBagsVariable(VMat the_vmat, Func the_f, int maxbagsize, int nsamples, bool average, bool transpose = false);
+    SumOverBagsVariable(VMat the_vmat, Func the_f, int maxbagsize,
+                        int nsamples, bool average,
+                        bool transpose = false, bool call_build_ = true);
 
     //! Tags to use in the vmat given to this variable.
     static const int TARGET_COLUMN_FIRST = 1;



From tihocan at mail.berlios.de  Mon Apr 21 22:57:22 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 21 Apr 2008 22:57:22 +0200
Subject: [Plearn-commits] r8855 - trunk/plearn/var
Message-ID: <200804212057.m3LKvMNx018438@sheep.berlios.de>

Author: tihocan
Date: 2008-04-21 22:57:22 +0200 (Mon, 21 Apr 2008)
New Revision: 8855

Modified:
   trunk/plearn/var/SourceVariable.cc
Log:
Cosmetic change

Modified: trunk/plearn/var/SourceVariable.cc
===================================================================
--- trunk/plearn/var/SourceVariable.cc	2008-04-21 20:56:25 UTC (rev 8854)
+++ trunk/plearn/var/SourceVariable.cc	2008-04-21 20:57:22 UTC (rev 8855)
@@ -62,7 +62,7 @@
     random_type("none"),
     random_a(0.),
     random_b(1.),
-    random_clear_first_row(0)
+    random_clear_first_row(false)
 {}
 
 SourceVariable::SourceVariable(int thelength, int thewidth, bool call_build_):
@@ -72,7 +72,7 @@
     random_type("none"),
     random_a(0.),
     random_b(1.),
-    random_clear_first_row(0)
+    random_clear_first_row(false)
 {
     if (call_build_)
         build_();
@@ -86,7 +86,7 @@
     random_type("none"),
     random_a(0.),
     random_b(1.),
-    random_clear_first_row(0)
+    random_clear_first_row(false)
 {
     if (call_build_)
         build_();
@@ -99,7 +99,7 @@
     random_type("none"),
     random_a(0.),
     random_b(1.),
-    random_clear_first_row(0)
+    random_clear_first_row(false)
 {
     if (call_build_)
         build_();



From tihocan at mail.berlios.de  Mon Apr 21 22:57:53 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 21 Apr 2008 22:57:53 +0200
Subject: [Plearn-commits] r8856 - trunk/plearn/var
Message-ID: <200804212057.m3LKvrg4018486@sheep.berlios.de>

Author: tihocan
Date: 2008-04-21 22:57:52 +0200 (Mon, 21 Apr 2008)
New Revision: 8856

Modified:
   trunk/plearn/var/NegateElementsVariable.cc
   trunk/plearn/var/NegateElementsVariable.h
Log:
Implemented proper Object building mechanism

Modified: trunk/plearn/var/NegateElementsVariable.cc
===================================================================
--- trunk/plearn/var/NegateElementsVariable.cc	2008-04-21 20:57:22 UTC (rev 8855)
+++ trunk/plearn/var/NegateElementsVariable.cc	2008-04-21 20:57:52 UTC (rev 8856)
@@ -50,15 +50,42 @@
 
 /** NegateElementsVariable **/
 
-PLEARN_IMPLEMENT_OBJECT(NegateElementsVariable,
-                        "Elementwise negation and inversion...",
-                        "NO HELP");
+PLEARN_IMPLEMENT_OBJECT(
+        NegateElementsVariable,
+        "Elementwise negation of the input value.",
+        ""
+);
 
-NegateElementsVariable::NegateElementsVariable(Variable* input)
-    : inherited(input, input->length(), input->width())
-{}
+////////////////////////////
+// NegateElementsVariable //
+////////////////////////////
+NegateElementsVariable::NegateElementsVariable(Variable* input,
+                                               bool call_build_):
+    inherited(input, input->length(), input->width(), call_build_)
+{
+    if (call_build_)
+        build_();
+}
 
 
+///////////
+// build //
+///////////
+void NegateElementsVariable::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+////////////
+// build_ //
+////////////
+void NegateElementsVariable::build_()
+{
+    // Nothing to do here.
+}
+
 void NegateElementsVariable::recomputeSize(int& l, int& w) const
 {
     if (input) {

Modified: trunk/plearn/var/NegateElementsVariable.h
===================================================================
--- trunk/plearn/var/NegateElementsVariable.h	2008-04-21 20:57:22 UTC (rev 8855)
+++ trunk/plearn/var/NegateElementsVariable.h	2008-04-21 20:57:52 UTC (rev 8856)
@@ -49,19 +49,22 @@
 using namespace std;
 
 
-/*! * Elementwise negation and inversion... * */
-
 class NegateElementsVariable: public UnaryVariable
 {
     typedef UnaryVariable inherited;
 
 public:
-    //!  Default constructor for persistence
+
+    //! Default constructor.
     NegateElementsVariable() {}
-    NegateElementsVariable(Variable* input);
 
+    //! Convenience constructor.
+    NegateElementsVariable(Variable* input, bool call_build_ = true);
+
     PLEARN_DECLARE_OBJECT(NegateElementsVariable);
 
+    virtual void build();
+
     virtual void recomputeSize(int& l, int& w) const;
     
     virtual void fprop();
@@ -69,6 +72,11 @@
     virtual void bbprop();
     virtual void symbolicBprop();
     virtual void rfprop();
+
+private:
+
+    void build_();
+
 };
 
 DECLARE_OBJECT_PTR(NegateElementsVariable);



From tihocan at mail.berlios.de  Mon Apr 21 22:58:25 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 21 Apr 2008 22:58:25 +0200
Subject: [Plearn-commits] r8857 - trunk/plearn/var
Message-ID: <200804212058.m3LKwPAW018524@sheep.berlios.de>

Author: tihocan
Date: 2008-04-21 22:58:25 +0200 (Mon, 21 Apr 2008)
New Revision: 8857

Modified:
   trunk/plearn/var/TimesScalarVariable.cc
   trunk/plearn/var/TimesScalarVariable.h
Log:
Implemented proper Object building mechanism

Modified: trunk/plearn/var/TimesScalarVariable.cc
===================================================================
--- trunk/plearn/var/TimesScalarVariable.cc	2008-04-21 20:57:52 UTC (rev 8856)
+++ trunk/plearn/var/TimesScalarVariable.cc	2008-04-21 20:58:25 UTC (rev 8857)
@@ -50,18 +50,27 @@
 
 /** TimesScalarVariable **/
 
-PLEARN_IMPLEMENT_OBJECT(TimesScalarVariable,
-                        "Multiplies a matrix var by a scalar var",
-                        "NO HELP");
+PLEARN_IMPLEMENT_OBJECT(
+        TimesScalarVariable,
+       "Multiplies a matrix variable by a scalar variable.",
+       "The first input is the matrix, while the second one is the scalar."
+);
 
-TimesScalarVariable::TimesScalarVariable(Variable* input1, Variable* input2)
-    : inherited(input1, input2, input1->length(), input1->width())
+/////////////////////////
+// TimesScalarVariable //
+/////////////////////////
+TimesScalarVariable::TimesScalarVariable(Variable* input1, Variable* input2,
+                                         bool call_build_):
+    inherited(input1, input2, input1->length(), input1->width(), call_build_)
 {
-    build_();
+    if (call_build_)
+        build_();
 }
 
-void
-TimesScalarVariable::build()
+///////////
+// build //
+///////////
+void TimesScalarVariable::build()
 {
     inherited::build();
     build_();

Modified: trunk/plearn/var/TimesScalarVariable.h
===================================================================
--- trunk/plearn/var/TimesScalarVariable.h	2008-04-21 20:57:52 UTC (rev 8856)
+++ trunk/plearn/var/TimesScalarVariable.h	2008-04-21 20:58:25 UTC (rev 8857)
@@ -55,10 +55,14 @@
     typedef BinaryVariable inherited;
 
 public:
-    //!  Default constructor for persistence
+
+    //! Default constructor.
     TimesScalarVariable() {}
-    TimesScalarVariable(Variable* input1, Variable* input2);
 
+    //! Convenience constructor.
+    TimesScalarVariable(Variable* input1, Variable* input2,
+                        bool call_build_ = true);
+
     PLEARN_DECLARE_OBJECT(TimesScalarVariable);
 
     virtual void build();
@@ -69,7 +73,8 @@
     virtual void symbolicBprop();
     virtual void rfprop();
 
-protected:
+private:
+
     void build_();
 };
 



From tihocan at mail.berlios.de  Mon Apr 21 22:59:04 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 21 Apr 2008 22:59:04 +0200
Subject: [Plearn-commits] r8858 - trunk/plearn/opt
Message-ID: <200804212059.m3LKx43A018587@sheep.berlios.de>

Author: tihocan
Date: 2008-04-21 22:59:04 +0200 (Mon, 21 Apr 2008)
New Revision: 8858

Modified:
   trunk/plearn/opt/ConjGradientOptimizer.cc
Log:
Using pout instead of cout

Modified: trunk/plearn/opt/ConjGradientOptimizer.cc
===================================================================
--- trunk/plearn/opt/ConjGradientOptimizer.cc	2008-04-21 20:58:25 UTC (rev 8857)
+++ trunk/plearn/opt/ConjGradientOptimizer.cc	2008-04-21 20:59:04 UTC (rev 8858)
@@ -294,7 +294,7 @@
         real delta_n = pownorm(delta);
         if (abs(dp) > restart_coeff *delta_n ) {
             if (verbosity >= 5)
-                cout << "Restart triggered !" << endl;
+                pout << "Restart triggered !" << endl;
             gamma = 0;
         }
     }



From tihocan at mail.berlios.de  Mon Apr 21 22:59:30 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 21 Apr 2008 22:59:30 +0200
Subject: [Plearn-commits] r8859 - trunk/python_modules/plearn/math
Message-ID: <200804212059.m3LKxUa5018657@sheep.berlios.de>

Author: tihocan
Date: 2008-04-21 22:59:30 +0200 (Mon, 21 Apr 2008)
New Revision: 8859

Modified:
   trunk/python_modules/plearn/math/__init__.py
Log:
Added a logadd function

Modified: trunk/python_modules/plearn/math/__init__.py
===================================================================
--- trunk/python_modules/plearn/math/__init__.py	2008-04-21 20:59:04 UTC (rev 8858)
+++ trunk/python_modules/plearn/math/__init__.py	2008-04-21 20:59:30 UTC (rev 8859)
@@ -47,3 +47,17 @@
     if minabs<1.0:
         return abs(a-b) <= numtol
     return abs(a-b) <= numtol*minabs
+
+def logadd(log_a, log_b):
+    """Stable computation of log(exp(log_a) + exp(log_b))."""
+    if log_a < log_b:
+        tmp = log_a
+        log_a = log_b
+        log_b = tmp
+    elif log_a == log_b:
+        return math.log(2.0) + log_a
+    negative_absolute_difference = log_b - log_a
+    if negative_absolute_difference < -18.42:
+        return log_a
+    return log_a + math.log(1.0 + math.exp(negative_absolute_difference))
+



From tihocan at mail.berlios.de  Mon Apr 21 23:00:03 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 21 Apr 2008 23:00:03 +0200
Subject: [Plearn-commits] r8860 - trunk/python_modules/plearn/math
Message-ID: <200804212100.m3LL034v018792@sheep.berlios.de>

Author: tihocan
Date: 2008-04-21 23:00:02 +0200 (Mon, 21 Apr 2008)
New Revision: 8860

Modified:
   trunk/python_modules/plearn/math/arrays.py
Log:
Added logadd functions for vectors and columns of a matrix

Modified: trunk/python_modules/plearn/math/arrays.py
===================================================================
--- trunk/python_modules/plearn/math/arrays.py	2008-04-21 20:59:30 UTC (rev 8859)
+++ trunk/python_modules/plearn/math/arrays.py	2008-04-21 21:00:02 UTC (rev 8860)
@@ -39,7 +39,10 @@
 from numpy.numarray.linear_algebra import determinant
 from numpy import *
 import numpy.numarray as ufunc #most ufuncs are part of the numpy module
+import numpy
 
+import plearn.math
+
 def _2D_shape(M):
     if len(M.shape)==1:
         return M.shape[0], 1
@@ -225,6 +228,23 @@
 def all(x):
     return logical_and.reduce(ravel(x))
 
+def vectorLogadd(v):
+    """Logadd on a vector."""
+    if len(v) == 0:
+        return -numpy.inf
+    result = v[0]
+    for i in range(1, len(v)):
+        result = plearn.math.logadd(result, v[i])
+    return result
+
+def columnLogadd(m):
+    """Logadd on columns of a matrix."""
+    n_cols = m.shape[1]
+    result = numpy.zeros(n_cols)
+    for i in range(n_cols):
+        result[i] = vectorLogadd(m[:, i])
+    return result
+
 if __name__ == '__main__':
     a = array(range(10))
     print "lag(%s): %s"%(a, lag(a))



From louradou at mail.berlios.de  Tue Apr 22 06:05:52 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Tue, 22 Apr 2008 06:05:52 +0200
Subject: [Plearn-commits] r8861 - in trunk/plearn_learners: classifiers
	online
Message-ID: <200804220405.m3M45q9T014169@sheep.berlios.de>

Author: louradou
Date: 2008-04-22 06:05:51 +0200 (Tue, 22 Apr 2008)
New Revision: 8861

Modified:
   trunk/plearn_learners/classifiers/ToBagClassifier.cc
   trunk/plearn_learners/online/NLLCostModule.cc
Log:
* in NLLCostModule, bug fixed when target is missing.
* in ToBagClassifier, a tiny code improvement for a PLASSERT.



Modified: trunk/plearn_learners/classifiers/ToBagClassifier.cc
===================================================================
--- trunk/plearn_learners/classifiers/ToBagClassifier.cc	2008-04-21 21:00:02 UTC (rev 8860)
+++ trunk/plearn_learners/classifiers/ToBagClassifier.cc	2008-04-22 04:05:51 UTC (rev 8861)
@@ -207,8 +207,10 @@
 #endif
         return;
     }
-
-    PLASSERT( sum(output) < 1.001 & sum(output) > 0.999);
+    // Ensure the distribution probabilities sum to 1. We relax a
+    // bit the default tolerance as probabilities using
+    // exponentials could suffer numerical imprecisions.
+    PLASSERT( is_equal( sum(output), 1., 1., 1e-5, 1e-5 ) );
     bag_output.appendRow(output);
     if (bag_info & SumOverBagsVariable::TARGET_COLUMN_LAST) {
         // Perform majority vote.

Modified: trunk/plearn_learners/online/NLLCostModule.cc
===================================================================
--- trunk/plearn_learners/online/NLLCostModule.cc	2008-04-21 21:00:02 UTC (rev 8860)
+++ trunk/plearn_learners/online/NLLCostModule.cc	2008-04-22 04:05:51 UTC (rev 8861)
@@ -151,7 +151,7 @@
 
         for( int i=0; i<batch_size; i++ )
         {
-            if( (*prediction)(i).hasMissing() )
+            if( (*prediction)(i).hasMissing() || is_missing((*target)(i,0)) )
                 (*cost)(i,0) = MISSING_VALUE;
             else
             {



From tihocan at mail.berlios.de  Tue Apr 22 16:06:32 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 22 Apr 2008 16:06:32 +0200
Subject: [Plearn-commits] r8862 - trunk/plearn/opt
Message-ID: <200804221406.m3ME6WvT007906@sheep.berlios.de>

Author: tihocan
Date: 2008-04-22 16:06:32 +0200 (Tue, 22 Apr 2008)
New Revision: 8862

Modified:
   trunk/plearn/opt/Optimizer.cc
   trunk/plearn/opt/Optimizer.h
Log:
- Added early_stop as a learnt option
- Added setToOptimize and optimizeN as remote methods


Modified: trunk/plearn/opt/Optimizer.cc
===================================================================
--- trunk/plearn/opt/Optimizer.cc	2008-04-22 04:05:51 UTC (rev 8861)
+++ trunk/plearn/opt/Optimizer.cc	2008-04-22 14:06:32 UTC (rev 8862)
@@ -94,9 +94,35 @@
 {
     declareOption(ol, "nstages", &Optimizer::nstages, OptionBase::buildoption, 
         "Number of iterations to perform on the next call to optimizeN(..).");
+
+    declareOption(ol, "early_stop", &Optimizer::early_stop,
+                  OptionBase::learntoption, 
+        "Whether an early stopping criterion has been met.");
+
     inherited::declareOptions(ol);
 }
 
+////////////////////
+// declareMethods //
+////////////////////
+void Optimizer::declareMethods(RemoteMethodMap& rmm)
+{
+    // Insert a backpointer to remote methods; note that this
+    // different than for declareOptions()
+    rmm.inherited(inherited::_getRemoteMethodMap_());
+        
+    declareMethod(rmm, "setToOptimize", &Optimizer::remote_setToOptimize,
+            (BodyDoc("Set cost to minimize with respect to given parameters"),
+             ArgDoc("params", "List of parameters (variables) to optimize"),
+             ArgDoc("cost", "Cost to be minimized")));
+
+    declareMethod(rmm, "optimizeN", &Optimizer::remote_optimizeN,
+            (BodyDoc("Launch nstages steps of optimization."),
+             ArgDoc("stats", "VecStatsCollector to collect training statistics"),
+             RetDoc("Boolean value indicating whether a stopping criterion "
+                    "has been met.")));
+}
+
 ///////////////////
 // setToOptimize //
 ///////////////////
@@ -118,6 +144,14 @@
     other_weight = the_other_weight;
 }
 
+//////////////////////////
+// remote_setToOptimize //
+//////////////////////////
+void Optimizer::remote_setToOptimize(const VarArray& params, Var cost)
+{
+    setToOptimize(params, cost);
+}
+
 /*
 void Optimizer::setVarArrayOption(const string& optionname, VarArray value)
 {

Modified: trunk/plearn/opt/Optimizer.h
===================================================================
--- trunk/plearn/opt/Optimizer.h	2008-04-22 04:05:51 UTC (rev 8861)
+++ trunk/plearn/opt/Optimizer.h	2008-04-22 14:06:32 UTC (rev 8862)
@@ -104,6 +104,9 @@
 
     virtual void setToOptimize(const VarArray& the_params, Var the_cost, VarArray the_other_costs = VarArray(0), TVec<VarArray> the_other_params = TVec<VarArray>(0), real the_other_weight = 1);
 
+    //! Remote version of setToOptimize.
+    void remote_setToOptimize(const VarArray& params, Var cost);
+
     /*
     virtual void setVarArrayOption(const string& optionname,
                                    const VarArray& value);
@@ -126,6 +129,11 @@
      * }
      * return false
      */
+
+    bool remote_optimizeN(PP<VecStatsCollector> stats_coll) {
+        PLASSERT( stats_coll.isNotNull() );
+        return optimizeN(*stats_coll);
+    }
       
     //!  verify gradient with uniform random initialization of parameters
     //!  using step for the finite difference approximation of the gradient
@@ -144,6 +152,9 @@
 
     static void declareOptions(OptionList& ol);
 
+    //! Declare the methods that are remote-callable.
+    static void declareMethods(RemoteMethodMap& rmm);
+
 public:
 
     //--------------------------- UTILITY FUNCTIONS ----------------------------



From tihocan at mail.berlios.de  Tue Apr 22 19:26:06 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 22 Apr 2008 19:26:06 +0200
Subject: [Plearn-commits] r8863 -
	trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results
Message-ID: <200804221726.m3MHQ6Bp013900@sheep.berlios.de>

Author: tihocan
Date: 2008-04-22 19:26:05 +0200 (Tue, 22 Apr 2008)
New Revision: 8863

Modified:
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
Log:
Fixed test PL_GaussianProcessRegressor_Hyperopt

Modified: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2008-04-22 14:06:32 UTC (rev 8862)
+++ trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2008-04-22 17:26:05 UTC (rev 8863)
@@ -58,7 +58,8 @@
 slope_ratio = 10 ;
 minibatch_n_samples = 0 ;
 minibatch_n_line_searches = 3 ;
-nstages = 1  )
+nstages = 1 ;
+early_stop = 0  )
 ;
 save_gram_matrix = 0 ;
 alpha = 5  1  [ 



From tihocan at mail.berlios.de  Tue Apr 22 19:34:22 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 22 Apr 2008 19:34:22 +0200
Subject: [Plearn-commits] r8864 - trunk/plearn/math
Message-ID: <200804221734.m3MHYM3w015251@sheep.berlios.de>

Author: tihocan
Date: 2008-04-22 19:34:22 +0200 (Tue, 22 Apr 2008)
New Revision: 8864

Modified:
   trunk/plearn/math/StatsCollector.h
Log:
sum() and sumsquare() now return 0 when no non missing value has been observed, instead of NaN

Modified: trunk/plearn/math/StatsCollector.h
===================================================================
--- trunk/plearn/math/StatsCollector.h	2008-04-22 17:26:05 UTC (rev 8863)
+++ trunk/plearn/math/StatsCollector.h	2008-04-22 17:34:22 UTC (rev 8864)
@@ -236,10 +236,13 @@
     real nmissing() const               { return nmissing_; }
     real nnonmissing() const            { return nnonmissing_; }
     real sumsquarew() const             { return sumsquarew_; }
-    real sum() const                    { return real(sum_+nnonmissing_*first_); }
-    //real sumsquare() const { return real(sumsquare_); }
-    real sumsquare() const              { return real(sumsquare_+2*first_*sum() -
-                                                      first_*first_*nnonmissing_); }
+    real sum() const                    { return real(nnonmissing_ > 0
+                                                ? sum_ + nnonmissing_*first_
+                                                : 0); }
+    real sumsquare() const              { return real(nnonmissing_ > 0
+                                                ? sumsquare_+2*first_*sum() -
+                                                    first_*first_*nnonmissing_
+                                                : 0); }
     real min() const                    { return min_; }
     real max() const                    { return max_; }
     real agemin() const                 { return agemin_; }



From tihocan at mail.berlios.de  Tue Apr 22 20:02:00 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 22 Apr 2008 20:02:00 +0200
Subject: [Plearn-commits] r8865 - in
	trunk/plearn_learners/regressors/test/RegressionTree/.pytest:
	PL_RegressionTree/expected_results/expdir
	PL_RegressionTree_MultiClass/expected_results/expdir
Message-ID: <200804221802.m3MI204g017908@sheep.berlios.de>

Author: tihocan
Date: 2008-04-22 20:02:00 +0200 (Tue, 22 Apr 2008)
New Revision: 8865

Modified:
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave
Log:
Fixed tests PL_RegressionTree_MultiClass and PL_RegressionTree

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave	2008-04-22 17:34:22 UTC (rev 8864)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave	2008-04-22 18:02:00 UTC (rev 8865)
@@ -150,8 +150,7 @@
 rerun_after_sub = 0 ;
 provide_sub_expdir = 1 ;
 save_best_learner = 0 ;
-splitter = *0 ;
-verbosity = 0  )
+splitter = *0 )
 ] ;
 provide_strategy_expdir = 1 ;
 save_final_learner = 0 ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave	2008-04-22 17:34:22 UTC (rev 8864)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave	2008-04-22 18:02:00 UTC (rev 8865)
@@ -164,8 +164,7 @@
 rerun_after_sub = 0 ;
 provide_sub_expdir = 1 ;
 save_best_learner = 0 ;
-splitter = *0 ;
-verbosity = 0  )
+splitter = *0 )
 ] ;
 provide_strategy_expdir = 1 ;
 save_final_learner = 0 ;



From nouiz at mail.berlios.de  Wed Apr 23 21:14:50 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 23 Apr 2008 21:14:50 +0200
Subject: [Plearn-commits] r8866 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200804231914.m3NJEomW032691@sheep.berlios.de>

Author: nouiz
Date: 2008-04-23 21:14:50 +0200 (Wed, 23 Apr 2008)
New Revision: 8866

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
Added option --raw=STRING --rank=CONDOR_EXPRESSION --[no_]prefserver for the condor backend


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-04-22 18:02:00 UTC (rev 8865)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-04-23 19:14:50 UTC (rev 8866)
@@ -668,6 +668,8 @@
         self.getenv = False
         self.nice = False
         self.req = ''
+        self.raw = ''
+        self.rank = ''
         self.copy_local_source_file = False
         self.files = ''
         DBIBase.__init__(self, commands, **args)
@@ -821,6 +823,12 @@
                 should_transfer_files = Yes
                 transfer_input_files = %s
                 '''%(self.files+','+launch_file+','+self.tasks[0].commands[0].split()[0]))) # no directory
+        if self.raw:
+            condor_dat.write( self.raw+'\n')
+        if self.rank:
+            condor_dat.write( dedent('''\
+                rank = %s
+                ''' %(self.rank)))
         if len(condor_datas)!=0:
             for i in condor_datas:
                 condor_dat.write("arguments      = sh "+i+" $$(Arch) \nqueue\n")

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-04-22 18:02:00 UTC (rev 8865)
+++ trunk/scripts/dbidispatch	2008-04-23 19:14:50 UTC (rev 8866)
@@ -3,7 +3,7 @@
 from plearn.utilities.toolkit import search_file
 
 ScriptName="launchdbi.py"
-ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|**--nodbilog] [--condor|--bqtools[=nb_proc]|--cluster[=nb_proc]|--local[=nb_proc]|--ssh[=nb_proc]] [--nb_proc=nb_proc] [--mem=X] [--os=X] [--test|*--no_test] [--long|**--no_long] [--micro[=nb_batch]] [--duree=X] [**--cwait|--no_cwait] [--req="CONDOR_REQUIREMENT"] [--force|**--no_force] [--nice|**--no_nice] [--interruptible|**--no_interruptible] [--cpu=nb_cpu_per_node] [**--getenv|--no_getenv] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- signals the default option value in dbidispatch. An ** before -- signal a default option value in dbi'
+ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|**--nodbilog] [--condor|--bqtools[=nb_proc]|--cluster[=nb_proc]|--local[=nb_proc]|--ssh[=nb_proc]] [--nb_proc=nb_proc] [--mem=X] [--os=X] [--test|*--no_test] [--long|**--no_long] [--micro[=nb_batch]] [--duree=X] [**--cwait|--no_cwait] [--req="CONDOR_REQUIREMENT"] [--force|**--no_force] [--nice|**--no_nice] [--interruptible|**--no_interruptible] [--cpu=nb_cpu_per_node] [**--getenv|--no_getenv] [--32|--64|--3264] [--[no_]prefserver] [--rank=RANK_EXPRESSION] [--files=file1[,file2...]] [--raw=CONDOR_EXPRESSION] {--file=FILEPATH | <command-template>} \n An * before -- signals the default option value in dbidispatch. An ** before -- signal a default option value in dbi'
 LongHelp="""Dispatches jobs with dbi.py. dbi can dispatch jobs on condor, bqtools, cluster, local and ssh. If no system is selected on the command line, we try them in the previous order. ssh is never automaticaly selected.
 
 %(ShortHelp)s
@@ -64,6 +64,8 @@
   dbidispatch '--req=Machine=="computer.example.com"' 
 
   The '--server'(--no_server) option add the requirement that the executing host must be a server dedicated to computing. This is equivalent to: dbidispatch '--req=SERVER==True'(SERVER==False)
+  The '--prefserver' option will tell that you prefer to execute on server first. This is equivalent to 'rank=SERVER=?=True' in the submit file.
+  The '--rank=STRING' option add rank=STRING in the submit file.
   The '--machine=full_host_name' option add the requirement that the executing host is full_host_name
      dbidispatch --machine=computer.example.com
         witch is equivalent to
@@ -73,6 +75,7 @@
         witch is equivalent to
      dbidispatch '--req=regexp("computer0*", target.Machine)'
   The '--nice'('--no_nice') option set the nice_user option to condor. If nice, the job(s) will have the lowest possible priority.
+  The '--raw=STRING1[\nSTRING2...]' option add all the STRINGX parameter to the submit file of condor.
 
 where <command-template> is interpreted as follows: the first argument
 is the <command> above, and the rest are interpreted as <arguments>.
@@ -164,8 +167,16 @@
                   "--no_getenv", "--no_cwait", "--no_clean_up" , "--no_nice"]:
         dbi_param[argv[5:]]=False
     elif argv.split('=')[0] in ["--duree","--cpu","--mem","--os",
-                                "--nb_proc","--req", "--files"]:
-        dbi_param[argv.split('=')[0][2:]]=argv.split('=')[1]
+                                "--nb_proc","--req", "--files", "--raw",
+                                "--rank"]:
+        param=argv.split('=')[0][2:]
+        if param in ["req", "files", "raw", "rank"]:
+            #param that we happend to if defined more then one time
+            dbi_param.setdefault(param,'True')
+            dbi_param[param]+='&&('+argv.split('=',1)[1]+')'
+        else:
+            #otherwise we erase the old value
+            dbi_param[param]=argv.split('=',1)[1]
     elif argv.startswith('--machine=') or argv.startswith('--machines='):
         if argv.split('=')[0] == "--machine":
             new='Machine=="'+argv.split('=')[1]+'"'
@@ -175,15 +186,17 @@
         if s:
             s+=' && '
         dbi_param["req"]=s+new
-    elif argv=="--server" or argv=="--no_server":
-        if 'req' in dbi_param:
-            dbi_param["req"]+=' && '
+    elif argv=="--server" or argv=="--no_server" \
+            or argv=='--prefserver' or argv=="--no_prefserver":
+        if argv.find('prefserver')!=-1:
+            param='rank'
         else:
-            dbi_param['req']=''
-        if argv=="--server":
-            dbi_param["req"]+='SERVER==True'
+            param='req'
+        dbi_param.setdefault(param,'True')
+        if argv.find('no_')!=-1:
+            dbi_param[param]+='&&(SERVER==True)'
         else:
-            dbi_param["req"]+='SERVER==False'
+            dbi_param[param]+='&&(SERVER==False)'
     elif argv[0:1] == '-':
 	print "Unknow option (%s)"%argv
 	print ShortHelp
@@ -203,7 +216,7 @@
     valid_dbi_param +=["cwait","force","nb_proc","arch","interruptible",
                        "duree","cpu","mem","os"]
 elif launch_cmd=="Condor":
-    valid_dbi_param +=["req", "arch", "getenv", "nice", "files"]
+    valid_dbi_param +=["req", "arch", "getenv", "nice", "files", "rank", "raw"]
 elif launch_cmd=="Bqtools":
     valid_dbi_param +=["micro", "long","nb_proc","duree"]
 elif launch_cmd=="Local":



From nouiz at mail.berlios.de  Wed Apr 23 21:17:58 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 23 Apr 2008 21:17:58 +0200
Subject: [Plearn-commits] r8867 - trunk/scripts
Message-ID: <200804231917.m3NJHwqN000173@sheep.berlios.de>

Author: nouiz
Date: 2008-04-23 21:17:58 +0200 (Wed, 23 Apr 2008)
New Revision: 8867

Modified:
   trunk/scripts/dbidispatch
Log:
bugfix


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-04-23 19:14:50 UTC (rev 8866)
+++ trunk/scripts/dbidispatch	2008-04-23 19:17:58 UTC (rev 8867)
@@ -179,13 +179,11 @@
             dbi_param[param]=argv.split('=',1)[1]
     elif argv.startswith('--machine=') or argv.startswith('--machines='):
         if argv.split('=')[0] == "--machine":
-            new='Machine=="'+argv.split('=')[1]+'"'
+            new='&&(Machine=="'+argv.split('=')[1]+'")'
         elif argv.split('=')[0] == "--machines":
-            new=dbi_param["req"]='regexp("'+argv.split('=')[1]+'", target.Machine)'
-        s=dbi_param.get('req','')
-        if s:
-            s+=' && '
-        dbi_param["req"]=s+new
+            new='&&(regexp("'+argv.split('=')[1]+'", target.Machine))'
+        dbi_param.setdefault('req','True')
+        dbi_param["req"]+=new
     elif argv=="--server" or argv=="--no_server" \
             or argv=='--prefserver' or argv=="--no_prefserver":
         if argv.find('prefserver')!=-1:



From nouiz at mail.berlios.de  Wed Apr 23 21:19:10 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 23 Apr 2008 21:19:10 +0200
Subject: [Plearn-commits] r8868 - trunk/scripts
Message-ID: <200804231919.m3NJJAOV000220@sheep.berlios.de>

Author: nouiz
Date: 2008-04-23 21:19:10 +0200 (Wed, 23 Apr 2008)
New Revision: 8868

Modified:
   trunk/scripts/dbidispatch
Log:
better display


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-04-23 19:17:58 UTC (rev 8867)
+++ trunk/scripts/dbidispatch	2008-04-23 19:19:10 UTC (rev 8868)
@@ -166,9 +166,8 @@
     elif argv in ["--no_force", "--no_interruptible", "--no_long", "--no_test",
                   "--no_getenv", "--no_cwait", "--no_clean_up" , "--no_nice"]:
         dbi_param[argv[5:]]=False
-    elif argv.split('=')[0] in ["--duree","--cpu","--mem","--os",
-                                "--nb_proc","--req", "--files", "--raw",
-                                "--rank"]:
+    elif argv.split('=')[0] in ["--duree","--cpu","--mem","--os","--nb_proc",
+                                "--req", "--files", "--raw", "--rank"]:
         param=argv.split('=')[0][2:]
         if param in ["req", "files", "raw", "rank"]:
             #param that we happend to if defined more then one time



From nouiz at mail.berlios.de  Wed Apr 23 21:23:45 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 23 Apr 2008 21:23:45 +0200
Subject: [Plearn-commits] r8869 - trunk/scripts
Message-ID: <200804231923.m3NJNjXn000596@sheep.berlios.de>

Author: nouiz
Date: 2008-04-23 21:23:44 +0200 (Wed, 23 Apr 2008)
New Revision: 8869

Modified:
   trunk/scripts/dbidispatch
Log:
added option --testdbi that set onlyt the dbi interface in test mode


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-04-23 19:19:10 UTC (rev 8868)
+++ trunk/scripts/dbidispatch	2008-04-23 19:23:44 UTC (rev 8869)
@@ -3,7 +3,7 @@
 from plearn.utilities.toolkit import search_file
 
 ScriptName="launchdbi.py"
-ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|**--nodbilog] [--condor|--bqtools[=nb_proc]|--cluster[=nb_proc]|--local[=nb_proc]|--ssh[=nb_proc]] [--nb_proc=nb_proc] [--mem=X] [--os=X] [--test|*--no_test] [--long|**--no_long] [--micro[=nb_batch]] [--duree=X] [**--cwait|--no_cwait] [--req="CONDOR_REQUIREMENT"] [--force|**--no_force] [--nice|**--no_nice] [--interruptible|**--no_interruptible] [--cpu=nb_cpu_per_node] [**--getenv|--no_getenv] [--32|--64|--3264] [--[no_]prefserver] [--rank=RANK_EXPRESSION] [--files=file1[,file2...]] [--raw=CONDOR_EXPRESSION] {--file=FILEPATH | <command-template>} \n An * before -- signals the default option value in dbidispatch. An ** before -- signal a default option value in dbi'
+ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|**--nodbilog] [--condor|--bqtools[=nb_proc]|--cluster[=nb_proc]|--local[=nb_proc]|--ssh[=nb_proc]] [--nb_proc=nb_proc] [--mem=X] [--os=X] [--test|*--no_test] [--testdbi] [--long|**--no_long] [--micro[=nb_batch]] [--duree=X] [**--cwait|--no_cwait] [--req="CONDOR_REQUIREMENT"] [--force|**--no_force] [--nice|**--no_nice] [--interruptible|**--no_interruptible] [--cpu=nb_cpu_per_node] [**--getenv|--no_getenv] [--32|--64|--3264] [--[no_]prefserver] [--rank=RANK_EXPRESSION] [--files=file1[,file2...]] [--raw=CONDOR_EXPRESSION] {--file=FILEPATH | <command-template>} \n An * before -- signals the default option value in dbidispatch. An ** before -- signal a default option value in dbi'
 LongHelp="""Dispatches jobs with dbi.py. dbi can dispatch jobs on condor, bqtools, cluster, local and ssh. If no system is selected on the command line, we try them in the previous order. ssh is never automaticaly selected.
 
 %(ShortHelp)s
@@ -13,6 +13,7 @@
   The --condor, --bqtools, --cluster, --local or --ssh option specify on which system the jobs will be sent. If not present, we will use the first available in the previously given order. ssh is never automaticaly selected.
   The --dbilog (--nodbilog) tells dbi to generate (or not) an additional log
   The '--test' option makes dbidispatch generate the file %(ScriptName)s, without executing it. That way you can see what dbidispatch generates. Also, this file calls dbi in test mode, so dbi executes everything in the script except the experiment in %(ScriptName)s (so you can check the script).
+  The '--testdbi' set only dbi in test mode. Not dbidispatch
   The '--no_test' option cancel the '--test' option.
   The --file=FILEPATH specifies a file containing the jobs to execute, one per line. This is instead of specifying only one job on the command line.
 
@@ -118,8 +119,8 @@
     sys.exit(1)
 FILE = ""
 dbi_param={}
+testmode=False
 
-
 PATH=os.getenv('PATH')
 if search_file('condor_submit',PATH):
     launch_cmd = 'Condor'
@@ -163,6 +164,9 @@
     elif argv in  ["--force", "--interruptible", "--long", "--test",
                    "--getenv", "--cwait", "--nice"]:
         dbi_param[argv[2:]]=True
+        testmode=True
+    elif argv=="--testdbi":
+        dbi_param["test"]=True
     elif argv in ["--no_force", "--no_interruptible", "--no_long", "--no_test",
                   "--no_getenv", "--no_cwait", "--no_clean_up" , "--no_nice"]:
         dbi_param[argv[5:]]=False
@@ -299,7 +303,7 @@
 SCRIPT.write("["+time.ctime()+"] "+str(sys.argv)+"\n")
 SCRIPT.close()
 
-if "test" in dbi_param:
+if testmode:
     print "We generated %s command in the file"% len(commands)
     print "The script %s was not launched"% ScriptName
     SCRIPT=open(ScriptName,'w');



From dumitruerhan at mail.berlios.de  Wed Apr 23 21:26:16 2008
From: dumitruerhan at mail.berlios.de (dumitruerhan at BerliOS)
Date: Wed, 23 Apr 2008 21:26:16 +0200
Subject: [Plearn-commits] r8870 -
	trunk/python_modules/plearn/learners/modulelearners
Message-ID: <200804231926.m3NJQGFh000785@sheep.berlios.de>

Author: dumitruerhan
Date: 2008-04-23 21:26:16 +0200 (Wed, 23 Apr 2008)
New Revision: 8870

Modified:
   trunk/python_modules/plearn/learners/modulelearners/__init__.py
Log:
computeOutputsTargets() now returns two VMatrices, instead of a VMatrix and a list


Modified: trunk/python_modules/plearn/learners/modulelearners/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/__init__.py	2008-04-23 19:23:44 UTC (rev 8869)
+++ trunk/python_modules/plearn/learners/modulelearners/__init__.py	2008-04-23 19:26:16 UTC (rev 8870)
@@ -558,9 +558,8 @@
        targetsize = dataSet.targetsize,
        weightsize=0,
        indices = range(dataSet.inputsize, dataSet.inputsize+dataSet.targetsize)
-    ).getMat()
-    return outputs, [int(target) for target in targets]
-
+    )
+    return outputs, targets
     
 if __name__ == '__main__':
 



From nouiz at mail.berlios.de  Wed Apr 23 22:07:52 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 23 Apr 2008 22:07:52 +0200
Subject: [Plearn-commits] r8871 - trunk/plearn_learners/hyper
Message-ID: <200804232007.m3NK7qc0003215@sheep.berlios.de>

Author: nouiz
Date: 2008-04-23 22:07:51 +0200 (Wed, 23 Apr 2008)
New Revision: 8871

Modified:
   trunk/plearn_learners/hyper/HyperCommand.cc
Log:
Added declareOption of HyperCommand::verbosity. This option was existing, but not declared


Modified: trunk/plearn_learners/hyper/HyperCommand.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperCommand.cc	2008-04-23 19:26:16 UTC (rev 8870)
+++ trunk/plearn_learners/hyper/HyperCommand.cc	2008-04-23 20:07:51 UTC (rev 8871)
@@ -73,6 +73,10 @@
 {
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
+
+    declareOption(
+        ol, "verbosity", &HyperCommand::verbosity, OptionBase::buildoption,
+        " The verbosity level. Default to 0.");
 }
 
 void HyperCommand::build_()



From louradou at mail.berlios.de  Wed Apr 23 22:17:41 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Wed, 23 Apr 2008 22:17:41 +0200
Subject: [Plearn-commits] r8872 - trunk/python_modules/plearn/learners
Message-ID: <200804232017.m3NKHfb9003551@sheep.berlios.de>

Author: louradou
Date: 2008-04-23 22:17:40 +0200 (Wed, 23 Apr 2008)
New Revision: 8872

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:
little bugs fixed



Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2008-04-23 20:07:51 UTC (rev 8871)
+++ trunk/python_modules/plearn/learners/SVM.py	2008-04-23 20:17:40 UTC (rev 8872)
@@ -3,6 +3,8 @@
 from libsvm import *
 
 from plearn.pyext import *
+
+
 from numpy.numarray import *
 from math import *
 import random
@@ -729,7 +731,8 @@
         self.test_stats      = None
         self.train_stats     = None
         self.validtype       = 'simple'
-
+        self.multiclass_strategy == 'onevsone'
+        
         self.n_fold   = 5
         self.balanceC = False
         self.balance_classes = False
@@ -772,7 +775,6 @@
         self.retrain_until_local_optimum_is_found = True
         self.max_ntrials = 50
         self.test_on_train = True
-
         
         self.verbosity = 0
         
@@ -791,6 +793,8 @@
         self.stats_are_uptodate = False
         self.input_means = None
         self.input_stds = None
+        self.model = None
+        self.best_model = None
         # Note: when we forget, we keep the value of
         #       'self.best_param'. This allow to initialize
         #       a new search (when data changed a bit) to
@@ -1352,7 +1356,7 @@
     def valid( self,
                dataspec,
                param= None):
-        if self.validtype[:6]=='simple' and self.validset_key in dataspec:
+        if self.validset_key in dataspec:
             return self.simplevalid(dataspec, param)
         else:
             return self.crossvalid(dataspec, param)



From louradou at mail.berlios.de  Wed Apr 23 22:32:36 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Wed, 23 Apr 2008 22:32:36 +0200
Subject: [Plearn-commits] r8873 - trunk/python_modules/plearn/learners
Message-ID: <200804232032.m3NKWaqT004465@sheep.berlios.de>

Author: louradou
Date: 2008-04-23 22:32:36 +0200 (Wed, 23 Apr 2008)
New Revision: 8873

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:


Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2008-04-23 20:17:40 UTC (rev 8872)
+++ trunk/python_modules/plearn/learners/SVM.py	2008-04-23 20:32:36 UTC (rev 8873)
@@ -731,7 +731,7 @@
         self.test_stats      = None
         self.train_stats     = None
         self.validtype       = 'simple'
-        self.multiclass_strategy == 'onevsone'
+        self.multiclass_strategy = 'onevsone'
         
         self.n_fold   = 5
         self.balanceC = False



From chrish at mail.berlios.de  Wed Apr 23 23:28:02 2008
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Wed, 23 Apr 2008 23:28:02 +0200
Subject: [Plearn-commits] r8874 - in trunk/python_modules/plearn: plide
	utilities
Message-ID: <200804232128.m3NLS2Dc008062@sheep.berlios.de>

Author: chrish
Date: 2008-04-23 23:28:02 +0200 (Wed, 23 Apr 2008)
New Revision: 8874

Modified:
   trunk/python_modules/plearn/plide/plide_options.py
   trunk/python_modules/plearn/utilities/options_dialog.py
Log:
Create a simpleOptionsDialog that shows only the plnamespaces given to it as arguments, without the tabs for expdir, etc.

Modified: trunk/python_modules/plearn/plide/plide_options.py
===================================================================
--- trunk/python_modules/plearn/plide/plide_options.py	2008-04-23 20:32:36 UTC (rev 8873)
+++ trunk/python_modules/plearn/plide/plide_options.py	2008-04-23 21:28:02 UTC (rev 8874)
@@ -143,25 +143,32 @@
 
 class PyPLearnOptionsDialog( GladeDialog ):
     """Class which is inialized with a PyPLearnScript code (string) and
-    creates a dialog box to query the script options.
+    creates a dialog box to query the script options. If the argument
+    include_standard_script_options is set to false, the tabs for the expdir,
+    logging and manual overrides will not be created.
     """
-    def __init__( self, options_holder ):
+    def __init__( self, options_holder, include_standard_script_options=True ):
         GladeDialog.__init__(self, gladeFile())
         self.options_holder = options_holder
-        self.set_title('Script Options ['+options_holder.script_name+']')
-        
-        ## Fill out the first page of notebook
-        self.w_launch_directory.set_text(options_holder.launch_directory)
-        for v in options_holder.log_verbosities:
-            self.w_plearn_log_verbosity.append_text(v)
-        self.w_plearn_log_verbosity.set_active(options_holder.log_verbosity)
-        self.w_named_logs_activate.set_text(' '.join(options_holder.log_enable))
+        self.set_title('Script Options [' + options_holder.script_name + ']')
+        self.include_standard_script_options = include_standard_script_options
 
-        ## Fill out the manual overrides page
-        buf = gtk.TextBuffer()
-        buf.set_text('\n'.join(options_holder.option_overrides))
-        self.w_manual_script_options.set_buffer(buf)
+        if self.include_standard_script_options:
+            ## Fill out the first page of notebook
+            self.w_launch_directory.set_text(options_holder.launch_directory)
+            for v in options_holder.log_verbosities:
+                self.w_plearn_log_verbosity.append_text(v)
+            self.w_plearn_log_verbosity.set_active(options_holder.log_verbosity)
+            self.w_named_logs_activate.set_text(' '.join(options_holder.log_enable))
 
+            ## Fill out the manual overrides page
+            buf = gtk.TextBuffer()
+            buf.set_text('\n'.join(options_holder.option_overrides))
+            self.w_manual_script_options.set_buffer(buf)
+        else:
+            self.w_option_groups.remove_page(0)
+            self.w_option_groups.remove_page(0)
+
         ## This holds a map from a widget-value-getting function to the
         ## associated group/field of the OptionsHolder.  Used to update the
         ## OptionsHolder when the user clicks OK on the dialog box.
@@ -366,15 +373,16 @@
             if value!='None' and opt.cast(value) != dftval:
                 group.set(option_name, value)
 
-        ## Updates from the first page of the dialog
-        self.options_holder.launch_directory = self.w_launch_directory.get_text()
-        self.options_holder.log_verbosity    = self.w_plearn_log_verbosity.get_active()
-        self.options_holder.log_enable       = self.w_named_logs_activate.get_text().split()
+        if self.include_standard_script_options:
+            ## Updates from the first page of the dialog
+            self.options_holder.launch_directory = self.w_launch_directory.get_text()
+            self.options_holder.log_verbosity    = self.w_plearn_log_verbosity.get_active()
+            self.options_holder.log_enable       = self.w_named_logs_activate.get_text().split()
 
-        ## Updates from manual overrides
-        buf  = self.w_manual_script_options.get_buffer()
-        text = buf.get_text(buf.get_start_iter(), buf.get_end_iter())
-        self.options_holder.option_overrides = text.split()
+            ## Updates from manual overrides
+            buf  = self.w_manual_script_options.get_buffer()
+            text = buf.get_text(buf.get_start_iter(), buf.get_end_iter())
+            self.options_holder.option_overrides = text.split()
 
     def on_pick_directory_clicked( self, button ):
         chooser = gtk.FileChooserDialog(

Modified: trunk/python_modules/plearn/utilities/options_dialog.py
===================================================================
--- trunk/python_modules/plearn/utilities/options_dialog.py	2008-04-23 20:32:36 UTC (rev 8873)
+++ trunk/python_modules/plearn/utilities/options_dialog.py	2008-04-23 21:28:02 UTC (rev 8874)
@@ -114,3 +114,26 @@
     return result == gtk.RESPONSE_OK, \
            options_holder.verbosity_map.get(options_holder.log_verbosity, 5), \
            options_holder.log_enable
+
+def simpleOptionsDialog(plnamespaces, name=''):
+    """Pops a dialog showing only the given plnamespaces. No extra
+    tabs (expdir, verbosity, manual overrides) added.
+
+    @param plnamespaces   A list of strings naming the plnamespace subclasses.
+    @return True if the user clicked ok, false otherwise."""
+    
+    from plearn.plide.plide_options import PyPLearnOptionsDialog, PyPLearnOptionsHolder
+    import gtk
+    
+    PyPLearnOptionsDialog.define_injected(None, gladeFile)
+    options_holder = PyPLearnOptionsHolder(name, None, '', plnamespaces)
+    
+    options_dialog = PyPLearnOptionsDialog(options_holder,
+                                           include_standard_script_options=False)
+    result = options_dialog.run()
+    if result == gtk.RESPONSE_OK:
+        options_dialog.update_options_holder()
+    options_dialog.destroy()
+    del options_dialog
+
+    return result == gtk.RESPONSE_OK



From chrish at mail.berlios.de  Thu Apr 24 00:18:35 2008
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Thu, 24 Apr 2008 00:18:35 +0200
Subject: [Plearn-commits] r8875 - in trunk/python_modules/plearn: plide
	utilities
Message-ID: <200804232218.m3NMIZQX013012@sheep.berlios.de>

Author: chrish
Date: 2008-04-24 00:18:34 +0200 (Thu, 24 Apr 2008)
New Revision: 8875

Modified:
   trunk/python_modules/plearn/plide/plide_options.py
   trunk/python_modules/plearn/utilities/options_dialog.py
Log:
* Add optional argument in plide's PyPLearnOptionsDialog to pop a dialog
  box without the first two tabs (expdir, logging, manual overrides).
* Create new simpleOptionsDialog function that pops a standalone dialog box 
  with tabs for only the plnamespaces passed as arguments.
* Fix longstanding bug im options_dialog where the dialog wouldn't go away
  when the over clicked Ok/Cancel.


Modified: trunk/python_modules/plearn/plide/plide_options.py
===================================================================
--- trunk/python_modules/plearn/plide/plide_options.py	2008-04-23 21:28:02 UTC (rev 8874)
+++ trunk/python_modules/plearn/plide/plide_options.py	2008-04-23 22:18:34 UTC (rev 8875)
@@ -32,8 +32,6 @@
 import gtk
 
 from plearn.utilities.metaprog import public_members
-#from plearn.pyplearn.pyplearn  import *
-#from plearn.pyplearn           import *
 from plearn.pyplearn           import plargs
 from plearn.utilities.toolkit  import doc as toolkit_doc
 
@@ -166,6 +164,10 @@
             buf.set_text('\n'.join(options_holder.option_overrides))
             self.w_manual_script_options.set_buffer(buf)
         else:
+            # The notebook tabs containing the options for expdir, logging
+            # and manual overrides (i.e. not the user-defined plnamespaces)
+            # correspond to the first two notebook tabs (see the .glade file).
+            # Remove those first two tabs.
             self.w_option_groups.remove_page(0)
             self.w_option_groups.remove_page(0)
 

Modified: trunk/python_modules/plearn/utilities/options_dialog.py
===================================================================
--- trunk/python_modules/plearn/utilities/options_dialog.py	2008-04-23 21:28:02 UTC (rev 8874)
+++ trunk/python_modules/plearn/utilities/options_dialog.py	2008-04-23 22:18:34 UTC (rev 8875)
@@ -108,14 +108,19 @@
     options_dialog= plide_options.PyPLearnOptionsDialog(options_holder)
     result= options_dialog.run()
     if result == gtk.RESPONSE_OK:
-        options_dialog.update_options_holder()
-    options_dialog.destroy()
-    #plargs.expdir= options_holder.launch_directory
+       options_dialog.update_options_holder()
+
+    # Because we are not running under the Gtk main loop, we need to call
+    # the Gtk main loop manually a couple of times to allow it to clean up
+    # and remove the window. 
+    for i in range(5):
+        gtk.main_iteration()
+    
     return result == gtk.RESPONSE_OK, \
            options_holder.verbosity_map.get(options_holder.log_verbosity, 5), \
            options_holder.log_enable
 
-def simpleOptionsDialog(plnamespaces, name=''):
+def simpleOptionsDialog(plnamespaces):
     """Pops a dialog showing only the given plnamespaces. No extra
     tabs (expdir, verbosity, manual overrides) added.
 
@@ -126,14 +131,18 @@
     import gtk
     
     PyPLearnOptionsDialog.define_injected(None, gladeFile)
-    options_holder = PyPLearnOptionsHolder(name, None, '', plnamespaces)
+    options_holder = PyPLearnOptionsHolder('', None, '', plnamespaces)
     
     options_dialog = PyPLearnOptionsDialog(options_holder,
                                            include_standard_script_options=False)
     result = options_dialog.run()
     if result == gtk.RESPONSE_OK:
         options_dialog.update_options_holder()
-    options_dialog.destroy()
-    del options_dialog
 
+    # Because we are not running under the Gtk main loop, we need to call
+    # the Gtk main loop manually a couple of times to allow it to clean up
+    # and remove the window. 
+    for i in range(5):
+        gtk.main_iteration()
+
     return result == gtk.RESPONSE_OK



From tihocan at mail.berlios.de  Thu Apr 24 16:13:59 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 24 Apr 2008 16:13:59 +0200
Subject: [Plearn-commits] r8876 - trunk/plearn/vmat
Message-ID: <200804241413.m3OEDwGQ016580@sheep.berlios.de>

Author: tihocan
Date: 2008-04-24 16:13:58 +0200 (Thu, 24 Apr 2008)
New Revision: 8876

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
Does not try to mess with stat files when there is no metadatadir

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-04-23 22:18:34 UTC (rev 8875)
+++ trunk/plearn/vmat/VMatrix.cc	2008-04-24 14:13:58 UTC (rev 8876)
@@ -1530,9 +1530,13 @@
 {
     TVec<StatsCollector> stats;
     PPath metadatadir = getMetaDataDir();
-    PPath statsfile =  metadatadir / filename;
-    if(!metadatadir.isEmpty()) lockMetaDataDir(); // don't try to lock nothing
-    bool uptodate= isUpToDate(statsfile, true, true);
+    PPath statsfile;
+    bool uptodate = false;
+    if (!metadatadir.isEmpty()) {
+        lockMetaDataDir();
+        statsfile =  metadatadir / filename;
+        uptodate = isUpToDate(statsfile, true, true);
+    }
     if (uptodate)
         PLearn::load(statsfile, stats);
     else
@@ -1542,7 +1546,8 @@
         if(!metadatadir.isEmpty())
             PLearn::save(statsfile, stats);
     }
-    if(!metadatadir.isEmpty()) unlockMetaDataDir();// don't try to unlock nothing
+    if (!metadatadir.isEmpty())
+        unlockMetaDataDir();
     return stats;
 }
 



From tihocan at mail.berlios.de  Thu Apr 24 16:16:19 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 24 Apr 2008 16:16:19 +0200
Subject: [Plearn-commits] r8877 - trunk/plearn_learners/generic
Message-ID: <200804241416.m3OEGJeK017222@sheep.berlios.de>

Author: tihocan
Date: 2008-04-24 16:16:19 +0200 (Thu, 24 Apr 2008)
New Revision: 8877

Modified:
   trunk/plearn_learners/generic/PLearner.cc
Log:
Avoiding crashes when the learner's outputsize is undefined (-1)

Modified: trunk/plearn_learners/generic/PLearner.cc
===================================================================
--- trunk/plearn_learners/generic/PLearner.cc	2008-04-24 14:13:58 UTC (rev 8876)
+++ trunk/plearn_learners/generic/PLearner.cc	2008-04-24 14:16:19 UTC (rev 8877)
@@ -910,8 +910,9 @@
     Vec input;
     Vec target;
     real weight;
+    int out_size = outputsize() >= 0 ? outputsize() : 0;
 
-    Vec output(outputsize());
+    Vec output(out_size);
     Vec costs(nTestCosts());
 
     if (test_stats) {
@@ -1076,7 +1077,7 @@
         {
             int n_batches = len/test_minibatch_size, i=0;
             b_inputs.resize(test_minibatch_size,inputsize());
-            b_outputs.resize(test_minibatch_size,outputsize());
+            b_outputs.resize(test_minibatch_size, out_size);
             b_costs.resize(test_minibatch_size,costs.length());
             b_targets.resize(test_minibatch_size,targetsize());
             b_weights.resize(test_minibatch_size);
@@ -1095,7 +1096,7 @@
             if (i<len)
             {
                 b_inputs.resize(len-i,inputsize());
-                b_outputs.resize(len-i,outputsize());
+                b_outputs.resize(len-i, out_size);
                 b_costs.resize(len-i,costs.length());
                 b_targets.resize(len-i,targetsize());
                 b_weights.resize(len-i);
@@ -1185,6 +1186,9 @@
     VMat testoutputs= 0;
     VMat testcosts= 0;
     int outsize= outputsize();
+    if (outsize < 0)
+        // Negative outputsize: the output will be empty to avoid a crash.
+        outsize = 0;
     int costsize= nTestCosts();
     int len= testset.length();
     if(rtestoutputs) testoutputs= new MemoryVMatrix(len, outsize);



From tihocan at mail.berlios.de  Thu Apr 24 16:17:31 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 24 Apr 2008 16:17:31 +0200
Subject: [Plearn-commits] r8878 - trunk/plearn_learners/online
Message-ID: <200804241417.m3OEHVxU017339@sheep.berlios.de>

Author: tihocan
Date: 2008-04-24 16:17:31 +0200 (Thu, 24 Apr 2008)
New Revision: 8878

Modified:
   trunk/plearn_learners/online/ModuleLearner.cc
Log:
Does not crash anymore when there are no outputs or costs computed

Modified: trunk/plearn_learners/online/ModuleLearner.cc
===================================================================
--- trunk/plearn_learners/online/ModuleLearner.cc	2008-04-24 14:16:19 UTC (rev 8877)
+++ trunk/plearn_learners/online/ModuleLearner.cc	2008-04-24 14:17:31 UTC (rev 8878)
@@ -483,19 +483,29 @@
         }
         store_weights->setData(all_ones);
     }
-    PLASSERT( store_outputs );
-    // make the store_output temporarily point to output
-    Mat& net_out = store_outputs->getData();
-    Mat old_net_out = net_out;
-    output.resize(input.length(),outputsize());
-    net_out = output;
+    // Make the store_output temporarily point to output
+    Mat old_net_out;
+    Mat* net_out = store_outputs ? &store_outputs->getData()
+                                 : NULL;
+    output.resize(input.length(),outputsize() >= 0 ? outputsize() : 0);
+    if (net_out) {
+        old_net_out = *net_out;
+        *net_out = output;
+    }
 
     // Forward propagation.
     network->fprop(null_pointers);
 
-    // Restore output_store.
-    net_out = old_net_out;
+    // Restore store_outputs.
+    if (net_out)
+        *net_out = old_net_out;
 
+    if (!store_costs) {
+        // Do not bother with costs.
+        costs.resize(input.length(), 0);
+        return;
+    }
+
     // Copy costs.
     // Note that a more efficient implementation may be done when only one cost
     // is computed (see code in previous version).



From tihocan at mail.berlios.de  Thu Apr 24 16:21:05 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 24 Apr 2008 16:21:05 +0200
Subject: [Plearn-commits] r8879 - trunk/plearn/var
Message-ID: <200804241421.m3OEL5bT017567@sheep.berlios.de>

Author: tihocan
Date: 2008-04-24 16:21:04 +0200 (Thu, 24 Apr 2008)
New Revision: 8879

Modified:
   trunk/plearn/var/TimesColumnVariable.cc
   trunk/plearn/var/TimesColumnVariable.h
   trunk/plearn/var/TimesRowVariable.cc
   trunk/plearn/var/TimesRowVariable.h
Log:
Better help and more proper Object building mechanism

Modified: trunk/plearn/var/TimesColumnVariable.cc
===================================================================
--- trunk/plearn/var/TimesColumnVariable.cc	2008-04-24 14:17:31 UTC (rev 8878)
+++ trunk/plearn/var/TimesColumnVariable.cc	2008-04-24 14:21:04 UTC (rev 8879)
@@ -47,38 +47,50 @@
 namespace PLearn {
 using namespace std;
 
-
 /** TimesColumnVariable **/
 
-PLEARN_IMPLEMENT_OBJECT(TimesColumnVariable,
-                        "Multiplies each column of a matrix var elementwise with a single column variable",
-                        "NO HELP");
+PLEARN_IMPLEMENT_OBJECT(
+        TimesColumnVariable,
+       "Multiplies each column of a matrix elementwise with a column variable",
+       "More formally: result(i,j) = input1(i,j) * input2[i]."
+);
 
-TimesColumnVariable::TimesColumnVariable(Variable* input1, Variable* input2)
-    : inherited(input1, input2, input1->length(), input1->width())
+/////////////////////////
+// TimesColumnVariable //
+/////////////////////////
+TimesColumnVariable::TimesColumnVariable(Variable* input1, Variable* input2,
+                                         bool call_build_):
+    inherited(input1, input2, input1->length(), input1->width(), call_build_)
 {
-    build_();
+    if (call_build_)
+        build_();
 }
 
-void
-TimesColumnVariable::build()
+///////////
+// build //
+///////////
+void TimesColumnVariable::build()
 {
     inherited::build();
     build_();
 }
 
-void
-TimesColumnVariable::build_()
+////////////
+// build_ //
+////////////
+void TimesColumnVariable::build_()
 {
     if (input1 && input2) {
-        if(!input2->isColumnVec())
-            PLERROR("IN TimesColumnVariable: input2 is not a column");
-        if(input2->length() != input1->length())
-            PLERROR("IN TimesColumnVariable: input1 and input2 have a different length()");
+        PLCHECK_MSG(input2->isColumnVec(),
+                    "input2 must be a column variable");
+        PLCHECK_MSG(input2->length() == input1->length(),
+                    "input1 and input2 must have same length");
     }
 }
 
-
+///////////////////
+// recomputeSize //
+///////////////////
 void TimesColumnVariable::recomputeSize(int& l, int& w) const
 {
     if (input1) {
@@ -88,6 +100,9 @@
         l = w = 0;
 }
 
+///////////
+// fprop //
+///////////
 void TimesColumnVariable::fprop()
 {
     int k=0;
@@ -96,7 +111,9 @@
             valuedata[k] = input1->valuedata[k] * input2->valuedata[i];
 }
 
-
+///////////
+// bprop //
+///////////
 void TimesColumnVariable::bprop()
 {
     int k=0;
@@ -108,15 +125,18 @@
         }
 }
 
-
+///////////////////
+// symbolicBprop //
+///////////////////
 void TimesColumnVariable::symbolicBprop()
 {
     input1->accg(g*input2);
     input2->accg(rowSum(g*input1));
 }
 
-
-//???
+////////////
+// rfprop //
+////////////
 void TimesColumnVariable::rfprop()
 {
     if (rValue.length()==0) resizeRValue();

Modified: trunk/plearn/var/TimesColumnVariable.h
===================================================================
--- trunk/plearn/var/TimesColumnVariable.h	2008-04-24 14:17:31 UTC (rev 8878)
+++ trunk/plearn/var/TimesColumnVariable.h	2008-04-24 14:21:04 UTC (rev 8879)
@@ -49,16 +49,19 @@
 using namespace std;
 
 
-//!  multiplies each column of a matrix var elementwise with a single column variable
 class TimesColumnVariable: public BinaryVariable
 {
     typedef BinaryVariable inherited;
 
 public:
-    //!  Default constructor for persistence
+
+    //! Default constructor.
     TimesColumnVariable() {}
-    TimesColumnVariable(Variable* input1, Variable* input2);
 
+    //! Convenience constructor.
+    TimesColumnVariable(Variable* input1, Variable* input2,
+                        bool call_build_ = true);
+
     PLEARN_DECLARE_OBJECT(TimesColumnVariable);
 
     virtual void build();
@@ -69,7 +72,8 @@
     virtual void symbolicBprop();
     virtual void rfprop();
 
-protected:
+private:
+
     void build_();
 };
 

Modified: trunk/plearn/var/TimesRowVariable.cc
===================================================================
--- trunk/plearn/var/TimesRowVariable.cc	2008-04-24 14:17:31 UTC (rev 8878)
+++ trunk/plearn/var/TimesRowVariable.cc	2008-04-24 14:21:04 UTC (rev 8879)
@@ -50,34 +50,48 @@
 
 /** TimesRowVariable **/
 
-PLEARN_IMPLEMENT_OBJECT(TimesRowVariable,
-                        "Multiplies each row of a matrix var elementwise with a single row variable",
-                        "NO HELP");
+PLEARN_IMPLEMENT_OBJECT(
+        TimesRowVariable,
+       "Multiplies each row of a matrix elementwise with a row variable.",
+       "More formally: result(i,j) = input1(i,j) * input2[j]."
+);
 
-TimesRowVariable::TimesRowVariable(Variable* input1, Variable* input2)
-    : inherited(input1, input2, input1->length(), input1->width())
+//////////////////////
+// TimesRowVariable //
+//////////////////////
+TimesRowVariable::TimesRowVariable(Variable* input1, Variable* input2,
+                                   bool call_build_):
+    inherited(input1, input2, input1->length(), input1->width(), call_build_)
 {
-    build_();
+    if (call_build_)
+        build_();
 }
 
-void
-TimesRowVariable::build()
+///////////
+// build //
+///////////
+void TimesRowVariable::build()
 {
     inherited::build();
     build_();
 }
 
-void
-TimesRowVariable::build_()
+////////////
+// build_ //
+////////////
+void TimesRowVariable::build_()
 {
     if (input1 && input2) {
-        if (!input2->isRowVec())
-            PLERROR("IN TimesRowVariable: input2 is not a row");
-        if (input2->width() != input1->width())
-            PLERROR("IN TimesRowVariable: input1 and input2 have a different width()");
+        PLCHECK_MSG(input2->isRowVec(),
+                    "input1 must be a row vector");
+        PLCHECK_MSG(input2->width() == input1->width(),
+                    "input1 and input2 must have same width");
     }
 }
 
+///////////////////
+// recomputeSize //
+///////////////////
 void TimesRowVariable::recomputeSize(int& l, int& w) const
 {
     if (input1) {
@@ -87,6 +101,9 @@
         l = w = 0;
 }
 
+///////////
+// fprop //
+///////////
 void TimesRowVariable::fprop()
 {
     int k=0;
@@ -95,7 +112,9 @@
             valuedata[k] = input1->valuedata[k] * input2->valuedata[j];
 }
 
-
+///////////
+// bprop //
+///////////
 void TimesRowVariable::bprop()
 {
     int k=0;
@@ -107,14 +126,18 @@
         }
 }
 
-
+///////////////////
+// symbolicBprop //
+///////////////////
 void TimesRowVariable::symbolicBprop()
 {
     input1->accg(g*input2);
     input2->accg(columnSum(g*input1));
 }
 
-
+////////////
+// rfprop //
+////////////
 void TimesRowVariable::rfprop()
 {
     if (rValue.length()==0) resizeRValue();
@@ -125,7 +148,6 @@
 }
 
 
-
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/var/TimesRowVariable.h
===================================================================
--- trunk/plearn/var/TimesRowVariable.h	2008-04-24 14:17:31 UTC (rev 8878)
+++ trunk/plearn/var/TimesRowVariable.h	2008-04-24 14:21:04 UTC (rev 8879)
@@ -49,16 +49,19 @@
 using namespace std;
 
 
-//!  multiplies each row of a matrix var elementwise with a single row variable
 class TimesRowVariable: public BinaryVariable
 {
     typedef BinaryVariable inherited;
 
 public:
-    //!  Default constructor for persistence
+
+    //! Default constructor.
     TimesRowVariable() {}
-    TimesRowVariable(Variable* input1, Variable* input2);
 
+    //! Convenience constructor.
+    TimesRowVariable(Variable* input1, Variable* input2,
+                     bool call_build_ = true);
+
     PLEARN_DECLARE_OBJECT(TimesRowVariable);
 
     virtual void build();
@@ -69,7 +72,8 @@
     virtual void symbolicBprop();
     virtual void rfprop();
 
-protected:
+private:
+
     void build_();
 };
 



From tihocan at mail.berlios.de  Thu Apr 24 18:36:28 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 24 Apr 2008 18:36:28 +0200
Subject: [Plearn-commits] r8880 - trunk/python_modules/plearn/pymake
Message-ID: <200804241636.m3OGaSsW007233@sheep.berlios.de>

Author: tihocan
Date: 2008-04-24 18:36:27 +0200 (Thu, 24 Apr 2008)
New Revision: 8880

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
- Disabled automatic recompilation of dynamic libraries compiled with -pyso, until someone explains me why it would be necessary
- Removed duplicated computation of dependencies


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-04-24 14:21:04 UTC (rev 8879)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-04-24 16:36:27 UTC (rev 8880)
@@ -920,7 +920,7 @@
         else:
             info = file_info(cctarget)
             if info.hasmain or create_dll or create_so or create_pyso:
-                if not force_link and not force_recompilation and info.corresponding_output_is_up_to_date() and not create_dll and not create_so and not create_pyso:
+                if not force_link and not force_recompilation and info.corresponding_output_is_up_to_date() and not create_dll and not create_so:
                     info.make_symbolic_link(linkname, None, info.corresponding_output) # remake the correct symbolic link
                     print 'Target',info.filebase,'is up to date.'
                 else:
@@ -2809,7 +2809,6 @@
         print '++++ Computing dependencies of '+target+' ...',
         get_ccfiles_to_compile_and_link(target, ccfiles_to_compile, executables_to_link, linkname)
         print ' done'
-        get_ccfiles_to_compile_and_link(target, ccfiles_to_compile, executables_to_link, linkname)
 
         if distribute:
             # We dont want to compile. We will extract the necessary file to compile



From tihocan at mail.berlios.de  Thu Apr 24 18:50:36 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 24 Apr 2008 18:50:36 +0200
Subject: [Plearn-commits] r8881 - trunk/plearn/io
Message-ID: <200804241650.m3OGoagu031745@sheep.berlios.de>

Author: tihocan
Date: 2008-04-24 18:50:34 +0200 (Thu, 24 Apr 2008)
New Revision: 8881

Modified:
   trunk/plearn/io/RPPath.cc
   trunk/plearn/io/RPPath.h
Log:
The absolute() method now returns a RPPath object, so that it can be manipulated remotely as well

Modified: trunk/plearn/io/RPPath.cc
===================================================================
--- trunk/plearn/io/RPPath.cc	2008-04-24 16:36:27 UTC (rev 8880)
+++ trunk/plearn/io/RPPath.cc	2008-04-24 16:50:34 UTC (rev 8881)
@@ -80,7 +80,7 @@
     rmm.inherited(inherited::_getRemoteMethodMap_());
 
     declareMethod(rmm, "absolute", &RPPath::absolute,
-            (BodyDoc("Return the absolute path.")));
+            (BodyDoc("Return the absolute path as a RPPath object.")));
 
     declareMethod(rmm, "canonical", &RPPath::canonical,
             (BodyDoc("Return the canonic path.")));
@@ -104,9 +104,12 @@
 //////////////
 // absolute //
 //////////////
-PPath RPPath::absolute()
+PP<RPPath> RPPath::absolute()
 {
-    return path.absolute();
+    PP<RPPath> abs_path = new RPPath();
+    abs_path->path = path.absolute();
+    abs_path->build();
+    return abs_path;
 }
 
 ///////////////

Modified: trunk/plearn/io/RPPath.h
===================================================================
--- trunk/plearn/io/RPPath.h	2008-04-24 16:36:27 UTC (rev 8880)
+++ trunk/plearn/io/RPPath.h	2008-04-24 16:50:34 UTC (rev 8881)
@@ -57,12 +57,14 @@
 public:
     //#####  Public Member Functions  #########################################
 
-    //! Default constructor
+    //! Default constructor.
     RPPath();
 
-    // The methods below are forwarded to the underlying PPath.
+    //! Return the RPPath object that represents the same path, but in its
+    //! absolute form.
+    PP<RPPath> absolute();
 
-    PPath absolute();
+    //! Forwarded to the underlying PPath.
     string canonical();
 
     //#####  PLearn::Object Protocol  #########################################



From tihocan at mail.berlios.de  Thu Apr 24 19:25:47 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 24 Apr 2008 19:25:47 +0200
Subject: [Plearn-commits] r8882 - trunk/python_modules/plearn/pymake
Message-ID: <200804241725.m3OHPl8q006445@sheep.berlios.de>

Author: tihocan
Date: 2008-04-24 19:25:47 +0200 (Thu, 24 Apr 2008)
New Revision: 8882

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
Does not systematically relink with -so, also fixed link for -so and -pyso

Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-04-24 16:50:34 UTC (rev 8881)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-04-24 17:25:47 UTC (rev 8882)
@@ -920,9 +920,10 @@
         else:
             info = file_info(cctarget)
             if info.hasmain or create_dll or create_so or create_pyso:
-                if not force_link and not force_recompilation and info.corresponding_output_is_up_to_date() and not create_dll and not create_so:
-                    info.make_symbolic_link(linkname, None, info.corresponding_output) # remake the correct symbolic link
-                    print 'Target',info.filebase,'is up to date.'
+                if not force_link and not force_recompilation and info.corresponding_output_is_up_to_date() and not create_dll:
+                    # Refresh symbolic link.
+                    info.make_symbolic_link(linkname, None, info.corresponding_output)
+                    print 'Target', info.filebase, 'is up to date.'
                 else:
                     executables_to_link[info] = 1
                     for ccfile in info.get_ccfiles_to_link():
@@ -1910,6 +1911,10 @@
         # User-provided path for the link itself?
         if linkname == '':
             symlink_from = join(self.filedir, self.filebase)
+            if create_so or create_pyso:
+                symlink_from += '.so'
+                if create_so:
+                    symlink_from = 'lib' + symlink_from
         else:
             symlink_from = linkname
 
@@ -2806,9 +2811,9 @@
 
         print '*** Running pymake on '+os.path.basename(target)+' using configuration file: ' + configpath
         print '*** Running pymake on '+os.path.basename(target)+' using options: ' + string.join(map(lambda o: '-'+o, options))
-        print '++++ Computing dependencies of '+target+' ...',
+        print '++++ Computing dependencies of '+target
         get_ccfiles_to_compile_and_link(target, ccfiles_to_compile, executables_to_link, linkname)
-        print ' done'
+        print '++++ Dependencies computed'
 
         if distribute:
             # We dont want to compile. We will extract the necessary file to compile



From tihocan at mail.berlios.de  Thu Apr 24 19:31:26 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 24 Apr 2008 19:31:26 +0200
Subject: [Plearn-commits] r8883 - trunk/python_modules/plearn/pymake
Message-ID: <200804241731.m3OHVQCp021266@sheep.berlios.de>

Author: tihocan
Date: 2008-04-24 19:31:26 +0200 (Thu, 24 Apr 2008)
New Revision: 8883

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
Ooops, fixed link for -so!

Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-04-24 17:25:47 UTC (rev 8882)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-04-24 17:31:26 UTC (rev 8883)
@@ -1910,11 +1910,12 @@
 
         # User-provided path for the link itself?
         if linkname == '':
-            symlink_from = join(self.filedir, self.filebase)
-            if create_so or create_pyso:
-                symlink_from += '.so'
-                if create_so:
-                    symlink_from = 'lib' + symlink_from
+            linkbase = self.filebase
+            if create_so:
+                linkbase = 'lib%s.so' % linkbase
+            elif create_pyso:
+                linkbase = '%s.so' % linkbase
+            symlink_from = join(self.filedir, linkbase)
         else:
             symlink_from = linkname
 



From nouiz at mail.berlios.de  Thu Apr 24 20:05:39 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 24 Apr 2008 20:05:39 +0200
Subject: [Plearn-commits] r8884 - trunk/python_modules/plearn/pymake
Message-ID: <200804241805.m3OI5dbn032014@sheep.berlios.de>

Author: nouiz
Date: 2008-04-24 20:05:38 +0200 (Thu, 24 Apr 2008)
New Revision: 8884

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
print the total number of file to compile if we don't do an incremental build


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-04-24 17:31:26 UTC (rev 8883)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-04-24 18:05:38 UTC (rev 8884)
@@ -2825,7 +2825,20 @@
             generate_vcproj_files(target, ccfiles_to_compile, executables_to_link, linkname)
 
         else:
-            print '++++ Compiling',len(ccfiles_to_compile),'files...'
+            l=reduce(lambda x,y:x+y.get_ccfiles_to_link(),
+                     executables_to_link,[])
+            if verbose >=4:
+                print "Link files:"
+                for i in l:
+                    print i.filebase
+                print 
+                print
+                print "Files to compile: "
+                for i in ccfiles_to_compile:
+                    print i.filebase
+            print '++++ Compiling',
+            print str(len(ccfiles_to_compile))+'/'+str(len(l)),'files...'
+
             if platform=='win32':
                 win32_parallel_compile(ccfiles_to_compile.keys())
             else:



From nouiz at mail.berlios.de  Thu Apr 24 20:42:15 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 24 Apr 2008 20:42:15 +0200
Subject: [Plearn-commits] r8885 - trunk/python_modules/plearn/pymake
Message-ID: <200804241842.m3OIgF6G001789@sheep.berlios.de>

Author: nouiz
Date: 2008-04-24 20:42:14 +0200 (Thu, 24 Apr 2008)
New Revision: 8885

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
correctly handle the -m32 option if the platform append something to linux-x86_64, linux-i386 or linux-ia64.


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-04-24 18:05:38 UTC (rev 8884)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-04-24 18:42:14 UTC (rev 8885)
@@ -2568,9 +2568,9 @@
 
     if 'm32' in optionargs:
         force_32bits = 1
-        if platform=='linux-x86_64' or platform=='linux-ia64':
+        if platform.startswith('linux-x86_64') or platform.startswith('linux-ia64'):
             target_platform = 'linux-i386'
-        elif platform=='linux-i386':
+        elif platform.startswith('linux-i386'):
             pass
         else:
             print 'Warning: you probably should not be using the "-m32" option'



From nouiz at mail.berlios.de  Thu Apr 24 20:53:05 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 24 Apr 2008 20:53:05 +0200
Subject: [Plearn-commits] r8886 - trunk/plearn_learners/hyper
Message-ID: <200804241853.m3OIr5YD002659@sheep.berlios.de>

Author: nouiz
Date: 2008-04-24 20:53:04 +0200 (Thu, 24 Apr 2008)
New Revision: 8886

Modified:
   trunk/plearn_learners/hyper/EarlyStoppingOracle.cc
Log:
modified to allow loading a served version of EarlyStoppingOracle


Modified: trunk/plearn_learners/hyper/EarlyStoppingOracle.cc
===================================================================
--- trunk/plearn_learners/hyper/EarlyStoppingOracle.cc	2008-04-24 18:42:14 UTC (rev 8885)
+++ trunk/plearn_learners/hyper/EarlyStoppingOracle.cc	2008-04-24 18:53:04 UTC (rev 8886)
@@ -47,21 +47,21 @@
 using namespace std;
 
 EarlyStoppingOracle::EarlyStoppingOracle()
-    : min_value(-FLT_MAX),
-      max_value(FLT_MAX),
-      max_degradation(FLT_MAX),
+    : nreturned(0),
+      previous_objective(REAL_MAX),
+      best_objective(REAL_MAX),
+      best_step(-1),
+      met_early_stopping(false),
+      min_value(-REAL_MAX),
+      max_value(REAL_MAX),
+      max_degradation(REAL_MAX),
       relative_max_degradation(-1),
-      min_improvement(-FLT_MAX),
+      min_improvement(-REAL_MAX),
       relative_min_improvement(-1),
       max_degraded_steps(100),
       min_n_steps(0)
 {
     //    build_();
-    nreturned = 0;
-    previous_objective = FLT_MAX;
-    best_objective = FLT_MAX;
-    best_step = -1;
-    met_early_stopping = false;
 }
 
 PLEARN_IMPLEMENT_OBJECT(EarlyStoppingOracle, "ONE LINE DESCR", "NO HELP");
@@ -104,6 +104,23 @@
     declareOption(ol, "min_n_steps", &EarlyStoppingOracle::min_n_steps, OptionBase::buildoption,
                   "minimum required number of steps before allowing early stopping\n");
 
+    //learnt option
+    declareOption(ol, "nreturned", &EarlyStoppingOracle::nreturned,
+                  OptionBase::learntoption,
+                  "The number of returned option\n");
+
+    declareOption(ol, "best_objective", &EarlyStoppingOracle::best_objective,
+                  OptionBase::learntoption,
+                  "The best objective see up to date\n");
+
+    declareOption(ol, "best_step", &EarlyStoppingOracle::best_step,
+                  OptionBase::learntoption,
+                  "The step where we have see the best objective\n");
+
+    declareOption(ol, "met_early_stopping", &EarlyStoppingOracle::met_early_stopping,
+                  OptionBase::learntoption,
+                  "True if we met the early stopping criterion\n");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }



From nouiz at mail.berlios.de  Thu Apr 24 21:04:14 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 24 Apr 2008 21:04:14 +0200
Subject: [Plearn-commits] r8887 - trunk/plearn_learners/hyper
Message-ID: <200804241904.m3OJ4EGo003674@sheep.berlios.de>

Author: nouiz
Date: 2008-04-24 21:04:13 +0200 (Thu, 24 Apr 2008)
New Revision: 8887

Modified:
   trunk/plearn_learners/hyper/HyperOptimize.cc
   trunk/plearn_learners/hyper/HyperOptimize.h
Log:
modified to allow reloding save HyperOptimize


Modified: trunk/plearn_learners/hyper/HyperOptimize.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperOptimize.cc	2008-04-24 18:53:04 UTC (rev 8886)
+++ trunk/plearn_learners/hyper/HyperOptimize.cc	2008-04-24 19:04:13 UTC (rev 8887)
@@ -107,7 +107,9 @@
 
 
 HyperOptimize::HyperOptimize()
-    : which_cost_pos(-1),
+    : best_objective(REAL_MAX),
+      trialnum(0),
+      which_cost_pos(-1),
       which_cost(),
       min_n_trials(0),
       provide_tester_expdir(false),
@@ -170,6 +172,24 @@
         "functions through the getOption() mechanism. If an expdir is declared\n"
         "this matrix is available under the name 'results.pmat' in the expdir.");
     
+    declareOption(ol, "best_objective", &HyperOptimize::best_objective,
+                  OptionBase::learntoption,
+                  "The best objective seen up to date.");
+
+    declareOption(ol, "best_results", &HyperOptimize::best_results,
+                  OptionBase::learntoption,
+                  "The best result seen up to date." );
+
+    declareOption(ol, "best_learner", &HyperOptimize::best_learner,
+                  OptionBase::learntoption,
+                  "A copy of the learner to the best learner seen up to date." );
+
+    declareOption(ol, "trialnum", &HyperOptimize::trialnum,
+                  OptionBase::learntoption, "The number of trial done." );
+
+    declareOption(ol, "option_vals", &HyperOptimize::option_vals,
+                  OptionBase::learntoption,"The option value to try." );
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -290,22 +310,20 @@
 
 Vec HyperOptimize::optimize()
 {
-    real best_objective = REAL_MAX;
-    Vec best_results;
-    PP<PLearner> best_learner;
-
     TVec<string> option_names;
     option_names = oracle->getOptionNames();
 
-    TVec<string> option_vals = oracle->generateFirstTrial();
+    if(option_vals.size()==0 && trialnum>0)
+        return best_results;//the optimization if finished
+    else if(option_vals.size()==0)
+        option_vals = oracle->generateFirstTrial();
+//        option_vals = oracle->generateNextTrial(option_vals, MISSING_VALUE);
     if (option_vals.size() != option_names.size())
         PLERROR("HyperOptimize::optimize: the number (%d) of option values (%s) "
                 "does not match the number (%d) of option names (%s) ",
                 option_vals.size(), tostring(option_vals).c_str(),
                 option_names.size(), tostring(option_names).c_str());
 
-    int trialnum = 0;
-
     which_cost_pos= getResultNames().find(which_cost);
     if(which_cost_pos < 0){
         if(!pl_islong(which_cost))

Modified: trunk/plearn_learners/hyper/HyperOptimize.h
===================================================================
--- trunk/plearn_learners/hyper/HyperOptimize.h	2008-04-24 18:53:04 UTC (rev 8886)
+++ trunk/plearn_learners/hyper/HyperOptimize.h	2008-04-24 19:04:13 UTC (rev 8887)
@@ -108,6 +108,11 @@
 protected:
     //! Store the results computed for each trial
     VMat resultsmat;
+    real best_objective;
+    Vec best_results;
+    PP<PLearner> best_learner;
+    int trialnum;
+    TVec<string> option_vals;
 
 public:
 



From louradou at mail.berlios.de  Thu Apr 24 21:06:36 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Thu, 24 Apr 2008 21:06:36 +0200
Subject: [Plearn-commits] r8888 - trunk/python_modules/plearn/learners
Message-ID: <200804241906.m3OJ6aWD003897@sheep.berlios.de>

Author: louradou
Date: 2008-04-24 21:06:36 +0200 (Thu, 24 Apr 2008)
New Revision: 8888

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:
* added an option testlevel to control at which
  frequency we want to test the models in the run()
* little change to compute input stats when balance_classes=True
* when test_on_train is False, no more 'None' column is written in the resultfile.amat



Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2008-04-24 19:04:13 UTC (rev 8887)
+++ trunk/python_modules/plearn/learners/SVM.py	2008-04-24 19:06:36 UTC (rev 8888)
@@ -24,14 +24,14 @@
         List of attributes:    
         -------------------    
 
-    public: buildoption
+    user options:
 
 
         'C_initvalue': <float> Default value for 'C'.
 
         'verbosity': <int> Level of verbosity.
 
-    public: learntoption
+    learnt options:
 
         'best_param': <dict> of hyperparameters'values      
                       which gave the best performance by    
@@ -52,7 +52,7 @@
 
         'input_stds': <list> of standard deviation values for each input component.
 
-    protected:
+    internally managed options:
 
         'kernel_type': <str> corresponding to the kernel.    
                        in ['gaussian','linear','poly'].
@@ -562,7 +562,7 @@
         List of attributes:    
         -------------------    
 
-    public: buildoption
+    user options:
 
         'costnames': <list> of cost names. By default, ['class_error'].
                  All cost names implemented are:
@@ -630,6 +630,11 @@
 
         'test_on_train': <bool> Should we test best models on {test, train} (1)
 
+        'testlevel': <int> Frequency of test:
+                     - 0: write results only at the end of the run()
+                     - 1: write results each time a better validation cost is found in run()
+                     - 2: write all intermediate results
+
         'results_filename': <string> Path to an output file for results
         
         'preproc_optionnames': <string> or <list of strings> indicating the names of the
@@ -641,7 +646,7 @@
 
         'verbosity': <int> Level of verbosity.
 
-    public: learntoption
+    learnt options:
 
         'best_model':
 
@@ -670,9 +675,8 @@
         'input_avgstd': <float> input std
 
         'validtype': <str> type of validation: 'simple' or 'cross'.
-        
-       
-    protected:
+               
+    internally managed options:
 
         'param_names': <list> of all hyperparameter names. It    
                        always includes at first the positive     
@@ -698,6 +702,7 @@
                         'max_ntrials',
                         'retrain_on_valid',
                         'test_on_train',
+                        'testlevel',
                         'verbosity',
                         'results_filename',
                         'preproc_optionnames',
@@ -774,9 +779,10 @@
         self.retrain_on_valid = True
         self.retrain_until_local_optimum_is_found = True
         self.max_ntrials = 50
-        self.test_on_train = True
+        self.test_on_train = False
         
         self.verbosity = 0
+        self.testlevel = 1
         
         self.trainset_key = 'trainset'
         self.validset_key = 'validset'
@@ -819,7 +825,8 @@
 
     def additional_preproc(self, input_vmat, isTrain=False):
         if self.balance_classes and isTrain:
-            return ReplicateSamplesVMatrix(source = input_vmat, operate_on_bags = True)
+            return ReplicateSamplesVMatrix(source = input_vmat,
+                                           operate_on_bags = (input_vmat.targetsize > 1 ))
         return input_vmat
 
     ## specific to libsvm
@@ -881,7 +888,7 @@
                      self.inputsize, self.input_avgstd )
         assert vmat <> None
 
-        samples, targets = self.get_svminputlist( vmat )
+        samples, targets = self.get_svminputlist( vmat, True )
 
         self.all_experts[0].verbosity = self.verbosity
         self.inputsize, self.input_avgstd = self.all_experts[0].get_input_stats(samples)
@@ -983,7 +990,8 @@
     def write_results(  self, param,
                         valid_stats,
                         test_stats = None,
-                        train_stats= None ):
+                        train_stats= None,
+                        only_stdout=False ):
         if valid_stats==None and param == self.best_param:
                 valid_stats = self.valid_stats
         if test_stats==None and param == self.best_param:
@@ -996,16 +1004,16 @@
         
         # If no file specified, print on stdout in a readable format
         if self.results_filename == None or self.verbosity > 0:
-            print "\n -- Trial with parameters"
-            for pn in param:
-                print "    ",pn," = ",param[pn]
+            #print "\n -- Trial with parameters"
+            #for pn in param:
+            #    print "    ",pn," = ",param[pn]
             if train_costs <> None:
                 print " -- train costs: ", train_costs
             if valid_costs <> None:
                 print " -- valid costs: ", valid_costs
             if test_costs <> None:
                 print " -- test costs: ",  test_costs
-            if self.results_filename == None:
+            if self.results_filename == None or only_stdout:
                 return
 
 
@@ -1041,8 +1049,13 @@
         
         costnames_string = ""
         costvalues_string = ""
+        
+        all_set_names = ['valid','test']
+        if self.test_on_train:
+            all_set_names = ['train']+all_set_names
+        
         for cn in self.costnames:
-            for dataset in ['train','valid','test']:
+            for dataset in all_set_names:
                 costs = eval(dataset+'_costs')
                 
                 # Special processing for the confusion matrix
@@ -1357,6 +1370,7 @@
                dataspec,
                param= None):
         if self.validset_key in dataspec:
+            self.validtype = 'simple'
             return self.simplevalid(dataspec, param)
         else:
             return self.crossvalid(dataspec, param)
@@ -1366,11 +1380,9 @@
     """
     def simplevalid( self,
                      dataspec,
-                     param= None,
+                     param,
                      validstats = None,
                      verbosity = True ):
-        if not param:
-            param = self.best_param
         if self.verbosity > 0 and verbosity:
             print "\n** Simple Validation"
             print "   with param %s" % param
@@ -1385,9 +1397,7 @@
     """
     def crossvalid( self,
                     dataspec,
-                    param = None ):
-        if not param:
-            param = self.best_param
+                    param ):
         nclasses = self.nclasses
         n_fold = self.n_fold
         self.validtype = '%s-fold' % n_fold
@@ -1439,6 +1449,63 @@
         return validstats
 
 
+    def retrain_and_writeresults(self, dataspec):
+        trainset = self.train_inputspec(dataspec)
+        validset = self.valid_inputspec(dataspec)
+        testset  = self.test_inputspec(dataspec)
+        # Cross Validation
+        if 'fold' in self.validtype:
+            self.train( dataspec )
+
+        # Simple Validation
+        else:
+            self.validtype = 'simple'
+            # CAUTION: in the case of simple validation without retraining on {train+valid},
+            #          self.best_model is supposed to be updated
+            if self.retrain_on_valid:
+                """ Uncomment following lines if you want to check that
+                    retraining on {train + valid} sets does not degrade.
+                """
+                #train_stats = None
+                #test_stats = None
+                #if self.test_on_train:
+                #    if self.verbosity > 2:
+                #        print "\n** (testing simple valid on "+self.trainset_key+")"
+                #    train_stats = self.test( trainset )
+                #if testset <> None:
+                #    if self.verbosity > 2:
+                #        print "\n** (testing simple valid on "+self.testset_key+")"
+                #test_stats = self.test( testset  )
+                #
+                #self.write_results( self.best_param,
+                #                    valid_stats, test_stats, train_stats )
+                self.validtype = 'simple+retrain'
+                tv_set = ConcatRowsVMatrix(
+                            sources = [ trainset,
+                                        validset
+                                        ],
+                            fully_check_mappings = False,
+                        )
+                if self.verbosity > 0:
+                    print "\n** re-training model on { train + valid } "
+                self.train( {self.trainset_key: tv_set} )
+
+        train_stats = None
+        test_stats = None
+        if self.test_on_train:
+            if self.verbosity > 2:
+               print "\n** (testing on "+self.trainset_key+")"
+            train_stats = self.test( trainset )
+        if testset <> None:
+            if self.verbosity > 2:
+                print "\n** (testing on "+self.testset_key+")"
+            test_stats = self.test( testset )
+        self.update_trials( self.best_param,
+                            None, test_stats, train_stats )
+        self.write_results( self.best_param,
+                            self.valid_stats, self.test_stats, self.train_stats )
+
+
     """ THE interesting function of the class.
         See __main__ below for usage.
         dataspec is a dictionary which specifies train, valid, test sets.
@@ -1446,9 +1513,6 @@
         cf. train_inputspec(), valid_inputspec(), and test_inputspec().
     """
     def run(self, dataspec):
-        if self.verbosity > 3:
-            print "SVM::run() called ", dataspec.keys()
-
         trainset = self.train_inputspec(dataspec)
         validset = self.valid_inputspec(dataspec)
         testset  = self.test_inputspec(dataspec)
@@ -1476,73 +1540,30 @@
 
                 # We reject the model (to avoid testing on it)
                 self.model = None
-                
-                self.write_results( param, valid_stats )
+                self.write_results( param, valid_stats, None, None, self.testlevel < 2  )
 
             # Better valid cost is obtained!
             else:
-
-                # Cross Validation
-                if 'fold' in self.validtype:
-                    self.train( dataspec )
-
+                print "better cost was found!"
                 # Simple Validation
+                if 'fold' not in self.validtype:
+                    self.best_model = self.model
+                    
+                if self.testlevel > 0:
+                    self.retrain_and_writeresults(dataspec)
                 else:
-                    self.validtype = 'simple'
-                    self.best_model = self.model
-                    if self.retrain_on_valid:
+                    self.write_results( param, valid_stats, None, None, True  )
 
-                        """ Uncomment following lines if you want to check that
-                            retraining on {train + valid} sets does not degrade.
-                        """
-                        #train_stats = None
-                        #test_stats = None
-                        #if self.test_on_train:
-                        #    if self.verbosity > 2:
-                        #        print "\n** (testing simple valid on "+self.trainset_key+")"
-                        #    train_stats = self.test( trainset )
-                        #if testset <> None:
-                        #    if self.verbosity > 2:
-                        #        print "\n** (testing simple valid on "+self.testset_key+")"
-                        #test_stats = self.test( testset  )
-                        #
-                        #self.write_results( self.best_param,
-                        #                    valid_stats, test_stats, train_stats )
-
-                        self.validtype = 'simple+retrain'
-
-                        tv_set = ConcatRowsVMatrix(
-                                    sources = [ trainset,
-                                                validset
-                                                ],
-                                    fully_check_mappings = False,
-                                )
-                        if self.verbosity > 0:
-                            print "\n** re-training model on { train + valid } "
-                        self.train( {self.trainset_key: tv_set} )
-
-                train_stats = None
-                test_stats = None
-                if self.test_on_train:
-                    if self.verbosity > 2:
-                        print "\n** (testing on "+self.trainset_key+")"
-                    train_stats = self.test( trainset )
-                if testset <> None:
-                    if self.verbosity > 2:
-                        print "\n** (testing on "+self.testset_key+")"
-                    test_stats = self.test( testset )
-                self.update_trials( self.best_param,
-                                    None, test_stats, train_stats )
-                self.write_results( self.best_param,
-                                    self.valid_stats, self.test_stats, self.train_stats )
-
             if len(expert.trials_param_list)-L0 >= self.max_ntrials:
                 return dataspec
 
         if( self.retrain_until_local_optimum_is_found
         and expert.should_be_tuned_again() ):
-           self.run( dataspec )
+           return self.run( dataspec )
 
+        if self.testlevel == 0:
+             self.retrain_and_writeresults(dataspec)
+
         return dataspec
 
 """ Some AUXILIARY FUNCTIONS, to have access or modify 



From nouiz at mail.berlios.de  Thu Apr 24 22:08:02 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 24 Apr 2008 22:08:02 +0200
Subject: [Plearn-commits] r8889 - in
	trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir:
	. Split0 Split0/LearnerExpdir
Message-ID: <200804242008.m3OK82Ka009853@sheep.berlios.de>

Author: nouiz
Date: 2008-04-24 22:08:02 +0200 (Thu, 24 Apr 2008)
New Revision: 8889

Modified:
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/global_stats.pmat
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/split_stats.pmat
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave
Log:
Commit pytest modif do to modif in EarlyStoppingOracle and HyperOptimize


Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave	2008-04-24 19:06:36 UTC (rev 8888)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave	2008-04-24 20:08:02 UTC (rev 8889)
@@ -1,11 +1,14 @@
 DeepBeliefNet(
 cd_learning_rate = 0.0100000000000000002 ;
 cd_decrease_ct = 0 ;
+up_down_learning_rate = 0 ;
+up_down_decrease_ct = 0 ;
 grad_learning_rate = 0.100000000000000006 ;
 grad_decrease_ct = 0 ;
 batch_size = 1 ;
 n_classes = 2 ;
 training_schedule = 2 [ 20 10 ] ;
+up_down_nstages = 0 ;
 use_classification_cost = 1 ;
 reconstruct_layerwise = 0 ;
 layers = 2 [ *1 ->RBMBinomialLayer(
@@ -28,7 +31,8 @@
 random_gen = *2 ->PRandom(
 seed = 1827 ;
 fixed_seed = 0  )
- )
+;
+verbosity = 1  )
 *3 ->RBMBinomialLayer(
 size = 2 ;
 learning_rate = 0.100000000000000006 ;
@@ -39,20 +43,21 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 2 [ -0.0041988074813960885 0.00291521845148070419 ] ;
+bias = 2 [ -0.00419880748139610759 0.0029152184514807198 ] ;
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMBinomialLayer" ;
 use_fast_approximations = 0 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *2   )
+random_gen = *2  ;
+verbosity = 1  )
 ] ;
 i_output_layer = 1 ;
 connections = 1 [ *4 ->RBMMatrixConnection(
 weights = 2  2  [ 
 -0.222066209664342068 	0.582925561708350082 	
-0.516883916989072545 	-0.316272804129796303 	
+0.516883916989072434 	-0.316272804129796303 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -61,6 +66,10 @@
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
 L1_penalty_factor = 0 ;
 L2_penalty_factor = 0 ;
+L2_decrease_constant = 0 ;
+L2_shift = 100 ;
+L2_decrease_type = "one_over_t" ;
+L2_n_updates = 0 ;
 down_size = 2 ;
 up_size = 2 ;
 learning_rate = 0.100000000000000006 ;
@@ -72,15 +81,16 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *2   )
+random_gen = *2  ;
+verbosity = 1  )
 ] ;
 classification_module = *5 ->RBMClassificationModule(
 previous_to_last = *4  ;
 last_layer = *3  ;
 last_to_target = *6 ->RBMMatrixConnection(
 weights = 2  2  [ 
--0.603392333131359426 	0.370787384786479657 	
-0.09708625652197031 	-0.668099599496062679 	
+-0.603392333131359537 	0.370787384786479657 	
+0.0970862565219703239 	-0.668099599496062679 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -89,6 +99,10 @@
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
 L1_penalty_factor = 0 ;
 L2_penalty_factor = 0 ;
+L2_decrease_constant = 0 ;
+L2_shift = 100 ;
+L2_decrease_type = "one_over_t" ;
+L2_n_updates = 0 ;
 down_size = 2 ;
 up_size = 2 ;
 learning_rate = 0.100000000000000006 ;
@@ -100,7 +114,8 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *2   )
+random_gen = *2  ;
+verbosity = 1  )
 ;
 target_layer = *7 ->RBMMultinomialLayer(
 size = 2 ;
@@ -112,14 +127,15 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 2 [ -0.00487910443291659449 0.00487910443291660317 ] ;
+bias = 2 [ -0.00487910443291655199 0.00487910443291657281 ] ;
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMMultinomialLayer" ;
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *2   )
+random_gen = *2  ;
+verbosity = 1  )
 ;
 joint_connection = *8 ->RBMMixedConnection(
 sub_connections = 1  2  [ 
@@ -140,7 +156,8 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *2   )
+random_gen = *2  ;
+verbosity = 1  )
 ;
 last_size = 2 ;
 input_size = 2 ;
@@ -149,21 +166,28 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *2   )
+random_gen = *2  ;
+verbosity = 1  )
 ;
 final_module = *0 ;
 final_cost = *0 ;
 partial_costs = []
 ;
+use_sample_for_up_layer = 0 ;
 online = 0 ;
 background_gibbs_update_ratio = 0 ;
 gibbs_chain_reinit_freq = 2147483647 ;
+use_mean_field_contrastive_divergence = 0 ;
+train_stats_window = -1 ;
 top_layer_joint_cd = 0 ;
 n_layers = 2 ;
 minibatch_size = 1 ;
 gibbs_down_state = 1 [ 0  0  [ 
 ]
 ] ;
+up_down_stage = 0 ;
+generative_connections = []
+;
 random_gen = *2  ;
 seed = 1827 ;
 stage = 30 ;

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave	2008-04-24 19:06:36 UTC (rev 8888)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave	2008-04-24 20:08:02 UTC (rev 8889)
@@ -25,19 +25,23 @@
 splits = 1  2  [ 
 (0 , 1 )	(0 , 1 )	
 ]
- )
 ;
+one_is_absolute = 0  )
+;
 statnames = 4 [ "E[train.E[NLL]]" "E[train.E[class_error]]" "E[test.E[NLL]]" "E[test.E[class_error]]" ] ;
 statmask = []
 ;
 learner = *5 ->DeepBeliefNet(
 cd_learning_rate = 0.0100000000000000002 ;
 cd_decrease_ct = 0 ;
+up_down_learning_rate = 0 ;
+up_down_decrease_ct = 0 ;
 grad_learning_rate = 0.100000000000000006 ;
 grad_decrease_ct = 0 ;
 batch_size = 1 ;
 n_classes = 2 ;
 training_schedule = 2 [ 20 10 ] ;
+up_down_nstages = 0 ;
 use_classification_cost = 1 ;
 reconstruct_layerwise = 0 ;
 layers = 2 [ *6 ->RBMBinomialLayer(
@@ -60,7 +64,8 @@
 random_gen = *7 ->PRandom(
 seed = 1827 ;
 fixed_seed = 0  )
- )
+;
+verbosity = 1  )
 *8 ->RBMBinomialLayer(
 size = 2 ;
 learning_rate = 0.100000000000000006 ;
@@ -71,20 +76,21 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 2 [ -0.0041988074813960885 0.00291521845148070419 ] ;
+bias = 2 [ -0.00419880748139610759 0.0029152184514807198 ] ;
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMBinomialLayer" ;
 use_fast_approximations = 0 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *7   )
+random_gen = *7  ;
+verbosity = 1  )
 ] ;
 i_output_layer = 1 ;
 connections = 1 [ *9 ->RBMMatrixConnection(
 weights = 2  2  [ 
 -0.222066209664342068 	0.582925561708350082 	
-0.516883916989072545 	-0.316272804129796303 	
+0.516883916989072434 	-0.316272804129796303 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -93,6 +99,10 @@
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
 L1_penalty_factor = 0 ;
 L2_penalty_factor = 0 ;
+L2_decrease_constant = 0 ;
+L2_shift = 100 ;
+L2_decrease_type = "one_over_t" ;
+L2_n_updates = 0 ;
 down_size = 2 ;
 up_size = 2 ;
 learning_rate = 0.100000000000000006 ;
@@ -104,15 +114,16 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *7   )
+random_gen = *7  ;
+verbosity = 1  )
 ] ;
 classification_module = *10 ->RBMClassificationModule(
 previous_to_last = *9  ;
 last_layer = *8  ;
 last_to_target = *11 ->RBMMatrixConnection(
 weights = 2  2  [ 
--0.603392333131359426 	0.370787384786479657 	
-0.09708625652197031 	-0.668099599496062679 	
+-0.603392333131359537 	0.370787384786479657 	
+0.0970862565219703239 	-0.668099599496062679 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -121,6 +132,10 @@
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
 L1_penalty_factor = 0 ;
 L2_penalty_factor = 0 ;
+L2_decrease_constant = 0 ;
+L2_shift = 100 ;
+L2_decrease_type = "one_over_t" ;
+L2_n_updates = 0 ;
 down_size = 2 ;
 up_size = 2 ;
 learning_rate = 0.100000000000000006 ;
@@ -132,7 +147,8 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *7   )
+random_gen = *7  ;
+verbosity = 1  )
 ;
 target_layer = *12 ->RBMMultinomialLayer(
 size = 2 ;
@@ -144,14 +160,15 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 2 [ -0.00487910443291659449 0.00487910443291660317 ] ;
+bias = 2 [ -0.00487910443291655199 0.00487910443291657281 ] ;
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMMultinomialLayer" ;
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *7   )
+random_gen = *7  ;
+verbosity = 1  )
 ;
 joint_connection = *13 ->RBMMixedConnection(
 sub_connections = 1  2  [ 
@@ -172,7 +189,8 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *7   )
+random_gen = *7  ;
+verbosity = 1  )
 ;
 last_size = 2 ;
 input_size = 2 ;
@@ -181,21 +199,28 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *7   )
+random_gen = *7  ;
+verbosity = 1  )
 ;
 final_module = *0 ;
 final_cost = *0 ;
 partial_costs = []
 ;
+use_sample_for_up_layer = 0 ;
 online = 0 ;
 background_gibbs_update_ratio = 0 ;
 gibbs_chain_reinit_freq = 2147483647 ;
+use_mean_field_contrastive_divergence = 0 ;
+train_stats_window = -1 ;
 top_layer_joint_cd = 0 ;
 n_layers = 2 ;
 minibatch_size = 1 ;
 gibbs_down_state = 1 [ 0  0  [ 
 ]
 ] ;
+up_down_stage = 0 ;
+generative_connections = []
+;
 random_gen = *7  ;
 seed = 1827 ;
 stage = 30 ;
@@ -243,21 +268,34 @@
 values = []
 ;
 range = 3 [ 0 31 2 ] ;
-min_value = -3.4028234663852886e+38 ;
-max_value = 3.4028234663852886e+38 ;
-max_degradation = 3.4028234663852886e+38 ;
+min_value = -1.79769313486231571e+308 ;
+max_value = 1.79769313486231571e+308 ;
+max_degradation = 1.79769313486231571e+308 ;
 relative_max_degradation = -1 ;
-min_improvement = -3.4028234663852886e+38 ;
+min_improvement = -1.79769313486231571e+308 ;
 relative_min_improvement = -1 ;
 max_degraded_steps = 31 ;
-min_n_steps = 0  )
+min_n_steps = 0 ;
+nreturned = 16 ;
+best_objective = 0.614096318051678636 ;
+best_step = 16 ;
+met_early_stopping = 0  )
 ;
 provide_tester_expdir = 0 ;
 sub_strategy = []
 ;
 rerun_after_sub = 0 ;
 provide_sub_expdir = 1 ;
-splitter = *0  )
+save_best_learner = 0 ;
+splitter = *0 ;
+auto_save = 0 ;
+best_objective = 0.614096318051678636 ;
+best_results = 4 [ 0.651261219307483818 0.5 0.614096318051678636 0 ] ;
+best_learner = *5  ;
+trialnum = 16 ;
+option_vals = []
+;
+verbosity = 0  )
 ] ;
 provide_strategy_expdir = 1 ;
 save_final_learner = 1 ;

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/train_stats.psave	2008-04-24 19:06:36 UTC (rev 8888)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/train_stats.psave	2008-04-24 20:08:02 UTC (rev 8889)
@@ -60,12 +60,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 0.614096318051678525 ;
-max_ = 0.614096318051678525 ;
+min_ = 0.614096318051678636 ;
+max_ = 0.614096318051678636 ;
 agmemin_ = 0 ;
 agemax_ = 0 ;
-first_ = 0.614096318051678525 ;
-last_ = 0.614096318051678525 ;
+first_ = 0.614096318051678636 ;
+last_ = 0.614096318051678636 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/metainfos.txt	2008-04-24 19:06:36 UTC (rev 8888)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/metainfos.txt	2008-04-24 20:08:02 UTC (rev 8889)
@@ -1,2 +1,2 @@
-__REVISION__ = "PL8398"
+__REVISION__ = "PL8883"
 

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave	2008-04-24 19:06:36 UTC (rev 8888)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave	2008-04-24 20:08:02 UTC (rev 8889)
@@ -34,19 +34,23 @@
 splits = 1  2  [ 
 (0 , 1 )	(0 , 1 )	
 ]
- )
 ;
+one_is_absolute = 0  )
+;
 statnames = 4 [ "E[train.E[NLL]]" "E[train.E[class_error]]" "E[test.E[NLL]]" "E[test.E[class_error]]" ] ;
 statmask = []
 ;
 learner = *6 ->DeepBeliefNet(
 cd_learning_rate = 0.0100000000000000002 ;
 cd_decrease_ct = 0 ;
+up_down_learning_rate = 0 ;
+up_down_decrease_ct = 0 ;
 grad_learning_rate = 0.100000000000000006 ;
 grad_decrease_ct = 0 ;
 batch_size = 1 ;
 n_classes = 2 ;
 training_schedule = 2 [ 20 10 ] ;
+up_down_nstages = 0 ;
 use_classification_cost = 1 ;
 reconstruct_layerwise = 0 ;
 layers = 2 [ *7 ->RBMBinomialLayer(
@@ -69,7 +73,8 @@
 random_gen = *8 ->PRandom(
 seed = 1827 ;
 fixed_seed = 0  )
- )
+;
+verbosity = 1  )
 *9 ->RBMBinomialLayer(
 size = 2 ;
 learning_rate = 0 ;
@@ -87,13 +92,14 @@
 use_fast_approximations = 0 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *8   )
+random_gen = *8  ;
+verbosity = 1  )
 ] ;
 i_output_layer = 1 ;
 connections = 1 [ *10 ->RBMMatrixConnection(
 weights = 2  2  [ 
-0.211616747512905684 	0.21566475046848535 	
-0.59211590021607885 	0.667132771633056398 	
+0.211616747512905712 	0.215664750468485322 	
+0.59211590021607885 	0.667132771633056509 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -102,6 +108,10 @@
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
 L1_penalty_factor = 0 ;
 L2_penalty_factor = 0 ;
+L2_decrease_constant = 0 ;
+L2_shift = 100 ;
+L2_decrease_type = "one_over_t" ;
+L2_n_updates = 0 ;
 down_size = 2 ;
 up_size = 2 ;
 learning_rate = 0 ;
@@ -113,15 +123,16 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *8   )
+random_gen = *8  ;
+verbosity = 1  )
 ] ;
 classification_module = *11 ->RBMClassificationModule(
 previous_to_last = *10  ;
 last_layer = *9  ;
 last_to_target = *12 ->RBMMatrixConnection(
 weights = 2  2  [ 
--0.166376995329432703 	0.522222782976989097 	
-0.449826313170107683 	-0.266489754613600693 	
+-0.166376995329432731 	0.522222782976989097 	
+0.449826313170107572 	-0.266489754613600693 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -130,6 +141,10 @@
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
 L1_penalty_factor = 0 ;
 L2_penalty_factor = 0 ;
+L2_decrease_constant = 0 ;
+L2_shift = 100 ;
+L2_decrease_type = "one_over_t" ;
+L2_n_updates = 0 ;
 down_size = 2 ;
 up_size = 2 ;
 learning_rate = 0 ;
@@ -141,7 +156,8 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *8   )
+random_gen = *8  ;
+verbosity = 1  )
 ;
 target_layer = *13 ->RBMMultinomialLayer(
 size = 2 ;
@@ -160,7 +176,8 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *8   )
+random_gen = *8  ;
+verbosity = 1  )
 ;
 joint_connection = *14 ->RBMMixedConnection(
 sub_connections = 1  2  [ 
@@ -181,7 +198,8 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *8   )
+random_gen = *8  ;
+verbosity = 1  )
 ;
 last_size = 2 ;
 input_size = 2 ;
@@ -190,21 +208,28 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *8   )
+random_gen = *8  ;
+verbosity = 1  )
 ;
 final_module = *0 ;
 final_cost = *0 ;
 partial_costs = []
 ;
+use_sample_for_up_layer = 0 ;
 online = 0 ;
 background_gibbs_update_ratio = 0 ;
 gibbs_chain_reinit_freq = 2147483647 ;
+use_mean_field_contrastive_divergence = 0 ;
+train_stats_window = -1 ;
 top_layer_joint_cd = 0 ;
 n_layers = 2 ;
 minibatch_size = 0 ;
 gibbs_down_state = 1 [ 0  0  [ 
 ]
 ] ;
+up_down_stage = 0 ;
+generative_connections = []
+;
 random_gen = *8  ;
 seed = 1827 ;
 stage = 0 ;
@@ -252,21 +277,35 @@
 values = []
 ;
 range = 3 [ 0 31 2 ] ;
-min_value = -3.4028234663852886e+38 ;
-max_value = 3.4028234663852886e+38 ;
-max_degradation = 3.4028234663852886e+38 ;
+min_value = -1.79769313486231571e+308 ;
+max_value = 1.79769313486231571e+308 ;
+max_degradation = 1.79769313486231571e+308 ;
 relative_max_degradation = -1 ;
-min_improvement = -3.4028234663852886e+38 ;
+min_improvement = -1.79769313486231571e+308 ;
 relative_min_improvement = -1 ;
 max_degraded_steps = 31 ;
-min_n_steps = 0  )
+min_n_steps = 0 ;
+nreturned = 0 ;
+best_objective = 1.79769313486231571e+308 ;
+best_step = -1 ;
+met_early_stopping = 0  )
 ;
 provide_tester_expdir = 0 ;
 sub_strategy = []
 ;
 rerun_after_sub = 0 ;
 provide_sub_expdir = 1 ;
-splitter = *0  )
+save_best_learner = 0 ;
+splitter = *0 ;
+auto_save = 0 ;
+best_objective = 1.79769313486231571e+308 ;
+best_results = []
+;
+best_learner = *0 ;
+trialnum = 0 ;
+option_vals = []
+;
+verbosity = 0  )
 ] ;
 provide_strategy_expdir = 1 ;
 save_final_learner = 1 ;



From nouiz at mail.berlios.de  Thu Apr 24 22:18:13 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 24 Apr 2008 22:18:13 +0200
Subject: [Plearn-commits] r8890 - in
	trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir:
	. Split0
Message-ID: <200804242018.m3OKIDec011978@sheep.berlios.de>

Author: nouiz
Date: 2008-04-24 22:18:13 +0200 (Thu, 24 Apr 2008)
New Revision: 8890

Modified:
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave
Log:
removed not commited option


Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave	2008-04-24 20:08:02 UTC (rev 8889)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave	2008-04-24 20:18:13 UTC (rev 8890)
@@ -288,7 +288,6 @@
 provide_sub_expdir = 1 ;
 save_best_learner = 0 ;
 splitter = *0 ;
-auto_save = 0 ;
 best_objective = 0.614096318051678636 ;
 best_results = 4 [ 0.651261219307483818 0.5 0.614096318051678636 0 ] ;
 best_learner = *5  ;

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave	2008-04-24 20:08:02 UTC (rev 8889)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave	2008-04-24 20:18:13 UTC (rev 8890)
@@ -297,7 +297,6 @@
 provide_sub_expdir = 1 ;
 save_best_learner = 0 ;
 splitter = *0 ;
-auto_save = 0 ;
 best_objective = 1.79769313486231571e+308 ;
 best_results = []
 ;



From nouiz at mail.berlios.de  Thu Apr 24 22:47:13 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 24 Apr 2008 22:47:13 +0200
Subject: [Plearn-commits] r8891 - in
	trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir:
	. Split0 Split0/LearnerExpdir
Message-ID: <200804242047.m3OKlDLC018000@sheep.berlios.de>

Author: nouiz
Date: 2008-04-24 22:47:12 +0200 (Thu, 24 Apr 2008)
New Revision: 8891

Modified:
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_costs.pmat
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_outputs.pmat
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_costs.pmat
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_outputs.pmat
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/global_stats.pmat
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/split_stats.pmat
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/tester.psave
Log:
regenerated test from the few last commit


Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave	2008-04-24 20:18:13 UTC (rev 8890)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave	2008-04-24 20:47:12 UTC (rev 8891)
@@ -12,231 +12,231 @@
 weight_decay = 0.0100000000000000002 ;
 include_bias = 1 ;
 params = 225  2  [ 
-2.7151351062236615 	6.96796926051022236 	
--0.924570620989403458 	-5.50999860198525226 	
-2.74766209737707889 	16.2764471019291541 	
--0.897331116702694054 	24.1696624839723277 	
--0.257145462010700432 	-2.68785159119131878 	
--1.3282966539723533 	-18.5932956256279311 	
--5.49258347507660183 	-1.57002604296742554 	
-1.46070534955196196 	-3.53276434234297421 	
--1.03812617167629262 	-1.20731159585204617 	
-1.02580172582147133 	-26.7245637329914061 	
-0.791254761045375288 	82.4469215737143202 	
-0.836570941010414737 	-16.420263045944921 	
--1.58240826855158301 	-6.65940808940268703 	
--8.24967860415081411 	-3.4995708964716874 	
-0.978241143861720674 	-4.92116475768050154 	
-0.0717117261694705244 	-3.09067303052573861 	
--2.58531178640539849 	12.0354584295284699 	
-3.21107644425249639 	5.97223155883275147 	
-1.43705644733271809 	94.2974631733482198 	
-0.651016019177080518 	1.95403761232638362 	
--1.20978628334706184 	-7.36323983158574702 	
--0.676733526068720059 	-31.4356962338422257 	
--3.71894302925004183 	-5.30412867323935266 	
--2.68184912068139347 	-0.224631918495068694 	
--1.88827683783783651 	-11.2808510609680628 	
-1.56552507696869569 	-5.27853909851396796 	
--2.51225078969575177 	-3.22314543004070897 	
--8.82457968581596752 	-1.42245769348021844 	
--5.4971248965487387 	-14.759995096029046 	
-8.47592427242844337 	8.11107085181370557 	
-2.34953305575392202 	4.63659562958691041 	
--1.64301101094937252 	11.4873476009575572 	
-0.580660833465362791 	13.1755650470871277 	
--0.650250885491696851 	1.90203710703855133 	
--3.87917543422287148 	5.03518774179589901 	
-1.7039737911280386 	1.34775554555850574 	
--2.78522303432125629 	-64.1292521131980919 	
-2.32700743878955185 	24.743612559642397 	
--5.39566293426395394 	-9.82739376173094037 	
-12.9203582472649128 	5.29957416698956862 	
--10.7455769141728563 	89.9090483177386801 	
-4.31713435895606157 	-5.18586285077431786 	
-0.848047232584521082 	-2.17948525293295869 	
--9.4681032062965258 	-0.22074895143857387 	
-7.21414655874796917 	7.46866878382939792 	
--1.75858145601429361 	11.2871954305960074 	
-1.96560792644405802 	-6.52458829385219907 	
--2.79380808107006473 	-5.09955504189952258 	
--0.799059310236704512 	-12.9255049414569196 	
--4.25437046158016585 	8.96615214816819339 	
--9.43622335236651288 	3.78339978190638559 	
-4.36697916816686238 	-0.687503879096243442 	
--2.18796130106503472 	-6.69802616230832459 	
-3.17982632559527989 	-1.43703160328240198 	
--4.3141453664152607 	-5.51667561668627471 	
--2.74519292766851297 	7.73168925236620286 	
-1.05378206408368125 	0.178110338223795794 	
-0.460217896787359693 	10.0585449039864159 	
--0.0106331952235446964 	-19.6790725953361481 	
-0.424465188958869921 	-15.8551532141411631 	
-0.567218573767627854 	-2.6951904195179508 	
-4.45718670597547018 	5.62393864872558336 	
-2.19961229165357031 	-5.42279390073859879 	
--0.228762156049838294 	-0.831542078930034179 	
--2.4010318622261817 	15.8373411775190593 	
--1.34006398346898026 	-12.0359127014414575 	
-4.0381021751345596 	-2.91524838149849552 	
-0.593248196395251171 	0.218503040318817232 	
+2.71513510622207166 	6.96796926050934751 	
+-0.924570620987045566 	-5.50999860198858826 	
+2.74766209737875133 	16.2764471019206418 	
+-0.897331116697557052 	24.1696624838867358 	
+-0.257145462011077963 	-2.68785159119476535 	
+-1.3282966539726655 	-18.5932956256296649 	
+-5.49258347509019718 	-1.57002604298704584 	
+1.46070534954897835 	-3.53276434233635506 	
+-1.038126171674709 	-1.2073115958693148 	
+1.02580172581497386 	-26.7245637328128929 	
+0.791254761045932176 	82.4469215737125865 	
+0.836570941006688051 	-16.4202630459276833 	
+-1.58240826854525074 	-6.65940808938515527 	
+-8.24967860415612364 	-3.49957089648365738 	
+0.9782411438628974 	-4.92116475768973771 	
+0.0717117261764370351 	-3.09067303050886988 	
+-2.58531178640483761 	12.035458429525173 	
+3.211076444250756 	5.97223155883393897 	
+1.43705644733268212 	94.2974631733484614 	
+0.651016019176285488 	1.95403761231488304 	
+-1.20978628334898786 	-7.36323983157156103 	
+-0.676733526068380775 	-31.4356962338337986 	
+-3.71894302925004316 	-5.30412867324711534 	
+-2.6818491206821955 	-0.224631918494603705 	
+-1.88827683784314537 	-11.2808510609389074 	
+1.56552507696732834 	-5.27853909851336311 	
+-2.51225078969590498 	-3.22314543003026799 	
+-8.82457968582814622 	-1.42245769348111262 	
+-5.49712489654685843 	-14.7599950960045039 	
+8.47592427242672741 	8.11107085181684795 	
+2.34953305575425997 	4.63659562959487381 	
+-1.64301101095052804 	11.4873476009648563 	
+0.580660833463284676 	13.1755650470951462 	
+-0.650250885492095976 	1.90203710703754014 	
+-3.87917543422272804 	5.03518774179621609 	
+1.70397379112897029 	1.34775554557823152 	
+-2.78522303432040363 	-64.1292521132013178 	
+2.32700743878965444 	24.7436125596276284 	
+-5.39566293426405519 	-9.82739376173207368 	
+12.9203582472648133 	5.29957416697557626 	
+-10.7455769141731885 	89.9090483177333653 	
+4.31713435896750308 	-5.18586285076809528 	
+0.848047232583675648 	-2.17948525292697681 	
+-9.46810320629667324 	-0.220748951447968828 	
+7.21414655874827737 	7.46866878382293553 	
+-1.75858145600769644 	11.2871954305640294 	
+1.96560792644487359 	-6.52458829385446837 	
+-2.79380808107613055 	-5.09955504191000042 	
+-0.79905931023683674 	-12.925504941414971 	
+-4.25437046157972976 	8.96615214816773154 	
+-9.43622335236653065 	3.78339978190649218 	
+4.36697916817812093 	-0.68750387907888455 	
+-2.18796130106569242 	-6.69802616230909642 	
+3.17982632559557965 	-1.4370316032819157 	
+-4.31414536641474289 	-5.51667561668654649 	
+-2.74519292766675793 	7.73168925235037285 	
+1.0537820640848472 	0.178110338215527242 	
+0.460217896787073477 	10.058544903984771 	
+-0.0106331952212118993 	-19.6790725953963772 	
+0.424465188958454087 	-15.8551532141373048 	
+0.567218573772457435 	-2.69519041953220029 	
+4.45718670597664524 	5.62393864871897797 	
+2.19961229164894245 	-5.42279390073130685 	
+-0.228762156047200543 	-0.831542078921280403 	
+-2.40103186222691534 	15.8373411775187218 	
+-1.34006398347233469 	-12.0359127014981127 	
+4.03810217513344138 	-2.91524838149503962 	
+0.593248196381594317 	0.218503040354157629 	
 -0.894620455881181775 	1.01068418956454376 	
--0.905934974062951648 	-1.04508844319938632 	
-8.00428356105546257 	-1.26168634488796494 	
-0.619982738884564899 	54.1218393093347103 	
--0.705573849334636627 	-1.2538137788610173 	
-0.871066182002223743 	40.4927882352060706 	
--7.05074671248599127 	71.3212740911236835 	
--2.22324163215964843 	1.0734561275325698 	
--2.44557668837193853 	4.2937313954156906 	
-9.82162368083046999 	6.64093872400913376 	
--9.18335934249402008 	-3.97745257891762716 	
--0.836507465302499753 	-1.0321141864197878 	
--0.0534975469631229703 	19.5696227138288812 	
-1.07568581177046618 	-5.10104965789005504 	
--0.518952740814859759 	15.8636310713528932 	
--2.84087926537195301 	-12.9191491848439277 	
-2.04826664350717103 	12.2408805584981177 	
-1.80375307384732975 	11.6859209915770084 	
--1.25621312028974019 	-2.02266034406559303 	
--0.0484853946913663472 	3.8049818168441556 	
--0.677242736030050585 	-42.625477534183851 	
-1.59162699009028663 	-5.48763586147435767 	
--1.21972683978848395 	-1.50185397433300194 	
-1.24858166506431711 	4.87092530127280199 	
--7.04338199489003269 	17.3390887919741523 	
-3.47070361924823967 	1.42970456326448381 	
--4.2655063338911301 	-1.73980331015853862 	
-2.12547785989874116 	3.62923159123009187 	
-2.53259933864800901 	-38.0016204477437114 	
-2.21881274683373908 	-0.0138947589423880184 	
-3.1476000203292771 	-1.50792558450150538 	
-1.08730538646035324 	-5.46742192799836069 	
-0.805042144582562846 	-53.0054722631629431 	
--1.33448103050428291 	5.00293548641483543 	
--4.27305110186990333 	-0.418543428366281389 	
-1.70132006444962425 	-5.38749080326675145 	
--1.12532164909613996 	33.3451642986689407 	
--2.70415771239980796 	3.8957298727264229 	
-4.98777580381757346 	13.8821103996191333 	
-0.568511986630580113 	-6.88339363194706877 	
-2.78082104553625609 	-5.19833751433356372 	
--3.45195094389070523 	6.54989673235655356 	
--1.26923932124764582 	-82.4386151297649405 	
-6.72216551206190616 	-5.26765661015598763 	
--0.742712929107524844 	-2.30022073912311376 	
-1.55989131761279332 	-4.18345207235786098 	
-4.72158352754091215 	21.7128846360060805 	
--2.74322426786452844 	0.0600870120549067896 	
-5.66665271018079864 	-3.34386936425720371 	
--1.7405916077822694 	11.7068252411821998 	
-0.339841379787524656 	1.53151755701120851 	
--0.136156736953116003 	-12.0671867463523572 	
-2.99636501990413606 	1.68582324363194558 	
--1.96646003944210279 	-7.02906457252052252 	
--2.68253702359536872 	19.2149219172472989 	
-3.77166285715782834 	-15.0634217849804468 	
-1.10118597276613972 	11.2002484600089556 	
--0.212650967437634342 	0.824744814209136368 	
-3.73652792828373403 	-7.81218626519529735 	
--0.151775371881687632 	1.00699915467459045 	
-3.72484877108447021 	5.92264185066602611 	
--0.0755790292600937996 	-3.91733851844509173 	
--1.69431689893300641 	3.02479371219719839 	
-0.0753423487888285753 	14.9935199826456476 	
-2.23109480398829207 	-2.59140850098041486 	
-6.91777882726934212 	41.5668021912209156 	
-1.87173910301644164 	-16.2617861474499854 	
--2.610894441212122 	-39.3944996228497075 	
+-0.905934974062951315 	-1.04508844319939009 	
+8.00428356105542704 	-1.26168634488815745 	
+0.619982738889116813 	54.1218393092184655 	
+-0.705573849337669534 	-1.25381377885319911 	
+0.871066182005948986 	40.4927882352012602 	
+-7.05074671248575058 	71.3212740911221061 	
+-2.22324163214598158 	1.07345612751800834 	
+-2.44557668836901909 	4.29373139551074523 	
+9.8216236808294628 	6.64093872401048912 	
+-9.18335934248439933 	-3.97745257894915438 	
+-0.836507465302499531 	-1.03211418641978869 	
+-0.053497546963581534 	19.5696227138299683 	
+1.07568581176873801 	-5.10104965788098763 	
+-0.51895274081205478 	15.8636310713846687 	
+-2.84087926537020463 	-12.9191491848348825 	
+2.04826664350002785 	12.2408805586060261 	
+1.80375307384701089 	11.6859209915609643 	
+-1.25621312029488119 	-2.02266034407121387 	
+-0.0484853946924818577 	3.80498181684614956 	
+-0.677242736032365844 	-42.6254775342041086 	
+1.5916269901049056 	-5.48763586146515259 	
+-1.21972683978988194 	-1.50185397433043755 	
+1.24858166506403889 	4.87092530126864176 	
+-7.04338199489057715 	17.3390887919734524 	
+3.47070361924793991 	1.42970456326488016 	
+-4.26550633389315159 	-1.73980331015090517 	
+2.12547785989944504 	3.62923159122205607 	
+2.5325993386476946 	-38.0016204477384605 	
+2.21881274683430751 	-0.0138947589494949966 	
+3.14760002032383079 	-1.50792558449675762 	
+1.08730538645968133 	-5.46742192799871596 	
+0.805042144582074792 	-53.0054722631666593 	
+-1.33448103050452405 	5.00293548641393393 	
+-4.27305110186876824 	-0.418543428368392034 	
+1.70132006444656469 	-5.38749080327059016 	
+-1.12532164909935517 	33.3451642986786609 	
+-2.70415771236861247 	3.89572987273113291 	
+4.98777580381695707 	13.8821103996001245 	
+0.568511986632529109 	-6.88339363195469822 	
+2.78082104553195331 	-5.19833751432510915 	
+-3.45195094389145929 	6.54989673235798353 	
+-1.26923932125293937 	-82.4386151296812386 	
+6.7221655120617827 	-5.26765661015447861 	
+-0.742712929107001374 	-2.30022073912255554 	
+1.55989131761590083 	-4.18345207234387129 	
+4.72158352753704147 	21.7128846359820926 	
+-2.74322426786354923 	0.0600870120432775226 	
+5.66665271018074534 	-3.34386936425682135 	
+-1.7405916077807202 	11.7068252411723854 	
+0.339841379787169384 	1.53151755701152847 	
+-0.136156736944080564 	-12.0671867463559686 	
+2.99636501987962234 	1.68582324362498048 	
+-1.96646003943842773 	-7.02906457262245166 	
+-2.68253702359453339 	19.2149219172491286 	
+3.77166285715723992 	-15.0634217849763132 	
+1.10118597277121655 	11.2002484600206493 	
+-0.21265096744765738 	0.824744814217108657 	
+3.73652792828045532 	-7.81218626521123305 	
+-0.1517753718816853 	1.00699915467459311 	
+3.72484877108446399 	5.92264185066637783 	
+-0.0755790292585322709 	-3.91733851844826209 	
+-1.69431689893366721 	3.02479371219842008 	
+0.0753423487892740662 	14.9935199826461982 	
+2.23109480398673998 	-2.59140850098083764 	
+6.91777882726973914 	41.5668021912243546 	
+1.87173910300826329 	-16.2617861471825336 	
+-2.61089444121170011 	-39.3944996228537363 	
 -0.836329838029520878 	0.975444302819992171 	
-3.00474481016258865 	-21.1150530969548456 	
--8.6246033027724156 	3.04533586233992315 	
--11.9994315456001406 	-1.47919451474942232 	
-4.44069944340081424 	-18.2589018941206973 	
--2.27968651835379399 	1.02704086194237587 	
-6.31981383646970141 	1.54445289434102828 	
--5.26600895531991675 	-35.797753315782785 	
--0.481694055895810824 	2.2119594092289705 	
-7.77036312147051333 	1.09313418809397089 	
--4.6809721398523001 	-6.20894287205482076 	
--1.09714156805876417 	-2.85536759175997856 	
-0.54570023840243842 	11.8862474186002061 	
-1.48867902407078145 	-5.48427538647323942 	
--6.15587643151411612 	6.74361516830442742 	
--0.349433002185481179 	-6.23282874401665588 	
--2.64624601969778839 	3.41977326768260959 	
--2.94286030596189985 	-2.7536407661812845 	
-5.73828756539099682 	-5.5731081762760617 	
--4.97225940963330615 	3.6550947464273591 	
--6.10148670149677752 	-3.5099263102824203 	
-1.55044181583256746 	14.6775034364667842 	
-0.782385337009624093 	25.4813713200397167 	
--1.53273115443669905 	4.58635057373372401 	
-10.4172385843634707 	6.8663333943362943 	
--0.368394943953074205 	20.6264920696709204 	
-5.60810932980211341 	6.78678373498030751 	
-6.71791590246602066 	-5.80482782600071445 	
-0.388121780023255425 	-3.46219858678926329 	
-2.83447313288851621 	7.76167610821283915 	
--1.54630702793932451 	0.277670233017959622 	
-8.98371476935886726 	-0.793959411588884501 	
-2.70994943537675281 	14.8040061565928838 	
-6.68573003918294617 	0.0462762095938224136 	
--4.29261320816485004 	-2.95101472907809592 	
--4.54151110825021309 	16.1599843952047344 	
--5.15501226513595601 	6.64257668637025311 	
-1.9073310659665581 	2.85019230852685856 	
-1.43237792951583942 	11.0408267483431235 	
--1.36458349374152954 	-1.30153240656581781 	
--4.30599353659678563 	-10.9529742342064758 	
-0.0316284965242700083 	1.25596215072053163 	
--2.60581041885561548 	-6.14526724575121097 	
--0.312851078676800676 	4.88991765871333239 	
-5.29462188951449519 	-16.5770775166692452 	
--4.00703215573595894 	2.75362305989386025 	
-8.72817290377258459 	9.97806304735738259 	
-0.312638773499881673 	-2.14323866156373199 	
--1.81776211067225546 	-44.0518064486709733 	
-2.55609366479082256 	-4.74886921353495595 	
--0.833149682701808447 	11.5472059416397173 	
--1.51305776067360043 	-109.480058674138334 	
-0.963485614629060239 	-6.81413026706003766 	
--8.58496213584483847 	13.0940886747311964 	
-0.0915420976403832215 	5.86702156899371197 	
-4.79191376947731396 	-6.83937697735480477 	
-0.534268386342521273 	-12.4849809673474379 	
-2.17337508306685523 	-14.4118021194800612 	
-0.140164109164259837 	62.16477814568497 	
-0.167570766918804098 	-24.6465822114029365 	
-1.23667489086611737 	1.84320815139914407 	
--0.440814885865148109 	-1.5540078086496576 	
-3.86906628870645619 	5.74016181018549343 	
--4.46332178439692751 	14.5109110420846736 	
-0.978359207200913406 	-0.176691538655378344 	
--0.253112600078602312 	11.6452773486085555 	
-9.90282705194929491 	7.59072425472290746 	
--1.48088213993077722 	-12.6227827040926677 	
--2.07209465738966392 	6.02898523253392682 	
-0.0851342612633791129 	65.918637305018251 	
--4.36632696181662183 	-11.3811871240997462 	
--1.71368855874337322 	1.31764702087338548 	
-2.15686598960616616 	18.4587976763140915 	
-0.511354889667731349 	-43.8065302488140631 	
-3.7998582616395109 	2.19302119575025722 	
-2.20001654545043479 	-4.52230341999792085 	
--3.10342048881173493 	1.69054390982317559 	
-2.49515664288340444 	-12.6407876685416056 	
--2.05596705726674811 	-15.4018265269667936 	
-2.45357401304169231 	-17.9766881878872269 	
-14.0087664049932883 	-2.14150296573378363 	
--0.458146105055885766 	-94.2866379405297295 	
--7.43642773069070628 	-0.417123011304259772 	
--0.401022533514366897 	12.800581209304859 	
--0.954443961086527848 	-18.250506249697839 	
--3.53127796312637265 	1.20073147588584428 	
--4.42051731555071115 	-59.221195467169288 	
-1.77815493365281663 	47.1410132671280238 	
--7.07469141008806623 	-2.82886733983377647 	
+3.00474481016317796 	-21.1150530969481025 	
+-8.62460330276778109 	3.04533586235480191 	
+-11.9994315455979592 	-1.47919451474482977 	
+4.44069944340338374 	-18.2589018941275931 	
+-2.27968651835906844 	1.02704086191163668 	
+6.31981383647014194 	1.54445289435077759 	
+-5.26600895531925595 	-35.7977533157710113 	
+-0.481694055896209672 	2.21195940922255518 	
+7.77036312147073716 	1.09313418809460505 	
+-4.68097213985389882 	-6.20894287206248841 	
+-1.0971415680584542 	-2.85536759176862898 	
+0.545700238407987648 	11.8862474186525926 	
+1.48867902407183705 	-5.48427538646079693 	
+-6.1558764315134562 	6.74361516831379859 	
+-0.34943300218334733 	-6.2328287440142951 	
+-2.64624601969912154 	3.41977326768898227 	
+-2.94286030596212944 	-2.75364076618103537 	
+5.73828756539082274 	-5.57310817627025834 	
+-4.97225940962958202 	3.65509474643111654 	
+-6.10148670149821548 	-3.5099263102813012 	
+1.55044181583261187 	14.6775034364680952 	
+0.782385337009910753 	25.481371320069055 	
+-1.53273115443696883 	4.58635057373261112 	
+10.4172385843581061 	6.86633339432157452 	
+-0.368394943949268971 	20.6264920696424987 	
+5.60810932982655341 	6.78678373501238852 	
+6.71791590246840631 	-5.80482782599815295 	
+0.388121780022244123 	-3.46219858678257397 	
+2.83447313289494707 	7.76167610817524611 	
+-1.54630702794617703 	0.277670233006282186 	
+8.9837147693589845 	-0.793959411589270636 	
+2.70994943537632693 	14.8040061566573282 	
+6.68573003919279518 	0.0462762095885054375 	
+-4.29261320816020042 	-2.95101472906158513 	
+-4.54151110825627313 	16.1599843952082907 	
+-5.15501226513589383 	6.64257668637242293 	
+1.90733106596579205 	2.8501923085260672 	
+1.43237792951475584 	11.0408267483489499 	
+-1.36458349374178489 	-1.30153240656658253 	
+-4.30599353659890038 	-10.9529742342083214 	
+0.0316284965238302559 	1.25596215070410944 	
+-2.60581041885303533 	-6.14526724575781902 	
+-0.312851078671457561 	4.8899176585817985 	
+5.29462188951335122 	-16.5770775166756401 	
+-4.00703215573606908 	2.75362305989316081 	
+8.7281729037734479 	9.97806304735961547 	
+0.312638773494846978 	-2.14323866166241261 	
+-1.81776211066742133 	-44.0518064486871594 	
+2.55609366479101618 	-4.74886921353842606 	
+-0.833149682703483663 	11.5472059416640835 	
+-1.51305776067499043 	-109.480058674132408 	
+0.96348561462821225 	-6.81413026705121183 	
+-8.58496213584385792 	13.0940886747191065 	
+0.0915420976503914519 	5.86702156901551231 	
+4.79191376944235348 	-6.83937697733123517 	
+0.534268386340944978 	-12.4849809673482426 	
+2.17337508306810578 	-14.4118021194967199 	
+0.140164109164443162 	62.1647781456503097 	
+0.167570766919056896 	-24.6465822115656401 	
+1.23667489086769566 	1.84320815143338623 	
+-0.440814885867638395 	-1.55400780863914356 	
+3.86906628870721292 	5.74016181018787464 	
+-4.46332178439648342 	14.5109110420769749 	
+0.978359207199548941 	-0.176691538654043939 	
+-0.253112600092994244 	11.6452773485220398 	
+9.90282705194954538 	7.59072425472390933 	
+-1.48088213993055762 	-12.6227827040919376 	
+-2.07209465738824505 	6.0289852325298714 	
+0.0851342612640778873 	65.9186373049947889 	
+-4.36632696181617952 	-11.3811871241025226 	
+-1.71368855874052861 	1.31764702088757302 	
+2.15686598960512166 	18.4587976762973902 	
+0.511354889660306289 	-43.8065302486330737 	
+3.79985826164594931 	2.1930211957444623 	
+2.2000165454424403 	-4.52230342001110674 	
+-3.10342048881031829 	1.69054390982302927 	
+2.49515664287580075 	-12.6407876685394616 	
+-2.05596705724929718 	-15.4018265269691028 	
+2.4535740130399093 	-17.9766881878857845 	
+14.0087664049954643 	-2.1415029657413216 	
+-0.458146105055854791 	-94.286637940529971 	
+-7.43642773069096918 	-0.417123011311428815 	
+-0.401022533507097045 	12.8005812092002085 	
+-0.954443961086675396 	-18.2505062496947659 	
+-3.53127796312802422 	1.20073147588600682 	
+-4.42051731554893923 	-59.221195467202314 	
+1.77815493364989341 	47.1410132671640412 	
+-7.07469141009799074 	-2.82886733985023353 	
 ]
 ;
 training_inputs = *2 ->MemoryVMatrix(

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/final_learner.psave	2008-04-24 20:18:13 UTC (rev 8890)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/final_learner.psave	2008-04-24 20:47:12 UTC (rev 8891)
@@ -261,231 +261,231 @@
 weight_decay = 0.0100000000000000002 ;
 include_bias = 1 ;
 params = 225  2  [ 
-2.7151351062236615 	6.96796926051022236 	
--0.924570620989403458 	-5.50999860198525226 	
-2.74766209737707889 	16.2764471019291541 	
--0.897331116702694054 	24.1696624839723277 	
--0.257145462010700432 	-2.68785159119131878 	
--1.3282966539723533 	-18.5932956256279311 	
--5.49258347507660183 	-1.57002604296742554 	
-1.46070534955196196 	-3.53276434234297421 	
--1.03812617167629262 	-1.20731159585204617 	
-1.02580172582147133 	-26.7245637329914061 	
-0.791254761045375288 	82.4469215737143202 	
-0.836570941010414737 	-16.420263045944921 	
--1.58240826855158301 	-6.65940808940268703 	
--8.24967860415081411 	-3.4995708964716874 	
-0.978241143861720674 	-4.92116475768050154 	
-0.0717117261694705244 	-3.09067303052573861 	
--2.58531178640539849 	12.0354584295284699 	
-3.21107644425249639 	5.97223155883275147 	
-1.43705644733271809 	94.2974631733482198 	
-0.651016019177080518 	1.95403761232638362 	
--1.20978628334706184 	-7.36323983158574702 	
--0.676733526068720059 	-31.4356962338422257 	
--3.71894302925004183 	-5.30412867323935266 	
--2.68184912068139347 	-0.224631918495068694 	
--1.88827683783783651 	-11.2808510609680628 	
-1.56552507696869569 	-5.27853909851396796 	
--2.51225078969575177 	-3.22314543004070897 	
--8.82457968581596752 	-1.42245769348021844 	
--5.4971248965487387 	-14.759995096029046 	
-8.47592427242844337 	8.11107085181370557 	
-2.34953305575392202 	4.63659562958691041 	
--1.64301101094937252 	11.4873476009575572 	
-0.580660833465362791 	13.1755650470871277 	
--0.650250885491696851 	1.90203710703855133 	
--3.87917543422287148 	5.03518774179589901 	
-1.7039737911280386 	1.34775554555850574 	
--2.78522303432125629 	-64.1292521131980919 	
-2.32700743878955185 	24.743612559642397 	
--5.39566293426395394 	-9.82739376173094037 	
-12.9203582472649128 	5.29957416698956862 	
--10.7455769141728563 	89.9090483177386801 	
-4.31713435895606157 	-5.18586285077431786 	
-0.848047232584521082 	-2.17948525293295869 	
--9.4681032062965258 	-0.22074895143857387 	
-7.21414655874796917 	7.46866878382939792 	
--1.75858145601429361 	11.2871954305960074 	
-1.96560792644405802 	-6.52458829385219907 	
--2.79380808107006473 	-5.09955504189952258 	
--0.799059310236704512 	-12.9255049414569196 	
--4.25437046158016585 	8.96615214816819339 	
--9.43622335236651288 	3.78339978190638559 	
-4.36697916816686238 	-0.687503879096243442 	
--2.18796130106503472 	-6.69802616230832459 	
-3.17982632559527989 	-1.43703160328240198 	
--4.3141453664152607 	-5.51667561668627471 	
--2.74519292766851297 	7.73168925236620286 	
-1.05378206408368125 	0.178110338223795794 	
-0.460217896787359693 	10.0585449039864159 	
--0.0106331952235446964 	-19.6790725953361481 	
-0.424465188958869921 	-15.8551532141411631 	
-0.567218573767627854 	-2.6951904195179508 	
-4.45718670597547018 	5.62393864872558336 	
-2.19961229165357031 	-5.42279390073859879 	
--0.228762156049838294 	-0.831542078930034179 	
--2.4010318622261817 	15.8373411775190593 	
--1.34006398346898026 	-12.0359127014414575 	
-4.0381021751345596 	-2.91524838149849552 	
-0.593248196395251171 	0.218503040318817232 	
+2.71513510622207166 	6.96796926050934751 	
+-0.924570620987045566 	-5.50999860198858826 	
+2.74766209737875133 	16.2764471019206418 	
+-0.897331116697557052 	24.1696624838867358 	
+-0.257145462011077963 	-2.68785159119476535 	
+-1.3282966539726655 	-18.5932956256296649 	
+-5.49258347509019718 	-1.57002604298704584 	
+1.46070534954897835 	-3.53276434233635506 	
+-1.038126171674709 	-1.2073115958693148 	
+1.02580172581497386 	-26.7245637328128929 	
+0.791254761045932176 	82.4469215737125865 	
+0.836570941006688051 	-16.4202630459276833 	
+-1.58240826854525074 	-6.65940808938515527 	
+-8.24967860415612364 	-3.49957089648365738 	
+0.9782411438628974 	-4.92116475768973771 	
+0.0717117261764370351 	-3.09067303050886988 	
+-2.58531178640483761 	12.035458429525173 	
+3.211076444250756 	5.97223155883393897 	
+1.43705644733268212 	94.2974631733484614 	
+0.651016019176285488 	1.95403761231488304 	
+-1.20978628334898786 	-7.36323983157156103 	
+-0.676733526068380775 	-31.4356962338337986 	
+-3.71894302925004316 	-5.30412867324711534 	
+-2.6818491206821955 	-0.224631918494603705 	
+-1.88827683784314537 	-11.2808510609389074 	
+1.56552507696732834 	-5.27853909851336311 	
+-2.51225078969590498 	-3.22314543003026799 	
+-8.82457968582814622 	-1.42245769348111262 	
+-5.49712489654685843 	-14.7599950960045039 	
+8.47592427242672741 	8.11107085181684795 	
+2.34953305575425997 	4.63659562959487381 	
+-1.64301101095052804 	11.4873476009648563 	
+0.580660833463284676 	13.1755650470951462 	
+-0.650250885492095976 	1.90203710703754014 	
+-3.87917543422272804 	5.03518774179621609 	
+1.70397379112897029 	1.34775554557823152 	
+-2.78522303432040363 	-64.1292521132013178 	
+2.32700743878965444 	24.7436125596276284 	
+-5.39566293426405519 	-9.82739376173207368 	
+12.9203582472648133 	5.29957416697557626 	
+-10.7455769141731885 	89.9090483177333653 	
+4.31713435896750308 	-5.18586285076809528 	
+0.848047232583675648 	-2.17948525292697681 	
+-9.46810320629667324 	-0.220748951447968828 	
+7.21414655874827737 	7.46866878382293553 	
+-1.75858145600769644 	11.2871954305640294 	
+1.96560792644487359 	-6.52458829385446837 	
+-2.79380808107613055 	-5.09955504191000042 	
+-0.79905931023683674 	-12.925504941414971 	
+-4.25437046157972976 	8.96615214816773154 	
+-9.43622335236653065 	3.78339978190649218 	
+4.36697916817812093 	-0.68750387907888455 	
+-2.18796130106569242 	-6.69802616230909642 	
+3.17982632559557965 	-1.4370316032819157 	
+-4.31414536641474289 	-5.51667561668654649 	
+-2.74519292766675793 	7.73168925235037285 	
+1.0537820640848472 	0.178110338215527242 	
+0.460217896787073477 	10.058544903984771 	
+-0.0106331952212118993 	-19.6790725953963772 	
+0.424465188958454087 	-15.8551532141373048 	
+0.567218573772457435 	-2.69519041953220029 	
+4.45718670597664524 	5.62393864871897797 	
+2.19961229164894245 	-5.42279390073130685 	
+-0.228762156047200543 	-0.831542078921280403 	
+-2.40103186222691534 	15.8373411775187218 	
+-1.34006398347233469 	-12.0359127014981127 	
+4.03810217513344138 	-2.91524838149503962 	
+0.593248196381594317 	0.218503040354157629 	
 -0.894620455881181775 	1.01068418956454376 	
--0.905934974062951648 	-1.04508844319938632 	
-8.00428356105546257 	-1.26168634488796494 	
-0.619982738884564899 	54.1218393093347103 	
--0.705573849334636627 	-1.2538137788610173 	
-0.871066182002223743 	40.4927882352060706 	
--7.05074671248599127 	71.3212740911236835 	
--2.22324163215964843 	1.0734561275325698 	
--2.44557668837193853 	4.2937313954156906 	
-9.82162368083046999 	6.64093872400913376 	
--9.18335934249402008 	-3.97745257891762716 	
--0.836507465302499753 	-1.0321141864197878 	
--0.0534975469631229703 	19.5696227138288812 	
-1.07568581177046618 	-5.10104965789005504 	
--0.518952740814859759 	15.8636310713528932 	
--2.84087926537195301 	-12.9191491848439277 	
-2.04826664350717103 	12.2408805584981177 	
-1.80375307384732975 	11.6859209915770084 	
--1.25621312028974019 	-2.02266034406559303 	
--0.0484853946913663472 	3.8049818168441556 	
--0.677242736030050585 	-42.625477534183851 	
-1.59162699009028663 	-5.48763586147435767 	
--1.21972683978848395 	-1.50185397433300194 	
-1.24858166506431711 	4.87092530127280199 	
--7.04338199489003269 	17.3390887919741523 	
-3.47070361924823967 	1.42970456326448381 	
--4.2655063338911301 	-1.73980331015853862 	
-2.12547785989874116 	3.62923159123009187 	
-2.53259933864800901 	-38.0016204477437114 	
-2.21881274683373908 	-0.0138947589423880184 	
-3.1476000203292771 	-1.50792558450150538 	
-1.08730538646035324 	-5.46742192799836069 	
-0.805042144582562846 	-53.0054722631629431 	
--1.33448103050428291 	5.00293548641483543 	
--4.27305110186990333 	-0.418543428366281389 	
-1.70132006444962425 	-5.38749080326675145 	
--1.12532164909613996 	33.3451642986689407 	
--2.70415771239980796 	3.8957298727264229 	
-4.98777580381757346 	13.8821103996191333 	
-0.568511986630580113 	-6.88339363194706877 	
-2.78082104553625609 	-5.19833751433356372 	
--3.45195094389070523 	6.54989673235655356 	
--1.26923932124764582 	-82.4386151297649405 	
-6.72216551206190616 	-5.26765661015598763 	
--0.742712929107524844 	-2.30022073912311376 	
-1.55989131761279332 	-4.18345207235786098 	
-4.72158352754091215 	21.7128846360060805 	
--2.74322426786452844 	0.0600870120549067896 	
-5.66665271018079864 	-3.34386936425720371 	
--1.7405916077822694 	11.7068252411821998 	
-0.339841379787524656 	1.53151755701120851 	
--0.136156736953116003 	-12.0671867463523572 	
-2.99636501990413606 	1.68582324363194558 	
--1.96646003944210279 	-7.02906457252052252 	
--2.68253702359536872 	19.2149219172472989 	
-3.77166285715782834 	-15.0634217849804468 	
-1.10118597276613972 	11.2002484600089556 	
--0.212650967437634342 	0.824744814209136368 	
-3.73652792828373403 	-7.81218626519529735 	
--0.151775371881687632 	1.00699915467459045 	
-3.72484877108447021 	5.92264185066602611 	
--0.0755790292600937996 	-3.91733851844509173 	
--1.69431689893300641 	3.02479371219719839 	
-0.0753423487888285753 	14.9935199826456476 	
-2.23109480398829207 	-2.59140850098041486 	
-6.91777882726934212 	41.5668021912209156 	
-1.87173910301644164 	-16.2617861474499854 	
--2.610894441212122 	-39.3944996228497075 	
+-0.905934974062951315 	-1.04508844319939009 	
+8.00428356105542704 	-1.26168634488815745 	
+0.619982738889116813 	54.1218393092184655 	
+-0.705573849337669534 	-1.25381377885319911 	
+0.871066182005948986 	40.4927882352012602 	
+-7.05074671248575058 	71.3212740911221061 	
+-2.22324163214598158 	1.07345612751800834 	
+-2.44557668836901909 	4.29373139551074523 	
+9.8216236808294628 	6.64093872401048912 	
+-9.18335934248439933 	-3.97745257894915438 	
+-0.836507465302499531 	-1.03211418641978869 	
+-0.053497546963581534 	19.5696227138299683 	
+1.07568581176873801 	-5.10104965788098763 	
+-0.51895274081205478 	15.8636310713846687 	
+-2.84087926537020463 	-12.9191491848348825 	
+2.04826664350002785 	12.2408805586060261 	
+1.80375307384701089 	11.6859209915609643 	
+-1.25621312029488119 	-2.02266034407121387 	
+-0.0484853946924818577 	3.80498181684614956 	
+-0.677242736032365844 	-42.6254775342041086 	
+1.5916269901049056 	-5.48763586146515259 	
+-1.21972683978988194 	-1.50185397433043755 	
+1.24858166506403889 	4.87092530126864176 	
+-7.04338199489057715 	17.3390887919734524 	
+3.47070361924793991 	1.42970456326488016 	
+-4.26550633389315159 	-1.73980331015090517 	
+2.12547785989944504 	3.62923159122205607 	
+2.5325993386476946 	-38.0016204477384605 	
+2.21881274683430751 	-0.0138947589494949966 	
+3.14760002032383079 	-1.50792558449675762 	
+1.08730538645968133 	-5.46742192799871596 	
+0.805042144582074792 	-53.0054722631666593 	
+-1.33448103050452405 	5.00293548641393393 	
+-4.27305110186876824 	-0.418543428368392034 	
+1.70132006444656469 	-5.38749080327059016 	
+-1.12532164909935517 	33.3451642986786609 	
+-2.70415771236861247 	3.89572987273113291 	
+4.98777580381695707 	13.8821103996001245 	
+0.568511986632529109 	-6.88339363195469822 	
+2.78082104553195331 	-5.19833751432510915 	
+-3.45195094389145929 	6.54989673235798353 	
+-1.26923932125293937 	-82.4386151296812386 	
+6.7221655120617827 	-5.26765661015447861 	
+-0.742712929107001374 	-2.30022073912255554 	
+1.55989131761590083 	-4.18345207234387129 	
+4.72158352753704147 	21.7128846359820926 	
+-2.74322426786354923 	0.0600870120432775226 	
+5.66665271018074534 	-3.34386936425682135 	
+-1.7405916077807202 	11.7068252411723854 	
+0.339841379787169384 	1.53151755701152847 	
+-0.136156736944080564 	-12.0671867463559686 	
+2.99636501987962234 	1.68582324362498048 	
+-1.96646003943842773 	-7.02906457262245166 	
+-2.68253702359453339 	19.2149219172491286 	
+3.77166285715723992 	-15.0634217849763132 	
+1.10118597277121655 	11.2002484600206493 	
+-0.21265096744765738 	0.824744814217108657 	
+3.73652792828045532 	-7.81218626521123305 	
+-0.1517753718816853 	1.00699915467459311 	
+3.72484877108446399 	5.92264185066637783 	
+-0.0755790292585322709 	-3.91733851844826209 	
+-1.69431689893366721 	3.02479371219842008 	
+0.0753423487892740662 	14.9935199826461982 	
+2.23109480398673998 	-2.59140850098083764 	
+6.91777882726973914 	41.5668021912243546 	
+1.87173910300826329 	-16.2617861471825336 	
+-2.61089444121170011 	-39.3944996228537363 	
 -0.836329838029520878 	0.975444302819992171 	
-3.00474481016258865 	-21.1150530969548456 	
--8.6246033027724156 	3.04533586233992315 	
--11.9994315456001406 	-1.47919451474942232 	
-4.44069944340081424 	-18.2589018941206973 	
--2.27968651835379399 	1.02704086194237587 	
-6.31981383646970141 	1.54445289434102828 	
--5.26600895531991675 	-35.797753315782785 	
--0.481694055895810824 	2.2119594092289705 	
-7.77036312147051333 	1.09313418809397089 	
--4.6809721398523001 	-6.20894287205482076 	
--1.09714156805876417 	-2.85536759175997856 	
-0.54570023840243842 	11.8862474186002061 	
-1.48867902407078145 	-5.48427538647323942 	
--6.15587643151411612 	6.74361516830442742 	
--0.349433002185481179 	-6.23282874401665588 	
--2.64624601969778839 	3.41977326768260959 	
--2.94286030596189985 	-2.7536407661812845 	
-5.73828756539099682 	-5.5731081762760617 	
--4.97225940963330615 	3.6550947464273591 	
--6.10148670149677752 	-3.5099263102824203 	
-1.55044181583256746 	14.6775034364667842 	
-0.782385337009624093 	25.4813713200397167 	
--1.53273115443669905 	4.58635057373372401 	
-10.4172385843634707 	6.8663333943362943 	
--0.368394943953074205 	20.6264920696709204 	
-5.60810932980211341 	6.78678373498030751 	
-6.71791590246602066 	-5.80482782600071445 	
-0.388121780023255425 	-3.46219858678926329 	
-2.83447313288851621 	7.76167610821283915 	
--1.54630702793932451 	0.277670233017959622 	
-8.98371476935886726 	-0.793959411588884501 	
-2.70994943537675281 	14.8040061565928838 	
-6.68573003918294617 	0.0462762095938224136 	
--4.29261320816485004 	-2.95101472907809592 	
--4.54151110825021309 	16.1599843952047344 	
--5.15501226513595601 	6.64257668637025311 	
-1.9073310659665581 	2.85019230852685856 	
-1.43237792951583942 	11.0408267483431235 	
--1.36458349374152954 	-1.30153240656581781 	
--4.30599353659678563 	-10.9529742342064758 	
-0.0316284965242700083 	1.25596215072053163 	
--2.60581041885561548 	-6.14526724575121097 	
--0.312851078676800676 	4.88991765871333239 	
-5.29462188951449519 	-16.5770775166692452 	
--4.00703215573595894 	2.75362305989386025 	
-8.72817290377258459 	9.97806304735738259 	
-0.312638773499881673 	-2.14323866156373199 	
--1.81776211067225546 	-44.0518064486709733 	
-2.55609366479082256 	-4.74886921353495595 	
--0.833149682701808447 	11.5472059416397173 	
--1.51305776067360043 	-109.480058674138334 	
-0.963485614629060239 	-6.81413026706003766 	
--8.58496213584483847 	13.0940886747311964 	
-0.0915420976403832215 	5.86702156899371197 	
-4.79191376947731396 	-6.83937697735480477 	
-0.534268386342521273 	-12.4849809673474379 	
-2.17337508306685523 	-14.4118021194800612 	
-0.140164109164259837 	62.16477814568497 	
-0.167570766918804098 	-24.6465822114029365 	
-1.23667489086611737 	1.84320815139914407 	
--0.440814885865148109 	-1.5540078086496576 	
-3.86906628870645619 	5.74016181018549343 	
--4.46332178439692751 	14.5109110420846736 	
-0.978359207200913406 	-0.176691538655378344 	
--0.253112600078602312 	11.6452773486085555 	
-9.90282705194929491 	7.59072425472290746 	
--1.48088213993077722 	-12.6227827040926677 	
--2.07209465738966392 	6.02898523253392682 	
-0.0851342612633791129 	65.918637305018251 	
--4.36632696181662183 	-11.3811871240997462 	
--1.71368855874337322 	1.31764702087338548 	
-2.15686598960616616 	18.4587976763140915 	
-0.511354889667731349 	-43.8065302488140631 	
-3.7998582616395109 	2.19302119575025722 	
-2.20001654545043479 	-4.52230341999792085 	
--3.10342048881173493 	1.69054390982317559 	
-2.49515664288340444 	-12.6407876685416056 	
--2.05596705726674811 	-15.4018265269667936 	
-2.45357401304169231 	-17.9766881878872269 	
-14.0087664049932883 	-2.14150296573378363 	
--0.458146105055885766 	-94.2866379405297295 	
--7.43642773069070628 	-0.417123011304259772 	
--0.401022533514366897 	12.800581209304859 	
--0.954443961086527848 	-18.250506249697839 	
--3.53127796312637265 	1.20073147588584428 	
--4.42051731555071115 	-59.221195467169288 	
-1.77815493365281663 	47.1410132671280238 	
--7.07469141008806623 	-2.82886733983377647 	
+3.00474481016317796 	-21.1150530969481025 	
+-8.62460330276778109 	3.04533586235480191 	
+-11.9994315455979592 	-1.47919451474482977 	
+4.44069944340338374 	-18.2589018941275931 	
+-2.27968651835906844 	1.02704086191163668 	
+6.31981383647014194 	1.54445289435077759 	
+-5.26600895531925595 	-35.7977533157710113 	
+-0.481694055896209672 	2.21195940922255518 	
+7.77036312147073716 	1.09313418809460505 	
+-4.68097213985389882 	-6.20894287206248841 	
+-1.0971415680584542 	-2.85536759176862898 	
+0.545700238407987648 	11.8862474186525926 	
+1.48867902407183705 	-5.48427538646079693 	
+-6.1558764315134562 	6.74361516831379859 	
+-0.34943300218334733 	-6.2328287440142951 	
+-2.64624601969912154 	3.41977326768898227 	
+-2.94286030596212944 	-2.75364076618103537 	
+5.73828756539082274 	-5.57310817627025834 	
+-4.97225940962958202 	3.65509474643111654 	
+-6.10148670149821548 	-3.5099263102813012 	
+1.55044181583261187 	14.6775034364680952 	
+0.782385337009910753 	25.481371320069055 	
+-1.53273115443696883 	4.58635057373261112 	
+10.4172385843581061 	6.86633339432157452 	
+-0.368394943949268971 	20.6264920696424987 	
+5.60810932982655341 	6.78678373501238852 	
+6.71791590246840631 	-5.80482782599815295 	
+0.388121780022244123 	-3.46219858678257397 	
+2.83447313289494707 	7.76167610817524611 	
+-1.54630702794617703 	0.277670233006282186 	
+8.9837147693589845 	-0.793959411589270636 	
+2.70994943537632693 	14.8040061566573282 	
+6.68573003919279518 	0.0462762095885054375 	
+-4.29261320816020042 	-2.95101472906158513 	
+-4.54151110825627313 	16.1599843952082907 	
+-5.15501226513589383 	6.64257668637242293 	
+1.90733106596579205 	2.8501923085260672 	
+1.43237792951475584 	11.0408267483489499 	
+-1.36458349374178489 	-1.30153240656658253 	
+-4.30599353659890038 	-10.9529742342083214 	
+0.0316284965238302559 	1.25596215070410944 	
+-2.60581041885303533 	-6.14526724575781902 	
+-0.312851078671457561 	4.8899176585817985 	
+5.29462188951335122 	-16.5770775166756401 	
+-4.00703215573606908 	2.75362305989316081 	
+8.7281729037734479 	9.97806304735961547 	
+0.312638773494846978 	-2.14323866166241261 	
+-1.81776211066742133 	-44.0518064486871594 	
+2.55609366479101618 	-4.74886921353842606 	
+-0.833149682703483663 	11.5472059416640835 	
+-1.51305776067499043 	-109.480058674132408 	
+0.96348561462821225 	-6.81413026705121183 	
+-8.58496213584385792 	13.0940886747191065 	
+0.0915420976503914519 	5.86702156901551231 	
+4.79191376944235348 	-6.83937697733123517 	
+0.534268386340944978 	-12.4849809673482426 	
+2.17337508306810578 	-14.4118021194967199 	
+0.140164109164443162 	62.1647781456503097 	
+0.167570766919056896 	-24.6465822115656401 	
+1.23667489086769566 	1.84320815143338623 	
+-0.440814885867638395 	-1.55400780863914356 	
+3.86906628870721292 	5.74016181018787464 	
+-4.46332178439648342 	14.5109110420769749 	
+0.978359207199548941 	-0.176691538654043939 	
+-0.253112600092994244 	11.6452773485220398 	
+9.90282705194954538 	7.59072425472390933 	
+-1.48088213993055762 	-12.6227827040919376 	
+-2.07209465738824505 	6.0289852325298714 	
+0.0851342612640778873 	65.9186373049947889 	
+-4.36632696181617952 	-11.3811871241025226 	
+-1.71368855874052861 	1.31764702088757302 	
+2.15686598960512166 	18.4587976762973902 	
+0.511354889660306289 	-43.8065302486330737 	
+3.79985826164594931 	2.1930211957444623 	
+2.2000165454424403 	-4.52230342001110674 	
+-3.10342048881031829 	1.69054390982302927 	
+2.49515664287580075 	-12.6407876685394616 	
+-2.05596705724929718 	-15.4018265269691028 	
+2.4535740130399093 	-17.9766881878857845 	
+14.0087664049954643 	-2.1415029657413216 	
+-0.458146105055854791 	-94.286637940529971 	
+-7.43642773069096918 	-0.417123011311428815 	
+-0.401022533507097045 	12.8005812092002085 	
+-0.954443961086675396 	-18.2505062496947659 	
+-3.53127796312802422 	1.20073147588600682 	
+-4.42051731554893923 	-59.221195467202314 	
+1.77815493364989341 	47.1410132671640412 	
+-7.07469141009799074 	-2.82886733985023353 	
 ]
 ;
 training_inputs = *6 ->MemoryVMatrix(
@@ -782,23 +782,33 @@
 ;
 rerun_after_sub = 0 ;
 provide_sub_expdir = 1 ;
+save_best_learner = 0 ;
 splitter = *9 ->KFoldSplitter(
 K = 10 ;
 append_train = 0 ;
 append_non_constant_test = 0 ;
 include_test_in_train = 0 ;
 cross_range = (0 , 1 ) )
- )
+;
+best_objective = 0.2204549456115516 ;
+best_results = 2 [ 0.2204549456115516 0.0496004907388373079 ] ;
+best_learner = *4  ;
+trialnum = 25 ;
+option_vals = []
+;
+verbosity = 0  )
 *10 ->HyperRetrain(
 splitter = *11 ->FractionSplitter(
 round_to_closest = 0 ;
 splits = 1  1  [ 
 (0 , 1 )	
 ]
- )
 ;
+one_is_absolute = 0  )
+;
 provide_tester_expdir = 0 ;
-call_forget = 1  )
+call_forget = 1 ;
+verbosity = 0  )
 ] ;
 provide_strategy_expdir = 1 ;
 save_final_learner = 1 ;

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_costs.pmat
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_costs.pmat	2008-04-24 20:18:13 UTC (rev 8890)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_costs.pmat	2008-04-24 20:47:12 UTC (rev 8891)
@@ -1,3 +1,3 @@
 MATRIX 75 1 DOUBLE LITTLE_ENDIAN                               
-?hXBK???6?$,?l??a?????5m? ???=0??b???o?y[????????i??x?>V?^??H???????E%v??XK<8dl?~`???o?I?e?????vS???y???'???/4?x???????x???^G????T??\j???y?l?)	2?V?b?kb???u?@??^x???(R?T???a?H???|?]?\w?2r???pP?jZ,)U[?????y:????q???O??<???uQ?\?m???????v??????V?5q?{?Ja???t?3?v?c?T???^&z?z???4A????=*u?up?=H??P?????A?????*?E??1??D3g??mn??rWm?Oa?2??@?Q????G???x???K????Z?7????X??????
-??????????o:???H??[???e???=??:8??????~m;???????W??????b????%/???w?81? r????~cP?M*?????J??!e???~4C?????%??????
\ No newline at end of file
+_hXBK?????$,?l??ia?????5m? ??)=0??b???o?y[??????i???x?>V???H???????E%v????<8dl??h???o??xe?????vS???????'???f6?x??????x????????T??-j???y?z?)	2?V??
+?kb????A??^x???(R?T??oi?H??????]?\w??????pP???,)U[??)??y:???0????O?\???uQ?~?m?????:?v????????5q?{????t??pv?c?T?D?^&z?z?R?3A????y*u?up?P??P???G?A??????E??1????g??mn??tWm?Oa?(??@?Q?????G???z???K??g?Z?7???La??????[y*(Xr?A/y?IB???<?rmx??e?x'?????u9K????B{????5r??x???$i?????????????|??^s??cx?g??6y???D???????????o:??H??[???????=??f+??????Ym;???????W?????b????%/???J91? r????~cP???*?????*??!e????C???????????
\ No newline at end of file

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_outputs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_stats.psave	2008-04-24 20:18:13 UTC (rev 8890)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_stats.psave	2008-04-24 20:47:12 UTC (rev 8891)
@@ -14,16 +14,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 75 ;
 sumsquarew_ = 75 ;
-sum_ = 8.32876645342360256 ;
-sumsquare_ = 7.8668222754001027 ;
-sumcube_ = 10.95481426570392 ;
-sumfourth_ = 17.4914867139880847 ;
-min_ = 0.0004033866614889374 ;
-max_ = 1.79622562497000371 ;
+sum_ = 8.32876645342397737 ;
+sumsquare_ = 7.86682227539818513 ;
+sumcube_ = 10.9548142656995928 ;
+sumfourth_ = 17.491486713979409 ;
+min_ = 0.000403386661495011372 ;
+max_ = 1.79622562496960114 ;
 agmemin_ = 13 ;
 agemax_ = 8 ;
-first_ = 0.0133271392087509453 ;
-last_ = 0.0703378652394524712 ;
+first_ = 0.013327139208753001 ;
+last_ = 0.0703378652397716186 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_outputs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_stats.psave	2008-04-24 20:18:13 UTC (rev 8890)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_stats.psave	2008-04-24 20:47:12 UTC (rev 8891)
@@ -14,16 +14,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 300 ;
 sumsquarew_ = 300 ;
-sum_ = 19.3910908077380881 ;
-sumsquare_ = 13.9110920621991241 ;
-sumcube_ = 15.8389851220151474 ;
-sumfourth_ = 22.1170742798119626 ;
-min_ = 3.99687001152846492e-05 ;
-max_ = 1.79622562497000371 ;
+sum_ = 19.3910908077334234 ;
+sumsquare_ = 13.9110920621933758 ;
+sumcube_ = 15.8389851220074238 ;
+sumfourth_ = 22.117074279799926 ;
+min_ = 3.99687001153368061e-05 ;
+max_ = 1.79622562496960114 ;
 agmemin_ = 232 ;
 agemax_ = 8 ;
-first_ = 0.00559245542604559776 ;
-last_ = 0.0703378652394524712 ;
+first_ = 0.00559245542604428977 ;
+last_ = 0.0703378652397716186 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/train_stats.psave	2008-04-24 20:18:13 UTC (rev 8890)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/train_stats.psave	2008-04-24 20:47:12 UTC (rev 8891)
@@ -39,12 +39,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 0.0521801135176526432 ;
-max_ = 0.0521801135176526432 ;
+min_ = 0.0521801135176278436 ;
+max_ = 0.0521801135176278436 ;
 agmemin_ = 0 ;
 agemax_ = 0 ;
-first_ = 0.0521801135176526432 ;
-last_ = 0.0521801135176526432 ;
+first_ = 0.0521801135176278436 ;
+last_ = 0.0521801135176278436 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/metainfos.txt	2008-04-24 20:18:13 UTC (rev 8890)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/metainfos.txt	2008-04-24 20:47:12 UTC (rev 8891)
@@ -1,4 +1,4 @@
-__REVISION__ = "PL8398"
+__REVISION__ = "PL8883"
 DataOpt.data                                  = PLEARNDIR:examples/data/test_suite/sin_signcos_1x_2y.amat
 HyperKRR.kfold                                = 10
 HyperKRR.lambda_list                          = 1e-8,1e-6,1e-4,1e-2,1e0

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/tester.psave	2008-04-24 20:18:13 UTC (rev 8890)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/tester.psave	2008-04-24 20:47:12 UTC (rev 8891)
@@ -17,8 +17,9 @@
 splits = 1  3  [ 
 (0 , 0.75 )	(0.75 , 1 )	(0 , 1 )	
 ]
- )
 ;
+one_is_absolute = 0  )
+;
 statnames = 2 [ "E[test1.E[mse]]" "E[test2.E[mse]]" ] ;
 statmask = []
 ;
@@ -99,23 +100,34 @@
 ;
 rerun_after_sub = 0 ;
 provide_sub_expdir = 1 ;
+save_best_learner = 0 ;
 splitter = *9 ->KFoldSplitter(
 K = 10 ;
 append_train = 0 ;
 append_non_constant_test = 0 ;
 include_test_in_train = 0 ;
 cross_range = (0 , 1 ) )
- )
+;
+best_objective = 1.79769313486231571e+308 ;
+best_results = []
+;
+best_learner = *0 ;
+trialnum = 0 ;
+option_vals = []
+;
+verbosity = 0  )
 *10 ->HyperRetrain(
 splitter = *11 ->FractionSplitter(
 round_to_closest = 0 ;
 splits = 1  1  [ 
 (0 , 1 )	
 ]
- )
 ;
+one_is_absolute = 0  )
+;
 provide_tester_expdir = 0 ;
-call_forget = 1  )
+call_forget = 1 ;
+verbosity = 0  )
 ] ;
 provide_strategy_expdir = 1 ;
 save_final_learner = 1 ;



From nouiz at mail.berlios.de  Thu Apr 24 23:27:56 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 24 Apr 2008 23:27:56 +0200
Subject: [Plearn-commits] r8892 - trunk/python_modules/plearn/pymake
Message-ID: <200804242127.m3OLRuRK023150@sheep.berlios.de>

Author: nouiz
Date: 2008-04-24 23:27:56 +0200 (Thu, 24 Apr 2008)
New Revision: 8892

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
Parse the PYMAKE_OPTION environment parameter. It's value is prepended to the command line arguments


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-04-24 20:47:12 UTC (rev 8891)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-04-24 21:27:56 UTC (rev 8892)
@@ -174,6 +174,11 @@
 a corresponding .<target>.override_compile_options file:
 { 'opt': '-Wall -g',
   'opt_boundcheck': '-Wall -g -DBOUNDCHECK'  }
+
+
+The environnement variable PYMAKE_OPTION is prepended to the command line 
+option. So you can define your default option their is they won't conflict
+with the one you add on the command line.
 """
 
 
@@ -2518,24 +2523,30 @@
     i=0
     linkname = ''
     link_target_override = None
-    while i < len(args):
-        if args[i] == '-o':
-            linkname = args[i+1]
-            i = i + 1
 
-        elif args[i] == '-link-target':
-            link_target_override = args[i+1]
+    env_options=os.getenv('PYMAKE_OPTION')
+    option_to_parse=[]
+    if env_options:
+        option_to_parse=env_options.split()
+    option_to_parse+=args
+    
+    while i < len(option_to_parse):
+        if option_to_parse[i] == '-o':
+            linkname = option_to_parse[i+1]
             i = i + 1
+        elif option_to_parse[i] == '-link-target':
+            link_target_override = option_to_parse[i+1]
+            i = i + 1
 
-        elif args[i][0]=='-':
-            optionargs.append(args[i][1:])
+        elif option_to_parse[i][0]=='-':
+            optionargs.append(option_to_parse[i][1:])
 
         else:
             otherargs.append(args[i])
         i = i + 1
     del i # We don't need it anymore and it might be confusing
+    
 
-
     ####  Checking optionargs to know which task to perform
 
     ##  Options specifying the type of compiled file to produce



From nouiz at mail.berlios.de  Thu Apr 24 23:29:55 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 24 Apr 2008 23:29:55 +0200
Subject: [Plearn-commits] r8893 - trunk/python_modules/plearn/pymake
Message-ID: <200804242129.m3OLTtpT023269@sheep.berlios.de>

Author: nouiz
Date: 2008-04-24 23:29:55 +0200 (Thu, 24 Apr 2008)
New Revision: 8893

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
bugfix on last commit


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-04-24 21:27:56 UTC (rev 8892)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-04-24 21:29:55 UTC (rev 8893)
@@ -2542,7 +2542,7 @@
             optionargs.append(option_to_parse[i][1:])
 
         else:
-            otherargs.append(args[i])
+            otherargs.append(option_to_parse[i])
         i = i + 1
     del i # We don't need it anymore and it might be confusing
     



From nouiz at mail.berlios.de  Fri Apr 25 16:55:41 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 25 Apr 2008 16:55:41 +0200
Subject: [Plearn-commits] r8894 - trunk/plearn_learners/generic
Message-ID: <200804251455.m3PEtf0S005830@sheep.berlios.de>

Author: nouiz
Date: 2008-04-25 16:55:40 +0200 (Fri, 25 Apr 2008)
New Revision: 8894

Modified:
   trunk/plearn_learners/generic/AddCostToLearner.cc
Log:
correctly implemented the train_time attribute to be able to save it then reload it.


Modified: trunk/plearn_learners/generic/AddCostToLearner.cc
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.cc	2008-04-24 21:29:55 UTC (rev 8893)
+++ trunk/plearn_learners/generic/AddCostToLearner.cc	2008-04-25 14:55:40 UTC (rev 8894)
@@ -84,6 +84,7 @@
 //////////////////////
 AddCostToLearner::AddCostToLearner()
     : bag_size(0),
+      train_time(0),
       check_output_consistency(1),
       combine_bag_outputs_method(1),
       compute_costs_on_bags(0),
@@ -184,6 +185,10 @@
                   OptionBase::buildoption,
         "If true, then during training we find the best threshold between\n"
         "classes.");
+
+    declareOption(ol, "train_time",
+                  &AddCostToLearner::train_time, OptionBase::buildoption,
+                  "The time it took to train in second.");
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }



From nouiz at mail.berlios.de  Fri Apr 25 17:16:43 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 25 Apr 2008 17:16:43 +0200
Subject: [Plearn-commits] r8895 - trunk/plearn_learners/regressors
Message-ID: <200804251516.m3PFGhcG008796@sheep.berlios.de>

Author: nouiz
Date: 2008-04-25 17:16:42 +0200 (Fri, 25 Apr 2008)
New Revision: 8895

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
allow RegressionTreeRegister to be saved and reloded correctly


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2008-04-25 14:55:40 UTC (rev 8894)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2008-04-25 15:16:42 UTC (rev 8895)
@@ -73,12 +73,13 @@
                   "The indicator to report progress through a progress bar\n");
     declareOption(ol, "verbosity", &RegressionTreeRegisters::verbosity, OptionBase::buildoption,
                   "The desired level of verbosity\n");
-    declareOption(ol, "tsource", &RegressionTreeRegisters::tsource, OptionBase::learntoption,
+    declareOption(ol, "tsource", &RegressionTreeRegisters::tsource,
+                  OptionBase::learntoption | OptionBase::nosave,
                   "The source VMatrix transposed");
 
     declareOption(ol, "source", &RegressionTreeRegisters::source,
-                  OptionBase::buildoption|OptionBase::nosave,
-                  "DEPRECATED The source VMatrix");
+                  OptionBase::buildoption,
+                  "The source VMatrix");
 
     declareOption(ol, "next_id", &RegressionTreeRegisters::next_id, OptionBase::learntoption,
                   "The next id for creating a new leave\n");
@@ -98,6 +99,8 @@
     deepCopyField(tsorted_row, copies);
     deepCopyField(leave_register, copies);
     deepCopyField(getExample_tmp, copies);
+//    deepCopyField(tsource,copies);
+//    deepCopyField(source,copies);
 }
 
 void RegressionTreeRegisters::build()
@@ -114,6 +117,7 @@
 
 void RegressionTreeRegisters::initRegisters(VMat the_train_set)
 {   
+    source = the_train_set;
     VMat tmp = VMat(new TransposeVMatrix(the_train_set));
     PP<MemoryVMatrixNoSave> tmp2 = new MemoryVMatrixNoSave(tmp);
     tsource = VMat(tmp2 );



From nouiz at mail.berlios.de  Fri Apr 25 19:58:30 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 25 Apr 2008 19:58:30 +0200
Subject: [Plearn-commits] r8896 - trunk/plearn_learners/hyper
Message-ID: <200804251758.m3PHwUcP016386@sheep.berlios.de>

Author: nouiz
Date: 2008-04-25 19:58:29 +0200 (Fri, 25 Apr 2008)
New Revision: 8896

Modified:
   trunk/plearn_learners/hyper/ExplicitListOracle.cc
Log:
modif to save it correctly


Modified: trunk/plearn_learners/hyper/ExplicitListOracle.cc
===================================================================
--- trunk/plearn_learners/hyper/ExplicitListOracle.cc	2008-04-25 15:16:42 UTC (rev 8895)
+++ trunk/plearn_learners/hyper/ExplicitListOracle.cc	2008-04-25 17:58:29 UTC (rev 8896)
@@ -46,7 +46,8 @@
 using namespace std;
 
 ExplicitListOracle::ExplicitListOracle()
-    :OptionsOracle()
+    :OptionsOracle(),
+     nreturned(0)
 /* ### Initialise all fields to their default value */
 {
     // ...
@@ -65,6 +66,10 @@
     declareOption(ol, "option_values", &ExplicitListOracle::option_values, OptionBase::buildoption,
                   "A matrix with as many columns as there are options, giving their values");
 
+    declareOption(ol, "nreturned", &ExplicitListOracle::nreturned,
+                  OptionBase::learntoption,
+                  "The number of returned option");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }



From nouiz at mail.berlios.de  Fri Apr 25 20:18:24 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 25 Apr 2008 20:18:24 +0200
Subject: [Plearn-commits] r8897 - trunk/plearn_learners/hyper
Message-ID: <200804251818.m3PIIOh9018421@sheep.berlios.de>

Author: nouiz
Date: 2008-04-25 20:18:23 +0200 (Fri, 25 Apr 2008)
New Revision: 8897

Modified:
   trunk/plearn_learners/hyper/CartesianProductOracle.cc
   trunk/plearn_learners/hyper/OracleObjectGenerator.cc
   trunk/plearn_learners/hyper/OracleObjectGenerator.h
Log:
modif to allow reloading of saved object


Modified: trunk/plearn_learners/hyper/CartesianProductOracle.cc
===================================================================
--- trunk/plearn_learners/hyper/CartesianProductOracle.cc	2008-04-25 17:58:29 UTC (rev 8896)
+++ trunk/plearn_learners/hyper/CartesianProductOracle.cc	2008-04-25 18:18:23 UTC (rev 8897)
@@ -68,6 +68,10 @@
                   "options in the option_names field. Each sub-list contains the values to be tried"
                   "for the corresponding option."
         );
+    declareOption(ol, "option_values_indices",
+                  &CartesianProductOracle::option_values_indices,
+                  OptionBase::learntoption,
+                  "The indices of each option value.");
 
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);

Modified: trunk/plearn_learners/hyper/OracleObjectGenerator.cc
===================================================================
--- trunk/plearn_learners/hyper/OracleObjectGenerator.cc	2008-04-25 17:58:29 UTC (rev 8896)
+++ trunk/plearn_learners/hyper/OracleObjectGenerator.cc	2008-04-25 18:18:23 UTC (rev 8897)
@@ -54,7 +54,6 @@
 {
     if (oracle.isNull())
         PLERROR("An OracleObjectGenerator MUST contain an oracle (an OptionsOracle).");
-    //PLERROR("An OracleObjectGenerator MUST contain an oracle (a CartesianProductOracle).");
 
     oracle->build();
     last_params.resize(0);
@@ -77,8 +76,11 @@
 {
     declareOption(ol, "oracle", &OracleObjectGenerator::oracle,
                   OptionBase::buildoption, "The OptionsOracle used to generate the new Object parameters. \n");
-    //OptionBase::buildoption, "The CartesianProductOracle used to generate the new Object parameters. \n");
 
+    declareOption(ol,"last_params", &OracleObjectGenerator::last_params,
+                  OptionBase::learntoption,
+                  "The last parameter returned by the oracle. \n");
+
     inherited::declareOptions(ol);
 }
 

Modified: trunk/plearn_learners/hyper/OracleObjectGenerator.h
===================================================================
--- trunk/plearn_learners/hyper/OracleObjectGenerator.h	2008-04-25 17:58:29 UTC (rev 8896)
+++ trunk/plearn_learners/hyper/OracleObjectGenerator.h	2008-04-25 18:18:23 UTC (rev 8897)
@@ -62,7 +62,6 @@
 
     //! The template Object from which we will generate other Objects
     PP<OptionsOracle> oracle;
-    //PP<CartesianProductOracle> oracle;
 
 
     // ******************



From louradou at mail.berlios.de  Fri Apr 25 22:16:34 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Fri, 25 Apr 2008 22:16:34 +0200
Subject: [Plearn-commits] r8898 - trunk/python_modules/plearn/learners
Message-ID: <200804252016.m3PKGYVm029283@sheep.berlios.de>

Author: louradou
Date: 2008-04-25 22:16:34 +0200 (Fri, 25 Apr 2008)
New Revision: 8898

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:
* added options min_cost and max_cost to possibly
  end the hyper-optimization prematurely given the
  measured costs.
* a little clean up of the code.



Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2008-04-25 18:18:23 UTC (rev 8897)
+++ trunk/python_modules/plearn/learners/SVM.py	2008-04-25 20:16:34 UTC (rev 8898)
@@ -620,21 +620,30 @@
                              (valid_samples = None), were the model is re-trained
                              in any case.
 
+        'test_on_train': <bool> Should we test best models on the train set.
+
         'retrain_until_local_optimum_is_found': <bool> when calling run(), whether or
                          not to continue to tune hyperparameters until a local optimum
                          is found. If False, you can re-run run() several times. If True,
                          you can also re-run to possibly find better performance.
 
-        'max_ntrials': <int> when calling run(), maximum number of hyperparameters to try
-                      since the last forget().
-
-        'test_on_train': <bool> Should we test best models on {test, train} (1)
-
         'testlevel': <int> Frequency of test:
                      - 0: write results only at the end of the run()
                      - 1: write results each time a better validation cost is found in run()
                      - 2: write all intermediate results
 
+        'max_ntrials': <int> when calling run(), maximum number of hyperparameters to try
+                      since the last forget(). If set to 1, then no hyperoptimization will be performed
+                      (first arbitrary hyperaparameters will be chosen, this is recommended for debugging only)
+                      
+        'min_cost': <float> minimum sufficient cost value. If this value is reached then the hyperoptimization
+                    will stop immediately. Recommended for speed-up when the optimization is easy.
+                    
+        'max_cost': <float> maximum admissible cost value. If this value is not reached during the first set
+                    of hyperoptimization (e.g. 9 trials with a RBF kernel), then the hyperoptimization will stop.
+                    Recommended when you test SVM with different front-end and you suspect some front-ends to be
+                    simply bad ones (you do not want to waste time with them).
+
         'results_filename': <string> Path to an output file for results
         
         'preproc_optionnames': <string> or <list of strings> indicating the names of the
@@ -700,6 +709,8 @@
                         'outputs_type',
                         'retrain_until_local_optimum_is_found',
                         'max_ntrials',
+                        'min_cost',
+                        'max_cost',
                         'retrain_on_valid',
                         'test_on_train',
                         'testlevel',
@@ -778,8 +789,11 @@
 
         self.retrain_on_valid = True
         self.retrain_until_local_optimum_is_found = True
+
+        self.test_on_train = False
         self.max_ntrials = 50
-        self.test_on_train = False
+        self.min_cost = None
+        self.max_cost = None
         
         self.verbosity = 0
         self.testlevel = 1
@@ -823,12 +837,6 @@
             return None
         return dataspec[ self.testset_key ]
 
-    def additional_preproc(self, input_vmat, isTrain=False):
-        if self.balance_classes and isTrain:
-            return ReplicateSamplesVMatrix(source = input_vmat,
-                                           operate_on_bags = (input_vmat.targetsize > 1 ))
-        return input_vmat
-
     ## specific to libsvm
     """ Return samples and targets in the format required
         by libsvm, i.e. lists of float.
@@ -855,7 +863,9 @@
         return samples, targets
 
     def get_svminputlist(self, input_vmat, isTrain=False):
-        input_vmat = self.additional_preproc( input_vmat, isTrain )
+        if self.balance_classes and isTrain:
+            input_vmat = ReplicateSamplesVMatrix(source = input_vmat,
+                                                 operate_on_bags = (input_vmat.targetsize > 1 ) )
         samples, targets = self.get_datalist( input_vmat )
         if self.normalize_inputs:
             if self.input_means == None:
@@ -906,8 +916,8 @@
         self.class_priors = class_priors
 
         if self.verbosity > 0:
-            print "  ( class priors: %s  -- %d samples)" % \
-                     ( class_priors, len(targets) )
+            print "  ( class priors: %s  -- %d samples of dim %d )" % \
+                     ( class_priors, len(targets), len(samples[0]) )
 
         if self.balanceC:
             weight = [ 1./p for p in class_priors.values() ]
@@ -1147,15 +1157,18 @@
              ):
         if self.verbosity > 3:
             print "SVM::train() called ", dataspec.keys()
-        if self.verbosity > 1:
-            print "launching libsvm with param %s " % param    
         
         if param == None:
             if not self.best_param:
+                if self.verbosity > 0:
+                    print "WARNING: train() called without hyper-parameters "+\
+                          "and no best hyperparameters found => run() called"
                 return self.run(dataspec)
             param = self.best_param.copy()
         elif 'kernel_type' not in param:
             param['kernel_type'] = self.kernel_type
+        if self.verbosity > 1:
+            print "launching libsvm with param %s " % param    
         
         trainset = self.train_inputspec(dataspec)
         self.get_data_stats( trainset )
@@ -1454,17 +1467,16 @@
         validset = self.valid_inputspec(dataspec)
         testset  = self.test_inputspec(dataspec)
         # Cross Validation
-        if 'fold' in self.validtype:
+        if self.validset_key not in dataspec:
+            if self.verbosity > 0:
+                print "\n** training model on entire train"
             self.train( dataspec )
 
         # Simple Validation
         else:
-            self.validtype = 'simple'
-            # CAUTION: in the case of simple validation without retraining on {train+valid},
-            #          self.best_model is supposed to be updated
             if self.retrain_on_valid:
                 """ Uncomment following lines if you want to check that
-                    retraining on {train + valid} sets does not degrade.
+                    retraining on {train + valid} sets does not degrade test performance.
                 """
                 #train_stats = None
                 #test_stats = None
@@ -1490,6 +1502,12 @@
                     print "\n** re-training model on { train + valid } "
                 self.train( {self.trainset_key: tv_set} )
 
+            # CAUTION: in the case of simple validation without retraining on {train+valid},
+            #          self.best_model is supposed to be up-to-date or None
+            elif self.best_model == None:
+                self.validtype = 'simple'
+                self.train( dataspec )
+
         train_stats = None
         test_stats = None
         if self.test_on_train:
@@ -1504,8 +1522,8 @@
                             None, test_stats, train_stats )
         self.write_results( self.best_param,
                             self.valid_stats, self.test_stats, self.train_stats )
+        return dataspec
 
-
     """ THE interesting function of the class.
         See __main__ below for usage.
         dataspec is a dictionary which specifies train, valid, test sets.
@@ -1513,6 +1531,8 @@
         cf. train_inputspec(), valid_inputspec(), and test_inputspec().
     """
     def run(self, dataspec):
+        assert self.testlevel >= 0
+        assert self.max_ntrials > 0
         trainset = self.train_inputspec(dataspec)
         validset = self.valid_inputspec(dataspec)
         testset  = self.test_inputspec(dataspec)
@@ -1531,8 +1551,14 @@
         else:
             param_to_try = expert.choose_new_param()
         
+        local_retrain_until_local_optimum_is_found = True
         for param in param_to_try:
 
+            if self.max_ntrials == 1: # No hyper-optimization (debug)
+                self.best_param = param
+                self.validset = None
+                return self.retrain_and_writeresults(dataspec)
+            
             valid_stats = self.valid(dataspec, param)
 
             # No improvement measured
@@ -1544,9 +1570,8 @@
 
             # Better valid cost is obtained!
             else:
-                print "better cost was found!"
                 # Simple Validation
-                if 'fold' not in self.validtype:
+                if self.validset_key in dataspec:
                     self.best_model = self.model
                     
                 if self.testlevel > 0:
@@ -1554,15 +1579,19 @@
                 else:
                     self.write_results( param, valid_stats, None, None, True  )
 
-            if len(expert.trials_param_list)-L0 >= self.max_ntrials:
-                return dataspec
+            if( len(expert.trials_param_list)-L0 >= self.max_ntrials
+            or  ( self.min_cost <> None and expert.best_cost <= self.min_cost ) ):
+                local_retrain_until_local_optimum_is_found = False
+                break
 
         if( self.retrain_until_local_optimum_is_found
-        and expert.should_be_tuned_again() ):
+        and local_retrain_until_local_optimum_is_found
+        and expert.should_be_tuned_again()
+        and ( self.max_cost == None or expert.best_cost <= self.max_cost ) ):
            return self.run( dataspec )
 
         if self.testlevel == 0:
-             self.retrain_and_writeresults(dataspec)
+             return self.retrain_and_writeresults(dataspec)
 
         return dataspec
 



From nouiz at mail.berlios.de  Fri Apr 25 22:28:02 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 25 Apr 2008 22:28:02 +0200
Subject: [Plearn-commits] r8899 - trunk/plearn_learners/hyper
Message-ID: <200804252028.m3PKS2DV030063@sheep.berlios.de>

Author: nouiz
Date: 2008-04-25 22:28:02 +0200 (Fri, 25 Apr 2008)
New Revision: 8899

Modified:
   trunk/plearn_learners/hyper/HyperLearner.cc
   trunk/plearn_learners/hyper/HyperLearner.h
   trunk/plearn_learners/hyper/HyperOptimize.cc
   trunk/plearn_learners/hyper/HyperOptimize.h
Log:
first version that allow saving the HyperLearner after some HyperOptimize trial.
Test it as not all class save correctly all field.


Modified: trunk/plearn_learners/hyper/HyperLearner.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.cc	2008-04-25 20:16:34 UTC (rev 8898)
+++ trunk/plearn_learners/hyper/HyperLearner.cc	2008-04-25 20:28:02 UTC (rev 8899)
@@ -62,6 +62,7 @@
     "They are accessible in the higher level PTester as: \n"
     "for ex: E[test1.E[mse]]");
 
+bool HyperLearner::reloading = false;
 
 TVec<string> HyperLearner::getTrainCostNames() const
 {
@@ -72,7 +73,9 @@
 }
 
 HyperLearner::HyperLearner()
-    : provide_strategy_expdir(true), save_final_learner(true)
+    : provide_strategy_expdir(true),
+      save_final_learner(true),
+      reloaded(false)
 {
     // Forward the 'test' method to the underlying learner.
     forward_test = true;
@@ -124,6 +127,12 @@
     declareOption(ol, "save_final_learner", &HyperLearner::save_final_learner, OptionBase::buildoption,
                   "should final learner be saved in expdir/final_learner.psave");
 
+    declareOption(ol, "reloaded", &HyperLearner::reloaded,
+                  OptionBase::learntoption|OptionBase::nosave,
+                  "Used internally to don't reload a file many as the build function\n"
+                  " can be called many time after the expdir is set. In particular\n"
+                  " PLearn::HyperLearner::setTrainingSet.");
+
     inherited::declareOptions(ol);
 
     // Hide some unused options.
@@ -263,6 +272,8 @@
 
     for(int commandnum=0; commandnum<strategy.length(); commandnum++)
         strategy[commandnum]->setHyperLearner(this);
+
+    auto_load();
 }
 
 /////////
@@ -312,7 +323,43 @@
 }
 
 
+void HyperLearner::auto_save()
+{
+    if(expdir.isEmpty())
+        PLERROR("In HyperLearner::auto_save - we can't auto_save as"
+                " we don't have any expdir");
+    PPath f = expdir/"hyper_learner_auto_save.psave";
+    PPath tmp=f+".tmp";
 
+    if(verbosity>0)
+        perr << "In HyperLearner::auto_save() - We save the hlearner"
+             << endl;
+    PLearn::save(tmp, this);
+    mvforce(tmp,f);
+}
+
+void HyperLearner::auto_load()
+{
+    if(expdir.isEmpty()){
+        if(verbosity>1)
+            pout<<"In HyperLearner::auto_load() - no expdir. Can't reload."<<endl;
+        return;
+    }
+    PPath f = expdir/"hyper_learner_auto_save.psave";
+    bool isf=isfile(f);
+    if(stage==0 && !reloading && !reloaded && isf){
+        if(verbosity>0)
+            pout<<"In HyperLearner::auto_load() - reloading from file: "<<f<<endl;
+        reloading = true;
+        PLearn::load(f,*this);
+        reloading = false;
+        reloaded = true;
+    }
+    else if(isf && verbosity>1)
+        pout<<"In HyperLearner::auto_load() - no file to reload."<<endl;
+
+}
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/hyper/HyperLearner.h
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.h	2008-04-25 20:16:34 UTC (rev 8898)
+++ trunk/plearn_learners/hyper/HyperLearner.h	2008-04-25 20:28:02 UTC (rev 8899)
@@ -76,7 +76,8 @@
 
     bool provide_strategy_expdir; //!< should each strategy step be provided a directory expdir/Step#
     bool save_final_learner; //!< should final learner be saved in expdir/final_learner.psave
-
+    bool reloaded; //!< Need to don't reload each time build is called as it can be called many time.
+    static bool reloading;
     // HyperLearner methods
 
     HyperLearner();
@@ -112,6 +113,12 @@
 
     virtual void run();
 
+    //! Save the current HyperLearner in its expdir
+    void auto_save();
+
+    //! Load the previously saved HyperLearner in its expdir if available
+    void auto_load();
+
 }; // class HyperLearner
 
 DECLARE_OBJECT_PTR(HyperLearner);

Modified: trunk/plearn_learners/hyper/HyperOptimize.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperOptimize.cc	2008-04-25 20:16:34 UTC (rev 8898)
+++ trunk/plearn_learners/hyper/HyperOptimize.cc	2008-04-25 20:28:02 UTC (rev 8899)
@@ -115,7 +115,9 @@
       provide_tester_expdir(false),
       rerun_after_sub(false),
       provide_sub_expdir(true),
-      save_best_learner(false)
+      save_best_learner(false),
+      auto_save(0),
+      auto_save_test(0)
 { }
 
 
@@ -164,6 +166,17 @@
         "If not specified, we'll use default splitter specified in the hyper-learner's tester option");
 
     declareOption(
+        ol, "auto_save", &HyperOptimize::auto_save, OptionBase::buildoption,
+        "Save the hlearner and reload it if necessary.\n"
+        "0 mean never, 1 mean always and >0 save iff trialnum%auto_save == 0.\n"
+        "In the last case, it save after the last trial.\n");
+
+    declareOption(
+        ol, "auto_save_test", &HyperOptimize::auto_save_test, OptionBase::buildoption,
+        "exit after each auto_save. This is usefull to test auto_save.\n"
+        "0 mean never, 1 mean always and >0 save iff trialnum%auto_save == 0");
+
+    declareOption(
         ol, "resultsmat", &HyperOptimize::resultsmat,
         OptionBase::learntoption | OptionBase::nosave,
         "Gives access to the results of all trials during the last training.\n"
@@ -215,10 +228,10 @@
 void HyperOptimize::setExperimentDirectory(const PPath& the_expdir)
 {
     inherited::setExperimentDirectory(the_expdir);
-    createResultsMat();    
+    getResultsMat();    
 }
 
-void HyperOptimize::createResultsMat()
+void HyperOptimize::getResultsMat()
 {
     TVec<string> cost_fields = getResultNames();
     TVec<string> option_fields = hlearner->option_fields;
@@ -230,7 +243,12 @@
     if (! expdir.isEmpty())
     {
         string fname = expdir+"results.pmat";
-        resultsmat = new FileVMatrix(fname,0,w);
+        if(isfile(fname)){
+            //we reload the old version if it exist
+            resultsmat = new FileVMatrix(fname, true);
+            return;
+        }else
+            resultsmat = new FileVMatrix(fname,0,w);
     }
     else
         resultsmat = new MemoryVMatrix(0,w);
@@ -310,20 +328,22 @@
 
 Vec HyperOptimize::optimize()
 {
+//in the case when auto_save is true. This function can be called even
+//if the optimisation is finished. We must not redo it in this case.
+    if(trialnum>0&&!option_vals&&resultsmat.length()==trialnum+1)
+        return best_results;
     TVec<string> option_names;
     option_names = oracle->getOptionNames();
 
-    if(option_vals.size()==0 && trialnum>0)
-        return best_results;//the optimization if finished
-    else if(option_vals.size()==0)
-        option_vals = oracle->generateFirstTrial();
-//        option_vals = oracle->generateNextTrial(option_vals, MISSING_VALUE);
-    if (option_vals.size() != option_names.size())
-        PLERROR("HyperOptimize::optimize: the number (%d) of option values (%s) "
-                "does not match the number (%d) of option names (%s) ",
-                option_vals.size(), tostring(option_vals).c_str(),
-                option_names.size(), tostring(option_names).c_str());
-
+    if(trialnum==0){
+        if(option_vals.size()==0)
+            option_vals = oracle->generateFirstTrial();
+        if (option_vals.size() != option_names.size())
+            PLERROR("HyperOptimize::optimize: the number (%d) of option values (%s) "
+                    "does not match the number (%d) of option names (%s) ",
+                    option_vals.size(), tostring(option_vals).c_str(),
+                    option_names.size(), tostring(option_names).c_str());
+    }
     which_cost_pos= getResultNames().find(which_cost);
     if(which_cost_pos < 0){
         if(!pl_islong(which_cost))
@@ -391,6 +411,13 @@
             }
         }
         ++trialnum;
+        if(auto_save>0 &&
+           (trialnum%auto_save==0 || !option_vals) ){
+            hlearner->auto_save();
+            if(auto_save_test>0 && trialnum%auto_save_test==0)
+                PLERROR("In HyperOptimize::optimize() - auto_save_test is true,"
+                        " exiting");
+        }
     }
 
     // Detect the case where no trials at all were performed!

Modified: trunk/plearn_learners/hyper/HyperOptimize.h
===================================================================
--- trunk/plearn_learners/hyper/HyperOptimize.h	2008-04-25 20:16:34 UTC (rev 8898)
+++ trunk/plearn_learners/hyper/HyperOptimize.h	2008-04-25 20:28:02 UTC (rev 8899)
@@ -133,6 +133,8 @@
     bool rerun_after_sub;
     bool provide_sub_expdir; // should sub_strategy be provided an expdir
     bool save_best_learner;
+    int auto_save;
+    int auto_save_test;
     PP<Splitter> splitter;  // (if not specified, use default splitter specified in PTester)
 
     // ****************
@@ -158,7 +160,7 @@
     // (Please implement in .cc)
     static void declareOptions(OptionList& ol);
 
-    void createResultsMat();
+    void getResultsMat();
     void reportResult(int trialnum,  const Vec& results);
     Vec runTest(int trialnum);
 



From nouiz at mail.berlios.de  Fri Apr 25 22:48:44 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 25 Apr 2008 22:48:44 +0200
Subject: [Plearn-commits] r8900 - trunk/plearn_learners/hyper
Message-ID: <200804252048.m3PKmifV031721@sheep.berlios.de>

Author: nouiz
Date: 2008-04-25 22:48:43 +0200 (Fri, 25 Apr 2008)
New Revision: 8900

Modified:
   trunk/plearn_learners/hyper/StepwiseSelectionOracle.cc
Log:
better implementation to allow saving, but not complete as we need to implement serialisation of priority_queue.


Modified: trunk/plearn_learners/hyper/StepwiseSelectionOracle.cc
===================================================================
--- trunk/plearn_learners/hyper/StepwiseSelectionOracle.cc	2008-04-25 20:28:02 UTC (rev 8899)
+++ trunk/plearn_learners/hyper/StepwiseSelectionOracle.cc	2008-04-25 20:48:43 UTC (rev 8900)
@@ -80,6 +80,24 @@
                   OptionBase::buildoption,
                   "Maximum number of variables that should be permitted in the search");
 
+    declareOption(ol, "current_indexes_searchset",
+                  &StepwiseSelectionOracle:: current_indexes_searchset,
+                  OptionBase::learntoption,
+                  "Contains the remaining combinations to generate for the"
+                  " current variable.");
+
+    //Should be saved, but PLearn don't save priority_queue now.
+    //TODO: implement in PStream.h 
+    //void writePriorityQueue(PStream& out, const PriorityQueueTemplate& s)
+    //void readPriorityQueue(PStream& in, const PriorityQueueTemplate& s)
+    //operator>>(PStream &in, priority_queue<T> &v)
+    //operator>>(PStream &in, priority_queue<T> &v)
+//     declareOption(ol, "combination_performance",
+//                   &StepwiseSelectionOracle:: combination_performance,
+//                   OptionBase::learntoption,
+//                   "This remembers the performance of each combination tried in the"
+//                   " current search set.");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }



From nouiz at mail.berlios.de  Fri Apr 25 22:53:58 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 25 Apr 2008 22:53:58 +0200
Subject: [Plearn-commits] r8901 - trunk/plearn_learners/regressors
Message-ID: <200804252053.m3PKrwaX031942@sheep.berlios.de>

Author: nouiz
Date: 2008-04-25 22:53:58 +0200 (Fri, 25 Apr 2008)
New Revision: 8901

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
added comment and optimization for a rare case that should not happen.


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2008-04-25 20:48:43 UTC (rev 8900)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2008-04-25 20:53:58 UTC (rev 8901)
@@ -99,7 +99,11 @@
     deepCopyField(tsorted_row, copies);
     deepCopyField(leave_register, copies);
     deepCopyField(getExample_tmp, copies);
+//tsource should be deep copied, but as currently when it is deep copied
+// the copy is not used anymore to train. To save memory we don't do it.
+// It is deep copied eavily by HyperLearner and HyperOptimizer
 //    deepCopyField(tsource,copies);
+//no need to deep copy source as we don't reuse it after initialization
 //    deepCopyField(source,copies);
 }
 
@@ -117,6 +121,9 @@
 
 void RegressionTreeRegisters::initRegisters(VMat the_train_set)
 {   
+    if(the_train_set==source && tsource)
+        //we set the existing source file
+        return;
     source = the_train_set;
     VMat tmp = VMat(new TransposeVMatrix(the_train_set));
     PP<MemoryVMatrixNoSave> tmp2 = new MemoryVMatrixNoSave(tmp);



From nouiz at mail.berlios.de  Fri Apr 25 22:58:23 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 25 Apr 2008 22:58:23 +0200
Subject: [Plearn-commits] r8902 - trunk/python_modules/plearn/pymake
Message-ID: <200804252058.m3PKwN55032176@sheep.berlios.de>

Author: nouiz
Date: 2008-04-25 22:58:22 +0200 (Fri, 25 Apr 2008)
New Revision: 8902

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
better message error


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-04-25 20:53:58 UTC (rev 8901)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-04-25 20:58:22 UTC (rev 8902)
@@ -2650,8 +2650,8 @@
             optionargs.remove(option)
             if (option != 'local'):
                 if (option[5] != '='):
-                    print 'Syntax for \'-local\' option is \'-local=<nb_proc>\', but' \
-                          ' read \'' + option + '\': one processor will be used'
+                    print 'Syntax is \'-local=<nb_proc>\''\
+                          '. Read \'' + option + '\'. Will ignoring the option'
                     # Keep default value (defined in config file or above
                     # nprocesses_on_localhost=1
                 else:



From larocheh at mail.berlios.de  Sat Apr 26 22:37:19 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Sat, 26 Apr 2008 22:37:19 +0200
Subject: [Plearn-commits] r8903 - trunk/plearn_learners/online
Message-ID: <200804262037.m3QKbJPN017756@sheep.berlios.de>

Author: larocheh
Date: 2008-04-26 22:37:19 +0200 (Sat, 26 Apr 2008)
New Revision: 8903

Modified:
   trunk/plearn_learners/online/RBMBinomialLayer.cc
   trunk/plearn_learners/online/RBMBinomialLayer.h
   trunk/plearn_learners/online/RBMLayer.cc
   trunk/plearn_learners/online/RBMLayer.h
Log:
Added a function that computes the gradient w/r to the free energy contribution...


Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2008-04-25 20:58:22 UTC (rev 8902)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2008-04-26 20:37:19 UTC (rev 8903)
@@ -487,6 +487,27 @@
     return result;
 }
 
+void RBMBinomialLayer::freeEnergyContributionGradient(
+    const Vec& unit_activations,
+    Vec& unit_activations_gradient,
+    real output_gradient, bool accumulate) const
+{
+    PLASSERT( unit_activations.size() == size );
+    unit_activations_gradient.resize( size );
+    if( !accumulate ) unit_activations_gradient.clear();
+    real* a = unit_activations.data();
+    real* ga = unit_activations_gradient.data();
+    for (int i=0; i<size; i++)
+    {
+        if (use_fast_approximations)
+            ga[i] -= output_gradient *
+                fastsigmoid( a[i] );
+        else
+            ga[i] -= output_gradient *
+                sigmoid( a[i] );
+    }
+}
+
 int RBMBinomialLayer::getConfigurationCount()
 {
     return size < 31 ? 1<<size : INFINITE_CONFIGURATIONS;

Modified: trunk/plearn_learners/online/RBMBinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.h	2008-04-25 20:58:22 UTC (rev 8902)
+++ trunk/plearn_learners/online/RBMBinomialLayer.h	2008-04-26 20:37:19 UTC (rev 8903)
@@ -125,6 +125,16 @@
     //! the OTHER layer of an RBM, from which unit_activations was computed.
     virtual real freeEnergyContribution(const Vec& unit_activations) const;
 
+    //! Computes gradient of the result of freeEnergyContribution
+    //! -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! with respect to unit_activations. Optionally, a gradient
+    //! with respect to freeEnergyContribution can be given
+    virtual void freeEnergyContributionGradient(const Vec& unit_activations,
+                                                Vec& unit_activations_gradient,
+                                                real output_gradient = 1,
+                                                bool accumulate = false) 
+        const;
+    
     virtual int getConfigurationCount();
 
     virtual void getConfiguration(int conf_index, Vec& output);

Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2008-04-25 20:58:22 UTC (rev 8902)
+++ trunk/plearn_learners/online/RBMLayer.cc	2008-04-26 20:37:19 UTC (rev 8903)
@@ -761,6 +761,14 @@
     return 0;
 }
 
+void RBMLayer::freeEnergyContributionGradient(const Vec& unit_activations,
+                                              Vec& unit_activations_gradient,
+                                              real output_gradient,
+                                              bool accumulate ) const
+{
+    PLERROR("RBMLayer::freeEnergyContributionGradient(Vec, Vec) not implemented in subclass %s\n",classname().c_str());
+}
+
 int RBMLayer::getConfigurationCount()
 {
     PLERROR("RBMLayer::getConfigurationCount() not implemented in subclass %s\n",classname().c_str());

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2008-04-25 20:58:22 UTC (rev 8902)
+++ trunk/plearn_learners/online/RBMLayer.h	2008-04-26 20:37:19 UTC (rev 8903)
@@ -289,6 +289,16 @@
     //! the OTHER layer of an RBM, from which unit_activations was computed.
     virtual real freeEnergyContribution(const Vec& unit_activations) const;
 
+    //! Computes gradient of the result of freeEnergyContribution
+    //! -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! with respect to unit_activations. Optionally, a gradient
+    //! with respect to freeEnergyContribution can be given
+    virtual void freeEnergyContributionGradient(const Vec& unit_activations,
+                                                Vec& unit_activations_gradient,
+                                                real output_gradient = 1,
+                                                bool accumulate = false ) 
+        const;
+
     //! Returns a number of different configurations the layer can be in.
     virtual int getConfigurationCount();
 



From larocheh at mail.berlios.de  Sat Apr 26 22:37:58 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Sat, 26 Apr 2008 22:37:58 +0200
Subject: [Plearn-commits] r8904 - trunk/plearn_learners_experimental
Message-ID: <200804262037.m3QKbwVC017832@sheep.berlios.de>

Author: larocheh
Date: 2008-04-26 22:37:58 +0200 (Sat, 26 Apr 2008)
New Revision: 8904

Added:
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
Log:
Attempt at training RBMs with pseudolikelihood...


Added: trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-04-26 20:37:19 UTC (rev 8903)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-04-26 20:37:58 UTC (rev 8904)
@@ -0,0 +1,958 @@
+// -*- C++ -*-
+
+// PseudolikelihoodRBM.cc
+//
+// Copyright (C) 2008 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file PseudolikelihoodRBM.cc */
+
+
+#define PL_LOG_MODULE_NAME "PseudolikelihoodRBM"
+#include "PseudolikelihoodRBM.h"
+#include <plearn_learners/online/RBMLayer.h>
+#include <plearn/io/pl_log.h>
+
+#define minibatch_hack 0 // Do we force the minibatch setting? (debug hack)
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    PseudolikelihoodRBM,
+    "Restricted Boltzmann Machine trained by (generalized) pseudolikelihood.",
+    "");
+
+///////////////////
+// PseudolikelihoodRBM //
+///////////////////
+PseudolikelihoodRBM::PseudolikelihoodRBM() :
+    learning_rate( 0. ),
+    decrease_ct( 0. ),
+    cd_learning_rate( 0. ),
+    cd_decrease_ct( 0. ),
+    cd_n_gibbs( 1 ),
+    n_classes( -1 ),
+    compute_input_space_nll( false ),
+    pseudolikelihood_context_size ( 0 ),
+    log_Z( MISSING_VALUE ),
+    Z_is_up_to_date( false )
+{
+    random_gen = new PRandom();
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void PseudolikelihoodRBM::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "learning_rate", &PseudolikelihoodRBM::learning_rate,
+                  OptionBase::buildoption,
+                  "The learning rate used for pseudolikelihood training.\n");
+
+    declareOption(ol, "decrease_ct", &PseudolikelihoodRBM::decrease_ct,
+                  OptionBase::buildoption,
+                  "The decrease constant of the learning rate.\n");
+
+    declareOption(ol, "cd_learning_rate", &PseudolikelihoodRBM::cd_learning_rate,
+                  OptionBase::buildoption,
+                  "The learning rate used for contrastive divergence learning.\n");
+
+    declareOption(ol, "cd_decrease_ct", &PseudolikelihoodRBM::cd_decrease_ct,
+                  OptionBase::buildoption,
+                  "The decrease constant of the contrastive divergence "
+                  "learning rate.\n");
+
+    declareOption(ol, "cd_n_gibbs", &PseudolikelihoodRBM::cd_n_gibbs,
+                  OptionBase::buildoption,
+                  "Number of negative phase gibbs sampling steps.\n");
+
+    declareOption(ol, "n_classes", &PseudolikelihoodRBM::n_classes,
+                  OptionBase::buildoption,
+                  "Number of classes in the training set (for supervised learning).\n"
+                  );
+
+    declareOption(ol, "compute_input_space_nll", 
+                  &PseudolikelihoodRBM::compute_input_space_nll,
+                  OptionBase::buildoption,
+                  "Indication that the input space NLL should be "
+                  "computed during test.\n"
+                  );
+
+    declareOption(ol, "pseudolikelihood_context_size", 
+                  &PseudolikelihoodRBM::pseudolikelihood_context_size,
+                  OptionBase::buildoption,
+                  "Number of additional input variables chosen to form the joint\n"
+                  "condition likelihoods in generalized pseudolikelihood\n"
+                  "(default = 0, which corresponds to standard pseudolikelihood).\n"
+                  );
+
+    declareOption(ol, "input_layer", &PseudolikelihoodRBM::input_layer,
+                  OptionBase::buildoption,
+                  "The binomial input layer of the RBM.\n");
+
+    declareOption(ol, "hidden_layer", &PseudolikelihoodRBM::hidden_layer,
+                  OptionBase::buildoption,
+                  "The hidden layer of the RBM.\n");
+
+    declareOption(ol, "connection", &PseudolikelihoodRBM::connection,
+                  OptionBase::buildoption,
+                  "The connection weights between the input and hidden layer.\n");
+
+    declareOption(ol, "log_Z", &PseudolikelihoodRBM::log_Z,
+                  OptionBase::learntoption,
+                  "Normalisation constant (on log scale).\n");
+
+    declareOption(ol, "Z_is_up_to_date", &PseudolikelihoodRBM::Z_is_up_to_date,
+                  OptionBase::learntoption,
+                  "Indication that the normalisation constant Z is up to date.\n");
+
+//    declareOption(ol, "target_weights_L1_penalty_factor", 
+//                  &PseudolikelihoodRBM::target_weights_L1_penalty_factor,
+//                  OptionBase::buildoption,
+//                  "Target weights' L1_penalty_factor.\n");
+//
+//    declareOption(ol, "target_weights_L2_penalty_factor", 
+//                  &PseudolikelihoodRBM::target_weights_L2_penalty_factor,
+//                  OptionBase::buildoption,
+//                  "Target weights' L2_penalty_factor.\n");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+////////////
+// build_ //
+////////////
+void PseudolikelihoodRBM::build_()
+{
+    MODULE_LOG << "build_() called" << endl;
+
+    if( inputsize_ > 0 && targetsize_ >= 0)
+    {
+        if( n_classes > 1 && targetsize_ != 1 )
+            PLERROR("In PseudolikelihoodRBM::build_(): can't use supervised "
+                "learning (n_classes > 1) if there is no target field "
+                "(targetsize() != 1)");
+        
+        if( compute_input_space_nll && n_classes > 1 )
+            PLERROR("In PseudolikelihoodRBM::build_(): compute_input_space_nll "
+                    "is not compatible with n_classes > 1");
+
+        if( pseudolikelihood_context_size < 0 )
+            PLERROR("In PseudolikelihoodRBM::build_(): "
+                    "pseudolikelihood_context_size should be >= 0.");
+
+        build_layers_and_connections();
+        build_costs();
+    }
+}
+
+/////////////////
+// build_costs //
+/////////////////
+void PseudolikelihoodRBM::build_costs()
+{
+    cost_names.resize(0);
+    
+    int current_index = 0;
+    if( compute_input_space_nll || n_classes > 1 )
+    {
+        cost_names.append("NLL");
+        nll_cost_index = current_index;
+        current_index++;
+    }
+    
+    if( n_classes > 1 )
+    {
+        cost_names.append("class_error");
+        class_cost_index = current_index;
+        current_index++;
+    }
+
+    PLASSERT( current_index == cost_names.length() );
+}
+
+//////////////////////////////////
+// build_layers_and_connections //
+//////////////////////////////////
+void PseudolikelihoodRBM::build_layers_and_connections()
+{
+    MODULE_LOG << "build_layers_and_connections() called" << endl;
+
+    if( !input_layer )
+        PLERROR("In PseudolikelihoodRBM::build_layers_and_connections(): "
+                "input_layer must be provided");
+    if( !hidden_layer )
+        PLERROR("In PseudolikelihoodRBM::build_layers_and_connections(): "
+                "hidden_layer must be provided");
+
+    if( !connection )
+        PLERROR("PseudolikelihoodRBM::build_layers_and_connections(): \n"
+                "connection must be provided");
+
+    if( connection->up_size != hidden_layer->size ||
+        connection->down_size != input_layer->size )
+        PLERROR("PseudolikelihoodRBM::build_layers_and_connections(): \n"
+                "connection's size (%d x %d) should be %d x %d",
+                connection->up_size, connection->down_size,
+                hidden_layer->size, input_layer->size);
+
+    hidden_activation_pos_i.resize( hidden_layer->size );
+    hidden_activation_neg_i.resize( hidden_layer->size );
+    hidden_activation_gradient.resize( hidden_layer->size );
+    hidden_activation_pos_i_gradient.resize( hidden_layer->size );
+    hidden_activation_neg_i_gradient.resize( hidden_layer->size );
+    connection_gradient.resize( connection->up_size, connection->down_size );
+
+    if( inputsize_ >= 0 )
+        PLASSERT( input_layer->size == inputsize() );
+
+    if( n_classes > 1 )
+    {
+        class_output.resize( n_classes );
+        before_class_output.resize( n_classes );
+        class_gradient.resize( n_classes );
+        target_one_hot.resize( n_classes );
+    }
+
+    if( !input_layer->random_gen )
+    {
+        input_layer->random_gen = random_gen;
+        input_layer->forget();
+    }
+
+    if( !hidden_layer->random_gen )
+    {
+        hidden_layer->random_gen = random_gen;
+        hidden_layer->forget();
+    }
+
+    if( !connection->random_gen )
+    {
+        connection->random_gen = random_gen;
+        connection->forget();
+    }
+}
+
+///////////
+// build //
+///////////
+void PseudolikelihoodRBM::build()
+{
+    inherited::build();
+    build_();
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void PseudolikelihoodRBM::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(input_layer, copies);
+    deepCopyField(hidden_layer, copies);
+    deepCopyField(connection, copies);
+    deepCopyField(cost_names, copies);
+
+    deepCopyField(target_one_hot, copies);
+    deepCopyField(input_gradient, copies);
+    deepCopyField(class_output, copies);
+    deepCopyField(before_class_output, copies);
+    deepCopyField(class_gradient, copies);
+    deepCopyField(hidden_activation_pos_i, copies);
+    deepCopyField(hidden_activation_neg_i, copies);
+    deepCopyField(hidden_activation_gradient, copies);
+    deepCopyField(hidden_activation_pos_i_gradient, copies);
+    deepCopyField(hidden_activation_neg_i_gradient, copies);
+    deepCopyField(connection_gradient, copies);
+    deepCopyField(context_indices, copies);
+    deepCopyField(context_indices_per_i, copies);
+    deepCopyField(hidden_activations_context, copies);
+    deepCopyField(hidden_activations_context_k_gradient, copies);
+    deepCopyField(nums, copies);
+    deepCopyField(nums_act, copies);
+    deepCopyField(context_probs, copies);
+    deepCopyField(gnums_act, copies);
+    deepCopyField(conf, copies);
+    deepCopyField(pos_input, copies);
+    deepCopyField(pos_hidden, copies);
+    deepCopyField(neg_input, copies);
+    deepCopyField(neg_hidden, copies);
+}
+
+
+////////////////
+// outputsize //
+////////////////
+int PseudolikelihoodRBM::outputsize() const
+{
+    return n_classes > 1 ? n_classes : hidden_layer->size;
+}
+
+////////////
+// forget //
+////////////
+void PseudolikelihoodRBM::forget()
+{
+    inherited::forget();
+
+    input_layer->forget();
+    hidden_layer->forget();
+    connection->forget();
+    Z_is_up_to_date = false;
+}
+
+///////////
+// train //
+///////////
+void PseudolikelihoodRBM::train()
+{
+    MODULE_LOG << "train() called " << endl;
+
+    MODULE_LOG << "stage = " << stage
+        << ", target nstages = " << nstages << endl;
+
+    PLASSERT( train_set );
+
+    Vec input( inputsize() );
+    Vec target( targetsize() );
+    int target_index;
+    real weight; // unused
+    real lr;
+
+    TVec<string> train_cost_names = getTrainCostNames() ;
+    Vec train_costs( train_cost_names.length() );
+    train_costs.fill(MISSING_VALUE) ;
+
+    int nsamples = train_set->length();
+    int init_stage = stage;
+    if( !initTrain() )
+    {
+        MODULE_LOG << "train() aborted" << endl;
+        return;
+    }
+
+    PP<ProgressBar> pb;
+
+    // clear stats of previous epoch
+    train_stats->forget();
+
+    if( report_progress )
+        pb = new ProgressBar( "Training "
+                              + classname(),
+                              nstages - stage );
+
+    for( ; stage<nstages ; stage++ )
+    {
+        Z_is_up_to_date = false;
+        train_set->getExample(stage%nsamples, input, target, weight);
+
+        if( pb )
+            pb->update( stage - init_stage + 1 );
+
+        if( targetsize() == 1 )
+        {
+            target_one_hot.clear();
+            if( !is_missing(target[0]) )
+            {
+                target_index = (int)round( target[0] );
+                target_one_hot[ target_index ] = 1;
+            }
+            PLERROR("In PseudolikelihoodRBM::train(): supervised learning "
+                    "not implemented yet.");
+
+            if( decrease_ct != 0 )
+                lr = learning_rate / (1.0 + stage * decrease_ct );
+            else
+                lr = learning_rate;
+
+            setLearningRate(lr);
+        }
+        else
+        {
+            if( !fast_is_equal(learning_rate, 0.) )
+            {
+                if( decrease_ct != 0 )
+                    lr = learning_rate / (1.0 + stage * decrease_ct );
+                else
+                    lr = learning_rate;
+
+                setLearningRate(lr);
+
+                if( pseudolikelihood_context_size == 0 )
+                {
+                    // Compute input_probs
+                    //
+                    //a = W x + c
+                    //  for i in 1...d
+                    //      num_pos = b_i
+                    //      num_neg = 0
+                    //      for j in 1...h
+                    //          num_pos += softplus( a_j - W_ji x_i + W_ji)
+                    //          num_neg += softplus( a_j - W_ji x_i)
+                    //      p_i = exp(num_pos) / (exp(num_pos) + exp(num_neg))
+
+                    connection->setAsDownInput( input );
+                    hidden_layer->getAllActivations( 
+                        (RBMMatrixConnection*) connection );
+
+                    real num_pos_act;
+                    real num_neg_act;
+                    real num_pos;
+                    real num_neg;
+                    real* a = hidden_layer->activation.data();
+                    real* a_pos_i = hidden_activation_pos_i.data();
+                    real* a_neg_i = hidden_activation_neg_i.data();
+                    real* w, *gw;
+                    int m = connection->weights.mod();
+                    real input_i, input_probs_i;
+                    real pseudolikelihood = 0;
+                    real* ga_pos_i = hidden_activation_pos_i_gradient.data();
+                    real* ga_neg_i = hidden_activation_neg_i_gradient.data();
+                    hidden_activation_gradient.clear();
+                    connection_gradient.clear();
+                    for( int i=0; i<input_layer->size ; i++ )
+                    {
+                        num_pos_act = input_layer->bias[i];
+                        num_neg_act = 0;
+                        w = &(connection->weights(0,i));
+                        input_i = input[i];
+                        for( int j=0; j<hidden_layer->size; j++,w+=m )
+                        {
+                            a_pos_i[j] = a[j] - *w * ( input_i - 1 );
+                            a_neg_i[j] = a[j] - *w * input_i;
+                        }
+                        num_pos_act -= hidden_layer->freeEnergyContribution(
+                            hidden_activation_pos_i);
+                        num_neg_act -= hidden_layer->freeEnergyContribution(
+                            hidden_activation_neg_i);
+                        num_pos = safeexp(num_pos_act);
+                        num_neg = safeexp(num_neg_act);
+                        input_probs_i = num_pos / (num_pos + num_neg);
+
+                        // Compute input_prob gradient
+                        if( input_layer->use_fast_approximations )
+                            pseudolikelihood += tabulated_softplus( 
+                                num_pos_act - num_neg_act ) 
+                                - input_i * (num_pos_act - num_neg_act);
+                        else
+                            pseudolikelihood += softplus( 
+                                num_pos_act - num_neg_act ) 
+                                - input_i * (num_pos_act - num_neg_act);;
+                        input_gradient[i] = input_probs_i - input_i;
+
+                        hidden_layer->freeEnergyContributionGradient(
+                            hidden_activation_pos_i,
+                            hidden_activation_pos_i_gradient,
+                            -input_gradient[i],
+                            false);
+                        hidden_activation_gradient += hidden_activation_pos_i_gradient;
+
+                        hidden_layer->freeEnergyContributionGradient(
+                            hidden_activation_neg_i,
+                            hidden_activation_neg_i_gradient,
+                            input_gradient[i],
+                            false);
+                        hidden_activation_gradient += hidden_activation_neg_i_gradient;
+
+                        gw = &(connection_gradient(0,i));
+                        for( int j=0; j<hidden_layer->size; j++,gw+=m )
+                        {
+                            *gw -= ga_pos_i[j] * ( input_i - 1 );
+                            *gw -= ga_neg_i[j] * input_i;
+                        }
+                    }
+
+                    externalProductAcc( connection_gradient, hidden_activation_gradient,
+                                        input );
+
+                    // Hidden bias update
+                    multiplyScaledAdd(hidden_activation_gradient, 1.0, -lr,
+                                      hidden_layer->bias);
+                    // Connection weights update
+                    multiplyScaledAdd( connection_gradient, 1.0, -lr,
+                                       connection->weights );
+                    // Input bias update
+                    multiplyScaledAdd(input_gradient, 1.0, -lr,
+                                      input_layer->bias);
+
+                    // N.B.: train costs contains pseudolikelihood
+                    //       or pseudoNLL, not NLL
+                    train_costs[nll_cost_index] = pseudolikelihood;
+                }
+                else
+                {
+                    // Generate contexts
+                    context_indices.resize( input_layer->size - 1);
+                    context_indices_per_i.resize( input_layer->size, 
+                                                  pseudolikelihood_context_size );
+                    for( int i=0; i<context_indices.length(); i++)
+                        context_indices[i] = i;
+                    int tmp,k;
+                    int n = input_layer->size-1;
+                    int* c = context_indices.data();
+                    int* ci;
+                    for( int i=0; i<context_indices_per_i.length(); i++)
+                    {
+                        ci = context_indices_per_i[i];
+                        for (int j = 0; j < context_indices_per_i.width(); j++) {
+                            k = j + random_gen->uniform_multinomial_sample(n - j);
+                            tmp = c[j];
+                            c[j] = c[k];
+                            c[k] = tmp;
+                            if( c[j] >= i )
+                                ci[j] = c[j]+1;
+                            else
+                                ci[j] = c[j];
+                        }
+                    }
+
+                    connection->setAsDownInput( input );
+                    hidden_layer->getAllActivations( 
+                        (RBMMatrixConnection *) connection );
+
+                    int n_conf = ipow(2, pseudolikelihood_context_size);
+                    nums_act.resize( 2 * n_conf );
+                    gnums_act.resize( 2 * n_conf );
+                    context_probs.resize( 2 * n_conf );
+                    hidden_activations_context.resize( 2*n_conf, hidden_layer->size );
+                    hidden_activations_context_k_gradient.resize( hidden_layer->size );
+                    real* nums_data;
+                    real* gnums_data;
+                    real* cp_data;
+                    real* a = hidden_layer->activation.data();
+                    real* w, *gw, *gi, *ac, *gac;
+                    int* context_i;
+                    int m = connection->weights.mod();
+                    int conf_index;
+                    real input_i, input_j, bi, Zi, log_Zi;
+                    real pseudolikelihood = 0;
+
+                    input_gradient.clear();
+                    hidden_activation_gradient.clear();
+                    connection_gradient.clear();
+                    gi = input_gradient.data();
+                    for( int i=0; i<input_layer->size ; i++ )
+                    {
+                        nums_data = nums_act.data();
+                        cp_data = context_probs.data();
+                        input_i = input[i];
+                        bi = input_layer->bias[i];
+
+                        // input_i = 1
+                        for( int k=0; k<n_conf; k++)
+                        {
+                            *nums_data = bi;
+                            *cp_data = input_i;
+                            conf_index = k;
+                            ac = hidden_activations_context[k];
+
+                            w = &(connection->weights(0,i));
+                            for( int j=0; j<hidden_layer->size; j++,w+=m )
+                                ac[j] = a[j] - *w * ( input_i - 1 );
+
+                            context_i = context_indices_per_i[i];
+                            for( int l=0; l<pseudolikelihood_context_size; l++ )
+                            {
+                                w = &(connection->weights(0,*context_i));
+                                input_j = input[*context_i];
+                                for( int j=0; j<hidden_layer->size; j++,w+=m )
+                                {
+                                    if( conf_index & 1)
+                                    {
+                                        ac[j] -=  *w * ( input_j - 1 );
+                                        *cp_data *= input_j;
+                                    }
+                                    else
+                                    {
+                                        ac[j] -=  *w * input_j;
+                                        *cp_data *= (1-input_j);
+                                    }
+                                }
+                                conf_index >>= 1;
+                                context_i++;
+                            }
+                            *nums_data -= hidden_layer->freeEnergyContribution(
+                                hidden_activations_context(k));
+                            nums_data++;
+                            cp_data++;
+                        }
+
+                        // input_i = 0
+                        for( int k=0; k<n_conf; k++)
+                        {
+                            *nums_data = 0;
+                            *cp_data = (1-input_i);
+                            conf_index = k;
+                            ac = hidden_activations_context[n_conf + k];
+                        
+                            w = &(connection->weights(0,i));
+                            for( int j=0; j<hidden_layer->size; j++,w+=m )
+                                ac[j] = a[j] - *w * input_i;
+
+                            context_i = context_indices_per_i[i];
+                            for( int l=0; l<pseudolikelihood_context_size; l++ )
+                            {
+                                w = &(connection->weights(0,*context_i));
+                                input_j = input[*context_i];
+                                for( int j=0; j<hidden_layer->size; j++,w+=m )
+                                {
+                                    if( conf_index & 1)
+                                    {
+                                        ac[j] -=  *w * ( input_j - 1 );
+                                        *cp_data *= input_j;
+                                    }
+                                    else
+                                    {
+                                        ac[j] -=  *w * input_j;
+                                        *cp_data *= (1-input_j);
+                                    }
+                                }
+                                conf_index >>= 1;
+                                context_i++;
+                            }
+                            *nums_data -= hidden_layer->freeEnergyContribution(
+                                hidden_activations_context(n_conf + k));
+                            nums_data++;
+                            cp_data++;
+                        }
+                    
+
+                        // Gradient computation
+                        exp( nums_act, nums);
+                        Zi = sum(nums);
+                        log_Zi = pl_log(Zi);
+
+                        nums_data = nums_act.data();
+                        gnums_data = gnums_act.data();
+                        cp_data = context_probs.data();
+
+                        // Compute input_prob gradient
+
+                        // input_i = 1                    
+                        for( int k=0; k<n_conf; k++)
+                        {
+                            pseudolikelihood -= *cp_data * (*nums_data - log_Zi);
+                            *gnums_data = *nums_data/Zi - *cp_data;
+                            *gi += *gnums_data;
+                        
+                            hidden_layer->freeEnergyContributionGradient(
+                                hidden_activations_context(k),
+                                hidden_activations_context_k_gradient,
+                                -*gnums_data,
+                                false);
+                            hidden_activation_gradient += 
+                                hidden_activations_context_k_gradient;
+                        
+                            gac = hidden_activations_context_k_gradient.data();
+                            gw = &(connection_gradient(0,i));
+                            for( int j=0; j<hidden_layer->size; j++,w+=m )
+                                *gw -= gac[j] * ( input_i - 1 );
+
+                            context_i = context_indices_per_i[i];
+                            for( int l=0; l<pseudolikelihood_context_size; l++ )
+                            {
+                                gw = &(connection_gradient(0,*context_i));
+                                input_j = input[*context_i];
+                                for( int j=0; j<hidden_layer->size; j++,w+=m )
+                                {
+                                    if( conf_index & 1)
+                                        *gw -= gac[j] * ( input_j - 1 );
+                                    else
+                                        *gw -= gac[j] * input_j;
+                                }
+                                conf_index >>= 1;
+                                context_i++;
+                            }
+
+                            nums_data++;
+                            gnums_data++;
+                            cp_data++;
+                        }
+
+                        // input_i = 0
+                        for( int k=0; k<n_conf; k++)
+                        {
+                            pseudolikelihood -= *cp_data * (*nums_data - log_Zi);
+                            *gnums_data = *nums_data/Zi - *cp_data;
+                            *gi += *gnums_data;
+                        
+                            hidden_layer->freeEnergyContributionGradient(
+                                hidden_activations_context(n_conf + k),
+                                hidden_activations_context_k_gradient,
+                                -*gnums_data,
+                                false);
+                            hidden_activation_gradient += 
+                                hidden_activations_context_k_gradient;
+                        
+                            gac = hidden_activations_context_k_gradient.data();
+                            gw = &(connection_gradient(0,i));
+                            for( int j=0; j<hidden_layer->size; j++,w+=m )
+                                *gw -= gac[j] *input_i;
+
+                            context_i = context_indices_per_i[i];
+                            for( int l=0; l<pseudolikelihood_context_size; l++ )
+                            {
+                                gw = &(connection_gradient(0,*context_i));
+                                input_j = input[*context_i];
+                                for( int j=0; j<hidden_layer->size; j++,w+=m )
+                                {
+                                    if( conf_index & 1)
+                                        *gw -= gac[j] * ( input_j - 1 );
+                                    else
+                                        *gw -= gac[j] * input_j;
+                                }
+                                conf_index >>= 1;
+                                context_i++;
+                            }
+
+                            nums_data++;
+                            gnums_data++;
+                            cp_data++;
+                        }
+                        gi++;
+                    }
+
+                    externalProductAcc( connection_gradient, hidden_activation_gradient,
+                                        input );
+
+                    // Hidden bias update
+                    multiplyScaledAdd(hidden_activation_gradient, 1.0, -lr,
+                                      hidden_layer->bias);
+                    // Connection weights update
+                    multiplyScaledAdd( connection_gradient, 1.0, -lr,
+                                       connection->weights );
+                    // Input bias update
+                    multiplyScaledAdd(input_gradient, 1.0, -lr,
+                                      input_layer->bias);
+
+                    // N.B.: train costs contains pseudolikelihood
+                    //       or pseudoNLL, not NLL
+                    train_costs[nll_cost_index] = pseudolikelihood;
+                }
+            }
+            
+            // CD learning
+            if( !fast_is_equal(cd_learning_rate, 0.) )
+            {
+                if( cd_decrease_ct != 0 )
+                    lr = cd_learning_rate / (1.0 + stage * cd_decrease_ct );
+                else
+                    lr = cd_learning_rate;
+
+                setLearningRate(lr);
+
+                // Positive phase
+                pos_input = input;
+                connection->setAsDownInput( input );
+                hidden_layer->getAllActivations( 
+                    (RBMMatrixConnection*) connection );
+                hidden_layer->computeExpectation();
+                pos_hidden.resize( hidden_layer->size );
+                pos_hidden << hidden_layer->expectation;
+
+                // Negative phase
+                for(int i=0; i<cd_n_gibbs; i++)
+                {
+                    hidden_layer->generateSample();
+                    connection->setAsUpInput( hidden_layer->sample );
+                    input_layer->getAllActivations( 
+                        (RBMMatrixConnection*) connection );
+                    input_layer->computeExpectation();
+                    input_layer->generateSample();
+                    connection->setAsDownInput( input_layer->sample );
+                    hidden_layer->getAllActivations( 
+                        (RBMMatrixConnection*) connection );
+                    hidden_layer->computeExpectation();
+                }
+                
+                neg_input = input_layer->sample;
+                neg_hidden = hidden_layer->expectation;
+
+                input_layer->update(pos_input,neg_input);
+                hidden_layer->update(pos_hidden,neg_hidden);
+                connection->update(pos_input,pos_hidden,
+                                   neg_input,neg_hidden);
+            }
+            
+        }
+        train_stats->update( train_costs );
+        
+    }
+    
+    train_stats->finalize();
+}
+
+
+///////////////////
+// computeOutput //
+///////////////////
+void PseudolikelihoodRBM::computeOutput(const Vec& input, Vec& output) const
+{
+    // Compute the output from the input.
+    output.resize(0);
+    if( n_classes > 1 )
+    {
+        // Get output probabilities
+        PLERROR("n_classes > 1 not implemented yet");
+    }
+    else
+    {
+        // Get hidden layer representation
+        connection->setAsDownInput( input );
+        hidden_layer->getAllActivations( (RBMMatrixConnection *) connection );
+        hidden_layer->computeExpectation();
+        output << hidden_layer->expectation;
+    }
+}
+
+
+void PseudolikelihoodRBM::computeCostsFromOutputs(const Vec& input, 
+                                                  const Vec& output,
+                                                  const Vec& target, 
+                                                  Vec& costs) const
+{
+
+    // Compute the costs from *already* computed output.
+    costs.resize( cost_names.length() );
+    costs.fill( MISSING_VALUE );
+
+    if( n_classes > 1 )
+    {
+        costs[class_cost_index] =
+            (argmax(output) == (int) round(target[0]))? 0 : 1;
+        costs[nll_cost_index] = -pl_log(output[(int) round(target[0])]);
+    }
+    else
+    {        
+        compute_Z();
+        connection->setAsDownInput( input );
+        hidden_layer->getAllActivations( (RBMMatrixConnection *) connection );
+        costs[nll_cost_index] = hidden_layer->freeEnergyContribution(
+            hidden_layer->activation) + log_Z;
+
+    }
+}
+
+TVec<string> PseudolikelihoodRBM::getTestCostNames() const
+{
+    // Return the names of the costs computed by computeCostsFromOutputs
+    // (these may or may not be exactly the same as what's returned by
+    // getTrainCostNames).
+
+    return cost_names;
+}
+
+TVec<string> PseudolikelihoodRBM::getTrainCostNames() const
+{
+    return cost_names;
+}
+
+
+//#####  Helper functions  ##################################################
+
+void PseudolikelihoodRBM::setLearningRate( real the_learning_rate )
+{
+    input_layer->setLearningRate( the_learning_rate );
+    hidden_layer->setLearningRate( the_learning_rate );
+    connection->setLearningRate( the_learning_rate );
+    //target_layer->setLearningRate( the_learning_rate );
+    //last_to_target->setLearningRate( the_learning_rate );
+}
+
+void PseudolikelihoodRBM::compute_Z() const
+{
+    if( Z_is_up_to_date ) return;
+
+    int input_n_conf = input_layer->getConfigurationCount(); 
+    int hidden_n_conf = hidden_layer->getConfigurationCount();
+    if( input_n_conf == RBMLayer::INFINITE_CONFIGURATIONS && 
+        hidden_n_conf == RBMLayer::INFINITE_CONFIGURATIONS )
+        PLERROR("In PseudolikelihoodRBM::computeCostsFromOutputs: "
+                "RBM's input and hidden layers are too big "
+                "for NLL computations.");
+
+    log_Z = 0;
+    if( input_n_conf < hidden_n_conf )
+    {
+        conf.resize( input_layer->size );
+        for(int i=0; i<input_n_conf; i++)
+        {
+            input_layer->getConfiguration(i,conf);
+            connection->setAsDownInput( conf );
+            hidden_layer->getAllActivations( (RBMMatrixConnection *) connection );
+            if( i == 0 )
+                log_Z = -hidden_layer->freeEnergyContribution(
+                    hidden_layer->activation);
+            else
+                log_Z = logadd(-hidden_layer->freeEnergyContribution(
+                                   hidden_layer->activation),
+                               log_Z);
+        }
+    }
+    else
+    {
+        conf.resize( hidden_layer->size );
+        for(int i=0; i<hidden_n_conf; i++)
+        {
+            hidden_layer->getConfiguration(i,conf);
+            connection->setAsUpInput( conf );
+            input_layer->getAllActivations( (RBMMatrixConnection *) connection );
+            if( i == 0 )
+                log_Z = -input_layer->freeEnergyContribution(
+                    input_layer->activation);
+            else
+                log_Z = logadd(-input_layer->freeEnergyContribution(
+                                   hidden_layer->activation),
+                               log_Z);
+        }        
+    }
+    
+    Z_is_up_to_date = true;
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2008-04-26 20:37:19 UTC (rev 8903)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2008-04-26 20:37:58 UTC (rev 8904)
@@ -0,0 +1,292 @@
+// -*- C++ -*-
+
+// PseudolikelihoodRBM.h
+//
+// Copyright (C) 2008 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file PseudolikelihoodRBM.h */
+
+#ifndef PseudolikelihoodRBM_INC
+#define PseudolikelihoodRBM_INC
+
+#include <plearn_learners/generic/PLearner.h>
+#include <plearn_learners/online/OnlineLearningModule.h>
+#include <plearn_learners/online/CostModule.h>
+#include <plearn_learners/online/CrossEntropyCostModule.h>
+#include <plearn_learners/online/NLLCostModule.h>
+#include <plearn_learners/online/RBMClassificationModule.h>
+#include <plearn_learners/online/RBMMultitaskClassificationModule.h>
+#include <plearn_learners/online/RBMLayer.h>
+#include <plearn_learners/online/RBMMixedLayer.h>
+#include <plearn_learners/online/RBMConnection.h>
+#include <plearn/misc/PTimer.h>
+#include <plearn/sys/Profiler.h>
+
+namespace PLearn {
+using namespace std;
+
+/**
+ * Restricted Boltzmann Machine trained by (generalized) pseudolikelihood
+ */
+class PseudolikelihoodRBM : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! The learning rate used for pseudolikelihood training
+    real learning_rate;
+
+    //! The decrease constant of the learning rate
+    real decrease_ct;
+
+    //! The learning rate used for contrastive divergence learning
+    real cd_learning_rate;
+
+    //! The decrease constant of the contrastive divergence learning rate
+    real cd_decrease_ct;
+
+    //! Number of negative phase gibbs sampling steps
+    int cd_n_gibbs;
+
+    //! Number of classes in the training set (for supervised learning)
+    int n_classes;
+
+    //! Indication that the input space NLL should be computed
+    //! during test
+    bool compute_input_space_nll;
+
+    //! Number of additional input variables chosen to form the joint
+    //! condition likelihoods in generalized pseudolikelihood
+    //! (default = 0, which corresponds to standard pseudolikelihood)
+    int pseudolikelihood_context_size;
+
+    //! The binomial input layer of the RBM
+    PP<RBMBinomialLayer> input_layer;
+
+    //! The hidden layer of the RBM
+    PP<RBMLayer> hidden_layer;
+
+    //! The connection weights between the input and hidden layer
+    PP<RBMMatrixConnection> connection;
+
+    ////! Target weights' L1_penalty_factor
+    //real target_weights_L1_penalty_factor;
+    //
+    ////! Target weights' L2_penalty_factor
+    //real target_weights_L2_penalty_factor;
+
+    //#####  Public Learnt Options  ###########################################
+    //! The computed cost names
+    TVec<string> cost_names;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    PseudolikelihoodRBM();
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Returns the size of this learner's output, (which typically
+    //! may depend on its inputsize(), targetsize() and set options).
+    virtual int outputsize() const;
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training costs
+    //! measured on-line in the process.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void train();
+
+    //! Computes the output from the input.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeOutput(const Vec& input, Vec& output) const;
+
+    //! Computes the costs from already computed output.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                         const Vec& target, Vec& costs) const;
+
+    //! Returns the names of the costs computed by computeCostsFromOutpus (and
+    //! thus the test method).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec<std::string> getTestCostNames() const;
+
+    //! Returns the names of the objective costs that the train method computes
+    //! and  for which it updates the VecStatsCollector train_stats.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec<std::string> getTrainCostNames() const;
+
+
+    // *** SUBCLASS WRITING: ***
+    // While in general not necessary, in case of particular needs
+    // (efficiency concerns for ex) you may also want to overload
+    // some of the following methods:
+    // virtual void computeOutputAndCosts(const Vec& input, const Vec& target,
+    //                                    Vec& output, Vec& costs) const;
+    // virtual void computeCostsOnly(const Vec& input, const Vec& target,
+    //                               Vec& costs) const;
+    // virtual void test(VMat testset, PP<VecStatsCollector> test_stats,
+    //                   VMat testoutputs=0, VMat testcosts=0) const;
+    // virtual int nTestCosts() const;
+    // virtual int nTrainCosts() const;
+    // virtual void resetInternalState();
+    // virtual bool isStatefulLearner() const;
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(PseudolikelihoodRBM);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+
+    //#####  Not Options  #####################################################
+
+    ////! Matrix connection weights between the hidden layer and the target layer
+    ////! (pointer to classification_module->last_to_target)
+    //PP<RBMMatrixConnection> last_to_target;
+    //
+    ////! Connection weights between the hidden layer and the target layer
+    ////! (pointer to classification_module->last_to_target)
+    //PP<RBMConnection> last_to_target_connection;
+    //
+    ////! Connection weights between the hidden layer and the visible layer
+    ////! (pointer to classification_module->joint_connection)
+    //PP<RBMConnection> joint_connection;
+    //
+    ////! Part of the RBM visible layer corresponding to the target
+    ////! (pointer to classification_module->target_layer)
+    //PP<RBMLayer> target_layer;
+
+    //! Temporary variables for Contrastive Divergence
+    mutable Vec target_one_hot;
+
+    //! Temporary variables for RBM computations
+    mutable Vec input_gradient;
+    mutable Vec class_output;
+    mutable Vec before_class_output;
+    mutable Vec class_gradient;
+    mutable Vec hidden_activation_pos_i;
+    mutable Vec hidden_activation_neg_i;
+    mutable Vec hidden_activation_gradient;
+    mutable Vec hidden_activation_pos_i_gradient;
+    mutable Vec hidden_activation_neg_i_gradient;
+    mutable Mat connection_gradient;
+    mutable TVec<int> context_indices;
+    mutable TMat<int> context_indices_per_i;
+    mutable Mat hidden_activations_context;
+    mutable Vec hidden_activations_context_k_gradient;
+    mutable Vec nums;
+    mutable Vec nums_act;
+    mutable Vec context_probs;
+    mutable Vec gnums_act;
+    mutable Vec conf;
+    mutable Vec pos_input;
+    mutable Vec pos_hidden;
+    mutable Vec neg_input;
+    mutable Vec neg_hidden;
+
+    //! Keeps the index of the NLL cost in train_costs
+    int nll_cost_index;
+
+    //! Keeps the index of the class_error cost in train_costs
+    int class_cost_index;
+
+    //! Normalisation constant (on log scale)
+    mutable real log_Z;
+
+    // Indication that the normalisation constant Z is up to date
+    mutable bool Z_is_up_to_date;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+    void build_layers_and_connections();
+
+    void build_costs();
+
+    void setLearningRate( real the_learning_rate );
+
+    void compute_Z() const;
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(PseudolikelihoodRBM);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From tihocan at mail.berlios.de  Mon Apr 28 15:47:12 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 28 Apr 2008 15:47:12 +0200
Subject: [Plearn-commits] r8905 - in
	trunk/plearn_learners/generic/test/NNet: . .pytest
	.pytest/PL_NNet_1_hidden_bug
	.pytest/PL_NNet_1_hidden_bug/expected_results
	.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet
	.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/Split0
	.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/global_stats.pmat.metadata
	.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/split_stats.pmat.metadata
Message-ID: <200804281347.m3SDlCKs017164@sheep.berlios.de>

Author: tihocan
Date: 2008-04-28 15:47:12 +0200 (Mon, 28 Apr 2008)
New Revision: 8905

Added:
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/RUN.log
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/Split0/
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/Split0/final_learner.psave
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/global_stats.pmat
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/global_stats.pmat.metadata/
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/global_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/global_stats.pmat.metadata/sizes
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/split_stats.pmat
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/split_stats.pmat.metadata/
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/split_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/split_stats.pmat.metadata/sizes
   trunk/plearn_learners/generic/test/NNet/nnet_regression.pyplearn
Modified:
   trunk/plearn_learners/generic/test/NNet/pytest.config
Log:
Added test PL_NNet_1_hidden_bug to test for both a NNet on regression, and to make sure an old bug does not come back


Property changes on: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug
___________________________________________________________________
Name: svn:ignore
   + .plearn
run_results
PSAVEDIFF


Added: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/RUN.log
===================================================================

Added: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/Split0/final_learner.psave	2008-04-26 20:37:58 UTC (rev 8904)
+++ trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/Split0/final_learner.psave	2008-04-28 13:47:12 UTC (rev 8905)
@@ -0,0 +1,64 @@
+*1 ->NNet(
+nhidden = 1 ;
+nhidden2 = 0 ;
+noutputs = 2 ;
+weight_decay = 0 ;
+bias_decay = 0 ;
+layer1_weight_decay = 0 ;
+layer1_bias_decay = 0 ;
+layer2_weight_decay = 0 ;
+layer2_bias_decay = 0 ;
+output_layer_weight_decay = 0 ;
+output_layer_bias_decay = 0 ;
+direct_in_to_out_weight_decay = 0 ;
+penalty_type = "L2_square" ;
+L1_penalty = 0 ;
+fixed_output_weights = 0 ;
+input_reconstruction_penalty = 0 ;
+direct_in_to_out = 0 ;
+rbf_layer_size = 0 ;
+first_class_is_junk = 1 ;
+output_transfer_func = "" ;
+hidden_transfer_func = "tanh" ;
+cost_funcs = 1 [ "mse" ] ;
+classification_regularizer = 0 ;
+first_hidden_layer = *0 ;
+first_hidden_layer_is_output = 0 ;
+n_non_params_in_first_hidden_layer = 0 ;
+transpose_first_hidden_layer = 0 ;
+margin = 1 ;
+do_not_change_params = 0 ;
+optimizer = *2 ->GradientOptimizer(
+start_learning_rate = 0.0100000000000000002 ;
+learning_rate = 0.0040016006402561026 ;
+decrease_constant = 0.00100000000000000002 ;
+lr_schedule = 0  0  [ 
+]
+;
+use_stochastic_hack = 0 ;
+verbosity = 0 ;
+nstages = 15  )
+;
+batch_size = 10 ;
+initialization_method = "uniform_linear" ;
+operate_on_bags = 0 ;
+max_bag_size = 20 ;
+paramsvalues = 9 [ -13.1263168777845713 19.5941887064242941 -12.3293937074457105 -28.661808141978188 26.1194879580729342 2.86359530614624225 10.1351639859538007 -75.9476149878473024 -43.2771486529794842 ] ;
+random_gen = *3 ->PRandom(
+seed = 1827 ;
+fixed_seed = 0  )
+;
+seed = 1827 ;
+stage = 100 ;
+n_examples = 150 ;
+inputsize = 4 ;
+targetsize = 2 ;
+weightsize = 0 ;
+forget_when_training_set_changes = 0 ;
+nstages = 100 ;
+report_progress = 1 ;
+verbosity = 1 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )

Added: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/global_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/global_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/global_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/global_stats.pmat.metadata/fieldnames	2008-04-26 20:37:58 UTC (rev 8904)
+++ trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/global_stats.pmat.metadata/fieldnames	2008-04-28 13:47:12 UTC (rev 8905)
@@ -0,0 +1 @@
+E[test1.E[mse]]	0

Added: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/global_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/global_stats.pmat.metadata/sizes	2008-04-26 20:37:58 UTC (rev 8904)
+++ trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/global_stats.pmat.metadata/sizes	2008-04-28 13:47:12 UTC (rev 8905)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/split_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/split_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/split_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/split_stats.pmat.metadata/fieldnames	2008-04-26 20:37:58 UTC (rev 8904)
+++ trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/split_stats.pmat.metadata/fieldnames	2008-04-28 13:47:12 UTC (rev 8905)
@@ -0,0 +1,2 @@
+splitnum	0
+test1.E[mse]	0

Added: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/split_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/split_stats.pmat.metadata/sizes	2008-04-26 20:37:58 UTC (rev 8904)
+++ trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/split_stats.pmat.metadata/sizes	2008-04-28 13:47:12 UTC (rev 8905)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/generic/test/NNet/nnet_regression.pyplearn
===================================================================
--- trunk/plearn_learners/generic/test/NNet/nnet_regression.pyplearn	2008-04-26 20:37:58 UTC (rev 8904)
+++ trunk/plearn_learners/generic/test/NNet/nnet_regression.pyplearn	2008-04-28 13:47:12 UTC (rev 8905)
@@ -0,0 +1,38 @@
+# Simple basic NNet experiment for a multivariate regression task.
+# It has only one hidden neuron so as to serve as a test for a bug
+# that appeared with 1-hidden-neuron networks.
+
+from plearn.pyplearn import pl
+
+nnet = pl.NNet(
+        cost_funcs = [ 'mse' ],
+        direct_in_to_out = False,
+        nhidden = 1,
+        noutputs = 2,
+        nstages = 100,
+        optimizer = pl.GradientOptimizer(
+            start_learning_rate = 1e-2,
+            decrease_constant = 1e-3,
+            ),
+        output_transfer_func = '',
+        weight_decay = 0,
+        batch_size = 10,
+        )
+
+tester = pl.PTester(
+        expdir = 'expdir-nnet',
+        learner = nnet,
+        dataset = pl.MemoryVMatrix(
+            source = pl.AutoVMatrix( filename = "PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat" )),
+        statnames = [ 'E[test1.E[mse]]' ],
+        splitter = pl.FractionSplitter(
+            splits = TMat(1, 2, [ (0, 0.75), (0.75, 1) ]),
+            ),
+        save_initial_tester = False,
+        save_stat_collectors = False,
+        save_test_names = False,
+        )
+
+def main():
+    return tester
+

Modified: trunk/plearn_learners/generic/test/NNet/pytest.config
===================================================================
--- trunk/plearn_learners/generic/test/NNet/pytest.config	2008-04-26 20:37:58 UTC (rev 8904)
+++ trunk/plearn_learners/generic/test/NNet/pytest.config	2008-04-28 13:47:12 UTC (rev 8905)
@@ -5,39 +5,31 @@
     For each Test instance you declare in a config file, a test will be ran
     by PyTest.
     
-      @ivar(name):
-    The name of the Test must uniquely determine the
+    @ivar name: The name of the Test must uniquely determine the
     test. Among others, it will be used to identify the test's results
-    (.PyTest/name/*_results/) and to report test informations.
-      @type(name):
-    String
+    (.PyTest/I{name}/*_results/) and to report test informations.
+    @type name: String
     
-      @ivar(description):
-    The description must provide other users an
+    @ivar description: The description must provide other users an
     insight of what exactly is the Test testing. You are encouraged
     to used triple quoted strings for indented multi-lines
     descriptions.
-      @type(description):
-    String
+    @type description: String
     
-      @ivar(category):
-    The category to which this test belongs. By default, a
+    @ivar category: The category to which this test belongs. By default, a
     test is considered a 'General' test.
     
     It is not desirable to let an extensive and lengthy test as 'General',
     while one shall refrain abusive use of categories since it is likely
     that only 'General' tests will be ran before most commits...
     
-      @type(category):
-    string
+    @type category: string
     
-      @ivar(program):
-    The program to be run by the Test. The program's name
-    PRGNAME is used to lookup for the program in the following manner:
+    @ivar program: The program to be run by the Test. The program's name
+    PRGNAME is used to lookup for the program in the following manner::
     
     1) Look for a local program named PRGNAME
-    2) Look for a plearn-like command (plearn, plearn_tests, ...) named 
-PRGNAME
+    2) Look for a plearn-like command (plearn, plearn_tests, ...) named PRGNAME
     3) Call 'which PRGNAME'
     4) Fail
     
@@ -46,50 +38,39 @@
     "compiler = 'pymake'"). If no compiler is provided while the program is
     believed to be compilable, 'pymake' will be assigned by
     default. Arguments to be forwarded to the compiler can be provided as a
-    string through the 'compile_options' keyword argument. @type program:
-    Program
+    string through the 'compile_options' keyword argument.  @type program:
+    L{Program}
     
-      @ivar(arguments):
-    The command line arguments to be passed to the program
+    @ivar arguments: The command line arguments to be passed to the program
     for the test to proceed.
-      @type(arguments):
-    String
+    @type arguments: String
     
-      @ivar(resources):
-    A list of resources that are used by your program
+    @ivar resources: A list of resources that are used by your program
     either in the command line or directly in the code (plearn or pyplearn
-    files, databases, ...). The elements of the list must be string
+    files, databases, ...).  The elements of the list must be string
     representations of the path, absolute or relative, to the resource.
-      @type(resources):
-    List of Strings
+    @type resources: List of Strings
     
-      @ivar(precision):
-    The precision (absolute and relative) used when comparing
+    @ivar precision: The precision (absolute and relative) used when comparing
     floating numbers in the test output (default = 1e-6)
-      @type(precision):
-    float
+    @type precision: float
     
-      @ivar(pfileprg):
-    The program to be used for comparing files of psave &
-    vmat formats. It can be either:
-      - "__program__": maps to this test's program if its compilable;
+    @ivar pfileprg: The program to be used for comparing files of psave &
+    vmat formats. It can be either::
+    - "__program__": maps to this test's program if its compilable;
     maps to 'plearn_tests' otherwise (default);
-      - "__plearn__": always maps to 'plearn_tests' (for when the program
+    - "__plearn__": always maps to 'plearn_tests' (for when the program
     under test is not a version of PLearn);
-      - A Program (see 'program' option) instance
-      - None: if you are sure no files are to be compared.
+    - A Program (see 'program' option) instance
+    - None: if you are sure no files are to be compared.
     
-      @ivar(ignored_files_re):
-    Default behaviour of a test is to compare all
+    @ivar ignored_files_re: Default behaviour of a test is to compare all
     files created by running the test. In some case, one may prefer some of
     these files to be ignored.
-      @type(ignored_files_re):
-    list of regular expressions
+    @type ignored_files_re: list of regular expressions
     
-      @ivar(disabled):
-    If true, the test will not be ran.
-      @type(disabled):
-    bool
+    @ivar disabled: If true, the test will not be ran.
+    @type disabled: bool
     
 """
 Test(
@@ -126,3 +107,21 @@
     difftime = None
     )
 
+Test(
+    name = "PL_NNet_1_hidden_bug",
+    description = "Regression test for bug with a single hidden unit",
+    category = "General",
+    program = Program(
+        name = "plearn_tests",
+        compiler = "pymake"
+        ),
+    arguments = "nnet_regression.pyplearn",
+    resources = [ "nnet_regression.pyplearn" ],
+    precision = 1e-06,
+    pfileprg = "__program__",
+    disabled = False,
+    runtime = None,
+    difftime = None
+    )
+
+



From nouiz at mail.berlios.de  Mon Apr 28 17:57:34 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 28 Apr 2008 17:57:34 +0200
Subject: [Plearn-commits] r8906 - in
	trunk/plearn_learners/regressors/test/RegressionTree: .
	.pytest/PL_RegressionTree/expected_results/expdir
	.pytest/PL_RegressionTree_MultiClass/expected_results/expdir
Message-ID: <200804281557.m3SFvY3i002084@sheep.berlios.de>

Author: nouiz
Date: 2008-04-28 17:57:34 +0200 (Mon, 28 Apr 2008)
New Revision: 8906

Modified:
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/experiment.plearn
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/experiment.plearn
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave
   trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn
   trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass.pyplearn
Log:
fixed test broken after commit of auto_save in HyperLearner and HyperOptimize


Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/experiment.plearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/experiment.plearn	2008-04-28 13:47:12 UTC (rev 8905)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/experiment.plearn	2008-04-28 15:57:34 UTC (rev 8906)
@@ -84,7 +84,7 @@
                 "E[test2.E[base_reward_l1]]"
                 ]
             ),
-        verbosity = 2
+        verbosity = 1
         ),
     provide_learner_expdir = 1,
     save_learners = 0,

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt	2008-04-28 13:47:12 UTC (rev 8905)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt	2008-04-28 15:57:34 UTC (rev 8906)
@@ -1,2 +1,2 @@
-__REVISION__ = "PL8839"
+__REVISION__ = "PL8883"
 data                                          = PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave	2008-04-28 13:47:12 UTC (rev 8905)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave	2008-04-28 15:57:34 UTC (rev 8906)
@@ -142,7 +142,11 @@
 min_improvement = -3.40282000000000014e+38 ;
 relative_min_improvement = -1 ;
 max_degraded_steps = 120 ;
-min_n_steps = 2  )
+min_n_steps = 2 ;
+nreturned = 0 ;
+best_objective = 1.79769313486231571e+308 ;
+best_step = -1 ;
+met_early_stopping = 0  )
 ;
 provide_tester_expdir = 1 ;
 sub_strategy = []
@@ -150,7 +154,18 @@
 rerun_after_sub = 0 ;
 provide_sub_expdir = 1 ;
 save_best_learner = 0 ;
-splitter = *0 )
+splitter = *0 ;
+auto_save = 0 ;
+auto_save_diff_time = 10800 ;
+auto_save_test = 0 ;
+best_objective = 1.79769313486231571e+308 ;
+best_results = []
+;
+best_learner = *0 ;
+trialnum = 0 ;
+option_vals = []
+;
+verbosity = 0  )
 ] ;
 provide_strategy_expdir = 1 ;
 save_final_learner = 0 ;
@@ -167,7 +182,7 @@
 forget_when_training_set_changes = 0 ;
 nstages = 1 ;
 report_progress = 1 ;
-verbosity = 2 ;
+verbosity = 1 ;
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/experiment.plearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/experiment.plearn	2008-04-28 13:47:12 UTC (rev 8905)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/experiment.plearn	2008-04-28 15:57:34 UTC (rev 8906)
@@ -94,7 +94,7 @@
                 "E[test2.E[base_reward_l1]]"
                 ]
             ),
-        verbosity = 2
+        verbosity = 1
         ),
     provide_learner_expdir = 1,
     save_learners = 0,

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt	2008-04-28 13:47:12 UTC (rev 8905)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt	2008-04-28 15:57:34 UTC (rev 8906)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL8839"
+__REVISION__ = "PL8883"
 datatest                                      = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat
 datatrain                                     = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave	2008-04-28 13:47:12 UTC (rev 8905)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave	2008-04-28 15:57:34 UTC (rev 8906)
@@ -156,7 +156,11 @@
 min_improvement = -3.40282000000000014e+38 ;
 relative_min_improvement = -1 ;
 max_degraded_steps = 120 ;
-min_n_steps = 2  )
+min_n_steps = 2 ;
+nreturned = 0 ;
+best_objective = 1.79769313486231571e+308 ;
+best_step = -1 ;
+met_early_stopping = 0  )
 ;
 provide_tester_expdir = 1 ;
 sub_strategy = []
@@ -164,7 +168,18 @@
 rerun_after_sub = 0 ;
 provide_sub_expdir = 1 ;
 save_best_learner = 0 ;
-splitter = *0 )
+splitter = *0 ;
+auto_save = 0 ;
+auto_save_diff_time = 10800 ;
+auto_save_test = 0 ;
+best_objective = 1.79769313486231571e+308 ;
+best_results = []
+;
+best_learner = *0 ;
+trialnum = 0 ;
+option_vals = []
+;
+verbosity = 0  )
 ] ;
 provide_strategy_expdir = 1 ;
 save_final_learner = 0 ;
@@ -181,7 +196,7 @@
 forget_when_training_set_changes = 0 ;
 nstages = 1 ;
 report_progress = 1 ;
-verbosity = 2 ;
+verbosity = 1 ;
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn	2008-04-28 13:47:12 UTC (rev 8905)
+++ trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn	2008-04-28 15:57:34 UTC (rev 8906)
@@ -25,7 +25,7 @@
     forget_when_training_set_changes = 0 ,
     nstages = 1 ,
     report_progress = 1 ,
-    verbosity = 2 ,
+    verbosity = 1 ,
     learner = pl.RegressionTree(
         nstages = 10
         ,loss_function_weight = 1

Modified: trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass.pyplearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass.pyplearn	2008-04-28 13:47:12 UTC (rev 8905)
+++ trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass.pyplearn	2008-04-28 15:57:34 UTC (rev 8906)
@@ -21,7 +21,7 @@
     forget_when_training_set_changes = 0 ,
     nstages = 1 ,
     report_progress = 1 ,
-    verbosity = 2 ,
+    verbosity = 1 ,
     learner = pl.RegressionTree(
         nstages = 10
         ,loss_function_weight = 1



From nouiz at mail.berlios.de  Mon Apr 28 18:03:03 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 28 Apr 2008 18:03:03 +0200
Subject: [Plearn-commits] r8907 - trunk/plearn_learners/hyper
Message-ID: <200804281603.m3SG33JX002532@sheep.berlios.de>

Author: nouiz
Date: 2008-04-28 18:03:03 +0200 (Mon, 28 Apr 2008)
New Revision: 8907

Modified:
   trunk/plearn_learners/hyper/HyperLearner.cc
   trunk/plearn_learners/hyper/HyperOptimize.cc
   trunk/plearn_learners/hyper/HyperOptimize.h
Log:
Implemented HyperOptimize::auto_save_diff_time, this set a mininum amount of time between save point. Default 3h.


Modified: trunk/plearn_learners/hyper/HyperLearner.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.cc	2008-04-28 15:57:34 UTC (rev 8906)
+++ trunk/plearn_learners/hyper/HyperLearner.cc	2008-04-28 16:03:03 UTC (rev 8907)
@@ -342,21 +342,21 @@
 {
     if(expdir.isEmpty()){
         if(verbosity>1)
-            pout<<"In HyperLearner::auto_load() - no expdir. Can't reload."<<endl;
+            PLWARNING("In HyperLearner::auto_load() - no expdir. Can't reload.");
         return;
     }
     PPath f = expdir/"hyper_learner_auto_save.psave";
     bool isf=isfile(f);
     if(stage==0 && !reloading && !reloaded && isf){
         if(verbosity>0)
-            pout<<"In HyperLearner::auto_load() - reloading from file: "<<f<<endl;
+            PLWARNING("In HyperLearner::auto_load() - reloading from file %s",f.c_str());
         reloading = true;
         PLearn::load(f,*this);
         reloading = false;
         reloaded = true;
     }
     else if(isf && verbosity>1)
-        pout<<"In HyperLearner::auto_load() - no file to reload."<<endl;
+        PLWARNING("In HyperLearner::auto_load() - no file to reload.");
 
 }
 

Modified: trunk/plearn_learners/hyper/HyperOptimize.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperOptimize.cc	2008-04-28 15:57:34 UTC (rev 8906)
+++ trunk/plearn_learners/hyper/HyperOptimize.cc	2008-04-28 16:03:03 UTC (rev 8907)
@@ -48,6 +48,7 @@
 #include <plearn/base/stringutils.h>
 #include <plearn/vmat/FileVMatrix.h>
 #include <plearn/vmat/MemoryVMatrix.h>
+#include <plearn/sys/Profiler.h>
 
 namespace PLearn {
 using namespace std;
@@ -117,7 +118,8 @@
       provide_sub_expdir(true),
       save_best_learner(false),
       auto_save(0),
-      auto_save_test(0)
+      auto_save_test(0),
+      auto_save_diff_time(3*60*60)
 { }
 
 
@@ -169,9 +171,16 @@
         ol, "auto_save", &HyperOptimize::auto_save, OptionBase::buildoption,
         "Save the hlearner and reload it if necessary.\n"
         "0 mean never, 1 mean always and >0 save iff trialnum%auto_save == 0.\n"
-        "In the last case, it save after the last trial.\n");
+        "In the last case, it save after the last trial.\n"
+        "See auto_save_diff_time as both condition must be true to save.\n");
 
     declareOption(
+        ol, "auto_save_diff_time", &HyperOptimize::auto_save_diff_time,
+        OptionBase::buildoption,
+        "HyperOptimize::auto_save_diff_time is the mininum amount of time before the\n"
+        " first save point and between two save point in second. Default 3h.");
+
+    declareOption(
         ol, "auto_save_test", &HyperOptimize::auto_save_test, OptionBase::buildoption,
         "exit after each auto_save. This is usefull to test auto_save.\n"
         "0 mean never, 1 mean always and >0 save iff trialnum%auto_save == 0");
@@ -203,6 +212,10 @@
     declareOption(ol, "option_vals", &HyperOptimize::option_vals,
                   OptionBase::learntoption,"The option value to try." );
 
+//     declareOption(ol, "auto_save_timer", &HyperOptimize::auto_save_timer,
+//                   OptionBase::learntoption|OptionBase::nosave,
+//                   "The last time a save was done." );
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -216,6 +229,8 @@
     // ###  - Building of a "reloaded" object: i.e. from the complete set of all serialised options.
     // ###  - Updating or "re-building" of an object after a few "tuning" options have been modified.
     // ### You should assume that the parent class' build_() has already been called.
+
+    auto_save_timer.activate();
 }
 
 // ### Nothing to add here, simply calls build_
@@ -356,6 +371,8 @@
     Vec results;
     while(option_vals)
     {
+        auto_save_timer.start("auto_save");
+
         if(verbosity>0)
             perr << "In HyperOptimize::optimize() - We optimize with "
                 "parameters " << option_names << " with value " << option_vals
@@ -411,12 +428,20 @@
             }
         }
         ++trialnum;
-        if(auto_save>0 &&
-           (trialnum%auto_save==0 || !option_vals) ){
-            hlearner->auto_save();
-            if(auto_save_test>0 && trialnum%auto_save_test==0)
-                PLERROR("In HyperOptimize::optimize() - auto_save_test is true,"
-                        " exiting");
+
+        auto_save_timer.end("auto_save");
+        if(auto_save>0){
+            if(trialnum%auto_save!=0 && option_vals)
+                continue;
+            int s=auto_save_timer.getStats("auto_save").wall_duration;
+            s/=auto_save_timer.ticksPerSecond();
+            if(s>auto_save_diff_time|| ! option_vals){
+                hlearner->auto_save();
+                auto_save_timer.reset("auto_save");
+                if(auto_save_test>0 && trialnum%auto_save_test==0)
+                    PLERROR("In HyperOptimize::optimize() - auto_save_test is true,"
+                            " exiting");
+            }
         }
     }
 

Modified: trunk/plearn_learners/hyper/HyperOptimize.h
===================================================================
--- trunk/plearn_learners/hyper/HyperOptimize.h	2008-04-28 15:57:34 UTC (rev 8906)
+++ trunk/plearn_learners/hyper/HyperOptimize.h	2008-04-28 16:03:03 UTC (rev 8907)
@@ -50,7 +50,7 @@
 
 namespace PLearn {
 using namespace std;
-
+class Profiler;
 /**
  *  Carry out an hyper-parameter optimization according to an Oracle
  *
@@ -113,6 +113,7 @@
     PP<PLearner> best_learner;
     int trialnum;
     TVec<string> option_vals;
+    Profiler auto_save_timer;
 
 public:
 
@@ -135,6 +136,7 @@
     bool save_best_learner;
     int auto_save;
     int auto_save_test;
+    int auto_save_diff_time;
     PP<Splitter> splitter;  // (if not specified, use default splitter specified in PTester)
 
     // ****************



From tihocan at mail.berlios.de  Mon Apr 28 18:05:47 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 28 Apr 2008 18:05:47 +0200
Subject: [Plearn-commits] r8908 - in
	trunk/plearn_learners/generic/test/NNet: .
	.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet
	.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/Split0
Message-ID: <200804281605.m3SG5laT002673@sheep.berlios.de>

Author: tihocan
Date: 2008-04-28 18:05:47 +0200 (Mon, 28 Apr 2008)
New Revision: 8908

Modified:
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/Split0/final_learner.psave
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/global_stats.pmat
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/split_stats.pmat
   trunk/plearn_learners/generic/test/NNet/nnet_regression.pyplearn
Log:
Fixed test PL_NNet_1_hidden_bug so that it can pass on other computers (hopefully)

Modified: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/Split0/final_learner.psave	2008-04-28 16:03:03 UTC (rev 8907)
+++ trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/Split0/final_learner.psave	2008-04-28 16:05:47 UTC (rev 8908)
@@ -29,33 +29,34 @@
 margin = 1 ;
 do_not_change_params = 0 ;
 optimizer = *2 ->GradientOptimizer(
-start_learning_rate = 0.0100000000000000002 ;
-learning_rate = 0.0040016006402561026 ;
+start_learning_rate = 0.00100000000000000002 ;
+learning_rate = 0.000870322019147084396 ;
 decrease_constant = 0.00100000000000000002 ;
 lr_schedule = 0  0  [ 
 ]
 ;
 use_stochastic_hack = 0 ;
 verbosity = 0 ;
-nstages = 15  )
+nstages = 15 ;
+early_stop = 0  )
 ;
 batch_size = 10 ;
 initialization_method = "uniform_linear" ;
 operate_on_bags = 0 ;
 max_bag_size = 20 ;
-paramsvalues = 9 [ -13.1263168777845713 19.5941887064242941 -12.3293937074457105 -28.661808141978188 26.1194879580729342 2.86359530614624225 10.1351639859538007 -75.9476149878473024 -43.2771486529794842 ] ;
+paramsvalues = 9 [ -0.557025969227852968 1.14928163614964474 -0.45781959698541691 -2.16987919303079169 1.08539803737234242 3.51007804148783187 4.0760403120233013 -18.5217581493060095 -10.6940446511284577 ] ;
 random_gen = *3 ->PRandom(
 seed = 1827 ;
 fixed_seed = 0  )
 ;
 seed = 1827 ;
-stage = 100 ;
+stage = 10 ;
 n_examples = 150 ;
 inputsize = 4 ;
 targetsize = 2 ;
 weightsize = 0 ;
 forget_when_training_set_changes = 0 ;
-nstages = 100 ;
+nstages = 10 ;
 report_progress = 1 ;
 verbosity = 1 ;
 nservers = 0 ;

Modified: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/generic/test/NNet/nnet_regression.pyplearn
===================================================================
--- trunk/plearn_learners/generic/test/NNet/nnet_regression.pyplearn	2008-04-28 16:03:03 UTC (rev 8907)
+++ trunk/plearn_learners/generic/test/NNet/nnet_regression.pyplearn	2008-04-28 16:05:47 UTC (rev 8908)
@@ -9,10 +9,11 @@
         direct_in_to_out = False,
         nhidden = 1,
         noutputs = 2,
-        nstages = 100,
+        nstages = 10,
         optimizer = pl.GradientOptimizer(
-            start_learning_rate = 1e-2,
+            start_learning_rate = 1e-3,
             decrease_constant = 1e-3,
+            verbosity = 0,
             ),
         output_transfer_func = '',
         weight_decay = 0,



From tihocan at mail.berlios.de  Mon Apr 28 18:23:15 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 28 Apr 2008 18:23:15 +0200
Subject: [Plearn-commits] r8909 - trunk/commands/PLearnCommands
Message-ID: <200804281623.m3SGNFsD005330@sheep.berlios.de>

Author: tihocan
Date: 2008-04-28 18:23:15 +0200 (Mon, 28 Apr 2008)
New Revision: 8909

Modified:
   trunk/commands/PLearnCommands/VMatCommand.cc
Log:
Minor typo fixes

Modified: trunk/commands/PLearnCommands/VMatCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/VMatCommand.cc	2008-04-28 16:05:47 UTC (rev 8908)
+++ trunk/commands/PLearnCommands/VMatCommand.cc	2008-04-28 16:23:15 UTC (rev 8909)
@@ -86,8 +86,8 @@
         "         --delimiter=C:   use character C as the field delimiter (default = ',')\n"
         "         --convert-date:  first column is assumed to be in CYYMMDD format; it is\n"
         "                          exported as YYYYMMDD in the .csv file (19000000 is added)\n"
-        "         --mat_to_mem:    Load the original matrice to memory\n"
-        "         --save_vmat:     Save the source vmat in the destinatino metadatadir\n"
+        "         --mat_to_mem:    Load the original matrix into memory\n"
+        "         --save_vmat:     Save the source vmat in the destination metadatadir\n"
         "   or: vmat gendef <source> [binnum1 binnum2 ...] \n"
         "       Generate stats for dataset (will put them in its associated metadatadir). \n"
         "   or: vmat genvmat <source_dataset> <dest_vmat> [binned{num} | onehot{num} | normalized]\n"



From nouiz at mail.berlios.de  Mon Apr 28 18:55:49 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 28 Apr 2008 18:55:49 +0200
Subject: [Plearn-commits] r8910 - trunk/scripts
Message-ID: <200804281655.m3SGtnPj013757@sheep.berlios.de>

Author: nouiz
Date: 2008-04-28 18:55:48 +0200 (Mon, 28 Apr 2008)
New Revision: 8910

Modified:
   trunk/scripts/dbidispatch
Log:
the raw mode must be contatenated by newline


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-04-28 16:23:15 UTC (rev 8909)
+++ trunk/scripts/dbidispatch	2008-04-28 16:55:48 UTC (rev 8910)
@@ -173,10 +173,13 @@
     elif argv.split('=')[0] in ["--duree","--cpu","--mem","--os","--nb_proc",
                                 "--req", "--files", "--raw", "--rank"]:
         param=argv.split('=')[0][2:]
-        if param in ["req", "files", "raw", "rank"]:
+        if param in ["req", "files", "rank"]:
             #param that we happend to if defined more then one time
             dbi_param.setdefault(param,'True')
             dbi_param[param]+='&&('+argv.split('=',1)[1]+')'
+        elif param == "raw":
+            dbi_param.setdefault(param,'')
+            dbi_param[param]+='\n'+argv.split('=',1)[1]
         else:
             #otherwise we erase the old value
             dbi_param[param]=argv.split('=',1)[1]



From nouiz at mail.berlios.de  Mon Apr 28 22:23:02 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 28 Apr 2008 22:23:02 +0200
Subject: [Plearn-commits] r8911 - trunk/plearn/io
Message-ID: <200804282023.m3SKN2eN022673@sheep.berlios.de>

Author: nouiz
Date: 2008-04-28 22:23:02 +0200 (Mon, 28 Apr 2008)
New Revision: 8911

Modified:
   trunk/plearn/io/fileutils.cc
Log:
quote parameter so that special caracter as space are correctly handled.


Modified: trunk/plearn/io/fileutils.cc
===================================================================
--- trunk/plearn/io/fileutils.cc	2008-04-28 16:55:48 UTC (rev 8910)
+++ trunk/plearn/io/fileutils.cc	2008-04-28 20:23:02 UTC (rev 8911)
@@ -377,7 +377,7 @@
 void mv(const PPath& source, const PPath& destination)
 {
     // TODO Cross-platform
-    string command = "\\mv " + source.absolute() + " " + destination.absolute();
+    string command = "\\mv '" + source.absolute() + "' '" + destination.absolute()+"'";
     system(command.c_str());
 }
 
@@ -386,7 +386,7 @@
 /////////////
 void mvforce(const PPath& source, const PPath& destination)
 {
-    string command = "\\mv -f " + source.absolute() + " " + destination.absolute();
+    string command = "\\mv -f '" + source.absolute() + "' '" + destination.absolute()+"'";
     system(command.c_str());
 }
 



From nouiz at mail.berlios.de  Tue Apr 29 16:31:35 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 29 Apr 2008 16:31:35 +0200
Subject: [Plearn-commits] r8912 - trunk/python_modules/plearn/pymake
Message-ID: <200804291431.m3TEVZJZ003237@sheep.berlios.de>

Author: nouiz
Date: 2008-04-29 16:31:35 +0200 (Tue, 29 Apr 2008)
New Revision: 8912

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
added option -dependency_include target [dep] that do as -dependency but with the include dependency instead of the link dependency


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-04-28 20:23:02 UTC (rev 8911)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-04-29 14:31:35 UTC (rev 8912)
@@ -136,6 +136,10 @@
           will look for dependency (which must be a filename.cc or filename.h
           without full path or a library_name) and print out the first path it
           finds in the dependency graph that links target and dependency.
+  -dependency_include: like -dependency execpt that this one list include
+              dependency. i.e. Dependency that make file to be recompiled.
+              -dependency list dependency that make file to be included in 
+              the executable.
   -dist: extract all the sources necessary to compile the target, in a
          directory called <target>.dist, and create there a Makefile that is
          able to compile and link the target.
@@ -143,7 +147,9 @@
   -getoptions: print the specific options for the target
   -vcproj: a Visual Studio project file (.vcproj) for the target will be
            created.
-
+  -o filename: the name of the output file
+  -link-target
+  
 The configuration file 'config' for pymake is searched for
 first in the .pymake subdirectory of the current directory, then similarly
 in the .pymake subdirectory of parent directories of the current directory,
@@ -350,7 +356,7 @@
     return files_to_copy
 
 def copy_ofiles_locally(executables_to_link):
-    print '++++ Copying remaining ofiles locally for ', string.join(map(lambda x: x.filebase, executables_to_link)) 
+    print '++++ Copying remaining ofiles locally for ', string.join(map(lambda x: x.filebase, executables_to_link))
     files_to_copy= get_ofiles_to_copy(executables_to_link)
     for f in files_to_copy:
         copy_ofile_locally(f)
@@ -1165,15 +1171,20 @@
 
     makefile.close()
 
-def find_dependency(args):
+def find_dependency(args,type='link'):
+    '''their is 2 type supported: link and incude
+    link type list dependency at link time, so all .h file 'include'
+        indirectly their .cc file
+    include type is the dependency that make that file need to be recompiled'''
     global sourcedirs
 
     target = args[0]
     configpath = get_config_path(target)
     execfile( configpath, globals() )
+    options = getOptions(options_choices, optionargs)
     sourcedirs = unique(sourcedirs)
 
-    if isccfile(target):
+    if isccfile(target) or type=='include':
         cctarget = target
     else:
         cctarget = get_ccpath_from_noncc_path(target)
@@ -1187,13 +1198,15 @@
 
     if len(args) == 1: # only the target was specified: generate the full graph
         print 'Generating dependency graph in '+target+'.dot ...'
-        info.save_dependency_graph(target+'.dot')
+        info.save_dependency_graph(target+'.dot', type)
         print 'Generating dependency graph view in '+target+'.ps ...'
         os.system('dot -T ps '+target+'.dot > '+target+'.ps')
+        print 'Generating dependency graph view in '+target+'.png ...'
+        os.system('dot -T png '+target+'.dot > '+target+'.png')
     elif len(args) == 2: # target was specified with a possible source dependency
         dep = args[1]
         print 'First encountered dependency path linking '+target+' to '+dep+ ' :'
-        if not info.print_dependency_path(dep,[]):
+        if not info.print_dependency_path(dep,[], type):
             print 'THERE APPEARS TO BE NO SUCH DEPENDENCY.'
 
 
@@ -2154,7 +2167,7 @@
         """returns the filename (without the directory part) of the current node"""
         return self.filebase+self.fileext
 
-    def print_dependency_path(self, dep, visited_files):
+    def print_dependency_path(self, dep, visited_files, type='link'):
         if self in visited_files:
             return False
         visited_files.append(self)
@@ -2168,17 +2181,16 @@
             print '  LIBRARY '+dep
             return True
         else:
-            if not self.is_ccfile and self.corresponding_ccfile and self.corresponding_ccfile.print_dependency_path(dep, visited_files):
+            if type=='link' and not self.is_ccfile and self.corresponding_ccfile and self.corresponding_ccfile.print_dependency_path(dep, visited_files, type):
                 print '  '+self.filepath
                 return True
             for hfile in self.includes_from_sourcedirs:
-                if hfile.print_dependency_path(dep, visited_files):
+                if hfile.print_dependency_path(dep, visited_files, type):
                     print '  '+self.filepath
                     return True
             return False
 
-
-    def build_dependency_graph(self, dotfile, visited_files):
+    def build_dependency_graph(self, dotfile, visited_files, type='link'):
         if self not in visited_files:
             visited_files.append(self)
 
@@ -2186,12 +2198,12 @@
                 dotfile.write(self.dotid()+'[shape="box",label="'+self.filename()+'",fontsize=10,height=0.2,width=0.4,fontname="Helvetica",color="darkgreen",style="filled",fontcolor="white"];\n')
             else: # it's a .h file
                 dotfile.write(self.dotid()+'[shape="box",label="'+self.filename()+'",fontsize=10,height=0.2,width=0.4,fontname="Helvetica",color="blue4",style="filled",fontcolor="white"];\n')
-                if self.corresponding_ccfile:
-                    self.corresponding_ccfile.build_dependency_graph(dotfile, visited_files)
+                if type=='link' and self.corresponding_ccfile:
+                    self.corresponding_ccfile.build_dependency_graph(dotfile, visited_files, type)
                     dotfile.write(self.dotid()+' -> '+self.corresponding_ccfile.dotid()+' [dir=none,color="darkgreen",fontsize=10,style="dashed",fontname="Helvetica"];\n')
 
             for include in self.includes_from_sourcedirs:
-                include.build_dependency_graph(dotfile, visited_files)
+                include.build_dependency_graph(dotfile, visited_files, type)
                 dotfile.write(self.dotid()+' -> '+include.dotid()+' [dir=forward,color="blue4",fontsize=10,style="solid",fontname="Helvetica"];\n')
 
             for lib in self.triggered_libraries:
@@ -2202,7 +2214,7 @@
                 print self.filename(),'->',lib.name
 
 
-    def save_dependency_graph(self, dotfilename):
+    def save_dependency_graph(self, dotfilename, type='link'):
         """Will save the dependency graph originating from this node into the dotfilename file
         This file should end in .dot The dot program can be used to generate a postscript file from it.
         The dependency graph considers include dependencies, as well as linkage dependencies,
@@ -2217,7 +2229,7 @@
   node [fontname="Helvetica",fontsize=10,shape=record];
 """)
         visited_files = []
-        self.build_dependency_graph(dotfile, visited_files)
+        self.build_dependency_graph(dotfile, visited_files, type)
         dotfile.write("}\n")
         dotfile.close()
 
@@ -2544,6 +2556,7 @@
         else:
             otherargs.append(option_to_parse[i])
         i = i + 1
+    del option_to_parse, env_options
     del i # We don't need it anymore and it might be confusing
     
 
@@ -2736,13 +2749,21 @@
     if 'dependency' in optionargs:
         if 1 <= len(otherargs) <= 2:
             optionargs.remove('dependency')
-            options = getOptions(options_choices,optionargs)
             find_dependency(otherargs)
             sys.exit()
         else:
             print 'BAD ARGUMENTS: with -dependency, usage is'
             print '"pymake -dependency target [dependency]"'
             sys.exit(100)
+    elif 'dependency_include' in optionargs:
+        if 1 <= len(otherargs) <= 2:
+            optionargs.remove('dependency_include')
+            find_dependency(otherargs,'include')
+            sys.exit()
+        else:
+            print 'BAD ARGUMENTS: with -dependency_include, usage is'
+            print '"pymake -dependency_include target [dependency]"'
+            sys.exit(100)
 
     if 'dist' in optionargs:
         distribute = 1



From nouiz at mail.berlios.de  Tue Apr 29 16:33:04 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 29 Apr 2008 16:33:04 +0200
Subject: [Plearn-commits] r8913 - trunk/python_modules/plearn/parallel
Message-ID: <200804291433.m3TEX4cG003337@sheep.berlios.de>

Author: nouiz
Date: 2008-04-29 16:33:03 +0200 (Tue, 29 Apr 2008)
New Revision: 8913

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
redirect the output of condor_submit to the console instead of a file. This way the user get imediate feed back if condor_submit failed


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-04-29 14:31:35 UTC (rev 8912)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-04-29 14:33:03 UTC (rev 8913)
@@ -166,9 +166,12 @@
         # It should not take the "" or " " value. Use "." instead.
         self.tmp_dir = 'TMP_DBI'
         #
-        self.file_redirect_stdout = True
-        self.file_redirect_stderr = True
-        self.redirect_stderr_to_stdout = False
+        if not hasattr(self, 'file_redirect_stdout'):
+            self.file_redirect_stdout = True
+        if not hasattr(self, 'file_redirect_stderr'):
+            self.file_redirect_stderr = True
+        if not hasattr(self, 'redirect_stderr_to_stdout'):
+            self.redirect_stderr_to_stdout = False
 
         # Initialize the namespace
         self.test = False
@@ -672,6 +675,10 @@
         self.rank = ''
         self.copy_local_source_file = False
         self.files = ''
+        self.file_redirect_stdout = False
+        self.file_redirect_stderr = False
+        self.redirect_stderr_to_stdout = False
+
         DBIBase.__init__(self, commands, **args)
         if not os.path.exists(self.log_dir):
             os.mkdir(self.log_dir) # condor log are always generated



From nouiz at mail.berlios.de  Tue Apr 29 16:48:32 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 29 Apr 2008 16:48:32 +0200
Subject: [Plearn-commits] r8914 - trunk/scripts
Message-ID: <200804291448.m3TEmWUp004702@sheep.berlios.de>

Author: nouiz
Date: 2008-04-29 16:48:32 +0200 (Tue, 29 Apr 2008)
New Revision: 8914

Modified:
   trunk/scripts/dbidispatch
Log:
allow directory starting with /cluster at iro


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-04-29 14:33:03 UTC (rev 8913)
+++ trunk/scripts/dbidispatch	2008-04-29 14:48:32 UTC (rev 8914)
@@ -228,10 +228,13 @@
 
 from socket import gethostname
 if  launch_cmd == 'Condor' and gethostname().endswith(".iro.umontreal.ca"):
-    if not os.path.abspath(os.path.curdir).startswith("/home/fringant2/") and not dbi_param.get('files'):
+    p = os.path.abspath(os.path.curdir)
+    if p.startswith("/home/fringant2/") or p.startswith("/cluster") or dbi_param.get('files'):
+        pass
+    else:
         raise Exception("You must be in a subfolder of /home/fringant2/")
     f=os.getenv("CONDOR_LOCAL_SOURCE")
-    if f and not f.startswith("/home/fringant2/"):
+    if f and not f.startswith("/home/fringant2/") and not f.startswith("/cluster"):
         dbi_param['copy_local_source_file']=True
 
 print "\n\nThe jobs will be launched on the system:", launch_cmd



From saintmlx at mail.berlios.de  Tue Apr 29 20:21:12 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 29 Apr 2008 20:21:12 +0200
Subject: [Plearn-commits] r8915 - trunk/plearn/python
Message-ID: <200804291821.m3TILCxO010087@sheep.berlios.de>

Author: saintmlx
Date: 2008-04-29 20:21:11 +0200 (Tue, 29 Apr 2008)
New Revision: 8915

Modified:
   trunk/plearn/python/PythonObjectWrapper.cc
Log:
- fix mem leak introduced in r8773



Modified: trunk/plearn/python/PythonObjectWrapper.cc
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.cc	2008-04-29 14:48:32 UTC (rev 8914)
+++ trunk/plearn/python/PythonObjectWrapper.cc	2008-04-29 18:21:11 UTC (rev 8915)
@@ -208,6 +208,7 @@
     PyObject* pyarr= 
         PyArray_CastToType(reinterpret_cast<PyArrayObject*>(pyarr0),
                            PyArray_DescrFromType(PL_NPY_REAL), 0);
+    Py_XDECREF(pyarr0);
     if (! pyarr)
         PLPythonConversionError("ConvertFromPyObject<Vec>", pyobj,
                                 print_traceback);
@@ -232,6 +233,7 @@
     PyObject* pyarr= 
         PyArray_CastToType(reinterpret_cast<PyArrayObject*>(pyarr0),
                            PyArray_DescrFromType(PL_NPY_REAL), 0);
+    Py_XDECREF(pyarr0);
     if (! pyarr)
         PLPythonConversionError("ConvertFromPyObject<Mat>", pyobj,
                                 print_traceback);



From nouiz at mail.berlios.de  Wed Apr 30 16:12:26 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 30 Apr 2008 16:12:26 +0200
Subject: [Plearn-commits] r8916 - in trunk/plearn: ker opt var vmat vmat/test
Message-ID: <200804301412.m3UECQJ6002032@sheep.berlios.de>

Author: nouiz
Date: 2008-04-30 16:12:24 +0200 (Wed, 30 Apr 2008)
New Revision: 8916

Modified:
   trunk/plearn/ker/Kernel.h
   trunk/plearn/opt/ConjGradientOptimizer.cc
   trunk/plearn/var/BiasWeightAffineTransformVariable.cc
   trunk/plearn/var/MatrixSumOfVariable.h
   trunk/plearn/vmat/FinancePreprocVMatrix.cc
   trunk/plearn/vmat/GeneralizedOneHotVMatrix.h
   trunk/plearn/vmat/JoinVMatrix.cc
   trunk/plearn/vmat/JoinVMatrix.h
   trunk/plearn/vmat/MissingIndicatorVMatrix.cc
   trunk/plearn/vmat/MixtureVMatrix.h
   trunk/plearn/vmat/MultiTargetOneHotVMatrix.cc
   trunk/plearn/vmat/OneHotVMatrix.cc
   trunk/plearn/vmat/VMat_basic_stats.cc
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
   trunk/plearn/vmat/test/FileVMatrixTest.cc
Log:
moved dependency to leaf node in the include graph


Modified: trunk/plearn/ker/Kernel.h
===================================================================
--- trunk/plearn/ker/Kernel.h	2008-04-29 18:21:11 UTC (rev 8915)
+++ trunk/plearn/ker/Kernel.h	2008-04-30 14:12:24 UTC (rev 8916)
@@ -44,6 +44,7 @@
 #define Kernel_INC
 
 #include <plearn/base/Object.h>
+#include <plearn/math/TMat_maths.h>
 #include <plearn/vmat/VMat.h>
 
 namespace PLearn {

Modified: trunk/plearn/opt/ConjGradientOptimizer.cc
===================================================================
--- trunk/plearn/opt/ConjGradientOptimizer.cc	2008-04-29 18:21:11 UTC (rev 8915)
+++ trunk/plearn/opt/ConjGradientOptimizer.cc	2008-04-30 14:12:24 UTC (rev 8916)
@@ -43,6 +43,7 @@
 #include "ConjGradientOptimizer.h"
 #include <plearn/io/pl_log.h>
 #include <plearn/var/SumOfVariable.h>
+#include <plearn/math/TMat_maths.h>
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn/var/BiasWeightAffineTransformVariable.cc
===================================================================
--- trunk/plearn/var/BiasWeightAffineTransformVariable.cc	2008-04-29 18:21:11 UTC (rev 8915)
+++ trunk/plearn/var/BiasWeightAffineTransformVariable.cc	2008-04-30 14:12:24 UTC (rev 8916)
@@ -41,6 +41,7 @@
  ******************************************************* */
 
 #include "BiasWeightAffineTransformVariable.h"
+#include <plearn/math/TMat_maths.h>
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn/var/MatrixSumOfVariable.h
===================================================================
--- trunk/plearn/var/MatrixSumOfVariable.h	2008-04-29 18:21:11 UTC (rev 8915)
+++ trunk/plearn/var/MatrixSumOfVariable.h	2008-04-30 14:12:24 UTC (rev 8916)
@@ -44,6 +44,7 @@
 #define MatrixSumOfVariable_INC
 
 #include "NaryVariable.h"
+#include <plearn/math/TMat_maths.h>
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn/vmat/FinancePreprocVMatrix.cc
===================================================================
--- trunk/plearn/vmat/FinancePreprocVMatrix.cc	2008-04-29 18:21:11 UTC (rev 8915)
+++ trunk/plearn/vmat/FinancePreprocVMatrix.cc	2008-04-30 14:12:24 UTC (rev 8916)
@@ -41,6 +41,7 @@
 #include "FinancePreprocVMatrix.h"
 #include <plearn/base/PDate.h>
 #include <plearn/base/tostring.h>
+#include <plearn/math/TMat_maths.h>
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn/vmat/GeneralizedOneHotVMatrix.h
===================================================================
--- trunk/plearn/vmat/GeneralizedOneHotVMatrix.h	2008-04-29 18:21:11 UTC (rev 8915)
+++ trunk/plearn/vmat/GeneralizedOneHotVMatrix.h	2008-04-30 14:12:24 UTC (rev 8916)
@@ -45,6 +45,7 @@
 #define GeneralizedOneHotVMatrix_INC
 
 #include "SourceVMatrix.h"
+#include <plearn/math/TMat_maths.h>
 #include "VMat.h"
 
 namespace PLearn {

Modified: trunk/plearn/vmat/JoinVMatrix.cc
===================================================================
--- trunk/plearn/vmat/JoinVMatrix.cc	2008-04-29 18:21:11 UTC (rev 8915)
+++ trunk/plearn/vmat/JoinVMatrix.cc	2008-04-30 14:12:24 UTC (rev 8916)
@@ -39,7 +39,6 @@
 
 #include "JoinVMatrix.h"
 #include <plearn/base/stringutils.h>
-#include <plearn/math/TMat_maths.h>
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn/vmat/JoinVMatrix.h
===================================================================
--- trunk/plearn/vmat/JoinVMatrix.h	2008-04-29 18:21:11 UTC (rev 8915)
+++ trunk/plearn/vmat/JoinVMatrix.h	2008-04-30 14:12:24 UTC (rev 8916)
@@ -41,6 +41,7 @@
 #define JOINVMATRIX_H
 
 #include "RowBufferedVMatrix.h"
+#include <plearn/math/TMat_maths.h>
 #include "VMat.h"
 #include <map>
 

Modified: trunk/plearn/vmat/MissingIndicatorVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MissingIndicatorVMatrix.cc	2008-04-29 18:21:11 UTC (rev 8915)
+++ trunk/plearn/vmat/MissingIndicatorVMatrix.cc	2008-04-30 14:12:24 UTC (rev 8916)
@@ -41,6 +41,7 @@
 
 
 #include "MissingIndicatorVMatrix.h"
+#include <plearn/math/TMat_maths.h>
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn/vmat/MixtureVMatrix.h
===================================================================
--- trunk/plearn/vmat/MixtureVMatrix.h	2008-04-29 18:21:11 UTC (rev 8915)
+++ trunk/plearn/vmat/MixtureVMatrix.h	2008-04-30 14:12:24 UTC (rev 8916)
@@ -41,6 +41,7 @@
 #define MixtureVMatrix_INC
 
 #include <plearn/vmat/RowBufferedVMatrix.h>
+#include <plearn/math/TMat_maths.h>
 #include <plearn/vmat/VMat.h>
 
 namespace PLearn {

Modified: trunk/plearn/vmat/MultiTargetOneHotVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MultiTargetOneHotVMatrix.cc	2008-04-29 18:21:11 UTC (rev 8915)
+++ trunk/plearn/vmat/MultiTargetOneHotVMatrix.cc	2008-04-30 14:12:24 UTC (rev 8916)
@@ -42,6 +42,7 @@
 
 
 #include "MultiTargetOneHotVMatrix.h"
+#include <plearn/math/TMat_maths.h>
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn/vmat/OneHotVMatrix.cc
===================================================================
--- trunk/plearn/vmat/OneHotVMatrix.cc	2008-04-29 18:21:11 UTC (rev 8915)
+++ trunk/plearn/vmat/OneHotVMatrix.cc	2008-04-30 14:12:24 UTC (rev 8916)
@@ -39,6 +39,7 @@
  ******************************************************* */
 
 #include "OneHotVMatrix.h"
+#include <plearn/math/TMat_maths.h>
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn/vmat/VMat_basic_stats.cc
===================================================================
--- trunk/plearn/vmat/VMat_basic_stats.cc	2008-04-29 18:21:11 UTC (rev 8915)
+++ trunk/plearn/vmat/VMat_basic_stats.cc	2008-04-30 14:12:24 UTC (rev 8916)
@@ -49,6 +49,7 @@
 //#include <plearn/math/TMat_maths.h>
 #include <plearn/math/stats_utils.h>
 #include <plearn/math/VecStatsCollector.h>
+#include <plearn/math/TMat_maths.h>
 //#include <plearn/sys/PLMPI.h>
 
 namespace PLearn {

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-04-29 18:21:11 UTC (rev 8915)
+++ trunk/plearn/vmat/VMatrix.cc	2008-04-30 14:12:24 UTC (rev 8916)
@@ -51,6 +51,7 @@
 #include <plearn/math/random.h>      //!< For uniform_multinomial_sample()
 #include <plearn/base/RemoteDeclareMethod.h>
 #include <nspr/prenv.h>
+#include <plearn/math/TMat_maths.h> //!< for dot, powdistance externalProductAcc
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-04-29 18:21:11 UTC (rev 8915)
+++ trunk/plearn/vmat/VMatrix.h	2008-04-30 14:12:24 UTC (rev 8916)
@@ -50,7 +50,6 @@
 #include <plearn/math/StatsCollector.h>
 #include "VMField.h"
 #include <plearn/dict/Dictionary.h>
-#include <plearn/math/TMat_maths.h>
 #include <plearn/io/PPath.h>
 
 #include <map>

Modified: trunk/plearn/vmat/test/FileVMatrixTest.cc
===================================================================
--- trunk/plearn/vmat/test/FileVMatrixTest.cc	2008-04-29 18:21:11 UTC (rev 8915)
+++ trunk/plearn/vmat/test/FileVMatrixTest.cc	2008-04-30 14:12:24 UTC (rev 8916)
@@ -42,6 +42,7 @@
 
 
 #include "FileVMatrixTest.h"
+#include <plearn/math/TMat_maths.h>
 #include <plearn/math/pl_math.h>
 #include <plearn/vmat/FileVMatrix.h>
 #include <plearn/io/fileutils.h>



From nouiz at mail.berlios.de  Wed Apr 30 16:12:59 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 30 Apr 2008 16:12:59 +0200
Subject: [Plearn-commits] r8917 - trunk/commands/PLearnCommands
Message-ID: <200804301412.m3UECxCC002086@sheep.berlios.de>

Author: nouiz
Date: 2008-04-30 16:12:58 +0200 (Wed, 30 Apr 2008)
New Revision: 8917

Modified:
   trunk/commands/PLearnCommands/TestDependenciesCommand.cc
Log:
forgeted in last commit


Modified: trunk/commands/PLearnCommands/TestDependenciesCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/TestDependenciesCommand.cc	2008-04-30 14:12:24 UTC (rev 8916)
+++ trunk/commands/PLearnCommands/TestDependenciesCommand.cc	2008-04-30 14:12:58 UTC (rev 8917)
@@ -38,6 +38,7 @@
 
 /*! \file TestDependenciesCommand.cc */
 #include "TestDependenciesCommand.h"
+#include <plearn/math/TMat_maths.h>
 #include <plearn/db/getDataSet.h>
 #include <plearn/math/stats_utils.h>
 #include <plearn/vmat/VMat_basic_stats.h>



From nouiz at mail.berlios.de  Wed Apr 30 16:13:28 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 30 Apr 2008 16:13:28 +0200
Subject: [Plearn-commits] r8918 - trunk/commands
Message-ID: <200804301413.m3UEDSMA002134@sheep.berlios.de>

Author: nouiz
Date: 2008-04-30 16:13:27 +0200 (Wed, 30 Apr 2008)
New Revision: 8918

Modified:
   trunk/commands/plearn_desjardins.cc
Log:
Added missing include. It was included indirectly by AdaBoost in the past, but not anymore


Modified: trunk/commands/plearn_desjardins.cc
===================================================================
--- trunk/commands/plearn_desjardins.cc	2008-04-30 14:12:58 UTC (rev 8917)
+++ trunk/commands/plearn_desjardins.cc	2008-04-30 14:13:27 UTC (rev 8918)
@@ -66,6 +66,7 @@
  * PLearner *
  ************/
 #include <plearn_learners/generic/AddCostToLearner.h>
+#include <plearn_learners/regressors/RegressionTree.h>
 
 /************
  * Splitter *



From tihocan at mail.berlios.de  Wed Apr 30 17:06:39 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 30 Apr 2008 17:06:39 +0200
Subject: [Plearn-commits] r8919 - trunk/plearn/var
Message-ID: <200804301506.m3UF6dux006358@sheep.berlios.de>

Author: tihocan
Date: 2008-04-30 17:06:38 +0200 (Wed, 30 Apr 2008)
New Revision: 8919

Modified:
   trunk/plearn/var/DiagonalizedFactorsProductVariable.cc
Log:
Added missing include - compilation was failing on mammouth

Modified: trunk/plearn/var/DiagonalizedFactorsProductVariable.cc
===================================================================
--- trunk/plearn/var/DiagonalizedFactorsProductVariable.cc	2008-04-30 14:13:27 UTC (rev 8918)
+++ trunk/plearn/var/DiagonalizedFactorsProductVariable.cc	2008-04-30 15:06:38 UTC (rev 8919)
@@ -42,6 +42,7 @@
 
 #include "DiagonalizedFactorsProductVariable.h"
 #include "Var_utils.h"
+#include <plearn/math/TMat_maths.h>
 
 namespace PLearn {
 using namespace std;



From nouiz at mail.berlios.de  Wed Apr 30 17:37:26 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 30 Apr 2008 17:37:26 +0200
Subject: [Plearn-commits] r8920 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200804301537.m3UFbQdd008165@sheep.berlios.de>

Author: nouiz
Date: 2008-04-30 17:37:25 +0200 (Wed, 30 Apr 2008)
New Revision: 8920

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
removed dependency to plearn for dbi.py


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-04-30 15:06:38 UTC (rev 8919)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-04-30 15:37:25 UTC (rev 8920)
@@ -15,10 +15,6 @@
 from threading import Thread,Lock
 from time import sleep
 import datetime
-from plearn.pymake.pymake import get_list_of_hosts
-#from plearn.pymake.pymake import get_distcc_hosts
-from plearn.pymake.pymake import locateconfigfile
-from plearn.pymake.pymake import get_platform
 
 try:
     from random import shuffle
@@ -1144,15 +1140,41 @@
     def __repr__(self):
         return str(self)
 
+def get_hostname():
+    from socket import gethostname
+    myhostname = gethostname()
+    pos = string.find(myhostname,'.')
+    if pos>=0:
+        myhostname = myhostname[0:pos]
+    return myhostname
+
+# copied from PLearn/python_modules/plearn/pymake/pymake.py
+def get_platform():
+    #should we use an env variable called PLATFORM???
+    #if not defined, use uname uname -i???
+    pymake_osarch = os.getenv('PYMAKE_OSARCH')
+    if pymake_osarch:
+        return pymake_osarch
+    platform = sys.platform
+    if platform=='linux2':
+        linux_type = os.uname()[4]
+        if linux_type == 'ppc':
+            platform = 'linux-ppc'
+        elif linux_type =='x86_64':
+            platform = 'linux-x86_64'
+        else:
+            platform = 'linux-i386'
+    return platform
+
+# copied from PLearn/python_modules/plearn/pymake/pymake.py
 def find_all_ssh_hosts():
     hostspath_list = [os.path.join(os.getenv("HOME"),".pymake",get_platform()+'.hosts')]
     if os.path.exists(hostspath_list[0])==0:
         print "[DBI] no host file %s for the ssh backend"%(hostspath_list[0])
         sys.exit(1)
     print "[DBI] using file %s for the list of host"%(hostspath_list[0])
-    from plearn.pymake.pymake import process_hostspath_list
-    from plearn.pymake.pymake import get_hostname
-    (list_of_hosts, nice_values) = process_hostspath_list(hostspath_list,19,get_hostname())
+#    from plearn.pymake.pymake import process_hostspath_list
+#    (list_of_hosts, nice_values) = process_hostspath_list(hostspath_list,19,get_hostname())
     shuffle(list_of_hosts)
     print list_of_hosts
     print nice_values

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-04-30 15:06:38 UTC (rev 8919)
+++ trunk/scripts/dbidispatch	2008-04-30 15:37:25 UTC (rev 8920)
@@ -145,7 +145,8 @@
         dbi_param["dolog"]=False
     elif argv == "--dbilog":
         dbi_param["dolog"]=True
-    elif argv.split('=')[0] in ["--bqtools","--cluster","--local","--condor"]:
+    elif argv.split('=')[0] in ["--bqtools","--cluster","--local","--condor",
+                                "--ssh"]:
         launch_cmd = argv[2].upper()+argv.split('=')[0][3:]
         if len(argv.split('='))>1:
             dbi_param["nb_proc"]=argv.split('=')[1]



From nouiz at mail.berlios.de  Wed Apr 30 17:53:15 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 30 Apr 2008 17:53:15 +0200
Subject: [Plearn-commits] r8921 - in trunk/scripts: . dbi.test
Message-ID: <200804301553.m3UFrFjb009567@sheep.berlios.de>

Author: nouiz
Date: 2008-04-30 17:53:15 +0200 (Wed, 30 Apr 2008)
New Revision: 8921

Added:
   trunk/scripts/dbi.test/
   trunk/scripts/dbi.test/dbi
   trunk/scripts/dbi.test/dbidispatch
Log:
temporary directory to be able to transfert the history of 2 files to mercurial.



Copied: trunk/scripts/dbi.test/dbi (from rev 8920, trunk/scripts/dbi)

Copied: trunk/scripts/dbi.test/dbidispatch (from rev 8920, trunk/scripts/dbidispatch)



